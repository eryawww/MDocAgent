{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "arXiv:1907.03060v1  [cs.CL]  6 Jul 2019 Exploiting Out-of-Domain Parallel Data through Multilingual Transfer Learning for Low-Resource Neural Machine Translation\u2217 Aizhan Imankulova\u2020 Raj Dabre\u2021 Atsushi Fujita\u2021 Kenji Imamura\u2021 \u2020Tokyo Metropolitan University 6-6 Asahigaoka, Hino, Tokyo 191-0065, Japan imankulova-aizhan@ed.tmu.ac.jp \u2021National Institute of Information and Communications Technology 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan {raj.dabre, atsushi.fujita, kenji.imamura}@nict.go.jp Abstract This paper proposes a novel multilin- gual multistage \ufb01ne-tuning approach for low-resource neural machine translation (NMT), taking a challenging Japanese\u2013 Russian pair for benchmarking.",
            "Al- though there are many solutions for low- resource scenarios, such as multilingual NMT and back-translation, we have em- pirically con\ufb01rmed their limited success when restricted to in-domain data. We therefore propose to exploit out-of-domain data through transfer learning, by us- ing it to \ufb01rst train a multilingual NMT model followed by multistage \ufb01ne-tuning on in-domain parallel and back-translated pseudo-parallel data. Our approach, which combines domain adaptation, multilin- gualism, and back-translation, helps im- prove the translation quality by more than 3.7 BLEU points, over a strong baseline, for this extremely low-resource scenario. 1 Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has enabled end-to-end training of a trans- lation system without needing to deal with word alignments, translation rules, and complicated de- coding algorithms, which are the characteristics of phrase-based statistical machine translation (PB- SMT) (Koehn et al., 2007).",
            "Although NMT can be signi\ufb01cantly better than PBSMT in resource- rich scenarios, PBSMT performs better in low- resource scenarios (Koehn and Knowles, 2017). \u2217The contents in this manuscript are identical to those in our formal publication at the 17th Machine Translation Sum- mit, whereas the style is slightly modi\ufb01ed. Only by exploiting cross-lingual transfer learn- ing techniques (Firat et al., 2016; Zoph et al., 2016; Kocmi and Bojar, 2018), can the NMT per- formance approach PBSMT performance in low- resource scenarios. However, such methods usually require an NMT model trained on a resource-rich language pair like French\u2194English (parent), which is to be \ufb01ne-tuned for a low-resource language pair like Uzbek\u2194English (child). On the other hand, multi- lingual approaches (Johnson et al., 2017) propose to train a single model to translate multiple lan- guage pairs. However, these approaches are effec- tive only when the parent target or source language is relatively resource-rich like English (En).",
            "On the other hand, multi- lingual approaches (Johnson et al., 2017) propose to train a single model to translate multiple lan- guage pairs. However, these approaches are effec- tive only when the parent target or source language is relatively resource-rich like English (En). Fur- thermore, the parents and children models should be trained on similar domains; otherwise, one has to take into account an additional problem of do- main adaptation (Chu et al., 2017). In this paper, we work on a linguisti- cally distant and thus challenging language pair Japanese\u2194Russian (Ja\u2194Ru) which has only 12k lines of news domain parallel corpus and hence is extremely resource-poor. Furthermore, the amount of indirect in-domain parallel corpora, i.e., Ja\u2194En and Ru\u2194En, are also small. As we demonstrate in Section 4, this severely lim- its the performance of prominent low-resource techniques, such as multilingual modeling, back- translation, and pivot-based PBSMT.",
            "As we demonstrate in Section 4, this severely lim- its the performance of prominent low-resource techniques, such as multilingual modeling, back- translation, and pivot-based PBSMT. To remedy this, we propose a novel multistage \ufb01ne-tuning method for NMT that combines multilingual mod- eling (Johnson et al., 2017) and domain adaptation (Chu et al., 2017). We have addressed two important research questions (RQs) in the context of extremely low- resource machine translation (MT) and our explo- rations have derived rational contributions (CTs) as follows:",
            "RQ1. What kind of translation quality can we ob- tain in an extremely low-resource scenario? CT1. We have made extensive comparisons with multiple architectures and MT paradigms to show how dif\ufb01cult the problem is. We have also explored the utility of back-translation and show that it is ineffective given the poor performance of base MT systems used to generate pseudo-parallel data. Our system- atic exploration shows that multilingualism is extremely useful for in-domain translation with very limited corpora (see Section 4). This type of exhaustive exploration has been missing from most existing works. RQ2. What are the effective ways to exploit out- of-domain data for extremely low-resource in-domain translation? CT2. Our proposal is to \ufb01rst train a multilin- gual NMT model on out-of-domain Ja\u2194En and Ru\u2194En data, then \ufb01ne-tune it on in- domain Ja\u2194En and Ru\u2194En data, and fur- ther \ufb01ne-tune it on Ja\u2194Ru data (see Sec- tion 5).",
            "We show that this stage-wise \ufb01ne- tuning is crucial for high-quality translation. We then show that the improved NMT mod- els lead to pseudo-parallel data of better qual- ity. This data can then be used to improve the performance even further thereby enabling the generation of better pseudo-parallel data. By iteratively generating pseudo-parallel data and \ufb01ne-tuning the model on said data, we can achieve the best performance for Japanese\u2194Russian translation. To the best of our knowledge, we are the \ufb01rst to perform such an extensive evaluation of extremely low-resource MT problem and propose a novel multilingual multistage \ufb01ne-tuning approach in- volving multilingual modeling and domain adap- tation to address it. 2 Our Japanese\u2013Russian Setting In this paper, we deal with Ja\u2194Ru news trans- lation. This language pair is very challenging because the languages involved have completely different writing system, phonology, morphology, grammar, and syntax. Among various domains, we experimented with translations in the news do- main, considering the importance of sharing news between different language speakers.",
            "This language pair is very challenging because the languages involved have completely different writing system, phonology, morphology, grammar, and syntax. Among various domains, we experimented with translations in the news do- main, considering the importance of sharing news between different language speakers. Moreover, news domain is one of the most challenging tasks, Ru Ja En #sent. Usage test development \u2713 \u2713 \u2713 913 600 313 \u2713 \u2713 173 - 173 \u2713 \u2713 276 - 276 \u2713 \u2713 0 - - \u2713 4 - - \u2713 287 - - \u2713 1 - - Total 1,654 - - Table 1: Manually aligned News Commentary data. due to large presence of out-of-vocabulary (OOV) tokens and long sentences.1 To establish and eval- uate existing methods, we also involved English as the third language. As direct parallel corpora are scarce, involving a language such as English for pivoting is quite common (Utiyama and Isahara, 2007). There has been no clean held-out parallel data for Ja\u2194Ru and Ja\u2194En news translation.",
            "As direct parallel corpora are scarce, involving a language such as English for pivoting is quite common (Utiyama and Isahara, 2007). There has been no clean held-out parallel data for Ja\u2194Ru and Ja\u2194En news translation. There- fore, we manually compiled development and test sets using News Commentary data2 as a source. Since the given Ja\u2194Ru and Ja\u2194En data share many lines in the Japanese side, we \ufb01rst compiled tri-text data. Then, from each line, correspond- ing parts across languages were manually identi- \ufb01ed, and unaligned parts were split off into a new line. Note that we have never merged two or more lines. As a result, we obtained 1,654 lines of data comprising trilingual, bilingual, and monolingual segments (mainly sentences) as summarized in Ta- ble 1. Finally, for the sake of comparability, we randomly chose 600 trilingual sentences to create a test set, and concatenated the rest of them and bilingual sentences to form development sets.",
            "Finally, for the sake of comparability, we randomly chose 600 trilingual sentences to create a test set, and concatenated the rest of them and bilingual sentences to form development sets. Our manually aligned development and test sets are publicly available.3 3 Related Work Koehn and Knowles (2017) showed that NMT is unable to handle low-resource language pairs as opposed to PBSMT. Transfer learning approaches (Firat et al., 2016; Zoph et al., 2016; Kocmi and Bojar, 2018) work well when a large helping par- allel corpus is available. This restricts one of the source or the target languages to be English which, in our case, is not possible. Approaches involving bi-directional NMT modeling is shown to drasti- cally improve low-resource translation (Niu et al., 1News domain translation is also the most competitive tasks in WMT indicating its importance. 2http:\/\/opus.nlpl.eu\/News-Commentary-v11.php 3https:\/\/github.com\/aizhanti\/JaRuNC",
            "2018). However, like most other, this work fo- cuses on translation from and into English. Remaining options include (a) unsupervised MT (Artetxe et al., 2018; Lample et al., 2018; Marie and Fujita, 2018), (b) parallel sentence mining from non-parallel or comparable corpora (Utiyama and Isahara, 2003; Tillmann and Xu, 2009), (c) generating pseudo-parallel data (Sen- nrich et al., 2016), and (d) MT based on pivot lan- guages (Utiyama and Isahara, 2007). The linguis- tic distance between Japanese and Russian makes it extremely dif\ufb01cult to learn bilingual knowledge, such as bilingual lexicons and bilingual word em- beddings. Unsupervised MT is thus not promising yet, due to its heavy reliance on accurate bilingual word embeddings. Neither does parallel sentence mining, due to the dif\ufb01culty of obtaining accurate bilingual lexicons.",
            "Unsupervised MT is thus not promising yet, due to its heavy reliance on accurate bilingual word embeddings. Neither does parallel sentence mining, due to the dif\ufb01culty of obtaining accurate bilingual lexicons. Pseudo-parallel data can be used to augment existing parallel corpora for train- ing, and previous work has reported that such data generated by so-called back-translation can sub- stantially improve the quality of NMT. However, this approach requires base MT systems that can generate somewhat accurate translations. It is thus infeasible in our scenario, because we can obtain only a weak system which is the consequence of an extremely low-resource situation. MT based on pivot languages requires large in-domain parallel corpora involving the pivot languages. This tech- nique is thus infeasible, because the in-domain parallel corpora for Ja\u2194En and Ru\u2194En pairs are also extremely limited, whereas there are large parallel corpora in other domains. Section 4 em- pirically con\ufb01rms the limit of these existing ap- proaches.",
            "Section 4 em- pirically con\ufb01rms the limit of these existing ap- proaches. Fortunately, there are two useful transfer learn- ing solutions using NMT: (e) multilingual model- ing to incorporate multiple language pairs into a single model (Johnson et al., 2017) and (f) domain adaptation to incorporate out-of-domain data (Chu et al., 2017). In this paper, we explore a novel method involving step-wise \ufb01ne-tuning to com- bine these two methods. By improving the trans- lation quality in this way, we can also increase the likelihood of pseudo-parallel data being useful to further improve translation quality. 4 Limit of Using only In-domain Data This section answers our \ufb01rst research question, [RQ1], about the translation quality that we can achieve using existing methods and in-domain par- Lang.pair Partition #sent.",
            "4 Limit of Using only In-domain Data This section answers our \ufb01rst research question, [RQ1], about the translation quality that we can achieve using existing methods and in-domain par- Lang.pair Partition #sent. #tokens #types Ja\u2194Ru train 12,356 341k \/ 229k 22k \/ 42k development 486 16k \/ 11k 2.9k \/ 4.3k test 600 22k \/ 15k 3.5k \/ 5.6k Ja\u2194En train 47,082 1.27M \/ 1.01M 48k \/ 55k development 589 21k \/ 16k 3.5k \/ 3.8k test 600 22k \/ 17k 3.5k \/ 3.8k Ru\u2194En train 82,072 1.61M \/ 1.83M 144k \/ 74k development 313 7.8k \/ 8.4k 3.2k \/ 2.3k test 600 15k \/ 17k 5.6k \/ 3.8k Table 2: Statistics on our in-domain parallel data.",
            "allel and monolingual data. We then use the strongest model to conduct experiments on gener- ating and utilizing back-translated pseudo-parallel data for augmenting NMT. Our intention is to em- pirically identify the most effective practices as well as recognize the limitations of relying only on in-domain parallel corpora. 4.1 Data To train MT systems among the three languages, i.e., Japanese, Russian, and English, we used all the parallel data provided by Global Voices,4 more speci\ufb01cally those available at OPUS.5 Table 2 summarizes the size of train\/development\/test splits used in our experiments. The number of parallel sentences for Ja\u2194Ru is 12k, for Ja\u2194En is 47k, and for Ru\u2194En is 82k. Note that the three corpora are not mutually exclusive: 9k out of 12k sentences in the Ja\u2194Ru corpus were also included in the other two parallel corpora, associated with identical English translations. This puts a limit on the positive impact that the helping corpora can have on the translation quality.",
            "This puts a limit on the positive impact that the helping corpora can have on the translation quality. Even when one focuses on low-resource lan- guage pairs, we often have access to larger quan- tities of in-domain monolingual data of each lan- guage. Such monolingual data are useful to im- prove quality of MT, for example, as the source of pseudo-parallel data for augmenting training data for NMT (Sennrich et al., 2016) and as the train- ing data for large and smoothed language mod- els for PBSMT (Koehn and Knowles, 2017). Ta- ble 3 summarizes the statistics on our monolingual corpora for several domains including the news domain. Note that we removed from the Global Voices monolingual corpora those sentences that are already present in the parallel corpus. 4https:\/\/globalvoices.org\/ 5http:\/\/opus.nlpl.eu\/GlobalVoices-v2015.php",
            "Corpus Ja Ru En Global Voices5 26k 24k 842k Wikinews6 37k 243k - News Crawl7 - 72M 194M Yomiuri (2007\u20132011)8 19M - - IWSLT9 411k 64k 66k Tatoeba10 5k 58k 208k Table 3: Number of lines in our monolingual data. Whereas the \ufb01rst four are from the news corpora (in- domain), the last two, i.e., \u201cIWSLT\u201d and \u201cTatoeba,\u201d are from other domains. We tokenized English and Russian sentences using tokenizer.perl of Moses (Koehn et al., 2007).11 To tokenize Japanese sentences, we used MeCab12 with the IPA dictionary. After tokeniza- tion, we eliminated duplicated sentence pairs and sentences with more than 100 tokens for all the languages. 4.2 MT Methods Examined We began with evaluating standard MT paradigms, i.e., PBSMT (Koehn et al., 2007) and NMT (Sutskever et al., 2014).",
            "4.2 MT Methods Examined We began with evaluating standard MT paradigms, i.e., PBSMT (Koehn et al., 2007) and NMT (Sutskever et al., 2014). As for PBSMT, we also examined two advanced methods: pivot- based translation relying on a helping language (Utiyama and Isahara, 2007) and induction of phrase tables from monolingual data (Marie and Fujita, 2018). As for NMT, we compared two types of encoder-decoder architectures: attentional RNN- based model (RNMT) (Bahdanau et al., 2015) and the Transformer model (Vaswani et al., 2017). In addition to standard uni-directional modeling, to cope with the low-resource problem, we exam- ined two multi-directional models: bi-directional model (Niu et al., 2018) and multi-to-multi (M2M) model (Johnson et al., 2017).",
            "In addition to standard uni-directional modeling, to cope with the low-resource problem, we exam- ined two multi-directional models: bi-directional model (Niu et al., 2018) and multi-to-multi (M2M) model (Johnson et al., 2017). After identifying the best model, we also exam- ined the usefulness of a data augmentation method based on back-translation (Sennrich et al., 2016). PBSMT Systems First, we built a PBSMT system for each of the six translation directions. We obtained phrase ta- bles from parallel corpus using SyMGIZA++13 6https:\/\/dumps.wikimedia.org\/backup-index.html (20180501) 7http:\/\/www.statmt.org\/wmt18\/translation-task.html 8https:\/\/www.yomiuri.co.jp\/database\/glossary\/ 9http:\/\/www.cs.jhu.edu\/\u223ckevinduh\/a\/multitarget-tedtalks\/ 10http:\/\/opus.nlpl.eu\/Tatoeba-v2.php 11https:\/\/github.com\/moses-smt\/mosesdecoder 12http:\/\/taku910.github.io\/mecab, version 0.996.",
            "13https:\/\/github.com\/emjotde\/symgiza-pp with the grow-diag-finalheuristics for word alignment, and Moses for phrase pair extraction. Then, we trained a bi-directional MSD (monotone, swap, and discontinuous) lexicalized reordering model. We also trained three 5-gram language models, using KenLM14 on the following mono- lingual data: (1) the target side of the parallel data, (2) the concatenation of (1) and the monolingual data from Global Voices, and (3) the concatena- tion of (1) and all monolingual data in the news domain in Table 3. Subsequently, using English as the pivot lan- guage, we examined the following three types of pivot-based PBSMT systems (Utiyama and Isa- hara, 2007; Cohn and Lapata, 2007) for each of Ja\u2192Ru and Ru\u2192Ja. Cascade: 2-step decoding using the source-to- English and English-to-target systems.",
            "Cascade: 2-step decoding using the source-to- English and English-to-target systems. Synthesize: Obtain a new phrase table from syn- thetic parallel data generated by translating English side of the target\u2013English training parallel data to the source language with the English-to-source system. Triangulate: Compile a new phrase table com- bining those for the source-to-English and English-to-target systems. Among these three, triangulation is the most com- putationally expensive method. Although we had \ufb01ltered the component phrase tables using the statistical signi\ufb01cance pruning method (Johnson et al., 2007), triangulation can generate an enor- mous number of phrase pairs. To reduce the com- putational cost during decoding and the negative effects of potentially noisy phrase pairs, we re- tained for each source phrase s only the k-best translations t according to the forward translation probability \u03c6(t|s) calculated from the conditional probabilities in the component models as de\ufb01ned in Utiyama and Isahara (2007).",
            "For each of the retained phrase pairs, we also calculated the back- ward translation probability, \u03c6(s|t), and lexical translation probabilities, \u03c6lex(t|s) and \u03c6lex(s|t), in the same manner as \u03c6(t|s). We also investigated the utility of recent ad- vances in unsupervised MT. Even though we be- gan with a publicly available implementation of unsupervised PBSMT (Lample et al., 2018),15 14https:\/\/github.com\/kpu\/kenlm 15https:\/\/github.com\/facebookresearch\/UnsupervisedMT",
            "ID System Parallel data Total size of Vocabulary Ja\u2194Ru Ja\u2194En Ru\u2194En training data size (a1), (b1) Ja\u2192Ru or Ru\u2192Ja 12k - - 12k 16k Ja\u2192En or En\u2192Ja - 47k - 47k 16k Ru\u2192En or En\u2192Ru - - 82k 82k 16k (a2), (b2) Ja\u2192Ru and Ru\u2192Ja 12k - - 24k 16k Ja\u2192En and En\u2192Ja - 47k - 94k 16k Ru\u2192En and En\u2192Ru - - 82k 164k 16k (a3), (b3) M2M systems 12k\u219282k 47k\u219282k 82k 492k 32k Table 4: Con\ufb01guration of uni-, bi-directional, and M2M NMT baseline systems. Arrows in \u201cParallel data\u201d columns indicate the over-sampling of the parallel data to match the size of the largest parallel data. it crashed due to unknown reasons.",
            "Arrows in \u201cParallel data\u201d columns indicate the over-sampling of the parallel data to match the size of the largest parallel data. it crashed due to unknown reasons. We there- fore followed another method described in Marie and Fujita (2018). Instead of short n-grams (Artetxe et al., 2018; Lample et al., 2018), we collected a set of phrases in Japanese and Russian from respective monolingual data using the word2phrase algorithm (Mikolov et al., 2013),16 as in Marie and Fujita (2018). To re- duce the complexity, we used randomly selected 10M monolingual sentences, and 300k most fre- quent phrases made of words among the 300k most frequent words. For each source phrase s, we selected 300-best target phrases t accord- ing to the translation probability as in Lample et al.",
            "For each source phrase s, we selected 300-best target phrases t accord- ing to the translation probability as in Lample et al. (2018): p(t|s) = exp(\u03b2 cos(emb(t),emb(s)) P t\u2032 exp(\u03b2 cos(emb(t\u2032),emb(s)), where emb(\u00b7) stands for a bilingual embed- ding of a given phrase, obtained through aver- aging bilingual embeddings of constituent words learned from the two monolingual data using fastText17 and vecmap.18 For each of the retained phrase pair, p(s|t) was computed analo- gously. We also computed lexical translation prob- abilities relying on those learned from the given small parallel corpus. Up to four phrase tables were jointly ex- ploited by the multiple decoding path ability of Moses. Weights for the features were tuned us- ing KB-MIRA (Cherry and Foster, 2012) on the development set; we took the best weights after 15 iterations.",
            "Up to four phrase tables were jointly ex- ploited by the multiple decoding path ability of Moses. Weights for the features were tuned us- ing KB-MIRA (Cherry and Foster, 2012) on the development set; we took the best weights after 15 iterations. Two hyper-parameters, namely, k for the number of pivot-based phrase pairs per source phrase and d for distortion limit, were determined by a grid search on k \u2208{10, 20, 40, 60, 80, 100} and d \u2208{8, 10, 12, 14, 16, 18, 20}. In contrast, we used predetermined hyper-parameters for phrase table induction from monolingual data, following the convention: 200 for the dimension of word and phrase embeddings and \u03b2 = 30. 16https:\/\/code.google.com\/archive\/p\/word2vec\/ 17https:\/\/fasttext.cc\/ 18https:\/\/github.com\/artetxem\/vecmap NMT Systems We used the open-source implementation of the RNMT and the Transformer models in tensor2tensor.19 A uni-directional model for each of the six translation directions was trained on the corresponding parallel corpus.",
            "Bi- directional and M2M models were realized by adding an arti\ufb01cial token that speci\ufb01es the tar- get language to the beginning of each source sen- tence and shuf\ufb02ing the entire training data (John- son et al., 2017). Table 4 contains some speci\ufb01c hyper- parameters20 for our baseline NMT models. The hyper-parameters not mentioned in this table used the default values in tensor2tensor. For M2M systems, we over-sampled Ja\u2192Ru and Ja\u2192En training data so that their sizes match the largest Ru\u2192En data. To reduce the number of unknown words, we used tensor2tensor\u2019s internal sub-word segmentation mechanism. Since we work in a low-resource setting, we used shared sub-word vocabularies of size 16k for the uni- and bi-directional models and 32k for the M2M models. The number of training iterations was determined by early-stopping: we evaluated our models on the development set every 1,000 updates, and stopped training if BLEU score for the development set was not improved for 10,000 updates (10 check-points).",
            "The number of training iterations was determined by early-stopping: we evaluated our models on the development set every 1,000 updates, and stopped training if BLEU score for the development set was not improved for 10,000 updates (10 check-points). Note that the development set was created by concatenating those for the individual translation directions without any over-sampling. Having trained the models, we averaged the last 10 check-points and decoded the test sets with a beam size of 4 and a length penalty which was tuned by a linear search on the BLEU score for the development set. 19https:\/\/github.com\/tensor\ufb02ow\/tensor2tensor, version 1.6.6. 20We compared two mini-batch sizes, 1024 and 6144 to- kens, and found that 6144 and 1024 worked better for RNMT and Transformer, respectively.",
            "ID System Ja\u2192Ru Ru\u2192Ja Ja\u2192En En\u2192Ja Ru\u2192En En\u2192Ru (a1) Uni-directional RNMT 0.58 1.86 2.41 7.83 18.42 13.64 (a2) Bi-directional RNMT 0.65 1.61 6.18 8.81 19.60 15.11 (a3) M2M RNMT 1.51 4.29 5.15 7.55 14.24 10.86 (b1) Uni-directional Transformer 0.70 1.96 4.36 7.97 20.70 16.24 (b2) Bi-directional Transformer 0.19 0.87 6.48 10.63 22.25 16.03 (b3) M2M Transformer 3.72 8.35 10.24 12.43 22.10 16.92 (c1) Uni-directional supervised PBSMT 2.02 4.45 8.19 10.27 22.37 16.",
            "72 8.35 10.24 12.43 22.10 16.92 (c1) Uni-directional supervised PBSMT 2.02 4.45 8.19 10.27 22.37 16.52 Table 5: BLEU scores of baseline systems. Bold indicates the best BLEU score for each translation direction. Similarly to PBSMT, we also evaluated \u201cCas- cade\u201d and \u201cSynthesize\u201d methods with uni- directional NMT models. 4.3 Results We evaluated MT models using case-sensitive and tokenized BLEU (Papineni et al., 2002) on test sets, using Moses\u2019s multi-bleu.perl. Statistical signi\ufb01cance (p < 0.05) on the difference of BLEU scores was tested by Moses\u2019s bootstrap- hypothesis-difference-signi\ufb01cance.pl. Tables 5 and 6 show BLEU scores of all the models, except the NMT systems augmented with back-translations.",
            "Tables 5 and 6 show BLEU scores of all the models, except the NMT systems augmented with back-translations. Whereas some models achieved reasonable BLEU scores for Ja\u2194En and Ru\u2194En translation, all the results for Ja\u2194Ru, which is our main concern, were abysmal. Among the NMT models, Transformer models (b\u2217) were proven to be better than RNMT models (a\u2217). RNMT models could not even outperform the uni-directional PBSMT models (c1). M2M models (a3) and (b3) outperformed their corre- sponding uni- and bi-directional models in most cases. It is worth noting that in this extremely low-resource scenario, BLEU scores of the M2M RNMT model for the largest language pair, i.e., Ru\u2194En, were lower than those of the uni- and bi-directional RNMT models as in Johnson et al. (2017). In contrast, with the M2M Transformer model, Ru\u2194En also bene\ufb01ted from multilingual- ism.",
            "(2017). In contrast, with the M2M Transformer model, Ru\u2194En also bene\ufb01ted from multilingual- ism. Standard PBSMT models (c1) achieved higher BLEU scores than uni-directional NMT mod- els (a1) and (b1), as reported by Koehn and Knowles (2017), whereas they underperform the M2M Transformer NMT model (b3). As shown in Table 6, pivot-based PBSMT systems always achieved higher BLEU scores than (c1). The best model with three phrase tables, labeled \u201cSyn- thesize \/ Triangulate \/ Gold,\u201d brought visible BLEU gains with substantial reduction of OOV tokens (3047\u21921180 for Ja\u2192Ru, 4463\u21921812 for Ru\u2192Ja).",
            "The best model with three phrase tables, labeled \u201cSyn- thesize \/ Triangulate \/ Gold,\u201d brought visible BLEU gains with substantial reduction of OOV tokens (3047\u21921180 for Ja\u2192Ru, 4463\u21921812 for Ru\u2192Ja). However, further extension with phrase System Ja\u2192Ru Ru\u2192Ja PBSMT: Cascade 3.65 7.62 PBSMT: Synthesize 3.37 6.72 PBSMT: Synthesize \/ Gold 2.94 6.95 PBSMT: Synthesize + Gold 3.07 6.62 PBSMT: Triangulate 3.75 7.02 PBSMT: Triangulate \/ Gold 3.93 7.02 PBSMT: Synthesize \/ Triangulate \/ Gold 4.02 7.07 PBSMT: Induced 0.37 0.65 PBSMT: Induced \/ Synthesize \/ Triangulate \/ Gold 2.85 6.86 RNMT: Cascade 1.19 6.73 RNMT: Synthesize 1.82 3.02 RNMT: Synthesize + Gold 1.62 3.24 Transformer NMT: Cascade 2.41 6.84 Transformer NMT: Synthesize 1.78 5.43 Transformer NMT: Synthesize + Gold 2.13 5.06 Table 6: BLEU scores of pivot-based systems.",
            "\u201cGold\u201d refers to the phrase table trained on the parallel data. Bold indicates the BLEU score higher than the best one in Table 5. \u201c\/\u201d indicates the use of separately trained multiple phrase tables, whereas so does \u201c+\u201d training on the mixture of parallel data. tables induced from monolingual data did not push the limit, despite their high coverage; only 336 and 677 OOV tokens were left for the two translation directions, respectively. This is due to the poor quality of the bilingual word embeddings used to extract the phrase table, as envisaged in Section 3. None of pivot-based approaches with uni- directional NMT models could even remotely rival the M2M Transformer NMT model (b3). 4.4 Augmentation with Back-translation Given that the M2M Transformer NMT model (b3) achieved best results for most of the transla- tion directions and competitive results for the rest, we further explored it through back-translation. We examined the utility of pseudo-parallel data for all the six translation directions, unlike the work of Lakew et al. (2017) and Lakew et al.",
            "We examined the utility of pseudo-parallel data for all the six translation directions, unlike the work of Lakew et al. (2017) and Lakew et al. (2018), which concentrate only on the zero-shot language pair, and the work of Niu et al. (2018), which compares only uni- or bi-directional mod- els. We investigated whether each translation di- rection in M2M models will bene\ufb01t from pseudo- parallel data and if so, what kind of improvement takes place.",
            "ID System Parallel data Total size of Pseudo Ja\u2194Ru Ja\u2194En Ru\u2194En training data #1\u2013#10 Ja\u2217\u2192Ru and\/or Ru\u2217\u2192Ja 12k\u219282k 12k\u219282k 47k\u219282k\u00d72 82k\u00d72 984k Ja\u2217\u2192En and\/or En\u2217\u2192Ja 47k\u219282k 12k\u219282k\u00d72 47k\u219282k 82k\u00d72 984k Ru\u2217\u2192En and\/or En\u2217\u2192Ru 82k 12k\u219282k\u00d72 47k\u219282k\u00d72 82k 984k All All of the above 12k\u219282k 47k\u219282k 82k 984k Table 7: Over-sampling criteria for pseudo-parallel data generated by back-translation.",
            "ID Pseudo-parallel data involved BLEU score Ja\u2217\u2192Ru Ru\u2217\u2192Ja Ja\u2217\u2192En En\u2217\u2192Ja Ru\u2217\u2192En En\u2217\u2192Ru Ja\u2192Ru Ru\u2192Ja Ja\u2192En En\u2192Ja Ru\u2192En En\u2192Ru (b3) - - - - - - 3.72 8.35 10.24 12.43 22.10 16.92 #1 \u2713 - - - - - \u20224.59 8.63 10.64 12.94 22.21 17.30 #2 - \u2713 - - - - 3.74 \u20228.85 10.13 13.05 22.48 17.20 #3 \u2713 \u2713 - - - - \u20224.56 \u20229.09 10.57 \u202213.23 22.48 \u202217.89 #4 - - \u2713 - - - 3.71 8.05 \u202211.00 12.66 22.17 16.76 #5 - - - \u2713 - - 3.62 8.10 9.92 \u202214.06 21.66 16.",
            "89 #4 - - \u2713 - - - 3.71 8.05 \u202211.00 12.66 22.17 16.76 #5 - - - \u2713 - - 3.62 8.10 9.92 \u202214.06 21.66 16.68 #6 - - \u2713 \u2713 - - 3.61 7.94 \u202211.51 \u202214.38 22.22 16.80 #7 - - - - \u2713 - 3.80 8.37 10.67 13.00 22.51 \u202217.73 #8 - - - - - \u2713 3.77 8.04 10.52 12.43 \u202222.85 17.13 #9 - - - - \u2713 \u2713 3.37 8.03 10.19 12.79 22.77 17.26 #10 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u20224.43 \u20229.38 \u202212.06 \u202214.43 \u202223.09 17.",
            "13 #9 - - - - \u2713 \u2713 3.37 8.03 10.19 12.79 22.77 17.26 #10 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u20224.43 \u20229.38 \u202212.06 \u202214.43 \u202223.09 17.30 Table 8: BLEU scores of M2M Transformer NMT systems trained on the mixture of given parallel corpus and pseudo-parallel data generated by back-translation using (b3). Six \u201cX\u2217\u2192Y\u201d columns show whether the pseudo- parallel data for each translation direction is involved. Bold indicates the scores higher than (b3) and \u201c\u2022\u201d indicates statistical signi\ufb01cance of the improvement. First, we selected sentences to be back- translated from in-domain monolingual data (Ta- ble 3), relying on the score proposed by Moore and Lewis (2010) via the following procedure. 1.",
            "First, we selected sentences to be back- translated from in-domain monolingual data (Ta- ble 3), relying on the score proposed by Moore and Lewis (2010) via the following procedure. 1. For each language, train two 4-gram lan- guage models, using KenLM: an in-domain one on all the Global Voices data, i.e., both parallel and monolingual data, and a general- domain one on the concatenation of Global Voices, IWSLT, and Tatoeba data. 2. For each language, discard sentences con- taining OOVs according to the in-domain language model. 3. For each translation direction, select the T- best monolingual sentences in the news do- main, according to the difference between cross-entropy scores given by the in-domain and general-domain language models. Whereas Niu et al. (2018) exploited monolin- gual data much larger than parallel data, we main- tained a 1:1 ratio between them (Johnson et al., 2017), setting T to the number of lines of parallel data of given language pair.",
            "Whereas Niu et al. (2018) exploited monolin- gual data much larger than parallel data, we main- tained a 1:1 ratio between them (Johnson et al., 2017), setting T to the number of lines of parallel data of given language pair. Selected monolingual sentences were then translated using the M2M Transformer NMT model (b3) to compose pseudo-parallel data. Then, the pseudo-parallel data were enlarged by over-sampling as summarized in Table 7. Finally, new NMT models were trained on the concate- nation of the original parallel and pseudo-parallel data from scratch in the same manner as the previ- ous NMT models with the same hyper-parameters. Table 8 shows the BLEU scores achieved by several reasonable combinations of six-way pseudo-parallel data. We observed that the use of all six-way pseudo-parallel data (#10) signi\ufb01- cantly improved the base model for all the transla- tion directions, except En\u2192Ru. A translation di- rection often bene\ufb01ted when the pseudo-parallel data for that speci\ufb01c direction was used.",
            "A translation di- rection often bene\ufb01ted when the pseudo-parallel data for that speci\ufb01c direction was used. 4.5 Summary We have evaluated an extensive variation of MT models21 that rely only on in-domain parallel and monolingual data. However, the resulting BLEU scores for Ja\u2192Ru and Ru\u2192Ja tasks do not exceed 10 BLEU points, implying the inherent limitation of the in-domain data as well as the dif\ufb01culty of these translation directions. 21Other conceivable options include transfer learning using parallel data between English and one of Japanese and Rus- sian as either source or target language, such as pre-training an En\u2192Ru model and \ufb01ne-tuning it for Ja\u2192Ru. Our M2M models conceptually subsume them, even though they do not explicitly divide the two steps during training. On the other hand, our method proposed in Section 5 explicitly conducts transfer learning for domain adaptation followed by addi- tional transfer learning across different languages.",
            "Domain \\ language pair Direct One-side shared in-domain A, \u2713 B, \u2713 out-of-domain C, \u00d7 D, \u2713 Table 9: Classi\ufb01cation of parallel data. 5 Exploiting Large Out-of-Domain Data Involving a Helping Language The limitation of relying only on in-domain data demonstrated in Section 4 motivates us to explore other types of parallel data. As raised in our sec- ond research question, [RQ2], we considered the effective ways to exploit out-of-domain data. According to language pair and domain, par- allel data can be classi\ufb01ed into four categories in Table 9. Among all the categories, out-of- domain data for the language pair of interest have been exploited in the domain adaptation scenarios (C\u2192A) (Chu et al., 2017). However, for Ja\u2194Ru, no out-of-domain data is available.",
            "Among all the categories, out-of- domain data for the language pair of interest have been exploited in the domain adaptation scenarios (C\u2192A) (Chu et al., 2017). However, for Ja\u2194Ru, no out-of-domain data is available. To exploit out- of-domain parallel data for Ja\u2194En and Ru\u2194En pairs instead, we propose a multistage \ufb01ne-tuning method, which combines two types of transfer learning, i.e., domain adaptation for Ja\u2194En and Ru\u2194En (D\u2192B) and multilingual transfer (B\u2192A), relying on the M2M model examined in Section 4. We also examined the utility of \ufb01ne-tuning for iter- atively generating and using pseudo-parallel data. 5.1 Multistage Fine-tuning Simply using NMT systems trained on out-of- domain data for in-domain translation is known to perform badly.",
            "We also examined the utility of \ufb01ne-tuning for iter- atively generating and using pseudo-parallel data. 5.1 Multistage Fine-tuning Simply using NMT systems trained on out-of- domain data for in-domain translation is known to perform badly. In order to effectively use large-scale out-of-domain data for our extremely low-resource task, we propose to perform domain adaptation through either (a) conventional \ufb01ne- tuning, where an NMT system trained on out-of- domain data is \ufb01ne-tuned only on in-domain data, or (b) mixed \ufb01ne-tuning (Chu et al., 2017), where pre-trained out-of-domain NMT system is \ufb01ne- tuned using a mixture of in-domain and out-of- domain data. The same options are available for transferring from Ja\u2194En and Ru\u2194En to Ja\u2194Ru. We inevitably involve two types of transfer learning, i.e., domain adaptation for Ja\u2194En and Ru\u2194En and multilingual transfer for Ja\u2194Ru pair. Among several conceivable options for managing these two problems, we examined the following multistage \ufb01ne-tuning. Stage 0.",
            "Among several conceivable options for managing these two problems, we examined the following multistage \ufb01ne-tuning. Stage 0. Out-of-domain pre-training: Pre-train a multilingual model only on the Ja\u2194En Lang.pair Corpus #sent. #tokens #types Ja\u2194En ASPEC 1,500,000 42.3M \/ 34.6M 234k \/ 1.02M Ru\u2194En UN 2,647,243 90.5M \/ 92.8M 757k \/ 593k Yandex 320,325 8.51M \/ 9.26M 617k \/ 407k Table 10: Statistics on our out-of-domain parallel data. and Ru\u2194En out-of-domain parallel data (I), where the vocabulary of the model is determined on the basis of the in-domain parallel data in the same manner as the M2M NMT models examined in Section 4. Stage 1.",
            "and Ru\u2194En out-of-domain parallel data (I), where the vocabulary of the model is determined on the basis of the in-domain parallel data in the same manner as the M2M NMT models examined in Section 4. Stage 1. Fine-tuning for domain adaptation: Fine-tune the pre-trained model (I) on the in-domain Ja\u2194En and Ru\u2194En parallel data (\ufb01ne-tuning, II) or on the mixture of in-domain and out-of-domain Ja\u2194En and Ru\u2194En parallel data (mixed \ufb01ne-tuning, III). Stage 2. Fine-tuning for Ja\u2194Ru pair: Further \ufb01ne-tune the models (each of II and III) for Ja\u2194Ru on in-domain parallel data for this language pair only (\ufb01ne-tuning, IV and VI) or on all the in-domain parallel data (mixed \ufb01ne-tuning, V and VII). We chose this way due to the following two rea- sons. First, we need to take a balance between several different parallel corpora sizes.",
            "We chose this way due to the following two rea- sons. First, we need to take a balance between several different parallel corpora sizes. The other reason is division of labor; we assume that solving each sub-problem one by one should enable grad- ual shift of parameters. 5.2 Data Selection As an additional large-scale out-of-domain paral- lel data for Ja\u2194En, we used the cleanest 1.5M sentences from the Asian Scienti\ufb01c Paper Excerpt Corpus (ASPEC) (Nakazawa et al., 2016).22 As for Ru\u2194En, we used the UN and Yandex cor- pora released for the WMT 2018 News Translation Task.23 We retained Ru\u2194En sentence pairs that contain at least one OOV token in both sides, ac- cording to the in-domain language model trained in Section 4.4. Table 10 summarizes the statistics on the remaining out-of-domain parallel data. 5.3 Results Table 11 shows the results of our multistage \ufb01ne- tuning, where the IDs of each row refer to those described in Section 5.1.",
            "Table 10 summarizes the statistics on the remaining out-of-domain parallel data. 5.3 Results Table 11 shows the results of our multistage \ufb01ne- tuning, where the IDs of each row refer to those described in Section 5.1. First of all, the \ufb01nal 22http:\/\/lotus.kuee.kyoto-u.ac.jp\/ASPEC\/ 23http:\/\/www.statmt.org\/wmt18\/translation-task.html",
            "ID Initialized Out-of-domain data In-domain data BLEU score Ja\u2194En Ru\u2194En Ja\u2194Ru Ja\u2194En Ru\u2194En Ja\u2192Ru Ru\u2192Ja Ja\u2192En En\u2192Ja Ru\u2192En En\u2192Ru (b3) - - - \u2713 \u2713 \u2713 3.72 8.35 10.24 12.43 22.10 16.92 I - \u2713 \u2713 - - - 0.00 0.15 4.59 4.15 \u202225.22 \u202220.37 II I - - - \u2713 \u2713 0.20 0.70 \u202214.10 \u202217.80 \u202228.23 \u202224.35 III I \u2713 \u2713 - \u2713 \u2713 0.23 1.07 \u202213.31 \u202217.74 \u202228.73 \u202225.22 IV II - - \u2713 - - \u20225.44 \u202210.67 0.12 3.97 0.11 3.66 V II - - \u2713 \u2713 \u2713 \u20226.90 \u202211.99 \u202214.34 \u202216.93 \u202227.50 \u202223.17 VI III - - \u2713 - - \u20225.91 \u202210.",
            "67 0.12 3.97 0.11 3.66 V II - - \u2713 \u2713 \u2713 \u20226.90 \u202211.99 \u202214.34 \u202216.93 \u202227.50 \u202223.17 VI III - - \u2713 - - \u20225.91 \u202210.83 0.26 2.18 0.18 1.10 VII III - - \u2713 \u2713 \u2713 \u20227.49 \u202212.10 \u202214.63 \u202217.51 \u202228.51 \u202224.60 I\u2019 - \u2713 \u2713 \u2713 \u2713 \u2713 \u20225.31 \u202210.73 \u202214.41 \u202216.34 \u202227.46 \u202223.21 II\u2019 I - - \u2713 \u2713 \u2713 \u20226.30 \u202211.64 \u202214.29 \u202216.83 \u202227.53 \u202223.00 III\u2019 I \u2713 \u2713 \u2713 \u2713 \u2713 \u20227.53 \u202212.33 \u202214.19 \u202216.77 \u202227.94 \u202223.97 Table 11: BLEU scores obtained through multistage \ufb01ne-tuning. \u201cInitialized\u201d column indicates the model used for initializing parameters that are \ufb01ne-tuned on the data indicated by \u2713.",
            "33 \u202214.19 \u202216.77 \u202227.94 \u202223.97 Table 11: BLEU scores obtained through multistage \ufb01ne-tuning. \u201cInitialized\u201d column indicates the model used for initializing parameters that are \ufb01ne-tuned on the data indicated by \u2713. Bold indicates the best BLEU score for each translation direction. \u201c\u2022\u201d indicates statistical signi\ufb01cance of the improvement over (b3). models of our multistage \ufb01ne-tuning, i.e., V and VII, achieved signi\ufb01cantly higher BLEU scores than (b3) in Table 5, a weak baseline without using any monolingual data, and #10 in Table 8, a strong baseline established with monolingual data. The performance of the initial model (I) de- pends on the language pair. For Ja\u2194Ru pair, it cannot achieve minimum level of quality since the model has never seen parallel data for this pair. The performance on Ja\u2194En pair was much lower than the two baseline models, re\ufb02ecting the cru- cial mismatch between training and testing do- mains.",
            "The performance on Ja\u2194En pair was much lower than the two baseline models, re\ufb02ecting the cru- cial mismatch between training and testing do- mains. In contrast, Ru\u2194En pair bene\ufb01ted the most and achieved surprisingly high BLEU scores. The reason might be due to the proximity of out-of- domain training data and in-domain test data. The \ufb01rst \ufb01ne-tuning stage signi\ufb01cantly pushed up the translation quality for Ja\u2194En and Ru\u2194En pairs, in both cases with \ufb01ne-tuning (II) and mixed \ufb01ne-tuning (III). At this stage, both models per- formed only poorly for Ja\u2194Ru pair as they have not yet seen Ja\u2194Ru parallel data. Either model had a consistent advantage to the other. When these models were further \ufb01ne-tuned only on the in-domain Ja\u2194Ru parallel data (IV and VI), we obtained translations of better quality than the two baselines for Ja\u2194Ru pair.",
            "Either model had a consistent advantage to the other. When these models were further \ufb01ne-tuned only on the in-domain Ja\u2194Ru parallel data (IV and VI), we obtained translations of better quality than the two baselines for Ja\u2194Ru pair. However, as a re- sult of complete ignorance of Ja\u2194En and Ru\u2194En pairs, the models only produced translations of poor quality for these language pairs. In contrast, mixed \ufb01ne-tuning for the second \ufb01ne-tuning stage (V and VII) resulted in consistently better mod- els than conventional \ufb01ne-tuning (IV and VI), ir- respective of the choice at the \ufb01rst stage, thanks to the gradual shift of parameters realized by in- domain Ja\u2194En and Ru\u2194En parallel data. Un- fortunately, the translation quality for Ja\u2194En and Ru\u2194En pairs sometimes degraded from II and III. Nevertheless, the BLEU scores still retain the large margin against two baselines. The last three rows in Table 11 present BLEU scores obtained by the methods with fewer \ufb01ne- tuning steps.",
            "Nevertheless, the BLEU scores still retain the large margin against two baselines. The last three rows in Table 11 present BLEU scores obtained by the methods with fewer \ufb01ne- tuning steps. The most naive model I\u2019, trained on the balanced mixture of whole \ufb01ve types of corpora from scratch, and the model II\u2019, obtained through a single-step conventional \ufb01ne-tuning of I on all the in-domain data, achieved only BLEU scores consistently worse than VII. In contrast, when we merged our two \ufb01ne-tuning steps into a single mixed \ufb01ne-tuning on I, we obtained a model III\u2019 which is better for the Ja\u2194Ru pair than VII. Nevertheless, they are still comparable to those of VII and the BLEU scores for the other two lan- guage pairs are much lower than VII. As such, we conclude that our multistage \ufb01ne-tuning leads to a more robust in-domain multilingual model. 5.4 Further Augmentation with Back-translation Having obtained a better model, we examined again the utility of back-translation.",
            "As such, we conclude that our multistage \ufb01ne-tuning leads to a more robust in-domain multilingual model. 5.4 Further Augmentation with Back-translation Having obtained a better model, we examined again the utility of back-translation. More pre- cisely, we investigated (a) whether the pseudo- parallel data generated by an improved NMT model leads to a further improvement, and (b) whether one more stage of \ufb01ne-tuning on the mix- ture of original parallel and pseudo-parallel data will result in a model better than training a new model from scratch as examined in Section 4.4. Given an NMT model, we \ufb01rst generated six- way pseudo-parallel data by translating monolin- gual data. For the sake of comparability, we used the identical monolingual sentences sampled in Section 4.4. Then, we further \ufb01ne-tuned the given model on the mixture of the generated pseudo- parallel data and the original parallel data, fol- lowing the same over-sampling procedure in Sec-",
            "No Initialized BT BLEU score Ja\u2192Ru Ru\u2192Ja Ja\u2192En En\u2192Ja Ru\u2192En En\u2192Ru #10 - (b3) 4.43 9.38 12.06 14.43 23.09 17.30 new #10 - VII \u20226.55 \u202211.36 \u202213.77 \u202215.59 \u202224.91 \u202220.55 VIII VII VII \u20227.83 \u202212.21 \u202215.06 \u202217.19 \u202228.49 \u202223.96 IX VIII VIII \u20228.03 \u202212.55 \u202215.07 \u202217.80 \u202228.16 \u202224.27 X IX IX \u20227.76 \u202212.59 \u202215.08 \u202218.12 \u202228.18 \u202224.67 XI X X \u20227.85 \u202212.97 \u202215.26 \u202217.83 \u202228.49 \u202224.36 XII XI XI \u20228.16 \u202213.09 \u202214.96 \u202217.74 \u202228.45 \u202224.35 Table 12: BLEU scores achieved through \ufb01ne-tuning on the mixture of the original parallel data and six-way pseudo-parallel data.",
            "\u201cInitialized\u201d column indicates the model used for initializing parameters and so does \u201cBT\u201d column the model used to generate pseudo-parallel data. \u201c\u2022\u201d indicates statistical signi\ufb01cance of the improvement over #10. Investigation step Ja\u2192Ru Ru\u2192Ja Uni-directional Transformer: (b1) in Table 5 0.70 1.96 M2M Transformer: (b3) in Table 5 3.72 8.35 + six-way pseudo-parallel data: #10 in Table 8 4.43 9.38 M2M multistage \ufb01ne-tuning: VII in Table 11 7.49 12.10 + six-way pseudo-parallel data: XII in Table 12 8.16 13.09 Table 13: Summary of our investigation: BLEU scores of the best NMT systems at each step. tion 4.4. We repeated these steps \ufb01ve times. Table 12 shows the results. \u201cnew #10\u201d in the second row indicates an M2M Transformer model trained from scratch on the mixture of six- way pseudo-parallel data generated by VII and the original parallel data.",
            "We repeated these steps \ufb01ve times. Table 12 shows the results. \u201cnew #10\u201d in the second row indicates an M2M Transformer model trained from scratch on the mixture of six- way pseudo-parallel data generated by VII and the original parallel data. It achieved higher BLEU scores than #10 in Table 8 thanks to the pseudo-parallel data of better quality, but under- performed the base NMT model VII. In contrast, our \ufb01ne-tuned model VIII successfully surpassed VII, and one more iteration (IX) further improved BLEU scores for all translation directions, except Ru\u2192En. Although further iterations did not nec- essarily gain BLEU scores, we came to a much higher plateau compared to the results in Sec- tion 4. 6 Conclusion In this paper, we challenged the dif\ufb01cult task of Ja\u2194Ru news domain translation in an extremely low-resource setting. We empirically con\ufb01rmed the limited success of well-established solutions when restricted to in-domain data.",
            "6 Conclusion In this paper, we challenged the dif\ufb01cult task of Ja\u2194Ru news domain translation in an extremely low-resource setting. We empirically con\ufb01rmed the limited success of well-established solutions when restricted to in-domain data. Then, to in- corporate out-of-domain data, we proposed a mul- tilingual multistage \ufb01ne-tuning approach and ob- served that it substantially improves Ja\u2194Ru trans- lation by over 3.7 BLEU points compared to a strong baseline, as summarized in Table 13. This paper contains an empirical comparison of several existing approaches and hence we hope that our paper can act as a guideline to researchers attempt- ing to tackle extremely low-resource translation. In the future, we plan to con\ufb01rm further \ufb01ne- tuning for each of speci\ufb01c translation directions. We will also explore the way to exploit out- of-domain pseudo-parallel data, better domain- adaptation approaches, and additional challenging language pairs. Acknowledgments This work was carried out when Aizhan Imankulova was taking up an internship at NICT, Japan. We would like to thank the review- ers for their insightful comments.",
            "Acknowledgments This work was carried out when Aizhan Imankulova was taking up an internship at NICT, Japan. We would like to thank the review- ers for their insightful comments. A part of this work was conducted under the program \u201cPromo- tion of Global Communications Plan: Research, Development, and Social Demonstration of Mul- tilingual Speech Translation Technology\u201d of the Ministry of Internal Affairs and Communications (MIC), Japan. References Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018. Unsupervised statistical machine translation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3632\u20133642, Brussels, Belgium. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate. In Proceedings of the 3rd International Conference on Learning Rep- resentations, San Diego, USA. Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation.",
            "2015. Neural machine translation by jointly learning to align and translate. In Proceedings of the 3rd International Conference on Learning Rep- resentations, San Diego, USA. Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In Pro- ceedings of the 2012 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 427\u2013436, Montr\u00b4eal, Canada. Kyunghyun Cho, Bart van Merri\u00a8enboer, C\u00b8 aglar G\u00a8ulc\u00b8ehre, Dzmitry Bahdanau, Fethi Bougares, Hol- ger Schwenk, and Yoshua Bengio. 2014. Learning",
            "phrase representations using RNN encoder\u2013decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Nat- ural Language Processing, pages 1724\u20131734, Doha, Qatar. Chenhui Chu, Raj Dabre, and Sadao Kurohashi. 2017. An empirical comparison of domain adaptation methods for neural machine translation. In Proceed- ings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 385\u2013391, Van- couver, Canada. Trevor Cohn and Mirella Lapata. 2007. Machine trans- lation by triangulation: Making effective use of mul- ti-parallel corpora. In Proceedings of the 45th An- nual Meeting of the Association of Computational Linguistics, pages 728\u2013735, Prague, Czech Repub- lic. Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. 2016. Multi-way, multilingual neural machine translation with a shared attention mechanism.",
            "Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. 2016. Multi-way, multilingual neural machine translation with a shared attention mechanism. In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, pages 866\u2013875, San Diego, USA. Howard Johnson, Joel Martin, George Foster, and Roland Kuhn. 2007. Improving translation qual- ity by discarding most of the phrasetable. In Pro- ceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Com- putational Natural Language Learning, pages 967\u2013 975, Prague, Czech Republic. Melvin Johnson, Mike Schuster, Quoc Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi\u00b4egas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2017. Google\u2019s multilingual neural machine translation system: En- abling zero-shot translation.",
            "2017. Google\u2019s multilingual neural machine translation system: En- abling zero-shot translation. Transactions of the As- sociation for Computational Linguistics, 5:339\u2013351. Tom Kocmi and Ond\u02c7rej Bojar. 2018. Trivial trans- fer learning for low-resource neural machine trans- lation. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 244\u2013 252, Brussels, Belgium. Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the As- sociation for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Ses- sions, pages 177\u2013180, Prague, Czech Republic. Philipp Koehn and Rebecca Knowles. 2017. Six chal- lenges for neural machine translation.",
            "Philipp Koehn and Rebecca Knowles. 2017. Six chal- lenges for neural machine translation. In Pro- ceedings of the First Workshop on Neural Machine Translation, pages 28\u201339, Vancouver, Canada. Surafel M Lakew, Mauro Cettolo, and Marcello Fed- erico. 2018. A comparison of transformer and re- current neural networks on multilingual neural ma- chine translation. In Proceedings of the 27th Inter- national Conference on Computational Linguistics, pages 641\u2013652, Santa Fe, USA. Surafel M Lakew, Quintino F. Lotito, Matteo Negri, Marco Turchi, and Marcello Federico. 2017. Im- proving zero-shot translation of low-resource lan- guages. In Proceedings of the 14th International Workshop on Spoken Language Translation, pages 113\u2013119, Tokyo, Japan. Guillaume Lample, Myle Ott, Alexis Conneau, Lu- dovic Denoyer, and Marc\u2019Aurelio Ranzato. 2018. Phrase-based & neural unsupervised machine trans- lation.",
            "Guillaume Lample, Myle Ott, Alexis Conneau, Lu- dovic Denoyer, and Marc\u2019Aurelio Ranzato. 2018. Phrase-based & neural unsupervised machine trans- lation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Process- ing, pages 5039\u20135049, Brussels, Belgium. Benjamin Marie and Atsushi Fujita. 2018. Unsuper- vised neural machine translation initialized by un- supervised statistical machine translation. CoRR, abs\/1810.12703. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor- rado, and Jeffrey Dean. 2013. Distributed represen- tations of words and phrases and their composition- ality. In Proceedings of the 26th International Con- ference on Neural Information Processing Systems - Volume 2, pages 3111\u20133119, Lake Tahoe, USA. Curran Associates Inc. Robert C. Moore and Will Lewis. 2010. Intelligent se- lection of language model training data.",
            "Curran Associates Inc. Robert C. Moore and Will Lewis. 2010. Intelligent se- lection of language model training data. In Proceed- ings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL) Short Papers, pages 220\u2013224, Uppsala, Sweden. Toshiaki Nakazawa, Manabu Yaguchi, Kiyotaka Uchi- moto, Masao Utiyama, Eiichiro Sumita, Sadao Kurohashi, and Hitoshi Isahara. 2016. ASPEC: Asian scienti\ufb01c paper excerpt corpus. In Proceed- ings of the Tenth International Conference on Lan- guage Resources and Evaluation, pages 2204\u20132208, Portoro\u02c7z, Slovenia. Xing Niu, Michael Denkowski, and Marine Carpuat. 2018. Bi-directional neural machine translation with synthetic parallel data. In Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 84\u201391, Melbourne, Australia.",
            "Xing Niu, Michael Denkowski, and Marine Carpuat. 2018. Bi-directional neural machine translation with synthetic parallel data. In Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 84\u201391, Melbourne, Australia. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Com- putational Linguistics, pages 311\u2013318, Philadelphia, USA. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Improving neural machine translation mod- els with monolingual data. In Proceedings of the",
            "54th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 86\u201396, Berlin, Germany. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural net- works. In Proceedings of the 27th International Conference on Neural Information Processing Sys- tems, pages 3104\u20133112, Montr\u00b4eal, Canada. Christoph Tillmann and Jian-ming Xu. 2009. A sim- ple sentence-level extraction algorithm for compara- ble data. In Proceedings of Human Language Tech- nologies: The 2009 Annual Conference of the North American Chapter of the Association for Computa- tional Linguistics, pages 93\u201396, Boulder, USA. Masao Utiyama and Hitoshi Isahara. 2003. Reliable measures for aligning Japanese-English news arti- cles and sentences. In Proceedings of the 41st An- nual Meeting of the Association for Computational Linguistics, pages 72\u201379, Sapporo, Japan.",
            "2003. Reliable measures for aligning Japanese-English news arti- cles and sentences. In Proceedings of the 41st An- nual Meeting of the Association for Computational Linguistics, pages 72\u201379, Sapporo, Japan. Masao Utiyama and Hitoshi Isahara. 2007. A com- parison of pivot methods for phrase-based statistical machine translation. In Human Language Technolo- gies 2007: The Conference of the North American Chapter of the Association for Computational Lin- guistics; Proceedings of the Main Conference, pages 484\u2013491, Rochester, USA. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of 30th Advances in Neu- ral Information Processing Systems, pages 5998\u2013 6008. Barret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight. 2016. Transfer learning for low-resource neural machine translation.",
            "In Proceedings of 30th Advances in Neu- ral Information Processing Systems, pages 5998\u2013 6008. Barret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight. 2016. Transfer learning for low-resource neural machine translation. In Proceedings of the 2016 Conference on Empirical Methods in Natu- ral Language Processing, pages 1568\u20131575, Austin, USA."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1907.03060.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 12627.0,
    "avg_doclen_est": 175.375
}
