{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Attentional Encoder Network for Targeted Sentiment Classi\ufb01cation Youwei Song, Jiahai Wang \u2217, Tao Jiang, Zhiyue Liu, Yanghui Rao School of Data and Computer Science Sun Yat-sen University Guangzhou, China {songyw5,jiangt59,liuzhy93}@mail2.sysu.edu.cn {wangjiah,raoyangh}@mail.sysu.edu.cn Abstract Targeted sentiment classi\ufb01cation aims at deter- mining the sentimental tendency towards spe- ci\ufb01c targets. Most of the previous approaches model context and target words with RNN and attention. However, RNNs are dif\ufb01cult to parallelize and truncated backpropagation through time brings dif\ufb01culty in remembering long-term patterns. To address this issue, this paper proposes an Attentional Encoder Net- work (AEN) which eschews recurrence and employs attention based encoders for the mod- eling between context and target. We raise the label unreliability issue and introduce label smoothing regularization. We also apply pre- trained BERT to this task and obtain new state- of-the-art results.",
      "We raise the label unreliability issue and introduce label smoothing regularization. We also apply pre- trained BERT to this task and obtain new state- of-the-art results. Experiments and analysis demonstrate the effectiveness and lightweight of our model. 1 1 Introduction Targeted sentiment classi\ufb01cation is a \ufb01ne-grained sentiment analysis task, which aims at determin- ing the sentiment polarities (e.g., negative, neutral, or positive) of a sentence over \u201copinion targets\u201d that explicitly appear in the sentence. For exam- ple, given a sentence \u201cI hated their service, but their food was great\u201d, the sentiment polarities for the target \u201cservice\u201d and \u201cfood\u201d are negative and positive respectively. A target is usually an entity or an entity aspect. In recent years, neural network models are designed to automatically learn useful low- dimensional representations from targets and con- texts and obtain promising results (Dong et al., 2014; Tang et al., 2016a). However, these neural network models are still in infancy to deal with the \ufb01ne-grained targeted sentiment classi\ufb01cation task. \u2217The corresponding author.",
      "However, these neural network models are still in infancy to deal with the \ufb01ne-grained targeted sentiment classi\ufb01cation task. \u2217The corresponding author. 1Source code is available at https://github.com/ songyouwei/ABSA-PyTorch/tree/aen. Attention mechanism, which has been success- fully used in machine translation (Bahdanau et al., 2014), is incorporated to enforce the model to pay more attention to context words with closer se- mantic relations with the target. There are already some studies use attention to generate target- speci\ufb01c sentence representations (Wang et al., 2016; Ma et al., 2017; Chen et al., 2017) or to transform sentence representations according to target words (Li et al., 2018). However, these stud- ies depend on complex recurrent neural networks (RNNs) as sequence encoder to compute hidden semantics of texts. The \ufb01rst problem with previous works is that the modeling of text relies on RNNs.",
      "However, these stud- ies depend on complex recurrent neural networks (RNNs) as sequence encoder to compute hidden semantics of texts. The \ufb01rst problem with previous works is that the modeling of text relies on RNNs. RNNs, such as LSTM, are very expressive, but they are hard to parallelize and backpropagation through time (BPTT) requires large amounts of memory and computation. Moreover, essentially every training algorithm of RNN is the truncated BPTT, which affects the model\u2019s ability to capture dependen- cies over longer time scales (Werbos, 1990). Al- though LSTM can alleviate the vanishing gradi- ent problem to a certain extent and thus maintain long distance information, this usually requires a large amount of training data. Another problem that previous studies ignore is the label unreliabil- ity issue, since neutral sentiment is a fuzzy senti- mental state and brings dif\ufb01culty for model learn- ing. As far as we know, we are the \ufb01rst to raise the label unreliability issue in the targeted sentiment classi\ufb01cation task. This paper propose an attention based model to solve the problems above.",
      "As far as we know, we are the \ufb01rst to raise the label unreliability issue in the targeted sentiment classi\ufb01cation task. This paper propose an attention based model to solve the problems above. Speci\ufb01cally, our model eschews recurrence and employs attention as a competitive alternative to draw the introspec- tive and interactive semantics between target and context words. To deal with the label unreliability issue, we employ a label smoothing regularization to encourage the model to be less con\ufb01dent with arXiv:1902.09314v2  [cs.CL]  1 Apr 2019",
      "fuzzy labels. We also apply pre-trained BERT (Devlin et al., 2018) to this task and show our model enhances the performance of basic BERT model. Experimental results on three benchmark datasets show that the proposed model achieves competitive performance and is a lightweight al- ternative of the best RNN based models. The main contributions of this work are pre- sented as follows: 1. We design an attentional encoder network to draw the hidden states and semantic interac- tions between target and context words. 2. We raise the label unreliability issue and add an effective label smoothing regularization term to the loss function for encouraging the model to be less con\ufb01dent with the training labels. 3. We apply pre-trained BERT to this task, our model enhances the performance of basic BERT model and obtains new state-of-the-art results. 4. We evaluate the model sizes of the compared models and show the lightweight of the pro- posed model. 2 Related Work The research approach of the targeted sentiment classi\ufb01cation task including traditional machine learning methods and neural networks methods.",
      "4. We evaluate the model sizes of the compared models and show the lightweight of the pro- posed model. 2 Related Work The research approach of the targeted sentiment classi\ufb01cation task including traditional machine learning methods and neural networks methods. Traditional machine learning methods, includ- ing rule-based methods (Ding et al., 2008) and statistic-based methods (Jiang et al., 2011), mainly focus on extracting a set of features like senti- ment lexicons features and bag-of-words features to train a sentiment classi\ufb01er (Rao and Ravichan- dran, 2009). The performance of these methods highly depends on the effectiveness of the feature engineering works, which are labor intensive. In recent years, neural network methods are getting more and more attention as they do not need handcrafted features and can encode sen- tences with low-dimensional word vectors where rich semantic information stained. In order to incorporate target words into a model, Tang et al. (2016a) propose TD-LSTM to extend LSTM by using two single-directional LSTM to model the left context and right context of the target word respectively. Tang et al.",
      "In order to incorporate target words into a model, Tang et al. (2016a) propose TD-LSTM to extend LSTM by using two single-directional LSTM to model the left context and right context of the target word respectively. Tang et al. (2016b) design MemNet which consists of a multi-hop attention mechanism with an external memory to capture the importance of each context word concern- ing the given target. Multiple attention is paid to the memory represented by word embeddings to build higher semantic information. Wang et al. (2016) propose ATAE-LSTM which concate- nates target embeddings with word representations and let targets participate in computing attention weights. Chen et al. (2017) propose RAM which adopts multiple-attention mechanism on the mem- ory built with bidirectional LSTM and nonlinearly combines the attention results with gated recur- rent units (GRUs). Ma et al. (2017) propose IAN which learns the representations of the target and context with two attention networks interactively.",
      "Ma et al. (2017) propose IAN which learns the representations of the target and context with two attention networks interactively. 3 Proposed Methodology Given a context sequence wc = {wc 1, wc 2, ..., wc n} and a target sequence wt = {wt 1, wt 2, ..., wt m}, where wt is a sub-sequence of wc. The goal of this model is to predict the sentiment polarity of the sentence wc over the target wt. Figure 1 illustrates the overall architecture of the proposed Attentional Encoder Network (AEN), which mainly consists of an embedding layer, an attentional encoder layer, a target-speci\ufb01c attention layer, and an output layer. Embedding layer has two types: GloVe embedding and BERT embedding. Accordingly, the models are named AEN-GloVe and AEN-BERT. 3.1 Embedding Layer 3.1.1 GloVe Embedding Let L \u2208Rdemb\u00d7|V | to be the pre-trained GloVe (Pennington et al., 2014) embedding matrix, where demb is the dimension of word vectors and |V | is the vocabulary size.",
      "Then we map each word wi \u2208R|V | to its corresponding embedding vector ei \u2208Rdemb\u00d71, which is a column in the embedding matrix L. 3.1.2 BERT Embedding BERT embedding uses the pre-trained BERT to generate word vectors of sequence. In order to facilitate the training and \ufb01ne-tuning of BERT model, we transform the given context and target to \u201c[CLS] + context + [SEP]\u201d and \u201c[CLS] + target + [SEP]\u201d respectively. 3.2 Attentional Encoder Layer The attentional encoder layer is a parallelizable and interactive alternative of LSTM and is applied",
      "\u2026 Intra-MHA Inter-MHA context target PCT sentiment softmax \u2026 \u2026 \u2026 Embedding Layer Attentional  Encoder Layer PCT \u2026 Output Layer attention weights \u00d7 dot product pool pool pool label smoothing Target-specific Attention Layer . Figure 1: Overall architecture of the proposed AEN. to compute the hidden states of the input embed- dings. This layer consists of two submodules: the Multi-Head Attention (MHA) and the Point-wise Convolution Transformation (PCT). 3.2.1 Multi-Head Attention Multi-Head Attention (MHA) is the attention that can perform multiple attention function in paral- lel. Different from Transformer (Vaswani et al., 2017), we use Intra-MHA for introspective con- text words modeling and Inter-MHA for context- perceptive target words modeling, which is more lightweight and target is modeled according to a given context.",
      "Different from Transformer (Vaswani et al., 2017), we use Intra-MHA for introspective con- text words modeling and Inter-MHA for context- perceptive target words modeling, which is more lightweight and target is modeled according to a given context. An attention function maps a key sequence k = {k1, k2, ..., kn} and a query sequence q = {q1, q2, ..., qm} to an output sequence o: Attention(k, q) = softmax(fs(k, q))k (1) where fs denotes the alignment function which learns the semantic relevance between qj and ki: fs(ki, qj) = tanh([ki; qj] \u00b7 Watt) (2) where Watt \u2208R2dhid are learnable weights. MHA can learn n head different scores in par- allel child spaces and is very powerful for align- ments.",
      "MHA can learn n head different scores in par- allel child spaces and is very powerful for align- ments. The nhead outputs are concatenated and projected to the speci\ufb01ed hidden dimension dhid, namely, MHA(k, q) = [o1; o2...; onhead] \u00b7 Wmh (3) oh = Attentionh(k, q) (4) where \u201c;\u201d denotes vector concatenation, Wmh \u2208 Rdhid\u00d7dhid, oh = {oh 1, oh 2, ..., oh m} is the output of the h-th head attention and h \u2208[1, nhead]. Intra-MHA, or multi-head self-attention, is a special situation for typical attention mechanism that q = k. Given a context embedding ec, we can get the introspective context representation cintra by: cintra = MHA(ec, ec) (5) The learned context representation cintra = {cintra 1 , cintra 2 , ..., cintra n } is aware of long-term dependencies.",
      "Inter-MHA is the generally used form of atten- tion mechanism that q is different from k. Given a context embedding ec and a target embedding et, we can get the context-perceptive target represen- tation tinter by: tinter = MHA(ec, et) (6) After this interactive procedure, each given tar- get word et j will have a composed representation selected from context embeddings ec. Then we get the context-perceptive target words modeling tinter = {tinter 1 , tinter 2 , ..., tinter m }. 3.2.2 Point-wise Convolution Transformation A Point-wise Convolution T ransformation (PCT) can transform contextual information gathered by the MHA. Point-wise means that the kernel sizes are 1 and the same transformation is applied to ev- ery single token belonging to the input.",
      "3.2.2 Point-wise Convolution Transformation A Point-wise Convolution T ransformation (PCT) can transform contextual information gathered by the MHA. Point-wise means that the kernel sizes are 1 and the same transformation is applied to ev- ery single token belonging to the input. Formally, given a input sequence h, PCT is de\ufb01ned as: PCT(h) = \u03c3(h \u2217W 1 pc + b1 pc) \u2217W 2 pc + b2 pc (7) where \u03c3 stands for the ELU activation, \u2217is the convolution operator, W 1 pc \u2208 Rdhid\u00d7dhid and W 2 pc \u2208Rdhid\u00d7dhid are the learnable weights of the two convolutional kernels, b1 pc \u2208Rdhid and b2 pc \u2208Rdhid are biases of the two convolutional kernels. Given cintra and tinter, PCTs are applied to get the output hidden states of the attentional en- coder layer hc = {hc 1, hc 2, ..., hc n} and ht =",
      "{ht 1, ht 2, ..., ht m} by: hc = PCT(cintra) (8) ht = PCT(tinter) (9) 3.3 Target-speci\ufb01c Attention Layer After we obtain the introspective context represen- tation hc and the context-perceptive target repre- sentation ht, we employ another MHA to obtain the target-speci\ufb01c context representation htsc = {htsc 1 , htsc 2 , ..., htsc m } by: htsc = MHA(hc, ht) (10) The multi-head attention function here also has its independent parameters. 3.4 Output Layer We get the \ufb01nal representations of the previous outputs by average pooling, concatenate them as the \ufb01nal comprehensive representation \u02dco, and use a full connected layer to project the concatenated vector into the space of the targeted C classes.",
      "3.4 Output Layer We get the \ufb01nal representations of the previous outputs by average pooling, concatenate them as the \ufb01nal comprehensive representation \u02dco, and use a full connected layer to project the concatenated vector into the space of the targeted C classes. \u02dco = [hc avg; ht avg; htsc avg] (11) x = \u02dc Wo T \u02dco + \u02dcbo (12) y = softmax(x) (13) = exp(x) PC k=1 exp(x) (14) where y \u2208RC is the predicted sentiment polarity distribution, \u02dc Wo \u2208R1\u00d7C and \u02dcbo \u2208RC are learn- able parameters. 3.5 Regularization and Model Training Since neutral sentiment is a very fuzzy sentimen- tal state, training samples which labeled neutral are unreliable. We employ a Label Smoothing Regularization (LSR) term in the loss function. which penalizes low entropy output distributions (Szegedy et al., 2016).",
      "We employ a Label Smoothing Regularization (LSR) term in the loss function. which penalizes low entropy output distributions (Szegedy et al., 2016). LSR can reduce over\ufb01tting by preventing a network from assigning the full probability to each training example during train- ing, replaces the 0 and 1 targets for a classi\ufb01er with smoothed values like 0.1 or 0.9. For a training sample x with the original ground-truth label distribution q(k|x), we replace q(k|x) with q(k|x) = (1 \u2212\u03f5)q(k|x) + \u03f5u(k) (15) where u(k) is the prior distribution over labels , and \u03f5 is the smoothing parameter. In this paper, we set the prior label distribution to be uniform u(k) = 1/C. LSR is equivalent to the KL divergence between the prior label distribution u(k) and the network\u2019s predicted distribution p\u03b8.",
      "In this paper, we set the prior label distribution to be uniform u(k) = 1/C. LSR is equivalent to the KL divergence between the prior label distribution u(k) and the network\u2019s predicted distribution p\u03b8. Formally, LSR term is de\ufb01ned as: Llsr = \u2212DKL(u(k)\u2225p\u03b8) (16) The objective function (loss function) to be op- timized is the cross-entropy loss with Llsr and L2 regularization, which is de\ufb01ned as: L(\u03b8) = \u2212 C X i=1 \u02c6yclog(yc) + Llsr + \u03bb X \u03b8\u2208\u0398 \u03b82 (17) where \u02c6y \u2208RC is the ground truth represented as a one-hot vector, y is the predicted sentiment dis- tribution vector given by the output layer, \u03bb is the coef\ufb01cient for L2 regularization term, and \u0398 is the parameter set.",
      "4 Experiments 4.1 Datasets and Experimental Settings We conduct experiments on three datasets: Se- mEval 2014 Task 4 2 (Pontiki et al., 2014) dataset composed of Restaurant reviews and Laptop re- views, and ACL 14 Twitter dataset gathered by Dong et al. (2014). These datasets are labeled with three sentiment polarities: positive, neutral and negative. Table 1 shows the number of train- ing and test instances in each category. Word embeddings in AEN-GloVe do not get updated in the learning process, but we \ufb01ne-tune pre-trained BERT 3 in AEN-BERT. Embedding di- mension ddim is 300 for GloVe and is 768 for pre- trained BERT. Dimension of hidden states dhid is set to 300. The weights of our model are initial- ized with Glorot initialization (Glorot and Bengio, 2010).",
      "Dimension of hidden states dhid is set to 300. The weights of our model are initial- ized with Glorot initialization (Glorot and Bengio, 2010). During training, we set label smoothing parameter \u03f5 to 0.2 (Szegedy et al., 2016), the co- ef\ufb01cient \u03bb of L2 regularization item is 10\u22125 and dropout rate is 0.1. Adam optimizer (Kingma and Ba, 2014) is applied to update all the parameters. We adopt the Accuracy and Macro-F1 metrics to evaluate the performance of the model. 2The detailed introduction of this task can be found at http://alt.qcri.org/semeval2014/task4. 3We use uncased BERT-base from https://github. com/google-research/bert.",
      "Table 1: Statistics of the datasets. Dataset Positive Neural Negative Train Test Train Test Train Test Twitter 1561 173 3127 346 1560 173 Restaurant 2164 728 637 196 807 196 Laptop 994 341 464 169 870 128 4.2 Model Comparisons In order to comprehensively evaluate and analysis the performance of AEN-GloVe, we list 7 baseline models and design 4 ablations of AEN-GloVe. We also design a basic BERT-based model to evaluate the performance of AEN-BERT. Non-RNN based baselines: \u2022 Feature-based SVM (Kiritchenko et al., 2014) is a traditional support vector machine based model with extensive feature engineering. \u2022 Rec-NN (Dong et al., 2014) \ufb01rstly uses rules to transform the dependency tree and put the opin- ion target at the root, and then learns the sentence representation toward target via semantic compo- sition using Recursive NNs.",
      "\u2022 Rec-NN (Dong et al., 2014) \ufb01rstly uses rules to transform the dependency tree and put the opin- ion target at the root, and then learns the sentence representation toward target via semantic compo- sition using Recursive NNs. \u2022 MemNet (Tang et al., 2016b) uses multi-hops of attention layers on the context word embed- dings for sentence representation to explicitly cap- tures the importance of each context word. RNN based baselines: \u2022 TD-LSTM (Tang et al., 2016a) extends LSTM by using two LSTM networks to model the left context with target and the right con- text with target respectively. The left and right target-dependent representations are concatenated for predicting the sentiment polarity of the target. \u2022 ATAE-LSTM (Wang et al., 2016) strengthens the effect of target embeddings, which appends the target embeddings with each word embeddings and use LSTM with attention to get the \ufb01nal rep- resentation for classi\ufb01cation.",
      "\u2022 ATAE-LSTM (Wang et al., 2016) strengthens the effect of target embeddings, which appends the target embeddings with each word embeddings and use LSTM with attention to get the \ufb01nal rep- resentation for classi\ufb01cation. \u2022 IAN (Ma et al., 2017) learns the representa- tions of the target and context with two LSTMs and attentions interactively, which generates the representations for targets and contexts with re- spect to each other. \u2022 RAM (Chen et al., 2017) strengthens Mem- Net by representing memory with bidirectional LSTM and using a gated recurrent unit network to combine the multiple attention outputs for sen- tence representation. AEN-GloVe ablations: \u2022 AEN-GloVe w/o PCT ablates PCT module. \u2022 AEN-GloVe w/o MHA ablates MHA mod- ule. \u2022 AEN-GloVe w/o LSR ablates label smooth- ing regularization. \u2022 AEN-GloVe-BiLSTM replaces the atten- tional encoder layer with two bidirectional LSTM.",
      "\u2022 AEN-GloVe w/o LSR ablates label smooth- ing regularization. \u2022 AEN-GloVe-BiLSTM replaces the atten- tional encoder layer with two bidirectional LSTM. Basic BERT-based model: \u2022 BERT-SPC feeds sequence \u201c[CLS] + context + [SEP] + target + [SEP]\u201d into the basic BERT model for sentence pair classi\ufb01cation task. 4.3 Main Results Table 2 shows the performance comparison of AEN with other models. BERT-SPC and AEN- BERT obtain substantial accuracy improvements, which shows the power of pre-trained BERT on small-data task. The overall performance of AEN- BERT is better than BERT-SPC, which suggests that it is important to design a downstream net- work customized to a speci\ufb01c task. As the prior knowledge in the pre-trained BERT is not speci\ufb01c to any particular domain, further \ufb01ne-tuning on the speci\ufb01c task is necessary for releasing the true power of BERT. The overall performance of TD-LSTM is not good since it only makes a rough treatment of the target words.",
      "The overall performance of TD-LSTM is not good since it only makes a rough treatment of the target words. ATAE-LSTM, IAN and RAM are at- tention based models, they stably exceed the TD- LSTM method on Restaurant and Laptop datasets. RAM is better than other RNN based models, but it does not perform well on Twitter dataset, which might because bidirectional LSTM is not good at modeling small and ungrammatical text. Feature-based SVM is still a competitive base- line, but relying on manually-designed features. Rec-NN gets the worst performances among all neural network baselines as dependency parsing is not guaranteed to work well on ungrammatical short texts such as tweets and comments. Like AEN, MemNet also eschews recurrence, but its overall performance is not good since it does not model the hidden semantic of embeddings, and the result of the last attention is essentially a linear combination of word embeddings. 4.4 Model Analysis As shown in Table 2, the performances of AEN- GloVe ablations are incomparable with AEN-",
      "Table 2: Main results. The results of baseline models are retrieved from published papers. \u201c-\u201d means not reported. Top 3 scores are in bold. Models Twitter Restaurant Laptop Accuracy Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1 RNN baselines TD-LSTM 0.7080 0.6900 0.7563 - 0.6813 - ATAE-LSTM - - 0.7720 - 0.6870 - IAN - - 0.7860 - 0.7210 - RAM 0.6936 0.6730 0.8023 0.7080 0.7449 0.7135 Non-RNN baselines Feature-based SVM 0.6340 0.6330 0.8016 - 0.7049 - Rec-NN 0.6630 0.6590 - - - - MemNet 0.6850 0.6691 0.7816 0.6583 0.7033 0.6409 AEN-GloVe ablations AEN-GloVe w/o PCT 0.7066 0.6907 0.",
      "6850 0.6691 0.7816 0.6583 0.7033 0.6409 AEN-GloVe ablations AEN-GloVe w/o PCT 0.7066 0.6907 0.8017 0.7050 0.7272 0.6750 AEN-GloVe w/o MHA 0.7124 0.6953 0.7919 0.7028 0.7178 0.6650 AEN-GloVe w/o LSR 0.7080 0.6920 0.8000 0.7108 0.7288 0.6869 AEN-GloVe-BiLSTM 0.7210 0.7042 0.7973 0.7037 0.7312 0.6980 Ours AEN-GloVe 0.7283 0.6981 0.8098 0.7214 0.7351 0.6904 BERT-SPC 0.7355 0.7214 0.8446 0.7698 0.",
      "7283 0.6981 0.8098 0.7214 0.7351 0.6904 BERT-SPC 0.7355 0.7214 0.8446 0.7698 0.7899 0.7503 AEN-BERT 0.7471 0.7313 0.8312 0.7376 0.7993 0.7631 GloVe in both accuracy and macro-F1 measure. This result shows that all of these discarded components are crucial for a good performance. Comparing the results of AEN-GloVe and AEN- GloVe w/o LSR, we observe that the accuracy of AEN-GloVe w/o LSR drops signi\ufb01cantly on all three datasets. We could attribute this phe- nomenon to the unreliability of the training sam- ples with neutral sentiment. The overall perfor- mance of AEN-GloVe and AEN-GloVe-BiLSTM is relatively close, AEN-GloVe performs better on the Restaurant dataset.",
      "The overall perfor- mance of AEN-GloVe and AEN-GloVe-BiLSTM is relatively close, AEN-GloVe performs better on the Restaurant dataset. More importantly, AEN- GloVe has fewer parameters and is easier to paral- lelize. To \ufb01gure out whether the proposed AEN-GloVe is a lightweight alternative of recurrent models, we study the model size of each model on the Restau- rant dataset. Statistical results are reported in Ta- ble 3. We implement all the compared models base on the same source code infrastructure, use the same hyperparameters, and run them on the same GPU 4. RNN-based and BERT-based models indeed have larger model size. ATAE-LSTM, IAN, RAM, and AEN-GloVe-BiLSTM are all atten- tion based RNN models, memory optimization for these models will be more dif\ufb01cult as the en- coded hidden states must be kept simultaneously in memory in order to perform attention mech- anisms.",
      "MemNet has the lowest model size as it only has one shared attention layer and two linear layers, it does not calculate hidden states 4NVIDIA GTX 1080ti. Table 3: Model sizes. Memory footprints are evaluated on the Restaurant dataset. Lowest 2 are in bold. Models Model size Params \u00d7106 Memory (MB) TD-LSTM 1.44 12.41 ATAE-LSTM 2.53 16.61 IAN 2.16 15.30 RAM 6.13 31.18 MemNet 0.36 7.82 AEN-BERT 112.93 451.84 AEN-GloVe-BiLSTM 3.97 22.52 AEN-GloVe 1.16 11.04 of word embeddings. AEN-GloVe\u2019s lightweight level ranks second, since it takes some more pa- rameters than MemNet in modeling hidden states of sequences. As a comparison, the model size of AEN-GloVe-BiLSTM is more than twice that of AEN-GloVe, but does not bring any performance improvements.",
      "As a comparison, the model size of AEN-GloVe-BiLSTM is more than twice that of AEN-GloVe, but does not bring any performance improvements. 5 Conclusion In this work, we propose an attentional encoder network for the targeted sentiment classi\ufb01cation task. which employs attention based encoders for the modeling between context and target. We raise the the label unreliability issue add a label smoothing regularization to encourage the model to be less con\ufb01dent with fuzzy labels. We also ap- ply pre-trained BERT to this task and obtain new state-of-the-art results. Experiments and analysis demonstrate the effectiveness and lightweight of the proposed model.",
      "References Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473. Peng Chen, Zhongqian Sun, Lidong Bing, and Wei Yang. 2017. Recurrent attention network on mem- ory for aspect sentiment analysis. In Proceedings of the 2017 Conference on Empirical Methods in Nat- ural Language Processing, pages 452\u2013461. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805. Xiaowen Ding, Bing Liu, and Philip S Yu. 2008. A holistic lexicon-based approach to opinion mining. In Proceedings of the 2008 international conference on web search and data mining, pages 231\u2013240. ACM.",
      "Xiaowen Ding, Bing Liu, and Philip S Yu. 2008. A holistic lexicon-based approach to opinion mining. In Proceedings of the 2008 international conference on web search and data mining, pages 231\u2013240. ACM. Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming Zhou, and Ke Xu. 2014. Adaptive recursive neural network for target-dependent twitter sentiment clas- si\ufb01cation. In Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pages 49\u201354. Xavier Glorot and Yoshua Bengio. 2010. Understand- ing the dif\ufb01culty of training deep feedforward neu- ral networks. In Proceedings of the thirteenth in- ternational conference on arti\ufb01cial intelligence and statistics, pages 249\u2013256. Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent twitter senti- ment classi\ufb01cation.",
      "Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent twitter senti- ment classi\ufb01cation. In Proceedings of the 49th An- nual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 151\u2013160. Association for Computational Linguistics. Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and Saif Mohammad. 2014. Nrc-canada-2014: Detect- ing aspects and sentiment in customer reviews. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 437\u2013 442. Xin Li, Lidong Bing, Wai Lam, and Bei Shi. 2018. Transformation networks for target-oriented senti- ment classi\ufb01cation.",
      "In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 437\u2013 442. Xin Li, Lidong Bing, Wai Lam, and Bei Shi. 2018. Transformation networks for target-oriented senti- ment classi\ufb01cation. In Proceedings of the 56th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 946\u2013956. Dehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng Wang. 2017. Interactive attention networks for aspect-level sentiment classi\ufb01cation. In Proceed- ings of the 26th International Joint Conference on Arti\ufb01cial Intelligence, pages 4068\u20134074. AAAI Press. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 confer- ence on empirical methods in natural language pro- cessing (EMNLP), pages 1532\u20131543.",
      "Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 confer- ence on empirical methods in natural language pro- cessing (EMNLP), pages 1532\u20131543. Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. Semeval-2014 task 4: As- pect based sentiment analysis. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27\u201335. Delip Rao and Deepak Ravichandran. 2009. Semi- supervised polarity lexicon induction. In Proceed- ings of the 12th Conference of the European Chap- ter of the Association for Computational Linguistics, pages 675\u2013682. Association for Computational Lin- guistics. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. 2016.",
      "Association for Computational Lin- guistics. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. 2016. Rethink- ing the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818\u20132826. Duyu Tang, Bing Qin, Xiaocheng Feng, and Ting Liu. 2016a. Effective lstms for target-dependent sen- timent classi\ufb01cation. In Proceedings of COLING 2016, the 26th International Conference on Compu- tational Linguistics: Technical Papers, pages 3298\u2013 3307. Duyu Tang, Bing Qin, and Ting Liu. 2016b. Aspect level sentiment classi\ufb01cation with deep memory net- work. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Process- ing, pages 214\u2013224.",
      "Duyu Tang, Bing Qin, and Ting Liu. 2016b. Aspect level sentiment classi\ufb01cation with deep memory net- work. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Process- ing, pages 214\u2013224. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, pages 5998\u20136008. Yequan Wang, Minlie Huang, Li Zhao, et al. 2016. Attention-based lstm for aspect-level sentiment clas- si\ufb01cation. In Proceedings of the 2016 conference on empirical methods in natural language processing, pages 606\u2013615. Paul J Werbos. 1990. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78(10):1550\u20131560."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1902.09314.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":6999,
  "avg_doclen":179.4615384615,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1902.09314.pdf"
    }
  }
}