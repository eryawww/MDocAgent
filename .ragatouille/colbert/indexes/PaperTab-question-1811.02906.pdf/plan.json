{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Transfer Learning from LDA to BiLSTM-CNN for Offensive Language Detection in Twitter Gregor Wiedemann Eugen Ruppert Raghav Jindal Chris Biemann Language Technology Group Department of Informatics University of Hamburg, Germany {gwiedemann, ruppert, biemann}@informatik.uni-hamburg.de raghavjindal2003@gmail.com Abstract We investigate different strategies for au- tomatic offensive language classi\ufb01cation on German Twitter data. For this, we em- ploy a sequentially combined BiLSTM- CNN neural network. Based on this model, three transfer learning tasks to improve the classi\ufb01cation performance with back- ground knowledge are tested. We compare 1. Supervised category transfer: social me- dia data annotated with near-offensive lan- guage categories, 2. Weakly-supervised category transfer: tweets annotated with emojis they contain, 3. Unsupervised cate- gory transfer: tweets annotated with topic clusters obtained by Latent Dirichlet Allo- cation (LDA).",
            "Weakly-supervised category transfer: tweets annotated with emojis they contain, 3. Unsupervised cate- gory transfer: tweets annotated with topic clusters obtained by Latent Dirichlet Allo- cation (LDA). Further, we investigate the effect of three different strategies to miti- gate negative effects of \u2018catastrophic forget- ting\u2019 during transfer learning. Our results indicate that transfer learning in general im- proves offensive language detection. Best results are achieved from pre-training our model on the unsupervised topic cluster- ing of tweets in combination with thematic user cluster information. 1 Introduction User-generated content in forums, blogs, and so- cial media not only contributes to a deliberative exchange of opinions and ideas but is also contami- nated with offensive language such as threats and discrimination against people, swear words or blunt insults. The automatic detection of such content can be a useful support for moderators of public platforms as well as for users who could receive warnings or would be enabled to \ufb01lter unwanted content. Although this topic now has been studied for more than two decades, so far there has been little work on offensive language detection for German social media content.",
            "Although this topic now has been studied for more than two decades, so far there has been little work on offensive language detection for German social media content. Regarding this, we present a new approach to detect offensive language as de\ufb01ned in the shared task of the GermEval 2018 workshop.1 For our contribution to the shared task, we focus on the question how to apply transfer learning for neural network-based text classi\ufb01ca- tion systems. In Germany, the growing interest in hate speech analysis and detection is closely related to recent political developments such as the increase of right- wing populism, and societal reactions to the ongo- ing in\ufb02ux of refugees seeking asylum (Ross et al., 2016). Content analysis studies such as Krei\u00dfel et al. (2018) have shown that a majority of hate speech comments in German Facebook is authored by a rather small group of very active users (5% of all accounts engaging in hate speech). The \ufb01ndings suggest that already such small groups are able to severely disturb social media debates for large audiences.",
            "(2018) have shown that a majority of hate speech comments in German Facebook is authored by a rather small group of very active users (5% of all accounts engaging in hate speech). The \ufb01ndings suggest that already such small groups are able to severely disturb social media debates for large audiences. From the perspective of natural language pro- cessing, the task of automatic detection of offen- sive language in social media is complex due to three major reasons. First, we can expect \u2018atypical\u2019 language data due to incorrect spellings, false gram- mar and non-standard language variations such as slang terms, intensi\ufb01ers, or emojis\/emoticons. For the automatic detection of offensive language, it is not quite clear whether these irregularities should be treated as \u2018noise\u2019 or as a signal. Second, the task cannot be reduced to an analysis of word-level semantics only, e.g. spotting offensive keyterms in the data. Instead, the assessment of whether or not a post contains offensive language can be highly de- pendent on sentence and discourse level semantics, as well as subjective criteria. In a crowd-sourcing experiment on \u2018hate speech\u2019 annotation, Ross et al.",
            "spotting offensive keyterms in the data. Instead, the assessment of whether or not a post contains offensive language can be highly de- pendent on sentence and discourse level semantics, as well as subjective criteria. In a crowd-sourcing experiment on \u2018hate speech\u2019 annotation, Ross et al. (2016) achieved only very low inter-rater agree- ment between annotators. Offensive language is 1https:\/\/projects.fzai.h-da.de\/iggsa arXiv:1811.02906v1  [cs.CL]  7 Nov 2018",
            "probably somewhat easier to achieve agreement on, but still sentence-level semantics and context or \u2018world knowledge\u2019 remains important. Third, there is a lack of a common de\ufb01nition of the ac- tual phenomenon to tackle. Published studies fo- cus on \u2018hostile messages\u2019, \u2018\ufb02ames\u2019, \u2018hate speech\u2019, \u2018discrimination\u2019, \u2018abusive language\u2019, or \u2018offensive language\u2019. Although certainly overlapping, each of these categories has been operationalized in a slightly different manner. Since category de\ufb01ni- tions do not match properly, publicly available an- notated datasets and language resources for one task cannot be used directly to train classi\ufb01ers for any respective other task. Contribution: For the offensive language detec- tion presented in this paper, our approach is to use semi-supervised text classi\ufb01cation to address all of the three challenges. In order to account for atypical language, we use sub-word embeddings to represent word tokens, words unseen during training, misspelled words and words speci\ufb01cally used in the context of social media such as emojis.",
            "In order to account for atypical language, we use sub-word embeddings to represent word tokens, words unseen during training, misspelled words and words speci\ufb01cally used in the context of social media such as emojis. To represent complex sequence information from tweets, we use a neural network model combining recurrent (e.g. Long-Short term memory, LSTM) (Hochreiter and Schmidhuber, 1997) and convolu- tional (CNN) layers. Both learning architectures, LSTM and CNN, have already been employed suc- cessfully in similar text classi\ufb01cation tasks such as sentiment analysis (Kim, 2014). We expect the combination of LSTM and CNN to be especially useful in the context of transfer learning. The main contribution of this paper is to investi- gate potential performance contributions of transfer learning to offensive language detection. For this, we investigate three different approaches to make use of knowledge learned by one task to improve classi\ufb01cation for our actual offensive language task. To pre-train our BiLSTM-CNN network, we em- ploy 1.",
            "For this, we investigate three different approaches to make use of knowledge learned by one task to improve classi\ufb01cation for our actual offensive language task. To pre-train our BiLSTM-CNN network, we em- ploy 1. Supervised category transfer: social media data annotated with near-offensive language cat- egories, 2. Weakly-supervised category transfer: tweets annotated with emojis they contain, and 3. Unsupervised category transfer: tweets anno- tated with topic clusters obtained by Latent Dirich- let Allocation (LDA) (Blei et al., 2003). Further, we investigate the effect of three different trans- fer learning strategies on the classi\ufb01cation perfor- mance to mitigate the effect of \u2018catastrophic forget- ting\u2019.2 The results indicate that transfer learning 2Catastrophic forgetting refers to the phenomenon that dur- on generic topic clusters of tweets derived from an LDA process of a large Twitter background corpus signi\ufb01cantly improves offensive language detec- tion. We present our \ufb01ndings in the following struc- ture: Section 2 addresses related work to our ap- proach.",
            "We present our \ufb01ndings in the following struc- ture: Section 2 addresses related work to our ap- proach. In Section 3, we introduce the details of the GermEval 2018 Shared Task together with our background corpora for knowledge transfer. In Section 4, we describe our BiLSTM-CNN model for text classi\ufb01cation. Section 5 introduces the different transfer learning setups we investigate. To evaluate these setups, we conduct a number of experiments for which results are presented in Sec- tion 6. This section also contains a brief discussion of errors made by our model. Finally, we give some concluding remarks. 2 Related Work Automatic detection of offensive language is a well- studied phenomenon for the English language. Ini- tial works on the detection of \u2018hostile messages\u2019 have been published already during the 1990s (Spertus, 1997). An overview of recent approaches comparing the different task de\ufb01nitions, feature sets and classi\ufb01cation methods is given by Schmidt and Wiegand (2017).",
            "An overview of recent approaches comparing the different task de\ufb01nitions, feature sets and classi\ufb01cation methods is given by Schmidt and Wiegand (2017). A major step forward to sup- port the task was the publication of a large publicly available, manually annotated dataset by Yahoo re- search (Nobata et al., 2016). They provide a classi- \ufb01cation approach for detection of abusive language in Yahoo user comments using a variety of linguis- tic features in a linear classi\ufb01cation model. One major result of their work was that learning text fea- tures from comments which are temporally close to the to-be-predicted data is more important than learning features from as much data as possible. This is especially important for real-life scenarios of classifying streams of comment data. In addition to token-based features, Xiang et al. (2012) success- fully employed topical features to detect offensive tweets. We will build upon this idea by employing topical data in our transfer learning setup. Transfer learning recently has gained a lot of attention since it can be easily applied to neural network learn- ing architectures.",
            "(2012) success- fully employed topical features to detect offensive tweets. We will build upon this idea by employing topical data in our transfer learning setup. Transfer learning recently has gained a lot of attention since it can be easily applied to neural network learn- ing architectures. For instance, Howard and Ruder (2018) propose a generic transfer learning setup for ing supervised learning of the actual task in a transfer learning setup the update of model parameters can overwrite knowl- edge obtained by the previously conducted training task. This will eventually eliminate any positive effect of pre-training and knowledge transfer from background corpora.",
            "text classi\ufb01cation based on language modeling for pre-training neural models with large background corpora. To improve offensive language detection for English social media texts, a transfer learning approach was recently introduced by Felbo et al. (2017). Their \u2018deepmoji\u2019 approach relies on the idea to pre-train a neural network model for an ac- tual offensive language classi\ufb01cation task by using emojis as weakly supervised training labels. On a large collection of millions of randomly collected English tweets containing emojis, they try to pre- dict the speci\ufb01c emojis from features obtained from the remaining tweet text. We will follow this idea of transfer learning to evaluate it for offensive lan- guage detection in German Twitter data together with other transfer learning strategies. 3 Data and Tasks 3.1 GermEval 2018 Shared Task Organizers of GermEval 2018 provide training and test datasets for two tasks. Task 1 is a binary clas- si\ufb01cation for deciding whether or not a German tweet contains offensive language (the respective category labels are \u2018offense\u2019 and \u2018other\u2019).",
            "Task 1 is a binary clas- si\ufb01cation for deciding whether or not a German tweet contains offensive language (the respective category labels are \u2018offense\u2019 and \u2018other\u2019). Task 2 is a multi-class classi\ufb01cation with more \ufb01ne-grained labels sub-categorizing the same tweets into either \u2018insult\u2019, \u2018profanity\u2019, \u2018abuse\u2019, or \u2018other\u2019. The training data contains 5,008 manually la- beled tweets sampled from Twitter from selected accounts that are suspected to contain a high share of offensive language. Manual inspection reveals a high share of political tweets among those la- beled as offensive. These tweets range from offend- ing single Twitter users, politicians and parties to degradation of whole social groups such as Mus- lims, migrants or refugees. The test data contains 3,532 tweets. To create a realistic scenario of truly unseen test data, training and test set are sampled from disjoint user accounts. No standard validation set is provided for the task. To optimize hyper- parameters of our classi\ufb01cation models and allow for early stopping to prevent the neural models from over\ufb01tting, we created our own validation set.",
            "No standard validation set is provided for the task. To optimize hyper- parameters of our classi\ufb01cation models and allow for early stopping to prevent the neural models from over\ufb01tting, we created our own validation set. For this, we used the last 808 examples from the provided training set. The remaining \ufb01rst 4,200 examples were used to train our models. 3.2 Background Knowledge Since the provided dataset for offensive language detection is rather small, we investigate the poten- tial of transfer learning to increase classi\ufb01cation performance. For this, we use the following labeled as well as unlabeled datasets. One Million Posts: A recently published re- source of German language social media data has been published by Schabus et al. (2017). Among other things, the dataset contains 11,773 labeled user comments posted to the Austrian newspaper website \u2018Der Standard\u2019.3 Comments have not been annotated for offensive language, but for categories such as positive\/negative sentiment, off-topic, inap- propriate or discriminating.",
            "(2017). Among other things, the dataset contains 11,773 labeled user comments posted to the Austrian newspaper website \u2018Der Standard\u2019.3 Comments have not been annotated for offensive language, but for categories such as positive\/negative sentiment, off-topic, inap- propriate or discriminating. Twitter: As a second resource, we use a back- ground corpus of German tweets that were col- lected using the Twitter streaming API from 2011 to 2017. Since the API provides a random fraction of all tweets (1%), language identi\ufb01cation is per- formed using \u2018langid.py\u2019 (Lui and Baldwin, 2012) to \ufb01lter for German tweets. For all years com- bined, we obtain about 18 million unlabeled Ger- man tweets from the stream, which can be used as a large, in-domain background corpus. 4 Text Classi\ufb01cation In the following section, we describe one linear classi\ufb01cation model in combination with speci\ufb01- cally engineered features, which we use as a base- line for the classi\ufb01cation task.",
            "4 Text Classi\ufb01cation In the following section, we describe one linear classi\ufb01cation model in combination with speci\ufb01- cally engineered features, which we use as a base- line for the classi\ufb01cation task. We further introduce a neural network model as a basis for our approach to transfer learning. This model achieves the high- est performance for offensive language detection, as compared to our baseline. 4.1 SVM baseline: Model: The baseline classi\ufb01er uses a linear Sup- port Vector Machine (Fan et al., 2008), which is suited for a high number of features. We use a text classi\ufb01cation framework for German (Ruppert et al., 2017) that has been used successfully for sentiment analysis before. Features: We induce token features based on the Twitter background corpus. Because tweets are usually very short, they are not an optimal source to obtain good estimates on inverse document fre- quencies (IDF). To obtain a better feature weight- ing, we calculate IDF scores based on the Twitter corpus combined with an in-house product review dataset (cf. ibid.).",
            "Because tweets are usually very short, they are not an optimal source to obtain good estimates on inverse document fre- quencies (IDF). To obtain a better feature weight- ing, we calculate IDF scores based on the Twitter corpus combined with an in-house product review dataset (cf. ibid.). From this combined corpus, we compute the IDF scores and 300-dimensional word 3http:\/\/derstandard.at",
            "embeddings (Mikolov et al., 2013) for all contained features. Following Ruppert et al. (2017), we use the IDF scores to obtain the highest-weighted terms per category in the training data. Here, we obtain words like Staatsfunk, Vasall (state media, vassal) or deutschlandfeindlichen (Germany-opposing) for the category \u2018abuse\u2019 and curse words for \u2018insult\u2019. Further, IDF scores are used to weight the word vectors of all terms in a tweet. Additionally, we em- ploy a polarity lexicon and perform lexical expan- sion on it to obtain new entries from our in-domain background corpus that are weighted on a \u2018positive\u2013 negative\u2019 continuum. Lexical expansion is based on distributional word similarity as described in Kumar et al. (2016). 4.2 BiLSTM-CNN for Text Classi\ufb01cation Model: For transfer learning, we rely on a neu- ral network architecture implemented in the Keras framework for Python.4 Our model (see Fig.",
            "(2016). 4.2 BiLSTM-CNN for Text Classi\ufb01cation Model: For transfer learning, we rely on a neu- ral network architecture implemented in the Keras framework for Python.4 Our model (see Fig. 1) combines a bi-directional LSTM layer (Hochreiter and Schmidhuber, 1997) with 100 units followed by three parallel convolutional layers (CNN), each with a different kernel size k \u22083,4,5, and a \ufb01lter size 200. The outputs of the three CNN blocks are max-pooled globally and concatenated. Finally, features encoded by the CNN blocks are fed into a dense layer with 100 units, followed by the pre- diction layer. Except for this \ufb01nal layer which uses Softmax activation, we rely on LeakyReLU activa- tion (Maas et al., 2013) for the other model layers. For regularization, dropout is applied to the LSTM layer and to each CNN block after global max- pooling (dropout rate 0.5).",
            "For regularization, dropout is applied to the LSTM layer and to each CNN block after global max- pooling (dropout rate 0.5). For training, we use the Nesterov Adam optimization and categorical cross- entropy loss with a learning rate of 0.002. The intuition behind this architecture is that the recur- rent LSTM layer can serve as a feature encoder for general language characteristics from sequences of semantic word embeddings. The convolutional layers on top of this can then encode category re- lated features delivered by the LSTM while the last dense layers \ufb01nally \ufb01ne-tune highly category- speci\ufb01c features for the actual classi\ufb01cation task. Features: As input, we feed 300-dimensional word embeddings obtained from fastText (Bo- janowski et al., 2017) into our model. Since fast- Text also makes use of sub-word information (char- acter n-grams), it has the great advantage that it can provide semantic embeddings also for words that 4https:\/\/keras.io Figure 1: BiLSTM-CNN model architecture. We use a combination of recurrent and convolutional cells for learning.",
            "We use a combination of recurrent and convolutional cells for learning. As input, we rely on (sub-)word embeddings. The \ufb01nal architecture also includes clustering information obtained from Twitter user ids. Dotted lines indicate dropout with rate 0.5 between layers. The last dense layer contains n units for prediction of the probability of each of the n classi\ufb01cation labels per task. have not been seen during training the embedding model. We use a model pre-trained with German language data from Wikipedia and Common Crawl provided by Mikolov et al. (2018). First, we unify all Twitter-typical user mentions (\u2018@username\u2019) and URLs into a single string representation and reduce all characters to lower case. Then, we split tweets into tokens at boundaries of changing char- acter classes. As an exception, sequences of emoji characters are split into single character tokens. Finally, for each token, an embedding vector is obtained from the fastText model. For offensive language detection in Twitter, users addressed in tweets might be an additional relevant signal.",
            "As an exception, sequences of emoji characters are split into single character tokens. Finally, for each token, an embedding vector is obtained from the fastText model. For offensive language detection in Twitter, users addressed in tweets might be an additional relevant signal. We assume it is more likely that politicians or news agencies are addressees of offensive lan- guage than, for instance, musicians or athletes. To make use of such information, we obtain a cluster- ing of user ids from our Twitter background corpus. From all tweets in our stream from 2016 or 2017, we extract those tweets that have at least two @- mentions and all of the @-mentions have been seen at least \ufb01ve times in the background corpus. Based",
            "Table 1: Examples of Twitter user clusters Cluster Accounts 26 breitbartnews, realdonaldtrump, jrch- eneyjohn, lindasuhler, barbmuenchen 28 dagibee, lilyachty, youngthug, chris- brown, richthekid 40 bvb, fcbayern, dfb, young, team 44 spdde, cdu, gruenen, martinschulz, fdp, dielinke 50 tagesschau, spiegelonline, zdf, zeiton- line, janboehm on the resulting 1.8 million lists of about 169,000 distinct user ids, we compute a topic model with K = 50 topics using Latent Dirichlet Allocation (Blei et al., 2003). For each of the user ids, we extract the most probable topic from the inferred user id-topic distribution as cluster id. This results in a thematic cluster id for most of the user ids in our background corpus grouping together accounts such as American or German political actors, musi- cians, media websites or sports clubs (see Table 1).",
            "This results in a thematic cluster id for most of the user ids in our background corpus grouping together accounts such as American or German political actors, musi- cians, media websites or sports clubs (see Table 1). For our \ufb01nal classi\ufb01cation approach, cluster ids for users mentioned in tweets are fed as a second input in addition to (sub-)word embeddings to the penul- timate dense layer of the neural network model. 5 Transfer Learning As mentioned earlier, we investigate potential strategies for transfer learning to achieve optimal performance. For this, we compare three different methods to pre-train our model with background data sets. We also compare three different strategies to combat \u2018catastrophic forgetting\u2019 during training on the actual target data. 5.1 Background Knowledge For a transfer learning setup, we need to specify a task to train the model and prepare the corre- sponding dataset. We compare the following three methods. Supervised near-category transfer: As intro- duced above, the \u2018One Million Post\u2019 corpus pro- vides annotation labels for more than 11,000 user comments.",
            "We compare the following three methods. Supervised near-category transfer: As intro- duced above, the \u2018One Million Post\u2019 corpus pro- vides annotation labels for more than 11,000 user comments. Although there is no directly compa- rable category capturing \u2018offensive language\u2019 as de\ufb01ned in the shared task, there are two closely related categories. From the resource, we extract all those comments in which a majority of the anno- tators agree that they contain either \u2018inappropriate\u2019 or \u2018discriminating\u2019 content, or none of the afore- mentioned. We treat the \ufb01rst two cases as exam- ples of \u2018offense\u2019 and the latter case as examples of \u2018other\u2019. This results in 3,599 training examples (519 offense, 3080 other) from on the \u2018One Million Post\u2019 corpus. We conduct pre-training of the neural model as a binary classi\ufb01cation task (similar to the Task 1 of GermEval 2018) Weakly-supervised emoji transfer: Following the approach of Felbo et al. (2017), we constructed a weakly-supervised training dataset from our Twit- ter background corpus.",
            "(2017), we constructed a weakly-supervised training dataset from our Twit- ter background corpus. From all tweets posted be- tween 2013 and 2017, we extract those containing at least one emoji character. In the case of several emojis in one tweet, we duplicate the tweet for each unique emoji type. Emojis are then removed from the actual tweets and treated as a label to predict by the neural model. This results in a multi-class classi\ufb01cation task to predict the right emoji out of 1,297 different ones. Our training dataset contains 1,904,330 training examples. Unsupervised topic transfer: As a \ufb01nal method, we create a training data set for transfer learning in a completely unsupervised manner. For this, we compute an LDA clustering with K = 1,000 topics5 on 10 million tweets sampled from 2016 and 2017 from our Twitter background corpus containing at least two meaningful words (i.e. alphanumeric sequences that are not stopwords, URLs or user mentions). Tweets also have been deduplicated before sampling.",
            "alphanumeric sequences that are not stopwords, URLs or user mentions). Tweets also have been deduplicated before sampling. From the topic-document distri- bution of the resulting LDA model, we determined the majority topic id for each tweet as a target label for prediction during pre-training our neural model. Pre-training of the neural model was conducted on the 10 million tweets with batch size 128 for 10 epochs. 5.2 Transfer Learning Strategies Once the neural model has been pre-trained on the above-speci\ufb01ed targets and corresponding datasets, we can apply it for learning our actual target task. For this, we need to remove the \ufb01nal prediction layer of the pre-trained model (i.e. Layer 4 in Fig. 1), and add a new dense layer for prediction of one of the actual label sets (two for Task 1, four for Task 2). The training for the actual GermEval 5For LDA, we used Mallet (http:\/\/mallet.cs. umass.edu) with Gibbs Sampling for 1,000 iterations and priors \u03b1 = 10\/K and \u03b2 = 0.01.",
            "tasks is conducted with batch size 32 for up to 50 epochs. To prevent the aforementioned effect of forgetting pre-trained knowledge during this task- speci\ufb01c model training, we evaluate three different strategies. Gradual unfreezing (GU): In Howard and Ruder (2018), gradual unfreezing of pre-trained model weights is proposed as one strategy to miti- gate forgetting. The basic idea is to initially freeze all pre-trained weights of the neural model and keep only the newly added last layer trainable (i.e. Layer 4 in Fig. 1). After training that last layer for one epoch on the GermEval training data, the next lower frozen layer is unfrozen and training will be repeated for another epoch. This will be iterated until all layers (4 to 1) are unfrozen. Single bottom-up unfreezing (BU): Following the approach of Felbo et al. (2017), we do not iter- atively unfreeze all layers of the model, but only one at a time. First, the newly added \ufb01nal predic- tion layer is trained while all other model weights remain frozen. Training is conducted for up to 50 epochs.",
            "(2017), we do not iter- atively unfreeze all layers of the model, but only one at a time. First, the newly added \ufb01nal predic- tion layer is trained while all other model weights remain frozen. Training is conducted for up to 50 epochs. The best performing model during these epochs with respect to our validation set is then used in the next step of \ufb01ne-tuning the pre-trained model layers. For the bottom-up strategy, we un- freeze the lowest layer (1) containing the most gen- eral knowledge \ufb01rst, then we continue optimization with the more speci\ufb01c layers (2 and 3) one after the other. During \ufb01ne-tuning of each single layer, all other layers remain frozen and training is per- formed for 50 epochs selecting the best performing model at the end of each layer optimization. In a \ufb01nal round of \ufb01ne-tuning, all layers are unfrozen.",
            "During \ufb01ne-tuning of each single layer, all other layers remain frozen and training is per- formed for 50 epochs selecting the best performing model at the end of each layer optimization. In a \ufb01nal round of \ufb01ne-tuning, all layers are unfrozen. Single top-down unfreezing (TU): This pro- ceeding is similar the one described above, but inverts the order of unfreezing single layers from top to bottom sequentially \ufb01ne-tuning layers 4, 3, 2, 1 individually, and all together in a \ufb01nal round. Baseline (Pre-train only): All strategies are compared to the baseline of no freezing of model weights, but training all layers at once directly after pre-training with one of the three transfer datasets. 6 Evaluation Since there is no prior state-of-the-art for the Germ- Eval Shared Task 2018 dataset, we evaluate the performance of our neural model compared to the baseline SVM architecture.",
            "6 Evaluation Since there is no prior state-of-the-art for the Germ- Eval Shared Task 2018 dataset, we evaluate the performance of our neural model compared to the baseline SVM architecture. We further compare the Table 2: Transfer learning performance (Task 1) Transfer Strategy F1 Accuracy None - 0.709 0.795 Category Pre-train only 0.712 0.809 GU 0.702 0.796 BU 0.709 0.802 TU 0.711 0.799 Emoji Pre-train only 0.720 0.811 GU 0.708 0.807 BU 0.739 0.817 TU 0.725 0.814 Topic Pre-train only 0.733 0.817 GU 0.712 0.801 BU 0.753 0.828 TU 0.732 0.817 different tasks and strategies for transfer learning introduced above and provide some \ufb01rst insights on error analysis. Transfer learning: First, we evaluate the perfor- mance of different transfer learning datasets and strategies.",
            "Transfer learning: First, we evaluate the perfor- mance of different transfer learning datasets and strategies. Tables 2 and 3 show that we achieve best performances for both tasks on our validation set by pre-training our neural model on the large Twitter datasets.6 The two approaches, emoji and topic transfer, substantially improve the classi\ufb01ca- tion performance compared to not using transfer learning at all (\u2018None\u2019). In contrast, pre-training on the annotated dataset from the \u2018One Million Posts\u2019 corpus does only lead to minor improve- ments. Comparing the three different strategies to reduce negative effects of forgetting in transfer learning, the strategy of unfreezing single layers during training from the lowest layers to the top of the model architecture (BU) performs best, espe- cially in conjunction with the pre-training on the large Twitter datasets. For these setups, the model can take full advantage of learning language regu- larities from generic to more task-speci\ufb01c features in its different layers.",
            "For these setups, the model can take full advantage of learning language regu- larities from generic to more task-speci\ufb01c features in its different layers. The other strategies (GU, TU) do not perform better than pre-training the 6For the binary classi\ufb01cation Task 1, we report precision (P), recall (R), and F1 for the targeted positive class \u2018offense\u2019. During training, we also optimized for binary F1. For the multi-class classi\ufb01cation Task 2, we report macro-F1 (average of precision, recall, and F1 of all individual four categories). During training, we also optimized for macro-F1. All reported results are average values obtained from 10 repeated runs of model training.",
            "Table 3: Transfer learning performance (Task 2) Transfer Strategy F1 Accuracy None - 0.578 0.747 Category Pre-train only 0.578 0.755 GU 0.560 0.751 BU 0.580 0.750 TU 0.581 0.759 Emoji Pre-train only 0.572 0.756 GU 0.564 0.756 BU 0.577 0.764 TU 0.592 0.757 Topic Pre-train only 0.597 0.762 GU 0.590 0.755 BU 0.607 0.764 TU 0.582 0.764 neural model and then immediately training the entire network on the actual task (\u2018Pre-train only\u2019). Final results: Tables 4 and 5 show the \ufb01nal re- sults for the two offensive language detection tasks on the of\ufb01cial test set. We compare the base- line SVM model with the BiLSTM-CNN neural model with the best performing transfer learning setup (BU). Additionally, we show the results when adding cluster information from users addressed in tweets (cf.",
            "We compare the base- line SVM model with the BiLSTM-CNN neural model with the best performing transfer learning setup (BU). Additionally, we show the results when adding cluster information from users addressed in tweets (cf. Section 4). Due to the fact that training and validation data were sampled from a different user account population than the test dataset (cf. Section 3), evaluation scores on the of\ufb01cial test data are drastically lower than scores achieved on our validation set during model selection. Compared to the already highly tweaked SVM baseline, our BiLSTM-CNN model architecture with topic transfer delivers comparable results for identifying offensive language in Task 1 and sig- ni\ufb01cantly improved results for Task 2. The SVM achieves a high precision but fails to identify many offensive tweets, which especially in Task 2 nega- tively affects the recall. In contrast, topic transfer leads to a signi\ufb01cant improvement, especially for Task 2. Performance gains mainly stem from increased recall due to the background knowledge incorporated into the model.",
            "In contrast, topic transfer leads to a signi\ufb01cant improvement, especially for Task 2. Performance gains mainly stem from increased recall due to the background knowledge incorporated into the model. We assume that not only language regu- larities are learned through pre-training but that also some aspects relevant for offensive language already are grouped together by the LDA clusters used for pre-training. As a second task-speci\ufb01c extension of our text classi\ufb01cation, we feed cluster information for users addressed in tweets into the process. Here the re- sults are mixed. While this information did not lead to major performance increases on our validation set (not shown), the improvements for the of\ufb01cial test set are quite signi\ufb01cant. For Task 1, the per- formance score increases several percentage points up to 75.2% F1 (Accuracy 77.5%). For Task 2, increases are still quite remarkable, although the absolute performance of this multi-class problem with 52.7% F1 (Accuracy 73.7%) is rather mod- erate.",
            "For Task 2, increases are still quite remarkable, although the absolute performance of this multi-class problem with 52.7% F1 (Accuracy 73.7%) is rather mod- erate. From these results, we infer that thematic user clusters apparently contribute a lot of informa- tion to generalize an offensive language detection model to unseen test data. Error analysis: Accuracy values for German of- fensive language detection around 75% signal some room for improvement in future work. What are the hard cases for classifying offensive language? We look at false positive (FP) and false negatives (FN) for Task 1. In our validation set, the ratio of FP and FN is about 60:40, which means our classi- \ufb01er slightly more often assumes offensive language than there is actually in the data compared to cases in which it misses to recognize offensive tweets. Looking more qualitatively into FP examples, we can see a lot of cases which actually express a very critical opinion and\/or use harsh language, but are not unequivocal insults. Another group of FP tweets does not express insults directly but for- mulates offensive content as a question.",
            "Looking more qualitatively into FP examples, we can see a lot of cases which actually express a very critical opinion and\/or use harsh language, but are not unequivocal insults. Another group of FP tweets does not express insults directly but for- mulates offensive content as a question. In other cases, it is really dependent on context whether a tweet addressing a speci\ufb01c group uses that group signi\ufb01er actually with a derogatory intention (e.g. calling people \u2018Jew\u2019, \u2018Muslim\u2019, or \u2018Communist\u2019). For FN tweets, we can identify insults that are rather subtle. They do not use derogatory vocab- ulary but express loathing by dehumanizing syn- tax (e.g. \u2018das was uns regiert\u2019 where the de\ufb01nite gender-neutral article \u2018das\u2019 refers to the German chancellor), metaphor (\u2018Der ist nicht die hellste Kerze\u2019, i.e. \u2018he is not the brightest light\u2019) or insin- uating an incestuous relationship of some persons parents (\u2018Hier dr\u00a8angt sich der Verdacht auf, das die Eltern der beiden Geschwister waren\u2019).",
            "\u2018he is not the brightest light\u2019) or insin- uating an incestuous relationship of some persons parents (\u2018Hier dr\u00a8angt sich der Verdacht auf, das die Eltern der beiden Geschwister waren\u2019). Another repeatedly occurring FN case are tweets express- ing suspicion against the government, democratic institutions, the media or elections. While those tweets certainly in most cases origin from a radi-",
            "Table 4: Offensive language detection performance % (Task 1) Model RunID Offense Other Average (of\ufb01cial rank score) P R F1 P R F1 P R F1 Acc. Baseline SVM coarse 1 71.52 46.17 56.12 76.52 90.52 82.93 74.02 68.34 71.07 75.42 BiLSTM-CNN + Topic transfer coarse 2 66.30 49.75 56.84 77.03 86.95 81.69 71.67 68.35 69.97 74.29 + User-cluster coarse 3 66.29 68.89 67.56 83.62 81.93 82.77 74.96 75.41 75.18 77.49 Table 5: Offensive language detection performance % (Task 2) Model RunID Abuse Insult Other Profanity Average (of\ufb01cial rank score) F F F F P R F Acc.",
            "Baseline SVM \ufb01ne 1 46.10 21.12 82.88 3.92 50.92 37.27 43.04 70.44 BiLSTM-CNN + Topic transfer \ufb01ne 2 51.96 40.18 84.26 15.58 51.06 46.07 48.44 72.79 + User cluster \ufb01ne 3 53.25 39.46 84.85 29.63 56.85 49.13 52.71 73.67 cal right-wing worldview and can be considered as abusive against democratic values, their language is not necessarily offensive per se. This more qual- itative look into the data opens up some directions to improve offensive language detection incorporat- ing technologies that are able to capture such more subtle insults as well as handling cases of questions and harsh but still not insulting critique. 7 Conclusion In this paper, we presented our neural network text classi\ufb01cation approach for offensive language de- tection on the GermEval 2018 Shared Task dataset. We used a combination of BiLSTM and CNN archi- tectures for learning.",
            "7 Conclusion In this paper, we presented our neural network text classi\ufb01cation approach for offensive language de- tection on the GermEval 2018 Shared Task dataset. We used a combination of BiLSTM and CNN archi- tectures for learning. As task-speci\ufb01c adaptations of standard text classi\ufb01cation, we evaluated dif- ferent datasets and strategies for transfer learning, as well as additional features obtained from users addressed in tweets. The coarse-grained offensive language detection could be realized to a much better extent than the \ufb01ne-grained task of separat- ing four different categories of insults (accuracy 77.5% vs. 73.7%). From our experiments, four main messages can be drawn: 1. Transfer learning of neural networks architec- tures can improve offensive language detec- tion drastically. 2. Transfer learning should be conducted on as much data as possible regarding availability and computational resources. We obtained best results in a completely unsupervised and task-agnostic pre-training setup on in-domain data.",
            "2. Transfer learning should be conducted on as much data as possible regarding availability and computational resources. We obtained best results in a completely unsupervised and task-agnostic pre-training setup on in-domain data. During pre-training, we predicted the primary topics of tweets obtained by an LDA process, which previously clustered our back- ground dataset of 10 million tweets into 1,000 topics. 3. To mitigate the effect of \u2018catastrophic forget- ting\u2019 in transfer learning, it is advised to train and optimize the different layers of the neu- ral network model separately. In our experi- ments on models pre-trained on large Twitter datasets, the bottom-up approach of training from the lowest to the top layer performed sig- ni\ufb01cantly better than all other tested strategies to freeze model weights during learning. 4. User mentions in tweets can contribute a lot of information to the classi\ufb01er since some ac- counts are much more likely to be targeted by offensive language than others. Clustering users thematically allows including informa- tion from users not seen during training.",
            "4. User mentions in tweets can contribute a lot of information to the classi\ufb01er since some ac- counts are much more likely to be targeted by offensive language than others. Clustering users thematically allows including informa- tion from users not seen during training. The fact that our unsupervised, task-agnostic pre- training by LDA topic transfer performed best sug- gests that this approach will also contribute ben- e\ufb01cially to other text classi\ufb01cation tasks such as sentiment analysis. Thus, in future work, we plan to evaluate our approach with regard to such other tasks. We also plan to evaluate more task-agnostic approaches for transfer learning, for instance em- ploying language modeling as a pre-training task.",
            "Acknowledgements: The paper was supported by BWFG Hamburg within the \u201cForum 4.0\u201d project as part of the ahoi.digital funding line, and by DAAD via a WISE stipend. References David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. Journal of Ma- chine Learning Research, 3:993\u20131022. Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the Associa- tion for Computational Linguistics, 5:135\u2013146. Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang- Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classi\ufb01cation. Journal of Machine Learning Research, 9:1871\u20131874. Bjarke Felbo, Alan Mislove, Anders S\u00f8gaard, Iyad Rahwan, and Sune Lehmann. 2017.",
            "LIBLINEAR: A library for large linear classi\ufb01cation. Journal of Machine Learning Research, 9:1871\u20131874. Bjarke Felbo, Alan Mislove, Anders S\u00f8gaard, Iyad Rahwan, and Sune Lehmann. 2017. Using millions of emoji occurrences to learn any-domain represen- tations for detecting sentiment, emotion and sarcasm. In Proceedings of the 2017 Conference on Empiri- cal Methods in Natural Language Processing, pages 1615\u20131625, Copenhagen, Denmark. ACL. Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735\u20131780. Jeremy Howard and Sebastian Ruder. 2018. Universal language model \ufb01ne-tuning for text classi\ufb01cation. In Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), pages 328\u2013339, Melbourne, Australia. ACL. Yoon Kim. 2014.",
            "In Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), pages 328\u2013339, Melbourne, Australia. ACL. Yoon Kim. 2014. Convolutional neural networks for sentence classi\ufb01cation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1746\u20131751, Doha, Qatar. ACL. Philip Krei\u00dfel, Julia Ebner, Alexander Urban, and Jakob Guhl. 2018. Hass auf Knopfdruck: Recht- sextreme Trollfabriken und das \u00a8Okosystem koor- dinierter Hasskampagnen im Netz. Institute for Strategic Dialogue, London, UK. Ayush Kumar, Sarah Kohail, Amit Kumar, Asif Ekbal, and Chris Biemann. 2016. IIT-TUDA at SemEval- 2016 Task 5: Beyond sentiment lexicon: Combin- ing domain dependency and distributional seman- tics features for aspect based sentiment analysis.",
            "2016. IIT-TUDA at SemEval- 2016 Task 5: Beyond sentiment lexicon: Combin- ing domain dependency and distributional seman- tics features for aspect based sentiment analysis. In Proceedings of the 10th International Workshop on Semantic Evaluation, pages 1129\u20131135, San Diego, CA, USA. ACL. Marco Lui and Timothy Baldwin. 2012. langid.py: An off-the-shelf language identi\ufb01cation tool. In Pro- ceedings of the 50th Annual Meeting of the Associ- ation for Computational Linguistics, Demo Session, pages 25\u201330, Jeju, Korea. ACL. Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. 2013. Recti\ufb01er nonlinearities improve neural net- work acoustic models. In ICML Workshop on Deep Learning for Audio, Speech, and Language Process- ing. Atlanta, GA, USA. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Ef\ufb01cient estimation of word repre- sentations in vector space.",
            "Atlanta, GA, USA. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Ef\ufb01cient estimation of word repre- sentations in vector space. In Workshop at Inter- national Conference on Learning Representations (ICLR), pages 1310\u20131318, Scottsdale, AZ, USA. Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin. 2018. Ad- vances in Pre-Training Distributed Word Represen- tations. In Proceedings of the 11th International Conference on Language Resources and Evaluation, Miyazaki, Japan. ELRA. Chikashi Nobata, Joel Tetreault, Achint Thomas, Yashar Mehdad, and Yi Chang. 2016. Abusive lan- guage detection in online user content. In Proceed- ings of the 25th International Conference on World Wide Web, pages 145\u2013153, Montreal, Canada. In- ternational World Wide Web Conferences Steering Committee.",
            "2016. Abusive lan- guage detection in online user content. In Proceed- ings of the 25th International Conference on World Wide Web, pages 145\u2013153, Montreal, Canada. In- ternational World Wide Web Conferences Steering Committee. Bj\u00a8orn Ross, Michael Rist, Guillermo Carbonell, Ben Cabrera, Nils Kurowsky, and Michael Wojatzki. 2016. Measuring the reliability of hate speech anno- tations: The case of the European refugee crisis. In Proceedings of 3rd Workshop on Natural Language Processing for Computer-Mediated Communication, pages 6\u20139, Bochum, Germany. Eugen Ruppert, Abhishek Kumar, and Chris Biemann. 2017. LT-ABSA: An extensible open-source system for document-level and aspect-based sentiment anal- ysis. In Proceedings of the GSCL GermEval Shared Task on Aspect-based Sentiment in Social Media Customer Feedback, pages 55\u201360, Berlin, Germany. Dietmar Schabus, Marcin Skowron, and Martin Trapp. 2017.",
            "In Proceedings of the GSCL GermEval Shared Task on Aspect-based Sentiment in Social Media Customer Feedback, pages 55\u201360, Berlin, Germany. Dietmar Schabus, Marcin Skowron, and Martin Trapp. 2017. One million posts: A data set of German on- line discussions. In Proceedings of the 40th Inter- national Conference on Research and Development in Information Retrieval, pages 1241\u20131244, Tokyo, Japan. Anna Schmidt and Michael Wiegand. 2017. A sur- vey on hate speech detection using natural language processing. In Proceedings of the 5th International Workshop on Natural Language Processing for So- cial Media, pages 1\u201310, Valencia, Spain. ACL. Ellen Spertus. 1997. Smokey: Automatic recogni- tion of hostile messages. In Proceedings of the 14th National Conference on Arti\ufb01cial Intelligence and",
            "Ninth Conference on Innovative Applications of Ar- ti\ufb01cial Intelligence, pages 1058\u20131065, Providence, RI, USA. AAAI Press. Guang Xiang, Bin Fan, Ling Wang, Jason Hong, and Carolyn Rose. 2012. Detecting offensive tweets via topical feature discovery over a large scale twit- ter corpus. In Proceedings of the 21st ACM In- ternational Conference on Information and Knowl- edge Management, pages 1980\u20131984, New York, NY, USA. ACM."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1811.02906.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 9662.000061035156,
    "avg_doclen_est": 185.8076934814453
}
