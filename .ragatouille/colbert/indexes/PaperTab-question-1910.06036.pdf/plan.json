{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Improving Question Generation With to the Point Context Jingjing Li1\u2217 Yifan Gao1\u2217 Lidong Bing2 Irwin King1 Michael R. Lyu1 1 Department of Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong 2 R&D Center Singapore, Machine Intelligence Technology Alibaba DAMO Academy 1{lijj, yfgao, king, lyu}@cse.cuhk.edu.hk 2l.bing@alibaba-inc.com Abstract Question generation (QG) is the task of gen- erating a question from a reference sen- tence and a speci\ufb01ed answer within the sen- tence. A major challenge in QG is to iden- tify answer-relevant context words to \ufb01nish the declarative-to-interrogative sentence transfor- mation. Existing sequence-to-sequence neural models achieve this goal by proximity-based answer position encoding under the intuition that neighboring words of answers are of high possibility to be answer-relevant. However, such intuition may not apply to all cases es- pecially for sentences with complex answer- relevant relations.",
            "Existing sequence-to-sequence neural models achieve this goal by proximity-based answer position encoding under the intuition that neighboring words of answers are of high possibility to be answer-relevant. However, such intuition may not apply to all cases es- pecially for sentences with complex answer- relevant relations. Consequently, the perfor- mance of these models drops sharply when the relative distance between the answer frag- ment and other non-stop sentence words that also appear in the ground truth question in- creases. To address this issue, we propose a method to jointly model the unstructured sen- tence and the structured answer-relevant rela- tion (extracted from the sentence in advance) for question generation. Speci\ufb01cally, the struc- tured answer-relevant relation acts as the to the point context and it thus naturally helps keep the generated question to the point, while the unstructured sentence provides the full infor- mation. Extensive experiments show that to the point context helps our question genera- tion model achieve signi\ufb01cant improvements on several automatic evaluation metrics. Fur- thermore, our model is capable of generating diverse questions for a sentence which conveys multiple relations of its answer fragment.",
            "Extensive experiments show that to the point context helps our question genera- tion model achieve signi\ufb01cant improvements on several automatic evaluation metrics. Fur- thermore, our model is capable of generating diverse questions for a sentence which conveys multiple relations of its answer fragment. 1 Introduction Question Generation (QG) is the task of automat- ically creating questions from a range of inputs, such as natural language text (Heilman and Smith, 2010), knowledge base (Serban et al., 2016) and \u2217These two authors contributed equally. Sentence: The daily mean temperature in January, the area\u2019s coldest month, is 32.6 \u25e6F (0.3 \u25e6C); however, temperatures usually drop to 10 \u25e6F (-12 \u25e6C) several times per winter and reach 50 \u25e6F (10 \u25e6C) several days each winter month. Reference Question: What is New York City \u2019s daily January mean temperature in degrees celsius ? Baseline Prediction: What is the coldest temperature in Celsius ?",
            "Reference Question: What is New York City \u2019s daily January mean temperature in degrees celsius ? Baseline Prediction: What is the coldest temperature in Celsius ? Structured Answer-relevant Relation: (The daily mean temperature in January; is; 32.6 \u25e6F (0.3 \u25e6C)) Figure 1: An example SQuAD question with the base- line\u2019s prediction. The answer (\u201c0.3 \u25e6C\u201d) is highlighted. image (Mostafazadeh et al., 2016). QG is an in- creasingly important area in NLP with various ap- plication scenarios such as intelligence tutor sys- tems, open-domain chatbots and question answer- ing dataset construction. In this paper, we focus on question generation from reading comprehension materials like SQuAD (Rajpurkar et al., 2016). As shown in Figure 1, given a sentence in the reading comprehension paragraph and the text fragment (i.e., the answer) that we want to ask about, we aim to generate a question that is asked about the speci\ufb01ed answer.",
            "As shown in Figure 1, given a sentence in the reading comprehension paragraph and the text fragment (i.e., the answer) that we want to ask about, we aim to generate a question that is asked about the speci\ufb01ed answer. Question generation for reading comprehen- sion is \ufb01rstly formalized as a declarative-to- interrogative sentence transformation problem with prede\ufb01ned rules or templates (Mitkov and Ha, 2003; Heilman and Smith, 2010). With the rise of neural models, Du et al. (2017) propose to model this task under the sequence-to-sequence (Seq2Seq) learning framework (Sutskever et al., 2014) with attention mechanism (Luong et al., 2015). However, question generation is a one- to-many sequence generation problem, i.e., sev- eral aspects can be asked given a sentence. Zhou et al.",
            "However, question generation is a one- to-many sequence generation problem, i.e., sev- eral aspects can be asked given a sentence. Zhou et al. (2017) propose the answer-aware question generation setting which assumes the answer, a contiguous span inside the input sentence, is al- arXiv:1910.06036v2  [cs.CL]  24 Oct 2019",
            "Distance B1 B2 B3 B4 MET R-L 0\u223c10 (72.8% of #) 45.25 30.31 22.06 16.54 21.54 46.26 >10 (27.2% of #) 35.67 21.72 14.82 10.46 16.72 37.63 Table 1: Performance for the average relative distance between the answer fragment and other non-stop sentence words that also appear in the ground truth question. (Bn: BLEU-n, MET: METEOR, R-L: ROUGE-L) ready known before question generation. To cap- ture answer-relevant words in the sentence, they adopt a BIO tagging scheme to incorporate the answer position embedding in Seq2Seq learning. Furthermore, Sun et al. (2018) propose that tokens close to the answer fragments are more likely to be answer-relevant. Therefore, they explicitly encode the relative distance between sentence words and the answer via position embedding and position- aware attention.",
            "Furthermore, Sun et al. (2018) propose that tokens close to the answer fragments are more likely to be answer-relevant. Therefore, they explicitly encode the relative distance between sentence words and the answer via position embedding and position- aware attention. Although existing proximity-based answer- aware approaches achieve reasonable perfor- mance, we argue that such intuition may not apply to all cases especially for sentences with complex structure. For example, Figure 1 shows such an example where those approaches fail. This sen- tence contains a few facts and due to the paren- thesis (i.e. \u201cthe area\u2019s coldest month\u201d), some facts intertwine: \u201cThe daily mean temperature in Jan- uary is 0.3\u25e6C\u201d and \u201cJanuary is the area\u2019s cold- est month\u201d. From the question generated by a proximity-based answer-aware baseline, we \ufb01nd that it wrongly uses the word \u201ccoldest\u201d but misses the correct word \u201cmean\u201d because \u201ccoldest\u201d has a shorter distance to the answer \u201c0.3\u25e6C\u201d.",
            "From the question generated by a proximity-based answer-aware baseline, we \ufb01nd that it wrongly uses the word \u201ccoldest\u201d but misses the correct word \u201cmean\u201d because \u201ccoldest\u201d has a shorter distance to the answer \u201c0.3\u25e6C\u201d. In summary, their intuition that \u201cthe neighbor- ing words of the answer are more likely to be answer-relevant and have a higher chance to be used in the question\u201d is not reliable. To quanti- tatively show this drawback of these models, we implement the approach proposed by Sun et al. (2018) and analyze its performance under different relative distances between the answer and other non-stop sentence words that also appear in the ground truth question. The results are shown in Table 1. We \ufb01nd that the performance drops at most 36% when the relative distance increases from \u201c0 \u223c10\u201d to \u201c> 10\u201d. In other words, when the useful context is located far away from the an- swer, current proximity-based answer-aware ap- proaches will become less effective, since they overly emphasize neighboring words of the an- swer.",
            "In other words, when the useful context is located far away from the an- swer, current proximity-based answer-aware ap- proaches will become less effective, since they overly emphasize neighboring words of the an- swer. To address this issue, we extract the structured answer-relevant relations from sentences and pro- pose a method to jointly model such structured relation and the unstructured sentence for ques- tion generation. The structured answer-relevant relation is likely to be to the point context and thus can help keep the generated question to the point. For example, Figure 1 shows our frame- work can extract the right answer-relevant relation (\u201cThe daily mean temperature in January\u201d, \u201cis\u201d, \u201c32.6\u25e6F (0.3\u25e6C)\u201d) among multiple facts. With the help of such structured information, our model is less likely to be confused by sentences with a complex structure. Speci\ufb01cally, we \ufb01rstly extract multiple relations with an off-the-shelf Open In- formation Extraction (OpenIE) toolbox (Saha and Mausam, 2018), then we select the relation that is most relevant to the answer with carefully de- signed heuristic rules.",
            "Speci\ufb01cally, we \ufb01rstly extract multiple relations with an off-the-shelf Open In- formation Extraction (OpenIE) toolbox (Saha and Mausam, 2018), then we select the relation that is most relevant to the answer with carefully de- signed heuristic rules. Nevertheless, it is challenging to train a model to effectively utilize both the unstructured sen- tence and the structured answer-relevant relation because both of them could be noisy: the unstruc- tured sentence may contain multiple facts which are irrelevant to the target question, while the lim- itation of the OpenIE tool may produce less accu- rate extracted relations. To explore their advan- tages simultaneously and avoid the drawbacks, we design a gated attention mechanism and a dual copy mechanism based on the encoder-decoder framework, where the former learns to control the information \ufb02ow between the unstructured and structured inputs, while the latter learns to copy words from two sources to maintain the informa- tiveness and faithfulness of generated questions. In the evaluations on the SQuAD dataset, our system achieves signi\ufb01cant and consistent im- provement as compared to all baseline methods.",
            "In the evaluations on the SQuAD dataset, our system achieves signi\ufb01cant and consistent im- provement as compared to all baseline methods. In particular, we demonstrate that the improve- ment is more signi\ufb01cant with a larger relative dis- tance between the answer and other non-stop sen- tence words that also appear in the ground truth question. Furthermore, our model is capable of generating diverse questions for a single sentence- answer pair where the sentence conveys multiple relations of its answer fragment.",
            "Sentence: Beyonc\u00b4e received critical acclaim and commer- cial success, selling one million digital copies worldwide in six days; The New York Times noted the album\u2019s un- conventional, unexpected release as signi\ufb01cant. N-ary Relations: 0.85 ::::::: (Beyonc\u00b4e;::::::: received ::::::::: commercial :::::: success ::::: selling;:::: one ::::: million::::: digital::::: copies::::::::: worldwide;:: in :: six::::: days) 0.92 (The New York Times; noted; the album\u2019s unconven- tional, unexpected release as signi\ufb01cant) Sentence: The daily mean temperature in January, the area\u2019s coldest month, is 32.6 \u25e6F (0.3 \u25e6C); however, tem- peratures usually drop to 10 \u25e6F (-12 \u25e6C) several times per winter and reach 50 \u25e6F (10 \u25e6C) several days each winter month.",
            "N-ary Relations: 0.95 ::: (The:::: daily::::: mean ::::::::: temperature :: in :::::: January;::: is; ::: 32.6::: \u25e6F ::: (0.3:::: \u25e6C)) 0.94 (temperatures; drop; to 10 \u25e6F (12 \u25e6C); several times per winter; usually) 0.90 (temperatures; reach; 50 \u25e6F) Figure 2: Examples for n-ary extractions from sen- tences using OpenIE. Con\ufb01dence scores are shown at the beginning of each relation. Answers are highlighted in sentences. Waved relations are selected according to our criteria in Section 2.2. 2 Framework Description In this section, we \ufb01rst introduce the task de\ufb01ni- tion and our protocol to extract structured answer- relevant relations. Then we formalize the task un- der the encoder-decoder framework with gated at- tention and dual copy mechanism.",
            "2 Framework Description In this section, we \ufb01rst introduce the task de\ufb01ni- tion and our protocol to extract structured answer- relevant relations. Then we formalize the task un- der the encoder-decoder framework with gated at- tention and dual copy mechanism. 2.1 Problem De\ufb01nition We formalize our task as an answer-aware Ques- tion Generation (QG) problem (Zhao et al., 2018), which assumes answer phrases are given before generating questions. Moreover, answer phrases are shown as text fragments in passages. For- mally, given the sentence S, the answer A, and the answer-relevant relation M, the task of QG aims to \ufb01nd the best question Q such that, Q = arg max Q Prob(Q|S, A, M), (1) where A is a contiguous span inside S. 2.2 Answer-relevant Relation Extraction We utilize an off-the-shelf toolbox of OpenIE 1 to the derive structured answer-relevant relations from sentences as to the point contexts.",
            "2.2 Answer-relevant Relation Extraction We utilize an off-the-shelf toolbox of OpenIE 1 to the derive structured answer-relevant relations from sentences as to the point contexts. Relations extracted by OpenIE can be represented either in a triple format or in an n-ary format with several secondary arguments, and we employ the latter to 1http:\/\/openie.allenai.org\/ Sentence Answer-relevant Relation Avg. length 32.46 13.04 # overlapped words 2.87 1.86 Copy ratio 8.85% 14.26% Table 2: Comparisons between sentences and answer- relevant relations. Overlapped words are those non-stop tokens co-occurring in the source (sen- tence\/relation) and the target question. Copy ratio means the proportion of source tokens that are used in the question. keep the extractions as informative as possible and avoid extracting too many similar relations in dif- ferent granularities from one sentence. We join all arguments in the extracted n-ary relation into a sequence as our to the point context. Figure 2 shows n-ary relations extracted from OpenIE.",
            "keep the extractions as informative as possible and avoid extracting too many similar relations in dif- ferent granularities from one sentence. We join all arguments in the extracted n-ary relation into a sequence as our to the point context. Figure 2 shows n-ary relations extracted from OpenIE. As we can see, OpenIE extracts multiple relations for complex sentences. Here we select the most infor- mative relation according to three criteria in the order of descending importance: (1) having the maximal number of overlapped tokens between the answer and the relation; (2) being assigned the highest con\ufb01dence score by OpenIE; (3) contain- ing maximum non-stop words. As shown in Fig- ure 2, our criteria can select answer-relevant rela- tions (waved in Figure 2), which is especially use- ful for sentences with extraneous information. In rare cases, OpenIE cannot extract any relation, we treat the sentence itself as the to the point context. Table 2 shows some statistics to verify the intu- ition that the extracted relations can serve as more to the point context.",
            "In rare cases, OpenIE cannot extract any relation, we treat the sentence itself as the to the point context. Table 2 shows some statistics to verify the intu- ition that the extracted relations can serve as more to the point context. We \ufb01nd that the tokens in relations are 61% more likely to be used in the target question than the tokens in sentences, and thus they are more to the point. On the other hand, on average the sentences contain one more question token than the relations (1.86 v.s. 2.87). Therefore, it is still necessary to take the original sentence into account to generate a more accurate question. 2.3 Our Proposed Model Overview. As shown in Figure 3, our framework consists offour components (1) Sentence Encoder and Relation Encoder, (2) Decoder, (3) Gated At- tention Mechanism and (4) Dual Copy Mecha- nism. The sentence encoder and relation encoder encode the unstructured sentence and the struc- tured answer-relevant relation, respectively. To select and combine the source information from the two encoders, a gated attention mechanism is",
            "Sentence Encoder !\" !# \u2026 $\" $# \u2026 %\" & %# & \u2026 \u2026 Word Emb POS Emb NER Emb Ans. Emb Relation Encoder \u2026 \u2026 '( ) Decoder with Gated Attention & Dual Copy \u2026 P+ !, $, %, & %, # %\" # %## !\" !- !, $\" $- $, \u2026 \u2026 \u2026 '( - \u2026 Vocabulary Distribution 1 \u221201 2 01 201 3 01 2(1 \u221201 3) Sentence Attention Concrete  Context Attention 61 71 ) 71 - 71 891 P: P; 01 2 01 3 Final Distribution : Gate Dual Copy Mechanism Gated Attention \u27e8 \u27e9 >?@ A, B1 Figure 3: The framework of our proposed model. (Best viewed in color) employed to jointly attend both contextualized in- formation sources, and a dual copy mechanism copies words from either the sentence or the re- lation. Answer-aware Encoder. We employ two en- coders to integrate information from the unstruc- tured sentence S and the answer-relevant relation M separately.",
            "Answer-aware Encoder. We employ two en- coders to integrate information from the unstruc- tured sentence S and the answer-relevant relation M separately. Sentence encoder takes in feature- enriched embeddings including word embeddings w, linguistic embeddings l and answer position embeddings a. We follow (Zhou et al., 2017) to transform POS and NER tags into continuous rep- resentation (lp and ln) and adopt a BIO labelling scheme to derive the answer position embedding (B: the \ufb01rst token of the answer, I: tokens within the answer fragment except the \ufb01rst one, O: tokens outside of the answer fragment). For each word wi in the sentence S, we simply concatenate all fea- tures as input: xs i = [wi; lp i ; ln i ; ai].",
            "For each word wi in the sentence S, we simply concatenate all fea- tures as input: xs i = [wi; lp i ; ln i ; ai]. Here [a; b] denotes the concatenation of vectors a and b. We use bidirectional LSTMs to encode the sen- tence (xs 1, xs 2, ..., xs n) to get a contextualized rep- resentation for each token: \u2212\u2192 h s i = \u2212\u2212\u2212\u2212\u2192 LSTM(\u2212\u2192 h s i\u22121, xs i), \u2190\u2212 h s i = \u2190\u2212\u2212\u2212\u2212 LSTM(\u2190\u2212 h s i+1, xs i), where \u2212\u2192 h s i and \u2190\u2212 h s i are the hidden states at the i-th time step of the forward and the backward LSTMs. The output state of the sentence encoder is the con- catenation of forward and backward hidden states: hs i = [\u2212\u2192 h s i; \u2190\u2212 h s i]. The contextualized representa- tion of the sentence is (hs 1, hs 2, ..., hs n).",
            "The output state of the sentence encoder is the con- catenation of forward and backward hidden states: hs i = [\u2212\u2192 h s i; \u2190\u2212 h s i]. The contextualized representa- tion of the sentence is (hs 1, hs 2, ..., hs n). For the relation encoder, we \ufb01rstly join all items in the n-ary relation M into a sequence. Then we only take answer position embedding as an extra feature for the sequence: xm i = [wi; ai]. Similarly, we take another bidirectional LSTMs to encode the relation sequence and de- rive the corresponding contextualized representa- tion (hm 1 , hm 2 , ..., hm n ). Decoder. We use an LSTM as the decoder to generate the question. The decoder predicts the word probability distribution at each decoding timestep to generate the question. At the t-th timestep, it reads the word embedding wt and the hidden state ut\u22121 of the previous timestep to gen-",
            "erate the current hidden state: ut = LSTM(ut\u22121, wt). (2) Gated Attention Mechanism. We design a gated attention mechanism to jointly attend the sentence representation and the relation representation. For sentence representation (hs 1, hs 2, ..., hs n), we employ the Luong et al. (2015)\u2019s attention mechanism to obtain the sen- tence context vector cs t, as t,i = exp(u\u22a4 t Wahs i) P j exp(u\u22a4 t Wahs j), cs t = X i as t,ihs i, where Wa is a trainable weight. Similarly, we ob- tain the vector cm t from the relation representation (hm 1 , hm 2 , ..., hm n ).",
            "Similarly, we ob- tain the vector cm t from the relation representation (hm 1 , hm 2 , ..., hm n ). To jointly model the sentence and the relation, a gating mechanism is designed to control the information \ufb02ow from two sources: gt = sigmoid(Wg[cs t; cm t ]), (3) ct = gt \u2299cs t + (1 \u2212gt) \u2299cm t , (4) \u02dcht = tanh(Wh[ut; ct]), (5) where \u2299represents element-wise dot production and Wg, Wh are trainable weights. Finally, the predicted probability distribution over the vocabu- lary V is computed as: PV = softmax(WV \u02dcht + bV ), (6) where WV and bV are parameters. Dual Copy Mechanism. To deal with the rare and unknown words, the decoder applies the point- ing method (See et al., 2017; Gu et al., 2016; Gul- cehre et al., 2016) to allow copying a token from the input sentence at the t-th decoding step.",
            "To deal with the rare and unknown words, the decoder applies the point- ing method (See et al., 2017; Gu et al., 2016; Gul- cehre et al., 2016) to allow copying a token from the input sentence at the t-th decoding step. We reuse the attention score \u03b1s t and \u03b1m t to derive the copy probability over two source inputs: PS(w) = X i:wi=w \u03b1s t,i, PM(w) = X i:wi=w \u03b1m t,i. Different from the standard pointing method, we design a dual copy mechanism to copy from two sources with two gates. The \ufb01rst gate is designed for determining copy tokens from two sources of inputs or generate next word from PV , which is computed as gv t = sigmoid(wv g \u02dcht + bv g). The sec- ond gate takes charge of selecting the source (sen- tence or relation) to copy from, which is computed as gc t = sigmoid(wc g[cs t; cm t ] + bc g).",
            "The sec- ond gate takes charge of selecting the source (sen- tence or relation) to copy from, which is computed as gc t = sigmoid(wc g[cs t; cm t ] + bc g). Finally, we Du Split Zhou Split # pairs (Train) 74689 86635 # pairs (Dev) 10427 8965 # pairs (Test) 11609 8964 Sentence avg. tokens 32.56 32.72 Question avg. tokens 11.42 11.31 Table 3: Dataset statistics on Du Split (Du et al., 2017) and Zhou Split (Zhou et al., 2017). combine all probabilities PV , PS and PM through two soft gates gv t and gc t. The probability of pre- dicting w as the t-th token of the question is: P(w) = (1 \u2212gv t )PV (w) (7) + gv t gc tPS(w) + gv t (1 \u2212gc t)PM(w). Training and Inference.",
            "Training and Inference. Given the answer A, sentence S and relation M, the training objective is to minimize the negative log-likelihood with re- gard to all parameters: L = \u2212 X Q\u2208{Q} log P(Q|A, S, M; \u03b8), (8) where {Q} is the set of all training instances, \u03b8 de- notes model parameters and logP(Q|A, S, M; \u03b8) is the conditional log-likelihood of Q. In testing, our model targets to generate a ques- tion Q by maximizing: Q = arg max Q log P(Q|A, S, M; \u03b8). (9) 3 Experimental Setting 3.1 Dataset & Metrics We conduct experiments on the SQuAD dataset (Rajpurkar et al., 2016). It contains 536 Wikipedia articles and 100k crowd-sourced question-answer pairs. The questions are written by crowd-workers and the answers are spans of tokens in the arti- cles. We employ two different data splits by fol- lowing Zhou et al. (2017) 2 and Du et al. (2017) 3.",
            "The questions are written by crowd-workers and the answers are spans of tokens in the arti- cles. We employ two different data splits by fol- lowing Zhou et al. (2017) 2 and Du et al. (2017) 3. In Zhou et al. (2017), the original SQuAD de- velopment set is evenly divided into dev and test sets, while Du et al. (2017) treats SQuAD devel- opment set as its development set and splits orig- inal SQuAD training set into a training set and a test set. We also \ufb01lter out questions which do not have any overlapped non-stop words with the cor- responding sentences and perform some prepro- cessing steps, such as tokenization and sentence splitting. The data statistics are given in Table 3. 2https:\/\/res.qyzhou.me\/redistribute.zip 3https:\/\/github.com\/xinyadu\/nqg\/tree\/master\/data\/raw",
            "Du Split (Du et al., 2017) Zhou Split (Zhou et al., 2017) B1 B2 B3 B4 MET R-L B1 B2 B3 B4 MET R-L s2s (Du et al., 2017) 43.09 25.96 17.50 12.28 16.62 39.75 - - - - - - NQG++ (Zhou et al., 2017) - - - - - - - - - 13.29 - - M2S+cp (Song et al., 2018) - - - 13.98 18.77 42.72 - - - 13.91 - - s2s+MP+GSA (Zhao et al., 2018) 43.47 28.23 20.40 15.32 19.29 43.91 44.51 29.07 21.06 15.82 19.67 44.24 Hybrid model (Sun et al., 2018) - - - - - - 43.02 28.14 20.",
            "32 19.29 43.91 44.51 29.07 21.06 15.82 19.67 44.24 Hybrid model (Sun et al., 2018) - - - - - - 43.02 28.14 20.51 15.64 - - ASs2s (Kim et al., 2019) - - - 16.20 19.92 43.96 - - - 16.17 - - Our model 45.66 30.21 21.82 16.27 20.36 44.35 44.40 29.48 21.54 16.37 20.68 44.73 Table 4: The main experimental results for our model and several baselines. \u2018-\u2019 means no results reported in their papers.",
            "27 20.36 44.35 44.40 29.48 21.54 16.37 20.68 44.73 Table 4: The main experimental results for our model and several baselines. \u2018-\u2019 means no results reported in their papers. (Bn: BLEU-n, MET: METEOR, R-L: ROUGE-L) We evaluate with all commonly-used metrics in question generation (Du et al., 2017): BLEU- 1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4) (Papineni et al., 2002), METEOR (MET) (Denkowski and Lavie, 2014) and ROUGE-L (R- L) (Lin, 2004). We use the evaluation script re- leased by Chen et al. (2015). 3.2 Baseline Models We compare with the following models. \u2022 s2s (Du et al., 2017) proposes an attention-based sequence-to-sequence neural network for ques- tion generation.",
            "We use the evaluation script re- leased by Chen et al. (2015). 3.2 Baseline Models We compare with the following models. \u2022 s2s (Du et al., 2017) proposes an attention-based sequence-to-sequence neural network for ques- tion generation. \u2022 NQG++ (Zhou et al., 2017) takes the answer po- sition feature and linguistic features into con- sideration and equips the Seq2Seq model with copy mechanism. \u2022 M2S+cp (Song et al., 2018) conducts multi- perspective matching between the answer and the sentence to derive an answer-aware sentence representation for question generation. \u2022 s2s+MP+GSA (Zhao et al., 2018) introduces a gated self-attention into the encoder and a max- out pointer mechanism into the decoder. We re- port their sentence-level results for a fair com- parison. \u2022 Hybrid (Sun et al., 2018) is a hybrid model which considers the answer embedding for the question word generation and the position of context words for modeling the relative distance between the context words and the answer.",
            "We re- port their sentence-level results for a fair com- parison. \u2022 Hybrid (Sun et al., 2018) is a hybrid model which considers the answer embedding for the question word generation and the position of context words for modeling the relative distance between the context words and the answer. \u2022 ASs2s (Kim et al., 2019) replaces the answer in the sentence with a special token to avoid its appearance in the generated questions. 3.3 Implementation Details We take the most frequent 20k words as our vocab- ulary and use the GloVe word embeddings (Pen- nington et al., 2014) for initialization. The embed- ding dimensions for POS, NER, answer position are set to 20. We use two-layer LSTMs in both encoder and decoder, and the LSTMs hidden unit size is set to 600. We use dropout (Srivastava et al., 2014) with the probability p = 0.3. All trainable parameters, except word embeddings, are randomly initialized with the Xavier uniform in (\u22120.1, 0.1) (Glorot and Bengio, 2010).",
            "All trainable parameters, except word embeddings, are randomly initialized with the Xavier uniform in (\u22120.1, 0.1) (Glorot and Bengio, 2010). For optimization in the training, we use SGD as the optimizer with a minibatch size of 64 and an initial learning rate of 1.0. We train the model for 15 epochs and start halving the learning rate after the 8th epoch. We set the gradi- ent norm upper bound to 3 during the training. We adopt the teacher-forcing for the training. In the testing, we select the model with the low- est perplexity and beam search with size 3 is employed for generating questions. All hyper- parameters and models are selected on the valida- tion dataset. 4 Results and Analysis 4.1 Main Results Table 4 shows automatic evaluation results for our model and baselines (copied from their pa- pers).",
            "All hyper- parameters and models are selected on the valida- tion dataset. 4 Results and Analysis 4.1 Main Results Table 4 shows automatic evaluation results for our model and baselines (copied from their pa- pers). Our proposed model which combines structured answer-relevant relations and unstruc- tured sentences achieves signi\ufb01cant improvements over proximity-based answer-aware models (Zhou et al., 2017; Sun et al., 2018) on both dataset splits. Presumably, our structured answer-relevant relation is a generalization of the context ex- plored by the proximity-based methods because they can only capture short dependencies around answer fragments while our extractions can cap- ture both short and long dependencies given the answer fragments. Moreover, our proposed frame- work is a general one to jointly leverage structured",
            "(a) Evaluation results on all sentences. Hybrid Our Model BLEU MET R-L BLEU MET R-L 0\u223c10 (72.8% of #) 28.54 21.54 46.26 29.73 (4.17%) 22.03 (2.27%) 46.85 (1.28%) >10 (27.2% of #) 20.67 16.72 37.63 22.12 (7.01%) 17.46 (4.43%) 38.47 (2.23%) (b) Evaluation results on sentences with more than 20 words.",
            "Hybrid Our Model BLEU MET R-L BLEU MET R-L 0\u223c10 (58.3% of #) 28.00 21.03 45.37 29.11 (3.96%) 21.50 (2.21%) 45.97 (1.31%) >10 (26.6% of #) 20.58 16.66 37.53 22.04 (7.09%) 17.38 (4.30%) 38.37 (2.24%) Table 5: Performance for the average relative distance between the answer fragment and other non-stop sentence words that also appear in the ground truth question (BLEU is the average over BLEU-1 to BLEU-4). Values in parenthesis are the improvement percentage of Our Model over Hybrid. (a) is based on all sentences while (b) only considers long sentences with more than 20 words. relations and unstructured sentences. All com- pared baseline models which only consider un- structured sentences can be further enhanced un- der our framework.",
            "(a) is based on all sentences while (b) only considers long sentences with more than 20 words. relations and unstructured sentences. All com- pared baseline models which only consider un- structured sentences can be further enhanced un- der our framework. Recall that existing proximity-based answer- aware models perform poorly when the distance between the answer fragment and other non-stop sentence words that also appear in the ground truth question is large (Table 1). Here we investigate whether our proposed model using the structured answer-relevant relations can alleviate this issue or not, by conducting experiments for our model un- der the same setting as in Table 1. The broken- down performances by different relative distances are shown in Table 5a. We \ufb01nd that our proposed model outperforms Hybrid (our re-implemented version for this experiment) on all ranges of rel- ative distances, which shows that the structured answer-relevant relations can capture both short and long term answer-relevant dependencies of the answer in sentences. Furthermore, comparing the performance difference between Hybrid and our model, we \ufb01nd the improvements become more signi\ufb01cant when the distance increases from \u201c0 \u223c 10\u201d to \u201c> 10\u201d.",
            "Furthermore, comparing the performance difference between Hybrid and our model, we \ufb01nd the improvements become more signi\ufb01cant when the distance increases from \u201c0 \u223c 10\u201d to \u201c> 10\u201d. One reason is that our model can extract relations with distant dependencies to the answer, which greatly helps our model ignore the extraneous information. Proximity-based answer- aware models may overly emphasize the neighbor- ing words of answers and become less effective as the useful context becomes further away from the answer in the complex sentences. In fact, the breakdown intervals in Table 5a naturally bound its sentence length, say for \u201c> 10\u201d, the sentences in this group must be longer than 10. Thus, the length variances in these two intervals could be Sentence: Beyonc\u00b4e received critical acclaim and commer- cial success, selling one million digital copies worldwide in six days; The New York Times noted the album \u2019s un- conventional, unexpected release as signi\ufb01cant. Reference Question: How many digital copies of her \ufb01fth album did Beyonc sell in six days? Baseline Prediction: How many digital copies did the New York Times sell in six days ?",
            "Reference Question: How many digital copies of her \ufb01fth album did Beyonc sell in six days? Baseline Prediction: How many digital copies did the New York Times sell in six days ? Structured Answer-relevant Relation: (Beyonc\u00b4e; re- ceived commercial success selling; one million digital copies worldwide; in six days) Our Model Prediction: How many digital copies did Be- yonc\u00b4e sell in six days ? Sentence: The daily mean temperature in January, the area\u2019s coldest month, is 32.6 \u25e6F (0.3 \u25e6C); however, tem- peratures usually drop to 10 \u25e6F (-12 \u25e6C) several times per winter and reach 50 \u25e6F (10 \u25e6C) several days each winter month. Reference Question: What is New York City \u2019s daily Jan- uary mean temperature in degrees celsius ? Baseline Prediction: What is the coldest temperature in Celsius ? Structured Answer-relevant Relation: (The daily mean temperature in January; is; 32.6 \u25e6F (0.3 \u25e6C)) Our Model Prediction: In degrees Celsius , what is the average temperature in January ?",
            "Baseline Prediction: What is the coldest temperature in Celsius ? Structured Answer-relevant Relation: (The daily mean temperature in January; is; 32.6 \u25e6F (0.3 \u25e6C)) Our Model Prediction: In degrees Celsius , what is the average temperature in January ? Figure 4: Example questions (with answers high- lighted) generated by crowd-workers (ground truth questions), the baseline model and our model. signi\ufb01cant. To further validate whether our model can extract long term dependency words. We rerun the analysis of Table 5b only for long sentences (length > 20) of each interval. The improvement percentages over Hybrid are shown in Table 5b, which become more signi\ufb01cant when the distance increases from \u201c0 \u223c10\u201d to \u201c> 10\u201d. 4.2 Case Study Figure 4 provides example questions generated by crowd-workers (ground truth questions), the base-",
            "line Hybrid (Sun et al., 2018), and our model. In the \ufb01rst case, there are two subsequences in the input and the answer has no relation with the sec- ond subsequence4. However, we see that the base- line model prediction copies irrelevant words \u201cThe New York Times\u201d while our model can avoid us- ing the extraneous subsequence \u201cThe New York Times noted ...\u201d with the help of the struc- tured answer-relevant relation. Compared with the ground truth question, our model cannot cap- ture the cross-sentence information like \u201cher \ufb01fth album\u201d, where the techniques in paragraph-level QG models (Zhao et al., 2018) may help. In the second case, as discussed in Section 1, this sen- tence contains a few facts and some facts inter- twine. We \ufb01nd that our model can capture dis- tant answer-relevant dependencies such as \u201cmean temperature\u201d while the proximity-based baseline model wrongly takes neighboring words of the an- swer like \u201ccoldest\u201d in the generated question.",
            "We \ufb01nd that our model can capture dis- tant answer-relevant dependencies such as \u201cmean temperature\u201d while the proximity-based baseline model wrongly takes neighboring words of the an- swer like \u201ccoldest\u201d in the generated question. 4.3 Diverse Question Generation Another interesting observation is that for the same answer-sentence pair, our model can gener- ate diverse questions by taking different answer- relevant relations as input. Such capability im- proves the interpretability of our model because the model is given not only what to be asked (i.e., the answer) but also the related fact (i.e., the answer-relevant relation) to be covered in the question. In contrast, proximity-based answer- aware models can only generate one question given the sentence-answer pair regardless of how many answer-relevant relations in the sentence. We think such capability can also validate our mo- tivation: questions should be generated according to the answer-aware relations instead of neighbor- ing words of answer fragments. Figure 5 show two examples of diverse question generation.",
            "We think such capability can also validate our mo- tivation: questions should be generated according to the answer-aware relations instead of neighbor- ing words of answer fragments. Figure 5 show two examples of diverse question generation. In the \ufb01rst case, the answer fragment \u2018Hugh L. Dryden\u2019 is the appositive to \u2018NASA Deputy Administrator\u2019 but the subject to the following tokens \u2018announced the Apollo program ...\u2019. Our framework can ex- tract these two answer-relevant relations, and by feeding them to our model separately, we can re- ceive two questions asking different relations with regard to the answer. 4One might think that the two subsequences should be re- garded as individual sentences, however, several off-the-shelf tools do recognize them as one sentence. Sentence: In July 1960, NASA Deputy Administrator Hugh L. Dryden announced the Apollo program to indus- try representatives at a series of Space Task Group confer- ences. Relation 1: (Hugh L. Dryden; [is] Deputy Administrator [of]; NASA) Question 1: Who was the NASA Deputy Administrator in 1960 ?",
            "Relation 1: (Hugh L. Dryden; [is] Deputy Administrator [of]; NASA) Question 1: Who was the NASA Deputy Administrator in 1960 ? Relation 2: (NASA Deputy Administrator Hugh L. Dry- den; announced; the Apollo program to industry represen- tatives at a series of Space Task Group conferences; In July 1960) Question 2: Who announced the Apollo program to in- dustry representatives ? Sentence: One of the network\u2019s strike-replacement pro- grams during that time was the game show Duel, which premiered in December 2007. Relation 1: (the game show Duel; premiered; in Decem- ber 2007) Question 1: What game premiered in December 2007 ? Relation 2: (One of the network\u2019s strike-replacement pro- grams during that time; was; the game show Duel) Question 2: What was the name of an network \u2019s strike - replacement programs ? Figure 5: Example diverse questions (with answers highlighted) generated by our model with different answer-relevant relations.",
            "Figure 5: Example diverse questions (with answers highlighted) generated by our model with different answer-relevant relations. 5 Related Work The topic of question generation, initially moti- vated for educational purposes, is tackled by de- signing many complex rules for speci\ufb01c question types (Mitkov and Ha, 2003; Rus et al., 2010). Heilman and Smith (2010) improve rule-based question generation by introducing a statistical ranking model. First, they remove extraneous in- formation in the sentence to transform it into a simpler one, which can be transformed easily into a succinct question with prede\ufb01ned sets of general rules. Then they adopt an overgenerate-and-rank approach to select the best candidate considering several features. With the rise of dominant neural sequence-to- sequence learning models (Sutskever et al., 2014), Du et al. (2017) frame question generation as a sequence-to-sequence learning problem. Com- pared with rule-based approaches, neural mod- els (Yuan et al., 2017) can generate more \ufb02u- ent and grammatical questions.",
            "(2017) frame question generation as a sequence-to-sequence learning problem. Com- pared with rule-based approaches, neural mod- els (Yuan et al., 2017) can generate more \ufb02u- ent and grammatical questions. However, ques- tion generation is a one-to-many sequence gener- ation problem, i.e., several aspects can be asked given a sentence, which confuses the model dur- ing train and prevents concrete automatic evalu- ation. To tackle this issue, Zhou et al. (2017) propose the answer-aware question generation set- ting which assumes the answer is already known",
            "and acts as a contiguous span inside the input sentence. They adopt a BIO tagging scheme to incorporate the answer position information as learned embedding features in Seq2Seq learning. Song et al. (2018) explicitly model the informa- tion between answer and sentence with a multi- perspective matching model. Kim et al. (2019) also focus on the answer information and proposed an answer-separated Seq2Seq model by masking the answer with special tokens. All answer-aware neural models treat question generation as a one- to-one mapping problem, but existing models per- form poorly for sentences with a complex struc- ture (as shown in Table 1). Our work is inspired by the process of extrane- ous information removing in (Heilman and Smith, 2010; Cao et al., 2018). Different from Heilman and Smith (2010) which directly use the simpli- \ufb01ed sentence for generation and Cao et al. (2018) which only consider aggregate two sources of in- formation via gated attention in summarization, we propose to combine the structured answer- relevant relation and the original sentence.",
            "(2018) which only consider aggregate two sources of in- formation via gated attention in summarization, we propose to combine the structured answer- relevant relation and the original sentence. Fac- toid question generation from structured text is initially investigated by Serban et al. (2016), but our focus here is leveraging structured inputs to help question generation over unstructured sen- tences. Our proposed model can take advantage of unstructured sentences and structured answer- relevant relations to maintain informativeness and faithfulness of generated questions. The proposed model can also be generalized in other conditional sequence generation tasks which require multiple sources of inputs, e.g., distractor generation for multiple choice questions (Gao et al., 2019b). 6 Conclusions and Future Work In this paper, we propose a question generation system which combines unstructured sentences and structured answer-relevant relations for gener- ation. The unstructured sentences maintain the in- formativeness of generated questions while struc- tured answer-relevant relations keep the faithful- ness of questions. Extensive experiments demon- strate that our proposed model achieves state-of- the-art performance across several metrics.",
            "The unstructured sentences maintain the in- formativeness of generated questions while struc- tured answer-relevant relations keep the faithful- ness of questions. Extensive experiments demon- strate that our proposed model achieves state-of- the-art performance across several metrics. Fur- thermore, our model can generate diverse ques- tions with different structured answer-relevant re- lations. For future work, there are some interest- ing dimensions to explore, such as dif\ufb01culty lev- els (Gao et al., 2019a), paragraph-level informa- tion (Zhao et al., 2018) and conversational ques- tion generation (Gao et al., 2019c). Acknowledgments This work is supported by the Research Grants Council of the Hong Kong Special Administrative Region, China (No. CUHK 14208815 and No. CUHK 14210717 of the General Research Fund). We would like to thank the anonymous reviewers for their comments. We would also like to thank Department of Computer Science and Engineer- ing, The Chinese University of Hong Kong for the conference grant support.",
            "CUHK 14208815 and No. CUHK 14210717 of the General Research Fund). We would like to thank the anonymous reviewers for their comments. We would also like to thank Department of Computer Science and Engineer- ing, The Chinese University of Hong Kong for the conference grant support. References Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018. Faithful to the original: Fact aware neural abstrac- tive summarization. In Thirty-Second AAAI Confer- ence on Arti\ufb01cial Intelligence. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr- ishna Vedantam, Saurabh Gupta, Piotr Doll\u00b4ar, and C. Lawrence Zitnick. 2015. Microsoft coco cap- tions: Data collection and evaluation server. CoRR, abs\/1504.00325. Michael Denkowski and Alon Lavie. 2014. Meteor universal: Language speci\ufb01c translation evaluation for any target language.",
            "2015. Microsoft coco cap- tions: Data collection and evaluation server. CoRR, abs\/1504.00325. Michael Denkowski and Alon Lavie. 2014. Meteor universal: Language speci\ufb01c translation evaluation for any target language. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 376\u2013380, Baltimore, Maryland, USA. Association for Computational Linguistics. Xinya Du, Junru Shao, and Claire Cardie. 2017. Learn- ing to ask: Neural question generation for reading comprehension. In Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1342\u2013 1352, Vancouver, Canada. Association for Compu- tational Linguistics. Yifan Gao, Lidong Bing, Wang Chen, Michael R. Lyu, and Irwin King. 2019a. Dif\ufb01culty controllable gen- eration of reading comprehension questions. In Pro- ceedings of the Twenty-Eightth International Joint Conference on Arti\ufb01cial Intelligence, IJCAI-19.",
            "2019a. Dif\ufb01culty controllable gen- eration of reading comprehension questions. In Pro- ceedings of the Twenty-Eightth International Joint Conference on Arti\ufb01cial Intelligence, IJCAI-19. In- ternational Joint Conferences on Arti\ufb01cial Intelli- gence Organization. Yifan Gao, Lidong Bing, Piji Li, Irwin King, and Michael R. Lyu. 2019b. Generating distractors for reading comprehension questions from real exam- inations. Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence, 33(01):6423\u20136430. Yifan Gao, Piji Li, Irwin King, and Michael R. Lyu. 2019c. Interconnected question generation with",
            "coreference alignment and conversation \ufb02ow mod- eling. In Proceedings of the 57th Annual Meet- ing of the Association for Computational Linguis- tics, pages 4853\u20134862, Florence, Italy. Association for Computational Linguistics. Xavier Glorot and Yoshua Bengio. 2010. Understand- ing the dif\ufb01culty of training deep feedforward neu- ral networks. In Proceedings of the Thirteenth International Conference on Arti\ufb01cial Intelligence and Statistics, volume 9 of Proceedings of Machine Learning Research, pages 249\u2013256, Chia Laguna Resort, Sardinia, Italy. PMLR. Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K. Li. 2016. Incorporating copying mechanism in sequence-to-sequence learning. In Proceedings of the 54th Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers), pages 1631\u20131640, Berlin, Germany. Association for Computational Linguistics.",
            "Incorporating copying mechanism in sequence-to-sequence learning. In Proceedings of the 54th Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers), pages 1631\u20131640, Berlin, Germany. Association for Computational Linguistics. Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio. 2016. Pointing the unknown words. In Proceedings of the 54th Annual Meeting of the Association for Computa- tional Linguistics (Volume 1: Long Papers), pages 140\u2013149, Berlin, Germany. Association for Compu- tational Linguistics. Michael Heilman and Noah A. Smith. 2010. Good question! statistical ranking for question genera- tion. In Human Language Technologies: The 2010 Annual Conference of the North American Chap- ter of the Association for Computational Linguistics, pages 609\u2013617, Los Angeles, California. Associa- tion for Computational Linguistics. Yanghoon Kim, Hwanhee Lee, Joongbo Shin, and Ky- omin Jung. 2019.",
            "Associa- tion for Computational Linguistics. Yanghoon Kim, Hwanhee Lee, Joongbo Shin, and Ky- omin Jung. 2019. Improving neural question gener- ation using answer separation. In AAAI Conference on Arti\ufb01cial Intelligence. Chin-Yew Lin. 2004. ROUGE: A package for auto- matic evaluation of summaries. In Text Summa- rization Branches Out: Proceedings of the ACL-04 Workshop, pages 74\u201381, Barcelona, Spain. Associa- tion for Computational Linguistics. Thang Luong, Hieu Pham, and Christopher D. Man- ning. 2015. Effective approaches to attention-based neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natu- ral Language Processing, pages 1412\u20131421, Lis- bon, Portugal. Association for Computational Lin- guistics. Ruslan Mitkov and Le An Ha. 2003. Computer-aided generation of multiple-choice tests.",
            "Association for Computational Lin- guistics. Ruslan Mitkov and Le An Ha. 2003. Computer-aided generation of multiple-choice tests. In Proceedings of the HLT-NAACL 03 Workshop on Building Edu- cational Applications Using Natural Language Pro- cessing, pages 17\u201322. Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Mar- garet Mitchell, Xiaodong He, and Lucy Vander- wende. 2016. Generating natural questions about an image. In Proceedings of the 54th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1802\u20131813, Berlin, Germany. Association for Computational Linguis- tics. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Com- putational Linguistics, pages 311\u2013318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.",
            "2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Com- putational Linguistics, pages 311\u2013318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, Doha, Qatar. Association for Computational Linguistics. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natu- ral Language Processing, pages 2383\u20132392, Austin, Texas. Association for Computational Linguistics. Vasile Rus, Brendan Wyse, Paul Piwek, Mihai Lin- tean, Svetlana Stoyanchev, and Christian Moldovan.",
            "Association for Computational Linguistics. Vasile Rus, Brendan Wyse, Paul Piwek, Mihai Lin- tean, Svetlana Stoyanchev, and Christian Moldovan. 2010. The \ufb01rst question generation shared task eval- uation challenge. In Proceedings of the 6th Interna- tional Natural Language Generation Conference. Swarnadeep Saha and Mausam. 2018. Open informa- tion extraction from conjunctive sentences. In Pro- ceedings of the 27th International Conference on Computational Linguistics, pages 2288\u20132299, Santa Fe, New Mexico, USA. Association for Computa- tional Linguistics. Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointer- generator networks. In Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073\u2013 1083, Vancouver, Canada. Association for Compu- tational Linguistics.",
            "In Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073\u2013 1083, Vancouver, Canada. Association for Compu- tational Linguistics. Iulian Vlad Serban, Alberto Garc\u00b4\u0131a-Dur\u00b4an, Caglar Gulcehre, Sungjin Ahn, Sarath Chandar, Aaron Courville, and Yoshua Bengio. 2016. Generating factoid questions with recurrent neural networks: The 30M factoid question-answer corpus. In Pro- ceedings of the 54th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers), pages 588\u2013598, Berlin, Germany. Associa- tion for Computational Linguistics. Linfeng Song, Zhiguo Wang, Wael Hamza, Yue Zhang, and Daniel Gildea. 2018. Leveraging context in- formation for natural question generation.",
            "Associa- tion for Computational Linguistics. Linfeng Song, Zhiguo Wang, Wael Hamza, Yue Zhang, and Daniel Gildea. 2018. Leveraging context in- formation for natural question generation. In Pro- ceedings of the 2018 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol- ume 2 (Short Papers), pages 569\u2013574, New Orleans,",
            "Louisiana. Association for Computational Linguis- tics. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from over\ufb01tting. Journal of Machine Learning Re- search, 15:1929\u20131958. Xingwu Sun, Jing Liu, Yajuan Lyu, Wei He, Yan- jun Ma, and Shi Wang. 2018. Answer-focused and position-aware neural question generation. In Pro- ceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing, pages 3930\u2013 3939, Brussels, Belgium. Association for Computa- tional Linguistics. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural net- works. In Advances in Neural Information Pro- cessing Systems 27, pages 3104\u20133112. Curran As- sociates, Inc.",
            "2014. Sequence to sequence learning with neural net- works. In Advances in Neural Information Pro- cessing Systems 27, pages 3104\u20133112. Curran As- sociates, Inc. Xingdi Yuan, Tong Wang, Caglar Gulcehre, Alessan- dro Sordoni, Philip Bachman, Saizheng Zhang, Sandeep Subramanian, and Adam Trischler. 2017. Machine comprehension by text-to-text neural ques- tion generation. In Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 15\u201325, Vancouver, Canada. Association for Computational Linguistics. Yao Zhao, Xiaochuan Ni, Yuanyuan Ding, and Qifa Ke. 2018. Paragraph-level neural question gener- ation with maxout pointer and gated self-attention networks. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Process- ing, pages 3901\u20133910, Brussels, Belgium. Associ- ation for Computational Linguistics.",
            "Paragraph-level neural question gener- ation with maxout pointer and gated self-attention networks. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Process- ing, pages 3901\u20133910, Brussels, Belgium. Associ- ation for Computational Linguistics. Qingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan, Hangbo Bao, and Ming Zhou. 2017. Neural ques- tion generation from text: A preliminary study. In Proceedings of the 6th CCF International Confer- ence on Natural Language Processing and Chi- nese Computing (NLPCC), pages 662\u2013671, Dalian, China."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1910.06036.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 10962.0,
    "avg_doclen_est": 171.28125
}
