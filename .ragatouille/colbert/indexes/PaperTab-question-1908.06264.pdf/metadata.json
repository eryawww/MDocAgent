{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "EmotionX-IDEA: Emotion BERT \u2013 an Affectional Model for Conversation Yen-Hao Huang , Ssu-Rui Lee , Mau-Yun Ma , Yi-Hsin Chen , Ya-Wen Yu , Yi-Shin Chen\u2217 Intelligent Data Engineering and Applications Laboratory National Tsing Hua University Hsinchu, Taiwan {yenhao0218, gn01697933, brian41005, eunicebes, evan800112, yishin}@gmail.com Abstract In this paper, we investigate the emotion recog- nition ability of the pre-training language model, namely BERT. By the nature of the framework of BERT, a two sentence structure, we adapt BERT to continues dialogue emotion prediction tasks, which rely heavily on the sentence-level context-aware understanding. The experiments show that by map- ping the continues dialogue into a causal utterance pair, which is constructed by the utterance and the reply utterance, models can better capture the emo- tions of the reply utterance. The present method have achieved 0.815 and 0.885 micro F1 score in the testing dataset of Friends and EmotionPush, re- spectively.",
      "The present method have achieved 0.815 and 0.885 micro F1 score in the testing dataset of Friends and EmotionPush, re- spectively. 1 Introduction Emotion detection has long been a topic of interest to scholars in natural language processing (NLP) domain. Researchers aim to recognize the emotion behind the text and distribute similar ones into the same group. Establishing an emotion classi\ufb01er can not only understand each user\u2019s feeling but also be extended to various application, for example, the motiva- tion behind a user\u2019s interests [Saravia et al., 2015]. Based on releasing of large text corpus on social media and the emo- tion categories proposed by [Ekman et al., 1987; Plutchik, 2001], numerous models have provided and achieved fabu- lous precision so far. For example, DeepMoji [Felbo et al., 2017] which utilized transfer learning concept to enhance emotions and sarcasm understanding behind the target sen- tence. CARER [Saravia et al., 2018] learned contextualized affect representations to make itself more sensitive to rare words and the scenario behind the texts.",
      "CARER [Saravia et al., 2018] learned contextualized affect representations to make itself more sensitive to rare words and the scenario behind the texts. As methods become mature, text-based emotion detecting applications can be extended from a single utterance to a dia- logue contributed by a series of utterances. Table 1 illustrates the difference between single utterance and dialogue emotion recognition. The same utterances in Table 1, even the same person said the same sentence, the emotion it convey may be various, which may depend on different background of the conversation, tone of speaking or personality. Therefore, for \u2217Corresponding Author emotion detection, the information from preceding utterances in a conversation is relatively critical. Table 1: Emotions depending on the context Monica I\u2019m gonna miss you! Rachel I mean it\u2019s the end of an era! Monica I know! (sadness) Chandler So, what do you think? Ross I think It\u2019s the most beautiful table I\u2019ve ever seen. Chandler I know! (joy) Monica Now, this is last minute so I want to apologize for the mess. Okay? Rachel Oh my God!",
      "(sadness) Chandler So, what do you think? Ross I think It\u2019s the most beautiful table I\u2019ve ever seen. Chandler I know! (joy) Monica Now, this is last minute so I want to apologize for the mess. Okay? Rachel Oh my God! It sure didn\u2019t look this way when I lived here. Monica I know! (surprise) In SocialNLP 2019 EmotionX, the challenge is to recog- nize emotions for all utterances in EmotionLines dataset, a dataset consists of dialogues. According to the needs for considering context at the same time, we develop two clas- si\ufb01cation models, inspired by bidirectional encoder repre- sentations from transformers (BERT) [Devlin et al., 2018], FriendsBERT and ChatBERT. In this paper, we introduce our approaches including causal utterance modeling, model pre-training, and \ufb01ne-turning. 2 Dataset EmotionLines [Chen et al., 2018] is a dialogue dataset com- posed of two subsets, Friends and EmotionPush, according to the source of the dialogues.",
      "2 Dataset EmotionLines [Chen et al., 2018] is a dialogue dataset com- posed of two subsets, Friends and EmotionPush, according to the source of the dialogues. The former comes from the scripts of the Friends TV sitcom. The other is made up of Facebook messenger chats. Each subset includes 1, 000 En- glish dialogues, and each dialogue can be further divided into a few consecutive utterances. All the utterances are anno- tated by \ufb01ve annotators on a crowd-sourcing platform (Ama- zon Mechanical Turk), and the labeling work is only based on the textual content. Annotator votes for one of the seven emotions, namely Ekman\u2019s six basic emotions [Ekman et al., 1987], plus the neutral. If none of the emotion gets more than three votes, the utterance will be marked as \u201cnon-neutral\u201d. For the datasets, there are properties worth additional men- tioning. Although Friends and EmotionPush share the same arXiv:1908.06264v1  [cs.CL]  17 Aug 2019",
      "data format, they are quite different in nature. Friends is a speech-based dataset which is annotated dialogues from the TV sitcom. It means most of the utterances are generated by the a few main characters. The personality of a character often affects the way of speaking, and therefore \u201cwho is the speaker\u201d might provide extra clues for emotion prediction. In contrast, EmotionPush does not have this trait due to the anonymous mechanism. In addition, features such as typo, hyperlink, and emoji that only appear in chat-based data will need some domain-speci\ufb01c techniques to process. Incidentally, the objective of the challenge is to predict the emotion for each utterance. Just, according to EmotionX 2019 speci\ufb01cation, there are only four emotions be selected as our label candidates, which are Joy, Sadness, Anger, and Neu- tral. These emotions will be considered during performance evaluation. The technical detail will also be introduced and discussed in following Section 4.1 and Section 5.1.",
      "These emotions will be considered during performance evaluation. The technical detail will also be introduced and discussed in following Section 4.1 and Section 5.1. 3 Model Description For this challenge, we adapt BERT which is proposed by [De- vlin et al., 2018] to help understand the context at the same time. Technically, BERT, designed on end-to-end architec- ture, is a deep pre-trained transformer encoder that dynam- ically provides language representation and BERT already achieved multiple state-of-the-art results on GLUE bench- mark [Wang et al., 2018] and many tasks. A quick recap for BERT\u2019s architecture and its pre-training tasks will be il- lustrated in the following subsections. 3.1 Model Architecture BERT, the Bidirectional Encoder Representations from Transformers, consists of several transformer encoder layers that enable the model to extract very deep language features on both token-level and sentence-level. Each transformer encoder contains multi-head self-attention layers that pro- vide ability to learn multiple attention feature of each word from their bidirectional context.",
      "Each transformer encoder contains multi-head self-attention layers that pro- vide ability to learn multiple attention feature of each word from their bidirectional context. The transformer and its self- attention mechanism are proposed by [Vaswani et al., 2017]. This self-attention mechanism can be interpreted as a key- value mapping given query. By given the embedding vec- tor for token input, the query (Q), key (K) and value (V ) are produced by the projection from each three parameter matrices where W Q \u2208Rdmodel\u00d7dk, W K \u2208Rdmodel\u00d7dk and W V \u2208Rdmodel\u00d7dv. The self-attention [Vaswani et al., 2017] is formally represented as: Attention(Q, K, V ) = softmax(QKT \u221adk )V (1) The dk = dv = dmodel = 1024 in BERT large version and 768 in BERT base version. Once model can extract at- tention feature, we can extend one self-attention into multi- head self-attention, this extension makes sub-space features can be extracted in same time by this multi-head con\ufb01gu- ration.",
      "Once model can extract at- tention feature, we can extend one self-attention into multi- head self-attention, this extension makes sub-space features can be extracted in same time by this multi-head con\ufb01gu- ration. Overall, the multi-attention mechanism is adopt for each transformer encoder, and several of encoder layer will be stacked together to form a deep transformer encoder. For the model input, BERT allow us take one sentence as input sequence or two sentences together as one input se- quence, and the maximum length of input sequence is 512. The way that BERT was designed is for giving model the sentence-level and token-level understanding. In two sen- tences case, a special token ([SEP]) will be inserted between two sentences. In addition, the \ufb01rst input token is also a spe- cial token ([CLS]), and its corresponding ouput will be vec- tor place for classi\ufb01cation during \ufb01ne-tuning.",
      "In addition, the \ufb01rst input token is also a spe- cial token ([CLS]), and its corresponding ouput will be vec- tor place for classi\ufb01cation during \ufb01ne-tuning. The outputs of the last encoder layer corresponding to each input token can be treated as word representations for each token, and the word representation of the \ufb01rst token ([CLS]) will be con- sider as classi\ufb01cation (output) representation for further \ufb01ne- tuning tasks. In BERT, this vector is denoted as C \u2208Rdmodel, and a classi\ufb01cation layer is denoted as W \u2208RK\u00d7dmodel, where K is number of classi\ufb01cation labels. Finally, the pre- diction P of BERT is represented as P = softmax(CW T ). 3.2 Pre-training Tasks In pre-training, intead of using unidirectional language mod- els, BERT developed two pre-training tasks: (1) Masked LM (cloze test) and (2) Next Sentence Prediction. At the \ufb01rst pre- training task, bidirectional language modeling can be done at this cloze-like pre-training.",
      "At the \ufb01rst pre- training task, bidirectional language modeling can be done at this cloze-like pre-training. In detail, 15% tokens of input se- quence will be masked at random and model need to predict those masked tokens. The encoder will try to learn contextual representations from every given tokens due to masking to- kens at random. Model will not know which part of the input is going to be masked, so that the information of each masked tokens should be inferred by remaining tokens. At Next Sen- tence Prediction, two sentences concatenated together will be considered as model input. In order to give model a good na- ture language understanding, knowing relationship between sentence is one of important abilities. When generating in- put sequences, 50% of time the sentence B is actually fol- lowed by sentence A, and rest 50% of the time the sentence B will be picked randomly from dataset, and model need to pre- dict if the sentence B is next sentence of sentence A. That is, the attention information will be shared between sentences.",
      "Such sentence-level understanding may have dif\ufb01culties to be learned at \ufb01rst pre-training task (Masked LM), therefore, the pre-training task (NSP) is developed as second training goal to capture the cross sentence relationship. In this competition, limited by the size of dataset and the challenge in contextual emotion recognition, we consider BERT with both two pre-training tasks can give a good start- ing point to extract emotion changing during dialogue-like conversation. Especially the second pre-training task, it might be more important for dialogue-like conversation where the emotion may various by the context of continuous utterances. That is, given a set of continues conversations, the emotion of current utterance might be in\ufb02uenced by previous utterance. By this assumption and with supporting from the experiment results of BERT, we can take sentence A as one-sentence con- text and consider sentence B as the target sentence for emo- tion prediction. The detail will be described in Section 4. 4 Methodology The main goal of the present work is to predict the emotion of utterance within the dialogue. Following are four major dif\ufb01culties we concern about:",
      "Causal Utterance Modeling Unifying  Tokenization Utterance  Preprocessing Friends EmotionPush Personality  Tokenization Utterance  Preprocessing Model Pre-training Completed Scripts   Pre-training Masked  Language Model Next Sentence   Prediction Twitter Emotion  Pre-training Fine-tuning Weighted Balanced   Warming Weighted Balanced   Warming Evaluation Evaluation FriendBERT ChatBERT Figure 1: Framework 1. The emotion of the utterances depends not only on the text but also on the interaction happened earlier. 2. The source of the two datasets are different. Friends is speech-based dialogues and EmotionPush is chat-based dialogues. It makes datasets possess different character- istics. 3. There are only 1, 000 dialogues in both training datasets which are not large enough for the stability of training a complex neural-based model. 4. The prediction targets (emotion labels) are highly unbal- anced. The proposed approach is summarized in Figure 1, which aims to overcome these challenges.",
      "4. The prediction targets (emotion labels) are highly unbal- anced. The proposed approach is summarized in Figure 1, which aims to overcome these challenges. The framework could be separated into three steps and described as follow: 4.1 Causal Utterance Modeling Given a dialogue D(i) which includes sequence of utterances denoted as D(i) = (u(i) 1 , u(i) 2 , ..., u(i) n ), where i is the index in dataset and n is the number of utterances in the given dia- logue. In order to conserve the emotional information of both utterance and conversation, we rearrange each two consecu- tive utterances ut, ut\u22121 into a single sentence representation xt as x(i) t = concat(u(i) t , u(i) t\u22121) (2) The corresponding sentence representation corpus X(i) are denoted as X(i) = (x(i) 1 , x(i) 2 , ..., x(i) n ). Note that the \ufb01rst utterance within a conversation does not have its causal utter- ance (previous sentence), therefore, the causal utterance will be set as [None].",
      "Note that the \ufb01rst utterance within a conversation does not have its causal utter- ance (previous sentence), therefore, the causal utterance will be set as [None]. A practical example of sentence represen- tation is shown in Table 2. Since the characteristics of two datasets are not identical, we customize different causal utterance modeling strategies to re\ufb01ne the information in text. For Friends, there are two speci\ufb01c properties. The \ufb01rst one is that most dialogues are surrounding with the six main characters, including Rachel, Monica, Phoebe, Joey, Chan- dler, and Ross. The utterance ratio of given by the six roles is up to 83.4%. Second, the personal characteristics of the six characters are very clear. Each leading role has its own emotion undulated rule. To make use of these features, we introduce the personality tokenization which help learning the personality of the six characters. Personality tokeniza- tion concatenate the speaker and says tokens before the input utterance if the speaker is one of the six characters. The ex- ample is shown in Table 3.",
      "To make use of these features, we introduce the personality tokenization which help learning the personality of the six characters. Personality tokeniza- tion concatenate the speaker and says tokens before the input utterance if the speaker is one of the six characters. The ex- ample is shown in Table 3. For EmotionPush, the text are informal chats which in- cluding like slang, acronym, typo, hyperlink, and emoji. An- other characteristic is that the speci\ufb01c name entities are to- kenized with random index. (e.g. \u201corganization 80\u201d, \u201cper- son 01\u201d, and \u201ctime 12\u201d). We consider some of these infor- mal text are related to expressing emotion such as repeated typing, purposed capitalization, and emoji (e.g. \u201c:D\u201d, \u201c:(\u201d, and \u201c<3\u201d)). Therefore, we keep most informal expressions but only process hyperlinks, empty utterance, and name enti- ties by unifying the tokens.",
      "\u201c:D\u201d, \u201c:(\u201d, and \u201c<3\u201d)). Therefore, we keep most informal expressions but only process hyperlinks, empty utterance, and name enti- ties by unifying the tokens. 4.2 Model Pre-training Since the size of both datasets are not large enough for com- plex neural-based model training as well as BERT model is only pre-train on formal text datasets, the issues of over\ufb01tting and domain bias are important considerations for design the pre-training process. To avoid our model over\ufb01tting on the training data and in- crease the understanding of informal text, we adapted BERT and derived two models, namely FriendsBERT and Chat- BERT, with different pre-training tasks before the formal training process for Friends and EmotionPush dataset, re- spectively. The pre-training strategies are described below. For pre-training FriendsBERT, we collect the com- pleted scripts of all ten seasons of Friends TV shows from emorynlp1 which includes 3,107 scenes within 61,309 ut- terances. All the utterances are followed the preprocessing methods mentions above to compose the corpus for Masked language model pre-training task.",
      "All the utterances are followed the preprocessing methods mentions above to compose the corpus for Masked language model pre-training task. The consequent utterances in the same scenes are considered as the consequent sentences to pre-train the Next Sentence Prediction task. In the pre- 1http://nlp.mathcs.emory.edu",
      "Table 2: An example of sentence representation Speaker Utterance Emotion Representation Label Joey What?! surprise [CLS] What?! [SEP] [None] [SEP] surprise Chandler What\u2019s wrong with you? non-neutral [CLS] What\u2019s wrong with you? [SEP] What?! [SEP] non-neutral Joey Nothing! neutral [CLS] Nothing! [SEP] What\u2019s wrong with you? [SEP] neutral Table 3: An example of personality tokenization Speaker Utterance Emotion Representation (with personality tokenization) Label Janice I\u2019m sorry. sadness [CLS] I\u2019m sorry. [SEP] [None] [SEP] sadness Chandler Ohhh. Don\u2019t go. sadness [CLS] [Chandler] [says] Ohhh. Don\u2019t go. [SEP] I\u2019m sorry. [SEP] sadness Janice No, I gotta go. non-neutral [CLS] No, I gotta go. [SEP] [Chandler] [says] Ohhh.Don\u2019t go.",
      "Don\u2019t go. [SEP] I\u2019m sorry. [SEP] sadness Janice No, I gotta go. non-neutral [CLS] No, I gotta go. [SEP] [Chandler] [says] Ohhh.Don\u2019t go. [SEP] non-neutral Table 4: Statistics for Twitter Dataset Emotions Amount Hashtags Anger 102,289 #mad, #pissed Anticipation 3,975 #pumped, #ready Disgust 8,934 #awful, #eww Fear 102,468 #fear, #worried Joy 167,027 #fun, #joy Sadness 214,454 #depressed, #grief Surprise 46,101 #strange, #surprise Trust 19,222 #hope, #secure training process, the training loss is the sum of the mean like- lihood of two pre-train tasks. For pre-training ChatBERT, we pre-train our model on the Twitter dataset, since the text and writing style on Twitter are close to the chat text where both may involved with many informal words or emoticons as well.",
      "For pre-training ChatBERT, we pre-train our model on the Twitter dataset, since the text and writing style on Twitter are close to the chat text where both may involved with many informal words or emoticons as well. The Twitter emotion dataset, 8 basic emotions from emotion wheel [Ekman et al., 1987], was collected by twitter streaming API with speci\ufb01c emotion-related hashtags, such as #anger, #joy, #cry, #sad and etc. The hashtags in tweets are treated as emotion label for model \ufb01ne-tuning. The tweets were \ufb01ne-grined process- ing followed the rules in [Abdul-Mageed and Ungar, 2017; Saravia et al., 2018], including duplicate tweets removing, the emotion hashtags must appearing in the last position of a tweet, and etc. The statis of tweets were summarized in Ta- ble 4. Each tweet and corresponding emotion label composes an emotion classi\ufb01cation dataset for pre-training.",
      "The statis of tweets were summarized in Ta- ble 4. Each tweet and corresponding emotion label composes an emotion classi\ufb01cation dataset for pre-training. 4.3 Fine-tuning Since our emotion recognition task is treated as a sequence- level classi\ufb01cation task, the model would be \ufb01ne-tuned on the processed training data. Following the BERT construc- tion, we take the \ufb01rst embedding vector which corresponds to the special token [CLS] from the \ufb01nal hidden state of the Transformer encoder. This vector represents the embedding vector of the corresponding conversation utterances which is denoted as C \u2208RH, where H is the embedding size. A dense neural layer is treated as a classi\ufb01cation layer which consists of parameters W \u2208RK\u00d7H and b \u2208RK, where K is the number of emotion class. The emotion prediction probabili- Table 5: Emotions Distribution of two dataset Processing EmotionLines Friends EmotionPush Dialogue 1,000 1,000 Utterance 14,503 14,742 Utterance (\ufb01ltered) 9,479 12,609 Train / Val 7,660 / 1,",
      "000 1,000 Utterance 14,503 14,742 Utterance (\ufb01ltered) 9,479 12,609 Train / Val 7,660 / 1,837 10,145 / 2,464 Emotions in Training / Validation set Anger 598 / 161 103 / 37 Joy 1,406 / 304 1,642 / 458 Neutral 5,243 / 1,287 7,973 / 1,882 Sadness 413 / 85 427 / 87 ties P \u2208RK are computed by a softmax activation function as P = softmax(CW T + b) (3) All the parameters in BERT and the classi\ufb01cation layer would be \ufb01ne-turned together to minimize the Negative Log Like- lihood (NLL) loss function, as Equation (4), based on the ground truth emotion label c. L = \u22121 N N X i=1 log \u0010 \u02c6p(i) c \u0011 (4) In order to tackle the problem of highly unbalanced emo- tion labels, we apply weighted balanced warming on NLL loss function, as Equation (5),",
      "L = \u22121 N N X i=1 log \u0010 \u02c6p(i) c \u0011 (4) In order to tackle the problem of highly unbalanced emo- tion labels, we apply weighted balanced warming on NLL loss function, as Equation (5), in the \ufb01rst epoch of \ufb01ne-tuning procedure. L = \u2212 1 PN i=1 w(i) c N X i=1 log \u0010 wc\u02c6p(i) c \u0011 (5) where w are the weights of corresponding emotion label c which are computed and normalize by the frequency as wc = min(freq(c)) freq(c) , \u2200c \u2208c (6) By adding the weighted balanced warming on NLL loss, the model could learn to predict the minor emotions (e.g.",
      "Table 6: Validation Results (Friends) Anger Joy Neutral Sadness Overall Models P. R. F1 P. R. F1 P. R. F1 P. R. F1 F1 BOW-LR 0.65 0.31 0.42 0.66 0.66 0.66 0.85 0.94 0.89 0.51 0.21 0.30 0.80 BOW-RF 0.74 0.30 0.43 0.66 0.62 0.64 0.84 0.96 0.89 0.83 0.18 0.29 0.81 TFIDF-RF 0.61 0.30 0.40 0.62 0.65 0.63 0.86 0.94 0.90 0.53 0.19 0.28 0.80 TextCNN 0.74 0.40 0.52 0.65 0.71 0.68 0.87 0.94 0.90 0.",
      "94 0.90 0.53 0.19 0.28 0.80 TextCNN 0.74 0.40 0.52 0.65 0.71 0.68 0.87 0.94 0.90 0.70 0.25 0.37 0.82 C-TextCNN 0.71 0.49 0.58 0.70 0.70 0.70 0.87 0.94 0.90 0.62 0.25 0.35 0.83 FriendsBERT-base-s 0.76 0.58 0.66 0.79 0.71 0.75 0.88 0.95 0.91 0.61 0.36 0.46 0.84 FriendsBERT-base 0.78 0.58 0.66 0.74 0.78 0.76 0.89 0.93 0.91 0.65 0.38 0.48 0.85 FriendsBERT-large 0.80 0.61 0.",
      "78 0.58 0.66 0.74 0.78 0.76 0.89 0.93 0.91 0.65 0.38 0.48 0.85 FriendsBERT-large 0.80 0.61 0.69 0.76 0.83 0.79 0.91 0.93 0.92 0.64 0.41 0.50 0.86 anger and sadness) earlier and make the training process more stable. Since the major evaluation metrics micro F1- score is effect by the number of each label, we only apply the weighted balanced warming in \ufb01rst epoch to optimize the performance.",
      "64 0.41 0.50 0.86 anger and sadness) earlier and make the training process more stable. Since the major evaluation metrics micro F1- score is effect by the number of each label, we only apply the weighted balanced warming in \ufb01rst epoch to optimize the performance. Table 7: Experimental Setup of Proposed Model FriendsBERT ChatBERT Pre-trained weights BERT-uncased BERT-cased Batch size 8 4 Learning rate (Adam) 2.5 \u00d7 10\u22126 2.5 \u00d7 10\u22126 Number of epochs 3 2 Max length (input tokens) 113 249 Dropout rate (last layer) 0.75 0.75 5 Experiments Since the EmotionX challenge only provided the gold la- bels in training data, we pick the best performance model (weights) to predict the testing data. In this section, we present the experiment and evaluation results. 5.1 Experimental Setup The EmotionX challenge consists of 1, 000 dialogues for both Friends and EmotionPush. In all of our experiments, each dataset is separated into top 800 dialogues for training and last 200 dialogues for validation.",
      "5.1 Experimental Setup The EmotionX challenge consists of 1, 000 dialogues for both Friends and EmotionPush. In all of our experiments, each dataset is separated into top 800 dialogues for training and last 200 dialogues for validation. Since the EmotionX chal- lenge considers only the four emotions (anger, joy, neutral, and sadness) in the evaluation stage, we ignore all the data point corresponding to other emotions directly. The details of emotions distribution are shown in Table 5. The hyperparameters and training setup of our models (FriendsBERT and ChatBERT) are shown in Table 7. Some common and easily implemented methods are selected as the baselines embedding methods and classi\ufb01cation mod- els. The baseline embedding methods are including bag-of- words (BOW), term frequency\u2013inverse document frequency (TFIDF), and neural-based word embedding. The classi\ufb01ca- tion models are including Logistic Regression (LR), Random Forest (RF), TextCNN [Kim, 2014] with initial word embed- ding as GloVe [Pennington et al., 2014], and our proposed model.",
      "The classi\ufb01ca- tion models are including Logistic Regression (LR), Random Forest (RF), TextCNN [Kim, 2014] with initial word embed- ding as GloVe [Pennington et al., 2014], and our proposed model. All the experiment results are based on the best per- formances of validation results. 5.2 Performance The experiment results of validation on Friends are shown in Table 6. The proposed model and baselines are evaluated based on the Precision (P.), Recall (R.), and F1-measure (F1). For the traditional baselines, namely BOW and TFIDF, we observe that they achieve surprising high F1 scores around 0.81, however, the scores for Anger and Sadness are lower. This explains that traditional approaches tend to predict the labels with large sample size, such as Joy and Neutral, but fail to take of scarce samples even when an ensemble ran- dom forest classi\ufb01er is adopted.",
      "This explains that traditional approaches tend to predict the labels with large sample size, such as Joy and Neutral, but fail to take of scarce samples even when an ensemble ran- dom forest classi\ufb01er is adopted. In order to prevent the unbalanced learning, we choose the weighted loss mecha- nism for both TextCNN and causal modeling TextCNN (C- TextCNN), these models suffer less than the traditional base- lines and achieve a slightly balance performance, where there are around 15% and 7% improvement on Anger and Sadness, respectively. We following adopt the casual utterance model- ing to original TextCNN, mapping previous utterance as well as target utterance into model. The causal utterance modeling improve the C-TextCNN over TextCNN for 6%, 2% and 1% on Anger, Joy and overall F1 score. Motivated from these pre- liminary experiments, the proposed FriendsBERT also adopt the ideas of both weighted loss and causal utterance mod- eling.",
      "Motivated from these pre- liminary experiments, the proposed FriendsBERT also adopt the ideas of both weighted loss and causal utterance mod- eling. As compared to the original BERT, single sentence BERT (FriendsBERT-base-s), the proposed FriendsBERT- base improve 1% for Joy and overall F1, and 2% for Sadness. For the \ufb01nal validation performance, our proposed approach achieves the highest scores, which are 0.85 and 0.86 for FriendsBERT-base and FriendsBERT-large, respectively. Overall, the proposed FriendsBERT successfully captures the sentence-level context-awarded information and outper- forms all the baselines, which not only achieves high perfor- mance on large sample labels, but also on small sample labels. The similar settings are also adapted to EmotionPush dataset for the \ufb01nal evaluation. 5.3 Evaluation Results The testing dataset consists of 240 dialogues including 3, 296 and 3, 536 utterances in Friends and EmotionPush respec- tively. We re-train our FriendsBERT and ChatBERT with top 920 training dialogues and predict the evaluation results using the model performing the best validation results.",
      "We re-train our FriendsBERT and ChatBERT with top 920 training dialogues and predict the evaluation results using the model performing the best validation results. The results are shown in Table 8 and Table 9. The present method achieves 81.5% and 88.5% micro F1-score on the testing dataset of Friends and EmotionPush, respectively.",
      "Table 8: Evaluation (Testing) Results of Friends precision recall f1-score support Anger 0.716 0.681 0.698 141 Joy 0.875 0.663 0.755 505 Neutral 0.814 0.942 0.873 1, 035 Sadness 0.713 0.512 0.596 121 Micro AVG 0.815 0.815 0.815 1, 802 Macro AVG 0.779 0.700 0.731 1, 802 Weighted AVG 0.816 0.815 0.808 1, 802 Table 9: Evaluation (Testing) Results of EmotionPush precision recall f1-score support Anger 0.818 0.333 0.474 27 Joy 0.812 0.745 0.777 601 Neutral 0.903 0.952 0.927 2, 146 Sadness 0.864 0.464 0.604 110 Micro AVG 0.885 0.885 0.885 2, 884 Macro AVG 0.849 0.624 0.",
      "903 0.952 0.927 2, 146 Sadness 0.864 0.464 0.604 110 Micro AVG 0.885 0.885 0.885 2, 884 Macro AVG 0.849 0.624 0.695 2, 884 Weighted AVG 0.882 0.885 0.879 2, 884 6 Conclusion and Future work In the present work, we propose FriendsBERT and Chat- BERT for the multi-utterance emotion recognition task on EmotionLines dataset. The proposed models are adapted from BERT [Devlin et al., 2018] with three main improve- ment during the model training procedure, which are the causal utterance modeling mechanism, speci\ufb01c model pre- training, and adapt weighted loss. The causal utterance mod- eling takes the advantages of the sentence-level context in- formation during model inference. The speci\ufb01c model pre- training helps to against the bias in different text domain. The weighted loss avoids our model to only predict on large size sample.",
      "The causal utterance mod- eling takes the advantages of the sentence-level context in- formation during model inference. The speci\ufb01c model pre- training helps to against the bias in different text domain. The weighted loss avoids our model to only predict on large size sample. The effectiveness and generalizability of the pro- posed methods are demonstrated from the experiments. In future work, we consider to include the conditional probabilistic constraint P(EmoB| \u02c6 EmoA). Model should pre- dict the emotion based on a certain understanding about con- text emotions. This might be more reasonable for guiding model than just predicting emotion of SentenceB directly. In addition, due to the limitation of BERT input format, am- biguous number of input sentences is now becoming an im- portant design requirement for our future work. Also, person- ality embedding development will be another future work of the emotion recognition. The personality embedding will be considered as sentence embedding injected into word embed- ding, and it seems this additional information can contribute some improvement potentially. References [Abdul-Mageed and Ungar, 2017] Muhammad Abdul- Mageed and Lyle Ungar.",
      "The personality embedding will be considered as sentence embedding injected into word embed- ding, and it seems this additional information can contribute some improvement potentially. References [Abdul-Mageed and Ungar, 2017] Muhammad Abdul- Mageed and Lyle Ungar. Emonet: Fine-grained emotion detection with gated recurrent neural networks. In Pro- ceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 718\u2013728, 2017. [Chen et al., 2018] Sheng-Yeh Chen, Chao-Chun Hsu, Chuan-Chun Kuo, Lun-Wei Ku, et al. Emotionlines: An emotion corpus of multi-party conversations. arXiv preprint arXiv:1802.08379, 2018. [Devlin et al., 2018] Jacob Devlin, Ming-Wei Chang, Ken- ton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805, 2018.",
      "Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805, 2018. [Ekman et al., 1987] Paul Ekman, Wallace V Friesen, Mau- reen O\u2019sullivan, Anthony Chan, Irene Diacoyanni- Tarlatzis, Karl Heider, Rainer Krause, William Ayhan LeCompte, Tom Pitcairn, Pio E Ricci-Bitti, et al. Uni- versals and cultural differences in the judgments of facial expressions of emotion. Journal of personality and social psychology, 53(4):712, 1987. [Felbo et al., 2017] Bjarke Felbo, Alan Mislove, Anders S\u00f8gaard, Iyad Rahwan, and Sune Lehmann. Using mil- lions of emoji occurrences to learn any-domain representa- tions for detecting sentiment, emotion and sarcasm. arXiv preprint arXiv:1708.00524, 2017. [Kim, 2014] Yoon Kim.",
      "Using mil- lions of emoji occurrences to learn any-domain representa- tions for detecting sentiment, emotion and sarcasm. arXiv preprint arXiv:1708.00524, 2017. [Kim, 2014] Yoon Kim. Convolutional neural networks for sentence classi\ufb01cation. arXiv preprint arXiv:1408.5882, 2014. [Pennington et al., 2014] Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543, 2014. [Plutchik, 2001] Robert Plutchik. The nature of emotions: Human emotions have deep evolutionary roots, a fact that may explain their complexity and provide tools for clinical practice. American scientist, 89(4):344\u2013350, 2001. [Saravia et al., 2015] Elvis Saravia, Carlos Argueta, and Yi- Shin Chen.",
      "American scientist, 89(4):344\u2013350, 2001. [Saravia et al., 2015] Elvis Saravia, Carlos Argueta, and Yi- Shin Chen. Emoviz: Mining the world\u2019s interest through emotion analysis. Advances in Social Networks Analy- sis and Mining (ASONAM), 2015 IEEE/ACM International Conference, 2015. [Saravia et al., 2018] Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, and Yi-Shin Chen. CARER: Contextualized affect representations for emotion recog- nition. In Proceedings of the 2018 Conference on Em- pirical Methods in Natural Language Processing, pages 3687\u20133697, 2018. [Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need.",
      "Attention is all you need. In Advances in neural information processing sys- tems, pages 5998\u20136008, 2017. [Wang et al., 2018] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. CoRR, abs/1804.07461, 2018."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1908.06264.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":7221,
  "avg_doclen":176.1219512195,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1908.06264.pdf"
    }
  }
}