{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Same Representation, Different Attentions: Shareable Sentence Representation Learning from Multiple Tasks Renjie Zheng, Junkun Chen, Xipeng Qiu\u2217 Shanghai Key Laboratory of Intelligent Information Processing, Fudan University School of Computer Science, Fudan University 825 Zhangheng Road, Shanghai, China Abstract Distributed representation plays an important role in deep learning based natural language processing. However, the representation of a sentence often varies in different tasks, which is usually learned from scratch and suffers from the limited amounts of training data. In this paper, we claim that a good sentence representation should be invariant and can bene\ufb01t the various subsequent tasks. To achieve this purpose, we propose a new scheme of informa- tion sharing for multi-task learning. More specif- ically, all tasks share the same sentence represen- tation and each task can select the task-speci\ufb01c in- formation from the shared sentence representation with attention mechanism. The query vector of each task\u2019s attention could be either static param- eters or generated dynamically. We conduct exten- sive experiments on 16 different text classi\ufb01cation tasks, which demonstrate the bene\ufb01ts of our archi- tecture.",
      "The query vector of each task\u2019s attention could be either static param- eters or generated dynamically. We conduct exten- sive experiments on 16 different text classi\ufb01cation tasks, which demonstrate the bene\ufb01ts of our archi- tecture. 1 Introduction The distributed representation plays an important role in deep learning based natural language processing (NLP) [Bengio et al., 2003; Collobert et al., 2011; Sutskever et al., 2014]. On word level, many successful methods have been proposed to learn a good representation for single word, which is also called word embedding, such as skip-gram [Mikolov et al., 2013], GloVe [Pennington et al., 2014], etc. There are also pre-trained word embeddings, which can easily used in down- stream tasks. However, on sentence level, there is still no generic sentence representation which is suitable for various NLP tasks. Currently, most of sentence encoding models are trained speci\ufb01cally for a certain task in a supervised way, which re- sults to different representations for the same sentence in dif- ferent tasks.",
      "Currently, most of sentence encoding models are trained speci\ufb01cally for a certain task in a supervised way, which re- sults to different representations for the same sentence in dif- ferent tasks. Taking the following sentence as an example for domain classi\ufb01cation task and sentiment classi\ufb01cation task, The infantile cart is easy to use, general text classi\ufb01cation models always learn two represen- tations separately. For domain classi\ufb01cation, the model can \u2217Corresponding Author, xpqiu@fudan.edu.cn P S X Y (a) P S X Y (b) S A X Y q (c) Figure 1: Three schemes of information sharing in multi-task lean- ing. (a) stacked shared-private scheme, (b) parallel shared-private scheme, (c) our proposed attentive sharing scheme. learn a better representation of \u201cinfantile cart\u201d while for sen- timent classi\ufb01cation, the model is able to learn a better repre- sentation of \u201ceasy to use\u201d.",
      "learn a better representation of \u201cinfantile cart\u201d while for sen- timent classi\ufb01cation, the model is able to learn a better repre- sentation of \u201ceasy to use\u201d. However, to train a good task-speci\ufb01c sentence representa- tion from scratch, we always need to prepare a large dataset which is always unavailable or costly. To alleviate this prob- lem, one approach is pre-training the model on large unla- beled corpora by unsupervised learning tasks, such as lan- guage modeling [Bengio et al., 2003]. This unsupervised pre- training may be helpful to improve the \ufb01nal performance, but the improvement is not guaranteed since it does not directly optimize the desired task. Another approach is multi-task learning [Caruana, 1997], which is an effective approach to improve the performance of a single task with the help of other related tasks. How- ever, most existing models on multi-task learning attempt to divide the representation of a sentence into private and shared spaces. The shared representation is used in all tasks, and the private one is different for each task.",
      "How- ever, most existing models on multi-task learning attempt to divide the representation of a sentence into private and shared spaces. The shared representation is used in all tasks, and the private one is different for each task. The two typical informa- tion sharing schemes are stacked shared-private scheme and parallel shared-private scheme (as shown in Figure 1(a) and 1(b) respectively). However, we cannot guarantee that a good sentence encoding model is learned by the shared layer. To learn a better shareable sentence representation, we pro- pose a new information-sharing scheme for multi-task learn- ing in this paper. In our proposed scheme, the representation of every sentence is fully shared among all different tasks. To extract the task-speci\ufb01c feature, we utilize the attention mechanism and introduce a task-dependent query vector to select the task-speci\ufb01c information from the shared sentence representation. The query vector of each task can be regarded arXiv:1804.08139v1  [cs.CL]  22 Apr 2018",
      "as learnable parameters (static) or be generated dynamically. If we take the former example, in our proposed model these two classi\ufb01cation tasks share the same representation which includes both domain information and sentiment information. On top of this shared representation, a task-speci\ufb01c query vector will be used to focus \u201cinfantile cart\u201d for domain clas- si\ufb01cation and \u201ceasy to use\u201d for sentiment classi\ufb01cation. The contributions of this papers can be summarized as fol- lows. \u2022 We propose a new information sharing scheme for multi- task learning. As a side effect, the model can be easily visualized and shows what speci\ufb01c parts of the sentence are focused in different tasks. \u2022 In our proposed scheme, we can learn a shareable generic sentence representation, which can be easily transferred to other tasks. The shareable sentence rep- resentation can also be improved by the auxiliary tasks, such as POS Tagging and Chunking. \u2022 We conduct extensive experiments on 16 sentiment clas- si\ufb01cation tasks. Experiments show that our proposed model is space ef\ufb01cient and converges quickly.",
      "The shareable sentence rep- resentation can also be improved by the auxiliary tasks, such as POS Tagging and Chunking. \u2022 We conduct extensive experiments on 16 sentiment clas- si\ufb01cation tasks. Experiments show that our proposed model is space ef\ufb01cient and converges quickly. 2 Sentence Encoding in Multi-task Learning 2.1 Neural Sentence Encoding Model The primary role of sentence encoding models is to rep- resent the variable-length sentence or paragraphs as \ufb01xed- length dense vector (distributed representation). Currently, the effective neural sentence encoding models include neu- ral Bag-of-words (NBOW), recurrent neural networks (RNN) [Sutskever et al., 2014; Chung et al., 2014], convolutional neural networks (CNN) [Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014], and syntactic-based compositional model [Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015].",
      "Given a text sequence x = {x1, x2, \u00b7 \u00b7 \u00b7 , xT }, we \ufb01rst use a lookup layer to get the vector representation (word embed- ding) xi of each word xi. Then we can use CNN or RNN to calculate the hidden state hi of each position i. The \ufb01nal rep- resentation of a sentence could be either the \ufb01nal hidden state of the RNN or the max (or average) pooling from all hidden states of RNN (or CNN). We use bidirectional LSTM (BiLSTM) to gain some de- pendency between adjacent words. The update rule of each LSTM unit can be written as follows: \u2212\u2192 ht = LSTM(\u2212\u2192 h t\u22121, xt, \u03b8p), (1) \u2190\u2212 ht = LSTM(\u2190\u2212 h t+1, xt, , \u03b8p), (2) h = 1 T T X t=1 \u2212\u2192 ht \u2295\u2190\u2212 ht, (3) where \u03b8p represents all the parameters of BiLSTM.",
      "The rep- resentation of the whole sequence is the average of the hidden states of all the positions, where \u2295denotes the concatenation operation. 2.2 Shared-Private Scheme in Multi-task Learning Multi-task Learning [Caruana, 1997] utilizes the correlation between related tasks to improve classi\ufb01cation by learning tasks in parallel, which has been widely used in various natu- ral language processing tasks, such as text classi\ufb01cation [Liu et al., 2016], semantic role labeling [Collobert and Weston, 2008], machine translation [Firat et al., 2016], and so on. To facilitate this, we give some explanation for notations used in this paper.",
      "To facilitate this, we give some explanation for notations used in this paper. Formally, we refer to Dk as a dataset with Nk samples for task k. Speci\ufb01cally, Dk = {(x(k) i , y(k) i )}Nk i=1 (4) where x(k) i and y(k) i denote a sentence and corresponding la- bel for task k. Shared-Private Scheme A common information sharing scheme is to divide the feature spaces into two parts: one is used to store task-speci\ufb01c features, the other is used to cap- ture task-invariant features. As shown in Figure 1(a) and 1(b), there are two schemes: stacked shared-private (SSP) scheme and parallel shared-private (PSP) scheme. In stacked scheme, the output of the shared LSTM layer is fed into the private LSTM layer, whose output is the \ufb01nal task-speci\ufb01c sentence representation. In parallel scheme, the \ufb01nal task-speci\ufb01c sentence representation is the concatena- tion of outputs from the shared LSTM layer and the private LSTM layer.",
      "In parallel scheme, the \ufb01nal task-speci\ufb01c sentence representation is the concatena- tion of outputs from the shared LSTM layer and the private LSTM layer. Task-Speci\ufb01c Output Layer For a sentence x(k) and its label y(k) in task k, its \ufb01nal representation is ultimately fed into the corresponding task-speci\ufb01c softmax layer for classi- \ufb01cation or other tasks. \u02c6y(k) = softmax(W (k)h(k) + b(k)) (5) where \u02c6y(k) is prediction probabilities; h(k) is the \ufb01nal task-speci\ufb01c representation; W (k) and b(k) are task-speci\ufb01c weight matrix and bias vector respectively. The total loss Ltask can be computed as: LAll = K X k=1 \u03b1kLT ask(\u02c6y(k), y(k)) (6) where \u03b1k (usually set to 1) is the weights for each task k respectively; LT ask(\u02c6y, y) is the cross-entropy of the predicted and true distributions.",
      "3 A New Information-Sharing Scheme for Multi-task Learning The key factor of multi-task learning is the information shar- ing scheme in latent representation space. Different from the traditional shared-private scheme, we introduce a new scheme for multi-task learning on NLP tasks, in which the sentence representation is shared among all the tasks, the task-speci\ufb01c information is selected by attention mechanism. In a certain task, not all information of a sentence is useful for the task, therefore we just need to select the key informa- tion from the sentence. Attention mechanism [Bahdanau et al., 2014; Mnih et al., 2014] is an effective method to select related information from a set of candidates. The attention",
      "\u2026 \u2026 \u02c6y(k) h1 \u03b11 (k) c(k) q(k) \u03b1 2 (k) \u03b1 3 (k) \u03b1T (k) h2 h3 x1 x2 x3 ... xT hT softmax Figure 2: Static Task-Attentive Sentence Encoding mechanism can effectively solve the capacity problem of se- quence models, thereby is widely used in many NLP tasks, such as machine translation [Luong et al., 2015], textual en- tailment [Zhao et al., 2016] and summarization [Rush et al., 2015]. 3.1 Static Task-Attentive Sentence Encoding We \ufb01rst introduce the static task-attentive sentence encod- ing model, in which the task query vector is a static learn- able parameter. As shown in Figure 2, our model consists of one shared BiLSTM layer and an attention layer. Formally, for a sentence in task k, we \ufb01rst use BiLSTM to calculate the shared representation [h1, \u00b7 \u00b7 \u00b7 , hT ].",
      "As shown in Figure 2, our model consists of one shared BiLSTM layer and an attention layer. Formally, for a sentence in task k, we \ufb01rst use BiLSTM to calculate the shared representation [h1, \u00b7 \u00b7 \u00b7 , hT ]. Then we use atten- tion mechanism to select the task-speci\ufb01c information from a generic task-independent sentence representation. Following [Luong et al., 2015], we use the dot-product attention to com- pute the attention distribution. We introduce a task-speci\ufb01c query vector q(k) to calculate the attention distribution \u03b1(k) over all positions. \u03b1(k) t = softmax(q(k)T ht), (7) where the task-speci\ufb01c query vector q(k) is a learned parame- ter. The \ufb01nal task-speci\ufb01c representation c(k) is summarized by c(k) = T X t=1 \u03b1(k) t ht. (8) At last, a task-speci\ufb01c fully connected layer followed by a softmax non-linear layer processes the task-speci\ufb01c context c(k) and predicts the probability distribution over classes.",
      "(8) At last, a task-speci\ufb01c fully connected layer followed by a softmax non-linear layer processes the task-speci\ufb01c context c(k) and predicts the probability distribution over classes. 3.2 Dynamic Task-Attentive Sentence Encoding Different from the static task-attentive sentence encoding model, the query vectors of the dynamic task-attentive sen- tence encoding model are generated dynamically. When each task belongs to a different domain, we can introduce an aux- iliary domain classi\ufb01er to predict the domain (or task) of the speci\ufb01c sentence. Thus, the domain information is also in- cluded in the shared sentence representation, which can be used to generate the task-speci\ufb01c query vector of attention. BiLSTM Domain  Attention Task  Attention x \u02c6y(DC) \u02c6y(k) Domain  Classi\ufb01er Task  Classi\ufb01er Figure 3: Dynamic Task-Attentive Sentence Encoding The original tasks and the auxiliary task of domain classi\ufb01- cation (DC) are joint learned in our multi-task learning frame- work.",
      "The query vector q(DC) of DC task is static and needs be learned in training phrase. The domain information is also selected with attention mechanism. \u03b1(DC) t = softmax(q(DC)T ht), (9) c(DC) = T X t=1 \u03b1(DC) t ht. (10) \u02c6y(DC) = softmax(W(DC)c(DC) + b(DC)), (11) where \u03b1(DC) is attention distribution of auxiliary DC task, and c(DC) is the attentive information for DC task, which is fed into the \ufb01nal classi\ufb01er to predict its domain \u02c6y(DC). Since c(DC) contains the domain information, we can use it to generate a more \ufb02exible query vector q(k) = Uc(DC) + b(k), (12) where U is a shared learnable weight matrix and b(k) is a task-speci\ufb01c bias vector. When we set U = 0, the dynamic query is equivalent to the static one. 4 Experiment In this section, we investigate the empirical performances of our proposed architectures on three experiments.",
      "When we set U = 0, the dynamic query is equivalent to the static one. 4 Experiment In this section, we investigate the empirical performances of our proposed architectures on three experiments. 4.1 Exp I: Sentiment Classi\ufb01cation We \ufb01rst conduct a multi-task experiment on sentiment classi- \ufb01cation. Dataset We use 16 different datasets from several popu- lar review corpora used in [Liu et al., 2017]. These datasets consist of 14 product review datasets and two movie review datasets. All the datasets in each task are partitioned randomly into training set, development set and testing set with the propor- tion of 70%, 10% and 20% respectively. The detailed statis- tics about all the datasets are listed in Table 1. Competitor Methods We compare our proposed two infor- mation sharing schemes, static attentive sentence encoding (SA-MTL) and dynamic attentive sentence encoding (DA- MTL), with the following multi-task learning frameworks.",
      "Dataset Train Dev. Test Avg. L Vocab. Books 1400 200 400 159 62K Elec. 1398 200 400 101 30K DVD 1400 200 400 173 69K Kitchen 1400 200 400 89 28K Apparel 1400 200 400 57 21K Camera 1397 200 400 130 26K Health 1400 200 400 81 26K Music 1400 200 400 136 60K Toys 1400 200 400 90 28K Video 1400 200 400 156 57K Baby 1300 200 400 104 26K Mag. 1370 200 400 117 30K Soft. 1315 200 400 129 26K Sports 1400 200 400 94 30K IMDB 1400 200 400 269 44K MR 1400 200 400 21 12K Table 1: Statistics of the 16 datasets.",
      "1315 200 400 129 26K Sports 1400 200 400 94 30K IMDB 1400 200 400 269 44K MR 1400 200 400 21 12K Table 1: Statistics of the 16 datasets. The columns 2-5 denote the number of samples in training, development and test sets. The last two columns represent the average length and vocabulary size of the corresponding dataset. \u2022 FS-MTL: This model is a combination of a fully shared BiLSTM and a classi\ufb01er. \u2022 SSP-MTL: This is the stacked shared-private model as shown in Figure 1(a) whose output of the shared BiL- STM layer is fed into the private BiLSTM layer. \u2022 PSP-MTL: The is the parallel shared-private model as shown in Figure 1(b). The \ufb01nal sentence representation is the concatenation of both private and shared BiLSTM. \u2022 ASP-MTL: This model is proposed by [Liu et al., 2017] based on PSP-MTL with uni-directional LSTM.",
      "The \ufb01nal sentence representation is the concatenation of both private and shared BiLSTM. \u2022 ASP-MTL: This model is proposed by [Liu et al., 2017] based on PSP-MTL with uni-directional LSTM. The model uses adversarial training to separate task-invariant and task-speci\ufb01c features from different tasks. Hyperparameters We initialize word embeddings with the 200d GloVe vectors (840B token version, [Pennington et al., 2014]). The other parameters are initialized by randomly sampling from uniform distribution in [-0.1, 0.1]. The mini- batch size is set to 32. For each task, we take hyperparameters which achieve the best performance on the development set via a small grid search. We use ADAM optimizer [Kingma and Ba, 2014] with the learning rate of 0.001. The BiLSTM models have 200 dimensions in each direction, and dropout with probability of 0.5. During the training step of multi-task models, we select different tasks randomly.",
      "The BiLSTM models have 200 dimensions in each direction, and dropout with probability of 0.5. During the training step of multi-task models, we select different tasks randomly. After the training step, we \ufb01x the parameters of the shared BiLSTM and \ufb01ne tune every task. Results Table 2 shows the performances of the different methods. From the table, we can see that the performances of most tasks can be improved with the help of multi-task learning. FS-MTL shows the minimum performance gain from multi-task learning since it puts all private and shared information into a uni\ufb01ed space. SSP-MTL and PSP-MTL achieve similar performance and are outperformed by ASP- MTL which can better separate the task-speci\ufb01c and task- invariant features by using adversarial training. Our proposed models (SA-MTL and DA-MTL) outperform ASP-MTL be- cause we model a richer representation from these 16 tasks. Figure 4: Convergence on the development datasets.",
      "Our proposed models (SA-MTL and DA-MTL) outperform ASP-MTL be- cause we model a richer representation from these 16 tasks. Figure 4: Convergence on the development datasets. Compared to SA-MTL, DA-MTL achieves a further improve- ment of +0.6 accuracy with the help of the dynamic and \ufb02ex- ible query vector. It is noteworthy that our models are also space ef\ufb01cient since the task-speci\ufb01c information is extracted by using only a query vector, instead of a BiLSTM layer in the shared-private models. We also present the convergence properties of our mod- els on the development datasets compared to other multi-task models in Figure 4. We can see that PSP-MTL converges much more slowly than the rest four models because each task-speci\ufb01c classi\ufb01er should consider the output of shared layer which is quite unstable during the beginning of train- ing phrase. Moreover, bene\ufb01t from the attention mechanism which is useful in feature extraction, SA-TML and DA-MTL are converged much more quickly than the rest of models.",
      "Moreover, bene\ufb01t from the attention mechanism which is useful in feature extraction, SA-TML and DA-MTL are converged much more quickly than the rest of models. Visualization Since all the tasks share the same sentence encoding layer, the query vector q of each task determines which part of the sentence to attend. Thus, similar tasks should have the similar query vectors. Here we simply cal- culate the Frobenius norm of each pair of tasks\u2019 q as the sim- ilarity. Figure 5 shows the similarity matrix of different task\u2019s query vector q in static attentive model. A darker cell means the higher similarity of the two task\u2019s q. Since the cells in the diagnose of the matrix denotes the similarity of one task, we leave them blank because they are meaningless. It\u2019s easy to \ufb01nd that q of \u201cDVD\u201d, \u201cVideo\u201d and \u201cIMDB\u201d have very high similarity. It makes sense because they are all reviews related to movie. However, another movie review \u201cMR\u201d has very low similarity to these three task. It\u2019s probably that the text in \u201cMR\u201d is very short that makes it different from these tasks.",
      "It makes sense because they are all reviews related to movie. However, another movie review \u201cMR\u201d has very low similarity to these three task. It\u2019s probably that the text in \u201cMR\u201d is very short that makes it different from these tasks. The similarity of q from \u201cBooks\u201d and \u201cVideo\u201d is also very high because these two datasets share a lot of similar senti- ment expressions. As shown in Figure 6, we also show the attention distribu- tions on a real example selected from the book review dataset. This piece of text involves two domains. The review is nega- tive in the book domain while it is positive from the perspec- tive of movie review. In our SA-MTL model, the \u201cBooks\u201d review classi\ufb01er from SA-MTL focus on the negative aspect of the book and evaluate the text as negative. In contrast, the \u201cDVD\u201d review classi\ufb01er focuses on the positive part of the movie and produce the result as positive. In case of DA-MTL,",
      "Task Single Task Multiple Tasks BiLSTM att-BiLSTM Avg. FS-MTL SSP-MTL PSP-MTL ASP-MTL* SA-MTL DA-MTL Books 81.0 82.0 81.5 84.0 85.5 85.5 87.0 86.8 88.5 Electronics 81.8 83.0 82.4 84.8 86.8 87.3 89.0 87.5 89.0 DVD 83.3 83.0 83.1 85.0 85.3 84.5 87.4 87.3 88.0 Kitchen 80.8 80.3 80.5 87.0 86.5 87.5 87.2 89.3 89.0 Apparel 87.5 86.5 87.0 86.8 85.3 85.8 88.7 87.3 88.8 Camera 87.0 89.5 88.3 89.0 90.5 90.3 91.3 90.3 91.",
      "0 86.8 85.3 85.8 88.7 87.3 88.8 Camera 87.0 89.5 88.3 89.0 90.5 90.3 91.3 90.3 91.8 Health 87.0 84.3 83.0 88.5 88.3 87.5 88.1 88.3 90.3 Music 81.8 82.0 81.8 81.0 84.5 83.0 82.6 84.0 85.0 Toys 81.5 85.0 85.4 88.3 87.0 87.8 88.8 89.3 89.5 Video 83.0 83.5 83.3 85.0 87.3 88.0 85.5 88.5 89.5 Baby 86.3 86.0 86.1 89.0 88.3 90.0 89.8 88.8 90.5 Magazine 92.0 92.",
      "3 88.0 85.5 88.5 89.5 Baby 86.3 86.0 86.1 89.0 88.3 90.0 89.8 88.8 90.5 Magazine 92.0 92.0 92.0 92.0 92.3 92.8 92.4 92.0 92.0 Software 84.5 83.0 83.8 86.3 88.5 90.3 87.3 89.3 90.8 Sports 86.0 84.8 85.4 88.3 88.8 86.8 86.7 89.8 89.8 IMDB 82.5 83.5 83.0 82.3 84.0 84.5 85.8 87.5 89.8 MR 74.8 76.0 75.4 71.3 70.8 69.0 77.3 73.0 75.5 AVG.",
      "3 84.0 84.5 85.8 87.5 89.8 MR 74.8 76.0 75.4 71.3 70.8 69.0 77.3 73.0 75.5 AVG. 83.7 84.0 83.9 85.5(+1.6) 86.2(2.3) 86.2 (+2.3) 87.2(+3.3) 87.6 (+3.7) 88.2 (+4.3) # Param. 644 K \u00d7 16 645 K \u00d7 16 \u2013 644 K 16,074 K 10,972 K 5,490K 668 K 818 K Table 2: Performances on 16 tasks. The column of \u201cSingle Task\u201d includes bidirectional LSTM (BiLSTM), bidirectional LSTM with attention (att-BiLSTM) and the average accuracy of the two models. The column of \u201cMultiple Tasks\u201d shows several multi-task models. * is from [Liu et al., 2017] .",
      "The column of \u201cMultiple Tasks\u201d shows several multi-task models. * is from [Liu et al., 2017] . Figure 5: Similarity Matrix of Different Task\u2019s query vector qk the model \ufb01rst focuses on the two domain words \u201cbook\u201d and \u201cmovie\u201d and judge the text is a book review because \u201cbook\u201d has a higher weight. Then, the model dynamically generates a query q and focuses on the part of the book review in this text, thereby \ufb01nally predicting a negative sentiment. 4.2 Exp II: Transferability of Shared Sentence Representation With attention mechanism, the shared sentence encoder in our proposed models can generate more generic task-invariant representations, which can be considered as off-the-shelf knowledge and then be used for unseen new tasks. To test the transferability of our learned shared represen- tation, we also design an experiment shown in Table 3. The multi-task learning results are derived by training the \ufb01rst 6 tasks in general multi-task learning.",
      "To test the transferability of our learned shared represen- tation, we also design an experiment shown in Table 3. The multi-task learning results are derived by training the \ufb01rst 6 tasks in general multi-task learning. For transfer learning, we choose the last 10 tasks to train our model with multi-task learning, then the learned shared sentence encoding layer are kept frozen and transferred to train the \ufb01rst 6 tasks. SSP-MTL PSP-MTL SA-MTL DA-MTL Multi-task 83.12 83.25 84.38 86.96 Transfer 82.54 82.58 86.50 87.67 Table 3: Results of \ufb01rst 6 tasks with multi-task learning and transfer learning Results and Analysis As shown in Table 3, we can see that SA-MTL and DA-MTL achieves better transfer learning per- formances compared to SSP-MTL and PSP-MTL. The reason is that by using attention mechanism, richer information can be captured into the shared representation layer, thereby ben- e\ufb01ting the other task.",
      "The reason is that by using attention mechanism, richer information can be captured into the shared representation layer, thereby ben- e\ufb01ting the other task. 4.3 Exp III: Introducing Sequence Labeling as Auxiliary Task A good sentence representation should include its linguis- tic information. Therefore, we incorporate sequence label- ing task (such as POS Tagging and Chunking) as an auxiliary task into the multi-task learning framework, which is trained jointly with the primary tasks (the above 16 tasks of senti- ment classi\ufb01cation). The auxiliary task shares the sentence encoding layer with the primary tasks and connected to a pri- vate fully connected layer followed by a softmax non-linear layer to process every hidden state ht and predicts the labels. Dataset We use CoNLL 2000 [Sang and Buchholz, 2000] sequence labeling dataset for both POS Tagging and Chunk- ing tasks. There are 8774 sentences in training data, 500 sen- tences in development data and 1512 sentences in test data. The average sentence length is 24 and has a total vocabulary size as 17k. Results The experiment results are shown in Table 4.",
      "There are 8774 sentences in training data, 500 sen- tences in development data and 1512 sentences in test data. The average sentence length is 24 and has a total vocabulary size as 17k. Results The experiment results are shown in Table 4. We use the same hyperparameters and training procedure as the former experiments. The result shows that by leveraging auxiliary tasks, the performances of SA-MTL and DA-MTL achieve more improvement than PSP-MTL and SSP-MTL.",
      "(a) Attention of task \u201cBooks\u201d in SA-MTL, Output: Negative (b) Attention of task \u201cDVD\u201d in SA-MTL, Output: Positive I have not read the original version of this work , but the translation lacks originality and art . A beautiful story , but the writing style lacks grace and creativity . This is the only time I have liked a movie better than the book . Do yourself a favor and skip the book . The movie is quite beautiful and moving (c) Attention of auxiliary Task (Domain Classi\ufb01cation) in DA-MTL, Ouptut: Books (d) Attention of Task \u201cBooks\u201d in DA-MTL, Ouptut: Negative Figure 6: Attention Distributions of four classi\ufb01ers from two models on the same text SSP-MTL PSP-MTL SA-MTL DA-MTL Origin 86.2 86.2 87.59 88.22 + Chunking 86.94 86.29 88.62 88.85 + POS Tagging 86.83 86.16 88.52 89.04 Table 4: Average precision of multi-task models with auxiliary tasks.",
      "Visualization For further analysis, Figure 7 shows the at- tention distribution produced by models trained with and without Chunking task on two pieces of texts. In the \ufb01rst piece of text, both of the models attend to the \ufb01rst \u201clike\u201d be- cause it represents positive sentiment on the book. The model trained with Chunking task also labels the three \u201clike\u201d as \u2019B- VP\u2019 (beginning of verb phrase) correctly. However, in the second piece of text, the same work \u201clike\u201d denotes a preposi- tion and has no sentiment meaning. The model trained with- out Chunking task fails to tell the difference with the for- mer text and focuses on it and produces the result as posi- tive. Meanwhile, the model trained with Chunking task suc- cessfully labels the \u201clike\u201d as \u2019B-PP\u2019 (beginning of preposi- tional phrase) and pays little attention to it and produces the right answer as negative. This example shows how the model trained with auxiliary task helps the primary tasks. I really liked the time-span of story settings , and the mystery that was written down over 2000 years ago !",
      "This example shows how the model trained with auxiliary task helps the primary tasks. I really liked the time-span of story settings , and the mystery that was written down over 2000 years ago ! Great for people who like short story mysteries , and as a lead-in to authors you might like (a) Model trained without Chunking task, Output: Positive I really [liked] B-VP the time-span of story settings , and the mystery that was written down over 2000 years ago ! Great for people who [like] B-VP short story mysteries , and as a lead-in to authors you might [like] B-VP (b) Model trained with Chunking task, Output: Positive I was trying my hardest to be creept out . But like someone said , it read like a pre-teen novel (c) Model trained without Chunking task, Output: Positive I was trying my hardest to be creept out .",
      "But like someone said , it read like a pre-teen novel (c) Model trained without Chunking task, Output: Positive I was trying my hardest to be creept out . But [like] B-PP someone said , it read [like] B-PP a pre-teen novel (d) Model trained with Chunking task, Output: Negative Figure 7: Attention distributions of two example texts from models trained with and without Chunking task 5 Related Work Neural networks based multi-task learning has been proven effective in many NLP problems [Collobert and Weston, 2008; Glorot et al., 2011; Liu et al., 2016; Liu et al., 2017; Ruder, 2017] In most of these models, there exists a task- dependent private layer separated from the shared layer. The private layers play more important role in these models. Dif- ferent from them, our model encodes all information into a shared representation layer, and uses attention mechanism to select the task-speci\ufb01c information from the shared represen- tation layer.",
      "The private layers play more important role in these models. Dif- ferent from them, our model encodes all information into a shared representation layer, and uses attention mechanism to select the task-speci\ufb01c information from the shared represen- tation layer. Thus, our model can learn a better generic sen- tence representation, which also has a strong transferability. Some recent work have also proposed sentence represen- tation using attention mechanism. [Lin et al., 2017] uses a 2-D matrix, whose each row attending on a different part of the sentence, to represent the embedding. [Vaswani et al., 2017] introduces multi-head attention to jointly attend to in- formation from different representation subspaces at different positions. [Wang et al., 2017] introduces human reading time as attention weights to improve sentence representation. Dif- ferent from these work, we use attention vector to select the task-speci\ufb01c information from a shared sentence representa- tion. Thus the learned sentence representation is much more generic and easy to transfer information to new tasks.",
      "Dif- ferent from these work, we use attention vector to select the task-speci\ufb01c information from a shared sentence representa- tion. Thus the learned sentence representation is much more generic and easy to transfer information to new tasks. 6 Conclusion In this paper, we propose a new information-sharing scheme for multi-task learning, which uses attention mechanism to select the task-speci\ufb01c information from a shared sentence encoding layer. We conduct extensive experiments on 16 dif- ferent sentiment classi\ufb01cation tasks, which demonstrates the bene\ufb01ts of our models. Moreover, the shared sentence en- coding model can be transferred to other tasks, which can be further boosted by introducing auxiliary tasks.",
      "References [Bahdanau et al., 2014] D. Bahdanau, K. Cho, and Y. Ben- gio. Neural machine translation by jointly learning to align and translate. ArXiv e-prints, September 2014. [Bengio et al., 2003] Yoshua Bengio, R\u00b4ejean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilis- tic language model. The Journal of Machine Learning Re- search, 2003. [Caruana, 1997] Rich Caruana. Multitask learning. Machine learning, 28(1):41\u201375, 1997. [Chung et al., 2014] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evalua- tion of gated recurrent neural networks on sequence mod- eling. arXiv preprint arXiv:1412.3555, 2014. [Collobert and Weston, 2008] Ronan Collobert and Jason Weston.",
      "Empirical evalua- tion of gated recurrent neural networks on sequence mod- eling. arXiv preprint arXiv:1412.3555, 2014. [Collobert and Weston, 2008] Ronan Collobert and Jason Weston. A uni\ufb01ed architecture for natural language pro- cessing: Deep neural networks with multitask learning. In Proceedings of ICML, 2008. [Collobert et al., 2011] Ronan Collobert, Jason Weston, L\u00b4eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493\u20132537, 2011. [Firat et al., 2016] Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. Multi-way, multilingual neural machine translation with a shared attention mechanism. arXiv preprint arXiv:1601.01073, 2016.",
      "[Firat et al., 2016] Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. Multi-way, multilingual neural machine translation with a shared attention mechanism. arXiv preprint arXiv:1601.01073, 2016. [Glorot et al., 2011] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sen- timent classi\ufb01cation: A deep learning approach. In ICML, pages 513\u2013520, 2011. [Kalchbrenner et al., 2014] Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network for modelling sentences. In Proceedings of ACL, 2014. [Kim, 2014] Yoon Kim. Convolutional neural networks for sentence classi\ufb01cation. arXiv preprint arXiv:1408.5882, 2014. [Kingma and Ba, 2014] Diederik P. Kingma and Jimmy Ba.",
      "Convolutional neural networks for sentence classi\ufb01cation. arXiv preprint arXiv:1408.5882, 2014. [Kingma and Ba, 2014] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2014. [Lin et al., 2017] Zhouhan Lin, Minwei Feng, C\u00b4\u0131cero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self- attentive sentence embedding. Proceedings of ICLR, 2017. [Liu et al., 2016] Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. Recurrent neural network for text classi\ufb01cation with multi-task learning. In Proceedings of IJCAI, pages 2873\u20132879, 2016. [Liu et al., 2017] Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. Adversarial multi-task learning for text classi\ufb01- cation. In ACL, 2017.",
      "[Liu et al., 2017] Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. Adversarial multi-task learning for text classi\ufb01- cation. In ACL, 2017. [Luong et al., 2015] Thang Luong, Hieu Pham, and Christo- pher D. Manning. Effective approaches to attention-based neural machine translation. In Proceedings of EMNLP, pages 1412\u20131421, 2015. [Mikolov et al., 2013] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed rep- resentations of words and phrases and their composition- ality. In NIPS, 2013. [Mnih et al., 2014] Volodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recurrent models of visual attention. In NIPS, 2014. [Pennington et al., 2014] Jeffrey Pennington, Richard Socher, and Christopher D. Manning.",
      "Recurrent models of visual attention. In NIPS, 2014. [Pennington et al., 2014] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In Proceedings of EMNLP, pages 1532\u20131543, 2014. [Ruder, 2017] Sebastian Ruder. An overview of multi- task learning in deep neural networks. arXiv preprint, arXiv:1706.05098, 2017. [Rush et al., 2015] Alexander M. Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive sentence summarization. In EMNLP, pages 379\u2013389, 2015. [Sang and Buchholz, 2000] Erik F. Tjong Kim Sang and Sabine Buchholz. Introduction to the conll-2000 shared task chunking. In CoNLL, 2000.",
      "[Sang and Buchholz, 2000] Erik F. Tjong Kim Sang and Sabine Buchholz. Introduction to the conll-2000 shared task chunking. In CoNLL, 2000. [Socher et al., 2013] Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, 2013. [Sutskever et al., 2014] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neu- ral networks. In NIPS, 2014. [Tai et al., 2015] Kai Sheng Tai, Richard Socher, and Christopher D Manning. Improved semantic representa- tions from tree-structured long short-term memory net- works. arXiv preprint arXiv:1503.00075, 2015.",
      "[Tai et al., 2015] Kai Sheng Tai, Richard Socher, and Christopher D Manning. Improved semantic representa- tions from tree-structured long short-term memory net- works. arXiv preprint arXiv:1503.00075, 2015. [Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. NPIS, 2017. [Wang et al., 2017] Shaonan Wang, Jiajun Zhang, and Chengqing Zong. Learning sentence representation with guidance of human attention. In IJCAI, pages 4137\u20134143, 2017. [Zhao et al., 2016] Kai Zhao, Liang Huang, and Mingbo Ma. Textual entailment with structured attentions and compo- sition. COLING, 2016.",
      "[Zhao et al., 2016] Kai Zhao, Liang Huang, and Mingbo Ma. Textual entailment with structured attentions and compo- sition. COLING, 2016. [Zhu et al., 2015] Xiaodan Zhu, Parinaz Sobihani, and Hongyu Guo. Long short-term memory over recursive structures. In ICML, 2015."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1804.08139.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":8317,
  "avg_doclen":176.9574468085,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1804.08139.pdf"
    }
  }
}