[
  "A MULTIMODAL DEEP LEARNING APPROACH FOR NAMED ENTITY RECOGNITION FROM SOCIAL MEDIA A PREPRINT Meysam Asgari-Chenaghlu \u2217 Department of Computer Engineering University of Tabriz Tabriz, Iran m.asgari@tabrizu.ac.ir M.Reza Feizi-Derakhshi Department of Computer Engineering University of Tabriz Tabriz, Iran mfeizi@tabrizu.ac.ir Leili Farzinvash Department of Computer Engineering University of Tabriz Tabriz, Iran l.farzinvash@tabrizu.ac.ir M. A. Balafar Department of Computer Engineering University of Tabriz Tabriz, Iran balafarila@tabrizu.ac.ir Cina Motamed Laboratoire d\u2019Informatique Signal et Image de la C\u00f4te d\u2019Opale Universit\u00e9 Littoral C\u00f4te d\u2019Opale Calais, France cina.motamed@univ-littoral.fr December 17, 2021 ABSTRACT Named Entity Recognition (NER) from social media posts is a challenging task. User generated content that forms the nature of social media, is noisy and contains grammatical and linguistic errors.",
  "User generated content that forms the nature of social media, is noisy and contains grammatical and linguistic errors. This noisy content makes it much harder for tasks such as named entity recognition. We propose two novel deep learning approaches utilizing multimodal deep learning and Transformers. Both of our approaches use image features from short social media posts to provide better results on the NER task. On the \ufb01rst approach, we extract image features using InceptionV3 and use fusion to combine textual and image features. This presents more reliable name entity recognition when the images related to the entities are provided by the user. On the second approach, we use image features combined with text and feed it into a BERT like Transformer. The experimental results, namely, the precision, recall and F1 score metrics show the superiority of our work compared to other state-of-the-art NER solutions. Keywords Deep Learning \u00b7 Natural Language Processing \u00b7 Social Media \u00b7 Named Entity Recognition \u00b7 Multimodal Learning \u00b7 Transformer 1 Introduction A common social media delivery system such as Twitter supports various media types like video, image and text. This media allows users to share their short posts called Tweets.",
  "Keywords Deep Learning \u00b7 Natural Language Processing \u00b7 Social Media \u00b7 Named Entity Recognition \u00b7 Multimodal Learning \u00b7 Transformer 1 Introduction A common social media delivery system such as Twitter supports various media types like video, image and text. This media allows users to share their short posts called Tweets. Users are able to share their tweets with other users that are usually following the source user. Hovewer there are rules to protect the privacy of users from unauthorized access to their timeline [1]. The very nature of user interactions on Twitter micro-blogging social media is oriented towards their \u2217Corresponding author arXiv:2001.06888v3  [cs.CL]  12 Jul 2020",
  "A PREPRINT - DECEMBER 17, 2021 Figure 1: A Tweet containing Image and Text: Geoffrey Hinton and Demis Hassabis are referred in text and respective images are provided with Tweet. daily life, \ufb01rst witness news-reporting and engaging in various events (sports, political stands, etc.) are examples of such activity. According to studies, news in twitter is propagated and reported faster than conventional news media [2]. Thus, extracting \ufb01rst hand news and entities occurring in this fast and versatile online media gives valuable information. However, abridged and noisy content of Tweets makes it even more dif\ufb01cult and challenging for tasks such as named entity recognition and information retrieval [3]. Named entity recognition task on the scope of social media is very important because many of related tasks directly or indirectly depend on it. This important tool can improve many data analytic and data scienti\ufb01c approaches for different streaming data analysis on various social media platforms. For example, detection of events, hot topics or trending topics from a social media can be done by many methods and systems [4] while a good named entity recognizer can extract the underlying entities [5].",
  "For example, detection of events, hot topics or trending topics from a social media can be done by many methods and systems [4] while a good named entity recognizer can extract the underlying entities [5]. Having the entities and the events at hand, one can easily infer any related information about a person or an entity occurring inside an event. The task of tracking and recovering information from social media posts is a concise de\ufb01nition of information retrieval in social media [6, 7]. However many challenges are blocking useful solutions to this issue, namely, the noisy nature of user generated content and the perplexity of words used in short posts. Sometimes different entities are called the same, for example \"Micheal Jordan\" refers to a basketball player and also a computer scientist in the \ufb01eld of arti\ufb01cial intelligence. The only thing that divides both of these is the context in which the entity appeared. If the context refers to something related to AI, the reader can conclude \"Micheal Jordan\" is the scientist, and if the context is refers to sports and basketball then he is the basketball player.",
  "The only thing that divides both of these is the context in which the entity appeared. If the context refers to something related to AI, the reader can conclude \"Micheal Jordan\" is the scientist, and if the context is refers to sports and basketball then he is the basketball player. The task of distinguishing between different named entities that appear to have the same textual appearance is called named entity disambiguation. There is more useful data on the subject rather than on plain text. For example, images and visual data are more descriptive than just text for tasks such as named entity recognition and disambiguation [8] while some methods only use the textual data [9]. The provided extra information as input is closely related to the textual data. As a clear example, \ufb01gure 1 shows a tweet containing an image and the related text. The combination of these multimodal data in order to achieve better performance in NLP related tasks is a promising alternative explored recently. Using such well provided data helps the model to have extra features which are useful for NER.",
  "The combination of these multimodal data in order to achieve better performance in NLP related tasks is a promising alternative explored recently. Using such well provided data helps the model to have extra features which are useful for NER. An NLP task such as named entity recognition in social media is a most challenging task because users tend to invent, mistype and epitomize words. Sometimes these words correspond to named entities which makes the recognition task even more dif\ufb01cult [10]. In some cases, the context that carries the entity (surrounding words and related image) is more descriptive than the entity word presentation [11]. This further information, namely, the image related to the post, is more important where it contains no misspells and is straight forward into the topic of the post. In some cases, the context that carries the entity (surrounding words and related image) is more descriptive than the entity word presentation [11]. 2",
  "A PREPRINT - DECEMBER 17, 2021 To \ufb01nd a solution to the issues at hand, and keeping multimodal data in mind, recognition of named entities from social media has become a research interest which utilizes image compared to NER task in a conventional text. Researchers in this \ufb01eld have tried to propose multimodal architectures based on deep neural networks with multimodal input that are capable of combining text and image [12, 11, 13]. This multimodal combination can be very misleading when the input image is irrelevant to the text or the textual data in a short sentence form has many out of vocabulary (OoV) tokens. Such inef\ufb01ciencies are another barrier for the model to provide better results. In this paper, we draw a better solution in terms of performance by proposing two novel models. One is called CWI (Character-Word-Image model) and the other is based on Transformers. We used multimodal deep neural network combined with Transformers to overcome the NER task in micro-blogging social media. The \ufb01rst model utilizes both of the character and the word level features by using various inner layers.",
  "We used multimodal deep neural network combined with Transformers to overcome the NER task in micro-blogging social media. The \ufb01rst model utilizes both of the character and the word level features by using various inner layers. This combination increases the understanding of model over OoV tokens. The skip connection used in character feature extraction when it is combined with other feature extractors, provides more accurate results. We also present use of Transformers in combination with image features. Compared to the base BERT model, our proposed model provides much better results in terms of evaluation metrics. However, these two models are useful in different use-cases; For example in any use-case that the person tag and metrics related to it, is prioritized over other ones, Transformer based model is more accurate; In case of a more general use-case, the \ufb01rst approach is preferred. The rest of the paper is organized as follows: section 2 provides an insight view of previous methods; section 3 describes the method we propose; section 4 shows experimental evaluation and test results; \ufb01nally, section 5 concludes the whole article.",
  "The rest of the paper is organized as follows: section 2 provides an insight view of previous methods; section 3 describes the method we propose; section 4 shows experimental evaluation and test results; \ufb01nally, section 5 concludes the whole article. 2 Related Work Many algorithms and methods have been proposed to detect, classify or extract information from a single type of data such as audio, text, image, etc. However, in the case of social media, data comes in a variety of types such as text, image, video or audio in a bounded style. Most of the time, it is very common to caption a video or image with textual information. This information about the video or image can refer to a person, location and etc. From a multimodal learning perspective, jointly computing such data is considered to be more valuable in terms of representation and evaluation. Named entity recognition task, on the other hand, is the task of recognizing named entities from a sentence or group of sentences in a document format. Named entity is formally de\ufb01ned as a word or phrase that clearly identi\ufb01es an item from set of other similar items [14, 15].",
  "Named entity is formally de\ufb01ned as a word or phrase that clearly identi\ufb01es an item from set of other similar items [14, 15]. Equation 1 expresses a sequence of tokens. ls = \u27e8w1, w2, . . . , wn\u27e9, (1) o = \u27e8Is, Ie, t\u27e9, (2) o = \u27e8T1, T2, . . . , Tn\u27e9. (3) From this equation, the NER task is de\ufb01ned as the recognition of tokens that correspond to interesting items. These items from natural language processing perspective are known as named entity categories; BIO2 proposes four major categories, namely, organization, person, location and miscellaneous [16]. From the biomedical domain, gene, protein, drug and disease names are known as named entities [17, 18]. The output of NER task is formulated in 2. Is \u2208[1, N] and Ie \u2208[1, N] is the start and end indices of each named entity and t is named entity type [19].",
  "The output of NER task is formulated in 2. Is \u2208[1, N] and Ie \u2208[1, N] is the start and end indices of each named entity and t is named entity type [19]. BIO2 tagging for named entity recognition is de\ufb01ned in equation 3. Table 1 shows BIO2 tags and their respective meanings; B and I indicate beginning and inside of the entity respectively, while O shows the outside of it. Even though many tagging standards have been proposed for NER task, BIO is the foremost accepted by many real world applications [20]. A named entity recognizer gets s as input and provides entity-tags for each token. This sequential process requires information from the whole sentence rather than only tokens and for that reason, it is also considered to be a sequence tagging problem. Another analogous problem to this issue is part of speech tagging and some methods are capable of doing both [21]. However, in cases where noise is present and the input sequence has linguistic typos, many methods fail to overcome the problem. As an example, consider a sequence of tokens where a new token invented by social media users gets trended.",
  "However, in cases where noise is present and the input sequence has linguistic typos, many methods fail to overcome the problem. As an example, consider a sequence of tokens where a new token invented by social media users gets trended. This trending new word is misspelled and is used in a sequence along with other tokens in which the whole sequence does not follow known linguistic grammar. For this special case, classical methods and those which use engineered features do not perform well. Modern machine learning approaches such as deep learning and character or subword level models perfom better in such problems [22]. 3",
  "A PREPRINT - DECEMBER 17, 2021 Table 1: BIO Tags and their respective meaning. Begin End Description B-PER I-PER Person B-LOC I-LOC Location B-ORG I-ORG Organization B-MISC I-MISC Miscellaneous O O Outside of entity Using the sequence s itself or adding more information to it divides two approaches: unimodal and multimodal. Although many approaches for NER have been proposed and reviewing them is not in the scope of this article, we focus on foremost analogues classical and deep learning approaches for named entity recognition in two subsections. In subsection 2.1, unimodal approaches for named entity recognition are presented while in subsection 2.2, emerging multimodal solutions are described. 2.1 Unimodal Named Entity Recognition The recognition of named entities from only textual data (unimodal learning approach) is a well studied and explored research \ufb01eld. For a prominent example of this category, the Stanford NER is a widely used baseline for many applications [23]. The incorporation of non-local information in information extraction is proposed by the authors using Gibbs sampling.",
  "For a prominent example of this category, the Stanford NER is a widely used baseline for many applications [23]. The incorporation of non-local information in information extraction is proposed by the authors using Gibbs sampling. The conditional random \ufb01eld (CRF) approach used in this article, creates a chain of cliques, where each clique represents the probabilistic relationship between two adjacent states. Also, the Viterbi algorithm has been used to infer the most likely state in the CRF output sequence. Equation 4 shows the proposed CRF method. p(o|s) = nQ i=1 \u03c6i(oi\u22121, oi, s) P o\u2032\u2208o nQ i=1 \u03c6i(o\u2032 i\u22121, o\u2032 i, s) (4) where \u03c6 is the potential function. CRF \ufb01nds the most probable likelihood by modeling the input sequence of tokens s as a normalized product of feature functions. In a simpler explanation, CRF outputs the most probable tags that follow each other. For example, it is more likely to have an I-PER, O or any other that that starts with B- after B-PER rather than encountering tags that start with I-.",
  "In a simpler explanation, CRF outputs the most probable tags that follow each other. For example, it is more likely to have an I-PER, O or any other that that starts with B- after B-PER rather than encountering tags that start with I-. T-NER is another approach that is speci\ufb01cally aimed to answer NER task in twitter [24]. A set of algorithms in their original work have been published to answer tasks such as POS (part of speech tagging), named entity segmentation and NER. Labeled LDA has been used by the authors in order to outperform baseline in [25] for NER task. Their approach strongly relies on the dictionary, contextual and orthographic features. Deep learning techniques use distributed word or character representation rather than raw one-hot vectors. Most of this research in NLP \ufb01eld use pretrained word embeddings such as Word2Vec [26], GloVe [27] or fastText [28]. These low dimensional real valued dense vectors have proved to provide better representation for words compared to one-hot vector or other space vector models.",
  "Most of this research in NLP \ufb01eld use pretrained word embeddings such as Word2Vec [26], GloVe [27] or fastText [28]. These low dimensional real valued dense vectors have proved to provide better representation for words compared to one-hot vector or other space vector models. The combination of word embedding along with bidirectional long-short term memory (LSTM) neural networks are examined in [21]. The authors also propose to add a CRF layer at the end of their neural network architecture in order to preserve output tag relativity. Utilization of recurrent neural networks (RNN) provides better sequential modeling over data. However, only using sequential information does not result in major improvements because these networks tend to rely on the most recent tokens. Instead of using RNN, authors used LSTM. The long and short term memory capability of these networks helps them to keep in memory what is important and forget what is not necessary to remember. Equation 5 formulates forget-gate of an LSTM neural network, eq. 6 shows input-gate, eq. 7 notes output-gate and eq. 8 presents memory-cell. Finally, eq.",
  "Equation 5 formulates forget-gate of an LSTM neural network, eq. 6 shows input-gate, eq. 7 notes output-gate and eq. 8 presents memory-cell. Finally, eq. 9 shows the hidden part of an LSTM unit [29, 30]. 4",
  "A PREPRINT - DECEMBER 17, 2021 Scaled Dot-Product Attention FF FF FF Scaled Dot-Product Attention FF FF FF Scale Softmax MatMul Q K V MatMul Scaled Dot-Product Attention Q K V FF FF FF h Concat FF Figure 2: Scaled dot-product attention mechanism. lft = \u03c3g(Wfxt + Ufht\u22121 + bf), (5) it = \u03c3g(Wixt + Uiht\u22121 + bi), (6) ot = \u03c3g(Woxt + Uoht\u22121 + bo), (7) ct = ft \u25e6ct\u22121 + it \u25e6\u03c3c(Wcxt + Ucht\u22121 + bc), (8) ht = ot \u25e6\u03c3h(ct). (9) For all these equations, \u03c3 is activation function (sigmoid or tanh are commonly used for LSTM) and \u25e6is concatenation operation. W and U are weights and b is the bias which should be learned over training process. LSTM is useful for capturing the relation of tokens in a forward sequential form; However, in natural language processing tasks, it is required to know the upcoming token.",
  "W and U are weights and b is the bias which should be learned over training process. LSTM is useful for capturing the relation of tokens in a forward sequential form; However, in natural language processing tasks, it is required to know the upcoming token. To overcome this problem, the authors have used a backward and forward LSTM combining the output of both. In a different approach, character embedding followed by a convolution layer is proposed in [31] for sequence labeling. The utilized architecture is followed by a bidirectional LSTM layer that ends in a CRF layer. Character embedding is a useful technique that the authors tried to use it in a combination with word embedding. Character embedding with the use of convolution as feature extractor from character level, captures relations between characters that form a word and reduces spelling noise. It also helps the model to have an embedding when pretrained word embedding is empty or initialized as random for new words. These words are encountered when they were not present in the training set. Thus, in the test phase, the model fails to provide a useful embedding.",
  "It also helps the model to have an embedding when pretrained word embedding is empty or initialized as random for new words. These words are encountered when they were not present in the training set. Thus, in the test phase, the model fails to provide a useful embedding. The NLP revolution of \"Attention is all you need\" was a game changer that eliminated need for any LSTM like sequential methods and replaced it with the scaled dot-product attention and positional encoding in Transformer stacks [32]. After this new research, many of researchers for various NLP tasks have used the Transformer paradigm; BERT, XLNet, ALBERT and T5 are examples of it [33, 34, 35, 36], however, there are many other related works too [37]. The foundation of these methods starts from tokenization and end at training on very huge data with a huge processing power. The tokenization part is done with Byte Pair Encoding (BPE) generally. The idea of utilizing BPE is novel itself in generating tokens even if it was proposed years ago for text compression [38]. The motivation behind using BPE is having better subword parts instead of words or characters [39].",
  "The tokenization part is done with Byte Pair Encoding (BPE) generally. The idea of utilizing BPE is novel itself in generating tokens even if it was proposed years ago for text compression [38]. The motivation behind using BPE is having better subword parts instead of words or characters [39]. Figure 2 shows the scaled dot-product attention and the multihead attention mechanism [32]. The attention mechanism has many forms and related studies in \ufb01elds of machine translation reviewed its effects on the translation task [40]. The transformer architecture proposed in [32] makes use of scaled dot-product attention that is computed using three vectors of Query, Key and Value (Q,K,V). Equation 10 shows this attention form. Attention(Q, K, V ) = softmax(QKT \u221adk )V (10) The denominator part of this equation, \u221adk is the scale part, proposed in the original article based on the embedding size. The rest of the equation is identical to the \ufb01gure 2.",
  "Attention(Q, K, V ) = softmax(QKT \u221adk )V (10) The denominator part of this equation, \u221adk is the scale part, proposed in the original article based on the embedding size. The rest of the equation is identical to the \ufb01gure 2. Attention head on the other hand, is where scaled dot-product attention units are used in a multi-way, but before using this attention type, a feed-forward (FF as shown in the \ufb01gure) is applied to each input. A Transformer, is simply a combination of multi-head attention units and feed-forward neural networks. Stacks of Transformer units in encoder and decoder part make a transformer based architecture. However, for 5",
  "A PREPRINT - DECEMBER 17, 2021 BERT [CLS] SW0 SW1 SWn SWn-1 E[CLS] E0 E1 En En-1 ... ... FF FF FF FF T0 T1 Tn-1 Tn [SEP] E[SEP] Figure 3: Transformer based NER proposed in [41]. many tasks, this architecture is useful. In the case of our study, a typical named entity recognizer architecture based on the Transformer is shown in \ufb01gure 3 [41]. The output embeddings of the last decoder or encoder part is used for generating \ufb01nal NER tags. 2.2 Multimodal Named Entity Recognition Multimodal learning has become an emerging research interest and with the rise of deep learning techniques, it has become more visible in different research areas ranging from medical imaging to image segmentation and natural language processing [42, 43, 44, 45, 46, 47, 48, 49, 50, 12, 51, 52, 53, 54, 55, 56, 57, 58, 59].",
  "On the other hand, very little research has been focused on the extraction of named entities with joint image and textual data concerning short and noisy content [60, 61, 12, 11] while several studies have been explored in textual named entity recognition using neural models [62, 63, 21, 64, 31, 65, 13, 66]. State-of-the-art methods have shown acceptable evaluation on structured and well formatted short texts. Techniques based on deep learning such as utilization of convolutional neural networks [66, 63], recurrent neural networks [64] and long short term memory neural networks [31, 21] are aimed to solve NER problem. The multimodal named entity recognizers can be categorized in two categories based on the tasks at hand, one tries to improve NER task with the utilization of visual data [60, 11, 61], and the other tries to give further information about the task at hand such as disambiguation of named entities [12]. We will refer to both of these tasks as MNER2.",
  "We will refer to both of these tasks as MNER2. To have a better understanding of MNER, equation 11 formulates the available multimodal data while equations 2 and 3 are true for this task. s\u2032 = \u27e8i, w1, w2, . . . , wn\u27e9 (11) i refers to image and the rest goes same as equation 1 for word token sequence. In [61] pioneering research was conducted using feature extraction from both image and textual data. The extracted features were fed to decision trees in order to output the named entity classes. Researchers have used multiple datasets ranging from buildings to human face images to train their image feature extractor (object detector and k-means clustering) and a text classi\ufb01er has been trained on texts acquired from DBPedia. Researchers in [60] proposed a MNER model with regards to triplet embedding of words, characters and image. Modality attention applied to this triplet indicates the importance of each embedding and their impact on the output while reducing the impact of irrelevant modals.",
  "Researchers in [60] proposed a MNER model with regards to triplet embedding of words, characters and image. Modality attention applied to this triplet indicates the importance of each embedding and their impact on the output while reducing the impact of irrelevant modals. Modality attention layer is applied to all embedding vectors for each modal, however the investigation of \ufb01ne-grained attention mechanism is still unclear [67]. The proposed method with Inception feature extraction [68] and pretrained GloVe word vectors shows good results on the dataset that the authors aggregated from Snapchat3. This method shows around 0.5 for precision and F-measure for four entity types (person, location, organization and misc) while for segmentation tasks (distinguishing between a named entity and a non-named entity) it shows around 0.7 for the metrics mentioned. An adaptive co-attention neural network with four generations are proposed in [11]. The adaptive co-attention part is similar to the multimodal attention proposed in [60] that enabled the authors to have better results over the dataset 2Multimodal Named Entity Recognizer 3A multimedia messaging application 6",
  "A PREPRINT - DECEMBER 17, 2021 Bidirectional LSTM InceptionV3 Image Feature Extractor W0 W1 W2 Wn s =\u2329 w0, w1, w2, ..., wn \u232a Image ... C0 C1 Cn ... C0 C1 Cn ... C0 C1 Cn ... C0 C1 Cn ... ... fastText (300) GloVe (200) Conv (16x2)+ MP Conv (32x3)+ MP Conv (64x4)+ MP Conv (64x4) Joint Word Embedding (500) Conv (32x3) Conv (16x2) h0 h1 h2 hn ... h0 h1 h2 hn ... Bidirectional LSTM Select Top 5 Feature Embedding h0 h1 h2 hn ... Forward LSTM TD + GN + SineRELU h0 h1 h2 hn ... h0 h1 h2 hn ... Conditional Random Field T0 T1 T2 T3 Tn ... Text Character Embedding Figure 4: Proposed CWI Model: Character (left),",
  ". Forward LSTM TD + GN + SineRELU h0 h1 h2 hn ... h0 h1 h2 hn ... Conditional Random Field T0 T1 T2 T3 Tn ... Text Character Embedding Figure 4: Proposed CWI Model: Character (left), Word (middle) and Image (right) feature extractors combined by bidirectional long-short term memory and the conditional random \ufb01eld at the end. they collected from Twitter. In their main proposal, convolutional layers are used for word representation, BiLSTM is utilized to combine word and character embeddings and an attention layer combines the best of the triplet (word, character and image features). VGG-Net16 [69] is used as a feature extractor for the image while the impact of other deep image feature extractors on the proposed solution is unclear, however the results show its superiority over related unimodal methods. 3 The Proposed Approach In the present work, we propose two different approaches for the NER problem. First we propose the CWI in subsection 3.1 and in subsection 3.2 we demonstrate our second approach, the multimodal transformer.",
  "3 The Proposed Approach In the present work, we propose two different approaches for the NER problem. First we propose the CWI in subsection 3.1 and in subsection 3.2 we demonstrate our second approach, the multimodal transformer. CWI is based on character- word-image features extracted using deep neural network and the multimodal transformer is utilizing transformer combined with image features. Both of these two use the same set of inputs, sentence and the related image from social media posts. For the transformer approach, we used a BERT like transformer that gets the image features extracted using InceptionV3. 3.1 CWI: Character-Word-Image In the present work, we propose multimodal deep approach (CWI) that is able to handle noise by co-learning semantics from three modalities, character, word and image. Our method is composed of three parts, convolutional character embedding, joint word embedding (fastText-GloVe) and InceptionV3 image feature extraction [68, 28, 27]. Figure 4 shows the CWI architecture in more detail. Character Feature Extraction shown in the left part of \ufb01gure 4 is a composition of six layers.",
  "Figure 4 shows the CWI architecture in more detail. Character Feature Extraction shown in the left part of \ufb01gure 4 is a composition of six layers. Each se- quence of words from a single tweet, \u27e8w1, w2, . . . , wn\u27e9is converted to a sequence of character representation \u27e8[c(0,0), c(0,1), . . . , c(0,k)], . . . , [c(n,0), c(n,1), . . . , c(n,k)]\u27e9and in order to apply one dimensional convolution, it is re- quired to be in a \ufb01xed length. k shows the \ufb01xed length of the character sequence representing each word. Rather than using the one-hot representation of characters, a randomly initialized (uniform distribution) embedding layer is used. The \ufb01rst three convolution layers are followed by a one dimensional pooling layer. In each layer, kernel size is increased 7",
  "A PREPRINT - DECEMBER 17, 2021 incrementally from 2 to 4 while the number of kernels are doubled starting from 16. Just like the \ufb01rst part, the second segment of this feature extractor uses three layers but with slight changes. Kernel size is reduced starting from 4 to 2 and the number of kernels is halved starting from 64. In this part, \u2297sign shows concatenation operation. TD + GN + SineRelu note targeted dropout, group normalization and sine-relu [70, 71, 72]. These layers prevent the character feature extractor from over\ufb01tting. Equation 12 de\ufb01nes SineRelu activation function which is slightly different from Relu. SineRelu(x) = \u001ax x > 0 \u03f5(sin x \u2212cos x) x \u22640 (12) Instead of using zero in the second part of this equation, \u03f5(sin x \u2212cos x) has been used for negative inputs, \u03f5 is a hyperparameter that controls the amplitude of sin x \u2212cos x wave.",
  "This slight change prevents the network from having dead-neurons and unlike Relu, it is differentiable everywhere. On the other hand, it has been proven that using GroupNormalization provides better results than BatchNormalization on various tasks [71]. However, the dropout has a major improvement on the neural network as an over\ufb01tting prevention technique [73], in our setup the TargtedDropout shows to provide better results. TargetedDropout randomly drops neurons whose output is over a threshold. On the other hand, skip connections presented in model, provide better learning in the character feature extraction part and enables it to learn in better way in terms of evaluation metrics. Word Feature Extraction is presented in the middle part of \ufb01gure 4. Joint embeddings from pretrained word vectors of GloVe4 [27] and fastText5 [28] by concatenation operation results in 500 dimensional word embedding. In order to have forward and backward information for each hidden layer, we used a bidirectional long-short term memory [29, 30]. For the words which were not in the pretrained tokens, we used a random initialization (uniform initialization) between -0.25 and 0.25 at each embedding.",
  "In order to have forward and backward information for each hidden layer, we used a bidirectional long-short term memory [29, 30]. For the words which were not in the pretrained tokens, we used a random initialization (uniform initialization) between -0.25 and 0.25 at each embedding. The result of this phase is extracted features for each word. FastText provides better embeddings when the GloVe fails, and the reason behind it is the structure of fastText itself which is able to capture morphological semantics using subword embeddings. Image Feature Extraction is shown in the right part of \ufb01gure 4. For this part, we have used InceptionV36 pretrained on ImageNet [74]. Many models were available as the \ufb01rst part of image feature extraction, however the main reason we used InceptionV3 as feature extractor backbone is better performance of it on ImageNet and the results obtained by this particular model were slightly better compared to others. Instead of using the headless version of InceptionV3 for image feature extraction, we have used the full model which outputs the 1000 classes of ImageNet.",
  "Instead of using the headless version of InceptionV3 for image feature extraction, we have used the full model which outputs the 1000 classes of ImageNet. Each of these classes resembles an item, the set of these items can present a person, location or anything that is identi\ufb01ed as a whole. To have better features extracted from the image, we have used an embedding layer. In other words, we looked at the top 5 extracted probabilities as words that is shown in eq. 13; Based on our assumption, these \ufb01ve words present textual keywords related to the image and combination of these words should provide useful information about the objects in visual data. An LSTM unit has been used to output the \ufb01nal image features. These combined embeddings from the most probable items in the image are the key to have extra information from a social media post. IW = arg sort x {x|x = Inception(i)}[1 : 5], x \u2208[0, 1] (13) where IW is image-word vector, x is output of InceptionV3 and i is the image.",
  "IW = arg sort x {x|x = Inception(i)}[1 : 5], x \u2208[0, 1] (13) where IW is image-word vector, x is output of InceptionV3 and i is the image. x is in domain of [0,1] and P \u2200k\u2208x k = 1 holds true, while P \u2200k\u2208IW k \u22641. Multimodal Fusion in our work is presented as the concatenation of three feature sets extracted from words, characters and images. Unlike previous methods, our original work does not include an the attention layer to remove noisy features. Instead, we stacked LSTM units from word and image feature extractors to have better results. The last layer presented at the top right side of \ufb01gure 4 shows this part. In our second proposed method, we have used attention layer applied to this triplet. Our proposed attention mechanism is able to detect on which modality to increase or decrease focus. Equations 14, 15 and 16 show attention mechanism related to the second proposed model.",
  "In our second proposed method, we have used attention layer applied to this triplet. Our proposed attention mechanism is able to detect on which modality to increase or decrease focus. Equations 14, 15 and 16 show attention mechanism related to the second proposed model. 46 billion tokens with 200 dimensional word vectors, available at: http://nlp.stanford.edu/data/glove.6B.zip 516 billion tokens with 300 dimensional word vectors, available at: https://dl.fbaipublicfiles.com/fasttext/ vectors-english/wiki-news-300d-1M.vec.zip 6InceptionV3 pretrained model on ImageNet, available at: https://keras.io/applications/#inceptionv3 8",
  "A PREPRINT - DECEMBER 17, 2021 Table 2: Transformer con\ufb01guration for NER task: Tiny and Small versions. Model Hidden Size # of Attention Heads # of Transformer Layers MSB-Tiny 128 2 2 MSB-Small 512 8 4 luit = tanh(Wwhit + bw) (14) \u03b1it = exp(h\u22a4 t uit) P t exp(h\u22a4 t uit) (15) \u03b2i = X t \u03b1ithit (16) Conditional Random Field is the last layer in our setup which forms the \ufb01nal output. The same implementation explained in eq. 4 is used for our method. 3.2 Multimodal Transformer Transformer mechanism described in section 2.1 is used here with some modi\ufb01cation on the hyper-parameters. Also, we changed the input format of the original BERT model that we describe in the current subsection. We call our modi\ufb01ed BERT model as MSB (Multimodal Small BERT).",
  "Also, we changed the input format of the original BERT model that we describe in the current subsection. We call our modi\ufb01ed BERT model as MSB (Multimodal Small BERT). The modi\ufb01ed version is smaller than the BERT original model and is the same size as small BERT from original BERT released models. Byte Pair Encoding: For the tokenization part, we have used BPE tokenizer [39]. The pretrained subotkens are released in [33]7. Before tokenization, we used preprocessing operations such as URL removal. Removing URLs helps the model to skip the unnecessary operations on the input. The rest of text is given to the model with no changes. However, we further pretrained BERT to \ufb01t our task at hand, on the related corpus such as crawled twitter corpus but about the tokenizer, we used same as the released version in the original format. Transformer Con\ufb01guration: We used transformers as our building block with getting motivation from BERT as our base and reduced the parameters using the main BERT-Tiny and Small con\ufb01guration.",
  "Transformer Con\ufb01guration: We used transformers as our building block with getting motivation from BERT as our base and reduced the parameters using the main BERT-Tiny and Small con\ufb01guration. The con\ufb01gurations we used are presented in table 2. For both of these con\ufb01gurations, the vocabulary size is 30522. Pretrained version are released by google 8,9. Figure 5 shows our approach and the utilization of image extracted features into BERT model. The [SEP] token has been used to separate the text and the outputs of image feature extractor (the labels). These two modalities in uniform structure are given to the transformer to extract the \ufb01nal named entities. Another variation of the model is also introduced that has an extra CRF layer. The conditional random \ufb01eld helps the model to correct the mistakes by equation 4. Combination of BPE and the transformer gains much improvement in terms of evaluation metrics because it solves the OoV problem and uses a real bidirectional form of NLU instead of left-to-right or right-to-left.",
  "The conditional random \ufb01eld helps the model to correct the mistakes by equation 4. Combination of BPE and the transformer gains much improvement in terms of evaluation metrics because it solves the OoV problem and uses a real bidirectional form of NLU instead of left-to-right or right-to-left. This uniform understanding of the splitted tokens with aid of more pretraining on the social media posts and other related details are provided in the next section. 4 Experimental Evaluation The present section provides evaluation results of our model against baselines. Before diving into our results, a brief description of the dataset and its statistics are provided. 4.1 Dataset In [11] a re\ufb01ned collection of tweets gathered from twitter is presented. Their dataset, which is labeled for named entity recognition task contains 8,257 tweets. There are 12,784 entities in total in this dataset. Table 3 shows statistics related to each named entity in the training, development and test sets.",
  "Their dataset, which is labeled for named entity recognition task contains 8,257 tweets. There are 12,784 entities in total in this dataset. Table 3 shows statistics related to each named entity in the training, development and test sets. Following CoNLL-2033 and the BIO2 tagging, this 7https://github.com/google-research/bert 8BERT-Tiny: https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-2_H-128_A-2.zip 9BERT-Small: https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-4_H-512_A-8.zip 9",
  "A PREPRINT - DECEMBER 17, 2021 BERT [CLS] SW0 SW1 SWn SWn-1 E[CLS] E0 E1 En En-1 ... ... FF FF FF FF T0 T1 Tn-1 Tn [SEP] E[SEP] InceptionV3 Image Feature Extractor Image Select Top 5 IW0 EIW0 IW1 EIW1 IW2 EIW2 IW3 EIW3 IW4 EIW4 Figure 5: Our proposed second approach: Multimodal Small BERT. dataset is also tagged manually by experts. Short tweets that contiain less than three words has been discarded by the annotators. Non-English tweets are also discarded. The overall dataset from 26.5 million tweets has been reduced to total 8,257 tweets from 12,784 users. Training, development and testing set is also splitted to 4,000, 1,000 and 3,257 tweets, respectively. All tweets contain images related to them. These images are posted by users and related samples from dataset are presented in 6.",
  "Training, development and testing set is also splitted to 4,000, 1,000 and 3,257 tweets, respectively. All tweets contain images related to them. These images are posted by users and related samples from dataset are presented in 6. Table 3: Statistics of named entity types in train, development and test sets [11]. Entity Type Train Dev. Test Total Person 2217 552 1816 4583 Location 2091 522 1697 4308 Organization 928 247 839 2012 Miscellaneous 940 225 726 1881 Total Entities 6176 1546 5078 12784 Table 4: Evaluation results of different approaches compared to ours. Method Per. Loc. Org. Misc. Overall Prec. Recall F1 Stanford NER [23] 73.85 69.35 41.81 21.80 60.98 62.00 61.48 BiLSTM+CRF [21] 76.77 72.56 41.33 26.80 68.14 61.09 64.",
  "85 69.35 41.81 21.80 60.98 62.00 61.48 BiLSTM+CRF [21] 76.77 72.56 41.33 26.80 68.14 61.09 64.42 LSTM+CNN+CRF [31] 80.86 75.39 47.77 32.61 66.24 68.09 67.15 T-NER [24] 83.64 76.18 50.26 34.56 69.54 68.65 69.09 BiLSTM+CNN+Co-Attention [11] 81.89 78.95 53.07 34.02 72.75 68.74 70.69 CWI (Ours) 85.81 76.68 50.18 35.65 73.64 69.68 71.61 CWI + Attention (Ours) 84.02 77.34 52.60 33.47 72.37 70.05 71.19 MSB-Tiny (Ours) 82.",
  "18 35.65 73.64 69.68 71.61 CWI + Attention (Ours) 84.02 77.34 52.60 33.47 72.37 70.05 71.19 MSB-Tiny (Ours) 82.17 76.47 51.09 34.31 71.08 69.75 70.41 MSB-Small (Ours) 86.32 74.36 50.73 35.12 72.89 70.10 72.74 MSB-Tiny + CRF (Ours) 84.21 75.16 52.89 35.31 72.87 69.41 71.10 MSB-Small + CRF(Ours) 86.44 77.16 52.91 36.05 74.97 72.04 73.47 4.2 Experimental Setup In order to obtain the best results in tab. 4 for our \ufb01rst model (CWI), we have used the following setup in tables 5, 6, 7 and 8.",
  "05 74.97 72.04 73.47 4.2 Experimental Setup In order to obtain the best results in tab. 4 for our \ufb01rst model (CWI), we have used the following setup in tables 5, 6, 7 and 8. For the second proposed method, the same parameter settings have been used with an additional attention layer. This additional layer has been added after layer 31 in table 8 and before the \ufb01nal CRF layer, indexed as 32. Adam 10",
  "A PREPRINT - DECEMBER 17, 2021 Table 5: Implementation details of our model (CWI): Character Feature Extractor. KS: Kernel Size; PS: Pooling Size; DR: Dropout Rate; TR: Target Rate; \u2191: prior layer MaxPooling has been applied to second dimension rather than channels ID Layer Name Con. Details 1 Input \u2013 35 \u00d7 40 2 Embedding \u2191 Embedding vector size is set to 40 and initialized in range of [-0.25, 0.25] with uniform distribution 3 1D conv. \u2191 KS: 2, # of Kernels: 16 4 1D MaxPooling \u2191 PS: 2 5 1D conv. \u2191 KS: 3, # of Kernels: 32 6 1D MaxPooling \u2191 PS: 2 7 1D conv. \u2191 KS: 4, # of Kernels: 64 8 1D MaxPooling \u2191 PS: 2 9 1D conv. \u2191 KS: 4, # of Kernels: 64 10 Concatenation 8,9 \u2013 11 1D conv.",
  "\u2191 KS: 4, # of Kernels: 64 8 1D MaxPooling \u2191 PS: 2 9 1D conv. \u2191 KS: 4, # of Kernels: 64 10 Concatenation 8,9 \u2013 11 1D conv. \u2191 KS: 3, # of Kernels: 32 12 Concatenation 6,11 \u2013 13 1D conv. \u2191 KS: 2, # of Kernels: 16 14 Concatenation 4,13 \u2013 15 Targeted Dropout \u2191 DR: 0.25, TR: 0.4 16 Sine Relu \u2191 \u03f5: 0.0025 17 Group Normalization \u2191 Applied to 16 groups Table 6: Implementation details of our model (CWI): Word Feature Extractor. ID Layer Name Con.",
  "ID Layer Name Con. Details 18 Input \u2013 35 19 GloVe 18 GloVe Embedding vector, vector size: 200 20 fastText 18 fastText Embedding vector, vector size: 300 21 Concatenation 19,20 \u2013 22 LSTM (Forward) 21 Size: 100 23 LSTM (Backward) 21 Size: 100 24 Concatenation 22,23 \u2013 optimizer with 8 \u00d7 10\u22125 has been used in training phase with 10 epochs. The MSB model has been also pretrained on twitter data by using the twitter API and gathered texts. This model has another variation that utilizes the CRF at then last layer for better performance. For the MSB-Tiny and Small version, we used pretrained weights from BERT that google released. We also trained the model on two different datasets, Twitter-Multimodal-NER dataset (TMN) [11] and CoNLL-2003 [75]. The language model that has been used is also trained on the twitter text data that gains more realistic texts to add to the modeling in the pretraining phase.",
  "The language model that has been used is also trained on the twitter text data that gains more realistic texts to add to the modeling in the pretraining phase. The \ufb01ne-tuning part has been done in two phases, we \ufb01rst \ufb01ne-tuned masked language modeling on CoNLL-2003 NER dataset and in the second phase, we trained the whole model on NER task TMN dataset. Table 7: Implementation details of our model (CWI): Image Feature Extractor. ID Layer Name Con. Details 25 Input \u2013 5 highest probability classes selected from InceptionV3 26 Embedding \u2191 50 27 LSTM (Forward) \u2191 Size: 50 11",
  "A PREPRINT - DECEMBER 17, 2021 Table 8: Implementation details of our model (CWI): Multimodal Fusion. ID Layer Name Con. Details 28 Concatenation 17,24,27 \u2013 29 LSTM (Forward) 28 Size: 100 30 LSTM (Backward) 28 Size: 100 31 Concatenation 29,30 \u2013 32 CRF 30 # of output Classes: 9, ac- cording to BIO2 1 Table 9: Effect of different word embedding sizes on our proposed model. Embedding Size Overall Per. Loc. Org. Misc. 350 69.87 81.08 72.60 49.41 34.03 400 70.23 83.12 74.37 51.87 34.29 500 71.61 85.81 76.68 50.18 35.65 600 70.98 84.29 75.92 47.81 38.71 Figure 6 shows some visual samples of the dataset.",
  "Also, we present the result of our different approaches on these samples in \ufb01g. 7. In this \ufb01gure, the Ground-Truth is highlighted with red color at the above line of each sentence and the results of our approaches are shown by different colors at the below lines of each sentence. Some samples such as the \ufb01rst one are not correctly labeled in the dataset, but our approach appropriately predicted the true labels. 4.3 Evaluation Results Table 4 presents the evaluation results of our proposed models. Compared to other state of the art methods, our \ufb01rst proposed model shows 1% improvement on f1 score. The effect of different word embedding sizes on our proposed method is presented in 9. Sensitivity to TD+SineRelu+GN is presented in tab. 10. Second approach is also gains around 3% improvement on the f1 score which is much higher than the \ufb01rst approach but in terms of model size and training time it has downsides.",
  "Sensitivity to TD+SineRelu+GN is presented in tab. 10. Second approach is also gains around 3% improvement on the f1 score which is much higher than the \ufb01rst approach but in terms of model size and training time it has downsides. The training time for the \ufb01rst model in a CoreI7 processor with Nvidia GeForce GTX 1650 is around 10 minutes while for the second approach it is 2 days for pretraining on twitter data and \ufb01ne tuning on the datasets. The training time for the InceptionV3 is not considered because we used the original released version with no changes from Keras Applications. Table 10: Effect of TD+GN+SineRelu on our proposed model. TD+GN+SineRelu Overall Per. Loc. Org. Misc. No 64.18 76.21 72.30 40.98 28.81 Yes 71.61 85.81 76.68 50.18 35.65 5 Conclusion In this article, we have proposed two NER approaches based on multimodal deep learning.",
  "No 64.18 76.21 72.30 40.98 28.81 Yes 71.61 85.81 76.68 50.18 35.65 5 Conclusion In this article, we have proposed two NER approaches based on multimodal deep learning. In our \ufb01rst model, we have used a new architecture in character feature extraction that has helped our model to overcome the issue of noise. We also used transformers as our building block to propose MSB. Instead of using direct image features from near last layers of image feature extractors such as Inception, we have used the direct output of the last layer. The last layer is 1000 classes of diverse objects that is result of InceptionV3 trained on ImageNet dataset. We used top 5 classes out of these and converted them to one-hot vectors. The resulting image feature embedding out of these high probability one-hot vectors helped our model to overcome the issue of noise in images posted by social media users. Evaluation results of our proposed models compared to other state of the art methods show its superiority to these methods in overall while in two categories (Person and Miscellaneous) our model outperformed others.",
  "Evaluation results of our proposed models compared to other state of the art methods show its superiority to these methods in overall while in two categories (Person and Miscellaneous) our model outperformed others. 6 Acknowledgments The authors would like to thank Shervin Minaee for reviewing this work, and providing very useful comments to improve this work. 12",
  "A PREPRINT - DECEMBER 17, 2021 Figure 6: Samples from twitter dataset with text and related image [11]. 13",
  "A PREPRINT - DECEMBER 17, 2021 The   Reeves-urrection   :   The   Second   Coming   of   Keanu   Reeves   . I-MISC I-MISC I-MISC I-MISC I-MISC I-MISC I-MISC B-ORG \u00a0 I-ORG \u00a0 B-MISC \u00a0 I-MISC \u00a0 I-MISC a. \u00a0 B-ORG \u00a0 I-ORG \u00a0 B-MISC \u00a0 I-MISC \u00a0 I-MISC \u00a0 I-MISC \u00a0 I-MISC \u00a0 I-MISC \u00a0 B-LOC \u00a0 I-LOC \u00a0 B-PER \u00a0 I-PER \u00a0 B-ORG \u00a0 I-ORG \u00a0 B-MISC \u00a0 I-MISC \u00a0 I-MISC \u00a0 B-PER \u00a0 I-PER B-LOC RT   @WESH   :   Woman   defends   treatment   of   10   pit   bulls   at   Winter   Garden   home I-LOC \u00a0 B-ORG \u00a0 B-LOC \u00a0 B-LOC \u00a0 B-LOC \u00a0 I-ORG \u00a0 I-LOC \u00a0 I-ORG \u00a0 I-LOC \u00a0 I-LOC \u00a0 I-LOC B-LOC I   think   this",
  "Winter   Garden   home I-LOC \u00a0 B-ORG \u00a0 B-LOC \u00a0 B-LOC \u00a0 B-LOC \u00a0 I-ORG \u00a0 I-LOC \u00a0 I-ORG \u00a0 I-LOC \u00a0 I-LOC \u00a0 I-LOC B-LOC I   think   this   display   at   Chicago   City   Hall   wishes   you   a   particularly   ominous   seasons   greetings I-LOC \u00a0 B-ORG \u00a0 B-LOC \u00a0 B-LOC \u00a0 I-ORG \u00a0 I-LOC \u00a0 I-ORG \u00a0 I-LOC B-PER RT   @MassDeception1   :   Judge   orders   Prince   Andrew   sex   allegations   struck   from   court   record I-PER \u00a0 B-PER \u00a0 B-PER \u00a0 B-PER \u00a0 B-PER \u00a0 I-PER \u00a0 I-PER \u00a0 I-PER \u00a0 I-PER I-LOC \u00a0 I-LOC \u00a0 B-LOC \u00a0 I-LOC \u00a0 I-LOC Ground-Truth: c. e. f.",
  "I-PER \u00a0 B-PER \u00a0 B-PER \u00a0 B-PER \u00a0 B-PER \u00a0 I-PER \u00a0 I-PER \u00a0 I-PER \u00a0 I-PER I-LOC \u00a0 I-LOC \u00a0 B-LOC \u00a0 I-LOC \u00a0 I-LOC Ground-Truth: c. e. f. WCI: CWI+Attention: MSB-Tiny: MSB-Small: Figure 7: Results of our Approaches on different samples from \ufb01g. 6, CRF variation of models has been used here. References [1] Twitter. About Twitter, Inc, 2014. [2] Miles Osborne, Victor Lavrenko, and Sasa Petrovic. Streaming First Story Detection with application to Twitter. Computational Linguistics, 2010. [3] Chenliang Li, Jianshu Weng, Qi He, Yuxia Yao, Anwitaman Datta, Aixin Sun, and Bu-Sung Lee. TwiNER: Named Entity Recognition in Targeted Twitter Stream. In Proceedings of SIGIR\u201912, August 12\u201316, 2012, Portland, Oregon, USA, 2012.",
  "TwiNER: Named Entity Recognition in Targeted Twitter Stream. In Proceedings of SIGIR\u201912, August 12\u201316, 2012, Portland, Oregon, USA, 2012. [4] Farzindar Atefeh and Wael Khreich. A survey of techniques for event detection in twitter. Computational Intelligence, 31(1):132\u2013164, 2015. [5] Chenliang Li, Jianshu Weng, Qi He, Yuxia Yao, Anwitaman Datta, Aixin Sun, and Bu-Sung Lee. Twiner: named entity recognition in targeted twitter stream. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, pages 721\u2013730, 2012. [6] Zhunchen Luo, Miles Osborne, and Ting Wang. An effective approach to tweets opinion retrieval. World Wide Web, 2015. [7] Vijay V. Raghavan, Venkat N. Gudivada, Zonghuan Wu, and William I. Grosky. Information retrieval. In The Practical Handbook of Internet Computing.",
  "An effective approach to tweets opinion retrieval. World Wide Web, 2015. [7] Vijay V. Raghavan, Venkat N. Gudivada, Zonghuan Wu, and William I. Grosky. Information retrieval. In The Practical Handbook of Internet Computing. 2004. [8] A.a Davis, A.a Veloso, A.S.b Da Silva, Wagner Meira Jr., and A.H.F.a Laender. Named entity disambiguation in streaming data. In ACL 2012, 2012. [9] J. T. L. Santos, I. M. Anastacio, and B. E. Martins. Named entity disambiguation over texts written in the portuguese or spanish languages. IEEE Latin America Transactions, 13(3):856\u2013862, March 2015. [10] Brian Locke and James Martin. Named Entity Recognition: Adapting to Microblogging. University of Colorado UG Thesis, 2009. [11] Qi Zhang, Jinlan Fu, Xiaoyu Liu, and Xuanjing Huang.",
  "[10] Brian Locke and James Martin. Named Entity Recognition: Adapting to Microblogging. University of Colorado UG Thesis, 2009. [11] Qi Zhang, Jinlan Fu, Xiaoyu Liu, and Xuanjing Huang. Adaptive Co-attention Network for Named Entity Recognition in Tweets. Aaai, 2018. [12] Seungwhan Moon, Leonardo Neves, and Vitor Carvalho. Multimodal Named Entity Disambiguation for Noisy Social Media Posts. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2018. [13] Gustavo Aguilar, Suraj Maharjan, Adrian Pastor L\u00f3pez Monroy, and Thamar Solorio. A multi-task approach for named entity recognition in social media data. In Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 148\u2013153, 2017. 14",
  "A PREPRINT - DECEMBER 17, 2021 [14] Rahul Sharnagat. Named Entity Recognition Literature Survey. In 11305R013, 2014. [15] C. Li, A. Sun, J. Weng, and Q. He. Tweet segmentation and its application to named entity recognition. IEEE Transactions on Knowledge and Data Engineering, 27(2):558\u2013570, FEBRUARY 2015. [16] Erik F Sang and Jorn Veenstra. Representing text chunks. In Proceedings of the ninth conference on Euro- pean chapter of the Association for Computational Linguistics, pages 173\u2013179. Association for Computational Linguistics, 1999. [17] K. Li, W. Ai, Z. Tang, F. Zhang, L. Jiang, K. Li, and K. Hwang. Hadoop recognition of biomedical named entity using conditional random \ufb01elds. IEEE Transactions on Parallel and Distributed Systems, 26(11):3040\u20133051, Nov 2015. [18] C. Wei, R. Leaman, and Z. Lu.",
  "Hadoop recognition of biomedical named entity using conditional random \ufb01elds. IEEE Transactions on Parallel and Distributed Systems, 26(11):3040\u20133051, Nov 2015. [18] C. Wei, R. Leaman, and Z. Lu. Simconcept: A hybrid approach for simplifying composite named entities in biomedical text. IEEE Journal of Biomedical and Health Informatics, 19(4):1385\u20131391, July 2015. [19] Jing Li, Aixin Sun, Jianglei Han, and Chenliang Li. A survey on deep learning for named entity recognition. arXiv preprint arXiv:1812.09449, 2018. [20] Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, and David McClosky. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, 2014. [21] Zhiheng Huang, Wei Xu, and Kai Yu. Bidirectional lstm-crf models for sequence tagging.",
  "The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, 2014. [21] Zhiheng Huang, Wei Xu, and Kai Yu. Bidirectional lstm-crf models for sequence tagging. arXiv preprint arXiv:1508.01991, 2015. [22] Tomasz Stanislawek, Anna Wr\u00f3blewska, Alicja W\u00f3jcika, Daniel Ziembicki, and Przemyslaw Biecek. Named entity recognition\u2013is there a glass ceiling? arXiv preprint arXiv:1910.02403, 2019. [23] Jenny Rose Finkel, Trond Grenager, and Christopher Manning. Incorporating non-local information into infor- mation extraction systems by Gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics - ACL \u201905, 2005. [24] Alan Ritter, Sam Clark, Mausam Etzioni, and Oren Etzioni. Named entity recognition in tweets: an experimental study.",
  "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics - ACL \u201905, 2005. [24] Alan Ritter, Sam Clark, Mausam Etzioni, and Oren Etzioni. Named entity recognition in tweets: an experimental study. In Proceedings of the Conference on Empirical Methods in Natural Language Processing(EMNLP\u201911), 2011. [25] Michael Collins and Yoram Singer. Unsupervised models for named entity classi\ufb01cation. In Proceedings of EMNLP/VLC-99, 1999. [26] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef\ufb01cient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. [27] Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014.",
  "[27] Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014. [28] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5:135\u2013146, 2017. [29] Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 45(11):2673\u20132681, 1997. [30] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997. [31] Xuezhe Ma and Eduard Hovy. End-to-end sequence labeling via bi-directional lstm-cnns-crf. arXiv preprint arXiv:1603.01354, 2016.",
  "[31] Xuezhe Ma and Eduard Hovy. End-to-end sequence labeling via bi-directional lstm-cnns-crf. arXiv preprint arXiv:1603.01354, 2016. [32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998\u20136008, 2017. [33] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [34] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding.",
  "[34] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in neural information processing systems, pages 5754\u20135764, 2019. [35] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019. [36] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uni\ufb01ed text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019. 15",
  "A PREPRINT - DECEMBER 17, 2021 [37] Shervin Minaee, Nal Kalchbrenner, Erik Cambria, Narjes Nikzad, Meysam Chenaghlu, and Jianfeng Gao. Deep learning based text classi\ufb01cation: A comprehensive review. arXiv preprint arXiv:2004.03705, 2020. [38] Yusuxke Shibata, Takuya Kida, Shuichi Fukamachi, Masayuki Takeda, Ayumi Shinohara, Takeshi Shinohara, and Setsuo Arikawa. Byte pair encoding: A text compression scheme that accelerates pattern matching. Technical report, Technical Report DOI-TR-161, Department of Informatics, Kyushu University, 1999. [39] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015. [40] Minh-Thang Luong, Hieu Pham, and Christopher D Manning.",
  "Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015. [40] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015. [41] Mikhail Arkhipov, Maria Tro\ufb01mova, Yuri Kuratov, and Alexey Sorokin. Tuning multilingual transformers for named entity recognition on slavic languages. BSNLP\u20192019, page 89, 2019. [42] E. A. Bernal, X. Yang, Q. Li, J. Kumar, S. Madhvanath, P. Ramesh, and R. Bala. Deep temporal multimodal fusion for medical procedure monitoring using wearable sensors. IEEE Transactions on Multimedia, 20(1):107\u2013118, Jan 2018. [43] D. Wang, P. Cui, M. Ou, and W. Zhu. Learning compact hash codes for multimodal representations using orthogonal deep structure.",
  "IEEE Transactions on Multimedia, 20(1):107\u2013118, Jan 2018. [43] D. Wang, P. Cui, M. Ou, and W. Zhu. Learning compact hash codes for multimodal representations using orthogonal deep structure. IEEE Transactions on Multimedia, 17(9):1404\u20131416, Sep. 2015. [44] C. Ding and D. Tao. Robust face recognition via multimodal deep face representation. IEEE Transactions on Multimedia, 17(11):2049\u20132058, Nov 2015. [45] F. Chen, R. Ji, J. Su, D. Cao, and Y. Gao. Predicting microblog sentiments via weakly supervised multimodal deep learning. IEEE Transactions on Multimedia, 20(4):997\u20131007, April 2018. [46] H. Li, J. Sun, Z. Xu, and L. Chen. Multimodal 2d+3d facial expression recognition with deep fusion convolutional neural network. IEEE Transactions on Multimedia, 19(12):2816\u20132831, Dec 2017.",
  "[46] H. Li, J. Sun, Z. Xu, and L. Chen. Multimodal 2d+3d facial expression recognition with deep fusion convolutional neural network. IEEE Transactions on Multimedia, 19(12):2816\u20132831, Dec 2017. [47] L. Pang, S. Zhu, and C. Ngo. Deep multimodal learning for affective analysis and retrieval. IEEE Transactions on Multimedia, 17(11):2008\u20132020, Nov 2015. [48] Y. Jiang, Z. Wu, J. Tang, Z. Li, X. Xue, and S. Chang. Modeling multimodal clues in a hybrid deep learning framework for video classi\ufb01cation. IEEE Transactions on Multimedia, 20(11):3137\u20133147, Nov 2018. [49] J. Shi, X. Zheng, Y. Li, Q. Zhang, and S. Ying. Multimodal neuroimaging feature learning with multimodal stacked deep polynomial networks for diagnosis of alzheimer\u2019s disease.",
  "[49] J. Shi, X. Zheng, Y. Li, Q. Zhang, and S. Ying. Multimodal neuroimaging feature learning with multimodal stacked deep polynomial networks for diagnosis of alzheimer\u2019s disease. IEEE Journal of Biomedical and Health Informatics, 22(1):173\u2013183, Jan 2018. [50] D. Ramachandram and G. W. Taylor. Deep multimodal learning: A survey on recent advances and trends. IEEE Signal Processing Magazine, 34(6):96\u2013108, Nov 2017. [51] Kuan Liu, Yanen Li, Ning Xu, and Prem Natarajan. Learn to combine modalities in multimodal deep learning. arXiv preprint arXiv:1805.11730, 2018. [52] Lisa Beinborn, Teresa Botschen, and Iryna Gurevych. Multimodal grounding for language processing. arXiv preprint arXiv:1806.06371, 2018.",
  "[52] Lisa Beinborn, Teresa Botschen, and Iryna Gurevych. Multimodal grounding for language processing. arXiv preprint arXiv:1806.06371, 2018. [53] Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y Ng. Multimodal deep learning. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 689\u2013696, 2011. [54] Samira Ebrahimi Kahou, Vincent Michalski, Kishore Konda, Roland Memisevic, and Christopher Pal. Recurrent Neural Networks for Emotion Recognition in Video. In Proceedings of the 2015 ACM on International Conference on Multimodal Interaction - ICMI \u201915, 2015. [55] Wei Liu, Wei Long Zheng, and Bao Liang Lu. Emotion recognition using multimodal deep learning. In Lecture Notes in Computer Science (including subseries Lecture Notes in Arti\ufb01cial Intelligence and Lecture Notes in Bioinformatics), 2016.",
  "[55] Wei Liu, Wei Long Zheng, and Bao Liang Lu. Emotion recognition using multimodal deep learning. In Lecture Notes in Computer Science (including subseries Lecture Notes in Arti\ufb01cial Intelligence and Lecture Notes in Bioinformatics), 2016. [56] Samira Ebrahimi Kahou, Xavier Bouthillier, Pascal Lamblin, Caglar Gulcehre, Vincent Michalski, Kishore Konda, S\u00e9bastien Jean, Pierre Froumenty, Yann Dauphin, Nicolas Boulanger-Lewandowski, Raul Chandias Ferrari, Mehdi Mirza, David Warde-Farley, Aaron Courville, Pascal Vincent, Roland Memisevic, Christopher Pal, and Yoshua Bengio. EmoNets: Multimodal deep learning approaches for emotion recognition in video. Journal on Multimodal User Interfaces, 2016. [57] Heung Il Suk, Seong Whan Lee, and Dinggang Shen. Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis. NeuroImage, 2014. 16",
  "A PREPRINT - DECEMBER 17, 2021 [58] Xi Cheng, Li Zhang, and Yefeng Zheng. Deep similarity learning for multimodal medical images. Computer Methods in Biomechanics and Biomedical Engineering: Imaging and Visualization, 2018. [59] Di Wu, Lionel Pigou, Pieter Jan Kindermans, Nam Do Hoang Le, Ling Shao, Joni Dambre, and Jean Marc Odobez. Deep Dynamic Neural Networks for Multimodal Gesture Segmentation and Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016. [60] Seungwhan Moon, Leonardo Neves, and Vitor Carvalho. Multimodal named entity recognition for short social media posts. arXiv preprint arXiv:1802.07862, 2018. [61] Diego Esteves, Rafael Peres, Jens Lehmann, and Giulio Napolitano. Named Entity Recognition in Twitter Using Images and Text. In Lecture Notes in Computer Science (including subseries Lecture Notes in Arti\ufb01cial Intelligence and Lecture Notes in Bioinformatics), 2018.",
  "Named Entity Recognition in Twitter Using Images and Text. In Lecture Notes in Computer Science (including subseries Lecture Notes in Arti\ufb01cial Intelligence and Lecture Notes in Bioinformatics), 2018. [62] Alexandre Passos, Vineet Kumar, and Andrew McCallum. Lexicon infused phrase embeddings for named entity resolution. arXiv preprint arXiv:1404.5367, 2014. [63] Jason PC Chiu and Eric Nichols. Named entity recognition with bidirectional lstm-cnns. Transactions of the Association for Computational Linguistics, 4:357\u2013370, 2016. [64] Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. Neural architectures for named entity recognition. arXiv preprint arXiv:1603.01360, 2016. [65] Timothy Baldwin, Marie-Catherine de Marneffe, Bo Han, Young-Bum Kim, Alan Ritter, and Wei Xu. Shared tasks of the 2015 workshop on noisy user-generated text: Twitter lexical normalization and named entity recognition.",
  "[65] Timothy Baldwin, Marie-Catherine de Marneffe, Bo Han, Young-Bum Kim, Alan Ritter, and Wei Xu. Shared tasks of the 2015 workshop on noisy user-generated text: Twitter lexical normalization and named entity recognition. In Proceedings of the Workshop on Noisy User-generated Text, pages 126\u2013135, 2015. [66] Emma Strubell, Patrick Verga, David Belanger, and Andrew McCallum. Fast and accurate entity recognition with iterated dilated convolutions. arXiv preprint arXiv:1702.02098, 2017. [67] Heeyoul Choi, Kyunghyun Cho, and Yoshua Bengio. Fine-grained attention mechanism for neural machine translation. Neurocomputing, 2018. [68] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818\u20132826, 2016.",
  "Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818\u20132826, 2016. [69] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. [70] Aidan N Gomez, Ivan Zhang, Kevin Swersky, Yarin Gal, and Geoffrey E Hinton. Targeted dropout. 2018. [71] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European Conference on Computer Vision (ECCV), pages 3\u201319, 2018. [72] SineReLU - an alternative to the relu activation function. https://medium.com/wilder.rodrigues/ sinerelu-an-alternative-to-the-relu-activation-function-e46a6199997d. Accessed: 2019-03- 07.",
  "[72] SineReLU - an alternative to the relu activation function. https://medium.com/wilder.rodrigues/ sinerelu-an-alternative-to-the-relu-activation-function-e46a6199997d. Accessed: 2019-03- 07. [73] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from over\ufb01tting. The Journal of Machine Learning Research, 15(1):1929\u20131958, 2014. [74] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. 2009. [75] Erik F Sang and Fien De Meulder. Introduction to the conll-2003 shared task: Language-independent named entity recognition. arXiv preprint cs/0306050, 2003. 17"
]