[
  "ReCo CoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension Sheng Zhang\u2020\u2217, Xiaodong Liu\u2021, Jingjing Liu\u2021, Jianfeng Gao\u2021, Kevin Duh\u2020 and Benjamin Van Durme\u2020 \u2020Johns Hopkins University \u2021Microsoft Research Abstract We present a large-scale dataset, ReCoRD, for machine reading comprehension requiring commonsense reasoning. Experiments on this dataset demonstrate that the performance of state-of-the-art MRC systems fall far behind human performance. ReCoRD represents a challenge for future research to bridge the gap between human and machine commonsense reading comprehension. ReCoRD is available at http://nlp.jhu.edu/record. 1 Introduction Machine reading comprehension (MRC) is a cen- tral task in natural language understanding, with techniques lately driven by a surge of large-scale datasets (Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2017; Nguyen et al., 2016), usually formalized as a task of answering questions given a passage.",
  "An in- creasing number of analyses (Jia and Liang, 2017; Rajpurkar et al., 2018; Kaushik and Lipton, 2018) have revealed that a large portion of questions in these datasets can be answered by simply match- ing the patterns between the question and the an- swer sentence in the passage. While systems may match or even outperform humans on these datasets, our intuition suggests that there are at least some instances in human reading compre- hension that require more than what existing chal- lenge tasks are emphasizing. One primary type of questions these datasets lack are the ones that require reasoning over common sense or under- standing across multiple sentences in the pas- sage (Rajpurkar et al., 2016; Trischler et al., 2017). To overcome this limitation, we introduce a large-scale dataset for reading comprehen- sion, ReCoRD ([\"rEk@rd]), which consists of over 120,000 examples, most of which require \u2217Work done when Sheng Zhang was visiting Microsoft.",
  "To overcome this limitation, we introduce a large-scale dataset for reading comprehen- sion, ReCoRD ([\"rEk@rd]), which consists of over 120,000 examples, most of which require \u2217Work done when Sheng Zhang was visiting Microsoft. Passage (Cloze-style) Query According to claims in the suit, \"Parts of 'Stairway to  Heaven,' instantly recognizable to the music fans across  the world, sound almost identical to signi\ufb01cant portions  of \u2018X.\u2019\u201d Reference Answers Taurus (CNN) -- A lawsuit has been \ufb01led claiming that the  iconic\u00a0Led Zeppelin\u00a0song \"Stairway to Heaven\" was far  from original. The suit, \ufb01led on May 31 in the\u00a0United  States District Court Eastern District of Pennsylvania,  was brought by the estate of the late musician\u00a0Randy  California \u00a0 against  the  surviving  members  of \u00a0 Led  Zeppelin \u00a0 and  their  record  label.",
  "The suit, \ufb01led on May 31 in the\u00a0United  States District Court Eastern District of Pennsylvania,  was brought by the estate of the late musician\u00a0Randy  California \u00a0 against  the  surviving  members  of \u00a0 Led  Zeppelin \u00a0 and  their  record  label.  The  copyright  infringement case alleges that the\u00a0Zeppelin\u00a0song was  taken from the single \"Taurus\" by the\u00a01960s band Spirit,  for whom\u00a0California\u00a0served as lead guitarist. \"Late in  1968, a then new band named\u00a0Led Zeppelin\u00a0began  touring in the\u00a0United States, opening for Spirit,\" the suit  states. \"It was during this time that\u00a0Jimmy Page,\u00a0Led  Zeppelin's guitarist, grew familiar with 'Taurus' and the  rest of\u00a0Spirit's catalog. Page stated in interviews that he  found  Spirit  to  be  'very  good'  and  that  the  band's  performances struck him 'on an emotional level.'",
  "Page stated in interviews that he  found  Spirit  to  be  'very  good'  and  that  the  band's  performances struck him 'on an emotional level.' \" \u2022 Suit claims similarities between two songs \u2022 Randy California was guitarist for the group Spirit \u2022 Jimmy Page has called the accusation \"ridiculous\" Figure 1: An example from ReCoRD. The passage is a snippet from a news article followed by some bullet points which summarize the news event. Named enti- ties highlighted in the passage are possible answers to the query. The query is a statement that is factually supported by the passage. X in the statement indicates a missing named entity. The goal is to \ufb01nd the correct entity in the passage that best \ufb01ts X. deep commonsense reasoning. ReCoRD is an acronym for the Reading Comprehension with Commonsense Reasoning Dataset. Figure 1 shows a ReCoRD example: the pas- sage describes a lawsuit claiming that the band \u201cLed Zeppelin\u201d had plagiarized the song \u201cTaurus\u201d arXiv:1810.12885v1  [cs.CL]  30 Oct 2018",
  "to their most iconic song, \u201cStairway to Heaven\u201d. The cloze-style query asks what does \u201cStairway to Heaven\u201d sound similar to. To \ufb01nd the correct answer, we need to understand from the passage that \u201ca copyright infringement case alleges that \u2018Stairway to Heaven\u2019 was taken from \u2018Taurus\u2019\u201d, and from the bullet point that \u201cthese two songs are claimed similar\u201d. Then based on the common- sense knowledge that \u201cif two songs are claimed similar, it is likely that (parts of) these songs sound almost identical\u201d, we can reasonably infer that the answer is \u201cTaurus\u201d. Differing from most of the existing MRC datasets, all queries and passages in ReCoRD are automatically mined from news articles, which maximally reduces the human elicitation bias (Gordon and Van Durme, 2013; Misra et al., 2016; Zhang et al., 2017), and the data collection method we propose is cost-ef\ufb01cient. Further analysis shows that a large portion of ReCoRD requires commonsense reasoning.",
  "Further analysis shows that a large portion of ReCoRD requires commonsense reasoning. Experiments on ReCoRD demonstrate that hu- man readers are able to achieve a high perfor- mance at 91.69 F1, whereas the state-of-the-art MRC models fall far behind at 46.65 F1. Thus, ReCoRD presents a real challenge for future re- search to bridge the gap between human and ma- chine commonsense reading comprehension. 2 Task Motivation A program has common sense if it auto- matically deduces for itself a suf\ufb01ciently wide class of immediate consequences of anything it is told and what it already knows. \u2013 McCarthy (1959) Commonsense Reasoning in MRC As illustrated by the example in Figure 1, the commonsense knowledge \u201cif two songs are claimed similar, it is likely that (parts of) these songs sound almost identica\u201d is not explicitly described in the pas- sage, but is necessary to acquire in order to gen- erate the answer. Human is able to infer the answer because the commonsense knowledge is commonly known by nearly all people. Our goal is to evaluate whether a machine is able to learn such knowledge.",
  "Human is able to infer the answer because the commonsense knowledge is commonly known by nearly all people. Our goal is to evaluate whether a machine is able to learn such knowledge. However, since commonsense knowledge is massive and mostly implicit, de\ufb01n- ing an explicit free-form evaluation is challeng- ing (Levesque et al., 2011). Motivated by Mc- Carthy (1959), we instead evaluate a machine\u2019s ability of commonsense reasoning \u2013 a reasoning process requiring commonsense knowledge; that is, if a machine has common sense, it can de- duce for itself the likely consequences or details of anything it is told and what it already knows rather than the unlikely ones. To formalize it in MRC, given a passage p (i.e., \u201canything it is told\u201d and \u201cwhat it already knows\u201d), and a set of conse- quences or details C which are factually supported by the passage p with different likelihood, if a machine M has common sense, it can choose the most likely consequence or detail c\u2217from C, i.e., c\u2217= arg max c\u2208C P(c | p, M).",
  "(1) Task De\ufb01nition With the above discussion, we propose a speci\ufb01c task to evaluate a machine\u2019s ability of commonsense reasoning in MRC: as shown in Figure 1, given a passage p describing an event, a set of text spans E marked in p, and a cloze-style query Q(X) with a missing text span indicated by X, a machine M is expected to act like human, reading the passage p and then using its hidden commonsense knowledge to choose a text span e \u2208E that best \ufb01ts X, i.e., e\u2217= arg max e\u2208E P(Q(e) | p, M). (2) Once the cloze-style query Q(X) is \ufb01lled in by a text span e, the resulted statement Q(e) becomes a consequence or detail c as described in Equa- tion (1), which is factually supported by the pas- sage with certain likelihood. 3 Data Collection We describe the framework for automatically gen- erating the dataset, ReCoRD, for our task de\ufb01ned in Equation (2), which consists of passages with text spans marked, cloze-style queries, and refer- ence answers.",
  "3 Data Collection We describe the framework for automatically gen- erating the dataset, ReCoRD, for our task de\ufb01ned in Equation (2), which consists of passages with text spans marked, cloze-style queries, and refer- ence answers. We collect ReCoRD in four stages as shown in Figure 2: (1) curating CNN/Daily Mail news articles, (2) generating passage-query- answers triples based on the news articles, (3) \ufb01l- tering out the queries that can be easily answered by state-of-the-art MRC models, and (4) \ufb01ltering out the queries ambiguous to human readers. 3.1 News Article Curation We choose to create ReCoRD by exploiting news articles, because the structure of news makes it a good source for our task: normally, the \ufb01rst few paragraphs of a news article summarize the news",
  "Machine Filtering (244k triples) CNN/Daily Mail News Article Curation (170k news articles) ReCoRD Human Filtering (120k triples)  Passage-Query-Answers Generation (770k triples) Co Figure 2: The overview of data collection stages. event, which can be used to generate passages of the task; and the rest of the news article provides consequences or details of the news event, which can be used to generate queries of the task. In addition, news providers such as CNN and Daily Mail supplement their articles with a number of bullet points (Svore et al., 2007; Woodsend and Lapata, 2010; Hermann et al., 2015), which out- line the highlights of the news and hence form a supplemental source for generating passages. We \ufb01rst downloaded CNN and Daily Mail news articles using the script1 provided by Hermann et al. (2015), and then sampled 148K articles from CNN and Daily Mail. In these articles, named en- tities and their coreference information have been annotated by a Google NLP pipeline, and will be used in the second stage of our data collection.",
  "(2015), and then sampled 148K articles from CNN and Daily Mail. In these articles, named en- tities and their coreference information have been annotated by a Google NLP pipeline, and will be used in the second stage of our data collection. Since these articles can be easily downloaded us- ing the public script, we are concerned about po- tential cheating if using them as the source for generating the dev./test datasets. Therefore, we crawled additional 22K news articles from the CNN and Daily Mail websites. These crawled articles have no overlap with the articles used in Hermann et al. (2015). We then ran the state- of-the-art named entity recognition model (Pe- ters et al., 2018) and the end-to-end coreference resolution model (Lee et al., 2017) provided by AllenNLP (Gardner et al., 2018) to annotate the crawled articles. Overall, we have collected 170K CNN/Daily Mail news articles with their named entities and coreference information annotated.",
  "Overall, we have collected 170K CNN/Daily Mail news articles with their named entities and coreference information annotated. 1https://github.com/deepmind/rc-data 3.2 Passage-Query-Answers Generation All passages, queries and answers in ReCoRD were automatically generated from the curated news articles. Figure 3 illustrates the generation process. (1) we split each news article into two parts as described in Section 3.1: the \ufb01rst few paragraphs which summarize the news event, and the rest of the news which provides the details or consequences of the news event. These two parts make a good source for generating passages and queries of our task respectively. (2) we enriched the \ufb01rst part of news article with the bullet points provided by the news editors. The \ufb01rst part of news article, together with the bullet points, is con- sidered as a candidate passage. To ensure that the candidate passages are informative enough, we re- quired the \ufb01rst part of news article to have at least 100 tokens and contain at least four different en- tities.",
  "To ensure that the candidate passages are informative enough, we re- quired the \ufb01rst part of news article to have at least 100 tokens and contain at least four different en- tities. (3) for each candidate passage, the second part of its corresponding news article was split into sentences by Stanford CoreNLP (Manning et al., 2014). Then we selected the sentences that sat- isfy the following conditions as potential details or consequences of the news event described by the passage: \u2022 Sentences should have at least 10 tokens, as longer sentences contain more information and thus are more likely to be inferrable details or consequences. \u2022 Sentences should not be questions, as we only consider details or consequences of a news event, not questions. \u2022 Sentences should not have 3-gram overlap with the corresponding passage, so they are less likely to be paraphrase of sentences in the pas- sage. \u2022 Sentences should have at least one named entity, so that we can replace it with X to generate a cloze-style query.",
  "\u2022 Sentences should not have 3-gram overlap with the corresponding passage, so they are less likely to be paraphrase of sentences in the pas- sage. \u2022 Sentences should have at least one named entity, so that we can replace it with X to generate a cloze-style query. \u2022 All named entities in sentences should have precedents in the passage according to corefer- ence, so that the sentences are not too discon- nected from the passage, and the correct entity can be found in the passage to \ufb01ll in X. Finally, we generated queries by replacing enti- ties in the selected sentences with X. We only replaced one entity in the selected sentence each time, and generated one cloze-style query. Based on coreference, the precedents of the replaced en-",
  "Copyright infringement suit filed against Led Zeppelin for \u2018Stairway to Heaven\u2019 By Lisa Respers France, CNN updated 12:49 PM EDT, Tue June 3, 2014 STORY HIGHLIGHTS \u2022 Suit claims similarity  between two songs \u2022 Randy California was  guitarist for the group Spirit \u2022 Jimmy Page has called the  accusation \"ridiculous\" (CNN) -- A lawsuit has been filed claiming that the iconic Led Zeppelin song  \"Stairway to Heaven\" was far from original. The suit, filed on May 31 in the United States District Court Eastern District of  Pennsylvania, was brought by the estate of the late musician Randy  California against the surviving members of Led Zeppelin and their record  label. The copyright infringement case alleges that the Zeppelin song was  taken from the single \"Taurus\" by the 1960s band Spirit, for whom California  served as lead guitarist. \"Late in 1968, a then new band named Led Zeppelin began touring in the  United States, opening for Spirit,\" the suit states. \"It was during this time that  Jimmy Page, Led Zeppelin's guitarist, grew familiar with 'Taurus' and the rest  of Spirit's catalog.",
  "\"It was during this time that  Jimmy Page, Led Zeppelin's guitarist, grew familiar with 'Taurus' and the rest  of Spirit's catalog. Page stated in interviews that he found Spirit to be 'very  good' and that the band's performances struck him 'on an emotional level.' \" One of the causes of action for the suit is listed as \"Falsification of Rock N'  Roll History\" and the typeface in the section headings of the filing resembles  that used for Led Zeppelin album covers. According to claims in the suit,  \"Parts of 'Stairway to Heaven,' instantly recognizable to the music fans  across the world, sound almost identical to significant portions of 'Taurus.' \" \u2026\u2026. Passage (Cloze-style) Query According to claims in the suit, \"Parts of 'Stairway to  Heaven,' instantly recognizable to the music fans across  the world, sound almost identical to signi\ufb01cant portions  of \u2018X.\u2019\u201d Reference Answers Taurus (CNN) -- A lawsuit has been \ufb01led claiming that the  iconic\u00a0Led Zeppelin\u00a0song \"Stairway to Heaven\" was far  from original.",
  "The suit, \ufb01led on May 31 in the\u00a0United  States District Court Eastern District of Pennsylvania,  was brought by the estate of the late musician\u00a0Randy  California \u00a0 against  the  surviving  members  of \u00a0 Led  Zeppelin \u00a0 and  their  record  label.  The  copyright  infringement case alleges that the\u00a0Zeppelin\u00a0song was  taken from the single \"Taurus\" by the\u00a01960s band Spirit,  for whom\u00a0California\u00a0served as lead guitarist. \"Late in  1968, a then new band named\u00a0Led Zeppelin\u00a0began  touring in the\u00a0United States, opening for Spirit,\" the suit  states. \"It was during this time that\u00a0Jimmy Page,\u00a0Led  Zeppelin's guitarist, grew familiar with 'Taurus' and the  rest of\u00a0Spirit's catalog. Page stated in interviews that he  found  Spirit  to  be  'very  good'  and  that  the  band's  performances struck him 'on an emotional level.'",
  "Page stated in interviews that he  found  Spirit  to  be  'very  good'  and  that  the  band's  performances struck him 'on an emotional level.' \" \u2022 Suit claims similarities between two songs \u2022 Randy California was guitarist for the group Spirit \u2022 Jimmy Page has called the accusation \"ridiculous\" The \ufb01rst few paragraphs  and the bullet points of the  news article summarize  the news event. The rest of the news  article provides details or  concequences of the  new event. The hidden commonsense  is used in comprehension  of the underlined sentence  (If two songs are claimed similar, it is likely that (parts of) these songs  sound almost identical.) Figure 3: Passage-query-answers generation from a CNN news article. tity in the passage became reference answers to the query. The passage-query-answers genera- tion process matched our task de\ufb01nition in Sec- tion 2, and therefore created queries that require some aspect of reasoning beyond immediate pat- tern matching. In total, we generated 770k (pas- sage, query, answers) triples.",
  "The passage-query-answers genera- tion process matched our task de\ufb01nition in Sec- tion 2, and therefore created queries that require some aspect of reasoning beyond immediate pat- tern matching. In total, we generated 770k (pas- sage, query, answers) triples. 3.3 Machine Filtering As discussed in Jia and Liang (2017); Rajpurkar et al. (2018); Wang and Bansal (2018); Kaushik and Lipton (2018), existing MRC models mostly learn to predict the answer by simply paraphrasing questions into declarative forms, and then match- ing them with the sentences in the passages. To overcome this limitation, we \ufb01ltered out triples whose queries can be easily answered by the state- of-the-art MRC architecture, Stochastic Answer Networks (SAN) (Liu et al., 2018). We choose SAN because it is competitive on existing MRC datasets, and it has components widely used in many MRC architectures such that low bias was anticipated in the \ufb01ltering (which is con\ufb01rmed by evaluation in Section 5).",
  "We choose SAN because it is competitive on existing MRC datasets, and it has components widely used in many MRC architectures such that low bias was anticipated in the \ufb01ltering (which is con\ufb01rmed by evaluation in Section 5). We used SAN to perform a \ufb01ve-fold cross validation on all 770k triples. The SAN models correctly answered 68% of these triples. We excluded those triples, and only kept 244k triples that could not be answered by SAN. These triples contain queries which could not be answered by simple paraphrasing, and other types of reasoning such as commonsense reasoning and multi-sentence reasoning are needed. 3.4 Human Filtering Since the \ufb01rst three stages of data collection were fully automated, the resulted triples could be noisy and ambiguous to human readers. Therefore, we employed crowdworkers to validate these triples. We used Amazon Mechanical Turk for validation.",
  "3.4 Human Filtering Since the \ufb01rst three stages of data collection were fully automated, the resulted triples could be noisy and ambiguous to human readers. Therefore, we employed crowdworkers to validate these triples. We used Amazon Mechanical Turk for validation. Crowdworkers were required to: 1) have a 95% HIT acceptance rate, 2) a minimum of 50 HITs, 3) be located in the United States, Canada, or Great Britain, and 4) not be granted the quali\ufb01cation of poor quality (which we will explain later in this section). Workers were asked to spend at least 30 seconds on each assignment, and paid $3.6 per hour on average. Figure 4 shows the crowdsourcing web inter- face. Each HIT corresponds to a triple in our data collection. In each HIT assignment, we \ufb01rst showed the expandable instructions for \ufb01rst-time workers, to help them better understand our task (see the Appendix A.2). Then we presented work- ers with a passage in which the named entities are highlighted and clickable.",
  "In each HIT assignment, we \ufb01rst showed the expandable instructions for \ufb01rst-time workers, to help them better understand our task (see the Appendix A.2). Then we presented work- ers with a passage in which the named entities are highlighted and clickable. After reading the pas- sage, workers were given a supported statement with a placeholder (i.e., a cloze-style query) in- dicating a missing entity. Based on their under- standing of the events that might be inferred from the passage, workers were asked to \ufb01nd the correct entity in the passage that best \ufb01ts the placeholder. If workers thought the answer is not obvious, they were allowed to guess one, and were required to report that case in the feedback box. Workers were also encouraged to write other feedback.",
  "Figure 4: The crowdsourcing web interface. To ensure quality and prevent spamming, we used the reference answers in the triples to com- pute workers\u2019 average performance after every 1000 submissions. While there might be corefer- ence or named entity recognition errors in the ref- erence answers, as reported in Chen et al. (2016) (also con\ufb01rmed by our analysis in Section 4), they only accounted for a very small portion of all the reference answers. Thus, the reference an- swers could be used for comparing workers\u2019 per- formance. Speci\ufb01cally, if a worker\u2019s performance was signi\ufb01cantly lower than the average perfor- mance of all workers, we blocked the worker by granting the quali\ufb01cation of poor quality. In prac- tice, workers were able to correctly answer about 50% of all queries. We blocked workers if their average accuracy was lower than 20%, and then republished their HIT assignments. Overall, 2,257 crowdworkers have participated in our task, and 51 of them have been granted the quali\ufb01cation of poor quality. Train / Dev.",
  "We blocked workers if their average accuracy was lower than 20%, and then republished their HIT assignments. Overall, 2,257 crowdworkers have participated in our task, and 51 of them have been granted the quali\ufb01cation of poor quality. Train / Dev. / Test Splits Among all the 244k triples collected from the third stage, we \ufb01rst ob- tained one worker answer for each triple. Com- pared to the reference answers, workers correctly answered queries in 122k triples. We then se- lected around 100k correctly-answered triples as the training set, restricting the origins of these triples to the news articles used in Hermann et al. (2015). As for the development and test sets, we solicited another worker answer to further ensure their quality. Therefore, each of the rest 22k triples has been validated by two workers. We only kept 20k triples that were correctly answered by both workers. The origins of these triples are either articles used in Hermann et al. (2015) or articles crawled by us (as described in Section 3.1), with a ratio of 3:7.",
  "We only kept 20k triples that were correctly answered by both workers. The origins of these triples are either articles used in Hermann et al. (2015) or articles crawled by us (as described in Section 3.1), with a ratio of 3:7. Finally, we randomly split the 20k triples into development and test sets, with 10k triples for each set. Table 1 summarizes the statis- tics of our dataset, ReCoRD. Train Dev. Test Overall queries 100,730 10,000 10,000 120,730 unique passages 65,709 7,133 7,279 80,121 passage vocab. 352,491 93,171 94,386 395,356 query vocab.",
  "Train Dev. Test Overall queries 100,730 10,000 10,000 120,730 unique passages 65,709 7,133 7,279 80,121 passage vocab. 352,491 93,171 94,386 395,356 query vocab. 119,069 30,844 31,028 134,397 tokens / passage 169.5 168.6 168.1 169.3 entities / passage 17.8 17.5 17.3 17.8 tokens / query 21.3 22.1 22.2 21.4 Table 1: Statistics of ReCoRD 4 Data Analysis ReCoRD differs from other reading comprehen- sion datasets due to its unique requirement for rea- soning more than just paraphrasing. In this sec- tion, we provide a qualitative analysis of ReCoRD which highlights its unique features. Reasoning Types We sampled 100 examples from the development set, and then manually catego- rized them into types shown in table 2.",
  "In this sec- tion, we provide a qualitative analysis of ReCoRD which highlights its unique features. Reasoning Types We sampled 100 examples from the development set, and then manually catego- rized them into types shown in table 2. The results show that signi\ufb01cantly different from existing datasets such as SQuAD (Rajpurkar et al., 2016), and NewsQA (Trischler et al., 2017), ReCoRD requires commonsense reasoning to answer 75% of queries. Owing to the machine \ufb01ltering stage, only 3% queries could be answered by paraphras- ing. The small percentage (6%) of ambiguous queries demonstrate the bene\ufb01t of the human \ufb01l- tering stage. We also noticed that 10% queries can be answered through partial clues. As the exam- ple shows, some of partial clues were caused by the incompleteness of named entity recognition in the stage of news article curation. Types of Commonsense Reasoning Formaliz- ing the commonsense knowledge needed for even simple reasoning problems is a huge undertaking.",
  "As the exam- ple shows, some of partial clues were caused by the incompleteness of named entity recognition in the stage of news article curation. Types of Commonsense Reasoning Formaliz- ing the commonsense knowledge needed for even simple reasoning problems is a huge undertaking. Based on the observation of the sampled queries that required commonsense reasoning, we roughly categorized them into the following four coarse- gained types:",
  "Reasoning Description Example % Paraphrasing The answer sentence can be found by paraphrasing the query with some syntactic or lexical variation. P: . . . Ralph Roberts. . . then acquired other cable sys- tems, changed the name of the company to Comcast and ran the company until he was aged 82 Q: X began acquiring smaller cable systems and built the company into the nation\u2019s \ufb01fth-largest by 1988. A: [Ralph Roberts] 3% Partial Clue Although a complete semantic match cannot be found between the query and the passage, the answer can be in- ferred through partial clues, such as some word/concept overlap. P:. . . Hani Al-Sibai says he has \u2018severe mobility prob- lems\u2019 to get disability cash. . . Q: However the photographs caught X-Sibai walking with apparent ease in the sunshine. A: [Hani Al] 10% Multi-sentence Reasoning It requires anaphora, or higher-level fusion of multiple sentences to \ufb01nd the answer. P: Donald Trump is of\ufb01cially a $10 billion man.",
  "A: [Hani Al] 10% Multi-sentence Reasoning It requires anaphora, or higher-level fusion of multiple sentences to \ufb01nd the answer. P: Donald Trump is of\ufb01cially a $10 billion man. . . HIs campaign won\u2019t release a copy of the \ufb01nancial disclo- sure even though the FEC says it can do so on its own. . . Q: The X campaign did provide a one-page summary of the billionaire\u2019s investment portfolio, which is re- markably modest for a man of his means. A: [Donald Trump] 6% Commonsense Reasoning It requires inference drew on common sense as well as multi-sentence rea- soning to \ufb01nd the answer. P: . . . Daniela Hantuchova knocks Venus Williams out of Eastbourne 6-2 5-7 6-2 . . . Q: Hantuchova breezed through the \ufb01rst set in just un- der 40 minutes after breaking Williams\u2019 serve twice to take it 6-2 and led the second 4-2 before X hit her stride.",
  ". . Q: Hantuchova breezed through the \ufb01rst set in just un- der 40 minutes after breaking Williams\u2019 serve twice to take it 6-2 and led the second 4-2 before X hit her stride. A: [Venus Williams] 75% Ambiguous The passage is not informative enough, or the query does not have a unique answer. P: The supermarket wars have heated up with the chief executive of Wesfarmers suggesting successful ri- val Aldi may not be paying its fair share of tax in Australia. . . Q: X\u2019s average corporate tax rate for the last three years was almost 31 per cent of net pro\ufb01t, and in 2013 it paid $81.6 million in income tax. A: [Aldi] 6% Table 2: An analysis of types of reasoning needed in 100 random samples from the dev. set of ReCoRD. Conceptual Knowledge: the presumed knowl- edge of properties of concepts (Miller, 1995; Liu and Singh, 2004; Pas\u00b8ca and Van Durme, 2008; Zhang et al., 2017).",
  "set of ReCoRD. Conceptual Knowledge: the presumed knowl- edge of properties of concepts (Miller, 1995; Liu and Singh, 2004; Pas\u00b8ca and Van Durme, 2008; Zhang et al., 2017). Causal Reasoning: the causal bridging infer- ence invoked between two events, which is vali- dated against common sense (Singer et al., 1992; Roemmele et al., 2011). Na\u00a8\u0131ve Psychology: the predictable human men- tal states in reaction to events (Stich and Raven- scroft, 1994). Other: Other types of common sense, such as social norms, planning, spatial reasoning, etc. We annotated one or more types to each of these queries, and computed the percentage of them in these queries as shown in Table 3. 5 Evaluation We are interested in the performance of existing MRC architectures on ReCoRD.",
  "We annotated one or more types to each of these queries, and computed the percentage of them in these queries as shown in Table 3. 5 Evaluation We are interested in the performance of existing MRC architectures on ReCoRD. According to the task de\ufb01nition in Section 2, ReCoRD can be for- malized as two types of machine reading com- prehension (MRC) datasets: passages with cloze- style queries, or passages with queries whose answers are spans in the passage. Therefore, we can evaluate two types of MRC models on ReCoRD, and compare them with human perfor- mance. All the evaluation is carried out based on the train /dev. /test split as illustrated in Table 1. 5.1 Methods DocQA2 (Clark and Gardner, 2018) is a strong baseline model for queries with extractive an- swers. It consists of components such as bi- directional attention \ufb02ow (Seo et al., 2016) and self attention which are widely used in MRC mod- els.",
  "It consists of components such as bi- directional attention \ufb02ow (Seo et al., 2016) and self attention which are widely used in MRC mod- els. We also evaluate DocQA with ELMo (Peters et al., 2018) to analyze the impact of largely pre- trained encoder on our dataset. 2https://github.com/allenai/ document-qa",
  "Reasoning Example % Conceptual Knowledge P: Suspended hundreds of feet in the air amid glistening pillars of ice illuminated with ghostly lights from below, this could easily be a computer-generated scene from the latest sci-\ufb01block- buster movie. But in fact these ethereal photographs were taken in real life. . . captured by photographer Thomas Senf as climber Stephan Siegrist, 43, scaled frozen waterfall. . . Q: With bright lights illuminating his efforts from below, MrX appears to be on the set of a sci-\ufb01movie. A: [Stephan Siegrist] Commonsense knowledge: Scenes such as \u201ca person suspended hundreds of feet in the air amid glistening pillars of ice illuminated with ghostly lights from below\u201d tend to be found in sci-\ufb01movies. 49.3% Causal Reasoning P: ...Jamie Lee Sharp, 25, stole keys to \u00a340,000 Porsche Boxster during raid. . .",
  "49.3% Causal Reasoning P: ...Jamie Lee Sharp, 25, stole keys to \u00a340,000 Porsche Boxster during raid. . . He \ufb01lmed him- self boasting about the car before getting behind the wheel Q: X was jailed for four years after pleading guilty to burglary, aggravated vehicle taking, driving whilst disquali\ufb01ed, drink-driving and driving without insurance. A: [Jamie Lee Sharp] Commonsense knowledge: If a person steals a car, the person may be arrested and jailed. 32.0% Na\u00a8\u0131ve Psychology P: Uruguay star Diego Forlan said Monday that he is leaving Atletico Madrid and is set to join Serie A Inter Milan...Forlan said \u201c. . . At the age of 33, going to a club like Inter is not an opportunity that comes up often. . . \u201d Q: \u201cI am happy with the decision that I have taken, it is normal that some players come and others go,\u201d X added. A: [Diego Forlan, Forlan] Commonsense knowledge: If a person has seized an valuable opportunity, the person will feel happy for it.",
  "\u201d Q: \u201cI am happy with the decision that I have taken, it is normal that some players come and others go,\u201d X added. A: [Diego Forlan, Forlan] Commonsense knowledge: If a person has seized an valuable opportunity, the person will feel happy for it. 28.0% Other P: A British backpacker who wrote a romantic note to locate a handsome stranger after spot- ting him on a New Zealand beach has \ufb01nally met her Romeo for the \ufb01rst time. Sarah Milne, from Glasgow, left a handmade poster for the man, who she saw in Picton on Friday. . . She said she would return to the same spot in Picton, New Zealand, on Tuesday in search for him...William Scott Chalmers revealed himself as the man and went to meet her. . .",
  ". . She said she would return to the same spot in Picton, New Zealand, on Tuesday in search for him...William Scott Chalmers revealed himself as the man and went to meet her. . . Q: Mr Chalmers, who brought a bottle of champagne with him, walked over to where Milne was sitting and said \u201cHello, I\u2019m X, you know you could have just asked for my number.\u201d A: [William Scott Chalmers] Commonsense knowledge: When two people meet each other for the \ufb01rst time, they will likely \ufb01rst introduce themselves. 12.0% Table 3: An analysis of speci\ufb01c types of commonsense reasoning in 75 random sampled queries illustrated in Table 2 which requires common sense reasoning. A query may require multiple types of commonsense reasoning. . QANet3 (Yu et al., 2018) is one of the top MRC models for SQuAD-style datasets. It is differ- ent from many other MRC models due to the use of transformer (Vaswani et al., 2017).",
  ". QANet3 (Yu et al., 2018) is one of the top MRC models for SQuAD-style datasets. It is differ- ent from many other MRC models due to the use of transformer (Vaswani et al., 2017). Through QANet, we can evaluate the reasoning ability of transformer on our dataset. SAN4 (Liu et al., 2018) is also a top-rank MRC model. It shares many components with DocQA, and employs a stochastic answer module. Since we used SAN to \ufb01lter out easy queries in our data collection, it is necessary to verify that the queries we collect is hard for not only SAN but also other MRC architectures. ASReader5 (Kadlec et al., 2016) is a strong base- line model for cloze-style datasets such as (Her- 3The of\ufb01cial implementation of QANet is not released. We use the implementation at https://github.com/ NLPLearn/QANet.",
  "We use the implementation at https://github.com/ NLPLearn/QANet. 4https://github.com/kevinduh/san_mrc 5https://github.com/rkadlec/asreader mann et al., 2015; Hill et al., 2015). Unlike other baseline models which search among all text spans in the passage, ASReader directly predicts an- swers from the candidate named entities. Language Models6 (LMs) (Trinh and Le, 2018) trained on large corpora recently achieved the state-of-the-art scores on the Winograd Schema Challenge (Levesque et al., 2011). Following in the same manner, we \ufb01rst concatenate the passage and the query together as a long sequence, and substitute X in the long sequence with each candi- date entity; we use LMs to compute the probabil- ity of each resultant sequence and the substitution that results in the most probable sequence will be the predicted answer. Random Guess acts as the lower bound of the evaluated models. It considers the queries in our 6https://github.com/tensorflow/models/ tree/master/research/lm_commonsense",
  "dataset as cloze style, and randomly picks a candi- date entity from the passage as the answer. 5.2 Human Performance As described in Section 3.4, we obtained two worker answers for each query in the development and test sets, and con\ufb01rmed that each query has been correctly answered by two different workers. To get human performance, we obtained an addi- tional worker answer for each query, and compare it with the reference answers. 5.3 Metrics We use two evaluation metrics similar to those used by SQuAD (Rajpurkar et al., 2016). Both ignore punctuations and articles (e.g., a, an, the). Exact Match (EM) measures the percentage of predictions that match any one of the reference an- swers exactly. (Macro-averaged) F1 measures the average over- lap between the prediction and the reference an- swers. We treat the prediction and the reference answer as bags of tokens, and compute their F1. We take the maximum F1 over all of the reference answers for a given query, and then average over all of the queries.",
  "We treat the prediction and the reference answer as bags of tokens, and compute their F1. We take the maximum F1 over all of the reference answers for a given query, and then average over all of the queries. 5.4 Results We show the evaluation results in Table 4. Hu- mans are able to get 91.31 EM and 91.69 F1 on the set, with similar results on the develop- ment set. In contrast, the best automatic method \u2013 DocQA with ELMo \u2013 achieves 45.44 EM and 46.65 F1 on the test set, illustrating a signi\ufb01cant gap between human and machine reading com- prehension on ReCoRD. All other methods with- out ELMo get EM/F1 scores signi\ufb01cantly lower than DocQA with ELMo, which shows the posi- tive impact of ELMo (see in Section 5.5). We also note that SAN leads to a result comparable with other strong baseline methods.",
  "We also note that SAN leads to a result comparable with other strong baseline methods. This con\ufb01rms that since SAN shares general components with many MRC models, using it to do machine \ufb01ltering does help us \ufb01lter out queries that are relatively easy to all the methods we evaluate. Finally, to our sur- prise, the unsupervised method (i.e., LM) which achieved the state-of-the-art scores on the Wino- grad Schema Challenge only leads to a result sim- ilar to the random guess baseline: a potential ex- planation is the lack of domain knowledge on our dataset. We leave this question for future work. Exact Match F1 Dev. Test Dev.",
  "We leave this question for future work. Exact Match F1 Dev. Test Dev. Test Human 91.28 91.31 91.64 91.69 DocQA w/ ELMo 44.13 45.44 45.39 46.65 DocQA w/o ELMo 36.59 38.52 37.89 39.76 SAN 38.14 39.77 39.09 40.72 QANet 35.38 36.51 36.75 37.79 ASReader 29.24 29.80 29.80 30.35 LM 16.73 17.57 17.41 18.15 Random Guess 18.41 18.55 19.06 19.12 Table 4: Performance of various methods and human. 5.5 Analysis Human Errors About 8% dev./test queries have not been correctly answered in the human evalua- tion. We analyzed samples from these queries, and found that in most queries human was able to nar- row down the set of possible candidate entities, but not able to \ufb01nd a unique answer.",
  "We analyzed samples from these queries, and found that in most queries human was able to nar- row down the set of possible candidate entities, but not able to \ufb01nd a unique answer. In many cases, two candidate entities equally \ufb01t X unless human has the speci\ufb01c background knowledge. We show an example in the Appendix A.1. For the method analysis, we mainly ana- lyzed the results of three representative methods: DocQA w/ ELMo, DocQA, and QANet. QANet DocQA DocQA w/ ELMo Human Human DocQA w/ ELMo DocQA QANet Figure 5: The Venn diagram of correct predictions from various methods and human on the development set. Impact of ELMo As shown in Figure 5, among all three methods the correct predictions of DocQA w/ ELMo have the largest overlap (92.6%) with the human predictions. As an ablation study, we analyzed queries which were only correctly an- swered after ELMo was added. We found that in some cases ELMo helped the prediction by incor- porating the knowledge of language models.",
  "As an ablation study, we analyzed queries which were only correctly an- swered after ELMo was added. We found that in some cases ELMo helped the prediction by incor- porating the knowledge of language models. We show an example in the Appendix A.1.",
  "Predictions of QANet Figure 5 shows that QANet correctly answered some ambiguous queries, which we think was due to the randomness of parameter initialization and did not re\ufb02ect the true reasoning ability. Since QANet uses the transformer-based encoder and DocQA uses the LSTM-based encoder, we see a signi\ufb01cant differ- ence of predictions between QANet and DocQA. Method OOC Rate DocQA w/ ELMo 6.27% DocQA 6.37% QANet 6.41% Table 5: The out-of-candidate-entities (OOC) rate of three analyzed methods. Impact of Cloze-style Setting Except ASReader, all the MRC models were evaluated under the ex- tractive setting, which means the information of candidate named entities was not used. Instead, extractive models searched answers from all pos- sible text spans in passages. To show the poten- tial bene\ufb01t of using the candidate entities in these models, we computed the percentage of model predictions that could not be found in the can- didate entities.",
  "Instead, extractive models searched answers from all pos- sible text spans in passages. To show the poten- tial bene\ufb01t of using the candidate entities in these models, we computed the percentage of model predictions that could not be found in the can- didate entities. As shown in Table 5, all three methods have about 6% OOC predictions. Mak- ing use of the candidate entities would potentially help them increase the performance by 6%. In Section 4, we manually labeled 100 ran- domly sampled queries with different types of rea- soning. In Figure 6 and 7, we show the perfor- mance of three analyzed methods on these queries. CSR MSR Partial clue Paraphrasing Ambiguous 0 10 20 30 40 50 60 70 DocQA w/ ELMo DocQA QANet Figure 6: Performance of three analyzed methods on the 100 random samples with reasoning types la- beled.(CSR stands for commonsense reasoning, and MSR stands for multi-sentence reasoning.)",
  "(CSR stands for commonsense reasoning, and MSR stands for multi-sentence reasoning.) Figure 6 shows that three methods performed poorly on queries requiring commonsense rea- soning, multi-sentence reasoning and partial clue. Compared to DocQA, QANet performed better on multi-sentence reasoning queries probably due to the use of transformer. Also, QANet outperformed DocQA on paraphrased queries probably because we used SAN to \ufb01ltering queries and SAN has an architecture similar to DocQA. As we expect, ELMo improved the performance of DocQA on paraphrased queries. Conceptual Knowledge Naive Psychology Causality Other 0 10 20 30 40 50 60 DocQA w/ ELMo DocQA QANet Figure 7: Performance of three analyzed methods on 75% of the random samples with speci\ufb01c common- sense reasoning types labeled. Among the 75% sampled queries that require commonsense reasoning, we see that ELMo sig- ni\ufb01cantly improved the performance of common- sense reasoning with presumed knowledge. For all other types of commonsense reasoning, all three methods have relatively poor performance.",
  "Among the 75% sampled queries that require commonsense reasoning, we see that ELMo sig- ni\ufb01cantly improved the performance of common- sense reasoning with presumed knowledge. For all other types of commonsense reasoning, all three methods have relatively poor performance. 6 Related Datasets ReCoRD relates to two strands of research in datasets: data for reading comprehension, and that for commonsense reasoning. Reading Comprehension The CNN/Daily Mail Corpus (Hermann et al., 2015), The Children\u2019s Book Test (CBT) (Hill et al., 2015), and LAM- BADA (Paperno et al., 2016) are closely related to ReCoRD: (1) The CNN/Daily Mail Corpus con- structed queries from the bullet points, most of which required limited reasoning ability (Chen et al., 2016). (2) CBT is a collection of 21 con- secutive sentences from book excerpts, with one word randomly removed from the last sentence. Since CBT has no machine or human \ufb01ltering to ensure quality, only a small portion of the CBT examples really probes machines\u2019 ability to under- stand the context.",
  "Since CBT has no machine or human \ufb01ltering to ensure quality, only a small portion of the CBT examples really probes machines\u2019 ability to under- stand the context. (3) Built in a similar manner to CBT, LAMBADA was \ufb01ltered to be human- guessable in the broader context only. Differing from ReCoRD, LAMBADA was designed to be a language modeling problem where contexts were",
  "not required to be event summaries, and answers were not necessarily in the context. Since all candidate answers were extracted from in the passage, ReCoRD can also be formalized as a extractive MRC dataset, similar to SQuAD (Ra- jpurkar et al., 2016) and NewsQA (Trischler et al., 2017). The difference is that questions in these datasets were curated from crowdworkers. Since it is hard to control the quality of crowdsourced questions, a large portion of questions in these datasets can be answered by word matching or paraphrasing (Jia and Liang, 2017; Rajpurkar et al., 2018; Wang and Bansal, 2018). There are other large-scale datasets (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017; Dunn et al., 2017; Kocisky et al., 2018; Reddy et al., 2018; Choi et al., 2018; Yang et al., 2018) targeting different aspects of reading comprehension. See (Gao et al., 2018) for a recent survey.",
  "See (Gao et al., 2018) for a recent survey. Commonsense Reasoning ROCStories Cor- pus (Mostafazadeh et al., 2016), SWAG (Zellers et al., 2018), and The Winograd Schema Chal- lenge (WSC) (Levesque et al., 2011) are related ReCoRD: (1) ROCStories assesses commonsense reasoning in story understanding by choosing the correct story ending from only two candi- dates. Stories in the corpus were all curated from crowdworkers, which could suffer from human elicitation bias (Gordon and Van Durme, 2013; Misra et al., 2016; Zhang et al., 2017). (2) SWAG uni\ufb01es commonsense reasoning and natural language inference. It selects an ending from multiple choices which is most likely to be anticipated from the situation describe in the premise. The counterfactual endings in SWAG were generated using language models with adversarial \ufb01ltering. (3) WSC foucses on intra-sentential pronoun disambiguation problems that require commonsense reasoning.",
  "The counterfactual endings in SWAG were generated using language models with adversarial \ufb01ltering. (3) WSC foucses on intra-sentential pronoun disambiguation problems that require commonsense reasoning. There are other datasets (Roemmele et al., 2011; Zhang et al., 2017; Rashkin et al., 2018a,b) targeting different aspects of commonsense reasoning. 7 Conclusion We introduced ReCoRD, a large-scale reading comprehension dataset requiring commonsense reasoning. Unlike existing machine reading com- prehension (MRC) datasets, ReCoRD contains a large portion of queries that require commonsense reasoning to be answered. Our baselines, includ- ing top performers on existing MRC datasets, are no match for human competence on ReCoRD. We hope that ReCoRD will spur more research in MRC with commonsense reasoning. References Danqi Chen, Jason Bolton, and Christopher D. Man- ning. 2016. A thorough examination of the cnn/daily mail reading comprehension task.",
  "We hope that ReCoRD will spur more research in MRC with commonsense reasoning. References Danqi Chen, Jason Bolton, and Christopher D. Man- ning. 2016. A thorough examination of the cnn/daily mail reading comprehension task. In Pro- ceedings of the 54th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers), pages 2358\u20132367, Berlin, Germany. Asso- ciation for Computational Linguistics. Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen- tau Yih, Yejin Choi, Percy Liang, and Luke Zettle- moyer. 2018. Quac: Question answering in context. arXiv preprint arXiv:1808.07036. Christopher Clark and Matt Gardner. 2018. Simple and effective multi-paragraph reading comprehen- sion. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 845\u2013855. Association for Computational Linguistics.",
  "2018. Simple and effective multi-paragraph reading comprehen- sion. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 845\u2013855. Association for Computational Linguistics. Matthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, Volkan Cirik, and Kyunghyun Cho. 2017. Searchqa: A new q&a dataset augmented with context from a search engine. arXiv preprint arXiv:1704.05179. Jianfeng Gao, Michel Galley, and Lihong Li. 2018. Neural approaches to conversational ai. arXiv preprint arXiv:1809.08267. Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson Liu, Matthew Pe- ters, Michael Schmitz, and Luke Zettlemoyer. 2018. Allennlp: A deep semantic natural language pro- cessing platform. arXiv preprint arXiv:1803.07640.",
  "2018. Allennlp: A deep semantic natural language pro- cessing platform. arXiv preprint arXiv:1803.07640. Jonathan Gordon and Benjamin Van Durme. 2013. Re- porting bias and knowledge acquisition. In Proceed- ings of the 2013 Workshop on Automated Knowledge Base Construction, AKBC \u201913, pages 25\u201330, New York, NY, USA. ACM. Karl Moritz Hermann, Tom\u00b4a\u02c7s Ko\u02c7cisk\u00b4y, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su- leyman, and Phil Blunsom. 2015. Teaching ma- chines to read and comprehend. In Advances in Neu- ral Information Processing Systems, pages 1693\u2013 1701. Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston. 2015. The goldilocks principle: Reading children\u2019s books with explicit memory representa- tions. arXiv preprint arXiv:1511.02301. Robin Jia and Percy Liang.",
  "2015. The goldilocks principle: Reading children\u2019s books with explicit memory representa- tions. arXiv preprint arXiv:1511.02301. Robin Jia and Percy Liang. 2017. Adversarial exam- ples for evaluating reading comprehension systems. In Proceedings of the 2017 Conference on Empiri- cal Methods in Natural Language Processing, pages",
  "2021\u20132031, Copenhagen, Denmark. Association for Computational Linguistics. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehen- sion. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 1601\u20131611, Vancouver, Canada. Association for Computational Linguistics. Rudolf Kadlec, Martin Schmid, Ond\u02c7rej Bajgar, and Jan Kleindienst. 2016. Text understanding with the at- tention sum reader network. In Proceedings of the 54th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 908\u2013918. Association for Computational Linguis- tics. Divyansh Kaushik and Zachary C. Lipton. 2018. How much reading does reading comprehension require? a critical investigation of popular benchmarks.",
  "Association for Computational Linguis- tics. Divyansh Kaushik and Zachary C. Lipton. 2018. How much reading does reading comprehension require? a critical investigation of popular benchmarks. Tomas Kocisky, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, and Edward Grefenstette. 2018. The narrativeqa reading comprehension challenge. Transactions of the Asso- ciation for Computational Linguistics, 6:317\u2013328. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. Race: Large-scale reading comprehension dataset from examinations. In Pro- ceedings of the 2017 Conference on Empirical Meth- ods in Natural Language Processing, pages 785\u2013 794, Copenhagen, Denmark. Association for Com- putational Linguistics. Kenton Lee, Luheng He, Mike Lewis, and Luke Zettle- moyer. 2017. End-to-end neural coreference reso- lution.",
  "Association for Com- putational Linguistics. Kenton Lee, Luheng He, Mike Lewis, and Luke Zettle- moyer. 2017. End-to-end neural coreference reso- lution. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Process- ing, pages 188\u2013197, Copenhagen, Denmark. Asso- ciation for Computational Linguistics. Hector J Levesque, Ernest Davis, and Leora Morgen- stern. 2011. The winograd schema challenge. In Aaai spring symposium: Logical formalizations of commonsense reasoning. H. Liu and P. Singh. 2004. Conceptnet &mdash; a practical commonsense reasoning tool-kit. BT Tech- nology Journal, 22(4):211\u2013226. Xiaodong Liu, Yelong Shen, Kevin Duh, and Jianfeng Gao. 2018. Stochastic answer networks for ma- chine reading comprehension.",
  "BT Tech- nology Journal, 22(4):211\u2013226. Xiaodong Liu, Yelong Shen, Kevin Duh, and Jianfeng Gao. 2018. Stochastic answer networks for ma- chine reading comprehension. In Proceedings of the 56th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 1694\u20131704. Association for Computational Linguis- tics. Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David Mc- Closky. 2014. The Stanford CoreNLP natural lan- guage processing toolkit. In Association for Compu- tational Linguistics (ACL) System Demonstrations, pages 55\u201360. John McCarthy. 1959. Programs with common sense. In Proceedings of the Teddington Conference on the Mechanization of Thought Processes, London: Her Majesty\u2019s Stationery Of\ufb01ce. George A. Miller. 1995. Wordnet: A lexical database for english. Commun.",
  "1959. Programs with common sense. In Proceedings of the Teddington Conference on the Mechanization of Thought Processes, London: Her Majesty\u2019s Stationery Of\ufb01ce. George A. Miller. 1995. Wordnet: A lexical database for english. Commun. ACM, 38(11):39\u201341. Ishan Misra, C Lawrence Zitnick, Margaret Mitchell, and Ross Girshick. 2016. Seeing through the human reporting bias: Visual classi\ufb01ers from noisy human- centric labels. In Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition, pages 2930\u20132939. Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A cor- pus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 839\u2013849, San Diego, California.",
  "2016. A cor- pus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 839\u2013849, San Diego, California. Association for Computational Linguis- tics. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. Ms marco: A human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268. Denis Paperno, Germ\u00b4an Kruszewski, Angeliki Lazari- dou, Ngoc Quan Pham, Raffaella Bernardi, San- dro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. 2016. The lambada dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1525\u20131534. Association for Computational Linguistics.",
  "2016. The lambada dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1525\u20131534. Association for Computational Linguistics. Marius Pas\u00b8ca and Benjamin Van Durme. 2008. Weakly-supervised acquisition of open-domain classes and class attributes from web documents and query logs. In Proceedings of ACL-08: HLT, pages 19\u201327. Association for Computational Linguistics. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227\u2013 2237. Association for Computational Linguistics. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don\u2019t know: Unanswerable ques- tions for squad.",
  "Association for Computational Linguistics. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don\u2019t know: Unanswerable ques- tions for squad. In Proceedings of the 56th Annual Meeting of the Association for Computational Lin- guistics (Volume 2: Short Papers), pages 784\u2013789. Association for Computational Linguistics.",
  "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natu- ral Language Processing, pages 2383\u20132392, Austin, Texas. Association for Computational Linguistics. Hannah Rashkin, Antoine Bosselut, Maarten Sap, Kevin Knight, and Yejin Choi. 2018a. Modeling naive psychology of characters in simple common- sense stories. In Proceedings of the 56th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), pages 2289\u2013 2299. Association for Computational Linguistics. Hannah Rashkin, Maarten Sap, Emily Allaway, Noah A. Smith, and Yejin Choi. 2018b. Event2mind: Commonsense inference on events, intents, and reactions. In Proceedings of the 56th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 463\u2013473. Association for Computational Linguistics.",
  "2018b. Event2mind: Commonsense inference on events, intents, and reactions. In Proceedings of the 56th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 463\u2013473. Association for Computational Linguistics. Siva Reddy, Danqi Chen, and Christopher D Manning. 2018. Coqa: A conversational question answering challenge. arXiv preprint arXiv:1808.07042. Melissa Roemmele, Cosmin Adrian Bejan, and An- drew S Gordon. 2011. Choice of plausible alterna- tives: An evaluation of commonsense causal reason- ing. In AAAI Spring Symposium: Logical Formal- izations of Commonsense Reasoning, pages 90\u201395. Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Bidirectional attention \ufb02ow for machine comprehension. arXiv preprint arXiv:1611.01603.",
  "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Bidirectional attention \ufb02ow for machine comprehension. arXiv preprint arXiv:1611.01603. Murray Singer, Michael Halldorson, Jeffrey C Lear, and Peter Andrusiak. 1992. Validation of causal bridging inferences in discourse understanding. Journal of Memory and Language, 31(4):507 \u2013 524. Stephen Stich and Ian Ravenscroft. 1994. What is folk psychology? Cognition, 50(1-3):447\u2013468. Krysta Svore, Lucy Vanderwende, and Christopher Burges. 2007. Enhancing single-document sum- marization by combining RankNet and third-party sources. In Proceedings of the 2007 Joint Con- ference on Empirical Methods in Natural Lan- guage Processing and Computational Natural Lan- guage Learning (EMNLP-CoNLL), pages 448\u2013457, Prague, Czech Republic. Association for Computa- tional Linguistics.",
  "In Proceedings of the 2007 Joint Con- ference on Empirical Methods in Natural Lan- guage Processing and Computational Natural Lan- guage Learning (EMNLP-CoNLL), pages 448\u2013457, Prague, Czech Republic. Association for Computa- tional Linguistics. Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847. Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har- ris, Alessandro Sordoni, Philip Bachman, and Ka- heer Suleman. 2017. Newsqa: A machine compre- hension dataset. In Proceedings of the 2nd Work- shop on Representation Learning for NLP, pages 191\u2013200, Vancouver, Canada. Association for Com- putational Linguistics. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need.",
  "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, pages 5998\u20136008. Yicheng Wang and Mohit Bansal. 2018. Robust ma- chine comprehension models via adversarial train- ing. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 2 (Short Papers), pages 575\u2013581. Association for Computational Linguistics. Kristian Woodsend and Mirella Lapata. 2010. Auto- matic generation of story highlights. In Proceed- ings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 565\u2013574, Up- psala, Sweden. Association for Computational Lin- guistics.",
  "2010. Auto- matic generation of story highlights. In Proceed- ings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 565\u2013574, Up- psala, Sweden. Association for Computational Lin- guistics. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben- gio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. 2018. Qanet: Combining local convolution with global self-attention for reading comprehen- sion. arXiv preprint arXiv:1804.09541. Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A large-scale adversarial dataset for grounded commonsense inference.",
  "arXiv preprint arXiv:1804.09541. Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A large-scale adversarial dataset for grounded commonsense inference. In Proceed- ings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP). Sheng Zhang, Rachel Rudinger, Kevin Duh, and Ben- jamin Van Durme. 2017. Ordinal common-sense in- ference. Transactions of the Association for Com- putational Linguistics, 5:379\u2013395.",
  "A Appendices A.1 Case Study Human Error Table 6 shows an example where the ambiguous query caused human error. The passage in this example describes \u201cambiverts\u201d, and there are two experts studying it: \u201cVanessa Van Edwards\u201d and \u201cAdam Grant\u201d. Both of them \ufb01t in the query asking who gave advice to am- biverts. There is no further information to help human choose a unique answer for this query. Passage: Your colleagues think you\u2019re quiet, but your friends think you\u2019re a party animal. If that sounds like you, then you may be what psychologists describe as an \u2019ambivert\u2019. Scientists believe around two-thirds of peo- ple are ambiverts; a personality category that has, up un- til now, been given relatively little attention.",
  "If that sounds like you, then you may be what psychologists describe as an \u2019ambivert\u2019. Scientists believe around two-thirds of peo- ple are ambiverts; a personality category that has, up un- til now, been given relatively little attention. \u2019Most peo- ple who are ambiverts have been told the wrong category their whole life,\u2019 Vanessa Van Edwards, an Orgeon-based behavioural expert, told DailyMail.com \u2019You hear extro- vert and you hear introvert, and you think \u2019ugh, that\u2019s not me\u2019.\u2019 Ambiversion is a label that has been around for some time, but gained popularity in 2013 with a paper in the journal Psychological Science, by Adam Grant the University of Pennsylvania.",
  "\u2022 Most ambiverts have been labelled incorrectly their whole life \u2022 They slide up and down personality spectrum depend- ing on the situation \u2022 Ambiverts are good at gaining people\u2019s trust and mak- ing their point heard \u2022 They often feel pressure to mirror personality of the person they are with Query: \u2019Read each situation more carefully,\u2019 X advised ambiverts, \u2019and ask yourself, \u2019What do I need to do right now to be most happy or successful?\u201d Reference answers: Adam Grant Table 6: An example illustrating a ambiguous query. Impact of ELMo Table 7 shows an example where DocQA w/ ELMo correctly answered but DocQA failed. The passage in this example describes a woman artist \u201cSarah Milne\u201d who launched a public appeal to \ufb01nd a handsome stranger \u201cWilliam Scott Chalmers\u201d, and invited him to meet her. The query asks the missing information in the greetings from \u201cWilliam Scott Chalmers\u201d when he went to meet \u201cSarah Milne\u201d. Our common sense about social norms tells us when two people meet each other for the \ufb01rst time, they are very likely to \ufb01rst introduce themselves. In the query of this example, when Mr.",
  "Our common sense about social norms tells us when two people meet each other for the \ufb01rst time, they are very likely to \ufb01rst introduce themselves. In the query of this example, when Mr. Chalmers said \u201cHello, I\u2019m . . .\u201d, it is very likely that he was introducing himself. Therefore, the name of Mr Chalmer \ufb01t X best. In this example, the prediction of DocQA with- out ELMo is \u201cNew Zealand\u201d which is not even close to the reference answer. The bene\ufb01t of using ELMo in this example is that its language model will help exclude \u201cNew Zealand\u201d from the likely candidate answers, because \u201cI\u2019m ...\u201d is usually followed by a person name rather than a location name. Such a pattern learnt by ELMo is useful in narrowing down candidiate answers in ReCoRD. Passage: A British backpacker who wrote a romantic note to locate a handsome stranger after spotting him on a New Zealand beach has \ufb01nally met her Romeo for the \ufb01rst time.",
  "Such a pattern learnt by ELMo is useful in narrowing down candidiate answers in ReCoRD. Passage: A British backpacker who wrote a romantic note to locate a handsome stranger after spotting him on a New Zealand beach has \ufb01nally met her Romeo for the \ufb01rst time. Sarah Milne, from Glasgow, left a handmade poster for the man, who she saw in Picton on Friday and described as \u2019shirtless, wearing black shorts with stars tat- tooed on his torso and running with a curly, bouncy and blonde dog\u2019. In her note, entitled \u2019Is this you? \u2019, she in- vited the mystery stranger to meet her on the same beach on Tuesday. But the message soon became a source of huge online interest with the identity of both the author and its intended target generating unexpected publicity.",
  "In her note, entitled \u2019Is this you? \u2019, she in- vited the mystery stranger to meet her on the same beach on Tuesday. But the message soon became a source of huge online interest with the identity of both the author and its intended target generating unexpected publicity. \u2022 Sarah Milne, a Glasgow artist, launched a public ap- peal to \ufb01nd the mystery man \u2022 She wrote a heart-warming message and drew a picture of him with his dog \u2022 She said she would return to the same spot in Picton, New Zealand, on Tuesday in search for him \u2022 William Scott Chalmers revealed himself as the man and went to meet her \u2022 He told Daily Mail Australia that he would ask her out for dinner Query: Mr Chalmers, who brought a bottle of cham- pagne with him, walked over to where Milne was sitting and said \u2019Hello, I\u2019m X, you know you could have just asked for my number.\u2019 Reference answers: William Scott Chalmers Table 7: An example illustrating the impact of ELMo. A.2 HIT Instructions We show the instructions for Amazon Mechanical Turk HITs in Figure 8.",
  "Figure 8: Amazon Mechanical Turk HIT Instructions."
]