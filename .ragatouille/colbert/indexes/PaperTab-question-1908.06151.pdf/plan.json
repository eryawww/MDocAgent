{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "The Transference Architecture for Automatic Post-Editing Santanu Pal1,2, Hongfei Xu1,2, Nico Herbig2, Sudip Kumar Naskar3, Antonio Kr\u00a8uger2, Josef van Genabith1,2 1Department of Language Science and Technology, Saarland University, Germany 2German Research Center for Arti\ufb01cial Intelligence (DFKI), Saarland Informatics Campus, Germany 3Jadavpur University, Kolkata, India {santanu.pal, josef.vangenabith}@uni-saarland.de {hongfei.xu, nico.herbig, krueger}@dfki.de, sudip.naskar@gmail.com Abstract In automatic post-editing (APE) it makes sense to condition post-editing (pe) decisions on both the source (src) and the machine translated text (mt) as input. This has led to multi-source encoder based APE approaches. A research challenge now is the search for architectures that best support the capture, preparation and provision of src and mt infor- mation and its integration with pe decisions. In this paper we present a new multi-source APE model, called transference.",
            "A research challenge now is the search for architectures that best support the capture, preparation and provision of src and mt infor- mation and its integration with pe decisions. In this paper we present a new multi-source APE model, called transference. Unlike pre- vious approaches, it (i) uses a transformer en- coder block for src, (ii) followed by a decoder block, but without masking for self-attention on mt, which effectively acts as second en- coder combining src \u2192mt, and (iii) feeds this representation into a \ufb01nal decoder block generating pe. Our model outperforms the state-of-the-art by 1 BLEU point on the WMT 2016, 2017, and 2018 English\u2013German APE shared tasks (PBSMT and NMT). We further investigate the importance of our newly intro- duced second encoder and \ufb01nd that a too small amount of layers does hurt the performance, while reducing the number of layers of the de- coder does not matter much.",
            "We further investigate the importance of our newly intro- duced second encoder and \ufb01nd that a too small amount of layers does hurt the performance, while reducing the number of layers of the de- coder does not matter much. 1 Introduction The performance of state-of-the-art MT systems is not perfect, thus, human interventions are still required to correct machine translated texts into publishable quality translations (TAUS\/CNGL Re- port, 2010). Automatic post-editing (APE) is a method that aims to automatically correct errors made by MT systems before performing actual human post-editing (PE) (Knight and Chander, 1994), thereby reducing the translators\u2019 workload and increasing productivity (Pal et al., 2016a). APE systems trained on human PE data serve as MT post-processing modules to improve the over- all performance. APE can therefore be viewed as a 2nd-stage MT system, translating predictable er- ror patterns in MT output to their corresponding corrections.",
            "APE systems trained on human PE data serve as MT post-processing modules to improve the over- all performance. APE can therefore be viewed as a 2nd-stage MT system, translating predictable er- ror patterns in MT output to their corresponding corrections. APE training data minimally involves MT output (mt) and the human post-edited (pe) version of mt, but additionally using the source (src) has been shown to provide further bene- \ufb01ts (Bojar et al., 2015, 2016, 2017). To provide awareness of errors in mt originat- ing from src, attention mechanisms (Bahdanau et al., 2015) allow modeling of non-local depen- dencies in the input or output sequences, and im- portantly also global dependencies between them (in our case src, mt and pe). The transformer architecture (Vaswani et al., 2017) is built solely upon such attention mechanisms completely re- placing recurrence and convolutions. The trans- former uses positional encoding to encode the in- put and output sequences, and computes both self- and cross-attention through so-called multi-head attentions, which are facilitated by parallelization.",
            "The trans- former uses positional encoding to encode the in- put and output sequences, and computes both self- and cross-attention through so-called multi-head attentions, which are facilitated by parallelization. Such multi-head attention allows to jointly attend to information at different positions from different representation subspaces, e.g. utilizing and com- bining information from src, mt, and pe. In this paper, we present a multi-source neural APE architecture called transference. Our model contains a source encoder which encodes src in- formation, a second encoder (encsrc\u2192mt) which takes the encoded representation from the source encoder (encsrc), combines this with the self- attention-based encoding of mt (encmt), and pre- pares a representation for the decoder (decpe) via cross-attention. Our second encoder (encsrc\u2192mt) can also be viewed as a standard transformer de- coding block, however, without masking, which acts as an encoder. We thus recombine the differ- ent blocks of the transformer architecture and re- purpose them for the APE task in a simple yet ef- fective way.",
            "We thus recombine the differ- ent blocks of the transformer architecture and re- purpose them for the APE task in a simple yet ef- fective way. The suggested architecture is inspired by the two-step approach professional translators arXiv:1908.06151v2  [cs.CL]  26 Aug 2019",
            "tend to use during post-editing: \ufb01rst, the source segment is compared to the corresponding transla- tion suggestion (similar to what our encsrc\u2192mt is doing), then corrections to the MT output are ap- plied based on the encountered errors (in the same way that our decpe uses the encoded representation of encsrc\u2192mt to produce the \ufb01nal translation). The paper makes the following contributions: (i) we propose a new multi-encoder model for APE that consists only of standard transformer en- coding and decoding blocks, (ii) by using a mix of self- and cross-attention we provide a repre- sentation of both src and mt for the decoder, al- lowing it to better capture errors in mt originat- ing from src; this advances the state-of-the-art in APE in terms of BLEU and TER, and (iii), we analyze the effect of varying the number of en- coder and decoder layers (Domhan, 2018), indi- cating that the encoders contribute more than de- coders in transformer-based neural APE.",
            "2 Related Research Recent advances in APE research are directed towards neural APE, which was \ufb01rst proposed by Pal et al. (2016b) and Junczys-Dowmunt and Grundkiewicz (2016) for the single-source APE scenario which does not consider src, i.e. mt \u2192 pe. In their work, Junczys-Dowmunt and Grund- kiewicz (2016) also generated a large synthetic training dataset through back translation, which we also use as additional training data. Exploiting source information as an additional input can help neural APE to disambiguate cor- rections applied at each time step; this naturally leads to multi-source APE ({src, mt} \u2192pe). A multi-source neural APE system can be con\ufb01g- ured either by using a single encoder that encodes the concatenation of src and mt (Niehues et al., 2016) or by using two separate encoders for src and mt and passing the concatenation of both en- coders\u2019 \ufb01nal states to the decoder (Libovick\u00b4y et al., 2016).",
            "A few approaches to multi-source neural APE were proposed in the WMT 2017 APE shared task. Junczys-Dowmunt and Grundkiewicz (2017) combine both mt and src in a single neural archi- tecture, exploring different combinations of atten- tion mechanisms including soft attention and hard monotonic attention. Chatterjee et al. (2017) built upon the two-encoder architecture of multi-source models (Libovick\u00b4y et al., 2016) by means of con- catenating both weighted contexts of encoded src and mt. Varis and Bojar (2017) compared two multi-source models, one using a single encoder with concatenation of src and mt sentences, and a second one using two character-level encoders for mt and src along with a character-level decoder. Recently, in the WMT 2018 APE shared task, several adaptations of the transformer architec- ture have been presented for multi-source APE. Pal et al. (2018) proposed an APE model that uses three self-attention-based encoders.",
            "Recently, in the WMT 2018 APE shared task, several adaptations of the transformer architec- ture have been presented for multi-source APE. Pal et al. (2018) proposed an APE model that uses three self-attention-based encoders. They introduce an additional joint encoder that at- tends over a combination of the two encoded se- quences from mt and src. Tebbifakhr et al. (2018), the NMT-subtask winner of WMT 2018 (wmt18nmt best ), employ sequence-level loss func- tions in order to avoid exposure bias during train- ing and to be consistent with the automatic eval- uation metrics. Shin and Lee (2018) propose that each encoder has its own self-attention and feed-forward layer to process each input sepa- rately. On the decoder side, they add two addi- tional multi-head attention layers, one for src \u2192 mt and another for src \u2192 pe. Thereafter another multi-head attention between the output of those attention layers helps the decoder to capture common words in mt which should re- main in pe.",
            "On the decoder side, they add two addi- tional multi-head attention layers, one for src \u2192 mt and another for src \u2192 pe. Thereafter another multi-head attention between the output of those attention layers helps the decoder to capture common words in mt which should re- main in pe. The APE PBSMT-subtask winner of WMT 2018 (wmt18smt best) (Junczys-Dowmunt and Grundkiewicz, 2018) also presented another transformer-based multi-source APE which uses two encoders and stacks an additional cross- attention component for src \u2192pe above the pre- vious cross-attention for mt \u2192pe. Comparing Shin and Lee (2018)\u2019s approach with the winner system, there are only two differences in the archi- tecture: (i) the cross-attention order of src \u2192mt and src \u2192pe in the decoder, and (ii) wmt18smt best additionally shares parameters between two en- coders.",
            "3 Transference Model for APE We propose a multi-source transformer model called transference ({src, mt}tr \u2192pe, Figure 1), which takes advantage of both the encodings of src and mt and attends over a combination of both sequences while generating the post-edited sentence. The second encoder, encsrc\u2192mt, makes use of the \ufb01rst encoder encsrc and a sub-encoder encmt for considering src and mt. Here, the encsrc encoder and the decpe decoder are equiva- lent to the original transformer for neural MT. Our",
            "encsrc\u2192mt follows an architecture similar to the transformer\u2019s decoder, the difference being that no masked multi-head self-attention is used to pro- cess mt. One self-attended encoder for src, s = (s1, s2, . . . , sk), returns a sequence of continuous representations, encsrc, and a second self-attended sub-encoder for mt, m = (m1, m2, . . . , ml), re- turns another sequence of continuous represen- tations, encmt. Self-attention at this point pro- vides the advantage of aggregating information from all of the words, including src and mt, and successively generates a new representation per word informed by the entire src and mt context. The internal encmt representation performs cross- attention over encsrc and prepares a \ufb01nal rep- resentation (encsrc\u2192mt) for the decoder (decpe). The decoder then generates the pe output in se- quence, p = (p1, p2, . . .",
            "The internal encmt representation performs cross- attention over encsrc and prepares a \ufb01nal rep- resentation (encsrc\u2192mt) for the decoder (decpe). The decoder then generates the pe output in se- quence, p = (p1, p2, . . . , pn), one word at a time from left to right by attending to previously gen- erated words as well as the \ufb01nal representations (encsrc\u2192mt) generated by the encoder. To summarize, our multi-source APE imple- mentation extends Vaswani et al. (2017) by intro- ducing an additional encoding block by which src and mt communicate with the decoder.",
            "To summarize, our multi-source APE imple- mentation extends Vaswani et al. (2017) by intro- ducing an additional encoding block by which src and mt communicate with the decoder. Our proposed approach differs from the WMT 2018 PBSMT winner system in several ways: (i) we use the original transformer\u2019s decoder with- out modi\ufb01cations; (ii) one of our encoder blocks (encsrc\u2192mt) is identical to the transformer\u2019s de- coder block but uses no masking in the self- attention layer, thus having one self-attention layer and an additional cross-attention for src \u2192mt; and (iii) in the decoder layer, the cross-attention is performed between the encoded representation from encsrc\u2192mt and pe. Our approach also differs from the WMT 2018 NMT winner system: (i) wmt18nmt best concatenates the encoded representation of two encoders and passes it as the key to the attention layer of the decoder, and (ii), the system additionally employs sequence-level loss functions based on maximum likelihood estimation and minimum risk training in order to avoid exposure bias during training.",
            "The main intuition is that our encsrc\u2192mt attends over the src and mt and informs the pe to better capture, process, and share information between src-mt-pe, which ef\ufb01ciently models error patterns and the corresponding corrections. Our model per- forms better than past approaches, as the experi- ment section will show. Figure 1: The transference model architecture for APE ({src, mt}tr \u2192pe). 4 Experiments We explore our approach on both APE sub-tasks of WMT 2018, where the 1st-stage MT system to which APE is applied is either a phrase-based sta- tistical machine translation (PBSMT) or a neural machine translation (NMT) model.",
            "4 Experiments We explore our approach on both APE sub-tasks of WMT 2018, where the 1st-stage MT system to which APE is applied is either a phrase-based sta- tistical machine translation (PBSMT) or a neural machine translation (NMT) model. For the PBSMT task, we compare against four baselines: the raw SMT output provided by the 1st-stage PBSMT system, the best-performing systems from WMT APE 2018 (wmt18smt best), which are a single model and an ensemble model by Junczys-Dowmunt and Grundkiewicz (2018), as well as a transformer trying to directly translate from src to pe (Transformer (src \u2192pe)), thus performing translation instead of APE. We evalu- ate the systems using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006).",
            "We evalu- ate the systems using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). For the NMT task, we consider two baselines: the raw NMT output provided by the 1st-stage NMT system and the best-performing system from the WMT 2018 NMT APE task (wmt18nmt best) (Tebbifakhr et al., 2018). Apart from the multi-encoder transference ar- chitecture described above ({src, mt}tr \u2192pe)",
            "and ensembling of this architecture, two simpler versions are also analyzed: \ufb01rst, a \u2018mono-lingual\u2019 (mt \u2192pe) APE model using only parallel mt\u2013pe data and therefore only a single encoder, and sec- ond, an identical single-encoder architecture, how- ever, using the concatenated src and mt text as input ({src + mt} \u2192pe) (Niehues et al., 2016). 4.1 Data For our experiments, we use the English\u2013German WMT 2016 (Bojar et al., 2016), 2017 (Bojar et al., 2017) and 2018 (Chatterjee et al., 2018) APE task data. All these released APE datasets con- sist of English\u2013German triplets containing source English text (src) from the IT domain, the cor- responding German translations (mt) from a 1st- stage MT system, and the corresponding human- post-edited version (pe).",
            "All these released APE datasets con- sist of English\u2013German triplets containing source English text (src) from the IT domain, the cor- responding German translations (mt) from a 1st- stage MT system, and the corresponding human- post-edited version (pe). The sizes of the datasets (train; dev; test), in terms of number of sentences, are (12,000; 1,000; 2,000), (11,000; 0; 2,000), and (13,442; 1,000; 1,023), for the 2016 PBSMT, the 2017 PBSMT, and the 2018 NMT data, respec- tively. One should note that for WMT 2018, we carried out experiments only for the NMT sub-task and ignored the data for the PBSMT task.",
            "One should note that for WMT 2018, we carried out experiments only for the NMT sub-task and ignored the data for the PBSMT task. Since the WMT APE datasets are small in size, we use \u2018arti\ufb01cial training data\u2019 (Junczys- Dowmunt and Grundkiewicz, 2016) containing 4.5M sentences as additional resources, 4M of which are weakly similar to the WMT 2016 train- ing data, while 500K are very similar according to TER statistics. For experimenting on the NMT data, we ad- ditionally use the synthetic eScape APE corpus (Negri et al., 2018), consisting of \u223c7M triples. For cleaning this noisy eScape dataset contain- ing many unrelated language words (e.g. Chinese), we perform the following two steps: (i) we use the cleaning process described in Tebbifakhr et al. (2018), and (ii) we use the Moses (Koehn et al., 2007) corpus cleaning scripts with minimum and maximum number of tokens set to 1 and 100, re- spectively.",
            "(2018), and (ii) we use the Moses (Koehn et al., 2007) corpus cleaning scripts with minimum and maximum number of tokens set to 1 and 100, re- spectively. After cleaning, we perform punctua- tion normalization, and then use the Moses tok- enizer (Koehn et al., 2007) to tokenize the eScape corpus with \u2018no-escape\u2019 option. Finally, we ap- ply true-casing. The cleaned version of the eScape corpus contains \u223c6.5M triplets. 4.2 Experiment Setup To build models for the PBSMT tasks from 2016 and 2017, we \ufb01rst train a generic APE model us- ing all the training data (4M + 500K + 12K + 11K) described in Section 4.1. Afterwards, we \ufb01ne-tune the trained model using the 500K arti\ufb01cial and 23K (12K + 11K) real PE training data.",
            "Afterwards, we \ufb01ne-tune the trained model using the 500K arti\ufb01cial and 23K (12K + 11K) real PE training data. We use the WMT 2016 development data (dev2016) con- taining 1,000 triplets to validate the models dur- ing training. To test our system performance, we use the WMT 2016 and 2017 test data (test2016, test2017) as two sub-experiments, each contain- ing 2,000 triplets (src, mt and pe). We compare the performance of our system with the four dif- ferent baseline systems described above: raw MT, wmt18smt best single and ensemble, as well as Trans- former (src \u2192pe).",
            "We compare the performance of our system with the four dif- ferent baseline systems described above: raw MT, wmt18smt best single and ensemble, as well as Trans- former (src \u2192pe). Additionally, we check the performance of our model on the WMT 2018 NMT APE task (where unlike in previous tasks, the 1st-stage MT sys- tem is provided by NMT): for this, we explore two experimental setups: (i) we use the PBSMT task\u2019s APE model as a generic model which is then \ufb01ne-tuned to a subset (12k) of the NMT data ({src, mt}nmt tr \u2192pegeneric,smt). One should note that it has been argued that the inclusion of SMT-speci\ufb01c data could be harmful when train- ing NMT APE models (Junczys-Dowmunt and Grundkiewicz, 2018).",
            "One should note that it has been argued that the inclusion of SMT-speci\ufb01c data could be harmful when train- ing NMT APE models (Junczys-Dowmunt and Grundkiewicz, 2018). (ii), we train a completely new generic model on the cleaned eScape data (\u223c6.5M) along with a subset (12K) of the orig- inal training data released for the NMT task ({src, mt}nmt tr \u2192pegeneric,nmt). The aforemen- tioned 12K NMT data are the \ufb01rst 12K of the overall 13.4K NMT data. The remaining 1.4K are used as validation data. The released devel- opment set (dev2018) is used as test data for our experiment, alongside the test2018, for which we could only obtain results for a few models by the WMT 2019 task organizers.",
            "The remaining 1.4K are used as validation data. The released devel- opment set (dev2018) is used as test data for our experiment, alongside the test2018, for which we could only obtain results for a few models by the WMT 2019 task organizers. We also explore an additional \ufb01ne-tuning step of {src, mt}nmt tr \u2192 pegeneric,nmt towards the 12K NMT data (called {src, mt}nmt tr \u2192peft), and a model averaging the 8 best checkpoints of {src, mt}nmt tr \u2192peft, which we call {src, mt}nmt tr \u2192peft avg. Last, we analyze the importance of our second encoder (encsrc\u2192mt), compared to the source en- coder (encsrc) and the decoder (decpe), by reduc- ing and expanding the amount of layers in the encoders and the decoder. Our standard setup, which we use for \ufb01ne-tuning, ensembling etc., is \ufb01xed to 6-6-6 for Nsrc-Nmt-Npe (cf.",
            "Our standard setup, which we use for \ufb01ne-tuning, ensembling etc., is \ufb01xed to 6-6-6 for Nsrc-Nmt-Npe (cf. Figure 1), where 6 is the value that was proposed by Vaswani et al. (2017) for the base model. We investigate",
            "what happens in terms of APE performance if we change this setting to 6-6-4 and 6-4-6. To handle out-of-vocabulary words and reduce the vocabulary size, instead of considering words, we consider subword units (Sennrich et al., 2016) by using byte-pair encoding (BPE). In the prepro- cessing step, instead of learning an explicit map- ping between BPEs in the src, mt and pe, we de- \ufb01ne BPE tokens by jointly processing all triplets. Thus, src, mt and pe derive a single BPE vocab- ulary. Since mt and pe belong to the same lan- guage (German) and src is a close language (En- glish), they naturally share a good fraction of BPE tokens, which reduces the vocabulary size to 28k. 4.3 Hyper-parameter Setup We follow a similar hyper-parameter setup for all reported systems. All encoders (for {src, mt}tr \u2192pe), and the decoder, are com- posed of a stack of Nsrc = Nmt = Npe = 6 identical layers followed by layer normalization.",
            "4.3 Hyper-parameter Setup We follow a similar hyper-parameter setup for all reported systems. All encoders (for {src, mt}tr \u2192pe), and the decoder, are com- posed of a stack of Nsrc = Nmt = Npe = 6 identical layers followed by layer normalization. The learning rate is varied throughout the training process, and increasing for the \ufb01rst training steps warmupsteps = 8000 and afterwards decreasing as described in (Vaswani et al., 2017). All remain- ing hyper-parameters are set analogously to those of the transformer\u2019s base model, except that we do not perform checkpoint averaging. At training time, the batch size is set to 25K tokens, with a maximum sentence length of 256 subwords. After each epoch, the training data is shuf\ufb02ed. During decoding, we perform beam search with a beam size of 4. We use shared embeddings between mt and pe in all our experiments.",
            "After each epoch, the training data is shuf\ufb02ed. During decoding, we perform beam search with a beam size of 4. We use shared embeddings between mt and pe in all our experiments. 5 Results The results of our four models, single- source (mt \u2192pe), multi-source single encoder ({src + pe} \u2192pe), transference ({src, mt}smt tr \u2192pe), and ensemble, in comparison to the four baselines, raw SMT, wmt18smt best (Junczys-Dowmunt and Grund- kiewicz, 2018) single and ensemble, as well as Transformer (src \u2192pe), are presented in Table 1 for test2016 and test2017. Table 2 reports the results obtained by our transference model ({src, mt}nmt tr \u2192pe) on the WMT 2018 NMT data for dev2018 (which we use as a test set) and test2018, compared to the baselines raw NMT and wmt18nmt best.",
            "5.1 Baselines The raw SMT output in Table 1 is a strong black- box PBSMT system (i.e., 1st-stage MT). We re- port its performance observed with respect to the ground truth (pe), i.e., the post-edited version of mt. The original PBSMT system scores over 62 BLEU points and below 25 TER on test2016 and test2017. Using a Transformer (src \u2192pe), we test if APE is really useful, or if potential gains are only achieved due to the good performance of the trans- former architecture. While we cannot do a full training of the transformer on the data that the raw MT engine was trained on due to the unavailability of the data, we use our PE datasets in an equivalent experimental setup as for all other models. The results of this system (Exp. 1.2 in Table 1) show that the performance is actually lower across both test sets, -5.52\/-9.43 absolute points in BLEU and +5.21\/+7.72 absolute in TER, compared to the raw SMT baseline.",
            "The results of this system (Exp. 1.2 in Table 1) show that the performance is actually lower across both test sets, -5.52\/-9.43 absolute points in BLEU and +5.21\/+7.72 absolute in TER, compared to the raw SMT baseline. We report four results from wmt18smt best, (i) wmt18smt best (single), which is the core multi- encoder implementation without ensembling but with checkpoint averaging, (ii) wmt18smt best (x4) which is an ensemble of four identical \u2018single\u2019 models trained with different random initializa- tions. The results of wmt18smt best (single) and wmt18smt best (x4) (Exp. 1.3 and 1.4) reported in Table 1 are from Junczys-Dowmunt and Grund- kiewicz (2018). Since their training procedure slightly differs from ours, we also trained the wmt18smt best system using exactly our experimen- tal setup in order to make a fair comparison.",
            "Since their training procedure slightly differs from ours, we also trained the wmt18smt best system using exactly our experimen- tal setup in order to make a fair comparison. This yields the baselines (iii) wmt18smt,generic best (single) (Exp. 1.5), which is similar to wmt18smt best (single), however, the training parameters and data are kept in line with our transference general model (Exp. 2.3) and (iv) wmt18smt,ft best (single) (Exp. 1.6), which is also trained maintaining the equivalent experimental setup compared to the \ufb01ne tuned version of the transference general model (Exp. 3.3). Compared to both raw SMT and Transformer (src \u2192pe) we see strong improve- ments for this state-of-the-art model, with BLEU scores of at least 68.14 and TER scores of at most 20.98 across the PBSMT testsets. wmt18smt best, however, performs better in its original setup (Exp. 1.3 and 1.4) compared to our experimental setup (Exp.",
            "wmt18smt best, however, performs better in its original setup (Exp. 1.3 and 1.4) compared to our experimental setup (Exp. 1.5 and 1.6).",
            "Exp. no. Models test2016 test2017 BLEU \u2191 TER \u2193 BLEU \u2191 TER \u2193 Baselines 1.1 Raw SMT 62.11 24.76 62.49 24.48 1.2 Transformer (src \u2192pe) 56.59 (-5.52) 29.97 (+5.21) 53.06 (-9.43) 32.20 (+7.72) 1.3 wmt18smt best (single) 70.86 (+8.75) 18.92 (-5.84) 69.72 (+7.23) 19.49 (-4.99) 1.4 wmt18smt best (x4) 71.04 (+8.93) 18.86 (-5.9) 70.46 (+7.97) 19.03 (-5.45) Baselines: Retrained wmt18smt best with our experimental setup 1.5 wmt18smt,generic best (single) 69.14 (+7.03) 20.41 (-4.35) 68.14 (+5.65) 20.",
            "45) Baselines: Retrained wmt18smt best with our experimental setup 1.5 wmt18smt,generic best (single) 69.14 (+7.03) 20.41 (-4.35) 68.14 (+5.65) 20.98 (-3.5) 1.6 wmt18smt,ft best (single) 70.12 (+8.01) 19.84 (-4.92) 69.16 (+6.67) 20.34 (-4.14) General models trained on 23K+4.5M data 2.1 mt \u2192pe 67.70 (+5.59) 21.90 (-2.86) 66.91 (+4.42) 22.32 (-2.16) 2.2 {src + mt} \u2192pe 69.32 (+7.21) 20.27 (-4.49) 68.26 (+5.77) 20.90 (-3.58) 2.3 {src, mt}smt tr \u2192pe 70.46 (+8.35) 19.21 (-5.",
            "32 (+7.21) 20.27 (-4.49) 68.26 (+5.77) 20.90 (-3.58) 2.3 {src, mt}smt tr \u2192pe 70.46 (+8.35) 19.21 (-5.55) 70.05 (+7.56) 19.46 (-5.02) Fine-tuning Exp. 2 models with 23K+500K data 3.1 mt \u2192pe 68.43 (+6.32) 21.29 (-3.47) 67.78 (+5.29) 21.63 (-2.85) 3.2 {src + mt} \u2192pe 69.87 (+7.76) 19.94 (-4.82) 68.57 (+6.08) 20.68 (-3.8) 3.3 {src, mt}smt tr \u2192pe 71.05 (+8.94) 19.05 (-5.71) 70.33 (+7.84) 19.23 (-5.25) 4.1 Exp3.3smt ens4ckpt 71.",
            "3 {src, mt}smt tr \u2192pe 71.05 (+8.94) 19.05 (-5.71) 70.33 (+7.84) 19.23 (-5.25) 4.1 Exp3.3smt ens4ckpt 71.59 (+9.48) 18.78 (-5.98) 70.89 (+8.4) 18.91 (-5.57) 4.2 ensemblesmt(x3) 72.19 (+10.08) 18.39 (-6.37) 71.58 (+9.09) 18.58 (-5.9) {src, mt}smt tr \u2192pe with different layer size 5.1 {src, mt}smt tr \u2192pe (6-6-4) 70.85 (+8.74) 19.00 (-5.76) 69.82 (+7.33) 19.67 (-4.81) 5.2 {src, mt}smt tr \u2192pe (6-4-6) 69.93 (+7.82) 19.70 (-5.06) 69.61 (+7.",
            "82 (+7.33) 19.67 (-4.81) 5.2 {src, mt}smt tr \u2192pe (6-4-6) 69.93 (+7.82) 19.70 (-5.06) 69.61 (+7.12) 19.68 (-4.8) Table 1: Evaluation results on the WMT APE test set 2016, and test set 2017 for the PBSMT task; (\u00b1X) value is the improvement over wmt18smt best (x4). The last section of the table shows the impact of increasing and decreasing the depth of the encoders and the decoder. 5.2 Single-Encoder Transformer for APE The two transformer architectures mt \u2192pe and {src + mt} \u2192pe use only a single encoder. Table 1 shows that mt \u2192pe (Exp. 2.1) pro- vides better performance (+4.42 absolute BLEU on test2017) compared to the original SMT, while {src + mt} \u2192pe (Exp. 2.2) provides further improvements by additionally using the src in- formation.",
            "2.1) pro- vides better performance (+4.42 absolute BLEU on test2017) compared to the original SMT, while {src + mt} \u2192pe (Exp. 2.2) provides further improvements by additionally using the src in- formation. {src + mt} \u2192pe improves over mt \u2192pe by +1.62\/+1.35 absolute BLEU points on test2016\/test2017. After \ufb01ne-tuning, both sin- gle encoder transformers (Exp. 3.1 and 3.2 in Table 1) show further improvements, +0.87 and +0.31 absolute BLEU points, respectively, for test2017 and a similar improvement for test2016. 5.3 Transference Transformer for APE In contrast to the two models above, our transfer- ence architecture uses multiple encoders. To fairly compare to wmt18smt best, we retrain the wmt18smt best system with our experimental setup (cf. Exp. 1.5 and 1.6 in Table 1).",
            "To fairly compare to wmt18smt best, we retrain the wmt18smt best system with our experimental setup (cf. Exp. 1.5 and 1.6 in Table 1). wmt18smt,generic best (single) is a generic model trained on all the training data; which is afterwards \ufb01ne-tuned with 500K arti- \ufb01cial and 23K real PE data (wmt18smt,ft best (sin- gle)). It is to be noted that in terms of perfor- mance the data processing method described in Junczys-Dowmunt and Grundkiewicz (2018) re- ported in Exp. 1.3 is better than ours (Exp. 1.6). The \ufb01ne-tuned version of the {src, mt}smt tr \u2192 pe model (Exp. 3.3 in Table 1) outperforms wmt18smt best (single) (Exp. 1.3) in BLEU on both test sets, however, the TER score for test2016 in- creases.",
            "3.3 in Table 1) outperforms wmt18smt best (single) (Exp. 1.3) in BLEU on both test sets, however, the TER score for test2016 in- creases. One should note that wmt18smt best (sin- gle) follows the transformer base model, which is an average of \ufb01ve checkpoints, while our Exp. 3.3 is not. When ensembling the 4 best checkpoints of our {src, mt}smt tr \u2192pe model (Exp. 4.1), the result beats the wmt18smt best (x4) system, which is an ensemble of four differ- ent randomly initialized wmt18smt best (single) sys- tems. Our ensemblesmt(x3) combines two {src, mt}smt tr \u2192 pe (Exp. 2.3) models ini- tialized with different random weights with the ensemble of the \ufb01ne-tuned transference model Exp3.3smt ens4ckpt(Exp. 4.1).",
            "2.3) models ini- tialized with different random weights with the ensemble of the \ufb01ne-tuned transference model Exp3.3smt ens4ckpt(Exp. 4.1). This ensemble provides the best results for all datasets, providing roughly +1 BLEU point and -0.5 TER when comparing against wmt18smt best (x4). The results on the WMT 2018 NMT datasets (dev2018 and test2018) are presented in Table 2. The raw NMT system serves as one baseline against which we compare the performance of the different models. We evaluate the system hypothe- ses with respect to the ground truth (pe), i.e., the post-edited version of mt. The baseline origi- nal NMT system scores 76.76 BLEU points and 15.08 TER on dev2018, and 74.73 BLEU points",
            "Exp. no. Models dev2018 test2018 BLEU \u2191 TER \u2193 BLEU \u2191 TER \u2193 6.1 Raw NMT 76.76 15.08 74.73 16.80 6.2 wmt18nmt best 77.74 (+0.98) 14.78 (-0.30) 75.53 (+0.80) 16.46 (-0.30) Fine-tuning Exp. 3.3 on 12k NMT data 7 {src, mt}nmt tr \u2192pegeneric,smt 77.09 (+0.33) 14.94 (-0.14) - - Transference model trained on eScape+ 12k NMT data 8 {src, mt}nmt tr \u2192pegeneric,nmt 77.25 (+0.49) 14.87 (-0.21) - - Fine-tuning model 8 on 12k NMT data 9 {src, mt}nmt tr \u2192peft 77.39 (+0.63) 14.71 (-0.37) - - Averaging 8 checkpoints of Exp.",
            "9 10 {src, mt}nmt tr \u2192peft avg 77.67 (+0.91) 14.52 (-0.56) 75.75 (+1.02) 16.15 (-0.69) Table 2: Evaluation results on the WMT APE 2018 development set for the NMT task (Exp. 10 results were obtained by the WMT 2019 task organizers). and 16.84 TER on test2018. For the WMT 2018 NMT data we \ufb01rst test our {src, mt}nmt tr \u2192pegeneric,smt model, which is the model from Exp. 3.3 \ufb01ne-tuned towards NMT data as described in Section 4.2. Table 2 shows that our PBSMT APE model \ufb01ne-tuned towards NMT (Exp. 7) can even slightly improve over the already very strong NMT system by about +0.3 BLEU and -0.1 TER, although these improve- ments are not statistically signi\ufb01cant.",
            "7) can even slightly improve over the already very strong NMT system by about +0.3 BLEU and -0.1 TER, although these improve- ments are not statistically signi\ufb01cant. The overall results improve when we train our model on eScape and NMT data instead of using the PBSMT model as a basis. Our proposed generic transference model (Exp. 8, {src, mt}nmt tr \u2192 pegeneric,nmt shows statisti- cally signi\ufb01cant improvements in terms of BLEU and TER compared to the baseline even before \ufb01ne-tuning, and further improvements after \ufb01ne- tuning (Exp. 9, {src, mt}nmt tr \u2192 peft). Fi- nally, after averaging the 8 best checkpoints, our {src, mt}nmt tr \u2192peft avg model (Exp. 10) also shows consistent improvements in comparison to the baseline and other experimental setups. Over- all our \ufb01ne-tuned model averaging the 8 best checkpoints achieves +1.02 absolute BLEU points and -0.69 absolute TER improvements over the baseline on test2018.",
            "10) also shows consistent improvements in comparison to the baseline and other experimental setups. Over- all our \ufb01ne-tuned model averaging the 8 best checkpoints achieves +1.02 absolute BLEU points and -0.69 absolute TER improvements over the baseline on test2018. Table 2 also shows the performance of our model compared to the win- ner system of WMT 2018 (wmt18nmt best ) for the NMT task (Tebbifakhr et al., 2018). wmt18nmt best scores 14.78 in TER and 77.74 in BLEU on the dev2018 and 16.46 in TER and 75.53 in BLEU on the test2018. In comparison to wmt18nmt best , our model (Exp. 10) achieves better scores in TER on both the dev2018 and test2018, however, in terms of BLEU our model scores slightly lower for dev2018, while some improvements are achieved on test2018. The number of layers (Nsrc-Nmt-Npe) in all en- coders and the decoder for these results is \ufb01xed to 6-6-6.",
            "The number of layers (Nsrc-Nmt-Npe) in all en- coders and the decoder for these results is \ufb01xed to 6-6-6. In Exp. 5.1, and 5.2 in Table 1, we see the results of changing this setting to 6-6-4 and 6-4-6. This can be compared to the results of Exp. 2.3, since no \ufb01ne-tuning or ensembling was performed for these three experiments. Exp. 5.1 shows that decreasing the number of layers on the decoder side does not hurt the performance. In fact, in the case of test2016, we got some improve- ment, while for test2017, the scores got slightly worse. In contrast, reducing the encsrc\u2192mt en- coder block\u2019s depth (Exp. 5.2) does indeed reduce the performance for all four scores, showing the importance of this second encoder.",
            "In contrast, reducing the encsrc\u2192mt en- coder block\u2019s depth (Exp. 5.2) does indeed reduce the performance for all four scores, showing the importance of this second encoder. 5.4 Analysis of Error Patterns In Table 3, we analyze and compare the best performing SMT (ensemblesmt(x3)) and NMT ({src, mt}nmt tr \u2192peft avg) model outputs with the original MT outputs on the WMT 2017 (SMT) APE test set and on the WMT 2018 (NMT) de- velopment set. Improvements are measured in terms of number of words which need to be (i) in- serted (In), (ii) deleted (De), (iii) substituted (Su), and (iv) shifted (Sh), as per TER (Snover et al., 2006), in order to turn the MT outputs into ref- erence translations.",
            "Our model provides promis- ing results by signi\ufb01cantly reducing the required number of edits (24% overall for PBSMT task and 3.6% for NMT task) across all edit operations, thereby leading to reduced post-editing effort and hence improving human post-editing productivity. When comparing PBSMT to NMT, we see that stronger improvements are achieved for PBSMT, probably because the raw SMT is worse than the raw NMT. For PBSMT, similar results are achieved for In, De, and Sh, while less gains are",
            "%In %De %Su %Sh ensemblesmt(x3) vs. raw SMT +31 +29 +15 +32 {src, mt}nmt tr \u2192peft avg vs. raw NMT +6 +2 +4 -2 Table 3: % of error reduction in terms of different edit operations achieved by our best systems compared to the raw MT baselines. obtained in terms of Su. For NMT, In is improved most, followed by Su, De, and last Sh. For shifts in NMT, the APE system even creates further errors, instead of reducing them, which is an issue we aim to prevent in the future. 5.5 Discussion The proposed transference architecture ({src, mt}smt tr \u2192pe, Exp. 2.3) shows slightly worse results than wmt18smt best (single) (Exp. 1.3) before \ufb01ne-tuning, and roughly similar results after \ufb01ne-tuning (Exp. 3.3). After ensembling, however, our transference model (Exp.",
            "1.3) before \ufb01ne-tuning, and roughly similar results after \ufb01ne-tuning (Exp. 3.3). After ensembling, however, our transference model (Exp. 4.2) shows consistent improvements when comparing against the best baseline ensemble wmt18smt best (x4) (Exp. 1.4). Due to the unavailability of the sentence- level scores of wmt18smt best (x4), we could not test if the improvements (roughly +1 BLEU, -0.5 TER) are statistically signi\ufb01cant. Interestingly, our approach of taking the model optimized for PBSMT and \ufb01ne-tuning it to the NMT task (Exp. 7) does not hurt the performance as was reported in the previous literature (Junczys-Dowmunt and Grundkiewicz, 2018). In contrast, some small, albeit statistically insigni\ufb01cant improvements over the raw NMT baseline were achieved. When we train the transference architecture directly for the NMT task (Exp. 8), we get slightly better and statistically signi\ufb01cant improvements compared to raw NMT.",
            "In contrast, some small, albeit statistically insigni\ufb01cant improvements over the raw NMT baseline were achieved. When we train the transference architecture directly for the NMT task (Exp. 8), we get slightly better and statistically signi\ufb01cant improvements compared to raw NMT. Fine-tuning this NMT model further towards the actual NMT data (Exp. 9), as well as performing checkpoint averaging using the 8 best checkpoints improves the results even further. The reasons for the effectiveness of our ap- proach can be summarized as follows. (1) Our encsrc\u2192mt contains two attention mechanisms: one is self-attention and another is cross-attention. The self-attention layer is not masked here; therefore, the cross-attention layer in encsrc\u2192mt is informed by both previous and future time- steps from the self-attended representation of mt (encmt) and additionally from encsrc. As a re- sult, each state representation of encsrc\u2192mt is learned from the context of src and mt. This might produce better representations for decpe which can access the combined context.",
            "As a re- sult, each state representation of encsrc\u2192mt is learned from the context of src and mt. This might produce better representations for decpe which can access the combined context. In con- trast, in wmt18smt best, the decpe accesses represen- tations from src and mt independently, \ufb01rst using the representation from mt and then using that of src. (2) The position-wise feed-forward layer in our encsrc\u2192mt of the transference model requires processing information from two attention mod- ules, while in the case of wmt18smt best, the position- wise feed-forward layer in decpe needs to process information from three attention modules, which may increase the learning dif\ufb01culty of the feed- forward layer. (3) Since pe is a post-edited ver- sion of mt, sharing the same language, mt and pe are quite similar compared to src. Therefore, at- tending over a \ufb01ne-tuned representation from mt along with src, which is what we have done in this work, might be a reason for the better results than those achieved by attending over src directly.",
            "Therefore, at- tending over a \ufb01ne-tuned representation from mt along with src, which is what we have done in this work, might be a reason for the better results than those achieved by attending over src directly. Evaluating the in\ufb02uence of the depth of our en- coders and decoder show that while the decoder depth appears to have limited importance, reduc- ing the encoder depth indeed hurts performance which is in line with Domhan (2018). 6 Conclusions In this paper, we presented a multi-encoder transformer-based APE model that repurposes the standard transformer blocks in a simple and effec- tive way for the APE task: \ufb01rst, our transference architecture uses a transformer encoder block for src, followed by a decoder block without mask- ing on mt that effectively acts as a second encoder combining src \u2192mt, and feeds this representa- tion into a \ufb01nal decoder block generating pe. The proposed model outperforms the best-performing system of WMT 2018 on the test2016, test2017, dev2018, and test2018 data and provides a new state-of-the-art in APE.",
            "The proposed model outperforms the best-performing system of WMT 2018 on the test2016, test2017, dev2018, and test2018 data and provides a new state-of-the-art in APE. Taking a departure from traditional transformer- based encoders, which perform self-attention only, our second encoder also performs cross-attention to produce representations for the decoder based on both src and mt. We also show that the en- coder plays a more pivotal role than the decoder in transformer-based APE, which could also be the case for transformer-based generation tasks in general. Our architecture is generic and can be used for any multi-source task, e.g., multi-source translation or summarization, etc.",
            "References Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In International Conference on Learning Representations (ICLR), San Diego, CA, USA. Ond\u02c7rej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, and Marco Turchi. 2017. Findings of the 2017 Conference on Machine Translation (WMT17). In Proceed- ings of the Second Conference on Machine Trans- lation, Volume 2: Shared Task Papers, pages 169\u2013 214, Copenhagen, Denmark. Association for Com- putational Linguistics.",
            "Findings of the 2017 Conference on Machine Translation (WMT17). In Proceed- ings of the Second Conference on Machine Trans- lation, Volume 2: Shared Task Papers, pages 169\u2013 214, Copenhagen, Denmark. Association for Com- putational Linguistics. Ond\u02c7rej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aure- lie Neveol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Spe- cia, Marco Turchi, Karin Verspoor, and Marcos Zampieri. 2016. Findings of the 2016 Conference on Machine Translation. In Proceedings of the First Conference on Machine Translation, pages 131\u2013 198, Berlin, Germany. Association for Computa- tional Linguistics.",
            "2016. Findings of the 2016 Conference on Machine Translation. In Proceedings of the First Conference on Machine Translation, pages 131\u2013 198, Berlin, Germany. Association for Computa- tional Linguistics. Ond\u02c7rej Bojar, Rajen Chatterjee, Christian Federmann, Barry Haddow, Matthias Huck, Chris Hokamp, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Carolina Scarton, Lucia Specia, and Marco Turchi. 2015. Findings of the 2015 Workshop on Statistical Machine Translation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 1\u201346, Lisbon, Portugal. Association for Computational Linguistics. Rajen Chatterjee, M. Amin Farajian, Matteo Negri, Marco Turchi, Ankit Srivastava, and Santanu Pal. 2017. Multi-Source Neural Automatic Post-Editing: FBK\u2019s participation in the WMT 2017 APE shared task.",
            "2017. Multi-Source Neural Automatic Post-Editing: FBK\u2019s participation in the WMT 2017 APE shared task. In Proceedings of the Second Conference on Machine Translation, Volume 2: Shared Task Pa- pers, pages 630\u2013638, Copenhagen, Denmark. Asso- ciation for Computational Linguistics. Rajen Chatterjee, Matteo Negri, Raphael Rubino, and Marco Turchi. 2018. Findings of the WMT 2018 Shared Task on Automatic Post-Editing. In Pro- ceedings of the Third Conference on Machine Trans- lation, Volume 2: Shared Task Papers, Brussels, Bel- gium. Association for Computational Linguistics. Tobias Domhan. 2018. How much attention do you need? a granular analysis of neural machine trans- lation architectures. In Proceedings of the 56th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1799\u2013 1808, Melbourne, Australia. Association for Com- putational Linguistics.",
            "In Proceedings of the 56th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1799\u2013 1808, Melbourne, Australia. Association for Com- putational Linguistics. Marcin Junczys-Dowmunt and Roman Grundkiewicz. 2016. Log-linear Combinations of Monolingual and Bilingual Neural Machine Translation Models for Automatic Post-Editing. In Proceedings of the First Conference on Machine Translation, pages 751\u2013 758, Berlin, Germany. Marcin Junczys-Dowmunt and Roman Grundkiewicz. 2017. The AMU-UEdin Submission to the WMT 2017 Shared Task on Automatic Post-Editing. In Proceedings of the Second Conference on Machine Translation, Volume 2: Shared Task Papers, pages 639\u2013646, Copenhagen, Denmark. Association for Computational Linguistics. Marcin Junczys-Dowmunt and Roman Grundkiewicz. 2018. MS-UEdin Submission to the WMT2018 APE Shared Task: Dual-Source Transformer for Automatic Post-Editing.",
            "Association for Computational Linguistics. Marcin Junczys-Dowmunt and Roman Grundkiewicz. 2018. MS-UEdin Submission to the WMT2018 APE Shared Task: Dual-Source Transformer for Automatic Post-Editing. In Proceedings of the Third Conference on Machine Translation, Volume 2: Shared Task Papers, pages 835\u2013839, Belgium, Brussels. Association for Computational Linguis- tics. Kevin Knight and Ishwar Chander. 1994. Automated Postediting of Documents. In Proceedings of the Twelfth National Conference on Arti\ufb01cial Intelli- gence (Vol. 1), AAAI \u201994, pages 779\u2013784, Seattle, Washington, USA. Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond\u02c7rej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation.",
            "2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Ses- sions, pages 177\u2013180, Prague, Czech Republic. Jind\u02c7rich Libovick\u00b4y, Jind\u02c7rich Helcl, Marek Tlust\u00b4y, Ond\u02c7rej Bojar, and Pavel Pecina. 2016. CUNI Sys- tem for WMT16 Automatic Post-Editing and Multi- modal Translation Tasks. In Proceedings of the First Conference on Machine Translation, pages 646\u2013 654, Berlin, Germany. Association for Computa- tional Linguistics. Matteo Negri, Marco Turchi, Rajen Chatterjee, and Nicola Bertoldi. 2018. ESCAPE: a Large-scale Synthetic Corpus for Automatic Post-Editing. In Proceedings of the Eleventh International Confer- ence on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Re- sources Association (ELRA).",
            "2018. ESCAPE: a Large-scale Synthetic Corpus for Automatic Post-Editing. In Proceedings of the Eleventh International Confer- ence on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Re- sources Association (ELRA). Jan Niehues, Eunah Cho, Thanh-Le Ha, and Alex Waibel. 2016. Pre-Translation for Neural Machine Translation. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1828\u20131836, Osaka, Japan. The COLING 2016 Organizing Com- mittee.",
            "Santanu Pal, Nico Herbig, Antonio Krger, and Josef van Genabith. 2018. A Transformer-Based Multi- Source Automatic Post-Editing System. In Proceed- ings of the Third Conference on Machine Transla- tion, Volume 2: Shared Task Papers, pages 840\u2013848, Belgium, Brussels. Association for Computational Linguistics. Santanu Pal, Sudip Kumar Naskar, and Josef van Gen- abith. 2016a. Multi-Engine and Multi-Alignment Based Automatic Post-Editing and Its Impact on Translation Productivity. In Proceedings of COL- ING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2559\u20132570, Osaka, Japan. Santanu Pal, Sudip Kumar Naskar, Mihaela Vela, and Josef van Genabith. 2016b. A Neural Network Based Approach to Automatic Post-Editing. In Pro- ceedings of the 54th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 2: Short Papers), pages 281\u2013286, Berlin, Germany.",
            "2016b. A Neural Network Based Approach to Automatic Post-Editing. In Pro- ceedings of the 54th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 2: Short Papers), pages 281\u2013286, Berlin, Germany. Associa- tion for Computational Linguistics. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Com- putational Linguistics, ACL \u201902, pages 311\u2013318, Philadelphia, Pennsylvania. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th An- nual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. Jaehun Shin and Jong-Hyeok Lee. 2018. Multi- encoder Transformer Network for Automatic Post- Editing.",
            "Jaehun Shin and Jong-Hyeok Lee. 2018. Multi- encoder Transformer Network for Automatic Post- Editing. In Proceedings of the Third Conference on Machine Translation, Volume 2: Shared Task Pa- pers, pages 853\u2013858, Belgium, Brussels. Associa- tion for Computational Linguistics. Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin- nea Micciulla, and John Makhoul. 2006. A study of Translation Edit Rate with Targeted Human Anno- tation. In Proceedings of Association for Machine Translation in the Americas, pages 223\u2013231, Cam- bridge, Massachusetts, USA. TAUS\/CNGL Report. 2010. Machine Translation Post- Editing Guidelines Published. Technical report, TAUS. Amirhossein Tebbifakhr, Ruchit Agrawal, Rajen Chat- terjee, Matteo Negri, and Marco Turchi. 2018. Multi-Source Transformer with Combined Losses for Automatic Post Editing. In Proceedings of the Third Conference on Machine Translation, Volume 2: Shared Task Papers, pages 859\u2013865, Belgium, Brussels.",
            "2018. Multi-Source Transformer with Combined Losses for Automatic Post Editing. In Proceedings of the Third Conference on Machine Translation, Volume 2: Shared Task Papers, pages 859\u2013865, Belgium, Brussels. Association for Computational Linguis- tics. Dusan Varis and Ond\u02c7rej Bojar. 2017. CUNI System for WMT17 Automatic Post-Editing Task. In Proceed- ings of the Second Conference on Machine Trans- lation, Volume 2: Shared Task Papers, pages 661\u2013 666, Copenhagen, Denmark. Association for Com- putational Linguistics. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need.",
            "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar- nett, editors, Advances in Neural Information Pro- cessing Systems 30, pages 5998\u20136008. Curran As- sociates, Inc."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1908.06151.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 11550.000503540039,
    "avg_doclen_est": 167.3913116455078
}
