{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Compositional Neural Machine Translation by Removing the Lexicon from Syntax COGSCI DRAFT SUMBISSION Tristan Thrush (tristant@mit.edu) MIT Department of Brain and Cognitive Sciences, 43 Vassar Street Cambridge, MA 02139 USA Abstract The meaning of a natural language utterance is largely deter- mined from its syntax and words. Additionally, there is evi- dence that humans process an utterance by separating knowl- edge about the lexicon from syntax knowledge. Theories from semantics and neuroscience claim that complete word mean- ings are not encoded in the representation of syntax. In this paper, we propose neural units that can enforce this constraint over an LSTM encoder and decoder. We demonstrate that our model achieves competitive performance across a variety of domains including semantic parsing, syntactic parsing, and English to Mandarin Chinese translation. In these cases, our model outperforms the standard LSTM encoder and decoder architecture on many or all of our metrics. To demonstrate that our model achieves the desired separation between the lexicon and syntax, we analyze its weights and explore its behavior when different neural modules are damaged.",
      "In these cases, our model outperforms the standard LSTM encoder and decoder architecture on many or all of our metrics. To demonstrate that our model achieves the desired separation between the lexicon and syntax, we analyze its weights and explore its behavior when different neural modules are damaged. When damaged, we \ufb01nd that the model displays the knowledge distortions that aphasics are evidenced to have. 1 Keywords: natural language processing; adversarial neural networks; machine translation; aphasia; neural attention Introduction Studies of Broca\u2019s and Wernicke\u2019s aphasia provide evidence that our brains understand an utterance by creating sepa- rate representations for word meaning and word arrangement (Caramazza & Zurif, 1976). There is a related thesis about human language, present across many theories of semantics, which is that syntactic categories are partially agnostic to the identity of words (Altshuler, Parsons, & Schwarzschild, 2018). This regularity in how humans derive meaning from an utterance is applicable to the task of natural language translation. This is because, by de\ufb01nition, translation ne- cessitates the creation of a meaning representation for an in- put.",
      "This regularity in how humans derive meaning from an utterance is applicable to the task of natural language translation. This is because, by de\ufb01nition, translation ne- cessitates the creation of a meaning representation for an in- put. According to the cognitive and neural imperative, we introduce new units to regularize an arti\ufb01cial neural encoder and decoder (Kalchbrenner & Blunsom, 2013). These are called the Lexicon and Lexicon-Adversary units (collectively, LLA). Tests are done on a diagnostic task, and naturalistic tasks including semantic parsing, syntactic parsing, and En- glish to Mandarin Chinese translation. We evaluate a Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) encoder and decoder, with and without the LLA units, and show that the LLA version achieves superior translation 1The code, trained models, tokenized data, and supplemental in- formation about the training, can be found at: NOT INCLUDED IN DRAFT SUBMISSION performance. In addition, we examine our model\u2019s weights, and its performance when some of its neurons are damaged.",
      "In addition, we examine our model\u2019s weights, and its performance when some of its neurons are damaged. We \ufb01nd that the model exhibits the knowledge and the lack thereof expected of a Broca\u2019s aphasic (Caramazza & Zurif, 1976) when one module\u2019s weights are corrupted. It also ex- hibits that expected of a Wernicke\u2019s aphasic (Caramazza & Zurif, 1976) when another module\u2019s weights are corrupted. Neural Motivation Caramazza and Zurif (1976) showed that Broca\u2019s aphasics were able to understand \u201cthe apple that the boy is eating is red\u201d with signi\ufb01cantly higher accuracy than \u201cthe cow that the monkey is scaring is yellow,\u201d along with similar pairs. The critical difference between these sentences is that, due to se- mantic constraints from the words, the \ufb01rst can be understood if it is presented as a set of words. The second cannot. This experiment provides evidence that the rest of the language neurons in the brain (largely Wernicke\u2019s area) can yield an understanding of word meanings but not how words are ar- ranged.",
      "The second cannot. This experiment provides evidence that the rest of the language neurons in the brain (largely Wernicke\u2019s area) can yield an understanding of word meanings but not how words are ar- ranged. This also suggests that Broca\u2019s area builds a repre- sentation of the syntax. In the same study, Wernicke\u2019s aphasics performed poorly regardless of the sentence type. This provides evidence that Broca\u2019s area cannot yield an understanding of word mean- ings. Taken together, the two experiments support the theory that Broca\u2019s area creates a representation of the syntax without en- coding complete word meanings. These other lexical aspects are represented separately in Wernicke\u2019s area, which does not encode syntax. Cognitive Motivation A tenet of generative grammar theories is that different words can share the same syntactic category (Altshuler et al., 2018). It is possible, for example, to know that the syntax for an utterance is a noun phrase that is composed of a determiner and a noun, followed by a verb phrase that is composed of a verb. One can know this without knowing the words.",
      "It is possible, for example, to know that the syntax for an utterance is a noun phrase that is composed of a determiner and a noun, followed by a verb phrase that is composed of a verb. One can know this without knowing the words. This also means that there are aspects of a word\u2019s meaning that the syntax does not determine; by de\ufb01nition, these aspects are invariant to word arrangement. arXiv:2002.08899v1  [cs.CL]  6 Feb 2020",
      "Model In a natural language translation setting, suppose that an in- put word corresponds to a set of output tokens independently of its context. Even though this information might be use- ful to determine the syntax of the input utterance in the \ufb01rst place, the syntax does not determine this knowledge at all (by supposition). So, we can impose the constraint that our model\u2019s representation of the input\u2019s syntax cannot con- tain this context-invariant information. This regularization is strictly preferable to allowing all aspects of word meaning to propagate into the input\u2019s syntax representation.2 Without such a constraint, all inputs could, in principle, be given their own syntactic categories. This scenario is refuted by cogni- tive and neural theories. We incorporate the regularization with neural units that can separate representations of word meaning and arrangement. With the exception of the equations that we list be- low, the encoding and decoding follows standard paradigms (Kalchbrenner & Blunsom, 2013). The input at a time step to the LSTM encoder is a vector embedding for the input token.",
      "With the exception of the equations that we list be- low, the encoding and decoding follows standard paradigms (Kalchbrenner & Blunsom, 2013). The input at a time step to the LSTM encoder is a vector embedding for the input token. The \ufb01nal hidden and cell states of the encoder are the start- ing hidden and cell states of the LSTM decoder. The decoder does not take tokens as inputs; it decodes by relying solely on its hidden and cell states. The tth output, ot, from the decoder is Softmax(W(ht)), where W is a fully connected layer and ht is the decoder\u2019s tth hidden state. ot is the length of the out- put dictionary. ot\u2019s index with the highest value corresponds to the token choice. The encoder and decoder\u2019s weights are optimized with the negative log likelihood loss. The inputs to the loss function are the log of the model\u2019s output and the ground-truth at each time step. Below, we describe our mod- i\ufb01cations.",
      "The encoder and decoder\u2019s weights are optimized with the negative log likelihood loss. The inputs to the loss function are the log of the model\u2019s output and the ground-truth at each time step. Below, we describe our mod- i\ufb01cations. l = \u03c3(\u2228(w1,w2,\u00b7\u00b7\u00b7 ,wm)) la = \u03c3(Wa2(ReLU(Wa1(GradReverse(h\u2322 e ce))))) o\u2032 t = l \u2299ot Where: m is the number of input tokens. wi is a vector embedding for the ith input token, and is the length of the output dictionary. It is not the same embedding used by the encoder LSTM. \u03c3 is the Sigmoid function. \u2228is the max pooling of a sequence of vectors of the same length. The weight at the output vector\u2019s ith index is the max of all input vectors\u2019 weights at their ith indices. he and ce are the \ufb01nal hidden and cell states of the encoder. Wa1 and Wa2 are fully connected layers. \u2322is concatenation. \u2299is the elementwise product.",
      "he and ce are the \ufb01nal hidden and cell states of the encoder. Wa1 and Wa2 are fully connected layers. \u2322is concatenation. \u2299is the elementwise product. 2While not disadvantaging our model over the standard one, our implementation choice of looking for lexical invariants in the iden- tity of the output tokens may not always provide signi\ufb01cant improve- ments. Consider the task of translating English into a binary code. GradReverse multiplies the gradient by a negative number upon backpropagation. l is the output of the Lexicon Unit. Due to the max pool- ing, only one input token can be responsible for the value at a particular index of the output vector. The weights, wi, are optimized solely by computing the binary cross entropy (BCE) loss between l and the indicator vector where the kth element is 1 if the kth token in the output dictionary is in the output and 0 otherwise.",
      "The weights, wi, are optimized solely by computing the binary cross entropy (BCE) loss between l and the indicator vector where the kth element is 1 if the kth token in the output dictionary is in the output and 0 otherwise. This procedure forces a wi to repre- sent the output tokens that are associated with its respective input token, without relying on aggregated contributions from the presence of several input tokens, and independently of the input word order. la is the output of the Lexicon-Adversary Unit. Its weights are optimized according to the BCE loss with l as the target. This means that la is the Lexicon-Adversary Unit\u2019s approxi- mation of l. Because he and ce are passed through a gradient reversal layer, the LSTM encoder is regularized to produce a representation that does not include information from l. Con- sequently, the LSTM decoder does not have this information either. o\u2032 t is the tth output of our model. It can be converted to a token by \ufb01nding the index with the highest weight. It is the result of combining l via an elementwise product with the information from the regularized decoder.",
      "o\u2032 t is the tth output of our model. It can be converted to a token by \ufb01nding the index with the highest weight. It is the result of combining l via an elementwise product with the information from the regularized decoder. The recurrent encoder and decoder are the only modules that can represent the syntax, but they lack the expressivity to encode all potential aspects of word meaning. So, they are not always capable of producing a theoretically denied represen- tation by giving all words their own syntactic category. The Lexicon Unit can represent these missing lexical aspects, but it lacks the expressivity to represent the syntax. See Figure 1 for the model. w1, w2, \u00b7\u00b7\u00b7 , wm \u2192 L \u2192 l em \u2192 \u2191E \u2192he,ce \u2192 \u2193D \u2192 o1 \u2299 \u2192 o\u2032 1 ... ... \u2193 \u2193D \u2192 o2 \u2299 \u2192 o\u2032 2 e2 \u2192 \u2191E LA ... ... ... ... ... ... e1 \u2192 \u2191E \u2193 \u2193D \u2192 on \u2299 \u2192 o\u2032 n la Figure 1: A graphic of our model.",
      "In addition to the terms described under our equations, we depict e terms which are embeddings for input tokens, for use by the LSTM encoder. The LSTM encoder is E, the LSTM decoder is D, the Lexical Unit is L and the Lexicon-Adversary Unit is LA. The dotted area contains the representation for the input\u2019s syntax, adver- sarially regularized to not include context-invariant lexical in- formation, l. The dashed area contains the representation for this lexical information, which does not have syntax knowl- edge. At every output step, l is combined with the decoder\u2019s output via an elementwise product.",
      "Experiments We used Lake, Linzen, and Baroni\u2019s (2019) small diagnos- tic, the Geoquery semantic parsing dataset (Zelle, 1995), the Wall Street Journal syntactic parsing dataset of sentences up to length 10 (Marcus, Marcinkiewicz, & Santorini, 1993), and the Tatoeba (Ho, 2020) English to Chinese translation dataset processed by Kelly (2020). To avoid the biases that can be introduced with hyperpa- rameter tuning, we used the same hyperparameters with every model on every domain. These were chosen arbitrarily and kept after they enabled all models to reach a similar train ac- curacy (typically, close to 100 percent) and after they enabled all models to achieve a peak validation performance and then gradually yield worse validation scores. The hyperparameters are as follows. LSTM hidden size = 300, Lexicon Unit batch size = 1, batch size for other modules = 30, epoch to stop training the Lexicon Unit and start training other modules = 30, epoch to stop training = 1000, and Lexicon-Adversary Unit hidden size = 1000.",
      "The optimizer used for the Lexicon Unit was a sparse implementation of Adam (Kingma & Ba, 2015) with a learning rate of 0.1 and otherwise the default Py- Torch settings (Torch.Optim, 2020). In the other cases it was Adam (Kingma & Ba, 2015) with the default PyTorch set- tings (Torch.Optim, 2020). The gradient through the encoder from the adversary\u2019s gradient reversal layer is multiplied by -0.0001. Additionally, the validation score is calculated after each train epoch and the model with the best is tested. To compute the Lexicon Unit to use, we measure its loss (BCE) on the validation set. Unless otherwise stated, we use the mean number of exact matches as the validation metric for the full model. To judge overall translation performance, we compared the LLA-LSTM encoder and decoder with the standard LSTM encoder and decoder. We also compared our model with one that does not have the adversary but is otherwise identical. The LLA-LSTM model shows improvements over the stan- dard model on many or all of the metrics for every naturalis- tic domain.",
      "We also compared our model with one that does not have the adversary but is otherwise identical. The LLA-LSTM model shows improvements over the stan- dard model on many or all of the metrics for every naturalis- tic domain. Many of the improvements over the other mod- els are several percentage points. In the few scenarios where the LLA-LSTM model does not improve upon the standard model, the discrepancy between the models is small. The dis- crepancy is also small when the LLA-LSTM model with no adversary performs better than the LLA-LSTM model. Table 1 displays the test results across the domains. Additionally, we provide evidence that the model learns knowledge of a separation between syntax and the lexicon that is similar to that of a human. Figure 2 displays the learned \u03c3(w) embeddings for some input words, across the domains. To avoid cherry-picking the results, we chose the input words arbitrarily, subject to the following constraint. We considered each word to typically have a different syn- tactic category than the other choices from that domain. This constraint was used to present a diverse selection of words.",
      "To avoid cherry-picking the results, we chose the input words arbitrarily, subject to the following constraint. We considered each word to typically have a different syn- tactic category than the other choices from that domain. This constraint was used to present a diverse selection of words. Table 2 displays the output behavior of models that we dam- aged to resemble the damage that causes aphasia. To avoid cherry-picking the results, we arbitrarily chose an input for each domain, subject to the following constraint. The input is not in the train set and the undamaged LLA-LSTM model produces a translation that we judge to be correct. For all in- puts that we chose, damage to the analog of Broca\u2019s area (the LSTMs) results in an output that describes content only if it is described by the input. However, the output does not show understanding of the input\u2019s syntax. In the naturalistic do- mains, damage to the analog of Wernicke\u2019s area (the Lexicon Unit) results in an output with incorrect content that would be acceptable if the input had different words but the same syntax.",
      "However, the output does not show understanding of the input\u2019s syntax. In the naturalistic do- mains, damage to the analog of Wernicke\u2019s area (the Lexicon Unit) results in an output with incorrect content that would be acceptable if the input had different words but the same syntax. These knowledge distortions are precisely those that are expected in the respective human aphasics (Caramazza & Zurif, 1976). We also provide corpus-level results from the damaged models by presenting mean precision on the test sets. Because the output languages in all of our domains use tokens to represent meanings in many cases, it is expected that the analog to Wernicke\u2019s area is responsible for main- taining a high precision. Sequences of Color The \ufb01rst experiment by Lake et al. (2019) included a dataset of 14 training pairs and 10 test pairs. In the dataset, an input is a sequence of words from an arti\ufb01cial language created by the authors. An output is a sequence of colored dots. Because the dataset is so small, we use the train set as the validation set.",
      "In the dataset, an input is a sequence of words from an arti\ufb01cial language created by the authors. An output is a sequence of colored dots. Because the dataset is so small, we use the train set as the validation set. The input and output dictionary are 7 and 4 words, re- spectively (not including the stop, \u201c< s >,\u201d token). In their paper, the authors argue that it is clear that the words have meanings. Four of the words correspond to unique output to- kens, and three of them correspond to functions of the output tokens (for example, repeating the same dot three times). The dataset showcases the contrast between human and standard neural network responses. Their paper shows that humans had high accuracy on the test set, whereas standard neural models scored essentially zero exact matches. The LLA-LSTM model that we tested appears to achieve only insigni\ufb01cantly higher results in Table 1. However, it has learned, from just 14 training examples, how to map some of the words to Lake et al.\u2019s interpretation of their context- invariant meanings. This is shown in Figure 2 (a).",
      "However, it has learned, from just 14 training examples, how to map some of the words to Lake et al.\u2019s interpretation of their context- invariant meanings. This is shown in Figure 2 (a). In the \ufb01g- ure, \u201cdax,\u201d \u201clug,\u201d \u201cwif,\u201d and \u201czup\u201d are interpreted correctly to mean \u201cr,\u201d \u201cg,\u201d \u201cb,\u201d and \u201cy,\u201d respectively. Here, the let- ters correspond to the types of unique dots, which are red, green, blue, and yellow, respectively. The other words, \u201cfep,\u201d \u201ckiki,\u201d and \u201cblicket,\u201d are taken by Lake et al. to have func- tional meanings, and so are correctly not associated strongly with any of the output tokens. The exceptions are two erro- neous associations between \u201ckiki\u201d and blue and \u201cblicket\u201d and green. Also, every sentence has a stop token, so the LLA units learned that the context-invariant meanings of each word in- clude it.",
      "The exceptions are two erro- neous associations between \u201ckiki\u201d and blue and \u201cblicket\u201d and green. Also, every sentence has a stop token, so the LLA units learned that the context-invariant meanings of each word in- clude it. The LLA units can handle cases where a word corre- sponds to multiple output tokens, and the output tokens need not be monolithic in the output sequence. As shown in tests from all of the other domains, these output token correspon- dences may or may not be relevant depending on the speci\ufb01c",
      "Table 1: Comparison of the models on the test sets. The metrics used are mean precision (Prec.), mean recall (Rec.), mean accuracy (Acc.), mean number of exact matches (Exact.), and corpus-level BLEU (Papineni et al., 2002). Colors GEO WSJ10 English to Mandarin Encoder and Decoder Model Prec. Rec. Acc. Exact Prec. Rec. Acc. Exact Prec. Rec. Acc.",
      "), and corpus-level BLEU (Papineni et al., 2002). Colors GEO WSJ10 English to Mandarin Encoder and Decoder Model Prec. Rec. Acc. Exact Prec. Rec. Acc. Exact Prec. Rec. Acc. Exact BLEU LLA-LSTM 90 62.50 50.90 0 97.00 92.65 88.31 50.4 76.81 56.90 42.49 2.51 11.71 LLA-LSTM (No Adversary) 90 62.50 49.48 0 97.08 92.68 89.80 47.2 73.42 57.85 43.67 2.76 11.40 LSTM 90 62.50 49.48 0 93.66 91.18 88.60 37.6 65.94 56.74 44.48 2.76 9.95 Table 2: Results for arti\ufb01cial Wernicke\u2019s and Broca\u2019s aphasia induced in the LLA-LSTM model.",
      "Damage to neural modules is done by randomly initializing their weights, causing the loss of all learned information. The inputs that we present are arbitrarily chosen, subject to the constraints listed in the text. Mean precision (Prec.) results on the test sets are also provided to demonstrate corpus-level results. An ellipses represents the repetition of the preceding word at least 1000 times. Colors GEO Modules Damaged wif kiki lug Prec. what is the capital of utah Prec None b g answer ( A ( capital ( A ) loc ( A B ) const ( B stateid ( utah ) ) ) ) ) LSTMs < s > 95 ( ( const ... 99.09 Lexicon Unit b g 90 answer ( A ( capital ( A ) loc ( A B ) const ( B stateid ( michigan ) ) ) ) ) 93.96 English to Mandarin WSJ10 Modules Damaged I ate some \ufb01sh . Prec. he needs it Prec.",
      "Prec. he needs it Prec. None \u6211\u5403\u4e86\u4e00\u4e9b\u9b5a\u3002 (s (np-sbj (prp he) ) (vp (vbz needs) (np (prp it) ) ) ) LSTMs \u5403\u6211... 87.37 (np ... 74.12 Lexicon Unit \u6211\u62ff\u4e86\u9019\u689d\u8a71\u3002 54.52 (s (np-sbj (prp he) ) (vp (vbz will) (np (prp it) ) ) ) 65.76 context of a word, but the recurrent component of the archi- tecture is capable of determining which to use. Semantic Parsing Geoquery (GEO) is a dataset where an input is an English geography query and the corresponding output is a parse that a computer could use to look up the answer in a database (Zelle, 1995). We used the standard test set of 250 pairs from Geoquery Data (2020). The remaining data were randomly split into a validation set of 100 pairs and a train set of 539 pairs. We tokenized the input data by splitting on the words and removing punctuation.",
      "We used the standard test set of 250 pairs from Geoquery Data (2020). The remaining data were randomly split into a validation set of 100 pairs and a train set of 539 pairs. We tokenized the input data by splitting on the words and removing punctuation. We tokenized the output data by removing commas and splitting on words, parentheses, and variables. There are 283 tokens in the input dictionary and 177 tokens in the output dictionary, respectively. Figure 2 (b) shows some weights for four input words, which are all relevant to the inputs. Many of the weights correspond directly to the correct predicates. Other tokens have high weights because they are typically important to any parse. These are parentheses, variables (A, B, C, and D), the \u201canswer\u201d token, and the stop token. Syntactic Parsing The Wall Street Journal portion of the Penn Treebank is a dataset where English sentences from The Wall Street Jour- nal are paired with human-generated phrase parses (Marcus et al., 1993). We use the test, validation, and train set from Kim, Dyer, and Rush\u2019s (2019) paper.",
      "We use the test, validation, and train set from Kim, Dyer, and Rush\u2019s (2019) paper. For ef\ufb01ciency, we only use sentences that have 10 or fewer words, lowercase all words, and modify Kim et al.\u2019s output data so that left parenthe- ses are paired with their corresponding nonterminal and right parentheses are paired with their corresponding terminal. The input and output data were both tokenized by splitting where there is a space. The test, validation, and train set are 398, 258, and 6007 pairs, respectively. There are 9243 tokens in the input dictionary and 9486 tokens in the output dictionary. Figure 2 (c) shows some weights for four input words. They all highlight the relevant terminal, and syntactic cate- gories that are usually associated with that word. The asso- ciated categories typically are either those of that word, the phrases headed by the category of that word, or those that select or are selected by that word.",
      "They all highlight the relevant terminal, and syntactic cate- gories that are usually associated with that word. The asso- ciated categories typically are either those of that word, the phrases headed by the category of that word, or those that select or are selected by that word. The relevant nontermi- nal terminology is as follows (Marcus et al., 1993): \u201c(in\u201d is a preposition or subordinating conjunction, \u201c(np\u201d is a noun phrase, \u201c(pp\u201d is a prepositional phrase, \u201c(np-subj\u201d is a noun phrase with a surface subject marking, \u201c(vp\u201d is a verb phrase, \u201c(vbn\u201d is a verb in the past participle, \u201c(adjp\u201d is an adjec- tive phrase, \u201c(vbp\u201d is a non-3rd person singular present verb, \u201c(prp\u201d is a personal pronoun, \u201c(rb\u201d is an adverb, \u201c(sq\u201d is the main clause of a wh-question, or it indicates an inverted yes or no question, and \u201c(s\u201d is the root.",
      "English to Chinese The Tatoeba (Ho, 2020) English to Chinese translation dataset, processed by Kelly (2020), is a product of a crowd- sourced effort to translate sentences of a user\u2019s choice into",
      "(a) (b) (c) (d) Figure 2: The learned \u03c3(w)\u2019s for some diverse input words; they are arbitrarily chosen, subject to the constraints listed in the text. Black is zero and white is one. (a) shows the results from the colors dataset; the input and output dictionaries are so small that all of the weights for all of the input words are shown. (b), (c), and (d) show results from the semantic parsing, syntactic parsing, and English to Mandarin translation datasets, respectively. Because there are so many words in the input dictionary, only four \u03c3(w)\u2019s are shown in each case. Because there are so many tokens in the output dictionary, if a weight within a \u03c3(w) is zero when rounded to the nearest tenth\u2019s place, then it is omitted. So, for each \u03c3(w), approximately 170, 9480, and 3430 weights were omitted for the semantic parsing, syntactic parsing, and English to Mandarin translation cases, respectively. another language. The data were split randomly into a test, validation, and train set of 1500, 1500, and 18205 pairs, re- spectively.",
      "another language. The data were split randomly into a test, validation, and train set of 1500, 1500, and 18205 pairs, re- spectively. The English was tokenized by splitting on punc- tuation and words. The Chinese was tokenized by splitting on punctuation and characters. There are 6919 and 3434 to- kens in the input and output dictionary, respectively. There are often many acceptable outputs when translating one nat- ural language to another. As a result, we use the corpus-level BLEU score (Papineni et al., 2002) to test models and score them on the validation set. Figure 2 (d) shows some weights for four input words. The listed Chinese words are an acceptable translation (depending on the context) and correspond roughly one-to-one with the English inputs. There are three exceptions.",
      "Figure 2 (d) shows some weights for four input words. The listed Chinese words are an acceptable translation (depending on the context) and correspond roughly one-to-one with the English inputs. There are three exceptions. Although \u4e48is correctly given a low weight, its presence seems to be an er- ror; it usually appears with another character to mean \u201cwhat.\u201d \u6211\u5011and \u6211\u4eectypically translate to \u201cwe,\u201d even though \u6211 alone translates to \u201cme.\u201d \u5011is a plural marker and \u4eecis the same, but simpli\ufb01ed; both versions evidently found their way into the dataset. The network has correctly learned to asso- ciate both Chinese words necessary to form the meaning of \u201cwe.\u201d Also, \u6b65\u6563means \u201cwalk,\u201d but \u6563generally does not appear alone to mean \u201cwalk.\u201d Again, the network has learned to correctly associate all of the necessary characters with an input word. The results from this dataset in Table 2 warrant a discus- sion for readers who do not know Chinese.",
      "The results from this dataset in Table 2 warrant a discus- sion for readers who do not know Chinese. As in the other cases, the model demonstrates the expected knowledge and lack thereof when different types of arti\ufb01cial aphasia are in- duced.3 The outputs are also productions that Chinese apha- sics are expected to make per Caramazza and Zurif\u2019s (1976) 3We consulted with a native Mandarin Chinese speaker to inter- pret the model\u2019s outputs. description. When the model is undamaged, its output is a correct translation for \u201cI ate some \ufb01sh.\u201d When the model\u2019s LSTMs are damaged (simulating the conditions for Broca\u2019s aphasia), the production has incorrect syntax, and translates word for word to \u201ceat I ...\u201d These are both correct content words. When the model\u2019s Lexicon Unit is damaged (simu- lating the conditions for Wernicke\u2019s aphasia), the production has correct syntax. Impressively, the Chinese actually has the same syntax as the correct translation for \u201cI ate some \ufb01sh.\u201d However, the content is nonsensical.",
      "Impressively, the Chinese actually has the same syntax as the correct translation for \u201cI ate some \ufb01sh.\u201d However, the content is nonsensical. The English translation is \u201cI took the utterance.\u201d Compared to the correct Mandarin translation, this incorrect one has the same subject and the same past-tense marker, \u4e86, for the verb. However it uses a different verb, object, and determiner. Related Work There is evidence that generic attention mechanisms for ma- chine translation already utilize the thesis that words have meanings that are independent of syntax. They learn corre- spondences between output tokens and a hidden state pro- duced immediately after an encoder reads a particular input word (Bahdanau, Cho, & Bengio, 2015). But the same mech- anism is not at play in our model. Generic attention mech- anisms do not necessarily impose a constraint on the input\u2019s syntax representation. Additionally, typical objective func- tions do not explicitly link input words with invariance in the output. Finally, one does not need to choose either LLA units or attention.",
      "Generic attention mech- anisms do not necessarily impose a constraint on the input\u2019s syntax representation. Additionally, typical objective func- tions do not explicitly link input words with invariance in the output. Finally, one does not need to choose either LLA units or attention. LLA units can be incorporated into recur- rent neural network systems with attention or other machine transduction architectures such as transformers (Vaswani et al., 2017). Recent work has incorporated some of the ideas in our pa- per into a neural machine translation model with the use of a speci\ufb01c attention mechanism (Russin, Jo, O\u2019Reilly, & Ben-",
      "gio, 2019). But the authors only demonstrate success on a single arti\ufb01cial dataset with a lexicon of about ten words, and they did not explore the effects of damaging parts of their model. Their optimization procedure also does not prohibit context-invariant lexical information from passing through the recurrent portion of their model. This incorrectly allows the possibility for a representation to be learned that gives every input word its own syntactic category. Lastly, their ar- chitecture provides a softer constraint than the one that we demonstrate, as information from several input words can ag- gregate and pass through the non-recurrent module that they use. There are other attempts to incorporate theories about hu- man language to regularize a transduction model, but many have not scaled to the level of generality that the LLA units and some attention architectures show. These include synchronous grammars (Chiang, 2006), data augmentation (Andreas, 2019), Meta learning (Lake, 2019), and hard-coded maps or copying capabilities from input to output (Liang, Jor- dan, & Klein, 2011) (Jia & Liang, 2016).",
      "All require hard- coded rules that are often broken by the real world. Conclusion Neural and cognitive theories provide an imperative for com- putational models to understand human language by separat- ing representations of word meanings from those of syntax. Using this constraint, we introduced new neural units that can provide this separation for the purpose of translating human languages. When added to an LSTM encoder and decoder, our units showed improvements in all of our experiment do- mains over the typical model. The domains were a small ar- ti\ufb01cial diagnostic dataset, semantic parsing, syntactic pars- ing, and English to Mandarin Chinese translation. We also showed that the model learns a representation of human lan- guage that is similar to that of our brains. When damaged, the model displays the same knowledge distortions that aphasics do. Acknowledgments NOT INCLUDED IN DRAFT SUBMISSION References Altshuler, D., Parsons, T., & Schwarzschild, R. (2018). A course in semantics (draft version). MIT Press. Andreas, J. (2019). Good-enough compositional data aug- mentation. arXiv.",
      "(2018). A course in semantics (draft version). MIT Press. Andreas, J. (2019). Good-enough compositional data aug- mentation. arXiv. Bahdanau, D., Cho, K., & Bengio, Y. (2015). Neural machine translation by jointly learning to align and translate. Pro- ceedings of the 3rd International Conference for Learning Representations. Caramazza, A., & Zurif, E. (1976). Dissociation of algorith- mic and heuristic processes in language comprehension: Evidence from aphasia. Brain and Language, 3, 572-582. Chiang, D. (2006). An introduction to synchronous gram- mars. Part of an Association for Computational Linguistics Tutorial. Geoquery data. (2020, January 14). Retrieved from http://www.cs.utexas.edu/users/ml/nldata/ geoquery.html Ho, T. (2020, January 14). Tatoeba.",
      "Part of an Association for Computational Linguistics Tutorial. Geoquery data. (2020, January 14). Retrieved from http://www.cs.utexas.edu/users/ml/nldata/ geoquery.html Ho, T. (2020, January 14). Tatoeba. Retrieved from https://tatoeba.org Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9, 1735-1780. Jia, R., & Liang, P. (2016). Data recombination for neural semantic parsing. Proceedings of the 54th Annual Confer- ence of the Association for Computational Linguistics. Kalchbrenner, N., & Blunsom, P. (2013). Recurrent contin- uous translation models. Proceedings of the 2013 Confer- ence on Empirical Methods in Natural Language Process- ing. Kelly, C. (2020, January 3). Tab-delimited bilingual sentence pairs.",
      "(2013). Recurrent contin- uous translation models. Proceedings of the 2013 Confer- ence on Empirical Methods in Natural Language Process- ing. Kelly, C. (2020, January 3). Tab-delimited bilingual sentence pairs. Retrieved from https://www.manythings.org/anki/ Kim, Y., Dyer, C., & Rush, A. (2019). Compound proba- bilistic context-free grammars for grammar induction. Pro- ceedings of the 57th Annual Conference of the Association for Computational Linguistics. Kingma, D., & Ba, J. (2015). Adam: A method for stochastic optimization. Proceedings of the 3rd International Confer- ence for Learning Representations. Lake, B. (2019). Compositional generalization through meta sequence-to-sequence learning. Proceedings of the 33rd Conference on Neural Information Processing Systems. Lake, B., Linzen, T., & Baroni, M. (2019). Human few-shot learning of compositional instructions.",
      "Compositional generalization through meta sequence-to-sequence learning. Proceedings of the 33rd Conference on Neural Information Processing Systems. Lake, B., Linzen, T., & Baroni, M. (2019). Human few-shot learning of compositional instructions. Proceedings of the 41st Annual Conference of the Cognitive Science Society. Liang, P., Jordan, M., & Klein, D. (2011). Learning dependency-based compositional semantics. Proceedings of the 49th Annual Conference of the Association for Com- putational Linguistics. Marcus, M., Marcinkiewicz, M., & Santorini, B. (1993). Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19, 313-330. Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). Bleu: a method for automatic evaluation of machine trans- lation. Proceedings of the 40th Annual Meeting of the As- sociation for Computational Linguistics.",
      "(2002). Bleu: a method for automatic evaluation of machine trans- lation. Proceedings of the 40th Annual Meeting of the As- sociation for Computational Linguistics. Russin, J., Jo, J., O\u2019Reilly, R., & Bengio, Y. (2019). Compo- sitional generalization in a deep seq2seq model by separat- ing syntax and semantics. arXiv preprint. Torch.optim. (2020, January 14). Retrieved from https://pytorch.org/docs/stable/optim.html Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., ... Polosukhin, I. (2017). Attention is all you need. Proceedings of the 31st Conference on Neural Information Processing Systems. Zelle, J. (1995). Using inductive logic programming to au- tomate the construction of natural language parsers. Un- published doctoral dissertation, Department of Computer Sciences, The University of Texas at Austin, Austin, TX."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-2002.08899.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":7714,
  "avg_doclen":183.6666666667,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-2002.08899.pdf"
    }
  }
}