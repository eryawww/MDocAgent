{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "arXiv:1905.10810v1  [cs.CL]  26 May 2019 Evaluation of basic modules for isolated spelling error correction in Polish texts Szymon Rutkowski\u2217 \u2217University of Warsaw Krakowskie Przedmie\u00b4scie 26/28, 00-927 Warsaw, Poland szymon@szymonrutkowski.pl Abstract Spelling error correction is an important problem in natural language processing, as a prerequisite for good performance in downstream tasks as well as an important feature in user-facing applications. For texts in Polish language, there exist works on speci\ufb01c error correction solutions, often developed for dealing with specialized corpora, but not evaluations of many different approaches on big resources of errors. We begin to address this problem by testing some basic and promising methods on PlEWi, a corpus of annotated spelling extracted from Polish Wikipedia. These modules may be further combined with appropriate solutions for error detection and context awareness. Following our results, combining edit distance with cosine distance of semantic vectors may be suggested for interpretable systems, while an LSTM, particularly enhanced by ELMo embeddings, seems to offer the best raw performance. 1.",
      "These modules may be further combined with appropriate solutions for error detection and context awareness. Following our results, combining edit distance with cosine distance of semantic vectors may be suggested for interpretable systems, while an LSTM, particularly enhanced by ELMo embeddings, seems to offer the best raw performance. 1. Introduction Spelling error correction is a fundamental NLP task. Most language processing applications bene\ufb01t greatly from being provided clean texts for their best performance. Human users of computers also often expect competent help in making spelling of their texts correct. Because of the lack of tests of many common spelling correction methods for Polish, it is useful to establish how they perform in a simple scenario. We constrain ourselves to the pure task of isolated correction of non-word errors. They are traditionally separated in error correction litera- ture (Kukich, 1992). Non-word errors are here incorrect word forms that not only differ from what was intended, but also do not constitute another, existing word them- selves. Much of the initial research on error correction fo- cused on this simple task, tackled without means of taking the context of the nearest words into account.",
      "Non-word errors are here incorrect word forms that not only differ from what was intended, but also do not constitute another, existing word them- selves. Much of the initial research on error correction fo- cused on this simple task, tackled without means of taking the context of the nearest words into account. It is true that, especially in the case of neural networks, it is often possible and desirable to combine problems of error detection, correction and context awareness into one task trained with a supervised training procedure. In lan- guage correction research for English language also gram- matical and regular spelling errors have been treated uni- formly with much success (Ge et al., 2018). However, when more traditional methods are used, be- cause of their predictability and interpretability for exam- ple, one can mix and match various approaches to dealing with the subproblems of detection, correction and context handling (often equivalent to employing some kind of a language model). We call it a modular approach to build- ing spelling error correction systems. There is recent re- search where this paradigm was applied, interestingly, to convolutional networks trained separately for various sub- tasks (Dronen, 2016).",
      "We call it a modular approach to build- ing spelling error correction systems. There is recent re- search where this paradigm was applied, interestingly, to convolutional networks trained separately for various sub- tasks (Dronen, 2016). In similar setups it is more useful to assess abilities of various solutions in isolation. The exact architecture of a spelling correction system should depend on characteristics of texts it will work on. Similar considerations eliminated from our focus hand- crafted solutions for the whole spelling correction pipeline, primarily the LanguageTool (Mi\u0142kowski, 2010). Its per- formance in \ufb01xing spelling of Polish tweets was already tested (Ogrodniczuk and Kope\u00b4c, 2017). For our purposes it would be given an unfair advantage, since it is a rule- based system making heavy use of words in context of the error. 2. Problems of spelling correction for Polish Published work on language correction for Polish dates back at least to 1970s, when simplest Levenshtein dis- tance solutions were used for cleaning mainframe in- puts (Subieta, 1976; Subieta, 1985).",
      "Problems of spelling correction for Polish Published work on language correction for Polish dates back at least to 1970s, when simplest Levenshtein dis- tance solutions were used for cleaning mainframe in- puts (Subieta, 1976; Subieta, 1985). Spelling correc- tion tests described in literature have tended to focus on one approach applied to a speci\ufb01c corpus. Limited ex- amples include works on spellchecking mammography reports and tweets (Mykowiecka and Marciniak, 2007; Ogrodniczuk and Kope\u00b4c, 2017). These works emphasized the importance of tailoring correction systems to speci\ufb01c problems of corpora they are applied to. For example, mammography reports suffer from poor typing, which in this case is a repetitive work done in relative hurry. Tweets, on the other hand, tend to contain emoticons and neolo- gisms that can trick solutions based on rules and dictio- naries, such as LanguageTool.",
      "For example, mammography reports suffer from poor typing, which in this case is a repetitive work done in relative hurry. Tweets, on the other hand, tend to contain emoticons and neolo- gisms that can trick solutions based on rules and dictio- naries, such as LanguageTool. The latter is, by itself, fairly well suited for Polish texts, since a number of extensions to the structure of this application was inspired by problems with morphology of Polish language (Mi\u0142kowski, 2010). These existing works pointed out more general, poten- tially useful qualities speci\ufb01c to spelling errors in Polish language texts. It is, primarily, the problem of leaving out diacritical signs, or, more rarely, adding them in wrong places. This phenomenon stems from using a variant of the US keyboard layout, where combinations of AltGr with some alphabetic keys produces characters unique to Pol- ish. When the user forgets or neglects to press the AltGr key, typos such as writing *olowek instead of o\u0142\u00f3wek ap- pear.",
      "When the user forgets or neglects to press the AltGr key, typos such as writing *olowek instead of o\u0142\u00f3wek ap- pear. In fact, (Ogrodniczuk and Kope\u00b4c, 2017) managed to get substantial performance on Twitter corpus by using this \u201ddiacritical swapping\u201d alone. 3. Methods 3.1. Baseline methods The methods that we evaluated are baselines are the ones we consider to be basic and with moderate potential",
      "of yielding particularly good results. Probably the most straightforward approach to error correction is selecting known words from a dictionary that are within the smallest edit distance from the error. We used the Levenshtein dis- tance metric (Levenshtein, 1966) implemented in Apache Lucene library (Apache Software Foundation, 2019). It is a version of edit distance that treats deletions, insertions and replacements as adding one unit distance, without giv- ing a special treatment to character swaps. The SGJP \u2013 Grammatical Dictionary of Polish (Saloni et al., 2019) was used as the reference vocabulary. Another simple approach is the aforementioned di- acritical swapping, which is a term that we introduce here for referring to a solution inspired by the work of (Ogrodniczuk and Kope\u00b4c, 2017). Namely, from the incor- rect form we try to produce all strings obtainable by ei- ther adding or removing diacritical marks from charac- ters. We then exclude options that are not present in SGJP, and select as the correction the one within the small- est edit distance from the error.",
      "We then exclude options that are not present in SGJP, and select as the correction the one within the small- est edit distance from the error. It is possible for the number of such diacritically-swapped options to become very big. For example, the token Modlin-Zegrze-Pultusk- R\u00f3\u02d9zan-Ostro\u0142\u02dbeka-\u0141om\u02d9za-Osowiec (taken from PlEWi cor- pus of spelling errors, see below) can yield over 229 = 536, 870, 912 states with this method, such as M\u00f3d\u0142i\u00b4n- \u02d9Z\u02dbegrz\u02dbe-Pu\u0142tu\u00b4sk-Ro\u00b4z \u02dba\u00b4n-\u00d3\u00b4str\u00f3lek \u02dba-L\u00f3mz \u02dba-\u00d3\u00b4s\u00f3wi\u02dbe\u00b4c. The actual correction here is just \ufb01xing the \u0142 in Pu\u0142tusk. Hence we only try to correct in this way tokens that are shorter than 17 characters. 3.2.",
      "The actual correction here is just \ufb01xing the \u0142 in Pu\u0142tusk. Hence we only try to correct in this way tokens that are shorter than 17 characters. 3.2. Vector distance A promising method, adapted from work on correcting texts by English language learners (Nagata et al., 2017), expands on the concept of selecting a correction nearest to the spelling error according to some notion of distance. Here, the Levenshtein distance is used in a weighted sum to cosine distance between word vectors. This is based on the observation that trained vectors models of distribu- tional semantics contain also representations of spelling er- rors, if they were not pruned. Their representations tend to be similar to those of their correct counterparts. For ex- ample, the token enginir will appear in similar contexts as engineer, and therefore will be assigned a similar vector embedding. The distance between two tokens a and b is thus de\ufb01ned as D(a, b) = LD(a, b) + CD(V(a), V(b)) 2 . Here LD is just Levenshtein distance between strings, and CD \u2013 cosine distance between vectors.",
      "The distance between two tokens a and b is thus de\ufb01ned as D(a, b) = LD(a, b) + CD(V(a), V(b)) 2 . Here LD is just Levenshtein distance between strings, and CD \u2013 cosine distance between vectors. V(a) de- notes the word vector for a. Both distance metrics are in our case roughly in the range [0,1] thanks to the scal- ing of edit distance performed automatically by Apache Lucene. We used a pretrained set of word embeddings of Polish (Mykowiecka et al., 2017), obtained with the \ufb02avor word2vec procedure using skipgrams and negative sam- pling (Mikolov et al., 2013). 3.3. Recurrent neural networks Another powerful approach, if conceptually sim- ple in linguistic terms, is using a character-based recurrent neural network. Here, we test uni- and bidirectional Long Short-Term Memory networks (Hochreiter and Schmidhuber, 1997) that are fed charac- ters of the error as their input and are expected to output its correct form, character after character.",
      "Here, we test uni- and bidirectional Long Short-Term Memory networks (Hochreiter and Schmidhuber, 1997) that are fed charac- ters of the error as their input and are expected to output its correct form, character after character. This is similar to traditional solutions conceptualizing the spelling error as a chain of characters, which are used as evidence to predict the most likely chain of replacements (original characters). This was done with n-gram methods, Markov chains and other probabilistic models (Araki et al., 1994). Since nowadays neural networks enjoy a large awareness as an element of software infrastructure, with actively maintained packages readily available, their evaluation seems to be the most practically useful. We used the PyTorch (Paszke et al., 2017) implementation of LSTM in particular. The bidirectional version (Schuster and Paliwal, 1997) of LSTM reads the character chains forward and back- wards at the same time. Predictions from networks running in both directions are averaged.",
      "The bidirectional version (Schuster and Paliwal, 1997) of LSTM reads the character chains forward and back- wards at the same time. Predictions from networks running in both directions are averaged. In order to provide the network an additional, broad picture peek at the whole error form we also evaluated a setup where the internal state of LSTM cells, instead of be- ing initialized randomly, is computed from an ELMo em- bedding (Peters et al., 2018) of the token. The ELMo em- bedder is capable of integrating linguistic information car- ried by the whole form (probably often not much in case of errors), as well as the string as a character chain. The lat- ter is processed with a convolutional neural network. How this representation is constructed is informed by the whole corpus on which the embedder was trained. The pretrained ELMo model that we used (Che et al., 2018) was trained on Wikipedia and Common Crawl corpora of Polish.",
      "How this representation is constructed is informed by the whole corpus on which the embedder was trained. The pretrained ELMo model that we used (Che et al., 2018) was trained on Wikipedia and Common Crawl corpora of Polish. The ELMo embedding network outputs three layers as matrices, which are supposed to re\ufb02ect subsequent com- positional layers of language, from phonetic phenomena at the bottom to lexical ones at the top. A weighted sum of these layers is computed, with weights trained along with the LSTM error-correcting network. Then we apply a trained linear transformation, followed by ReLU non- linearity: ReLU(x) = max(0, x) (applied cellwise) in order to obtain the initial setting of parameters for the main LSTM. Our ELMo-augmented LSTM is bidirectional. 4. Experimental setup PlEWi (Grundkiewicz, 2013) is an early version of WikEd (Grundkiewicz and Junczys-Dowmunt, 2014) er- ror corpus, containing error type annotations allowing us to select only non-word errors for evaluation.",
      "Speci\ufb01- cally, PlEWi supplied 550,755 [error, correction] pairs, from which 298,715 were unique. The corpus contains data extracted from histories of page versions of Polish Wikipedia. An algorithm designed by the corpus author",
      "Method Accuracy Perplexity Loss (train) Loss (test) Edit distance 0.3453 - - - Diacritic swapping 0.2279 - - - Vector distance 0.3945 - - - LSTM-1 net 0.4183 907 0.3 0.41 LSTM-2 net 0.6634 11182 0.1 0.37 LSTM-ELMo net 0.6818 706166 0.07 0.38 Table 1: Test results for all the methods used. The loss measure is cross-entropy. Layer I Layer II Layer III 0.036849 0.08134 0.039395 Table 2: Discovered optimal weights for summing lay- ers of ELMo embedding for initializing an error-correcting LSTM. The layers are numbered from the one that directly processes character and word input to the most abstract one. determined where the changes were correcting spelling er- rors, as opposed to expanding content and disagreements among Wikipedia editors.",
      "The layers are numbered from the one that directly processes character and word input to the most abstract one. determined where the changes were correcting spelling er- rors, as opposed to expanding content and disagreements among Wikipedia editors. The corpus features texts that are descriptive rather than conversational, contain relatively many proper names and are more likely to have been at least skimmed by the au- thors before submitting for online publication. Error cases provided by PlEWi are, therefore, not a balanced represen- tation of spelling errors in written Polish language. PlEWi does have the advantage of scale in comparison to existing literature, such as (Ogrodniczuk and Kope\u00b4c, 2017) operat- ing on a set of only 740 annotated errors in tweets. All methods were tested on a test subset of 25% of cases, with 75% left for training (where needed) and 5% for development. The methods that required training \u2013 namely recurrent neural networks \u2013 had their loss measured as cross-entropy loss measure between correct character labels and predic- tions. This value was minimized with Adam algorithm (Kingma and Ba, 2014).",
      "The methods that required training \u2013 namely recurrent neural networks \u2013 had their loss measured as cross-entropy loss measure between correct character labels and predic- tions. This value was minimized with Adam algorithm (Kingma and Ba, 2014). The networks were trained for 35 epochs. 5. Results The experimental results are presented in Table 1. Dia- critic swapping showed a remarkably poor performance, despite promising mentions in existing literature. This might be explained by the already mentioned feature of Wikipedia edits, which can be expected to be to some de- gree self-reviewed before submission. This can very well limit the number of most trivial mistakes. On the other hand, the vector distance method was able to bring a discernible improvement over pure Levenshtein distance, comparable even with the most basic LSTM. It is possible that assigning more \ufb01ne-tuned weights to edit distance and semantic distance would make the quality of predictions even higher. The idea of using vector space measurements explicitly can be also expanded if we were to consider the problem of contextualizing corrections. For example, the semantic distance of proposed corrections to the nearest words is likely to carry much information about their appropriateness.",
      "The idea of using vector space measurements explicitly can be also expanded if we were to consider the problem of contextualizing corrections. For example, the semantic distance of proposed corrections to the nearest words is likely to carry much information about their appropriateness. Looking from another angle, search- ing for words that seem semantically off in context may be a good heuristic for detecting errors that are not nonword (that is, they lead to wrong forms appearing in text which are nevertheless in-vocabulary). The good performance of recurrent network methods is hardly a surprise, given observed effectiveness of neural networks in many NLP tasks in the recent decade. It seems that bidirectional LSTM augmented with ELMo may al- ready hit the limit for correcting Polish spelling errors without contextual information. While it improves accu- racy in comparison to LSTM initialized withrandom noise, it makes the test cross-entropy slightly worse, which hints at over\ufb01tting. The perplexity measures actually increase sharply for more sophisticated architectures. Perplexity should show how little probability is assigned by the model to true answers.",
      "The perplexity measures actually increase sharply for more sophisticated architectures. Perplexity should show how little probability is assigned by the model to true answers. We measure it as perplexity(P, x) = 2\u22121 N P i\u2a7dN log P (xi), where x is a sequence of N characters, forming the cor- rect version of the word, and P(xi) is the estimated proba- bility of the ith character, given previous predicted charac- ters and the incorrect form. The observed increase of per- plexity for increasingly accurate models is most likely due to more re\ufb01ned predicted probability distributions, which go beyond just assigning the bulk of probability to the best answer. Interesting insights can be gained from weights as- signed by optimization to layers of ELMo network, which are taken as the word form embedding (Table 2). The \ufb01rst layer, and the one that is nearest to input of the network, is given relatively the least importance, while the middle one dominates both others taken together.",
      "The \ufb01rst layer, and the one that is nearest to input of the network, is given relatively the least importance, while the middle one dominates both others taken together. This suggests that in error correction, at least for Polish, the middle level of morphemes and other characteristic character chunks is more important than phenomena that are low-level or tied to some speci\ufb01c words. This observation should be taken into account in further research on practical solutions for spelling correction. 6. Conclusion Among the methods tested the bidirectional LSTM, especially initialized by ELMo embeddings, offers the best accuracy and raw performance. Adding ELMo to a straightforward PyTorch implementation of LSTM may be easier now than at the time of performing our tests, as since then the authors of ELMoForManyLangs package",
      "(Che et al., 2018) improved their programmatic interface. However, if a more interpretable and explainable output is required, some version of vector distance combined with edit distance may be the best direction. It should be noted that this method produces multiple candidate corrections with their similarity scores, as opposed to only one \u201cbest guess\u201c correction that can be obtained from a character- based LSTM. This is important in applications where it is up to humans to the make the \ufb01nal decision, and they are only to be aided by a machine. It is desirable for further reasearch to expand the cor- pus material into a wider and more representative set of texts. Nevertheless, the solution for any practical case has to be tailored to its characteristic error patterns. Works on language correction for English show that available cor- pora can be \u201dboosted\u201d (Ge et al., 2018), i.e. expanded by generating new errors consistent with a generative model inferred from the data. This may greatly aid in developing models that are dependent on learning from error corpora. A deliberate omission in this paper are the elements accompanying most real-word error correction solutions.",
      "expanded by generating new errors consistent with a generative model inferred from the data. This may greatly aid in developing models that are dependent on learning from error corpora. A deliberate omission in this paper are the elements accompanying most real-word error correction solutions. Some fairly obvious approaches to integrating evidence from context include n-grams and Markov chains, although the possibility of using measurements in spaces of seman- tic vectors was already mentioned in this article. Simi- larly, non-word errors can be easily detected with com- paring tokens against reference vocabulary, but in practice one should have ways of detecting mistakes masquerading as real words and \ufb01xing bad segmentation (tokens that are glued together or improperly separated). Testing how per- formant are various methods for dealing with these prob- lems in Polish language is left for future research. 7. References Apache Software Foundation, 2019. pylucene. Araki, Tetsuo, Satorn Ikehara, Nobuyuki Tsukahara, and Yasunori Komatsu, 1994. An evaluation to detect and correct erroneous characters wrongly substituted, deleted and inserted in japanese and english sentences using markov models.",
      "Araki, Tetsuo, Satorn Ikehara, Nobuyuki Tsukahara, and Yasunori Komatsu, 1994. An evaluation to detect and correct erroneous characters wrongly substituted, deleted and inserted in japanese and english sentences using markov models. In Proceedings of the 15th con- ference on Computational linguistics-Volume 1. Associ- ation for Computational Linguistics. Che, Wanxiang, Yijia Liu, Yuxuan Wang, Bo Zheng, and Ting Liu, 2018. Towards better ud parsing: Deep con- textualized word embeddings, ensemble, and treebank concatenation. CoRR, abs/1807.03121. Dronen, Nicholas A., 2016. Correcting Writing Errors with Convolutional Neural Networks. Ph.D. thesis, 200 University of Colorado at Boulder. Ge, Tao, Furu Wei, and Ming Zhou, 2018. Reach- ing human-level performance in automatic grammat- ical error correction: An empirical study. CoRR, abs/1807.01270. Grundkiewicz, Roman, 2013.",
      "Ge, Tao, Furu Wei, and Ming Zhou, 2018. Reach- ing human-level performance in automatic grammat- ical error correction: An empirical study. CoRR, abs/1807.01270. Grundkiewicz, Roman, 2013. Automatic extraction of pol- ish language errors from text edition history. In Ivan Habernal and V\u00e1clav Matou\u0161ek (eds.), Text, Speech, and Dialogue. Berlin, Heidelberg: Springer Berlin Heidel- berg. Grundkiewicz, Roman and Marcin Junczys-Dowmunt, 2014. The wiked error corpus: A corpus of corrective wikipedia edits and its application to grammatical error correction. In PolTAL. Hochreiter, Sepp and J\u00fcrgen Schmidhuber, 1997. Long short-term memory. Neural Comput., 9(8):1735\u20131780. Kingma, Diederik P. and Jimmy Ba, 2014. Adam: A method for stochastic optimization. CoRR, abs/1412.6980.",
      "Long short-term memory. Neural Comput., 9(8):1735\u20131780. Kingma, Diederik P. and Jimmy Ba, 2014. Adam: A method for stochastic optimization. CoRR, abs/1412.6980. Kukich, Karen, 1992. Techniques for automatically cor- recting words in text. Acm Computing Surveys (CSUR), 24(4):377\u2013439. Levenshtein, VI, 1966. Binary Codes Capable of Correct- ing Deletions, Insertions and Reversals. Soviet Physics Doklady, 10:707. Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean, 2013. Distributed representations of words and phrases and their compositionality. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural Infor- mation Processing Systems 26.",
      "In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural Infor- mation Processing Systems 26. Curran Associates, Inc., pages 3111\u20133119. Mi\u0142kowski, Marcin, 2010. Developing an open-source, rule-based proofreading tool. Software: Practice and Experience, 40(7):543\u2013566. Mykowiecka, Agnieszka and Ma\u0142gorzata Marciniak, 2007. Domain\u2013driven automatic spelling correction for mam- mography reports. In Advances in Soft Computing, vol- ume 35. Mykowiecka, Agnieszka, Ma\u0142gorzata Marciniak, and Pi- otr Rychlik, 2017. Testing word embeddings for Polish. Cognitive Studies / \u00c9tudes Cognitives, 17:1\u201319. Nagata, Ryo, Hiroya Takamura, and Graham Neubig, 2017. Adaptive spelling error correction models for learner english.",
      "Testing word embeddings for Polish. Cognitive Studies / \u00c9tudes Cognitives, 17:1\u201319. Nagata, Ryo, Hiroya Takamura, and Graham Neubig, 2017. Adaptive spelling error correction models for learner english. Procedia Computer Science, 112:474 \u2013 483. Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 21st Interna- tional Conference, KES-20176-8 September 2017, Mar- seille, France. Ogrodniczuk, Maciej and Mateusz Kope\u00b4c, 2017. Lexical correction of Polish Twitter political data. In Proceed- ings of the Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Hu- manities and Literature. Vancouver, Canada: Associa- tion for Computational Linguistics. Paszke, Adam, Sam Gross, and Adam Lerer, 2017. Auto- matic differentiation in pytorch.",
      "Vancouver, Canada: Associa- tion for Computational Linguistics. Paszke, Adam, Sam Gross, and Adam Lerer, 2017. Auto- matic differentiation in pytorch. Peters, Matthew E., Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer, 2018. Deep contextualized word represen- tations. CoRR, abs/1802.05365. Saloni, Zygmunt, Marcin Woli\u00b4nski, Robert Wo\u0142osz, W\u0142odzimierz Gruszczy\u00b4nski, and Danuta Skowro\u00b4nska, 2019. S\u0142ownik gramatyczny j\u02dbezyka polskiego \u2014 wer- sja online. Schuster, M. and K.K. Paliwal, 1997. Bidirectional recur- rent neural networks. Trans. Sig. Proc., 45(11):2673\u2013 2681. Subieta, Kazimierz, 1976.",
      "Schuster, M. and K.K. Paliwal, 1997. Bidirectional recur- rent neural networks. Trans. Sig. Proc., 45(11):2673\u2013 2681. Subieta, Kazimierz, 1976. Korekcja pojedynczych b\u0142\u02dbed\u00f3w w wyrazach na podstawie s\u0142ownika wzorc\u00f3w. Infor- matyka, 11(2):15\u201318.",
      "Subieta, Kazimierz, 1985. A simple method of data cor- rection. The Computer Journal, 28(4):372\u2013374."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1905.10810.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":5502,
  "avg_doclen":177.4838709677,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1905.10810.pdf"
    }
  }
}