[
  "Explicit Utilization of General Knowledge in Machine Reading Comprehension Chao Wang and Hui Jiang Department of Electrical Engineering and Computer Science Lassonde School of Engineering, York University 4700 Keele Street, Toronto, Ontario, Canada {chwang, hj}@eecs.yorku.ca Abstract To bridge the gap between Machine Reading Comprehension (MRC) models and human beings, which is mainly re\ufb02ected in the hunger for data and the robustness to noise, in this paper, we explore how to integrate the neu- ral networks of MRC models with the general knowledge of human beings. On the one hand, we propose a data enrichment method, which uses WordNet to extract inter-word semantic connections as general knowledge from each given passage-question pair. On the other hand, we propose an end-to-end MRC model named as Knowledge Aided Reader (KAR), which explicitly uses the above extracted gen- eral knowledge to assist its attention mecha- nisms. Based on the data enrichment method, KAR is comparable in performance with the state-of-the-art MRC models, and signi\ufb01cantly more robust to noise than them.",
  "Based on the data enrichment method, KAR is comparable in performance with the state-of-the-art MRC models, and signi\ufb01cantly more robust to noise than them. When only a subset (20%\u201380%) of the training examples are available, KAR outperforms the state-of- the-art MRC models by a large margin, and is still reasonably robust to noise. 1 Introduction Machine Reading Comprehension (MRC), as the name suggests, requires a machine to read a pas- sage and answer its relevant questions. Since the answer to each question is supposed to stem from the corresponding passage, a common MRC so- lution is to develop a neural-network-based MRC model that predicts an answer span (i.e. the an- swer start position and the answer end position) from the passage of each given passage-question pair. To facilitate the explorations and innovations in this area, many MRC datasets have been estab- lished, such as SQuAD (Rajpurkar et al., 2016), MS MARCO (Nguyen et al., 2016), and Trivi- aQA (Joshi et al., 2017).",
  "Consequently, many pi- oneering MRC models have been proposed, such as BiDAF (Seo et al., 2016), R-NET (Wang et al., 2017), and QANet (Yu et al., 2018). According to the leader board of SQuAD, the state-of-the-art MRC models have achieved the same performance as human beings. However, does this imply that they have possessed the same reading comprehen- sion ability as human beings? OF COURSE NOT. There is a huge gap between MRC models and human beings, which is mainly re\ufb02ected in the hunger for data and the robust- ness to noise. On the one hand, developing MRC models requires a large amount of training exam- ples (i.e. the passage-question pairs labeled with answer spans), while human beings can achieve good performance on evaluation examples (i.e. the passage-question pairs to address) without training examples. On the other hand, Jia and Liang (2017) revealed that intentionally injected noise (e.g.",
  "the passage-question pairs labeled with answer spans), while human beings can achieve good performance on evaluation examples (i.e. the passage-question pairs to address) without training examples. On the other hand, Jia and Liang (2017) revealed that intentionally injected noise (e.g. mis- leading sentences) in evaluation examples causes the performance of MRC models to drop signif- icantly, while human beings are far less likely to suffer from this. The reason for these phenomena, we believe, is that MRC models can only utilize the knowledge contained in each given passage- question pair, but in addition to this, human beings can also utilize general knowledge. A typical cate- gory of general knowledge is inter-word semantic connections. As shown in Table 1, such general knowledge is essential to the reading comprehen- sion ability of human beings. A promising strategy to bridge the gap mentioned above is to integrate the neural networks of MRC models with the general knowledge of human be- ings. To this end, it is necessary to solve two prob- lems: extracting general knowledge from passage- question pairs and utilizing the extracted gen- eral knowledge in the prediction of answer spans.",
  "To this end, it is necessary to solve two prob- lems: extracting general knowledge from passage- question pairs and utilizing the extracted gen- eral knowledge in the prediction of answer spans. The \ufb01rst problem can be solved with knowledge bases, which store general knowledge in struc- tured forms. A broad variety of knowledge bases are available, such as WordNet (Fellbaum, 1998) storing semantic knowledge, ConceptNet (Speer et al., 2017) storing commonsense knowledge, and arXiv:1809.03449v3  [cs.AI]  20 May 2019",
  "Passage Question Answer Teachers may use a lesson plan to facilitate student learning, providing a course of study which is called the curriculum. What can a teacher use to help students learn? lesson plan Manufacturing accounts for a signi\ufb01cant but declin- ing share of employment, although the city\u2019s gar- ment industry is showing a resurgence in Brooklyn. In what borough is the gar- ment business prominent? Brooklyn Table 1: Two examples about the importance of inter-word semantic connections to the reading comprehension ability of human beings: in the \ufb01rst one, we can \ufb01nd the answer because we know \u201cfacilitate\u201d is a synonym of \u201chelp\u201d; in the second one, we can \ufb01nd the answer because we know \u201cBrooklyn\u201d is a hyponym of \u201cborough\u201d. Freebase (Bollacker et al., 2008) storing factoid knowledge. In this paper, we limit the scope of general knowledge to inter-word semantic con- nections, and thus use WordNet as our knowl- edge base.",
  "Freebase (Bollacker et al., 2008) storing factoid knowledge. In this paper, we limit the scope of general knowledge to inter-word semantic con- nections, and thus use WordNet as our knowl- edge base. The existing way to solve the second problem is to encode general knowledge in vector space so that the encoding results can be used to enhance the lexical or contextual representations of words (Weissenborn et al., 2017; Mihaylov and Frank, 2018). However, this is an implicit way to utilize general knowledge, since in this way we can neither understand nor control the functioning of general knowledge. In this paper, we discard the existing implicit way and instead explore an explicit (i.e. understandable and controllable) way to utilize general knowledge. The contribution of this paper is two-fold. On the one hand, we propose a data enrichment method, which uses WordNet to extract inter-word seman- tic connections as general knowledge from each given passage-question pair.",
  "understandable and controllable) way to utilize general knowledge. The contribution of this paper is two-fold. On the one hand, we propose a data enrichment method, which uses WordNet to extract inter-word seman- tic connections as general knowledge from each given passage-question pair. On the other hand, we propose an end-to-end MRC model named as Knowledge Aided Reader (KAR), which explic- itly uses the above extracted general knowledge to assist its attention mechanisms. Based on the data enrichment method, KAR is comparable in per- formance with the state-of-the-art MRC models, and signi\ufb01cantly more robust to noise than them. When only a subset (20%\u201380%) of the training ex- amples are available, KAR outperforms the state- of-the-art MRC models by a large margin, and is still reasonably robust to noise. 2 Data Enrichment Method In this section, we elaborate a WordNet-based data enrichment method, which is aimed at extract- ing inter-word semantic connections from each passage-question pair in our MRC dataset.",
  "2 Data Enrichment Method In this section, we elaborate a WordNet-based data enrichment method, which is aimed at extract- ing inter-word semantic connections from each passage-question pair in our MRC dataset. The extraction is performed in a controllable manner, and the extracted results are provided as general knowledge to our MRC model. 2.1 Semantic Relation Chain WordNet is a lexical database of English, where words are organized into synsets according to their senses. A synset is a set of words expressing the same sense so that a word having multiple senses belongs to multiple synsets, with each synset cor- responding to a sense. Synsets are further related to each other through semantic relations. Accord- ing to the WordNet interface provided by NLTK (Bird and Loper, 2004), there are totally sixteen types of semantic relations (e.g. hypernyms, hy- ponyms, holonyms, meronyms, attributes, etc.). Based on synset and semantic relation, we de\ufb01ne a new concept: semantic relation chain. A semantic relation chain is a concatenated sequence of se- mantic relations, which links a synset to another synset.",
  "Based on synset and semantic relation, we de\ufb01ne a new concept: semantic relation chain. A semantic relation chain is a concatenated sequence of se- mantic relations, which links a synset to another synset. For example, the synset \u201ckeratin.n.01\u201d is related to the synset \u201cfeather.n.01\u201d through the semantic relation \u201csubstance holonym\u201d, the synset \u201cfeather.n.01\u201d is related to the synset \u201cbird.n.01\u201d through the semantic relation \u201cpart holonym\u201d, and the synset \u201cbird.n.01\u201d is related to the synset \u201cparrot.n.01\u201d through the semantic relation \u201chyponym\u201d, thus \u201csubstance holonym \u2192 part holonym \u2192hyponym\u201d is a semantic relation chain, which links the synset \u201ckeratin.n.01\u201d to the synset \u201cparrot.n.01\u201d. We name each semantic re- lation in a semantic relation chain as a hop, there- fore the above semantic relation chain is a 3-hop chain. By the way, each single semantic relation is equivalent to a 1-hop chain.",
  "We name each semantic re- lation in a semantic relation chain as a hop, there- fore the above semantic relation chain is a 3-hop chain. By the way, each single semantic relation is equivalent to a 1-hop chain. 2.2 Inter-word Semantic Connection The key problem in the data enrichment method is determining whether a word is semantically con- nected to another word. If so, we say that there exists an inter-word semantic connection between",
  "them. To solve this problem, we de\ufb01ne another new concept: the extended synsets of a word. Given a word w, whose synsets are represented as a set Sw, we use another set S\u2217 w to represent its extended synsets, which includes all the synsets that are in Sw or that can be linked to from Sw through semantic relation chains. Theoretically, if there is no limitation on semantic relation chains, S\u2217 w will include all the synsets in WordNet, which is meaningless in most situations. Therefore, we use a hyper-parameter \u03ba \u2208N to represent the per- mitted maximum hop count of semantic relation chains. That is to say, only the chains having no more than \u03ba hops can be used to construct S\u2217 w so that S\u2217 w becomes a function of \u03ba: S\u2217 w(\u03ba) (if \u03ba = 0, we will have S\u2217 w(0) = Sw).",
  "That is to say, only the chains having no more than \u03ba hops can be used to construct S\u2217 w so that S\u2217 w becomes a function of \u03ba: S\u2217 w(\u03ba) (if \u03ba = 0, we will have S\u2217 w(0) = Sw). Based on the above statements, we formulate a heuristic rule for deter- mining inter-word semantic connections: a word w1 is semantically connected to another word w2 if and only if S\u2217 w1(\u03ba) \u2229Sw2 \u0338= \u2205. 2.3 General Knowledge Extraction Given a passage-question pair, the inter-word se- mantic connections that connect any word to any passage word are regarded as the general knowl- edge we need to extract. Considering the require- ments of our MRC model, we only extract the positional information of such inter-word seman- tic connections. Speci\ufb01cally, for each word w, we extract a set Ew, which includes the positions of the passage words that w is semantically con- nected to (if w itself is a passage word, we will exclude its own position from Ew).",
  "Speci\ufb01cally, for each word w, we extract a set Ew, which includes the positions of the passage words that w is semantically con- nected to (if w itself is a passage word, we will exclude its own position from Ew). We can con- trol the amount of the extracted results by setting the hyper-parameter \u03ba: if we set \u03ba to 0, inter-word semantic connections will only exist between syn- onyms; if we increase \u03ba, inter-word semantic con- nections will exist between more words. That is to say, by increasing \u03ba within a certain range, we can usually extract more inter-word semantic connec- tions from a passage-question pair, and thus can provide the MRC model with more general knowl- edge. However, due to the complexity and diver- sity of natural languages, only a part of the ex- tracted results can serve as useful general knowl- edge, while the rest of them are useless for the prediction of answer spans, and the proportion of the useless part always rises when \u03ba is set larger. Therefore we set \u03ba through cross validation (i.e.",
  "Therefore we set \u03ba through cross validation (i.e. according to the performance of the MRC model on the development examples). 3 Knowledge Aided Reader In this section, we elaborate our MRC model: Knowledge Aided Reader (KAR). The key com- ponents of most existing MRC models are their attention mechanisms (Bahdanau et al., 2014), which are aimed at fusing the associated represen- tations of each given passage-question pair. These attention mechanisms generally fall into two cat- egories: the \ufb01rst one, which we name as mutual attention, is aimed at fusing the question repre- sentations into the passage representations so as to obtain the question-aware passage representa- tions; the second one, which we name as self at- tention, is aimed at fusing the question-aware pas- sage representations into themselves so as to ob- tain the \ufb01nal passage representations. Although KAR is equipped with both categories, its most re- markable feature is that it explicitly uses the gen- eral knowledge extracted by the data enrichment method to assist its attention mechanisms.",
  "Although KAR is equipped with both categories, its most re- markable feature is that it explicitly uses the gen- eral knowledge extracted by the data enrichment method to assist its attention mechanisms. There- fore we separately name the attention mechanisms of KAR as knowledge aided mutual attention and knowledge aided self attention. 3.1 Task De\ufb01nition Given a passage P = {p1, . . . , pn} and a relevant question Q = {q1, . . . , qm}, the task is to predict an answer span [as, ae], where 1 \u2264as \u2264ae \u2264n, so that the resulting subsequence {pas, . . . , pae} from P is an answer to Q. 3.2 Overall Architecture As shown in Figure 1, KAR is an end-to-end MRC model consisting of \ufb01ve layers: Lexicon Embedding Layer. This layer maps the words to the lexicon embeddings. The lexicon em- bedding of each word is composed of its word em- bedding and character embedding.",
  "This layer maps the words to the lexicon embeddings. The lexicon em- bedding of each word is composed of its word em- bedding and character embedding. For each word, we use the pre-trained GloVe (Pennington et al., 2014) word vector as its word embedding, and ob- tain its character embedding with a Convolutional Neural Network (CNN) (Kim, 2014). For both the passage and the question, we pass the con- catenation of the word embeddings and the charac- ter embeddings through a shared dense layer with ReLU activation, whose output dimensionality is d. Therefore we obtain the passage lexicon em- beddings LP \u2208Rd\u00d7n and the question lexicon embeddings LQ \u2208Rd\u00d7m. Context Embedding Layer. This layer maps the lexicon embeddings to the context embeddings.",
  "Figure 1: An end-to-end MRC model: Knowledge Aided Reader (KAR) For both the passage and the question, we process the lexicon embeddings (i.e. LP for the passage and LQ for the question) with a shared bidirec- tional LSTM (BiLSTM) (Hochreiter and Schmid- huber, 1997), whose hidden state dimensionality is 1 2d. By concatenating the forward LSTM out- puts and the backward LSTM outputs, we obtain the passage context embeddings CP \u2208Rd\u00d7n and the question context embeddings CQ \u2208Rd\u00d7m. Coarse Memory Layer. This layer maps the con- text embeddings to the coarse memories. First we use knowledge aided mutual attention (introduced later) to fuse CQ into CP , the outputs of which are represented as \u02dcG \u2208Rd\u00d7n. Then we process \u02dcG with a BiLSTM, whose hidden state dimension- ality is 1 2d.",
  "Then we process \u02dcG with a BiLSTM, whose hidden state dimension- ality is 1 2d. By concatenating the forward LSTM outputs and the backward LSTM outputs, we ob- tain the coarse memories G \u2208Rd\u00d7n, which are the question-aware passage representations. Re\ufb01ned Memory Layer. This layer maps the coarse memories to the re\ufb01ned memories. First we use knowledge aided self attention (introduced later) to fuse G into themselves, the outputs of which are represented as \u02dcH \u2208Rd\u00d7n. Then we process \u02dcH with a BiLSTM, whose hidden state di- mensionality is 1 2d. By concatenating the forward LSTM outputs and the backward LSTM outputs, we obtain the re\ufb01ned memories H \u2208Rd\u00d7n, which are the \ufb01nal passage representations. Answer Span Prediction Layer. This layer pre- dicts the answer start position and the answer end position based on the above layers. First we obtain the answer start position distribution os: ti = v\u22a4 s tanh(Wshpi + UsrQ) \u2208R os = softmax({t1, .",
  "This layer pre- dicts the answer start position and the answer end position based on the above layers. First we obtain the answer start position distribution os: ti = v\u22a4 s tanh(Wshpi + UsrQ) \u2208R os = softmax({t1, . . . , tn}) \u2208Rn where vs, Ws, and Us are trainable parameters; hpi represents the re\ufb01ned memory of each passage word pi (i.e. the i-th column in H); rQ represents the question summary obtained by performing an attention pooling over CQ. Then we obtain the an- swer end position distribution oe: ti = v\u22a4 e tanh(Wehpi + Ue[rQ; Hos]) \u2208R oe = softmax({t1, . . . , tn}) \u2208Rn where ve, We, and Ue are trainable parameters; [; ] represents vector concatenation.",
  ". . , tn}) \u2208Rn where ve, We, and Ue are trainable parameters; [; ] represents vector concatenation. Finally we construct an answer span prediction matrix O = uptri(oso\u22a4 e ) \u2208Rn\u00d7n, where uptri(X) represents the upper triangular matrix of a matrix X. There- fore, for the training, we minimize \u2212log(Oas,ae) on each training example whose labeled answer span is [as, ae]; for the inference, we separately take the row index and column index of the maxi- mum element in O as as and ae. 3.3 Knowledge Aided Mutual Attention As a part of the coarse memory layer, knowledge aided mutual attention is aimed at fusing the ques-",
  "tion context embeddings CQ into the passage con- text embeddings CP , where the key problem is to calculate the similarity between each passage con- text embedding cpi (i.e. the i-th column in CP ) and each question context embedding cqj (i.e. the j-th column in CQ). To solve this problem, Seo et al. (2016) proposed a similarity function: f(cpi, cqj) = v\u22a4 f [cpi; cqj; cpi \u2299cqj] \u2208R where vf is a trainable parameter; \u2299represents element-wise multiplication. This similarity func- tion has also been adopted by several other works (Clark and Gardner, 2017; Yu et al., 2018). How- ever, since context embeddings contain high-level information, we believe that introducing the pre- extracted general knowledge into the calculation of such similarities will make the results more rea- sonable.",
  "How- ever, since context embeddings contain high-level information, we believe that introducing the pre- extracted general knowledge into the calculation of such similarities will make the results more rea- sonable. Therefore we modify the above similarity function to the following form: f\u2217(cpi, cqj) = v\u22a4 f [c\u2217 pi; c\u2217 qj; c\u2217 pi \u2299c\u2217 qj] \u2208R where c\u2217 x represents the enhanced context embed- ding of a word x. We use the pre-extracted gen- eral knowledge to construct the enhanced context embeddings. Speci\ufb01cally, for each word w, whose context embedding is cw, to construct its enhanced context embedding c\u2217 w, \ufb01rst recall that we have extracted a set Ew, which includes the positions of the passage words that w is semantically con- nected to, thus by gathering the columns in CP whose indexes are given by Ew, we obtain the matching context embeddings Z \u2208Rd\u00d7|Ew|.",
  "Then by constructing a cw-attended summary of Z, we obtain the matching vector c+ w (if Ew = \u2205, which makes Z = {}, we will set c+ w = 0): ti = v\u22a4 c tanh(Wczi + Uccw) \u2208R c+ w = Z softmax({t1, . . . , t|Ew|}) \u2208Rd where vc, Wc, and Uc are trainable parameters; zi represents the i-th column in Z. Finally we pass the concatenation of cw and c+ w through a dense layer with ReLU activation, whose output dimen- sionality is d. Therefore we obtain the enhanced context embedding c\u2217 w \u2208Rd. Based on the modi\ufb01ed similarity function and the enhanced context embeddings, to perform knowledge aided mutual attention, \ufb01rst we con- struct a knowledge aided similarity matrix A \u2208 Rn\u00d7m, where each element Ai,j = f\u2217(cpi, cqj). Then following Yu et al.",
  "Then following Yu et al. (2018), we construct the passage-attended question summaries RQ and the question-attended passage summaries RP : RQ = CQ softmax\u22a4 r (A) \u2208Rd\u00d7n RP = CP softmaxc(A) softmax\u22a4 r (A) \u2208Rd\u00d7n where softmaxr represents softmax along the row dimension and softmaxc along the column dimen- sion. Finally following Clark and Gardner (2017), we pass the concatenation of CP , RQ, CP \u2299RQ, and RP \u2299RQ through a dense layer with ReLU ac- tivation, whose output dimensionality is d. There- fore we obtain the outputs \u02dcG \u2208Rd\u00d7n. 3.4 Knowledge Aided Self Attention As a part of the re\ufb01ned memory layer, knowledge aided self attention is aimed at fusing the coarse memories G into themselves.",
  "3.4 Knowledge Aided Self Attention As a part of the re\ufb01ned memory layer, knowledge aided self attention is aimed at fusing the coarse memories G into themselves. If we simply fol- low the self attentions of other works (Wang et al., 2017; Huang et al., 2017; Liu et al., 2017b; Clark and Gardner, 2017), then for each passage word pi, we should fuse its coarse memory gpi (i.e. the i-th column in G) with the coarse memories of all the other passage words. However, we believe that this is both unnecessary and distracting, since each passage word has nothing to do with many of the other passage words. Thus we use the pre- extracted general knowledge to guarantee that the fusion of coarse memories for each passage word will only involve a precise subset of the other pas- sage words.",
  "Thus we use the pre- extracted general knowledge to guarantee that the fusion of coarse memories for each passage word will only involve a precise subset of the other pas- sage words. Speci\ufb01cally, for each passage word pi, whose coarse memory is gpi, to perform the fu- sion of coarse memories, \ufb01rst recall that we have extracted a set Epi, which includes the positions of the other passage words that pi is semantically connected to, thus by gathering the columns in G whose indexes are given by Epi, we obtain the matching coarse memories Z \u2208Rd\u00d7|Epi|. Then by constructing a gpi-attended summary of Z, we obtain the matching vector g+ pi (if Epi = \u2205, which makes Z = {}, we will set g+ pi = 0): ti = v\u22a4 g tanh(Wgzi + Uggpi) \u2208R g+ pi = Z softmax({t1, . . . , t|Epi|}) \u2208Rd where vg, Wg, and Ug are trainable parameters.",
  ". . , t|Epi|}) \u2208Rd where vg, Wg, and Ug are trainable parameters. Finally we pass the concatenation of gpi and g+ pi through a dense layer with ReLU activation, whose output dimensionality is d. Therefore we obtain the fusion result \u02dchpi \u2208Rd, and further the outputs \u02dcH = {\u02dchp1, . . . , \u02dchpn} \u2208Rd\u00d7n.",
  "4 Related Works Attention Mechanisms. Besides those mentioned above, other interesting attention mechanisms in- clude performing multi-round alignment to avoid the problems of attention redundancy and atten- tion de\ufb01ciency (Hu et al., 2017), and using mutual attention as a skip-connector to densely connect pairwise layers (Tay et al., 2018). Data Augmentation. It is proved that properly augmenting training examples can improve the performance of MRC models. For example, Yang et al. (2017) trained a generative model to generate questions based on unlabeled text, which substan- tially boosted their performance; Yu et al. (2018) trained a back-and-forth translation model to para- phrase training examples, which brought them a signi\ufb01cant performance gain. Multi-step Reasoning. Inspired by the fact that human beings are capable of understanding com- plex documents by reading them over and over again, multi-step reasoning was proposed to bet- ter deal with dif\ufb01cult MRC tasks. For example, Shen et al. (2017) used reinforcement learning to dynamically determine the number of reasoning steps; Liu et al.",
  "For example, Shen et al. (2017) used reinforcement learning to dynamically determine the number of reasoning steps; Liu et al. (2017b) \ufb01xed the number of rea- soning steps, but used stochastic dropout in the output layer to avoid step bias. Linguistic Embeddings. It is both easy and effec- tive to incorporate linguistic embeddings into the input layer of MRC models. For example, Chen et al. (2017) and Liu et al. (2017b) used POS em- beddings and NER embeddings to construct their input embeddings; Liu et al. (2017a) used struc- tural embeddings based on parsing trees to con- structed their input embeddings. Transfer Learning.",
  "(2017b) used POS em- beddings and NER embeddings to construct their input embeddings; Liu et al. (2017a) used struc- tural embeddings based on parsing trees to con- structed their input embeddings. Transfer Learning. Several recent breakthroughs in MRC bene\ufb01t from feature-based transfer learn- ing (McCann et al., 2017; Peters et al., 2018) and \ufb01ne-tuning-based transfer learning (Radford et al., 2018; Devlin et al., 2018), which are based on certain word-level or sentence-level models pre- trained on large external corpora in certain super- vised or unsupervised manners. 5 Experiments 5.1 Experimental Settings MRC Dataset. The MRC dataset used in this pa- per is SQuAD 1.1, which contains over 100, 000 passage-question pairs and has been randomly par- titioned into three parts: a training set (80%), a development set (10%), and a test set (10%).",
  "The MRC dataset used in this pa- per is SQuAD 1.1, which contains over 100, 000 passage-question pairs and has been randomly par- titioned into three parts: a training set (80%), a development set (10%), and a test set (10%). Besides, we also use two of its adversarial sets, namely AddSent and AddOneSent (Jia and Liang, 2017), to evaluate the robustness to noise of MRC models. The passages in the adversarial sets con- tain misleading sentences, which are aimed at dis- tracting MRC models. Speci\ufb01cally, each passage in AddSent contains several sentences that are similar to the question but not contradictory to the answer, while each passage in AddOneSent con- tains a human-approved random sentence that may be unrelated to the passage. Implementation Details.",
  "Speci\ufb01cally, each passage in AddSent contains several sentences that are similar to the question but not contradictory to the answer, while each passage in AddOneSent con- tains a human-approved random sentence that may be unrelated to the passage. Implementation Details. We tokenize the MRC dataset with spaCy 2.0.13 (Honnibal and Mon- tani, 2017), manipulate WordNet 3.0 with NLTK 3.3, and implement KAR with TensorFlow 1.11.0 (Abadi et al., 2016). For the data enrichment method, we set the hyper-parameter \u03ba to 3. For the dense layers and the BiLSTMs, we set the dimen- sionality unit d to 600. For model optimization, we apply the Adam (Kingma and Ba, 2014) opti- mizer with a learning rate of 0.0005 and a mini- batch size of 32. For model evaluation, we use Exact Match (EM) and F1 score as evaluation met- rics.",
  "For model evaluation, we use Exact Match (EM) and F1 score as evaluation met- rics. To avoid over\ufb01tting, we apply dropout (Sri- vastava et al., 2014) to the dense layers and the BiLSTMs with a dropout rate of 0.3. To boost the performance, we apply exponential moving aver- age with a decay rate of 0.999. 5.2 Model Comparison in both Performance and the Robustness to Noise We compare KAR with other MRC models in both performance and the robustness to noise. Specif- ically, we not only evaluate the performance of KAR on the development set and the test set, but also do this on the adversarial sets. As for the com- parative objects, we only consider the single MRC models that rank in the top 20 on the SQuAD 1.1 leader board and have reported their performance on the adversarial sets. There are totally \ufb01ve such comparative objects, which can be considered as representatives of the state-of-the-art MRC mod- els.",
  "There are totally \ufb01ve such comparative objects, which can be considered as representatives of the state-of-the-art MRC mod- els. As shown in Table 2, on the development set and the test set, the performance of KAR is on par with that of the state-of-the-art MRC models; on the adversarial sets, KAR outperforms the state- of-the-art MRC models by a large margin. That is to say, KAR is comparable in performance with the state-of-the-art MRC models, and signi\ufb01cantly more robust to noise than them.",
  "Single MRC model Dev set (EM / F1) Test set (EM / F1) AddSent (F1) AddOneSent (F1) FusionNet (Huang et al., 2017) 75.3 / 83.6 76.0 / 83.9 51.4 60.7 RaSoR+TR+LM (Salant and Be- rant, 2017) 77.0 / 84.0 77.6 / 84.2 47.0 57.0 SAN (Liu et al., 2017b) 76.2 / 84.1 76.8 / 84.4 46.6 56.5 R.M-Reader (Hu et al., 2017) 78.9 / 86.3 79.5 / 86.6 58.5 67.0 QANet (with data augmentation) (Yu et al., 2018) 75.1 / 83.8 82.5 / 89.3 45.2 55.7 KAR (ours) 76.7 / 84.",
  "5 67.0 QANet (with data augmentation) (Yu et al., 2018) 75.1 / 83.8 82.5 / 89.3 45.2 55.7 KAR (ours) 76.7 / 84.9 76.1 / 83.5 60.1 72.3 Table 2: Model comparison based on SQuAD 1.1 and two of its adversarial sets: AddSent and AddOneSent. All the numbers are up to date as of October 18, 2018. Note that SQuAD 2.0 (Rajpurkar et al., 2018) is not involved in this paper, because it requires MRC models to deal with the problem of answer triggering, but this paper is aimed at improving the hunger for data and robustness to noise of MRC models. To verify the effectiveness of general knowledge, we \ufb01rst study the relationship between the amount of general knowledge and the performance of KAR.",
  "To verify the effectiveness of general knowledge, we \ufb01rst study the relationship between the amount of general knowledge and the performance of KAR. As shown in Table 3, by increasing \u03ba from 0 to 5 in the data enrichment method, the amount of general knowledge rises monotonically, but the performance of KAR \ufb01rst rises until \u03ba reaches 3 and then drops down. Then we conduct an ablation study by replacing the knowledge aided attention mechanisms with the mutual attention proposed by Seo et al. (2016) and the self attention proposed by Wang et al. (2017) separately, and \ufb01nd that the F1 score of KAR drops by 4.2 on the development set, 7.8 on AddSent, and 9.1 on AddOneSent.",
  "(2016) and the self attention proposed by Wang et al. (2017) separately, and \ufb01nd that the F1 score of KAR drops by 4.2 on the development set, 7.8 on AddSent, and 9.1 on AddOneSent. Fi- nally we \ufb01nd that after only one epoch of train- ing, KAR already achieves an EM of 71.9 and an F1 score of 80.8 on the development set, which is even better than the \ufb01nal performance of several strong baselines, such as DCN (EM / F1: 65.4 / 75.6) (Xiong et al., 2016) and BiDAF (EM / F1: 67.7 / 77.3) (Seo et al., 2016). The above empiri- cal \ufb01ndings imply that general knowledge indeed plays an effective role in KAR. To demonstrate the advantage of our explicit way to utilize general knowledge over the existing im- plicit way, we compare the performance of KAR with that reported by Weissenborn et al.",
  "The above empiri- cal \ufb01ndings imply that general knowledge indeed plays an effective role in KAR. To demonstrate the advantage of our explicit way to utilize general knowledge over the existing im- plicit way, we compare the performance of KAR with that reported by Weissenborn et al. (2017), which used an encoding-based method to utilize the general knowledge dynamically retrieved from Wikipedia and ConceptNet. Since their best model only achieved an EM of 69.5 and an F1 score of 79.7 on the development set, which is much lower than the performance of KAR, we have good rea- son to believe that our explicit way works better than the existing implicit way.",
  "Since their best model only achieved an EM of 69.5 and an F1 score of 79.7 on the development set, which is much lower than the performance of KAR, we have good rea- son to believe that our explicit way works better than the existing implicit way. \u03ba Average number of inter- word semantic connections per word Dev set (EM / F1) 0 0.39 74.2 / 82.8 1 0.63 74.6 / 83.1 2 1.24 75.1 / 83.5 3 2.21 76.7 / 84.9 4 3.68 75.9 / 84.3 5 5.58 75.3 / 83.8 Table 3: With \u03ba set to different values in the data en- richment method, we calculate the average number of inter-word semantic connections per word as an estima- tion of the amount of general knowledge, and evaluate the performance of KAR on the development set. 5.3 Model Comparison in the Hunger for Data We compare KAR with other MRC models in the hunger for data.",
  "5.3 Model Comparison in the Hunger for Data We compare KAR with other MRC models in the hunger for data. Speci\ufb01cally, instead of using all the training examples, we produce several train- ing subsets (i.e. subsets of the training examples) so as to study the relationship between the pro- portion of the available training examples and the performance. We produce each training subset by sampling a speci\ufb01c number of questions from all the questions relevant to each passage. By sepa- rately sampling 1, 2, 3, and 4 questions on each passage, we obtain four training subsets, which separately contain 20%, 40%, 60%, and 80% of the training examples. As shown in Figure 2, with KAR, SAN (re-implemented), and QANet (re- implemented without data augmentation) trained on these training subsets, we evaluate their perfor- mance on the development set, and \ufb01nd that KAR",
  "Figure 2: With KAR, SAN, and QANet (without data augmentation) trained on the training subsets, we eval- uate their performance on the development set. Figure 3: With KAR, SAN, and QANet (without data augmentation) trained on the training subsets, we eval- uate their performance on AddSent. performs much better than SAN and QANet. As shown in Figure 3 and Figure 4, with the above KAR, SAN, and QANet trained on the same train- ing subsets, we also evaluate their performance on the adversarial sets, and still \ufb01nd that KAR per- forms much better than SAN and QANet. That is to say, when only a subset of the training exam- ples are available, KAR outperforms the state-of- the-art MRC models by a large margin, and is still reasonably robust to noise.",
  "That is to say, when only a subset of the training exam- ples are available, KAR outperforms the state-of- the-art MRC models by a large margin, and is still reasonably robust to noise. 6 Analysis According to the experimental results, KAR is not only comparable in performance with the state-of- the-art MRC models, but also superior to them in terms of both the hunger for data and the robust- Figure 4: With KAR, SAN, and QANet (without data augmentation) trained on the training subsets, we eval- uate their performance on AddOneSent. ness to noise. The reasons for these achievements, we believe, are as follows: \u2022 KAR is designed to utilize the pre-extracted inter-word semantic connections from the data enrichment method. Some inter-word semantic connections, especially those ob- tained through multi-hop semantic relation chains, are very helpful for the prediction of answer spans, but they will be too covert to capture if we simply leverage recurrent neu- ral networks (e.g. BiLSTM) and pre-trained word vectors (e.g. GloVe).",
  "BiLSTM) and pre-trained word vectors (e.g. GloVe). \u2022 An inter-word semantic connection extracted from a passage-question pair usually also ap- pears in many other passage-question pairs, therefore it is very likely that the inter-word semantic connections extracted from a small amount of training examples actually cover a much larger amount of training examples. That is to say, we are actually using much more training examples for model optimiza- tion than the available ones. \u2022 Some inter-word semantic connections are distracting for the prediction of answer spans. For example, the inter-word semantic con- nection between \u201cbank\u201d and \u201cwaterside\u201d makes no sense given the context \u201cthe bank manager is walking along the waterside\u201d. It is the knowledge aided attention mechanisms that enable KAR to ignore such distracting inter-word semantic connections so that only the important ones are used.",
  "7 Conclusion In this paper, we innovatively integrate the neural networks of MRC models with the general knowl- edge of human beings. Speci\ufb01cally, inter-word se- mantic connections are \ufb01rst extracted from each given passage-question pair by a WordNet-based data enrichment method, and then provided as general knowledge to an end-to-end MRC model named as Knowledge Aided Reader (KAR), which explicitly uses the general knowledge to assist its attention mechanisms. Experimental results show that KAR is not only comparable in performance with the state-of-the-art MRC models, but also su- perior to them in terms of both the hunger for data and the robustness to noise. In the future, we plan to use some larger knowledge bases, such as Con- ceptNet and Freebase, to improve the quality and scope of the general knowledge. Acknowledgments This work is partially supported by a research do- nation from iFLYTEK Co., Ltd., Hefei, China, and a discovery grant from Natural Sciences and Engi- neering Research Council (NSERC) of Canada.",
  "Acknowledgments This work is partially supported by a research do- nation from iFLYTEK Co., Ltd., Hefei, China, and a discovery grant from Natural Sciences and Engi- neering Research Council (NSERC) of Canada. References Mart\u00b4\u0131n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. 2016. Tensor\ufb02ow: A system for large-scale machine learning. In OSDI, volume 16, pages 265\u2013 283. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473. Steven Bird and Edward Loper. 2004. Nltk: the nat- ural language toolkit. In Proceedings of the ACL 2004 on Interactive poster and demonstration ses- sions, page 31. Association for Computational Lin- guistics.",
  "Steven Bird and Edward Loper. 2004. Nltk: the nat- ural language toolkit. In Proceedings of the ACL 2004 on Interactive poster and demonstration ses- sions, page 31. Association for Computational Lin- guistics. Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collab- oratively created graph database for structuring hu- man knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247\u20131250. ACM. Danqi Chen, Adam Fisch, Jason Weston, and An- toine Bordes. 2017. Reading wikipedia to an- swer open-domain questions. arXiv preprint arXiv:1704.00051. Christopher Clark and Matt Gardner. 2017. Simple and effective multi-paragraph reading comprehen- sion. arXiv preprint arXiv:1710.10723. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.",
  "2017. Simple and effective multi-paragraph reading comprehen- sion. arXiv preprint arXiv:1710.10723. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805. Christiane Fellbaum. 1998. WordNet. Wiley Online Library. Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735\u20131780. Matthew Honnibal and Ines Montani. 2017. spacy 2: Natural language understanding with bloom embed- dings, convolutional neural networks and incremen- tal parsing. To appear. Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, and Ming Zhou. 2017. Reinforced mnemonic reader for machine reading comprehen- sion.",
  "To appear. Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, and Ming Zhou. 2017. Reinforced mnemonic reader for machine reading comprehen- sion. arXiv preprint arXiv:1705.02798. Hsin-Yuan Huang, Chenguang Zhu, Yelong Shen, and Weizhu Chen. 2017. Fusionnet: Fusing via fully- aware attention with application to machine compre- hension. arXiv preprint arXiv:1711.07341. Robin Jia and Percy Liang. 2017. Adversarial exam- ples for evaluating reading comprehension systems. arXiv preprint arXiv:1707.07328. Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehen- sion. arXiv preprint arXiv:1705.03551. Yoon Kim. 2014.",
  "2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehen- sion. arXiv preprint arXiv:1705.03551. Yoon Kim. 2014. Convolutional neural net- works for sentence classi\ufb01cation. arXiv preprint arXiv:1408.5882. Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Rui Liu, Junjie Hu, Wei Wei, Zi Yang, and Eric Ny- berg. 2017a. Structural embedding of syntactic trees for machine comprehension. arXiv preprint arXiv:1703.00572. Xiaodong Liu, Yelong Shen, Kevin Duh, and Jian- feng Gao. 2017b. Stochastic answer networks for machine reading comprehension. arXiv preprint arXiv:1712.03556. Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017.",
  "2017b. Stochastic answer networks for machine reading comprehension. arXiv preprint arXiv:1712.03556. Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Con- textualized word vectors. In Advances in Neural In- formation Processing Systems, pages 6294\u20136305. Todor Mihaylov and Anette Frank. 2018. Knowledge- able reader: Enhancing cloze-style reading com- prehension with external commonsense knowledge. arXiv preprint arXiv:1805.07858.",
  "Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. Ms marco: A human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 confer- ence on empirical methods in natural language pro- cessing (EMNLP), pages 1532\u20131543. Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. arXiv preprint arXiv:1802.05365. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing by generative pre-training. URL https://s3- us-west-2. amazonaws.",
  "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing by generative pre-training. URL https://s3- us-west-2. amazonaws. com/openai-assets/research- covers/languageunsupervised/language under- standing paper. pdf. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don\u2019t know: Unanswerable ques- tions for squad. arXiv preprint arXiv:1806.03822. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250. Shimi Salant and Jonathan Berant. 2017. Contextu- alized word representations for reading comprehen- sion. arXiv preprint arXiv:1712.03609.",
  "arXiv preprint arXiv:1606.05250. Shimi Salant and Jonathan Berant. 2017. Contextu- alized word representations for reading comprehen- sion. arXiv preprint arXiv:1712.03609. Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Bidirectional attention \ufb02ow for machine comprehension. arXiv preprint arXiv:1611.01603. Yelong Shen, Po-Sen Huang, Jianfeng Gao, and Weizhu Chen. 2017. Reasonet: Learning to stop reading in machine comprehension. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1047\u20131055. ACM. Robert Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Thirty-First AAAI Confer- ence on Arti\ufb01cial Intelligence.",
  "ACM. Robert Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Thirty-First AAAI Confer- ence on Arti\ufb01cial Intelligence. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from over\ufb01tting. The Journal of Machine Learning Research, 15(1):1929\u20131958. Yi Tay, Anh Tuan Luu, Siu Cheung Hui, and Jian Su. 2018. Densely connected attention propagation for reading comprehension. In Advances in Neural In- formation Processing Systems, pages 4906\u20134917. Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and Ming Zhou. 2017. Gated self-matching net- works for reading comprehension and question an- swering.",
  "In Advances in Neural In- formation Processing Systems, pages 4906\u20134917. Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and Ming Zhou. 2017. Gated self-matching net- works for reading comprehension and question an- swering. In Proceedings of the 55th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 189\u2013198. Dirk Weissenborn, Tom\u00b4a\u02c7s Ko\u02c7cisk`y, and Chris Dyer. 2017. Dynamic integration of background knowl- edge in neural nlu systems. arXiv preprint arXiv:1706.02596. Caiming Xiong, Victor Zhong, and Richard Socher. 2016. Dynamic coattention networks for question answering. arXiv preprint arXiv:1611.01604. Zhilin Yang, Junjie Hu, Ruslan Salakhutdinov, and William W Cohen. 2017. Semi-supervised qa with generative domain-adaptive nets.",
  "arXiv preprint arXiv:1611.01604. Zhilin Yang, Junjie Hu, Ruslan Salakhutdinov, and William W Cohen. 2017. Semi-supervised qa with generative domain-adaptive nets. arXiv preprint arXiv:1702.02206. Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. 2018. Qanet: Combining local convolution with global self-attention for reading comprehen- sion. arXiv preprint arXiv:1804.09541."
]