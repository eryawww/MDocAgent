{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "DENS: A Dataset for Multi-class Emotion Analysis Chen Liu and Muhammad Osama and Anderson de Andrade Wattpad Toronto, ON, Canada cecilia, muhammad.osama, anderson@wattpad.com Abstract We introduce a new dataset for multi-class emotion analysis from long-form narratives in English. The Dataset for Emotions of Nar- rative Sequences (DENS) was collected from both classic literature available on Project Gutenberg and modern online narratives avail- able on Wattpad, annotated using Amazon Mechanical Turk. A number of statistics and baseline benchmarks are provided for the dataset. Of the tested techniques, we \ufb01nd that the \ufb01ne-tuning of a pre-trained BERT model achieves the best results, with an av- erage micro-F1 score of 60.4%. Our results show that the dataset provides a novel opportu- nity in emotion analysis that requires moving beyond existing sentence-level techniques. 1 Introduction Humans experience a variety of complex emotions in daily life. These emotions are heavily re\ufb02ected in our language, in both spoken and written forms.",
            "Our results show that the dataset provides a novel opportu- nity in emotion analysis that requires moving beyond existing sentence-level techniques. 1 Introduction Humans experience a variety of complex emotions in daily life. These emotions are heavily re\ufb02ected in our language, in both spoken and written forms. Many recent advances in natural language pro- cessing on emotions have focused on product re- views (McAuley et al., 2015) and tweets (Mo- hammad et al., 2018; Kant et al., 2018). These datasets are often limited in length (e.g. by the number of words in tweets), purpose (e.g. prod- uct reviews), or emotional spectrum (e.g. binary classi\ufb01cation). Character dialogues and narratives in story- telling usually carry strong emotions. A memo- rable story is often one in which the emotional journey of the characters resonates with the reader. Indeed, emotion is one of the most important as- pects of narratives. In order to characterize narra- tive emotions properly, we must move beyond bi- nary constraints (e.g. good or bad, happy or sad).",
            "Indeed, emotion is one of the most important as- pects of narratives. In order to characterize narra- tive emotions properly, we must move beyond bi- nary constraints (e.g. good or bad, happy or sad). In this paper, we introduce the Dataset for Emo- tions of Narrative Sequences (DENS) for emotion analysis, consisting of passages from long-form \ufb01ctional narratives from both classic literature and modern stories in English. The data samples con- sist of self-contained passages that span several sentences and a variety of subjects. Each sample is annotated by using one of 9 classes and an indi- cator for annotator agreement. 2 Background Using the categorical basic emotion model (Plutchik, 1979), (Mohammad and Kiritchenko, 2015; Mohammad, 2012) studied creating lexicons from tweets for use in emotion analysis. Recently, (Mohammad et al., 2018), (Klinger et al., 2018) and (Kant et al., 2018) proposed shared-tasks for multi-class emotion analysis based on tweets. Fewer works have been reported on understand- ing emotions in narratives.",
            "Recently, (Mohammad et al., 2018), (Klinger et al., 2018) and (Kant et al., 2018) proposed shared-tasks for multi-class emotion analysis based on tweets. Fewer works have been reported on understand- ing emotions in narratives. Emotional Arc (Rea- gan et al., 2016) is one recent advance in this direction. The work used lexicons and unsuper- vised learning methods based on unlabelled pas- sages from titles in Project Gutenberg1. For labelled datasets on narratives, (Alm et al., 2005) provided a sentence-level annotated cor- pus of childrens\u2019 stories and (Kim and Klinger, 2018) provided phrase-level annotations on se- lected Project Gutenberg titles. To the best of our knowledge, the dataset in this work is the \ufb01rst to provide multi-class emo- tion labels on passages, selected from both Project Gutenberg and modern narratives. The dataset is available upon request for non-commercial, re- search only purposes2. 3 Dataset In this section, we describe the process used to col- lect and annotate the dataset.",
            "The dataset is available upon request for non-commercial, re- search only purposes2. 3 Dataset In this section, we describe the process used to col- lect and annotate the dataset. 1https:\/\/www.gutenberg.org\/ 2Please send requests to: academic dataset@wattpad.com arXiv:1910.11769v1  [cs.CL]  25 Oct 2019",
            "3.1 Plutchiks Wheel of Emotions The dataset is annotated based on a modi\ufb01ed Plutchiks wheel of emotions. The original Plutchiks wheel consists of 8 pri- mary emotions: Joy, Sadness, Anger, Fear, Antici- pation, Surprise, Trust, Disgust. In addition, more complex emotions can be formed by combing two basic emotions. For example, Love is de\ufb01ned as a combination of Joy and Trust (Fig. 1). Figure 1: Plutchik\u2019s wheel of emotions (Wikimedia, 2011) The intensity of an emotion is also captured in Plutchik\u2019s wheel. For example, the primary emo- tion of Anger can vary between Annoyance (mild) and Rage (intense). We conducted an initial survey based on 100 stories with a signi\ufb01cant fraction sampled from the romance genre. We asked readers to identify the major emotion exhibited in each story from a choice of the original 8 primary emotions. We found that readers have signi\ufb01cant dif\ufb01culty in identifying Trust as an emotion associated with romantic stories.",
            "We asked readers to identify the major emotion exhibited in each story from a choice of the original 8 primary emotions. We found that readers have signi\ufb01cant dif\ufb01culty in identifying Trust as an emotion associated with romantic stories. Hence, we modi\ufb01ed our annota- tion scheme by removing Trust and adding Love. We also added the Neutral category to denote pas- sages that do not exhibit any emotional content. The \ufb01nal annotation categories for the dataset are: Joy, Sadness, Anger, Fear, Anticipation, Sur- prise, Love, Disgust, Neutral. 3.2 Passage Selection We selected both classic and modern narratives in English for this dataset. The modern narratives were sampled based on popularity from Wattpad. We parsed selected narratives into passages, where a passage is considered to be eligible for annota- tion if it contained between 40 and 200 tokens. In long-form narratives, many non- conversational passages are intended for transition or scene introduction, and may not carry any emotion.",
            "We parsed selected narratives into passages, where a passage is considered to be eligible for annota- tion if it contained between 40 and 200 tokens. In long-form narratives, many non- conversational passages are intended for transition or scene introduction, and may not carry any emotion. We divided the eligible passages into two parts, and one part was pruned using selected emotion-rich but ambiguous lexicons such as cry, punch, kiss, etc.. Then we mixed this pruned part with the unpruned part for annotation in order to reduce the number of neutral passages. See Appendix A.1 for the lexicons used. 3.3 Mechanical Turk (MTurk) MTurk was set up using the standard sentiment template and instructed the crowd annotators to \u2018pick the best\/major emotion embodied in the pas- sage\u2019. We further provided instructions to clar- ify the intensity of an emotion, such as: \u201cRage\/Annoyance is a form of Anger\u201d, \u201cSeren- ity\/Ecstasy is a form of Joy\u201d, and \u201cLove includes Romantic\/Family\/Friendship\u201d, along with sample passages.",
            "We required all annotators have a \u2018master\u2019 MTurk quali\ufb01cation. Each passage was labelled by 3 unique annotators. Only passages with a majority agreement between annotators were ac- cepted as valid. This is equivalent to a Fleiss\u2019s \u03ba score of greater than 0.4. For passages without majority agreement be- tween annotators, we consolidated their labels us- ing in-house data annotators who are experts in narrative content. A passage is accepted as valid if the in-house annotator\u2019s label matched any one of the MTurk annotators\u2019 labels. The remaining passages are discarded. We provide the fraction of annotator agreement for each label in the dataset. Though passages may lose some emotional con- text when read independently of the complete nar- rative, we believe annotator agreement on our dataset supports the assertion that small excerpts can still convey coherent emotions. During the annotation process, several anno- tators had suggested for us to include additional emotions such as confused, pain, and jealousy,",
            "Genre Distribution (%) Mystery\/Thriller 19.7 Paranormal 16.6 Fantasy 13.2 Horror 11.3 Romance 8.7 Action\/Adventure 5.3 Other 9.3 Table 1: Genre distribution of the modern narratives which are common to narratives. As they were not part of the original Plutchiks wheel, we decided to not include them. An interesting future direction is to study the relationship between emotions such as pain versus sadness or confused versus surprise and improve the emotion model for narratives. 3.4 Dataset Statistics The dataset contains a total of 9710 passages, with an average of 6.24 sentences per passage, 16.16 words per sentence, and an average length of 86 words. The vocabulary size is 28K (when lowercased). It contains over 1600 unique titles across multi- ple categories, including 88 titles (1520 passages) from Project Gutenberg. All of the modern nar- ratives were written after the year 2000, with no- table amount of themes in coming-of-age, strong- female-lead, and LGBTQ+. The genre distribution is listed in Table 1.",
            "All of the modern nar- ratives were written after the year 2000, with no- table amount of themes in coming-of-age, strong- female-lead, and LGBTQ+. The genre distribution is listed in Table 1. In the \ufb01nal dataset, 21.0% of the data has con- sensus between all annotators, 73.5% has major- ity agreement, and 5.48% has labels assigned after consultation with in-house annotators. The distribution of data points over labels with top lexicons (lower-cased, normalized) is shown in Table 2. Note that the Disgust category is very small and should be discarded. Furthermore, we suspect that the data labelled as Surprise may be noisier than other categories and should be dis- carded as well. Table 3 shows a few examples labelled data from classic titles. More examples can be found in Table 6 in the Appendix A.2. 4 Benchmarks We performed benchmark experiments on the dataset using several different algorithms. In all experiments, we have discarded the data labelled with Surprise and Disgust. We pre-processed the data by using the SpaCy3 pipeline.",
            "4 Benchmarks We performed benchmark experiments on the dataset using several different algorithms. In all experiments, we have discarded the data labelled with Surprise and Disgust. We pre-processed the data by using the SpaCy3 pipeline. We masked out named entities with entity-type speci\ufb01c placeholders to reduce the chance of benchmark models utilizing named en- tities as a basis for classi\ufb01cation. Benchmark results are shown in Table 4. The dataset is approximately balanced after discarding the Surprise and Disgust classes. We report the average micro-F1 scores, with 5-fold cross valida- tion for each technique. We provide a brief overview of each bench- mark experiment below. Among all of the benchmarks, Bidirectional Encoder Representa- tions from Transformers (BERT) (Devlin et al., 2018) achieved the best performance with a 0.604 micro-F1 score. Overall, we observed that deep-learning based techniques performed better than lexical based methods. This suggests that a method which at- tends to context and themes could do well on the dataset.",
            "Overall, we observed that deep-learning based techniques performed better than lexical based methods. This suggests that a method which at- tends to context and themes could do well on the dataset. 4.1 Bag-of-Words-based Benchmarks We computed bag-of-words-based benchmarks using the following methods: \u2022 Classi\ufb01cation with TF-IDF + Linear SVM (TF-IDF + SVM) \u2022 Classi\ufb01cation with Depeche++ Emotion lex- icons (Araque et al., 2018) + Linear SVM (Depeche + SVM) \u2022 Classi\ufb01cation with NRC Emotion lexicons (Mohammad and Turney, 2010, 2013) + Lin- ear SVM (NRC + SVM) \u2022 Combination of TF-IDF and NRC Emotion lexicons (TF-NRC + SVM) 4.2 Doc2Vec + SVM We also used simple classi\ufb01cation models with learned embeddings. We trained a Doc2Vec model (Le and Mikolov, 2014) using the dataset and used the embedding document vectors as fea- tures for a linear SVM classi\ufb01er.",
            "We trained a Doc2Vec model (Le and Mikolov, 2014) using the dataset and used the embedding document vectors as fea- tures for a linear SVM classi\ufb01er. 4.3 Hierarchical RNN For this benchmark, we considered a Hierarchical RNN, following (Sordoni et al., 2015). We used two BiLSTMs (Graves et al., 2005) with 256 units each to model sentences and documents. The to- kens of a sentence were processed independently 3https:\/\/spacy.io\/",
            "Label Gutenberg Total Top Lexicons Neutral 318 1711 take, love, long, really, want, always, though, away, look Fear 159 1412 left, behind, right, want, let, death, go, say, think Sadness 195 1402 father, always, little, look, something, us, really, mother, think Anger 192 1306 feel, much, well, man, look, us, say, something, love Joy 241 1266 see, always, let, long, make, hand, away, get, really Love 162 1157 hand, know, right, let, happy, get, ever, us, look Anticipation 147 1020 know, long, life, make, get, think, blood, want, feel Surprise 102 362 love, \ufb01nd, looking, know, well, much, something, door, really Disgust 4 74 get, hand, inside, let, hate, table, men, always, make Table 2: Dataset label distribution Text Label I found this was a little too close upon him, but I made it up in what follows.",
            "He stood stock-still for a while and said nothing, and I went on thus: \u201cYou cannot,\u201d says I, \u2018without the highest injustice, believe that I yielded upon all these persuasions without a love not to be questioned, not to be shaken again by anything that could happen afterward. If you have such dishonourable thoughts of me, I must ask you what foundation in any of my behaviour have I given for such a suggestion?\u201d Angry She stretched hers eagerly and gratefully towards him. What had happened? Through all the numbness of her blood, there sprang a strange new warmth from his strong palm, and a pulse, which she had almost forgotten as a dream of the past, began to beat through her frame. She turned around all a-tremble, and saw his face in the glow of the coming day. Anticipation Ah! That moving procession that has left me by the road-side! Its fantastic colors are more brilliant and beautiful than the sun on the undulating waters. What matter if souls and bodies are failing beneath the feet of the ever-pressing multitude! It moves with the majestic rhythm of the spheres.",
            "That moving procession that has left me by the road-side! Its fantastic colors are more brilliant and beautiful than the sun on the undulating waters. What matter if souls and bodies are failing beneath the feet of the ever-pressing multitude! It moves with the majestic rhythm of the spheres. Its discordant clashes sweep upward in one harmonious tone that blends with the music of other worlds\u2013to complete God\u2019s orchestra. Joy Table 3: Sample data from classic titles Model micro-F1 TF-IDF + SVM 0.450 Depeche + SVM 0.254 NRC + SVM 0.286 TF-NRC + SVM 0.458 Doc2Vec + SVM 0.403 HRNN 0.469 BiRNN + Self-Attention 0.487 ELMo + BiRNN 0.516 Fine-tuned BERT 0.604 Table 4: Benchmark results (averaged 5-fold cross val- idation) of other sentence tokens. For each direction in the token-level BiLSTM, the last outputs were con- catenated and fed into the sentence-level BiLSTM as inputs.",
            "For each direction in the token-level BiLSTM, the last outputs were con- catenated and fed into the sentence-level BiLSTM as inputs. The outputs of the BiLSTM were connected to 2 dense layers with 256 ReLU units and a Softmax layer. We initialized tokens with publicly avail- able embeddings trained with GloVe (Pennington et al., 2014). Sentence boundaries were provided by SpaCy. Dropout was applied to the dense hid- den layers during training. 4.4 Bi-directional RNN and Self-Attention (BiRNN + Self-Attention) One challenge with RNN-based solutions for text classi\ufb01cation is \ufb01nding the best way to combine word-level representations into higher-level repre- sentations. Self-attention (Yang et al., 2016; Lin et al., 2017; Sinha et al., 2018) has been adapted to text",
            "classi\ufb01cation, providing improved interpretability and performance. We used (Lin et al., 2017) as the basis of this benchmark. The benchmark used a layered Bi-directional RNN (60 units) with GRU cells and a dense layer. Both self-attention layers were 60 units in size and cross-entropy was used as the cost function. Note that we have omitted the orthogonal reg- ularizer term, since this dataset is relatively small compared to the traditional datasets used for train- ing such a model. We did not observe any signi\ufb01- cant performance gain while using the regularizer term in our experiments. 4.5 ELMo embedding and Bi-directional RNN (ELMo + BiRNN) Deep Contextualized Word Representations (ELMo) (Peters et al., 2018) have shown recent success in a number of NLP tasks. The unsuper- vised nature of the language model allows it to utilize a large amount of available unlabelled data in order to learn better representations of words. We used the pre-trained ELMo model (v2) available on Tensorhub4 for this benchmark.",
            "The unsuper- vised nature of the language model allows it to utilize a large amount of available unlabelled data in order to learn better representations of words. We used the pre-trained ELMo model (v2) available on Tensorhub4 for this benchmark. We fed the word embeddings of ELMo as input into a one layer Bi-directional RNN (16 units) with GRU cells (with dropout) and a dense layer. Cross- entropy was used as the cost function. 4.6 Fine-tuned BERT Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018) has achieved state-of-the-art results on several NLP tasks, including sentence classi\ufb01cation. We used the \ufb01ne-tuning procedure outlined in the original work to adapt the pre-trained uncased BERTLARGE5 to a multi-class passage classi\ufb01ca- tion task. This technique achieved the best result among our benchmarks, with an average micro-F1 score of 60.4%. 5 Conclusion We introduce DENS, a dataset for multi-class emotion analysis from long-form narratives in En- glish.",
            "This technique achieved the best result among our benchmarks, with an average micro-F1 score of 60.4%. 5 Conclusion We introduce DENS, a dataset for multi-class emotion analysis from long-form narratives in En- glish. We provide a number of benchmark results based on models ranging from bag-of-word mod- els to methods based on pre-trained language mod- els (ELMo and BERT). 4https:\/\/tfhub.dev\/google\/elmo\/2 5https:\/\/tfhub.dev\/google\/bert_ uncased_L-24_H-1024_A-16\/1 Our benchmark results demonstrate that this dataset provides a novel challenge in emotion analysis. The results also demonstrate that attention-based models could signi\ufb01cantly im- prove performance on classi\ufb01cation tasks such as emotion analysis. Interesting future directions for this work in- clude: 1. incorporating common-sense knowledge into emotion analysis to capture semantic context and 2. using few-shot learning to bootstrap and improve performance of underrepresented emo- tions. Finally, as narrative passages often involve in- teractions between multiple emotions, one avenue for future datasets could be to focus on the multi- emotion complexities of human language and their contextual interactions.",
            "Finally, as narrative passages often involve in- teractions between multiple emotions, one avenue for future datasets could be to focus on the multi- emotion complexities of human language and their contextual interactions. References Cecilia Alm, Dan Roth, and Richard Sproat. 2005. Emotions from text: Machine learning for text-based emotion prediction. Oscar Araque, Lorenzo Gatti, Jacopo Staiano, and Marco Guerini. 2018. Depechemood++: a bilingual emotion lexicon built through simple yet powerful techniques. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805. Alex Graves, Santiago Fern\u00b4andez, and J\u00a8urgen Schmid- huber. 2005. Bidirectional lstm networks for im- proved phoneme classi\ufb01cation and recognition. In International Conference on Arti\ufb01cial Neural Net- works, pages 799\u2013804. Springer.",
            "2005. Bidirectional lstm networks for im- proved phoneme classi\ufb01cation and recognition. In International Conference on Arti\ufb01cial Neural Net- works, pages 799\u2013804. Springer. Neel Kant, Raul Puri, Nikolai Yakovenko, and Bryan Catanzaro. 2018. Practical text classi\ufb01cation with large pre-trained language models. arXiv preprint arXiv:1812.01207. Evgeny Kim and Roman Klinger. 2018. Who feels what and why? annotation of a literature corpus with semantic roles of emotions. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1345\u20131359. Association for Com- putational Linguistics. Roman Klinger, Orphee De Clercq, Saif Mohammad, and Alexandra Balahur. 2018. Iest: Wassa-2018 implicit emotions shared task. In Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 31\u201342, Brussels, Belgium. Association for Computational Linguistics.",
            "Quoc V. Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. CoRR, abs\/1405.4053. Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San- tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. 2017. A structured self-attentive sentence embedding. Julian McAuley, Rahul Pandey, and Jure Leskovec. 2015. Inferring networks of substitutable and com- plementary products. In Proceedings of the 21th ACM SIGKDD international conference on knowl- edge discovery and data mining, pages 785\u2013794. ACM. Saif M. Mohammad. 2012. #emotional tweets. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics \u2013 Volume 1: Proceed- ings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 246\u2013255, Montr\u00b4eal, Canada.",
            "Association for Computational Linguistics. Saif M. Mohammad, Felipe Bravo-Marquez, Mo- hammad Salameh, and Svetlana Kiritchenko. 2018. Semeval-2018 task 1: Affect in tweets. In Pro- ceedings of the International Workshop on Semantic Evaluation (SemEval-2018). Association for Com- putational Linguistics. Saif M. Mohammad and Svetlana Kiritchenko. 2015. Using hashtags to capture \ufb01ne emotion cate- gories from tweets. Computational Intelligence, 31(2):301\u2013326. Saif M. Mohammad and Peter D. Turney. 2010. Emo- tions evoked by common words and phrases: Us- ing mechanical turk to create an emotion lexicon. In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Gener- ation of Emotion in Text, CAAGET \u201910, pages 26\u2013 34, Stroudsburg, PA, USA. Association for Compu- tational Linguistics. Saif M. Mohammad and Peter D. Turney.",
            "Association for Compu- tational Linguistics. Saif M. Mohammad and Peter D. Turney. 2013. Crowdsourcing a word-emotion association lexicon. 29(3):436\u2013465. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 confer- ence on empirical methods in natural language pro- cessing (EMNLP), pages 1532\u20131543. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. In Proc. of NAACL. Robert Plutchik. 1979. Emotions: A general psy- choevolutionary theory, volume 1. Psycology Press,Taylor and Francis Group. Andrew J. Reagan, Lewis Mitchell, Dilan Kiley, Christopher M. Danforth, and Peter Sheridan Dodds. 2016. The emotional arcs of stories are dominated by six basic shapes.",
            "Psycology Press,Taylor and Francis Group. Andrew J. Reagan, Lewis Mitchell, Dilan Kiley, Christopher M. Danforth, and Peter Sheridan Dodds. 2016. The emotional arcs of stories are dominated by six basic shapes. EPJ Data Science, 5:31. Koustuv Sinha, Yue Dong, Jackie Chi Kit Cheung, and Derek Ruths. 2018. A hierarchical neural attention- based text classi\ufb01er. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing, pages 817\u2013823. Association for Computational Linguistics. Alessandro Sordoni, Yoshua Bengio, Hossein Vahabi, Christina Lioma, Jakob Grue Simonsen, and Jian- Yun Nie. 2015. A hierarchical recurrent encoder- decoder for generative context-aware query sugges- tion. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Man- agement, pages 553\u2013562. ACM. Wikimedia. 2011. Robert plutchik\u2019s wheel of emo- tions.",
            "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Man- agement, pages 553\u2013562. ACM. Wikimedia. 2011. Robert plutchik\u2019s wheel of emo- tions. File: Plutchik-wheel.svg. Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. 2016. Hierarchi- cal attention networks for document classi\ufb01cation. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Compu- tational Linguistics: Human Language Technolo- gies, pages 1480\u20131489. Association for Computa- tional Linguistics.",
            "A Appendices A.1 Lexicons cry punch blood knife \ufb02ower moon wind exclaim chuckle tear punch yell kiss touch warm dead shiver chill Table 5: Lexicons used to prune part of the data for labelling A.2 Sample Data Table 6 shows sample passages from classic titles with corresponding labels. Text Label He took his screwdriver and again took off the lid of the cof\ufb01n. Arthur looked on, very pale but silent. When the lid was removed he stepped forward. He evidently did not know that there was a leaden cof\ufb01n, or at any rate, had not thought of it. When he saw the rent in the lead, the blood rushed to his face for an instant, but as quickly fell away again, so that he remained of a ghastly whiteness. He was still silent. Van Helsing forced back the leaden \ufb02ange, and we all looked in and recoiled. Fear The chair went to matchwood at the bottom, and we rolled apart into the gut- ter. He sprang to his feet, waving his \ufb01sts and wheezing like an asthmatic.",
            "Fear The chair went to matchwood at the bottom, and we rolled apart into the gut- ter. He sprang to his feet, waving his \ufb01sts and wheezing like an asthmatic. \u201cHad enough?\u201d he panted. \u201cYou infernal bully!\u201d I cried, as I gathered myself together. Anger The judges sat grave and mute, gave me an easy hearing, and time to say all that I would, but, saying neither Yes nor No to it, pronounced the sentence of death upon me, a sentence that was to me like death itself, which, after it was read, confounded me. I had no more spirit left in me, I had no tongue to speak, or eyes to look up either to God or man. Sadness The Prince burst into a yelling, shrieking \ufb01t of laughter. Instantly the yellow- haired serfs in waiting, the Calmucks at the hall-door, and the half-witted dwarf who crawled around the table in his tow shirt, began laughing in cho- rus, as violently as they could.",
            "Instantly the yellow- haired serfs in waiting, the Calmucks at the hall-door, and the half-witted dwarf who crawled around the table in his tow shirt, began laughing in cho- rus, as violently as they could. The Princess Martha and Prince Boris laughed also; and while the old man\u2019s eyes were dimmed with streaming tears of mirth, quickly exchanged nods. The sound extended all over the castle, and was heard outside of the walls. Joy \u201cDo not be such an unreasonable child\u201d, he remonstrated, feebly. \u201cI do not love you with the wild, irrational passion of former years; but I have the ten- derest regard for you, and my heart warms at the sight of your sweet face, and I shall do all in my power to make you as happy as any man can make you who\u2013\u201d Love I looked around for his birds, and not seeing them, asked him where they were. He replied, without turning round, that they had all \ufb02own away. There were a few feathers about the room and on his pillow a drop of blood.",
            "He replied, without turning round, that they had all \ufb02own away. There were a few feathers about the room and on his pillow a drop of blood. I said nothing, but went and told the keeper to report to me if there were anything odd about him during the day. Neutral Table 6: Sample data from classic titles"
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1910.11769.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 5857.0,
    "avg_doclen_est": 183.03125
}
