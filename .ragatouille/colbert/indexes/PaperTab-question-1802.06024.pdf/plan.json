{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "arXiv:1802.06024v2  [cs.CL]  24 Feb 2018 Towards a Continuous Knowledge Learning Engine for Chatbots Sahisnu Mazumder, Nianzu Ma, Bing Liu Department of Computer Science, University of Illinois at Chicago, IL, USA sahisnumazumder@gmail.com, nma4@uic.edu, liub@cs.uic.edu Abstract Although chatbots have been very popular in recent years, they still have some serious weaknesses which limit the scope of their ap- plications. One major weakness is that they cannot learn new knowledge during the con- versation process, i.e., their knowledge is \ufb01xed beforehand and cannot be expanded or up- dated during conversation. In this paper, we propose to build a general knowledge learn- ing engine for chatbots to enable them to con- tinuously and interactively learn new knowl- edge during conversations. As time goes by, they become more and more knowledgeable and better and better at learning and conver- sation. We model the task as an open-world knowledge base completion problem and pro- pose a novel technique called lifelong inter- active learning and inference (LiLi) to solve it.",
            "As time goes by, they become more and more knowledgeable and better and better at learning and conver- sation. We model the task as an open-world knowledge base completion problem and pro- pose a novel technique called lifelong inter- active learning and inference (LiLi) to solve it. LiLi works by imitating how humans ac- quire knowledge and perform inference during an interactive conversation. Our experimental results show LiLi is highly promising. 1 Introduction Chatbots such as dialog and question-answering systems have a long history in AI and natural lan- guage processing. Early such systems were mostly built using markup languages such as AIML1, handcrafted conversation generation rules, and\/or information retrieval techniques (Banchs and Li, 2012; Ameixa et al., 2014; Lowe et al., 2015; Serban et al., 2015). Recent neural conversation models (Vinyals and Le, 2015; Xing et al., 2017; Li et al., 2017b) are even able to perform open- ended conversations.",
            "Recent neural conversation models (Vinyals and Le, 2015; Xing et al., 2017; Li et al., 2017b) are even able to perform open- ended conversations. However, since they do not use explicit knowledge bases and do not per- form inference, they often suffer from generic and dull responses (Xing et al., 2017; Li et al., 2017a). More recently, Ghazvininejad et al. (2017) and 1http:\/\/www.alicebot.org\/ Le et al. (2016) proposed to use knowledge bases (KBs) to help generate responses for knowledge- grounded conversation. However, one major weakness of all existing chat systems is that they do not explicitly or implicitly learn new knowl- edge in the conversation process. This seriously limits the scope of their applications. In contrast, we humans constantly learn new knowledge in our conversations. Even if some existing systems can use very large knowledge bases either harvested from a large data source such as the Web or built manually, these KBs still miss a large number of facts (knowledge) (West et al., 2014).",
            "In contrast, we humans constantly learn new knowledge in our conversations. Even if some existing systems can use very large knowledge bases either harvested from a large data source such as the Web or built manually, these KBs still miss a large number of facts (knowledge) (West et al., 2014). It is thus important for a chatbot to continuously learn new knowledge in the conversation process to expand its KB and to improve its conversation ability. In recent years, researchers have studied the problem of KB completion, i.e., inferring new facts (knowledge) automatically from existing facts in a KB. KB completion (KBC) is de\ufb01ned as a binary classi\ufb01cation problem: Given a query triple, (s, r, t), we want to predict whether the source entity s and target entity t can be linked by the relation r. However, existing approaches (Lao et al., 2011, 2015; Bordes et al., 2011, 2013; Nickel et al., 2015; Mazumder and Liu, 2017) solve this prob- lem under the closed-world assumption, i.e., s, r and t are all known to exist in the KB.",
            "This is a major weakness because it means that no new knowledge or facts may contain unknown entities or relations. Due to this limitation, KBC is clearly not suf\ufb01cient for knowledge learning in conversa- tions because in a conversation, the user can say anything, which may contain entities and relations that are not already in the KB. In this paper, we remove this assumption of KBC, and allow all s, r and t to be unknown. We call the new problem open-world knowledge base completion (OKBC). OKBC generalizes KBC. Below, we show that solving OKBC naturally pro-",
            "vides the ground for knowledge learning and in- ference in conversations. In essence, we formulate an abstract problem of knowledge learning and in- ference in conversations as a well-de\ufb01ned OKBC problem in the interactive setting. From the perspective of knowledge learning in conversations, essentially we can extract two key types of information, true facts and queries, from the user utterances. Queries are facts whose truth values need to be determined2. Note that we do not study fact or relation extraction in this paper as there is an extensive work on the topic. (1) For a true fact, we will incorporate it into the KB. Here we need to make sure that it is not already in the KB, which involves relation resolution and entity linking. After a fact is added to the KB, we may predict that some related facts involving some ex- isting relations in the KB may also be true (not logical implications as they can be automatically inferred). For example, if the user says \u201cObama was born in USA,\u201d the system may guess that (Obama, CitizenOf, USA) (meaning that Obama is a citizen of USA) could also be true based on the current KB.",
            "For example, if the user says \u201cObama was born in USA,\u201d the system may guess that (Obama, CitizenOf, USA) (meaning that Obama is a citizen of USA) could also be true based on the current KB. To verify this fact, it needs to solve a KBC problem by treating (Obama, CitizenOf, USA) as a query. This is a KBC problem because the fact (Obama, BornIn, USA) extracted from the original sentence has been added to the KB. Then Obama and USA are in the KB. If the KBC problem is solved, it learns a new fact (Obama, CitizenOf, USA) in addition to the extracted fact (Obama, BornIn, USA). (2) For a query fact, e.g., (Obama, BornIn, USA) extracted from the user question \u201cWas Obama born in USA?\u201d we need to solve the OKBC problem if any of \u201cObama, \u201cBornIn\u201d, or \u201cUSA\u201d is not already in the KB. We can see that OKBC is the core of a knowl- edge learning engine for conversation. Thus, in this paper, we focus on solving it.",
            "We can see that OKBC is the core of a knowl- edge learning engine for conversation. Thus, in this paper, we focus on solving it. We assume that other tasks such as fact\/relation extraction and resolution and guessing of related facts of an ex- tracted fact are solved by other sub-systems. We solve the OKBC problem by mimicking how humans acquire knowledge and perform rea- soning in an interactive conversation. Whenever we encounter an unknown concept or relation while answering a query, we perform inference us- ing our existing knowledge. If our knowledge does not allow us to draw a conclusion, we typically 2In this work we do not deal with subjective information such as beliefs and opinions, which we leave it to future work. ask questions to others to acquire related knowl- edge and use it in inference. The process typically involves an inference strategy (a sequence of ac- tions), which interleaves a sequence of processing and interactive actions. A processing action can be the selection of related facts, deriving inference chain, etc., that advances the inference process. An interactive action can be deciding what to ask, formulating a suitable question, etc., that enable us to interact.",
            "A processing action can be the selection of related facts, deriving inference chain, etc., that advances the inference process. An interactive action can be deciding what to ask, formulating a suitable question, etc., that enable us to interact. The process helps grow the knowl- edge over time and the gained knowledge enables us to communicate better in the future. We call this lifelong interactive learning and inference (LiLi). Lifelong learning is re\ufb02ected by the facts that the newly acquired facts are retained in the KB and used in inference for future queries, and that the accumulated knowledge in addition to the up- dated KB including past inference performances are leveraged to guide future interaction and learn- ing. LiLi should have the following capabilities: 1. to formulate an inference strategy for a given query that embeds processing and in- teractive actions. 2. to learn interaction behaviors (deciding what to ask and when to ask the user). 3. to leverage the acquired knowledge in the current and future inference process. 4. to perform 1, 2 and 3 in a lifelong manner for continuous knowledge learning.",
            "2. to learn interaction behaviors (deciding what to ask and when to ask the user). 3. to leverage the acquired knowledge in the current and future inference process. 4. to perform 1, 2 and 3 in a lifelong manner for continuous knowledge learning. This setting is ideal for many NLP applications like dialog and question-answering systems that naturally provide the scope for human interaction and demand real-time inference. LiLi starts with the closed-world KBC ap- proach path-ranking (PR) (Lao et al., 2011; Gardner and Mitchell, 2015)3 and extends KBC in a major way to open-world knowledge base com- pletion (OKBC). For a relation r, PR works by enumerating paths (except single-link path r) be- tween entity-pairs linked by r in the KB and use them as features to train a binary classi\ufb01er to pre- dict whether a query (s, r, t) should be in the KB. Here, a path between two entities is a sequence of relations linking them.",
            "Here, a path between two entities is a sequence of relations linking them. In our work, we adopt the latest PR method, C-PR (Mazumder and Liu, 3We choose path-ranking (PR) due to its high in- terpretability and better performance than latent features (Wang et al., 2016a; Toutanova, 2015)",
            "User: (Obama, CitizenOf, USA)? \u201cIs Obama a citizen of USA?\u201d [Query] LiLi: I do not know what \u201cCitizenOf\u201d mean? Can you provide me an example? [Ask for Clue] User: (David Cameron, CitizenOf, UK). \u201cDavid Cameron is a citizen of UK.\u201d [SF1] LiLi: Got it. Can you tell me how \u201cObama\u201d and \u201cHonolulu\u201d are related? [CLQ] User: (Obama, BornIn, Honolulu). \u201cObama was born in Honolulu.\u201d [SF2] LiLi: Got it. Can you tell also me how \u201cHonolulu\u201d and \u201cHawaii\u201d are related? [MLQ] User: (Honolulu, CapitalOfState, Hawaii). \u201cHonolulu is the state capital of Hawaii.\u201d [SF3] LiLi: (Obama, CitizenOf, USA) is correct. [Answer] Figure 1: An example of interactive inference and learning. Note that LiLi only works with triples. Each triple above is assumed to be extracted from the sentence after it.",
            "[Answer] Figure 1: An example of interactive inference and learning. Note that LiLi only works with triples. Each triple above is assumed to be extracted from the sentence after it. 2017) and extend it to make it work in the open- world setting. C-PR enumerates paths by perform- ing bidirectional random walks over the KB graph while leveraging the context of the source-target entity-pair. We also adopt and extend the com- positional vector space model (Neelakantan et al., 2015; Das et al., 2016) with continual learning ca- pability for prediction. Given an OKBC query (s, r, t) (e.g., \u201c(Obama, CitizenOf, USA), which means whether Obama a citizen of USA), LiLi interacts with the user (if needed) by dynamically formulating questions (see the interaction example in Figure 14, which will be further explained in \u00a73) and leverages the interactively acquired knowledge (supporting facts (SFs) in the \ufb01gure) for continued inference. To do so, LiLi formulates a query-speci\ufb01c infer- ence strategy and executes it.",
            "To do so, LiLi formulates a query-speci\ufb01c infer- ence strategy and executes it. We design LiLi in a Reinforcement Learning (RL) setting that per- forms sub-tasks like formulating and executing strategy, training a prediction model for inference, and knowledge retention for future use. To the best of our knowledge, our work is the \ufb01rst to address the OKBC problem and to propose an interactive learning mechanism to solve it in a continuous or lifelong manner. We empirically verify the effec- tiveness of LiLi on two standard real-world KBs: Freebase and WordNet. Experimental results show 4Note that the user query and responses are in triples as we are not building a conversation system but a knowledge acquisition system. Also, the query may be from a user or a system (e.g., a question-answer system, a conversation sys- tem that has extracted a candidate fact and wants to verify it and add it to the KB. This paper will not study the case that the query fact is already in the KB, which is easy to verify.",
            "This paper will not study the case that the query fact is already in the KB, which is easy to verify. Also note that, as our work focuses on knowledge learning and inference, rather than conversation modeling, we sim- ply use template-based question generation to model LiLi\u2019s interaction with the user. that LiLi is highly effective in terms of its predic- tive performance and strategy formulation ability. 2 Related Work To the best of our knowledge, we are not aware of any knowledge learning system that can learn new knowledge in the conversation process. This section thus discusses other related work. Among existing KB completion approaches, (Neelakantan et al., 2015) extended the vector space model for zero-shot KB inference. How- ever, the model cannot handle unknown enti- ties and can only work on \ufb01xed set of unknown relations with known embeddings. Recently, (Shi and Weninger, 2017) proposed a method us- ing external text corpus to perform inference on unknown entities. However, the method cannot handle unknown relations. Thus, these methods are not suitable for our open-world setting. None of the existing KB inference methods perform in- teractive knowledge learning like LiLi.",
            "However, the method cannot handle unknown relations. Thus, these methods are not suitable for our open-world setting. None of the existing KB inference methods perform in- teractive knowledge learning like LiLi. NELL (Mitchell et al., 2015) continuously up- dates its KB using facts extracted from the Web. Our task is very different as we do not do Web fact extraction (which is also useful). We focus on user interactions in this paper. Our work is related to interactive language learning (ILL) (Wang et al., 2016b, 2017), but these are not about KB completion. The work in (Li et al., 2016b) allows a learner to ask questions in dialogue. However, this work used RL to learn about whether to ask the user or not. The \u201cwhat to ask aspect\u201d was manually designed by modeling synthetic tasks. LiLi formulates query-speci\ufb01c inference strate- gies which embed interaction behaviors.",
            "However, this work used RL to learn about whether to ask the user or not. The \u201cwhat to ask aspect\u201d was manually designed by modeling synthetic tasks. LiLi formulates query-speci\ufb01c inference strate- gies which embed interaction behaviors. Also, no existing dialogue systems (Vinyals and Le, 2015; Li et al., 2016a; Bordes and Weston, 2016; Weston, 2016; Zhang et al., 2017) employ lifelong learning to train prediction models by using infor- mation\/knowledge retained in the past. Our work is related to general lifelong learning in (Chen and Liu, 2016; Ruvolo and Eaton, 2013; Chen and Liu, 2014, 2013; Bou Ammar et al., 2015; Shu et al., 2017). However, they learn only one type of tasks, e.g., supervised, topic model- ing or reinforcement learning (RL) tasks. None of them is suitable for our setting, which involves in- terleaving of RL, supervised and interactive learn- ing.",
            "However, they learn only one type of tasks, e.g., supervised, topic model- ing or reinforcement learning (RL) tasks. None of them is suitable for our setting, which involves in- terleaving of RL, supervised and interactive learn- ing. More details about lifelong learning can be found in the book (Chen and Liu, 2016).",
            "3 Interactive Knowledge Learning (LiLi) We design LiLi as a combination of two intercon- nected models: (1) a RL model that learns to for- mulate a query-speci\ufb01c inference strategy for per- forming the OKBC task, and (2) a lifelong predic- tion model to predict whether a triple should be in the KB, which is invoked by an action while executing the inference strategy and is learned for each relation as in C-PR. The framework improves its performance over time through user interaction and knowledge retention. Compared to the ex- isting KB inference methods, LiLi overcomes the following three challenges for OKBC: 1. Mapping open-world to close-world. Be- ing a closed-world method, C-PR cannot extract path features and learn a prediction model when any of s, r or t is unknown. LiLi solves this prob- lem through interactive knowledge acquisition. If r is unknown, LiLi asks the user to provide a clue (an example of r).",
            "LiLi solves this prob- lem through interactive knowledge acquisition. If r is unknown, LiLi asks the user to provide a clue (an example of r). And if s or t is unknown, LiLi asks the user to provide a link (relation) to connect the unknown entity with an existing entity (auto- matically selected) in the KB. We refer to such a query as a connecting link query (CLQ). The acquired knowledge reduces OKBC to KBC and makes the inference task feasible. 2. Spareseness of KB. A main issue of all PR methods like C-PR is the connectivity of the KB graph. If there is no path connecting s and t in the graph, path enumeration of C-PR gets stuck and inference becomes infeasible. In such cases, LiLi uses a template relation (\u201c@-?-@\u201d) as the missing link marker to connect entity-pairs and continues feature extraction. A path containing \u201c@-?-@\u201d is called an incomplete path. Thus, the extracted fea- ture set contains both complete (no missing link) and incomplete paths.",
            "A path containing \u201c@-?-@\u201d is called an incomplete path. Thus, the extracted fea- ture set contains both complete (no missing link) and incomplete paths. Next, LiLi selects an in- complete path from the feature set and asks the user to provide a link for path completion. We re- fer to such a query as missing link query (MLQ). 3. Limitation in user knowledge. If the user is unable to respond to MLQs or CLQs, LiLi uses a guessing mechanism (discussed later) to \ufb01ll the gap. This enables LiLi to continue its inference even if the user cannot answer a system question. 3.1 Components of LiLi As lifelong learning needs to retain knowledge learned from past tasks and use it to help fu- ture learning (Chen and Liu, 2016), LiLi uses a Knowledge Store (KS) for knowledge retention. KS has four components: (i) Knowledge Graph (G): G (the KB) is initialized with base KB triples (see \u00a74) and gets updated over time with the ac- quired knowledge.",
            "KS has four components: (i) Knowledge Graph (G): G (the KB) is initialized with base KB triples (see \u00a74) and gets updated over time with the ac- quired knowledge. (ii) Relation-Entity Matrix (M): M is a sparse matrix, with rows as rela- tions and columns as entity-pairs and is used by the prediction model. Given a triple (s, r, t) \u2208G, we set M[r, (s, t)] = 1 indicating r occurs for pair (s, t). (iii) Task Experience Store (T ): T stores the predictive performance of LiLi on past learned tasks in terms of Matthews correlation coef\ufb01cient (MCC)5 that measures the quality of binary clas- si\ufb01cation. So, for two tasks r and r\u2032 (each relation is a task), if T [r] > T [r\u2032] [where T [r]=MCC(r)], we say C-PR has learned r well compared to r\u2032.",
            "So, for two tasks r and r\u2032 (each relation is a task), if T [r] > T [r\u2032] [where T [r]=MCC(r)], we say C-PR has learned r well compared to r\u2032. (iv) Incomplete Feature DB (\u03a0DB): \u03a0DB stores the frequency of an incomplete path \u03c0 in the form of a tuple (r, \u03c0, e\u03c0 ij) and is used in formulating MLQs. \u03a0DB[(r, \u03c0, e\u03c0 ij)] = N implies LiLi has extracted incomplete path \u03c0 N times involving entity-pair e\u03c0 ij [(ei, ej)] for query relation r. The RL model learns even after training when- ever it encounters an unseen state (in testing) and thus, gets updated over time. KS is updated con- tinuously over time as a result of the execution of LiLi and takes part in future learning. The predic- tion model uses lifelong learning (LL), where we transfer knowledge (parameter values) from the model for a past most similar task to help learn for the current task. Similar tasks are identi\ufb01ed by factorizing M and computing a task similarity matrix Msim.",
            "The predic- tion model uses lifelong learning (LL), where we transfer knowledge (parameter values) from the model for a past most similar task to help learn for the current task. Similar tasks are identi\ufb01ed by factorizing M and computing a task similarity matrix Msim. Besides LL, LiLi uses T to identify poorly learned past tasks and acquire more clues for them to improve its skillset over time. LiLi also uses a stack, called Inference Stack (IS) to hold query and its state information for RL. LiLi always processes stack top (IS[top]). The clues from the user get stored in IS on top of the query during strategy execution and processed \ufb01rst. Thus, the prediction model for r is learned before performing inference on query, transform- ing OKBC to a KBC problem. Table 1 shows the parameters of LiLi used in the following sections.",
            "Thus, the prediction model for r is learned before performing inference on query, transform- ing OKBC to a KBC problem. Table 1 shows the parameters of LiLi used in the following sections. 3.2 Working of LiLi Given an OKBC query (s, r, t), we represent it as a data instance d. d consists of td (the query triple), \u03b4IL(d) (interaction limit set for d), expd (expe- rience list storing the transition history of MDP for d in RL) and md (mode of d) denoting if d is 5 en.wikipedia.org\/wiki\/Matthews correlation coef\ufb01cient",
            "Table 1: Parameters of LiLi. Para. Description \u03b1 learning rate of Q-learning agent \u03b3 discount factor of Q-learning agent \u03b4\u03c0 If St[ILO]=0 and feature set contains \u2265\u03b4\u03c0 # complete fea- tures, we consider feature set as complete and set St[CPF]=1. \u03b4IL max # times LiLi is allowed to ask user per query (we refer \u03b4IL as the interaction limit of LiLi per query). \u03b7\u03c0 maximum path length for C-PR \u03b7w number of random walks per query for C-PR l, h low and high contextual similarity threshold k rank of trancated SVD \u03b2 clue acquisition rate \u03c1 past task selection rate Table 2: State bits and their meanings. State bits Name Description QERS Query entities and relation searched Whether the query source (s) and target (t) entities and query relation (r) have been searched in KB or not. SEF Source Entity Found Whether the source entity (s) has been found in KB or not.",
            "State bits Name Description QERS Query entities and relation searched Whether the query source (s) and target (t) entities and query relation (r) have been searched in KB or not. SEF Source Entity Found Whether the source entity (s) has been found in KB or not. TEF Target Entity Found Whether the target entity (t) has been found in KB or not QRF Query Relation Found Whether the query relation (r) has been found in KB or not CLUE Clue bit set Whether the query is a clue or not. ILO Interaction Limit Over Whether the interaction limit is over for the query or not. PFE Path Feature extracted Whether path feature extraction has been done or not. NEFS Non-empty Feature set Whether the extracted feature set is non-empty or empty. CPF Complete path Found Whether the extracted path features are complete or not. INFI Inference Invoked Whether Inference instruction has been invoked or not. Table 3: Actions and their descriptions. Id Description Reward Structure [condition type] a0 Search source (h), target (t) entities and query relation (r) in KB.",
            "INFI Inference Invoked Whether Inference instruction has been invoked or not. Table 3: Actions and their descriptions. Id Description Reward Structure [condition type] a0 Search source (h), target (t) entities and query relation (r) in KB. r = ( 0 if St[QERS] = 0 [valid] \u22121 otherwise [invalid] a1 Ask user to provide an example\/clue for query relation r r = ( 0 if St[ILO] = 0 and St[CLUE] = 0 and St[QERS] = 1 and St[QRF ] = 0 [valid] \u22121 otherwise [invalid] a2 Ask user to provide missing link for path feature completion. r = ( 0 if St[P F E] = 1 and St[NEF S] = 1 and St[ILO] = 0 and St[CP F ] = 0 [valid] \u22121 otherwise [invalid] a3 Ask user to provide the connecting link for augmenting a new entity with KB.",
            "r = ( \u22121 if St[QERS] = 0 or (St[SEF ] = 1 and St[T EF ] = 1) or St[ILO] = 1 [invalid] 0 otherwise [valid] a4 Extract path features between source (s) and target (t) entities using C-PR r = ( \u22121 if St[QERS] = 0 or St[P F E] = 1 [invalid] or (a4 executed , but |\u03a0d| = 0*) 0 otherwise [valid] a5 Store query data instance in data buffer and invoke prediction model for inference. r = ( 1 if St[QRF ] = 1 and St[CP F ] = 1 [win] \u22121 otherwise [loss] \u2018T\u2019 (training), \u2018V \u2019 (validation), \u2018E\u2019 (evaluation) or \u2018C\u2019 (clue) instance and \u03a0d (feature set). We de- note \u03a0cp d (\u03a0cp d ) as the set of all complete (incom- plete) path features in \u03a0d.",
            "We de- note \u03a0cp d (\u03a0cp d ) as the set of all complete (incom- plete) path features in \u03a0d. Given a data instance d, LiLi starts its initialization as follows: it sets the state as S0 (based on md, explained later), pushes the query tuple (d, S0) into IS and feeds IS[top] to the RL-model for strategy formulation from S0. Inference Strategy Formulation. We view solving the strategy formulation problem as learn- ing to play an inference game, where the goal is to formulate a strategy that \u201dmakes the inference task possible\u201d. Considering PR methods, inference is possible, iff (1) r becomes known to its KB (by acquiring clues when r is unknown) and (2) path features are extracted between s and t (which in- turn requires s and t to be known to KB). If these conditions are met at the end of an episode (when strategy formulation \ufb01nishes for a given query) of the game, LiLi wins and thus, it trains the predic- tion model for r and uses it for inference.",
            "If these conditions are met at the end of an episode (when strategy formulation \ufb01nishes for a given query) of the game, LiLi wins and thus, it trains the predic- tion model for r and uses it for inference. LiLi\u2019s strategy formulation is modeled as a Markov Decision Process (MDP) with \ufb01nite state (S) and action (A) spaces. A state S \u2208S con- sists of 10 binary state variables (Table 2), each of which keeps track of results of an action a \u2208A taken by LiLi and thus, records the progress in inference process made so far. S0 is the initial state with all state bits set as 0. If the data in- stance (query) is a clue [md = C], S0[CLUE] is set as 1. A consists of 6 actions (Table 3). a0, a4, a5 are processing actions and a1, a2, a3 are interactive actions. Whenever a5 is executed, the MDP reaches the terminal state.",
            "A consists of 6 actions (Table 3). a0, a4, a5 are processing actions and a1, a2, a3 are interactive actions. Whenever a5 is executed, the MDP reaches the terminal state. Given an action a in state St, if a is invalid6 in St or the objec- tive of a is unsatis\ufb01ed (* marked the condition in a4), RL receives a negative reward (empirically set); else receives a positive reward.7. We use Q- learning (Watkins and Dayan, 1992) with \u01eb-greedy strategy to learn the optimal policy for training the RL model. Note that, the inference strategy is independent of KB type and correctness of pre- diction. Thus, the RL-model is trained only once from scratch (reused thereafter for other KBs) and also, independently of the prediction model. Sometimes the training dataset may not be enough to learn optimal policy for all S \u2208S. Thus, encountering an unseen state during test can make RL-model clueless about the action.",
            "Sometimes the training dataset may not be enough to learn optimal policy for all S \u2208S. Thus, encountering an unseen state during test can make RL-model clueless about the action. Given a state S, whenever an invalid a \u2208A \u2212{a2} is chosen, LiLi remains in S. For a2, LiLi remains 6 \u201cinvalid\u201d means performing a in St is meaningless (doesn\u2019t advance reasoning) like choosing repetitive process- ing actions during training (by random exploration of A). 7Unlike existing RL-based interactive or active learn- ing (Li et al., 2016b; Woodward and Finn, 2017), the user doesn\u2019t provide feedback to guide the learning process of LiLi. Rather, the RL-model uses an internal feedback mech- anism to self-learn its optimal policy. This is analogous to the idea of learning by self-realization observed in humans: whenever we try to solve a problem, we often try to formu- late strategy and re\ufb01ne it ourselves based on whether we can derive the answer of the problem without external guidance. Likewise, here the RL-model gets feedback based on whether it is able to advance the reasoning process or not.",
            "in S untill \u03b4IL > 0 (see Table 1 for \u03b4IL). So, if the state remains the same for (\u03b4IL+1) times, it implies LiLi has encountered a fault (an unseen state). RL-model instantly switches to the training mode and randomly explores A to learn the opti- mal action (fault-tolerant learning). While explor- ing A, the model chooses a5 only when it has tried all other a \u2208A to avoid abrupt end of episode. Execution of Actions. At any given point in time, let (d, St) be the current IS[top], \u02c6a is the chosen action and the current version of KS com- ponents are G, M, T and \u03a0DB. Then, if \u02c6a is in- valid in St, LiLi only updates IS[top] with (d, St) and returns IS[top] to RL-model. In this process, LiLi adds experience (St, a0, r, St) in expd and then, replaces IS[top] with (d, St).",
            "In this process, LiLi adds experience (St, a0, r, St) in expd and then, replaces IS[top] with (d, St). If \u02c6a is valid in St, LiLi \ufb01rst sets the next state St+1 = St and per- forms a sequence of operations O(\u02c6a) based on \u02c6a (discussed below). Unless speci\ufb01ed, in O(\u02c6a), LiLi always monitors \u03b4IL(d) and if \u03b4IL(d) becomes 0, LiLi sets St+1[ILO] = 1. Also, whenever LiLi asks the user a query, \u03b4IL(d) is decremented by 1. Once O(\u02c6a) ends, LiLi updates IS[top] with (d, St+1) and returns IS[top] to RL-model for choos- ing the next action. In O(a0), LiLi searches s, r, t in G and sets appropriate bits in St+1 (see Table 2).",
            "In O(a0), LiLi searches s, r, t in G and sets appropriate bits in St+1 (see Table 2). If r was un- known before and is just added to G or is in the bottom \u03c1% (see Table 1 for \u03c1) of T 8, LiLi ran- domly sets QRF = 0 with probability \u03b2. If d is a clue and s, t \u2208G, LiLi updates KS with triple td, where (s, r, t) and (t, r\u22121, s) gets added to G and M[r, (s, t)], M[r\u22121, (s, t)] are set as 1. In O(a1), LiLi asks the user to provide a clue (+ve instance) for r and corrupts s and t of the clue once at a time, to generate -ve instances by sampling nodes from G. These instances help in training prediction model for r while executing a5.",
            "In O(a1), LiLi asks the user to provide a clue (+ve instance) for r and corrupts s and t of the clue once at a time, to generate -ve instances by sampling nodes from G. These instances help in training prediction model for r while executing a5. In O(a2), LiLi selects an incomplete path \u03c0\u2217 from \u03a0DB to formulate MLQ, such that \u03c0\u2217 \u2208 \u03a0d is most frequently ob- served for r and sim(e\u03c0\u2217 ij ) is high, given by \u03c0\u2217= arg max \u03c0\u2208\u03a0d [loge\u03a0DB[(r, \u03c0, e\u03c0 ij)] \u2217sim(e\u03c0 ij)]. Here, sim(e\u03c0 ij) denotes the contextual similarity (Mazumder and Liu, 2017) of entity-pair e\u03c0 ij. If sim(e\u03c0 ij) is high, e\u03c0 ij is more likely to possess a relation between them and so, is a good candidate 8LiLi selects \u03c1% tasks from T for which it has performed poorly (evaluated on validation data in our case) and acquires clue with \u03b2 probability (while processing test data).",
            "This helps in improving skills of LiLi continuously on past poorly learned tasks. for formulating MLQ. When the user does not respond to MLQ (or CLQ in O(a3)), the guessing mechanism is used, which works as follows: Since contextual similarity of entity-pairs is highly cor- related with their class labels (Mazumder and Liu, 2017), LiLi divides the similarity range [-1, 1] into three segments, using a low (l) and high (h) similarity threshold and replaces the missing link with l in \u03c0\u2217to make it complete as follows: If h \u2265sim(e\u03c0\u2217 ij ) \u2265l, l= \u201c@-LooselyRelatedTo-@\u201d; else if sim(e\u03c0\u2217 ij ) \u2264l, l=\u201c@-NotRelatedTo-@\u201d; Otherwise, l=\u201c@-RelatedTo-@\u201d. In O(a3), LiLi asks CLQs for connecting un- known entities s and\/or t with G by selecting the most contextually relevant node (wrt s, t) from G, given by link e\u2217= arg max e\u2032\u2208G Relv(e\u2032, s, t)9.",
            "We adopt the contextual relevance idea in (Mazumder and Liu, 2017) which is computed us- ing word embedding (Mikolov et al., 2013)10 In O(a4), LiLi extracts path features \u03a0d be- tween (s, t) and updates \u03a0DB with incomplete features from \u03a0d. LiLi always trains the predic- tion model with complete features \u03a0cp d and once |\u03a0cp d | = \u03b4\u03c0 or \u03b4IL(d) = 0, LiLi stops asking MLQs. Thus, in both O(a4) and O(a2), LiLi al- ways monitors \u03a0d to check for the said require- ments and sets CPF to control interactions. In O(a5), if LiLi wins the episode, it adds d in one of data buffers Dmd based on its mode md. E.g., if md = T or C, d is used for training and added to Dtr. Similarly validation buffer Dval and evaluation buffer Deval are populated. If |Deval| > 0, LiLi invokes the prediction model for r11. Lifelong Relation Prediction.",
            "E.g., if md = T or C, d is used for training and added to Dtr. Similarly validation buffer Dval and evaluation buffer Deval are populated. If |Deval| > 0, LiLi invokes the prediction model for r11. Lifelong Relation Prediction. Given a rela- tion r, LiLi uses Dtr and Dval (see O(a5)) to train a prediction model (say, Fr) with parame- ters \u0398r. For a unknown r, the clue instances get stored in Dtr and |Dval| = 0. Thus, LiLi popu- lates Dval by taking 10% (see \u00a74) of the instances from Dtr and starts the training. For d \u2208Dtr, LiLi uses a LSTM (Hochreiter and Schmidhuber, 1997) to compose the vector representation of 9If # nodes in G is very large, a candidate set for e\u2032 is sampled for computing e\u2217.",
            "For d \u2208Dtr, LiLi uses a LSTM (Hochreiter and Schmidhuber, 1997) to compose the vector representation of 9If # nodes in G is very large, a candidate set for e\u2032 is sampled for computing e\u2217. 10Although s and t may be unknown to G, to avoid un- necessary complexity, we assume that LiLi has access to em- bedding vectors for all entities (known and unknown) in our datasets. In practice, we can update the embedding model continuously by fetching documents from the Web for un- known entities. 11We invoke the prediction model only when all instances for r are populated in the data buffers to enable batch pro- cessing.",
            "each feature \u03c0 \u2208\u03a0d as v\u03c0 and vector represen- tation of r as vr. Next, LiLi computes the pre- diction value, P(r|s, t) as sigmoid of the mean cosine similarity of all features and r, given by P(r|s, t) = sigmoid( 1 |\u03a0d| P \u03c0\u2208\u03a0d cos(vr, v\u03c0)) and maximize the log-likelihood of Dtr for training. Once Fr is trained, LiLi updates T [r] using Dval. We also train an inverse model for r, Fr\u22121 by re- versing the path features in Dtr and Dval which help in lifelong learning (discussed below). Unlike (Neelakantan et al., 2015; Das et al., 2016), while predicting the label for d \u2208Deval, we compute a relation-speci\ufb01c prediction threshold \u00b5r corre- sponding to r using Dval as: \u00b5r = 1 2[\u00b5+ r + \u00b5\u2212 r ] and infer d as +ve if P(r|h, t) \u2265\u00b5r and -ve other- wise.",
            "Here, \u00b5+ r (\u00b5\u2212 r ) is the mean prediction value for all +ve (-ve) examples in Dval. Models trained on a few examples (e.g., clues acquired for unknown r) with randomly initial- ized weights often perform poorly due to under- \ufb01tting. Thus, we transfer knowledge (weights) from the past most similar (wrt r) task in a life- long learning manner (Chen and Liu, 2016). LiLi uses M to \ufb01nd the past most similar task for r as follows: LiLi computes trancated SVD of M as Mk = Uk\u03a3kV T k and then, the similarity ma- trix Msim = Uk\u03a3k\u03a3T k U T k . Msim(r, r\u2032) pro- vides the similarity between relations r and r\u2032 in G. Thus, LiLi chooses a source relation rs = arg max r\u2032\u2208Rtr Msim(r, r\u2032) to transfer weights. Here, Rtr is the set of all r and r\u22121 for which LiLi has already learned a prediction model.",
            "Here, Rtr is the set of all r and r\u22121 for which LiLi has already learned a prediction model. Now, if Msim(r, rs) \u22640 or Rtr = \u03c6, LiLi randomly ini- tializes the weights \u0398r for Fr and proceeds with the training. Otherwise, LiLi uses \u0398rs as initial weights and \ufb01ne-tunes \u0398r with a low learning rate. A Running Example. Considering the exam- ple shown in Figure 1, LiLi works as follows: \ufb01rst, LiLi executes a0 and detects that the source entity \u201cObama\u201d and query relation \u201cCitizenOf\u201d are un- known. Thus, LiLi executes a1 to acquire clue (SF1) for \u201cCitizenOf\u201d and pushes the clue (+ve example) and two generated -ve examples into IS. Once the clues are processed and a predic- tion model is trained for \u201cCitizenOf\u201d by formu- lating separate strategies for them, LiLi becomes aware of \u201cCitizenOf\u201d. Now, as the clues have al- ready been popped from IS, the query becomes IS[top] and the strategy formulation process for the query resumes.",
            "Now, as the clues have al- ready been popped from IS, the query becomes IS[top] and the strategy formulation process for the query resumes. Next, LiLi asks user to pro- Table 4: Dataset statistics [kwn = known, unk = unknown] Freebase (FB) WordNet (WN) # Rels (Korg \/KB ) 1,345 \/ 1,248 18 \/ 14 # Entities (Korg \/KB) 13, 871 \/ 12, 306 13, 595 \/ 12, 363 # Triples (Korg \/KB) 854, 362 \/ 529,622 107, 146 \/ 58, 946 # Test Rels (kwn \/ unk) 50 (38 \/ 12) 18 (14 \/ 4) Avg. # train \/ valid hline \/ test instances\/rel. 1715 \/ 193 \/ 557 994 \/ 109 \/ 326 Entity statistics (Avg.",
            "# train \/ valid hline \/ test instances\/rel. 1715 \/ 193 \/ 557 994 \/ 109 \/ 326 Entity statistics (Avg. %) train valid test train valid test only source (s) unk 15.5 15.8 15.6 12.4 10.4 19.0 only target (t) unk 13.0 12.7 13.4 14.2 15.6 13.8 both s and t unk 2.9 3.3 2.8 3.6 3.6 6.2 vide a connecting link for \u201cObama\u201d by perform- ing a3. Now, the query entities and relation being known, LiLi enumerates paths between \u201cObama\u201d and \u201cUSA\u201d by performing a4. Let an extracted path be \u201cObama \u2212BornIn \u2192Honolulu \u2212 @\u2212? \u2212@ \u2192Hawaii \u2212StateOf \u2192USA\u201d with missing link between (Honolulu, Hawaii). LiLi asks the user to \ufb01ll the link by performing a2 and then, extracts the complete feature \u201cBornIn \u2192 CapitalOfState \u2192StateOf\u201d.",
            "\u2212@ \u2192Hawaii \u2212StateOf \u2192USA\u201d with missing link between (Honolulu, Hawaii). LiLi asks the user to \ufb01ll the link by performing a2 and then, extracts the complete feature \u201cBornIn \u2192 CapitalOfState \u2192StateOf\u201d. The feature set is then fed to the prediction model and inference is made as a result of a5. Thus, the formulated inference strategy is: \u201c\u27e8a0, a1, a3, a4, a2, a5\u27e9\u201d. 4 Experiments We now evaluate LiLi in terms of its predictive performance and strategy formulation abilities. Data: We use two standard datasets (see Table 4): (1) Freebase FB15k12, and (2) WordNet12. Using each dataset, we build a fairly large graph and use it as the original KB (Korg) for evaluation. We also augment Korg with inverse triples (t, r\u22121, s) for each (s, r, t) following existing KBC methods. Parameter Settings.",
            "Using each dataset, we build a fairly large graph and use it as the original KB (Korg) for evaluation. We also augment Korg with inverse triples (t, r\u22121, s) for each (s, r, t) following existing KBC methods. Parameter Settings. Unless speci\ufb01ed, the em- pirically set parameters (see Table 1) of LiLi are: \u03b1 = 0.8, \u03b3 = 0.9, \u03b4IL = 5, \u03b4\u03c0 = 3, \u03b7\u03c0 = 7, \u03b7w = 20, l = 0.07, h = 0.2, k = 300, \u03b2 = 0.5, \u03c1 = 25%. For training RL-model with \u01eb-greedy strategy, we use \u01ebstart = 1.0, \u01ebend = 0.1, pre- training steps=50000. We used Keras deep learn- ing library to implement and train the prediction model. We set batch-size as 128, max.",
            "We used Keras deep learn- ing library to implement and train the prediction model. We set batch-size as 128, max. training epoch as 150, dropout as 0.2, hidden units and embedding size as 300 and learning rate as 5e-3 which is reduced gradually on plateau with fac- tor 0.5 and patience 5. Adam optimizer and early stopping were used in training. We also shuf\ufb02e Dtr in each epoch and adjust class weights in- versely proportional to class frequencies in Dtr. Labeled Dataset Generation and Simulated User Creation. We create a simulated user for 12 https:\/\/everest.hds.utc.fr\/doku.php?id=en:smemlj12",
            "Table 5: Inference strategies formulated by LiLi (ordered by frequency). \u03b4IL = 0, \u03b4\u03c0 = 3 [C: 0.47] \u27e8a0, a1, a4, a2, a2, a5\u27e9 \u03b4IL = 5, \u03b4\u03c0 = 1 [ C: 1.0] \u03b4IL = 5, \u03b4\u03c0 = 3 [C: 1.0] \u27e8a0, a4, a5\u27e9 \u27e8a0, a3, a4, a2, a2, a5\u27e9 \u27e8a0, a4, a2, a5\u27e9 \u27e8a0, a4, a2, a2, a2, a5\u27e9 \u27e8a0, a3, a4, a5\u27e9 \u03b4IL = 1, \u03b4\u03c0 = 3 [ C: 0.97] \u27e8a0, a4, a5\u27e9 \u27e8a0, a1, a4, a2, a5\u27e9 \u27e8a0, a4, a2, a5\u27e9 \u27e8a0, a1, a4, a5\u27e9 \u27e8a0, a4, a2, a5\u27e9 \u27e8a0,",
            "a5\u27e9 \u27e8a0, a1, a4, a2, a5\u27e9 \u27e8a0, a4, a2, a5\u27e9 \u27e8a0, a1, a4, a5\u27e9 \u27e8a0, a4, a2, a5\u27e9 \u27e8a0, a3, a4, a2, a5\u27e9 \u27e8a0, a4, a5\u27e9 \u27e8a0, a4, a2, a2, a5\u27e9 \u27e8a0, a1, a3, a4, a2, a2, a2, a5\u27e9 \u27e8a0, a1, a4, a5\u27e9 \u27e8a0, a1, a4, a2, a5\u27e9 \u27e8a0, a3, a4, a2, a5\u27e9 \u27e8a0, a1, a4, a2, a2, a2, a5\u27e9 \u27e8a0, a1, a3, a4, a2, a2, a5\u27e9 \u27e8a0, a3, a4, a5\u27e9 \u27e8a0, a3, a4, a5\u27e9 \u27e8a0, a1, a4,",
            "a2, a5\u27e9 \u27e8a0, a1, a3, a4, a2, a2, a5\u27e9 \u27e8a0, a3, a4, a5\u27e9 \u27e8a0, a3, a4, a5\u27e9 \u27e8a0, a1, a4, a5\u27e9 \u27e8a0, a4, a5\u27e9 \u27e8a0, a1, a3, a4, a2, a5\u27e9 \u27e8a0, a4, a5\u27e9 \u27e8a0, a1, a3, a4, a2, a5\u27e9 \u27e8a0, a3, a4, a5\u27e9 \u27e8a0, a3, a4, a2, a2, a2, a5\u27e9 \u27e8a0, a3, a1, a4, a2, a2, a5\u27e9 \u03b4IL = 3, \u03b4\u03c0 = 3 [C: 1.0] \u27e8a0, a3, a1, a4, a2, a5\u27e9 \u27e8a0, a3, a1, a4, a2, a5\u27e9 \u27e8a0, a3, a4, a2,",
            "\u03b4\u03c0 = 3 [C: 1.0] \u27e8a0, a3, a1, a4, a2, a5\u27e9 \u27e8a0, a3, a1, a4, a2, a5\u27e9 \u27e8a0, a3, a4, a2, a5\u27e9 \u27e8a0, a3, a1, a4, a2, a2, a2, a5\u27e9 \u27e8a0, a4, a2, a2, a2, a5\u27e9 \u27e8a0, a1, a4, a5\u27e9 \u27e8a0, a1, a3, a4, a2, a5\u27e9 \u27e8a0, a3, a4, a2, a2, a5\u27e9 \u27e8a0, a3, a1, a4, a5\u27e9 \u27e8a0, a4, a2, a5\u27e9 \u27e8a0, a3, a1, a4, a5\u27e9 \u27e8a0, a3, a1, a4, a5\u27e9 \u27e8a0, a1, a4, a2, a2, a5\u27e9 \u27e8a0, a3, a1, a4,",
            "a3, a1, a4, a5\u27e9 \u27e8a0, a3, a1, a4, a5\u27e9 \u27e8a0, a1, a4, a2, a2, a5\u27e9 \u27e8a0, a3, a1, a4, a2, a5\u27e9 \u27e8a0, a4, a2, a2, a5\u27e9 \u27e8a0, a1, a3, a4, a5\u27e9 \u27e8a0, a1, a3, a4, a5\u27e9 \u27e8a0, a1, a4, a2, a5\u27e9 \u27e8a0, a1, a3, a4, a5\u27e9 Table 6: Comparison of predictive performance of various versions of LiLi [kwn = known, unk = unknown, all = overall]. KB Test Rel type Avg. +ve F1 Score Avg. MCC Single Sep F-th BG w\/o PTS LiLi Single Sep F-th BG w\/o PTS LiLi FB kwn 0.3796 0.5741 0.5069 0.5643 0.5547 0.5859 0.0937 0.",
            "MCC Single Sep F-th BG w\/o PTS LiLi Single Sep F-th BG w\/o PTS LiLi FB kwn 0.3796 0.5741 0.5069 0.5643 0.5547 0.5859 0.0937 0.2638 0.2382 0.2443 0.2573 0.2763 unk 0.5477 0.5425 0.4876 0.5398 0.5421 0.5567 0.2175 0.1752 0.1802 0.1664 0.1748 0.2119 all 0.4199 0.5665 0.5023 0.5584 0.5517 0.5789 0.1234 0.2425 0.2243 0.2256 0.2375 0.2609 WN kwn 0.3846 0.5851 0.5817 0.5554 0.6083 0.6343 0.2494 0.3838 0.",
            "2243 0.2256 0.2375 0.2609 WN kwn 0.3846 0.5851 0.5817 0.5554 0.6083 0.6343 0.2494 0.3838 0.3603 0.2980 0.4159 0.4096 unk 0.5732 0.5026 0.5861 0.5694 0.5539 0.5871 0.3348 0.2501 0.3123 0.3148 0.2667 0.3387 all 0.4265 0.5668 0.5827 0.5586 0.5962 0.6238 0.2684 0.3541 0.3496 0.3017 0.3828 0.3939 each KB to evaluate LiLi13. We create the la- beled datasets, the simulated user\u2019s knowledge base (Ku), and the base KB (KB) from Korg.",
            "3541 0.3496 0.3017 0.3828 0.3939 each KB to evaluate LiLi13. We create the la- beled datasets, the simulated user\u2019s knowledge base (Ku), and the base KB (KB) from Korg. KB used as the initial KB graph (G0) of LiLi. We followed (Mazumder and Liu, 2017) for la- beled dataset generation. For Freebase, we found 86 relations with \u22651000 triples and randomly se- lected 50 from various domains. We randomly shuf\ufb02e the list of 50 relations, select 25% of them as unknown relations and consider the rest (75%) as known relations. For each known relation r, we randomly shuf\ufb02e the list of distinct triples for r, choose 1000 triples and split them into 60% train- ing, 10% validation and 20% test. Rest 10% along with the leftover (not included in the list of 1000) triples are added to Ku.",
            "Rest 10% along with the leftover (not included in the list of 1000) triples are added to Ku. For each unknown rela- tion r, we remove all triples of r from Korg and add them to Ku. In this process, we also randomly choose 20% triples as test instances for unknown r which are excluded from Ku. Note that, now Ku has at least 10% of chosen triples for each r (known and unknown) and so, user is always able to provide clues for both cases. For each labeled dataset, we randomly choose 10% of the entities present in dataset triples, remove triples involving those entities from Korg and add to Ku. At this point, Korg gets reduced to KB and is used as G0 for LiLi. The dataset stats in Table 4 shows that the base KB (60% triples of Korg) is highly sparse (compared to original KB) which makes the in- ference task much harder. WordNet dataset being small, we select all 18 relations for evaluation and 13Crowdsourced-based training and evaluation is expen- sive and time consuming as user-interaction is needed in training.",
            "WordNet dataset being small, we select all 18 relations for evaluation and 13Crowdsourced-based training and evaluation is expen- sive and time consuming as user-interaction is needed in training. create labeled dataset, Ku and KB following Free- base. Although the user may provide clues 100% of the time, it often cannot respond to MLQs and CLQs (due to lack of required triples\/facts). Thus, we further enrich Ku with external KB triples14. Given a relation r and an observed triple (s, r, t) in training or testing, the pair (s, t) is regarded as a +ve instance for r. Following (Wang et al., 2016a), for each +ve instance (s, t), we gener- ate two negative ones, one by randomly corrupting the source s, and the other by corrupting the target t. Note that, the test triples are not in KB or Ku and none of the -ve instances overlap with the +ve ones. Baselines. As none of the existing KBC methods can solve the OKBC problem, we choose various versions of LiLi as baselines.",
            "Baselines. As none of the existing KBC methods can solve the OKBC problem, we choose various versions of LiLi as baselines. Single: Version of LiLi where we train a single prediction model F for all test relations. Sep: We do not transfer (past learned) weights for initializing Fr, i.e., we disable LL. F-th): Here, we use a \ufb01xed prediction thresh- old 0.5 instead of relation-speci\ufb01c threshold \u00b5r. BG: The missing or connecting links (when the user does not respond) are \ufb01lled with \u201c@- RelatedTo-@\u201d blindly, no guessing mechanism. w\/o PTS: LiLi does not ask for additional clues via past task selection for skillset improvement. Evaluation Metrics. To evaluate the strategy for- mulation ability, we introduce a measure called Coverage(C), de\ufb01ned as the fraction of total query data instances, for which LiLi has successfully 14Due to fair amount of entity overlapping, we choose NELL for enriching Ku in case of Freebase and ConceptNet for enriching Ku in case of WordNet.",
            "formulated strategies that lead to winning. If LiLi wins on all episodes for a given dataset, C is 1.0. To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score. 4.1 Results and Analysis Evaluation-I: Strategy Formulation Ability. Ta- ble 5 shows the list of inference strategies formu- lated by LiLi for various \u03b4IL and \u03b4\u03c0, which control the strategy formulation of LiLi. When \u03b4IL = 0, LiLi cannot interact with user and works like a closed-world method. Thus, C drops signi\ufb01cantly (0.47). When \u03b4IL = 1, i.e. with only one in- teraction per query, LiLi acquires knowledge well for instances where either of the entities or rela- tion is unknown. However, as one unknown entity may appear in multiple test triples, once the entity becomes known, LiLi doesn\u2019t need to ask for it again and can perform inference on future triples causing signi\ufb01cant increase in C (0.97).",
            "However, as one unknown entity may appear in multiple test triples, once the entity becomes known, LiLi doesn\u2019t need to ask for it again and can perform inference on future triples causing signi\ufb01cant increase in C (0.97). When \u03b4IL = 3, LiLi is able to perform inference on all instances and C becomes 1. For \u03b4\u03c0 = 1, LiLi uses a2 only once (as only one MLQ satis\ufb01es \u03b4\u03c0) com- pared to \u03b4\u03c0 = 3. In summary, LiLi\u2019s RL-model can effectively formulate query-speci\ufb01c inference strategies (based on speci\ufb01ed parameter values). Evaluation-II: Predictive Performance. Ta- ble 6 shows the comparative performance of LiLi with baselines. To judge the overall improve- ments, we performed paired t-test considering +ve F1 scores on each relation as paired data. Con- sidering both KBs and all relation types, LiLi out- performs Sep with p < 0.1.",
            "To judge the overall improve- ments, we performed paired t-test considering +ve F1 scores on each relation as paired data. Con- sidering both KBs and all relation types, LiLi out- performs Sep with p < 0.1. If we set \u03b2 = 0.05 (training with very few clues), LiLi outperforms Sep with p < 0.05 on Freebase considering MCC. Thus, the lifelong learning mechanism is effective in transferring helpful knowledge. Single model performs better than Sep for unknown relations due to the sharing of knowledge (weights) across tasks. However, for known relations, performance drops because, as a new relation arrives to the sys- tem, old weights get corrupted and catastrophic forgetting occurs. For unknown relations, as the relations are evaluated just after training, there is no chance for catastrophic forgetting.",
            "However, for known relations, performance drops because, as a new relation arrives to the sys- tem, old weights get corrupted and catastrophic forgetting occurs. For unknown relations, as the relations are evaluated just after training, there is no chance for catastrophic forgetting. The perfor- mance improvement (p < 0.05) of LiLi over F- th on Freebase signi\ufb01es that the relation-speci\ufb01c threshold \u00b5r works better than \ufb01xed threshold 0.5 because, if all prediction values for test instances lie above (or below) 0.5, F-th predicts all instances as +ve (-ve) which degrades its performance. Due Table 7: LiLi\u2019s performance on FB by varying \u03b2. Rel Type \u03b2 = 0.05 \u03b2 = 0.25 \u03b2 = 0.5 F(+) F(+) F(+) known 0.5796 0.5820 0.5859 unknown 0.5231 0.5414 0.5567 overall 0.5660 0.5722 0.5789 Table 8: Performance of LiLi on user\u2019s responses.",
            "KB Rel Type No Response to CLQs and MLQs Response to CLQs and MLQs F(+) MCC F(+) MCC FB known 0.5823 0.2775 0.5859 0.2763 unknown 0.5529 0.2049 0.5567 0.2119 overall 0.5753 0.2601 0.5789 0.2609 WN known 0.5990 0.3590 0.6343 0.4096 unknown 0.5952 0.3457 0.5871 0.3387 overall 0.5982 0.3561 0.6238 0.3939 to the utilization of contextual similarity (highly correlated with class labels) of entity-pairs, LiLi\u2019s guessing mechanism works better (p < 0.05) than blind guessing (BG). The past task selection mechanism of LiLi also improves its performance over w\/o PTS, as it acquires more clues during testing for poorly performed tasks (evaluated on validation set).",
            "The past task selection mechanism of LiLi also improves its performance over w\/o PTS, as it acquires more clues during testing for poorly performed tasks (evaluated on validation set). For Freebase, due to a large num- ber of past tasks [9 (25% of 38)], the performance difference is more signi\ufb01cant (p < 0.01). For WordNet, the number is relatively small [3 (25% of 14)] and hence, the difference is not signi\ufb01cant. Evaluation-III: User Interaction vs. Perfor- mance. Table 7 shows the results of LiLi by vary- ing clue acquisition rate (\u03b2). We use Freebase for tuning \u03b2 due to its higher number of unknown test relations compared to WordNet. LiLi\u2019s perfor- mance improves signi\ufb01cantly as it acquires more clues from the user. The results on \u03b2 = 0.5 out- performs (p < 0.05) that on \u03b2 = 0.05. Table 8 shows the results of LiLi on user responses to MLQ\u2019s and CLQ\u2019s.",
            "The results on \u03b2 = 0.5 out- performs (p < 0.05) that on \u03b2 = 0.05. Table 8 shows the results of LiLi on user responses to MLQ\u2019s and CLQ\u2019s. Answering MLQ\u2019s and CLQ\u2019s is very hard for simulated users (unlike crowd- sourcing) as often Ku lacks the required triple. Thus, we attempt to analyze how the performance is effected if the user does not respond at all. The results show a clear trend in overall performance improvement when the user responds. However, the improvement is not signi\ufb01cant as the simulated user\u2019s query satisfaction rate (1% MLQs and 10% CLQs) is very small. But, the analysis shows the effectiveness of LiLi\u2019s guessing mechanism and continual learning ability that help in achieving avg. +ve F1 of 0.57 and 0.62 on FB and WN re- spectively with minimal participation of the user.",
            "5 Conclusion In this paper, we are interested in building a generic engine for continuous knowledge learning in human-machine conversations. We \ufb01rst showed that the problem underlying the engine can be for- mulated as an open-world knowledge base com- pletion (OKBC) problem. We then proposed an lifelong interactive learning and inference (LiLi) approach to solving the OKBC problem. OKBC is a generalization of KBC. LiLi solves the OKBC problem by \ufb01rst formulating a query-speci\ufb01c in- ference strategy using RL and then executing it to solve the problem by interacting with the user in a lifelong learning manner. Experimental results showed the effectiveness of LiLi in terms of both predictive quality and strategy formulation ability. We believe that a system with the LiLi approach can serve as a knowledge learning engine for con- versations. Our future work will improve LiLi to make more accurate. Acknowledgments This work was supported in part by National Science Foundation (NSF) under grant no. IIS- 1407927 and IIS-1650900, and a gift from Huawei Technologies Co Ltd.",
            "Our future work will improve LiLi to make more accurate. Acknowledgments This work was supported in part by National Science Foundation (NSF) under grant no. IIS- 1407927 and IIS-1650900, and a gift from Huawei Technologies Co Ltd. References David Ameixa, Luisa Coheur, Pedro Fialho, and Paulo Quaresma. 2014. Luke, i am your father: dealing with out-of-domain requests by using movies subti- tles. In International Conference on Intelligent Vir- tual Agents. Rafael E Banchs and Haizhou Li. 2012. Iris: a chat- oriented dialogue system based on the vector space model. In Proceedings of the ACL 2012 System Demonstrations. pages 37\u201342. Antoine Bordes, Nicolas Usunier, Alberto Garcia- Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi- relational data. In NIPS. Antoine Bordes and Jason Weston. 2016. Learn- ing end-to-end goal-oriented dialog.",
            "2013. Translating embeddings for modeling multi- relational data. In NIPS. Antoine Bordes and Jason Weston. 2016. Learn- ing end-to-end goal-oriented dialog. arXiv preprint arXiv:1605.07683 . Antoine Bordes, Jason Weston, Ronan Collobert, and Yoshua Bengio. 2011. Learning structured embed- dings of knowledge bases. In AAAI. Haitham Bou Ammar, Eric Eaton, Jose Marcio Luna, and Paul Ruvolo. 2015. Autonomous Cross-Domain Knowledge Transfer in Lifelong Policy Gradient Re- inforcement Learning. In AAAI. Zhiyuan Chen and Bing Liu. 2013. Lifelong Learn- ing for Sentiment Classi\ufb01cation. In ACL-2015, short paper. Zhiyuan Chen and Bing Liu. 2014. Topic Modeling us- ing Topics from Many Domains, Lifelong Learning and Big Data. In ICML. Zhiyuan Chen and Bing Liu. 2016. Lifelong Machine learning.",
            "Zhiyuan Chen and Bing Liu. 2014. Topic Modeling us- ing Topics from Many Domains, Lifelong Learning and Big Data. In ICML. Zhiyuan Chen and Bing Liu. 2016. Lifelong Machine learning. Morgan and Claypool Publishers. Rajarshi Das, Arvind Neelakantan, David Belanger, and Andrew McCallum. 2016. Chains of reasoning over entities, relations, and text using recurrent neu- ral networks. arXiv preprint arXiv:1607.01426 . Matt Gardner and Tom M Mitchell. 2015. Ef\ufb01cient and expressive knowledge base completion using sub- graph feature extraction. In EMNLP. Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and Michel Galley. 2017. A knowledge-grounded neural conversation model. arXiv preprint arXiv:1702.01932 . Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997.",
            "2017. A knowledge-grounded neural conversation model. arXiv preprint arXiv:1702.01932 . Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997. Long short-term memory. Neural computation . Ni Lao, Einat Minkov, and William W Cohen. 2015. Learning relational features with backward random walks. In ACL. Ni Lao, Tom Mitchell, and William W Cohen. 2011. Random walk inference and learning in a large scale knowledge base. In EMNLP. Phong Le, Marc Dymetman, and Jean-Michel Ren- ders. 2016. Lstm-based mixture-of-experts for knowledge-aware dialogues. arXiv preprint arXiv:1605.01652 . Jiwei Li, Alexander H Miller, Sumit Chopra, Marc\u2019Aurelio Ranzato, and Jason Weston. 2016a. Dialogue learning with human-in-the-loop. arXiv preprint arXiv:1611.09823 .",
            "Jiwei Li, Alexander H Miller, Sumit Chopra, Marc\u2019Aurelio Ranzato, and Jason Weston. 2016a. Dialogue learning with human-in-the-loop. arXiv preprint arXiv:1611.09823 . Jiwei Li, Alexander H Miller, Sumit Chopra, Marc\u2019Aurelio Ranzato, and Jason Weston. 2016b. Learning through dialogue interactions. arXiv preprint arXiv:1612.04936 . Jiwei Li, Will Monroe, and Dan Jurafsky. 2017a. Data distillation for controlling speci\ufb01city in dialogue generation. arXiv preprint arXiv:1702.06703 . Jiwei Li, Will Monroe, Tianlin Shi, Alan Ritter, and Dan Jurafsky. 2017b. Adversarial learning for neural dialogue generation. arXiv preprint arXiv:1701.06547 . Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. 2015.",
            "2017b. Adversarial learning for neural dialogue generation. arXiv preprint arXiv:1701.06547 . Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. 2015. The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dia- logue systems. arXiv preprint arXiv:1506.08909 .",
            "Sahisnu Mazumder and Bing Liu. 2017. Context- aware path ranking for knowledge base completion. In IJCAI. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositional- ity. In NIPS. Tom M Mitchell, William W Cohen, Partha Pra- tim Talukdar, Justin Betteridge, Andrew Carlson, Matthew Gardner, Bryan Kisiel, Jayant Krishna- murthy, et al. 2015. Never ending learning. In AAAI. Arvind Neelakantan, Benjamin Roth, and Andrew Mc- Callum. 2015. Compositional vector space mod- els for knowledge base completion. arXiv preprint arXiv:1504.06662 . Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. 2015. A review of relational machine learning for knowledge graphs. arXiv preprint arXiv:1503.00759 .",
            "Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. 2015. A review of relational machine learning for knowledge graphs. arXiv preprint arXiv:1503.00759 . Paul Ruvolo and Eric Eaton. 2013. ELLA: An ef\ufb01cient lifelong learning algorithm. In ICML. Iulian Vlad Serban, Ryan Lowe, Peter Henderson, Lau- rent Charlin, and Joelle Pineau. 2015. A survey of available corpora for building data-driven dialogue systems. arXiv preprint arXiv:1512.05742 . Baoxu Shi and Tim Weninger. 2017. Open-world knowledge graph completion. arXiv preprint arXiv:1711.03438 . Lei Shu, Hu Xu, and Bing Liu. 2017. Lifelong learning crf for supervised aspect extraction. In ACL. Kristina Toutanova. 2015. Observed versus latent fea- tures for knowledge base and text inference. In ACL Workshop on Continuous Vector Space Models and their Compositionality.",
            "2017. Lifelong learning crf for supervised aspect extraction. In ACL. Kristina Toutanova. 2015. Observed versus latent fea- tures for knowledge base and text inference. In ACL Workshop on Continuous Vector Space Models and their Compositionality. Oriol Vinyals and Quoc Le. 2015. A neural conversa- tional model. arXiv preprint arXiv:1506.05869 . Quan Wang, Jing Liu, Yuanfei Luo, Bin Wang, and C Lin. 2016a. Knowledge base completion via cou- pled path ranking. In ACL. Sida I Wang, Samuel Ginn, Percy Liang, and Christoper D Manning. 2017. Naturalizing a pro- gramming language via interactive learning. arXiv preprint arXiv:1704.06956 . Sida I Wang, Percy Liang, and Christopher D Manning. 2016b. Learning language games through interac- tion. arXiv preprint arXiv:1606.02447 . Christopher JCH Watkins and Peter Dayan. 1992.",
            "Sida I Wang, Percy Liang, and Christopher D Manning. 2016b. Learning language games through interac- tion. arXiv preprint arXiv:1606.02447 . Christopher JCH Watkins and Peter Dayan. 1992. Q- learning. In Machine learning. Robert West, Evgeniy Gabrilovich, Kevin Murphy, Shaohua Sun, Rahul Gupta, and Dekang Lin. 2014. Knowledge base completion via search-based ques- tion answering. In WWW. Jason E Weston. 2016. Dialog-based language learn- ing. In NIPS. Mark Woodward and Chelsea Finn. 2017. Active one- shot learning. arXiv preprint arXiv:1702.06559 . Chen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang, Ming Zhou, and Wei-Ying Ma. 2017. Topic aware neural response generation. In AAAI. Haichao Zhang, Haonan Yu, and Wei Xu. 2017. Listen, interact and talk: Learning to speak via interaction.",
            "2017. Topic aware neural response generation. In AAAI. Haichao Zhang, Haonan Yu, and Wei Xu. 2017. Listen, interact and talk: Learning to speak via interaction. arXiv preprint arXiv:1705.09906 ."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1802.06024.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 13847.000564575195,
    "avg_doclen_est": 175.2784881591797
}
