[
  "Natural Language Interactions in Autonomous Vehicles: Intent Detection and Slot Filling from Passenger Utterances Eda Okur, Shachi H Kumar, Saurav Sahay, Asli Arslan Esme, and Lama Nachman Intel Labs, Anticipatory Computing Lab, USA {eda.okur, shachi.h.kumar, saurav.sahay, asli.arslan.esme, lama.nachman}@intel.com Abstract. Understanding passenger intents and extracting relevant slots are important building blocks towards developing contextual dialogue systems for natural interactions in autonomous vehicles (AV). In this work, we explored AMIE (Automated-vehicle Multi-modal In-cabin Ex- perience), the in-cabin agent responsible for handling certain passenger- vehicle interactions. When the passengers give instructions to AMIE, the agent should parse such commands properly and trigger the appropriate functionality of the AV system. In our current explorations, we focused on AMIE scenarios describing usages around setting or changing the des- tination and route, updating driving behavior or speed, \ufb01nishing the trip and other use-cases to support various natural commands.",
  "In our current explorations, we focused on AMIE scenarios describing usages around setting or changing the des- tination and route, updating driving behavior or speed, \ufb01nishing the trip and other use-cases to support various natural commands. We collected a multi-modal in-cabin dataset with multi-turn dialogues between the pas- sengers and AMIE using a Wizard-of-Oz scheme via a realistic scavenger hunt game activity. After exploring various recent Recurrent Neural Net- works (RNN) based techniques, we introduced our own hierarchical joint models to recognize passenger intents along with relevant slots associ- ated with the action to be performed in AV scenarios. Our experimental results outperformed certain competitive baselines and achieved overall F1-scores of 0.91 for utterance-level intent detection and 0.96 for slot \ufb01ll- ing tasks. In addition, we conducted initial speech-to-text explorations by comparing intent/slot models trained and tested on human transcrip- tions versus noisy Automatic Speech Recognition (ASR) outputs. Finally, we compared the results with single passenger rides versus the rides with multiple passengers.",
  "In addition, we conducted initial speech-to-text explorations by comparing intent/slot models trained and tested on human transcrip- tions versus noisy Automatic Speech Recognition (ASR) outputs. Finally, we compared the results with single passenger rides versus the rides with multiple passengers. Keywords: Intent recognition \u00b7 Slot \ufb01lling \u00b7 Hierarchical joint learning \u00b7 Spoken language understanding (SLU) \u00b7 In-cabin dialogue agent. 1 Introduction One of the exciting yet challenging areas of research in Intelligent Transporta- tion Systems is developing context-awareness technologies that can enable au- tonomous vehicles to interact with their passengers, understand passenger con- arXiv:1904.10500v1  [cs.CL]  23 Apr 2019",
  "2 E. Okur et al. text and situations, and take appropriate actions accordingly. To this end, build- ing multi-modal dialogue understanding capabilities situated in the in-cabin con- text is crucial to enhance passenger comfort and gain user con\ufb01dence in AV in- teraction systems. Among many components of such systems, intent recognition and slot \ufb01lling modules are one of the core building blocks towards carrying out successful dialogue with passengers. As an initial attempt to tackle some of those challenges, this study introduce in-cabin intent detection and slot \ufb01lling models to identify passengers\u2019 intent and extract semantic frames from the natural lan- guage utterances in AV. The proposed models are developed by leveraging User Experience (UX) grounded realistic (ecologically valid) in-cabin dataset. This dataset is generated with naturalistic passenger behaviors, multiple passenger interactions, and with presence of a Wizard-of-Oz (WoZ) agent in moving vehi- cles with noisy road conditions. 1.1 Background Long Short-Term Memory (LSTM) networks [7] are widely-used for tempo- ral sequence learning or time-series modeling in Natural Language Processing (NLP).",
  "1.1 Background Long Short-Term Memory (LSTM) networks [7] are widely-used for tempo- ral sequence learning or time-series modeling in Natural Language Processing (NLP). These neural networks are commonly employed for sequence-to-sequence (seq2seq) and sequence-to-one (seq2one) modeling problems, including slot \ufb01lling tasks [11] and utterance-level intent classi\ufb01cation [5,17] which are well-studied for various application domains. Bidirectional LSTMs (Bi-LSTMs) [18] are ex- tensions of traditional LSTMs which are proposed to improve model performance on sequence classi\ufb01cation problems even further. Jointly modeling slot extrac- tion and intent recognition [5,25] is also explored in several architectures for task-speci\ufb01c applications in NLP. Using Attention mechanism [16,24] on top of RNNs is yet another recent break-through to elevate the model performance by attending inherently crucial sub-modules of given input.",
  "Using Attention mechanism [16,24] on top of RNNs is yet another recent break-through to elevate the model performance by attending inherently crucial sub-modules of given input. There exist various architectures to build hierarchical learning models [27,10,22] for document-to- sentence level, and sentence-to-word level classi\ufb01cation tasks, which are highly domain-dependent and task-speci\ufb01c. Automatic Speech Recognition (ASR) technology has recently achieved human- level accuracy in many \ufb01elds [23,20]. For spoken language understanding (SLU), it is shown that training SLU models on true text input (i.e., human transcrip- tions) versus noisy speech input (i.e., ASR outputs) can achieve varying results [9]. Even greater performance degradations are expected in more challenging and realistic setups with noisy environments, such as moving vehicles in actual traf- \ufb01c conditions. As an example, a recent work [26] attempts to classify sentences as navigation-related or not using the DARPA supported CU-Move in-vehicle speech corpus [6], a relatively old and large corpus focusing on route navigation.",
  "As an example, a recent work [26] attempts to classify sentences as navigation-related or not using the DARPA supported CU-Move in-vehicle speech corpus [6], a relatively old and large corpus focusing on route navigation. For this binary intent classi\ufb01cation task, the authors observed that detection per- formances are largely a\ufb00ected by high ASR error rates due to background noise and multi-speakers in CU-Move dataset (not publicly available). For in-cabin dialogue between car assistants and driver/passengers, recent studies explored creating a public dataset using a WoZ approach [3], and improving ASR for passenger speech recognition [4].",
  "Natural Language Interactions in AVs: Intent Detection and Slot Filling 3 A preliminary report on research designed to collect data for human-agent interactions in a moving vehicle is presented in a previous study [19], with qual- itative analysis on initial observations and user interviews. Our current study is focused on the quantitative analysis of natural language interactions found in this in-vehicle dataset [14], where we address intent detection and slot extraction tasks for passengers interacting with the AMIE in-cabin agent. Contributions. In this study, we propose a UX grounded realistic intent recog- nition and slot \ufb01lling models with naturalistic passenger-vehicle interactions in moving vehicles. Based on observed interactions, we de\ufb01ned in-vehicle intent types and re\ufb01ned their relevant slots through a data driven process. After ex- ploring existing approaches for jointly training intents and slots, we applied certain variations of these models that perform best on our dataset to support various natural commands for interacting with the car-agent.",
  "After ex- ploring existing approaches for jointly training intents and slots, we applied certain variations of these models that perform best on our dataset to support various natural commands for interacting with the car-agent. The main dif- ferences in our proposed models can be summarized as follows: (1) Using the extracted intent keywords in addition to the slots to jointly model them with utterance-level intents (where most of the previous work [27,10] only join slots and utterance-level intents, ignoring the intent keywords); (2) The 2-level hierar- chy we de\ufb01ned by word-level detection/extraction for slots and intent keywords \ufb01rst, and then \ufb01ltering-out predicted non-slot and non-intent keywords instead of feeding them into the upper levels of the network (i.e., instead of using stacked RNNs with multiple recurrent hidden layers for the full utterance [10,22], which are computationally costly for long utterances with many non-slot & non-intent- related words), and \ufb01nally using only the predicted valid-slots and intent-related keywords as an input to the second level of the hierarchy; (3) Extending joint models [5,25] to include both beginning-of-utterance and end-of-utterance tokens to leverage Bi-LSTMs (after observing that we achieved better results by doing so).",
  "We compared our intent detection and slot \ufb01lling results with the results obtained from Dialog\ufb02ow1, a commercially available intent-based dialogue sys- tem by Google, and showed that our proposed models perform better for both tasks on the same dataset. We also conducted initial speech-to-text explorations by comparing models trained and tested (10-fold CV) on human transcriptions versus noisy ASR outputs (via Cloud Speech-to-Text2). Finally, we compared the results with single passenger rides versus the rides with multiple passengers. 2 Methodology 2.1 Data Collection and Annotation Our AV in-cabin dataset includes around 30 hours of multi-modal data collected from 30 passengers (15 female, 15 male) in a total of 20 rides/sessions. In 10 sessions, single passenger was present (i.e., singletons), whereas the remaining 1 https://dialog\ufb02ow.com 2 https://cloud.google.com/speech-to-text/",
  "4 E. Okur et al. Fig. 1: AMIE In-cabin Data Collection Setup 10 sessions include two passengers (i.e., dyads) interacting with the vehicle. The data is collected \u201din the wild\u201d on the streets of Richmond, British Columbia, Canada. Each ride lasted about 1 hour or more. The vehicle is modi\ufb01ed to hide the operator and the human acting as in-cabin agent from the passengers, using a variation of WoZ approach [21]. Participants sit in the back of the car and are separated by a semi-sound proof and translucent screen from the human driver and the WoZ AMIE agent at the front. In each session, the participants were playing a scavenger hunt game by receiving instructions over the phone from the Game Master. Passengers treat the car as AV and communicate with the WoZ AMIE agent via speech commands. Game objectives require passengers to inter- act naturally with the agent to go to certain destinations, update routes, stop the vehicle, give speci\ufb01c directions regarding where to pull over or park (sometimes with gesture), \ufb01nd landmarks, change speed, get in and out of the vehicle, etc.",
  "Game objectives require passengers to inter- act naturally with the agent to go to certain destinations, update routes, stop the vehicle, give speci\ufb01c directions regarding where to pull over or park (sometimes with gesture), \ufb01nd landmarks, change speed, get in and out of the vehicle, etc. Further details of the data collection design and scavenger hunt protocol can be found in the preliminary study [19]. See Fig. 1 for the vehicle instrumentation to enhance multi-modal data collection setup. Our study is the initial work on this multi-modal dataset to develop intent detection and slot \ufb01lling models, where we leveraged from the back-driver video/audio stream recorded by an RGB cam- era (facing towards the passengers) for manual transcription and annotation of in-cabin utterances. In addition, we used the audio data recorded by Lapel 1 Audio and Lapel 2 Audio (Fig. 1) as our input resources for the ASR.",
  "In addition, we used the audio data recorded by Lapel 1 Audio and Lapel 2 Audio (Fig. 1) as our input resources for the ASR. For in-cabin intent understanding, we described 4 groups of usages to support various natural commands for interacting with the vehicle: (1) Set/Change Des- tination/Route (including turn-by-turn instructions), (2) Set/Change Driving Behavior/Speed, (3) Finishing the Trip Use-cases, and (4) Others (open/close door/window/trunk, turn music/radio on/o\ufb00, change AC/temperature, show map, etc.). According to those scenarios, 10 types of passenger intents are iden-",
  "Natural Language Interactions in AVs: Intent Detection and Slot Filling 5 Table 1: AMIE Dataset Statistics: Utterance-level Intent Types AMIE Scenario Intent Type Utterance Count Stop 317 Finishing the Trip Park 450 Use-cases PullOver 295 DropO\ufb00 281 Set/Change SetDestination 552 Destination/Route SetRoute 676 Set/Change GoFaster 265 Driving Behavior/Speed GoSlower 238 Others OpenDoor 142 (Door, Music, etc.) Other 202 Total 3418 ti\ufb01ed and annotated as follows: SetDestination, SetRoute, GoFaster, GoSlower, Stop, Park, PullOver, DropO\ufb00, OpenDoor, and Other. For slot \ufb01lling task, rele- vant slots are identi\ufb01ed and annotated as: Location, Position/Direction, Object, Time Guidance, Person, Gesture/Gaze (e.g., \u2018this\u2019, \u2018that\u2019, \u2018over there\u2019, etc.), and None/O. In addition to utterance-level intents and slots, word-level intent related keywords are annotated as Intent.",
  "), and None/O. In addition to utterance-level intents and slots, word-level intent related keywords are annotated as Intent. We obtained 1331 utterances having commands to AMIE agent from our in-cabin dataset. We expanded this dataset via the creation of similar tasks on Amazon Mechanical Turk [2] and reached 3418 utterances with intents in total. Intent and slot annotations are obtained on the transcribed utterances by majority voting of 3 annotators. Those annotation results for utterance-level intent types, slots and intent keywords can be found in Table 1 and Table 2 as a summary of dataset statistics. Table 2: AMIE Dataset Statistics: Slots and Intent Keywords Slot Type Slot Count Keyword Type Keyword Count Location 4460 Intent 5921 Position/Direction 3187 Non-Intent 25000 Person 1360 Valid-Slot 10954 Object 632 Non-Slot 19967 Time Guidance 792 Intent \u222aValid-Slot 16875 Gesture/Gaze 523 Non-Intent \u2229Non-Slot 14046 None 19967 Total 30921 Total 30921",
  "6 E. Okur et al. 2.2 Detecting Utterance-level Intent Types As a baseline system, we implemented term-frequency and rule-based mapping mechanisms from word-level intent keywords extraction to utterance-level intent recognition. To further improve the utterance-level performance, we explored various RNN architectures and developed a hierarchical (2-level) models to rec- ognize passenger intents along with relevant entities/slots in utterances. Our hierarchical model has the following 2-levels: \u2013 Level-1: Word-level extraction (to automatically detect/predict and elimi- nate non-slot & non-intent keywords \ufb01rst, as they would not carry much information for understanding the utterance-level intent-type). \u2013 Level-2: Utterance-level recognition (to detect \ufb01nal intent-types from given utterances, by using valid slots and intent keywords as inputs only, which are detected at Level-1). RNN with LSTM Cells for Sequence Modeling. In this study, we em- ployed an RNN architecture with LSTM cells that are designed to exploit long range dependencies in sequential data.",
  "RNN with LSTM Cells for Sequence Modeling. In this study, we em- ployed an RNN architecture with LSTM cells that are designed to exploit long range dependencies in sequential data. LSTM has memory cell state to store rel- evant information and various gates, which can mitigate the vanishing gradient problem [7]. Given the input xt at time t, and hidden state from the previ- ous time step ht\u22121, the hidden and output layers for the current time step are computed. The LSTM architecture is speci\ufb01ed by the following equations: it = \u03c3(Wxixt + Whiht\u22121 + bi) (1) ft = \u03c3(Wxfxt + Whfht\u22121 + bf) (2) ot = \u03c3(Wxoxt + Whoht\u22121 + bo) (3) gt = tanh(Wxgxt + Whght\u22121 + bg) (4) ct = ft \u2299ct\u22121 + it \u2299gt (5) ht = ot \u2299tanh(ct) (6) where W and b denote the weight matrices and bias terms, respectively.",
  "The sig- moid (\u03c3) and tanh are activation functions applied element-wise, and \u2299denotes the element-wise vector product. LSTM has a memory vector ct to read/write or reset using a gating mechanism and activation functions. Here, input gate it scales down the input, the forget gate ft scales down the memory vector ct, and the output gate ot scales down the output to achieve \ufb01nal ht, which is used to predict yt (through a softmax activation). Similar to LSTMs, GRUs [1] are pro- posed as a simpler and faster alternative, having only reset and update gates. For Bi-LSTM [18,5], two LSTM architectures are traversed in forward and backward directions, where their hidden layers are concatenated to compute the output. Extracting Slots and Intent Keywords. For slot \ufb01lling and intent keywords extraction, we experimented with various con\ufb01gurations of seq2seq LSTMs [17] and GRUs [1], as well as Bi-LSTMs [18]. A sample network architecture can be",
  "Natural Language Interactions in AVs: Intent Detection and Slot Filling 7 Fig. 2: Seq2seq Bi-LSTM Network for Slot Filling and Intent Keyword Extraction seen in Fig. 2 where we jointly trained slots and intent keywords. The passenger utterance is fed into LSTM/GRU network via an embedding layer as a sequence of words, which are transformed into word vectors. We also experimented with GloVe [15], word2vec [12,13], and fastText [8] as pre-trained word embeddings. To prevent over\ufb01tting, we used a dropout layer with 0.5 rate for regularization. Best performing results are obtained with Bi-LSTMs and GloVe embeddings (6B tokens, 400K vocabulary size, vector dimension 100). Utterance-level Recognition. For utterance-level intent detection, we mainly experimented with 5 groups of models: (1) Hybrid: RNN + Rule-based, (2) Separate: Seq2one Bi-LSTM with Attention, (3) Joint: Seq2seq Bi-LSTM for slots/intent keywords & utterance-level intents, (4) Hierarchical & Separate, (5) Hierarchical & Joint.",
  "For (1), we detect/extract intent keywords and slots (via RNN) and map them into utterance-level intent-types (rule-based). For (2), we feed the whole utterance as input sequence and intent-type as single target into Bi-LSTM network with Attention mechanism. For (3), we jointly train word-level intent keywords/slots and utterance-level intents (by adding <BOU>/<EOU> terms to the beginning/end of utterances with intent-types as their labels). For (4) and (5), we detect/extract intent keywords/slots \ufb01rst, and then only feed the predicted keywords/slots as a sequence into (2) and (3), respectively. 3 Experiments and Results 3.1 Utterance-Level Intent Detection Experiments The details of 5 groups of models and their variations that we experimented with for utterance-level intent recognition are summarized in this section. Hybrid Models. Instead of purely relying on machine learning (ML) or deep learning (DL) system, hybrid models leverage both ML/DL and rule-based sys- tems. In this model, we de\ufb01ned our hybrid approach as using RNNs \ufb01rst for",
  "8 E. Okur et al. Fig. 3: Hybrid Models Network Architecture detecting/extracting intent keywords and slots; then applying rule-based map- ping mechanisms to identify utterance-level intents (using the predicted intent keywords and slots). A sample network architecture can be seen in Fig. 3 where we leveraged seq2seq Bi-LSTM networks for word-level extraction before the rule-based mapping to utterance-level intent classes.",
  "A sample network architecture can be seen in Fig. 3 where we leveraged seq2seq Bi-LSTM networks for word-level extraction before the rule-based mapping to utterance-level intent classes. The model variations are de\ufb01ned based on varying mapping mechanisms and networks as follows: \u2013 Hybrid-0: RNN (Seq2seq LSTM for intent keywords extraction) + Rule- based (mapping extracted intent keywords to utterance-level intents) \u2013 Hybrid-1: RNN (Seq2seq Bi-LSTM for intent keywords extraction) + Rule- based (mapping extracted intent keywords to utterance-level intents) \u2013 Hybrid-2: RNN (Seq2seq Bi-LSTM for intent keywords & slots extraction) + Rule-based (mapping extracted intent keywords & Position/Direction slots to utterance-level intents) \u2013 Hybrid-3: RNN (Seq2seq Bi-LSTM for intent keywords & slots extraction) + Rule-based (mapping extracted intent keywords & all slots to utterance-level intents) Separate Seq2one Models. This approach is based on separately training sequence-to-one RNNs for utterance-level intents only.",
  "This approach is based on separately training sequence-to-one RNNs for utterance-level intents only. These are called separate models as we do not leverage any information from the slot or intent keyword tags (i.e., utterance-level intents are not jointly trained with slots/intent keywords). Note that in seq2one models, we feed the utterance as an input sequence and the LSTM layer will only return the hidden state output at the last time step. This single output (or concatenated output of last hidden states from the for- ward and backward LSTMs in Bi-LSTM case) will be used to classify the intent type of the given utterance. The idea behind is that the last hidden state of",
  "Natural Language Interactions in AVs: Intent Detection and Slot Filling 9 (a) Separate Seq2one Network (b) Separate Seq2one with Attention Fig. 4: Separate Models Network Architecture the sequence will contain a latent semantic representation of the whole input utterance, which can be utilized for utterance-level intent prediction. See Fig. 4 (a) for sample network architecture of the seq2one Bi-LSTM network. Note that in the Bi-LSTM implementation for seq2one learning (i.e., when not returning sequences), the outputs of backward/reverse LSTM is actually ordered in re- verse time steps (tlast ... tfirst). Thus, as illustrated in Fig. 4 (a), we actually concatenate the hidden state outputs of forward LSTM at last time step and backward LSTM at \ufb01rst time step (i.e., \ufb01rst word in a given utterance), and then feed this merged result to the dense layer. Fig. 4 (b) depicts the seq2one Bi-LSTM network with Attention mechanism applied on top of Bi-LSTM layers.",
  "Fig. 4 (b) depicts the seq2one Bi-LSTM network with Attention mechanism applied on top of Bi-LSTM layers. For the Attention case, the hidden state outputs of all time steps are fed into the Attention mechanism that will allow to point at speci\ufb01c words in a sequence when computing a single output [16]. Another variation of Attention mechanism we examined is the AttentionWithContext, which incorporates a context/query vector jointly learned during the training process to assist the attention [24]. All seq2one model variations we experimented with can be summarized as follows: \u2013 Separate-0: Seq2one LSTM for utterance-level intents \u2013 Separate-1: Seq2one Bi-LSTM for utterance-level intents \u2013 Separate-2: Seq2one Bi-LSTM with Attention [16] for utterance-level intents \u2013 Separate-3: Seq2one Bi-LSTM with AttentionWithContext [24] for utterance- level intents Joint Seq2seq Models.",
  "Using sequence-to-sequence networks, the approach here is jointly training annotated utterance-level intents and slots/intent key- words by adding <BOU>/ <EOU> tokens to the beginning/end of each ut- terance, with utterance-level intent-type as labels of such tokens. Our approach is an extension of [5], in which only an <EOS> term is added with intent-type",
  "10 E. Okur et al. Fig. 5: Joint Models Network Architecture tags associated to this sentence \ufb01nal token, both for LSTM and Bi-LSTM cases. However, we experimented with adding both <BOU> and <EOU> terms as Bi-LSTMs will be used for seq2seq learning, and we observed that slightly bet- ter results can be achieved by doing so. The idea behind is that, since this is a seq2seq learning problem, at the last time step (i.e., prediction at <EOU>) the reverse pass in Bi-LSTM would be incomplete (refer to Fig. 4 (a) to observe the last Bi-LSTM cell). Therefore, adding <BOU> token and leveraging the backward LSTM output at \ufb01rst time step (i.e., prediction at <BOU>) would potentially help for joint seq2seq learning. An overall network architecture can be found in Fig. 5 for our joint models.",
  "Therefore, adding <BOU> token and leveraging the backward LSTM output at \ufb01rst time step (i.e., prediction at <BOU>) would potentially help for joint seq2seq learning. An overall network architecture can be found in Fig. 5 for our joint models. We will report the experimental results on two variations (with and without intent keywords) as follows: \u2013 Joint-1: Seq2seq Bi-LSTM for utterance-level intent detection (jointly trained with slots) \u2013 Joint-2: Seq2seq Bi-LSTM for utterance-level intent detection (jointly trained with slots & intent keywords) Hierarchical & Separate Models. Proposed hierarchical models are detect- ing/extracting intent keywords & slots using sequence-to-sequence networks \ufb01rst (i.e., level-1), and then feeding only the words that are predicted as intent key- words & valid slots (i.e., not the ones that are predicted as None/O) as an input sequence to various separate sequence-to-one models (described above) to rec- ognize \ufb01nal utterance-level intents (i.e., level-2). A sample network architecture is given in Fig. 6 (a).",
  "A sample network architecture is given in Fig. 6 (a). The idea behind \ufb01ltering out non-slot and non-intent key- words here resembles providing a summary of input sequence to the upper levels of the network hierarchy, where we actually learn this summarized sequence of keywords using another RNN layer. This would potentially result in focusing the utterance-level classi\ufb01cation problem on the most salient words of the in- put sequences (i.e., intent keywords & slots) and also e\ufb00ectively reducing the",
  "Natural Language Interactions in AVs: Intent Detection and Slot Filling 11 (a) Hierarchical & Separate Model (b) Hierarchical & Joint Model Fig. 6: Hierarchical Models Network Architecture length of input sequences (i.e., improving the long-term dependency issues ob- served in longer sequences). Note that according to our dataset statistics given in Table 2, 45% of the words found in transcribed utterances with passenger intents are annotated as non-slot and non-intent keywords (e.g., \u2019please\u2019, \u2019okay\u2019, \u2019can\u2019, \u2019could\u2019, incomplete/interrupted words, \ufb01ller sounds like \u2019uh\u2019/\u2019um\u2019, certain stop words, punctuation, and many others that are not related to intent/slots). Therefore, the proposed approach would result in reducing the sequence length nearly by half at the input layer of level-2 for utterance-level recognition.",
  "Therefore, the proposed approach would result in reducing the sequence length nearly by half at the input layer of level-2 for utterance-level recognition. For hierarchical & separate models, we experimented with 4 variations based on which separate model used at the second level of the hierarchy, and these are summarized as follows: \u2013 Hierarchical & Separate-0: Level-1 (Seq2seq LSTM for intent keywords & slots extraction) + Level-2 (Separate-0: Seq2one LSTM for utterance-level intent detection) \u2013 Hierarchical & Separate-1: Level-1 (Seq2seq Bi-LSTM for intent keywords & slots extraction) + Level-2 (Separate-1: Seq2one Bi-LSTM for utterance-level intent detection) \u2013 Hierarchical & Separate-2: Level-1 (Seq2seq Bi-LSTM for intent keywords & slots extraction) + Level-2 (Separate-2: Seq2one Bi-LSTM + Attention for utterance-level intent detection) \u2013 Hierarchical & Separate-3: Level-1 (Seq2seq Bi-LSTM for intent keywords & slots extraction) + Level-2 (Separate-3: Seq2one Bi-LSTM + AttentionWith- Context for utterance-level intent detection) Hierarchical & Joint Models.",
  "Proposed hierarchical models detect/extract intent keywords & slots using sequence-to-sequence networks \ufb01rst, and then only the words that are predicted as intent keywords & valid slots (i.e., not the ones that are predicted as None/O) are fed as input to the joint sequence-to-sequence",
  "12 E. Okur et al. models (described above). See Fig. 6 (b) for sample network architecture. Af- ter the \ufb01ltering or summarization of sequence at level-1, <BOU> and <EOU> tokens are appended to the shorter input sequence before level-2 for joint learn- ing. Note that in this case, using Joint-1 model (jointly training annotated slots & utterance-level intents) for the second level of the hierarchy would not make much sense (without intent keywords). Hence, Joint-2 model is used for the sec- ond level as described below: \u2013 Hierarchical & Joint-2: Level-1 (Seq2seq Bi-LSTM for intent keywords & slots extraction) + Level-2 (Joint-2 Seq2seq models with slots & intent key- words & utterance-level intents) Table 3 summarizes the results of various approaches we investigated for utterance-level intent understanding. We achieved 0.91 overall F1-score with our best-performing model, namely Hierarchical & Joint-2. All model results are obtained via 10-fold cross-validation (10-fold CV) on the same dataset.",
  "We achieved 0.91 overall F1-score with our best-performing model, namely Hierarchical & Joint-2. All model results are obtained via 10-fold cross-validation (10-fold CV) on the same dataset. For our AMIE scenarios, Table 4 shows the intent-wise detection results with the initial (Hybrid-0) and currently best performing (H-Joint-2) intent recognizers. With our best model (H-Joint-2), relatively problematic SetDestination and SetRoute intents detection performances in baseline model (Hybrid-0) jumped from 0.78 to 0.89 and 0.75 to 0.88, respectively. We compared our intent detection results with the Dialog\ufb02ow\u2019s Detect Intent API. The same AMIE dataset is used to train and test (10-fold CV) Dialog\ufb02ow\u2019s intent detection and slot \ufb01lling modules, using the recommended hybrid mode (rule-based and ML). As shown in Table 4, an overall F1-score of 0.89 is achieved with Dialog\ufb02ow for the same task.",
  "As shown in Table 4, an overall F1-score of 0.89 is achieved with Dialog\ufb02ow for the same task. As you can see, our Hierarchical & Joint models obtained higher results than the Dialog\ufb02ow for 8 out of 10 intent types. Table 3: Utterance-level Intent Detection Performance Results (10-fold CV) Model Type Prec Rec F1 Hybrid-0: RNN (LSTM) + Rule-based (intent keywords) 0.86 0.85 0.85 Hybrid-1: RNN (Bi-LSTM) + Rule-based (intent keywords) 0.87 0.86 0.86 Hybrid-2: RNN (Bi-LSTM) + Rule-based (intent keywords & Pos slots) 0.89 0.88 0.88 Hybrid-3: RNN (Bi-LSTM) + Rule-based (intent keywords & all slots) 0.90 0.90 0.90 Separate-0: Seq2one LSTM 0.87 0.86 0.86 Separate-1: Seq2one Bi-LSTM 0.88 0.88 0.",
  "90 0.90 0.90 Separate-0: Seq2one LSTM 0.87 0.86 0.86 Separate-1: Seq2one Bi-LSTM 0.88 0.88 0.88 Separate-2: Seq2one Bi-LSTM + Attention 0.88 0.88 0.88 Separate-3: Seq2one Bi-LSTM + AttentionWithContext 0.89 0.89 0.89 Joint-1: Seq2seq Bi-LSTM (uttr-level intents & slots) 0.88 0.87 0.87 Joint-2: Seq2seq Bi-LSTM (uttr-level intents & slots & intent keywords) 0.89 0.88 0.88 Hierarchical & Separate-0 (LSTM) 0.88 0.87 0.87 Hierarchical & Separate-1 (Bi-LSTM) 0.90 0.90 0.90 Hierarchical & Separate-2 (Bi-LSTM + Attention) 0.90 0.90 0.90 Hierarchical & Separate-3 (Bi-LSTM + AttentionWithContext) 0.90 0.",
  "90 0.90 0.90 Hierarchical & Separate-2 (Bi-LSTM + Attention) 0.90 0.90 0.90 Hierarchical & Separate-3 (Bi-LSTM + AttentionWithContext) 0.90 0.90 0.90 Hierarchical & Joint-2 (uttr-level intents & slots & intent keywords) 0.91 0.90 0.91",
  "Natural Language Interactions in AVs: Intent Detection and Slot Filling 13 Table 4: Intent-wise Performance Results of Utterance-level Intent Detection AMIE Intent Our Intent Detection Models Dialog\ufb02ow Scenario Type Baseline (Hybrid-0) Best (H-Joint-2) Intent Detection Prec Rec F1 Prec Rec F1 Prec Rec F1 Finishing Stop 0.88 0.91 0.90 0.93 0.91 0.92 0.89 0.90 0.90 the Trip Park 0.96 0.87 0.91 0.94 0.94 0.94 0.95 0.88 0.91 PullOver 0.95 0.96 0.95 0.97 0.94 0.96 0.95 0.97 0.96 DropO\ufb00 0.90 0.95 0.92 0.95 0.95 0.95 0.96 0.91 0.93 Dest/Route SetDest 0.70 0.88 0.78 0.89 0.",
  "90 0.95 0.92 0.95 0.95 0.95 0.96 0.91 0.93 Dest/Route SetDest 0.70 0.88 0.78 0.89 0.90 0.89 0.84 0.91 0.87 SetRoute 0.80 0.71 0.75 0.86 0.89 0.88 0.83 0.86 0.84 Speed GoFaster 0.86 0.89 0.88 0.89 0.90 0.90 0.94 0.92 0.93 GoSlower 0.92 0.84 0.88 0.89 0.86 0.88 0.93 0.87 0.90 Others OpenDoor 0.95 0.95 0.95 0.95 0.95 0.95 0.94 0.93 0.93 Other 0.92 0.72 0.80 0.83 0.81 0.",
  "90 Others OpenDoor 0.95 0.95 0.95 0.95 0.95 0.95 0.94 0.93 0.93 Other 0.92 0.72 0.80 0.83 0.81 0.82 0.88 0.73 0.80 Overall 0.86 0.85 0.85 0.91 0.90 0.91 0.90 0.89 0.89 3.2 Slot Filling and Intent Keyword Extraction Experiments Slot \ufb01lling and intent keyword extraction results are given in Table 5 and Table 6, respectively. For slot extraction, we reached 0.96 overall F1-score using seq2seq Bi-LSTM model, which is slightly better than using LSTM model. Although the overall performance is slightly improved with Bi-LSTM model, relatively problematic Object, Time Guidance, Gesture/Gaze slots F1-score performances increased from 0.80 to 0.89, 0.80 to 0.85, and 0.87 to 0.92, respectively.",
  "Although the overall performance is slightly improved with Bi-LSTM model, relatively problematic Object, Time Guidance, Gesture/Gaze slots F1-score performances increased from 0.80 to 0.89, 0.80 to 0.85, and 0.87 to 0.92, respectively. Note that with Dialog\ufb02ow, we reached 0.92 overall F1-score for the entity/slot \ufb01lling task on the same dataset. As you can see, our models reached signi\ufb01cantly higher F1-scores than the Dialog\ufb02ow for 6 out of 7 slot types (except Time Guidance). Table 5: Slot Filling Results (10-fold CV) Our Slot Filling Models Dialog\ufb02ow Se2qseq LSTM Se2qseq Bi-LSTM Slot Filling Slot Type Prec Rec F1 Prec Rec F1 Prec Rec F1 Location 0.94 0.92 0.93 0.96 0.94 0.95 0.94 0.81 0.87 Position/Direction 0.92 0.93 0.93 0.95 0.95 0.",
  "94 0.92 0.93 0.96 0.94 0.95 0.94 0.81 0.87 Position/Direction 0.92 0.93 0.93 0.95 0.95 0.95 0.91 0.92 0.91 Person 0.97 0.96 0.97 0.98 0.97 0.97 0.96 0.76 0.85 Object 0.82 0.79 0.80 0.93 0.85 0.89 0.96 0.70 0.81 Time Guidance 0.88 0.73 0.80 0.90 0.80 0.85 0.93 0.82 0.87 Gesture/Gaze 0.86 0.88 0.87 0.92 0.92 0.92 0.86 0.65 0.74 None 0.97 0.98 0.97 0.97 0.98 0.98 0.92 0.",
  "88 0.87 0.92 0.92 0.92 0.86 0.65 0.74 None 0.97 0.98 0.97 0.97 0.98 0.98 0.92 0.98 0.95 Overall 0.95 0.95 0.95 0.96 0.96 0.96 0.92 0.92 0.92",
  "14 E. Okur et al. Table 6: Intent Keyword Extraction Results (10-fold CV) Keyword Type Prec Rec F1 Intent 0.95 0.93 0.94 Non-Intent 0.98 0.99 0.99 Overall 0.98 0.98 0.98 3.3 Speech-to-Text Experiments for AMIE: Training and Testing Models on ASR Outputs For transcriptions, utterance-level audio clips were extracted from the passenger- facing video stream, which was the single source used for human transcriptions of all utterances from passengers, AMIE agent and the game master. Since our transcriptions-based intent/slot models assumed perfect (at least close to human- level) ASR in the previous sections, we experimented with more realistic scenario of using ASR outputs for intent/slot modeling. We employed Cloud Speech-to- Text API to obtain ASR outputs on audio clips with passenger utterances, which were segmented using transcription time-stamps. We observed an overall word error rate (WER) of 13.6% in ASR outputs for all 20 sessions of AMIE.",
  "We employed Cloud Speech-to- Text API to obtain ASR outputs on audio clips with passenger utterances, which were segmented using transcription time-stamps. We observed an overall word error rate (WER) of 13.6% in ASR outputs for all 20 sessions of AMIE. Considering that a generic ASR is used with no domain-speci\ufb01c acoustic models for this moving vehicle environment with in-cabin noise, the initial re- sults were quite promising for us to move on with the model training on ASR outputs. For initial explorations, we created a new dataset having utterances with commands using ASR outputs of the in-cabin data (20 sessions with 1331 spoken utterances). Human transcriptions version of this set is also created. Although the dataset size is limited, both slot/intent keyword extraction mod- els and utterance-level intent recognition models are not severely a\ufb00ected when trained and tested (10-fold CV) on ASR outputs instead of manual transcrip- tions. See Table 7 for the overall F1-scores of the compared models. Table 7: F1-scores of Models Trained/Tested on Transcriptions vs.",
  "See Table 7 for the overall F1-scores of the compared models. Table 7: F1-scores of Models Trained/Tested on Transcriptions vs. ASR Outputs Train/Test on Train/Test on Transcriptions ASR Outputs Slot Filling & Intent Keywords ALL Singleton Dyad ALL Singleton Dyad Slot Filling 0.97 0.96 0.96 0.95 0.94 0.93 Intent Keyword Extraction 0.98 0.98 0.97 0.97 0.96 0.96 Slot Filling & Intent Keyword Extraction 0.95 0.95 0.94 0.94 0.92 0.91 Utterance-level Intent Detection ALL Singleton Dyad ALL Singleton Dyad Hierarchical & Separate 0.87 0.85 0.86 0.85 0.84 0.83 Hierarchical & Separate + Attention 0.89 0.86 0.87 0.86 0.84 0.84 Hierarchical & Joint 0.89 0.87 0.88 0.87 0.85 0.85",
  "Natural Language Interactions in AVs: Intent Detection and Slot Filling 15 Singleton versus Dyad Sessions. After the ASR pipeline described above is completed for all 20 sessions of AMIE in-cabin dataset (ALL with 1331 utter- ances), we repeated all our experiments with the subsets for 10 sessions having single passenger (Singletons with 600 utterances) and remaining 10 sessions hav- ing two passengers (Dyads with 731 utterances). We observed overall WER of 13.5% and 13.7% for Singletons and Dyads, respectively. The overlapping speech cases with slightly more conversations going on (longer transcriptions) in Dyad sessions compared to the Singleton sessions may a\ufb00ect the ASR performance, which may also a\ufb00ect the intent/slots models performances. As shown in Table 7, although we have more samples with Dyads, the performance drops between the models trained on transcriptions vs. ASR outputs are slightly higher for the Dyads compared to the Singletons, as expected. 4 Discussion and Conclusion We introduced AMIE, the intelligent in-cabin car agent responsible for handling certain AV-passenger interactions.",
  "ASR outputs are slightly higher for the Dyads compared to the Singletons, as expected. 4 Discussion and Conclusion We introduced AMIE, the intelligent in-cabin car agent responsible for handling certain AV-passenger interactions. We develop hierarchical and joint models to extract various passenger intents along with relevant slots for actions to be per- formed in AV, achieving F1-scores of 0.91 for intent recognition and 0.96 for slot extraction. We show that even using the generic ASR with noisy outputs, our models are still capable of achieving comparable results with models trained on human transcriptions. We believe that the ASR can be improved by col- lecting more in-domain data to obtain domain-speci\ufb01c acoustic models. These initial models will allow us to collect more speech data via bootstrapping with the intent-based dialogue application we have built, and the hierarchy we de\ufb01ned will allow us to eliminate costly annotation e\ufb00orts in the future, especially for the word-level slots and intent keywords. Once enough domain-speci\ufb01c multi-modal data is collected, our future work is to explore training end-to-end dialogue agents for our in-cabin use-cases.",
  "Once enough domain-speci\ufb01c multi-modal data is collected, our future work is to explore training end-to-end dialogue agents for our in-cabin use-cases. We are planning to exploit other modalities for improved understanding of the in-cabin dialogue as well. Acknowledgments. We would like to show our gratitude to our colleagues from Intel Labs, especially Cagri Tanriover for his tremendous e\ufb00orts in coor- dinating and implementing the vehicle instrumentation to enhance multi-modal data collection setup (as he illustrated in Fig. 1), John Sherry and Richard Beck- with for their insight and expertise that greatly assisted the gathering of this UX grounded and ecologically valid dataset (via scavenger hunt protocol and WoZ research design). The authors are also immensely grateful to the members of GlobalMe, Inc., particularly Rick Lin and Sophie Salonga, for their extensive e\ufb00orts in organizing and executing the data collection, transcription, and certain annotation tasks for this research in collaboration with our team at Intel Labs.",
  "16 E. Okur et al. References 1. Chung, J., G\u00a8ul\u00b8cehre, C\u00b8., Cho, K., Bengio, Y.: Empirical evaluation of gated re- current neural networks on sequence modeling. CoRR abs/1412.3555 (2014), http://arxiv.org/abs/1412.3555 2. Crowston, K.: Amazon mechanical turk: A research tool for organizations and in- formation systems scholars. In: Bhattacherjee, A., Fitzgerald, B. (eds.) Shaping the Future of ICT Research. Methods and Approaches. pp. 210\u2013221. Springer Berlin Heidelberg (2012) 3. Eric, M., Krishnan, L., Charette, F., Manning, C.D.: Key-value retrieval networks for task-oriented dialogue. In: Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue. pp. 37\u201349. Association for Computational Linguistics (2017). https://doi.org/10.18653/v1/W17-5506 4.",
  "In: Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue. pp. 37\u201349. Association for Computational Linguistics (2017). https://doi.org/10.18653/v1/W17-5506 4. Fukui, M., Watanabe, T., Kanazawa, M.: Sound source separation for plural passenger speech recognition in smart mobility system. IEEE Transactions on Consumer Electronics 64(3), 399\u2013405 (Aug 2018). https://doi.org/10.1109/TCE.2018.2867801 5. Hakkani-Tur, D., Tur, G., Celikyilmaz, A., Chen, Y.N.V., Gao, J., Deng, L., Wang, Y.Y.: Multi-domain joint semantic frame parsing using bi-directional rnn-lstm. ISCA (June 2016), https://www.microsoft.com/en-us/research/publication/ multijoint/ 6.",
  ": Multi-domain joint semantic frame parsing using bi-directional rnn-lstm. ISCA (June 2016), https://www.microsoft.com/en-us/research/publication/ multijoint/ 6. Hansen, J.H., Angkititrakul, P., Plucienkowski, J., Gallant, S., Yapanel, U., Pellom, B., Ward, W., Cole, R.: Cu-move: Analysis & corpus development for interactive in-vehicle speech systems. In: Seventh European Conference on Speech Communi- cation and Technology (2001) 7. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation 9(8), 1735\u20131780 (1997) 8. Joulin, A., Grave, E., Bojanowski, P., Mikolov, T.: Bag of tricks for e\ufb03cient text classi\ufb01cation. In: Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers. pp. 427\u2013431.",
  "In: Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers. pp. 427\u2013431. Association for Computational Linguistics (April 2017) 9. Liu, B., Lane, I.: Joint online spoken language understanding and language mod- eling with recurrent neural networks. CoRR abs/1609.01462 (2016), http: //arxiv.org/abs/1609.01462 10. Meng, Z., Mou, L., Jin, Z.: Hierarchical rnn with static sentence-level attention for text-based speaker change detection. In: Proceedings of the 2017 ACM on Conference on Information and Knowledge Management. pp. 2203\u20132206. CIKM \u201917, ACM, New York, NY, USA (2017). https://doi.org/10.1145/3132847.3133110 11.",
  "pp. 2203\u20132206. CIKM \u201917, ACM, New York, NY, USA (2017). https://doi.org/10.1145/3132847.3133110 11. Mesnil, G., Dauphin, Y., Yao, K., Bengio, Y., Deng, L., Hakkani-Tur, D., He, X., Heck, L., Tur, G., Yu, D., Zweig, G.: Using recurrent neural networks for slot \ufb01lling in spoken language understanding. Trans. Audio, Speech and Lang. Proc. 23(3), 530\u2013539 (Mar 2015). https://doi.org/10.1109/TASLP.2014.2383614 12. Mikolov, T., Chen, K., Corrado, G.S., Dean, J.: E\ufb03cient estimation of word rep- resentations in vector space. CoRR abs/1301.3781 (2013) 13.",
  "Mikolov, T., Chen, K., Corrado, G.S., Dean, J.: E\ufb03cient estimation of word rep- resentations in vector space. CoRR abs/1301.3781 (2013) 13. Mikolov, T., Sutskever, I., Chen, K., Corrado, G., Dean, J.: Distributed repre- sentations of words and phrases and their compositionality. In: Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2. pp. 3111\u20133119. NIPS\u201913, Curran Associates Inc., USA (2013), http: //dl.acm.org/citation.cfm?id=2999792.2999959",
  "Natural Language Interactions in AVs: Intent Detection and Slot Filling 17 14. Okur, E., Kumar, S.H., Sahay, S., Esme, A.A., Nachman, L.: Conversational intent understanding for passengers in autonomous vehicles. In: 13th WiML Workshop, co-located with the 32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montreal, Canada (2018), http://arxiv.org/abs/1901.04899 15. Pennington, J., Socher, R., Manning, C.D.: Glove: Global vectors for word repre- sentation. In: Empirical Methods in Natural Language Processing (EMNLP). pp. 1532\u20131543 (2014), http://www.aclweb.org/anthology/D14-1162 16. Ra\ufb00el, C., Ellis, D.P.W.: Feed-forward networks with attention can solve some long- term memory problems. CoRR abs/1512.08756 (2015), http://arxiv.org/abs/ 1512.08756 17.",
  "Ra\ufb00el, C., Ellis, D.P.W.: Feed-forward networks with attention can solve some long- term memory problems. CoRR abs/1512.08756 (2015), http://arxiv.org/abs/ 1512.08756 17. Ravuri, S., Stolcke, A.: Recurrent neural network and lstm models for lexical utter- ance classi\ufb01cation. In: Proc. Interspeech. pp. 135\u2013139. ISCA - International Speech Communication Association, Dresden (September 2015) 18. Schuster, M., Paliwal, K.: Bidirectional recurrent neural networks. Trans. Sig. Proc. 45(11), 2673\u20132681 (Nov 1997). https://doi.org/10.1109/78.650093 19. Sherry, J., Beckwith, R., Arslan Esme, A., Tanriover, C.: Getting things done in an autonomous vehicle.",
  "https://doi.org/10.1109/78.650093 19. Sherry, J., Beckwith, R., Arslan Esme, A., Tanriover, C.: Getting things done in an autonomous vehicle. In: Social Robots in the Wild Workshop at the 13th Annual ACM/IEEE International Conference on Human-Robot Interaction (HRI) (March 2018), http://socialrobotsinthewild.org/wp-content/uploads/2018/ 02/HRI-SRW_2018_paper_3.pdf 20. Stolcke, A., Droppo, J.: Comparing human and machine errors in conversational speech transcription. CoRR abs/1708.08615 (2017), http://arxiv.org/abs/ 1708.08615 21. Wang, P., Sibi, S., Mok, B., Ju, W.: Marionette: Enabling on-road wizard-of-oz autonomous driving studies. In: Proceedings of the 2017 ACM/IEEE International Conference on Human-Robot Interaction. pp. 234\u2013243. HRI \u201917, ACM, New York, NY, USA (2017).",
  "In: Proceedings of the 2017 ACM/IEEE International Conference on Human-Robot Interaction. pp. 234\u2013243. HRI \u201917, ACM, New York, NY, USA (2017). https://doi.org/10.1145/2909824.3020256 22. Wen, L., Wang, X., Dong, Z., Chen, H.: Jointly modeling intent identi\ufb01cation and slot \ufb01lling with contextual and hierarchical information. In: Huang, X., Jiang, J., Zhao, D., Feng, Y., Hong, Y. (eds.) Natural Language Processing and Chinese Computing. pp. 3\u201315. Springer International Publishing, Cham (2018) 23. Xiong, W., Droppo, J., Huang, X., Seide, F., Seltzer, M., Stolcke, A., Yu, D., Zweig, G.: Achieving human parity in conversational speech recognition. CoRR abs/1610.05256 (2016), http://arxiv.org/abs/1610.05256 24.",
  "CoRR abs/1610.05256 (2016), http://arxiv.org/abs/1610.05256 24. Yang, Z., Yang, D., Dyer, C., He, X., Smola, A., Hovy, E.: Hierarchical atten- tion networks for document classi\ufb01cation. In: Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies. pp. 1480\u20131489 (2016). https://doi.org/10.18653/v1/N16-1174 25. Zhang, X., Wang, H.: A joint model of intent determination and slot \ufb01lling for spoken language understanding. In: Proceedings of the Twenty-Fifth International Joint Conference on Arti\ufb01cial Intelligence. pp. 2993\u20132999. IJCAI\u201916, AAAI Press (2016), http://dl.acm.org/citation.cfm?id=3060832.3061040 26. Zheng, Y., Liu, Y., Hansen, J.H.L.: Navigation-orientated natural spo- ken language understanding for intelligent vehicle dialogue.",
  "Zheng, Y., Liu, Y., Hansen, J.H.L.: Navigation-orientated natural spo- ken language understanding for intelligent vehicle dialogue. In: 2017 IEEE Intelligent Vehicles Symposium (IV). pp. 559\u2013564 (June 2017). https://doi.org/10.1109/IVS.2017.7995777 27. Zhou, Q., Wen, L., Wang, X., Ma, L., Wang, Y.: A hierarchical lstm model for joint tasks. In: Sun, M., Huang, X., Lin, H., Liu, Z., Liu, Y. (eds.) Chinese Computa- tional Linguistics and Natural Language Processing Based on Naturally Annotated Big Data. pp. 324\u2013335. Springer International Publishing, Cham (2016)"
]