{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Text Summarization with Pretrained Encoders Yang Liu and Mirella Lapata Institute for Language, Cognition and Computation School of Informatics, University of Edinburgh yang.liu2@ed.ac.uk, mlap@inf.ed.ac.uk Abstract Bidirectional Encoder Representations from Transformers (BERT; Devlin et al. 2019) rep- resents the latest incarnation of pretrained lan- guage models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summariza- tion and propose a general framework for both extractive and abstractive models. We intro- duce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter- sentence Transformer layers. For abstractive summarization, we propose a new \ufb01ne-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of al- leviating the mismatch between the two (the former is pretrained while the latter is not).",
            "For abstractive summarization, we propose a new \ufb01ne-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of al- leviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged \ufb01ne-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves state- of-the-art results across the board in both ex- tractive and abstractive settings.1 1 Introduction Language model pretraining has advanced the state of the art in many NLP tasks ranging from sentiment analysis, to question answering, natu- ral language inference, named entity recognition, and textual similarity. State-of-the-art pretrained models include ELMo (Peters et al., 2018), GPT (Radford et al., 2018), and more recently Bidirec- tional Encoder Representations from Transform- ers (BERT; Devlin et al. 2019).",
            "State-of-the-art pretrained models include ELMo (Peters et al., 2018), GPT (Radford et al., 2018), and more recently Bidirec- tional Encoder Representations from Transform- ers (BERT; Devlin et al. 2019). BERT combines both word and sentence representations in a single very large Transformer (Vaswani et al., 2017); it is 1Our code is available at https:\/\/github.com\/ nlpyang\/PreSumm. pretrained on vast amounts of text, with an unsu- pervised objective of masked language modeling and next-sentence prediction and can be \ufb01ne-tuned with various task-speci\ufb01c objectives. In most cases, pretrained language models have been employed as encoders for sentence- and paragraph-level natural language understanding problems (Devlin et al., 2019) involving various classi\ufb01cation tasks (e.g., predicting whether any two sentences are in an entailment relationship; or determining the completion of a sentence among four alternative sentences). In this paper, we ex- amine the in\ufb02uence of language model pretrain- ing on text summarization.",
            "In this paper, we ex- amine the in\ufb02uence of language model pretrain- ing on text summarization. Different from previ- ous tasks, summarization requires wide-coverage natural language understanding going beyond the meaning of individual words and sentences. The aim is to condense a document into a shorter ver- sion while preserving most of its meaning. Fur- thermore, under abstractive modeling formula- tions, the task requires language generation ca- pabilities in order to create summaries containing novel words and phrases not featured in the source text, while extractive summarization is often de- \ufb01ned as a binary classi\ufb01cation task with labels in- dicating whether a text span (typically a sentence) should be included in the summary. We explore the potential of BERT for text sum- marization under a general framework encom- passing both extractive and abstractive model- ing paradigms. We propose a novel document- level encoder based on BERT which is able to encode a document and obtain representations for its sentences.",
            "We explore the potential of BERT for text sum- marization under a general framework encom- passing both extractive and abstractive model- ing paradigms. We propose a novel document- level encoder based on BERT which is able to encode a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter- sentence Transformer layers to capture document- level features for extracting sentences. Our ab- stractive model adopts an encoder-decoder archi- tecture, combining the same pretrained BERT en- coder with a randomly-initialized Transformer de- arXiv:1908.08345v2  [cs.CL]  5 Sep 2019",
            "coder (Vaswani et al., 2017). We design a new training schedule which separates the optimizers of the encoder and the decoder in order to accom- modate the fact that the former is pretrained while the latter must be trained from scratch. Finally, motivated by previous work showing that the com- bination of extractive and abstractive objectives can help generate better summaries (Gehrmann et al., 2018), we present a two-stage approach where the encoder is \ufb01ne-tuned twice, \ufb01rst with an extractive objective and subsequently on the ab- stractive summarization task. We evaluate the proposed approach on three single-document news summarization datasets representative of different writing conventions (e.g., important information is concentrated at the beginning of the document or distributed more evenly throughout) and summary styles (e.g., ver- bose vs. more telegraphic; extractive vs. abstrac- tive). Across datasets, we experimentally show that the proposed models achieve state-of-the-art results under both extractive and abstractive set- tings.",
            "more telegraphic; extractive vs. abstrac- tive). Across datasets, we experimentally show that the proposed models achieve state-of-the-art results under both extractive and abstractive set- tings. Our contributions in this work are three- fold: a) we highlight the importance of document encoding for the summarization task; a variety of recently proposed techniques aim to enhance summarization performance via copying mecha- nisms (Gu et al., 2016; See et al., 2017; Nallap- ati et al., 2017), reinforcement learning (Narayan et al., 2018b; Paulus et al., 2018; Dong et al., 2018), and multiple communicating encoders (Ce- likyilmaz et al., 2018).",
            "We achieve better results with a minimum-requirement model without using any of these mechanisms; b) we showcase ways to effectively employ pretrained language models in summarization under both extractive and abstrac- tive settings; we would expect any improvements in model pretraining to translate in better summa- rization in the future; and c) the proposed models can be used as a stepping stone to further improve summarization performance as well as baselines against which new proposals are tested. 2 Background 2.1 Pretrained Language Models Pretrained language models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Dong et al., 2019; Zhang et al., 2019) have recently emerged as a key technology for achieving im- pressive gains in a wide variety of natural lan- guage tasks. These models extend the idea of word embeddings by learning contextual repre- sentations from large-scale corpora using a lan- guage modeling objective. Bidirectional Encoder Representations from Transformers (BERT; De- vlin et al.",
            "These models extend the idea of word embeddings by learning contextual repre- sentations from large-scale corpora using a lan- guage modeling objective. Bidirectional Encoder Representations from Transformers (BERT; De- vlin et al. 2019) is a new language representation model which is trained with a masked language modeling and a \u201cnext sentence prediction\u201d task on a corpus of 3,300M words. The general architecture of BERT is shown in the left part of Figure 1. Input text is \ufb01rst prepro- cessed by inserting two special tokens. [CLS] is appended to the beginning of the text; the output representation of this token is used to aggregate in- formation from the whole sequence (e.g., for clas- si\ufb01cation tasks). And token [SEP] is inserted after each sentence as an indicator of sentence bound- aries. The modi\ufb01ed text is then represented as a sequence of tokens X = [w1, w2, \u00b7 \u00b7 \u00b7 , wn].",
            "And token [SEP] is inserted after each sentence as an indicator of sentence bound- aries. The modi\ufb01ed text is then represented as a sequence of tokens X = [w1, w2, \u00b7 \u00b7 \u00b7 , wn]. Each token wi is assigned three kinds of embeddings: token embeddings indicate the meaning of each token, segmentation embeddings are used to dis- criminate between two sentences (e.g., during a sentence-pair classi\ufb01cation task) and position em- beddings indicate the position of each token within the text sequence. These three embeddings are summed to a single input vector xi and fed to a bidirectional Transformer with multiple layers: \u02dchl = LN(hl\u22121 + MHAtt(hl\u22121)) (1) hl = LN(\u02dchl + FFN(\u02dchl)) (2) where h0 = x are the input vectors; LN is the layer normalization operation (Ba et al., 2016); MHAtt is the multi-head attention operation (Vaswani et al., 2017); superscript l indicates the depth of the stacked layer.",
            "On the top layer, BERT will gen- erate an output vector ti for each token with rich contextual information. Pretrained language models are usually used to enhance performance in language understanding tasks. Very recently, there have been attempts to apply pretrained models to various generation problems (Edunov et al., 2019; Rothe et al., 2019). When \ufb01ne-tuning for a speci\ufb01c task, unlike ELMo whose parameters are usually \ufb01xed, parameters in BERT are jointly \ufb01ne-tuned with additional task- speci\ufb01c parameters. 2.2 Extractive Summarization Extractive summarization systems create a sum- mary by identifying (and subsequently concate- nating) the most important sentences in a doc- ument. Neural models consider extractive sum-",
            "E[CLS] Esent Eone E[SEP] E2nd Esent E[SEP] Esent Eagain E[SEP] EA EA EA EA EA EA EA EA E1 E2 E3 E4 E5 E6 E7 E8 E9 E10 Transformer Layers Input  Docum ent Token  Em beddings Segm ent  Em beddings Position  Em beddings T[CLS] Tsent Tone T[SEP] T2nd Tsent T[SEP] Tsent Tagain T[SEP] Contextual  Em beddings [CLS] sent one [SEP] 2nd sent [SEP] sent again [SEP] + + + + + + + + + + + + + + + + + + + + EA EA E[CLS] Esent Eone E[SEP] E[CLS] E2nd Esent E[SEP] E[CLS] Esent Eagain E[SEP] EA EA EA EA EB EB EB EB EA EA EA EA E1 E2 E3 E4 E5 E6 E7 E8 E9 E10 E11 E12 Transformer Layers T[CLS] Tsent Tone T[SEP] T[CLS] T2nd Tsent T[SEP]",
            "EA EA EA EA EB EB EB EB EA EA EA EA E1 E2 E3 E4 E5 E6 E7 E8 E9 E10 E11 E12 Transformer Layers T[CLS] Tsent Tone T[SEP] T[CLS] T2nd Tsent T[SEP] T[CLS] Tsent Tagain T[SEP] [CLS] sent one [SEP] [CLS] 2nd sent [SEP] [CLS] sent again [SEP] + + + + + + + + + + + + + + + + + + + + + + + + Original BERT BERT for Summarization Figure 1: Architecture of the original BERT model (left) and BERTSUM (right). The sequence on top is the input document, followed by the summation of three kinds of embeddings for each token. The summed vectors are used as input embeddings to several bidirectional Transformer layers, generating contextual vectors for each token. BERTSUM extends BERT by inserting multiple [CLS] symbols to learn sentence representations and using interval segmentation embeddings (illustrated in red and green color) to distinguish multiple sentences.",
            "The summed vectors are used as input embeddings to several bidirectional Transformer layers, generating contextual vectors for each token. BERTSUM extends BERT by inserting multiple [CLS] symbols to learn sentence representations and using interval segmentation embeddings (illustrated in red and green color) to distinguish multiple sentences. marization as a sentence classi\ufb01cation problem: a neural encoder creates sentence representations and a classi\ufb01er predicts which sentences should be selected as summaries. SUMMARUNNER (Nal- lapati et al., 2017) is one of the earliest neural approaches adopting an encoder based on Recur- rent Neural Networks. REFRESH (Narayan et al., 2018b) is a reinforcement learning-based system trained by globally optimizing the ROUGE metric. More recent work achieves higher performance with more sophisticated model structures. LA- TENT (Zhang et al., 2018) frames extractive sum- marization as a latent variable inference problem; instead of maximizing the likelihood of \u201cgold\u201d standard labels, their latent model directly max- imizes the likelihood of human summaries given selected sentences.",
            "LA- TENT (Zhang et al., 2018) frames extractive sum- marization as a latent variable inference problem; instead of maximizing the likelihood of \u201cgold\u201d standard labels, their latent model directly max- imizes the likelihood of human summaries given selected sentences. SUMO (Liu et al., 2019) capi- talizes on the notion of structured attention to in- duce a multi-root dependency tree representation of the document while predicting the output sum- mary. NEUSUM (Zhou et al., 2018) scores and se- lects sentences jointly and represents the state of the art in extractive summarization. 2.3 Abstractive Summarization Neural approaches to abstractive summarization conceptualize the task as a sequence-to-sequence problem, where an encoder maps a sequence of tokens in the source document x = [x1, ..., xn] to a sequence of continuous representations z = [z1, ..., zn], and a decoder then generates the target summary y = [y1, ..., ym] token-by-token, in an auto-regressive manner, hence modeling the con- ditional probability: p(y1, ..., ym|x1, ..., xn).",
            "Rush et al. (2015) and Nallapati et al. (2016) were among the \ufb01rst to apply the neural encoder- decoder architecture to text summarization. See et al. (2017) enhance this model with a pointer- generator network (PTGEN) which allows it to copy words from the source text, and a coverage mechanism (COV) which keeps track of words that have been summarized. Celikyilmaz et al. (2018) propose an abstractive system where multiple agents (encoders) represent the document together with a hierarchical attention mechanism (over the agents) for decoding. Their Deep Communicat- ing Agents (DCA) model is trained end-to-end with reinforcement learning. Paulus et al. (2018) also present a deep reinforced model (DRM) for abstractive summarization which handles the cov- erage problem with an intra-attention mechanism where the decoder attends over previously gen- erated words. Gehrmann et al.",
            "Paulus et al. (2018) also present a deep reinforced model (DRM) for abstractive summarization which handles the cov- erage problem with an intra-attention mechanism where the decoder attends over previously gen- erated words. Gehrmann et al. (2018) follow a bottom-up approach (BOTTOMUP); a content se- lector \ufb01rst determines which phrases in the source document should be part of the summary, and a copy mechanism is applied only to preselected phrases during decoding. Narayan et al. (2018a) propose an abstractive model which is particu- larly suited to extreme summarization (i.e., single sentence summaries), based on convolutional neu- ral networks and additionally conditioned on topic distributions (TCONVS2S). 3 Fine-tuning BERT for Summarization 3.1 Summarization Encoder Although BERT has been used to \ufb01ne-tune vari- ous NLP tasks, its application to summarization",
            "is not as straightforward. Since BERT is trained as a masked-language model, the output vectors are grounded to tokens instead of sentences, while in extractive summarization, most models ma- nipulate sentence-level representations. Although segmentation embeddings represent different sen- tences in BERT, they only apply to sentence- pair inputs, while in summarization we must en- code and manipulate multi-sentential inputs. Fig- ure 1 illustrates our proposed BERT architecture for SUMmarization (which we call BERTSUM). In order to represent individual sentences, we insert external [CLS] tokens at the start of each sentence, and each [CLS] symbol collects features for the sentence preceding it. We also use in- terval segment embeddings to distinguish multi- ple sentences within a document. For senti we assign segment embedding EA or EB depending on whether i is odd or even. For example, for document [sent1, sent2, sent3, sent4, sent5], we would assign embeddings [EA, EB, EA, EB, EA].",
            "For senti we assign segment embedding EA or EB depending on whether i is odd or even. For example, for document [sent1, sent2, sent3, sent4, sent5], we would assign embeddings [EA, EB, EA, EB, EA]. This way, document representations are learned hierarchically where lower Transformer layers represent adjacent sentences, while higher lay- ers, in combination with self-attention, represent multi-sentence discourse. Position embeddings in the original BERT model have a maximum length of 512; we over- come this limitation by adding more position em- beddings that are initialized randomly and \ufb01ne- tuned with other parameters in the encoder. 3.2 Extractive Summarization Let d denote a document containing sentences [sent1, sent2, \u00b7 \u00b7 \u00b7 , sentm], where senti is the i-th sentence in the document. Extractive summariza- tion can be de\ufb01ned as the task of assigning a label yi \u2208{0, 1} to each senti, indicating whether the sentence should be included in the summary. It is assumed that summary sentences represent the most important content of the document.",
            "Extractive summariza- tion can be de\ufb01ned as the task of assigning a label yi \u2208{0, 1} to each senti, indicating whether the sentence should be included in the summary. It is assumed that summary sentences represent the most important content of the document. With BERTSUM, vector ti which is the vector of the i-th [CLS] symbol from the top layer can be used as the representation for senti. Several inter-sentence Transformer layers are then stacked on top of BERT outputs, to capture document-level features for extracting summaries: \u02dchl = LN(hl\u22121 + MHAtt(hl\u22121)) (3) hl = LN(\u02dchl + FFN(\u02dchl)) (4) where h0 = PosEmb(T); T denotes the sen- tence vectors output by BERTSUM, and func- tion PosEmb adds sinusoid positional embed- dings (Vaswani et al., 2017) to T, indicating the position of each sentence.",
            "The \ufb01nal output layer is a sigmoid classi\ufb01er: \u02c6yi = \u03c3(WohL i + bo) (5) where hL i is the vector for senti from the top layer (the L-th layer ) of the Transformer. In experiments, we implemented Transformers with L = 1, 2, 3 and found that a Transformer with L = 2 performed best. We name this model BERTSUMEXT. The loss of the model is the binary classi\ufb01ca- tion entropy of prediction \u02c6yi against gold label yi. Inter-sentence Transformer layers are jointly \ufb01ne- tuned with BERTSUM. We use the Adam opti- mizer with \u03b21 = 0.9, and \u03b22 = 0.999).",
            "Inter-sentence Transformer layers are jointly \ufb01ne- tuned with BERTSUM. We use the Adam opti- mizer with \u03b21 = 0.9, and \u03b22 = 0.999). Our learn- ing rate schedule follows (Vaswani et al., 2017) with warming-up (warmup = 10, 000): lr = 2e\u22123 \u00b7 min (step \u22120.5, step \u00b7 warmup \u22121.5) 3.3 Abstractive Summarization We use a standard encoder-decoder framework for abstractive summarization (See et al., 2017). The encoder is the pretrained BERTSUM and the de- coder is a 6-layered Transformer initialized ran- domly. It is conceivable that there is a mis- match between the encoder and the decoder, since the former is pretrained while the latter must be trained from scratch. This can make \ufb01ne-tuning unstable; for example, the encoder might over\ufb01t the data while the decoder under\ufb01ts, or vice versa.",
            "This can make \ufb01ne-tuning unstable; for example, the encoder might over\ufb01t the data while the decoder under\ufb01ts, or vice versa. To circumvent this, we design a new \ufb01ne-tuning schedule which separates the optimizers of the en- coder and the decoder. We use two Adam optimizers with \u03b21 = 0.9 and \u03b22 = 0.999 for the encoder and the decoder, re- spectively, each with different warmup-steps and learning rates: lrE = \u02dclrE \u00b7 min(step\u22120.5, step \u00b7 warmup\u22121.5 E ) (6) lrD = \u02dclrD \u00b7 min(step\u22120.5, step \u00b7 warmup\u22121.5 D ) (7) where \u02dclrE = 2e\u22123, and warmupE = 20, 000 for the encoder and \u02dclrD = 0.1, and warmupD = 10, 000 for the decoder.",
            "This is based on the assumption that the pretrained encoder should be \ufb01ne-tuned with a smaller learning rate and smoother decay (so that the encoder can be trained with more accurate gradients when the decoder is becoming stable).",
            "Datasets # docs (train\/val\/test) avg. doc length avg. summary length % novel bi-grams words sentences words sentences in gold summary CNN 90,266\/1,220\/1,093 760.50 33.98 45.70 3.59 52.90 DailyMail 196,961\/12,148\/10,397 653.33 29.33 54.65 3.86 52.16 NYT 96,834\/4,000\/3,452 800.04 35.55 45.54 2.44 54.70 XSum 204,045\/11,332\/11,334 431.07 19.77 23.26 1.00 83.31 Table 1: Comparison of summarization datasets: size of training, validation, and test sets and average document and summary length (in terms of words and sentences). The proportion of novel bi-grams that do not appear in source documents but do appear in the gold summaries quanti\ufb01es corpus bias towards extractive methods.",
            "The proportion of novel bi-grams that do not appear in source documents but do appear in the gold summaries quanti\ufb01es corpus bias towards extractive methods. In addition, we propose a two-stage \ufb01ne-tuning approach, where we \ufb01rst \ufb01ne-tune the encoder on the extractive summarization task (Section 3.2) and then \ufb01ne-tune it on the abstractive summariza- tion task (Section 3.3). Previous work (Gehrmann et al., 2018; Li et al., 2018) suggests that using extractive objectives can boost the performance of abstractive summarization. Also notice that this two-stage approach is conceptually very sim- ple, the model can take advantage of information shared between these two tasks, without funda- mentally changing its architecture. We name the default abstractive model BERTSUMABS and the two-stage \ufb01ne-tuned model BERTSUMEXTABS. 4 Experimental Setup In this section, we describe the summarization datasets used in our experiments and discuss vari- ous implementation details.",
            "We name the default abstractive model BERTSUMABS and the two-stage \ufb01ne-tuned model BERTSUMEXTABS. 4 Experimental Setup In this section, we describe the summarization datasets used in our experiments and discuss vari- ous implementation details. 4.1 Summarization Datasets We evaluated our model on three benchmark datasets, namely the CNN\/DailyMail news high- lights dataset (Hermann et al., 2015), the New York Times Annotated Corpus (NYT; Sandhaus 2008), and XSum (Narayan et al., 2018a). These datasets represent different summary styles rang- ing from highlights to very brief one sentence summaries. The summaries also vary with respect to the type of rewriting operations they exemplify (e.g., some showcase more cut and paste opera- tions while others are genuinely abstractive). Ta- ble 1 presents statistics on these datasets (test set); example (gold-standard) summaries are provided in the supplementary material. CNN\/DailyMail contains news articles and as- sociated highlights, i.e., a few bullet points giving a brief overview of the article.",
            "Ta- ble 1 presents statistics on these datasets (test set); example (gold-standard) summaries are provided in the supplementary material. CNN\/DailyMail contains news articles and as- sociated highlights, i.e., a few bullet points giving a brief overview of the article. We used the stan- dard splits of Hermann et al. (2015) for training, validation, and testing (90,266\/1,220\/1,093 CNN documents and 196,961\/12,148\/10,397 DailyMail documents). We did not anonymize entities. We \ufb01rst split sentences with the Stanford CoreNLP toolkit (Manning et al., 2014) and pre-processed the dataset following See et al. (2017). Input doc- uments were truncated to 512 tokens. NYT contains 110,540 articles with abstractive summaries. Following Durrett et al. (2016), we split these into 100,834\/9,706 training\/test exam- ples, based on the date of publication (the test set contains all articles published from January 1, 2007 onward).",
            "Following Durrett et al. (2016), we split these into 100,834\/9,706 training\/test exam- ples, based on the date of publication (the test set contains all articles published from January 1, 2007 onward). We used 4,000 examples from the training as validation set. We also followed their \ufb01ltering procedure, documents with summaries less than 50 words were removed from the dataset. The \ufb01ltered test set (NYT50) includes 3,452 ex- amples. Sentences were split with the Stanford CoreNLP toolkit (Manning et al., 2014) and pre- processed following Durrett et al. (2016). Input documents were truncated to 800 tokens. XSum contains 226,711 news articles accompa- nied with a one-sentence summary, answering the question \u201cWhat is this article about?\u201d. We used the splits of Narayan et al. (2018a) for training, valida- tion, and testing (204,045\/11,332\/11,334) and fol- lowed the pre-processing introduced in their work.",
            "We used the splits of Narayan et al. (2018a) for training, valida- tion, and testing (204,045\/11,332\/11,334) and fol- lowed the pre-processing introduced in their work. Input documents were truncated to 512 tokens. Aside from various statistics on the three datasets, Table 1 also reports the proportion of novel bi-grams in gold summaries as a measure of their abstractiveness. We would expect mod- els with extractive biases to perform better on datasets with (mostly) extractive summaries, and abstractive models to perform more rewrite op- erations on datasets with abstractive summaries. CNN\/DailyMail and NYT are somewhat extrac- tive, while XSum is highly abstractive. 4.2 Implementation Details For both extractive and abstractive settings, we used PyTorch, OpenNMT (Klein et al., 2017) and the \u2018bert-base-uncased\u20192 version of BERT to im- plement BERTSUM. Both source and target texts 2https:\/\/git.io\/fhbJQ",
            "were tokenized with BERT\u2019s subwords tokenizer. Extractive Summarization All extractive mod- els were trained for 50,000 steps on 3 GPUs (GTX 1080 Ti) with gradient accumulation every two steps. Model checkpoints were saved and evalu- ated on the validation set every 1,000 steps. We selected the top-3 checkpoints based on the evalu- ation loss on the validation set, and report the av- eraged results on the test set. We used a greedy al- gorithm similar to Nallapati et al. (2017) to obtain an oracle summary for each document to train ex- tractive models. The algorithm generates an oracle consisting of multiple sentences which maximize the ROUGE-2 score against the gold summary. When predicting summaries for a new docu- ment, we \ufb01rst use the model to obtain the score for each sentence. We then rank these sentences by their scores from highest to lowest, and select the top-3 sentences as the summary. During sentence selection we use Trigram Blocking to reduce redundancy (Paulus et al., 2018).",
            "We then rank these sentences by their scores from highest to lowest, and select the top-3 sentences as the summary. During sentence selection we use Trigram Blocking to reduce redundancy (Paulus et al., 2018). Given summary S and candidate sen- tence c, we skip c if there exists a trigram over- lapping between c and S. The intuition is simi- lar to Maximal Marginal Relevance (MMR; Car- bonell and Goldstein 1998); we wish to minimize the similarity between the sentence being consid- ered and sentences which have been already se- lected as part of the summary. Abstractive Summarization In all abstractive models, we applied dropout (with probability 0.1) before all linear layers; label smoothing (Szegedy et al., 2016) with smoothing factor 0.1 was also used. Our Transformer decoder has 768 hidden units and the hidden size for all feed-forward lay- ers is 2,048. All models were trained for 200,000 steps on 4 GPUs (GTX 1080 Ti) with gradient ac- cumulation every \ufb01ve steps.",
            "Our Transformer decoder has 768 hidden units and the hidden size for all feed-forward lay- ers is 2,048. All models were trained for 200,000 steps on 4 GPUs (GTX 1080 Ti) with gradient ac- cumulation every \ufb01ve steps. Model checkpoints were saved and evaluated on the validation set ev- ery 2,500 steps. We selected the top-3 checkpoints based on their evaluation loss on the validation set, and report the averaged results on the test set. During decoding we used beam search (size 5), and tuned the \u03b1 for the length penalty (Wu et al., 2016) between 0.6 and 1 on the validation set; we decode until an end-of-sequence token is emitted and repeated trigrams are blocked (Paulus et al., 2018). It is worth noting that our decoder ap- plies neither a copy nor a coverage mechanism (See et al., 2017), despite their popularity in ab- stractive summarization. This is mainly because Model R1 R2 RL ORACLE 52.59 31.24 48.87 LEAD-3 40.42 17.",
            "This is mainly because Model R1 R2 RL ORACLE 52.59 31.24 48.87 LEAD-3 40.42 17.62 36.67 Extractive SUMMARUNNER (Nallapati et al., 2017) 39.60 16.20 35.30 REFRESH (Narayan et al., 2018b) 40.00 18.20 36.60 LATENT (Zhang et al., 2018) 41.05 18.77 37.54 NEUSUM (Zhou et al., 2018) 41.59 19.01 37.98 SUMO (Liu et al., 2019) 41.00 18.40 37.20 TransformerEXT 40.90 18.02 37.17 Abstractive PTGEN (See et al., 2017) 36.44 15.66 33.42 PTGEN+COV (See et al., 2017) 39.53 17.28 36.38 DRM (Paulus et al.,",
            "17 Abstractive PTGEN (See et al., 2017) 36.44 15.66 33.42 PTGEN+COV (See et al., 2017) 39.53 17.28 36.38 DRM (Paulus et al., 2018) 39.87 15.82 36.90 BOTTOMUP (Gehrmann et al., 2018) 41.22 18.68 38.34 DCA (Celikyilmaz et al., 2018) 41.69 19.47 37.92 TransformerABS 40.21 17.76 37.09 BERT-based BERTSUMEXT 43.25 20.24 39.63 BERTSUMEXT w\/o interval embeddings 43.20 20.22 39.59 BERTSUMEXT (large) 43.85 20.34 39.90 BERTSUMABS 41.72 19.39 38.76 BERTSUMEXTABS 42.13 19.60 39.",
            "20 20.22 39.59 BERTSUMEXT (large) 43.85 20.34 39.90 BERTSUMABS 41.72 19.39 38.76 BERTSUMEXTABS 42.13 19.60 39.18 Table 2: ROUGE F1 results on CNN\/DailyMail test set (R1 and R2 are shorthands for unigram and bigram overlap; RL is the longest common subsequence). Re- sults for comparison systems are taken from the au- thors\u2019 respective papers or obtained on our data by run- ning publicly released software. we focus on building a minimum-requirements model and these mechanisms may introduce ad- ditional hyper-parameters to tune. Thanks to the subwords tokenizer, we also rarely observe is- sues with out-of-vocabulary words in the out- put; moreover, trigram-blocking produces diverse summaries managing to reduce repetitions. 5 Results 5.1 Automatic Evaluation We evaluated summarization quality automati- cally using ROUGE (Lin, 2004).",
            "5 Results 5.1 Automatic Evaluation We evaluated summarization quality automati- cally using ROUGE (Lin, 2004). We report unigram and bigram overlap (ROUGE-1 and ROUGE-2) as a means of assessing informa- tiveness and the longest common subsequence (ROUGE-L) as a means of assessing \ufb02uency. Table 2 summarizes our results on the CNN\/DailyMail dataset. The \ufb01rst block in the ta- ble includes the results of an extractive ORACLE system as an upper bound. We also present the LEAD-3 baseline (which simply selects the \ufb01rst three sentences in a document). The second block in the table includes various extractive models trained on the CNN\/DailyMail dataset (see Section 2.2 for an overview). For",
            "Model R1 R2 RL ORACLE 49.18 33.24 46.02 LEAD-3 39.58 20.11 35.78 Extractive COMPRESS (Durrett et al., 2016) 42.20 24.90 \u2014 SUMO (Liu et al., 2019) 42.30 22.70 38.60 TransformerEXT 41.95 22.68 38.51 Abstractive PTGEN (See et al., 2017) 42.47 25.61 \u2014 PTGEN + COV (See et al., 2017) 43.71 26.40 \u2014 DRM (Paulus et al., 2018) 42.94 26.02 \u2014 TransformerABS 35.75 17.23 31.41 BERT-based BERTSUMEXT 46.66 26.35 42.62 BERTSUMABS 48.92 30.84 45.41 BERTSUMEXTABS 49.02 31.02 45.55 Table 3: ROUGE Recall results on NYT test set.",
            "Re- sults for comparison systems are taken from the au- thors\u2019 respective papers or obtained on our data by run- ning publicly released software. Table cells are \ufb01lled with \u2014 whenever results are not available. comparison to our own model, we also imple- mented a non-pretrained Transformer baseline (TransformerEXT) which uses the same architec- ture as BERTSUMEXT, but with fewer parameters. It is randomly initialized and only trained on the summarization task. TransformerEXT has 6 lay- ers, the hidden size is 512, and the feed-forward \ufb01lter size is 2,048. The model was trained with same settings as in Vaswani et al. (2017). The third block in Table 2 highlights the per- formance of several abstractive models on the CNN\/DailyMail dataset (see Section 2.3 for an overview).",
            "The model was trained with same settings as in Vaswani et al. (2017). The third block in Table 2 highlights the per- formance of several abstractive models on the CNN\/DailyMail dataset (see Section 2.3 for an overview). We also include an abstractive Trans- former baseline (TransformerABS) which has the same decoder as our abstractive BERTSUM mod- els; the encoder is a 6-layer Transformer with 768 hidden size and 2,048 feed-forward \ufb01lter size. The fourth block reports results with \ufb01ne-tuned BERT models: BERTSUMEXT and its two vari- ants (one without interval embeddings, and one with the large version of BERT), BERTSUM- ABS, and BERTSUMEXTABS. BERT-based mod- els outperform the LEAD-3 baseline which is not a strawman; on the CNN\/DailyMail corpus it is indeed superior to several extractive (Nalla- pati et al., 2017; Narayan et al., 2018b; Zhou et al., 2018) and abstractive models (See et al., 2017).",
            "BERT models collectively outperform all previously proposed extractive and abstractive systems, only falling behind the ORACLE upper bound. Among BERT variants, BERTSUMEXT performs best which is not entirely surprising; Model R1 R2 RL ORACLE 29.79 8.81 22.66 LEAD 16.30 1.60 11.95 Abstractive PTGEN (See et al., 2017) 29.70 9.21 23.24 PTGEN+COV (See et al., 2017) 28.10 8.02 21.72 TCONVS2S (Narayan et al., 2018a) 31.89 11.54 25.75 TransformerABS 29.41 9.77 23.01 BERT-based BERTSUMABS 38.76 16.33 31.15 BERTSUMEXTABS 38.81 16.50 31.27 Table 4: ROUGE F1 results on the XSum test set. Results for comparison systems are taken from the au- thors\u2019 respective papers or obtained on our data by run- ning publicly released software.",
            "Results for comparison systems are taken from the au- thors\u2019 respective papers or obtained on our data by run- ning publicly released software. CNN\/DailyMail summaries are somewhat extrac- tive and even abstractive models are prone to copy- ing sentences from the source document when trained on this dataset (See et al., 2017). Perhaps unsurprisingly we observe that larger versions of BERT lead to performance improvements and that interval embeddings bring only slight gains. Table 3 presents results on the NYT dataset. Following the evaluation protocol in Durrett et al. (2016), we use limited-length ROUGE Recall, where predicted summaries are truncated to the length of the gold summaries. Again, we report the performance of the ORACLE upper bound and LEAD-3 baseline. The second block in the table contains previously proposed extractive models as well as our own Transformer baseline. COM- PRESS (Durrett et al., 2016) is an ILP-based model which combines compression and anaphoricity constraints. The third block includes abstractive models from the literature, and our Transformer baseline. BERT-based models are shown in the fourth block.",
            "COM- PRESS (Durrett et al., 2016) is an ILP-based model which combines compression and anaphoricity constraints. The third block includes abstractive models from the literature, and our Transformer baseline. BERT-based models are shown in the fourth block. Again, we observe that they out- perform previously proposed approaches. On this dataset, abstractive BERT models generally per- form better compared to BERTSUMEXT, almost approaching ORACLE performance. Table 4 summarizes our results on the XSum dataset. Recall that summaries in this dataset are highly abstractive (see Table 1) consisting of a sin- gle sentence conveying the gist of the document. Extractive models here perform poorly as corrob- orated by the low performance of the LEAD base- line (which simply selects the leading sentence from the document), and the ORACLE (which se- lects a single-best sentence in each document) in Table 4. As a result, we do not report results for extractive models on this dataset. The second",
            "\u02dclrE \u02dclrD 1 0.1 0.01 0.001 2e-2 50.69 9.33 10.13 19.26 2e-3 37.21 8.73 9.52 16.88 Table 5: Model perplexity (CNN\/DailyMail; valida- tion set) under different combinations of encoder and decoder learning rates. block in Table 4 presents the results of various ab- stractive models taken from Narayan et al. (2018a) and also includes our own abstractive Transformer baseline. In the third block we show the results of our BERT summarizers which again are supe- rior to all previously reported models (by a wide margin). 5.2 Model Analysis Learning Rates Recall that our abstractive model uses separate optimizers for the encoder and decoder. In Table 5 we examine whether the combination of different learning rates (\u02dclrE and \u02dclrD) is indeed bene\ufb01cial.",
            "5.2 Model Analysis Learning Rates Recall that our abstractive model uses separate optimizers for the encoder and decoder. In Table 5 we examine whether the combination of different learning rates (\u02dclrE and \u02dclrD) is indeed bene\ufb01cial. Speci\ufb01cally, we re- port model perplexity on the CNN\/DailyMail val- idation set for varying encoder\/decoder learning rates. We can see that the model performs best with \u02dclrE = 2e \u22123 and \u02dclrD = 0.1. Position of Extracted Sentences In addition to the evaluation based on ROUGE, we also ana- lyzed in more detail the summaries produced by our model. For the extractive setting, we looked at the position (in the source document) of the sen- tences which were selected to appear in the sum- mary. Figure 2 shows the proportion of selected summary sentences which appear in the source document at positions 1, 2, and so on. The analysis was conducted on the CNN\/DailyMail dataset for Oracle summaries, and those produced by BERT- SUMEXT and the TransformerEXT.",
            "Figure 2 shows the proportion of selected summary sentences which appear in the source document at positions 1, 2, and so on. The analysis was conducted on the CNN\/DailyMail dataset for Oracle summaries, and those produced by BERT- SUMEXT and the TransformerEXT. We can see that Oracle summary sentences are fairly smoothly distributed across documents, while summaries created by TransformerEXT mostly concentrate on the \ufb01rst document sentences. BERTSUMEXT out- puts are more similar to Oracle summaries, indi- cating that with the pretrained encoder, the model relies less on shallow position features, and learns deeper document representations. Novel N-grams We also analyzed the output of abstractive systems by calculating the proportion of novel n-grams that appear in the summaries but not in the source texts. The results are shown in Figure 3.",
            "Novel N-grams We also analyzed the output of abstractive systems by calculating the proportion of novel n-grams that appear in the summaries but not in the source texts. The results are shown in Figure 3. In the CNN\/DailyMail dataset, the pro- 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Sentence position (in source document) 0.00 0.05 0.10 0.15 0.20 0.25 0.30 Proportion of selected sentences TRANSFORMEREXT BERTSUMEXT ORACLE Figure 2: Proportion of extracted sentences according to their position in the original document. portion of novel n-grams in automatically gener- ated summaries is much lower compared to refer- ence summaries, but in XSum, this gap is much smaller. We also observe that on CNN\/DailyMail, BERTEXTABS produces less novel n-ngrams than BERTABS, which is not surprising. BERTEXTABS is more biased towards selecting sentences from the source document since it is initially trained as an extractive model.",
            "We also observe that on CNN\/DailyMail, BERTEXTABS produces less novel n-ngrams than BERTABS, which is not surprising. BERTEXTABS is more biased towards selecting sentences from the source document since it is initially trained as an extractive model. The supplementary material includes examples of system output and additional ablation studies. 5.3 Human Evaluation In addition to automatic evaluation, we also evalu- ated system output by eliciting human judgments. We report experiments following a question- answering (QA) paradigm (Clarke and Lapata, 2010; Narayan et al., 2018b) which quanti\ufb01es the degree to which summarization models retain key information from the document. Under this paradigm, a set of questions is created based on the gold summary under the assumption that it highlights the most important document content. Participants are then asked to answer these ques- tions by reading system summaries alone without access to the article. The more questions a sys- tem can answer, the better it is at summarizing the document as a whole.",
            "Participants are then asked to answer these ques- tions by reading system summaries alone without access to the article. The more questions a sys- tem can answer, the better it is at summarizing the document as a whole. Moreover, we also assessed the overall qual- ity of the summaries produced by abstractive sys- tems which due to their ability to rewrite content may produce dis\ufb02uent or ungrammatical output. Speci\ufb01cally, we followed the Best-Worst Scal- ing (Kiritchenko and Mohammad, 2017) method where participants were presented with the output of two systems (and the original document) and",
            "1-grams 2-grams 3-grams 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Proportion of novel n-grams BERTSUMEXTABS BERTSUMABS Reference (a) CNN\/DailyMail Dataset 1-grams 2-grams 3-grams 0.0 0.2 0.4 0.6 0.8 1.0 Proportion of novel n-grams (b) XSum dataset Figure 3: Proportion of novel n-grams in model gener- ated summaries. Extractive CNN\/DM NYT LEAD 42.5\u2020 36.2\u2020 NEUSUM 42.2\u2020 \u2014 SUMO 41.7\u2020 38.1\u2020 Transformer 37.8\u2020 32.5\u2020 BERTSUM 58.9 41.9 Table 6: QA-based evaluation. Models with \u2020 are sig- ni\ufb01cantly different from BERTSUM (using a paired stu- dent t-test; p < 0.05).",
            "Models with \u2020 are sig- ni\ufb01cantly different from BERTSUM (using a paired stu- dent t-test; p < 0.05). Table cells are \ufb01lled with \u2014 whenever system output is not available. asked to decide which one was better according to the criteria of Informativeness, Fluency, and Suc- cinctness. Both types of evaluation were conducted on the Amazon Mechanical Turk platform. For the CNN\/DailyMail and NYT datasets we used the same documents (20 in total) and questions from previous work (Narayan et al., 2018b; Liu et al., 2019). For XSum, we randomly selected 20 documents (and their questions) from the release of Narayan et al. (2018a). We elicited 3 re- sponses per HIT. With regard to QA evaluation, we adopted the scoring mechanism from Clarke and Lapata (2010); correct answers were marked with a score of one, partially correct answers with 0.5, and zero otherwise.",
            "(2018a). We elicited 3 re- sponses per HIT. With regard to QA evaluation, we adopted the scoring mechanism from Clarke and Lapata (2010); correct answers were marked with a score of one, partially correct answers with 0.5, and zero otherwise. For quality-based evaluation, the rating of each system was com- puted as the percentage of times it was chosen as better minus the times it was selected as worse. Ratings thus range from -1 (worst) to 1 (best).",
            "For quality-based evaluation, the rating of each system was com- puted as the percentage of times it was chosen as better minus the times it was selected as worse. Ratings thus range from -1 (worst) to 1 (best). CNN\/DM NYT XSum Abstractive QA Rank QA Rank QA Rank LEAD 42.5\u2020 \u2014 36.2\u2020 \u2014 9.20\u2020 \u2014 PTGEN 33.3\u2020 -0.24\u2020 30.5\u2020 -0.27\u2020 23.7\u2020 -0.36\u2020 BOTTOMUP 40.6\u2020 -0.16\u2020 \u2014 \u2014 \u2014 \u2014 TCONVS2S \u2014 \u2014 \u2014 \u2014 52.1 -0.20\u2020 GOLD \u2014 0.22\u2020 \u2014 0.33\u2020 \u2014 0.38\u2020 BERTSUM 56.1 0.17 41.8 -0.07 57.5 0.19 Table 7: QA-based and ranking-based evaluation. Models with \u2020 are signi\ufb01cantly different from BERT- SUM (using a paired student t-test; p < 0.05).",
            "Models with \u2020 are signi\ufb01cantly different from BERT- SUM (using a paired student t-test; p < 0.05). Table cells are \ufb01lled with \u2014 whenever system output is not available. GOLD is not used in QA setting, and LEAD is not used in Rank evaluation. Results for extractive and abstractive systems are shown in Tables 6 and 7, respectively. We compared the best performing BERTSUM model in each setting (extractive or abstractive) against various state-of-the-art systems (whose output is publicly available), the LEAD baseline, and the GOLD standard as an upper bound. As shown in both tables participants overwhelmingly pre- fer the output of our model against comparison systems across datasets and evaluation paradigms. All differences between BERTSUM and compari- son models are statistically signi\ufb01cant (p < 0.05), with the exception of TCONVS2S (see Table 7; XSum) in the QA evaluation setting. 6 Conclusions In this paper, we showcased how pretrained BERT can be usefully applied in text summarization.",
            "6 Conclusions In this paper, we showcased how pretrained BERT can be usefully applied in text summarization. We introduced a novel document-level encoder and proposed a general framework for both abstrac- tive and extractive summarization. Experimental results across three datasets show that our model achieves state-of-the-art results across the board under automatic and human-based evaluation pro- tocols. Although we mainly focused on docu- ment encoding for summarization, in the future, we would like to take advantage the capabilities of BERT for language generation. Acknowledgments This research is supported by a Google PhD Fel- lowship to the \ufb01rst author. We gratefully acknowl- edge the support of the European Research Coun- cil (Lapata, award number 681760, \u201cTranslating Multiple Modalities into Text\u201d). We would also like to thank Shashi Narayan for providing us with the XSum dataset.",
            "References Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin- ton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450. Jaime G Carbonell and Jade Goldstein. 1998. The use of MMR and diversity-based reranking for reoder- ing documents and producing summaries. In Pro- ceedings of the 21st Annual International ACL SI- GIR Conference on Research and Development in Information Retrieval, pages 335\u2013336, Melbourne, Australia. Asli Celikyilmaz, Antoine Bosselut, Xiaodong He, and Yejin Choi. 2018. Deep communicating agents for abstractive summarization. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, Volume 1 (Long Pa- pers), pages 1662\u20131675, New Orleans, Louisiana. James Clarke and Mirella Lapata. 2010. Discourse constraints for document compression. Computa- tional Linguistics, 36(3):411\u2013441.",
            "James Clarke and Mirella Lapata. 2010. Discourse constraints for document compression. Computa- tional Linguistics, 36(3):411\u2013441. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Uni\ufb01ed language model pre-training for natural language understanding and generation. arXiv preprint arXiv:1905.03197. Yue Dong, Yikang Shen, Eric Crawford, Herke van Hoof, and Jackie Chi Kit Cheung. 2018. Bandit- Sum: Extractive summarization as a contextual ban- dit.",
            "arXiv preprint arXiv:1905.03197. Yue Dong, Yikang Shen, Eric Crawford, Herke van Hoof, and Jackie Chi Kit Cheung. 2018. Bandit- Sum: Extractive summarization as a contextual ban- dit. In Proceedings of the 2018 Conference on Em- pirical Methods in Natural Language Processing, pages 3739\u20133748, Brussels, Belgium. Greg Durrett, Taylor Berg-Kirkpatrick, and Dan Klein. 2016. Learning-based single-document summariza- tion with compression and anaphoricity constraints. In Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), pages 1998\u20132008, Berlin, Germany. Sergey Edunov, Alexei Baevski, and Michael Auli. 2019. Pre-trained language model representations for language generation.",
            "Sergey Edunov, Alexei Baevski, and Michael Auli. 2019. Pre-trained language model representations for language generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, Volume 1 (Long and Short Papers), pages 4052\u20134059, Minneapolis, Min- nesota. Sebastian Gehrmann, Yuntian Deng, and Alexander Rush. 2018. Bottom-up abstractive summarization. In Proceedings of the 2018 Conference on Empiri- cal Methods in Natural Language Processing, pages 4098\u20134109, Brussels, Belgium. Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K. Li. 2016. Incorporating copying mechanism in sequence-to-sequence learning. In Proceedings of the 54th Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers), pages 1631\u20131640, Berlin, Germany. Association for Computational Linguistics.",
            "Incorporating copying mechanism in sequence-to-sequence learning. In Proceedings of the 54th Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers), pages 1631\u20131640, Berlin, Germany. Association for Computational Linguistics. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su- leyman, and Phil Blunsom. 2015. Teaching ma- chines to read and comprehend. In Advances in Neu- ral Information Processing Systems, pages 1693\u2013 1701. Svetlana Kiritchenko and Saif Mohammad. 2017. Best-worst scaling more reliable than rating scales: A case study on sentiment intensity annotation. In Proceedings of the 55th Annual Meeting of the As- sociation for Computational Linguistics (Volume 2: Short Papers), pages 465\u2013470, Vancouver, Canada. Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush. 2017. OpenNMT: Open-source toolkit for neural machine translation.",
            "Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush. 2017. OpenNMT: Open-source toolkit for neural machine translation. In Proceedings of ACL 2017, System Demonstra- tions, pages 67\u201372, Vancouver, Canada. Wei Li, Xinyan Xiao, Yajuan Lyu, and Yuanzhuo Wang. 2018. Improving neural abstractive docu- ment summarization with explicit information selec- tion modeling. In Proceedings of the 2018 Con- ference on Empirical Methods in Natural Language Processing, pages 1787\u20131796, Brussels, Belgium. Chin-Yew Lin. 2004. ROUGE: A package for auto- matic evaluation of summaries. In Text Summariza- tion Branches Out, pages 74\u201381, Barcelona, Spain. Yang Liu, Ivan Titov, and Mirella Lapata. 2019. Single document summarization as tree induction.",
            "In Text Summariza- tion Branches Out, pages 74\u201381, Barcelona, Spain. Yang Liu, Ivan Titov, and Mirella Lapata. 2019. Single document summarization as tree induction. In Pro- ceedings of the 2019 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol- ume 1 (Long and Short Papers), pages 1745\u20131755, Minneapolis, Minnesota. Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language pro- cessing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Lin- guistics: System Demonstrations, pages 55\u201360, Bal- timore, Maryland. Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017. SummaRuNNer: A recurrent neural network based sequence model for extractive summarization of",
            "documents. In Proceedings of the 31st AAAI Con- ference on Arti\ufb01cial Intelligence, pages 3075\u20133081, San Francisco, California. Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, C\u00b8 a\u02d8glar G\u02d9ulc\u00b8ehre, and Bing Xiang. 2016. Ab- stractive text summarization using sequence-to- sequence RNNs and beyond. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pages 280\u2013290, Berlin, Ger- many. Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018a. Don\u2019t give me the details, just the summary! topic-aware convolutional neural networks for ex- treme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1797\u20131807, Brussels, Bel- gium. Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018b. Ranking sentences for extractive summa- rization with reinforcement learning.",
            "Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018b. Ranking sentences for extractive summa- rization with reinforcement learning. In Proceed- ings of the 2018 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, Volume 1 (Long Papers), pages 1747\u20131759, New Orleans, Louisiana. Romain Paulus, Caiming Xiong, and Richard Socher. 2018. A deep reinforced model for abstractive sum- marization. In Proceedings of the 6th International Conference on Learning Representations, Vancou- ver, Canada. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. In Proceedings of the 2018 Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long Papers), pages 2227\u20132237, New Orleans, Louisiana.",
            "In Proceedings of the 2018 Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long Papers), pages 2227\u20132237, New Orleans, Louisiana. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language un- derstanding by generative pre-training. In CoRR, abs\/1704.01444, 2017. Sascha Rothe, Shashi Narayan, and Aliaksei Sev- eryn. 2019. Leveraging pre-trained checkpoints for sequence generation tasks. arXiv preprint arXiv:1907.12461. Alexander M. Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sen- tence summarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan- guage Processing, pages 379\u2013389, Lisbon, Portugal. Evan Sandhaus. 2008.",
            "2015. A neural attention model for abstractive sen- tence summarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan- guage Processing, pages 379\u2013389, Lisbon, Portugal. Evan Sandhaus. 2008. The New York Times Annotated Corpus. Linguistic Data Consortium, Philadelphia, 6(12). Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointer- generator networks. In Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073\u2013 1083, Vancouver, Canada. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. 2016. Rethink- ing the inception architecture for computer vision. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), pages 2818\u20132826, Las Vegas, Nevada.",
            "2016. Rethink- ing the inception architecture for computer vision. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), pages 2818\u20132826, Las Vegas, Nevada. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, pages 5998\u20136008. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google\u2019s neural machine translation system: Bridging the gap between hu- man and machine translation. In arXiv preprint arXiv:1609.08144. Xingxing Zhang, Mirella Lapata, Furu Wei, and Ming Zhou. 2018.",
            "Google\u2019s neural machine translation system: Bridging the gap between hu- man and machine translation. In arXiv preprint arXiv:1609.08144. Xingxing Zhang, Mirella Lapata, Furu Wei, and Ming Zhou. 2018. Neural latent extractive document sum- marization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Pro- cessing, pages 779\u2013784, Brussels, Belgium. Xingxing Zhang, Furu Wei, and Ming Zhou. 2019. HI- BERT: Document level pre-training of hierarchical bidirectional transformers for document summariza- tion. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5059\u20135069, Florence, Italy. Association for Computational Linguistics. Qingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang, Ming Zhou, and Tiejun Zhao. 2018. Neural docu- ment summarization by jointly learning to score and select sentences.",
            "Association for Computational Linguistics. Qingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang, Ming Zhou, and Tiejun Zhao. 2018. Neural docu- ment summarization by jointly learning to score and select sentences. In Proceedings of the 56th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), pages 654\u2013663, Melbourne, Australia."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1908.08345.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 12116.999496459961,
    "avg_doclen_est": 175.6086883544922
}
