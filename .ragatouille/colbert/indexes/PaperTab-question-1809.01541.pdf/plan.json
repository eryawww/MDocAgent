{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Copenhagen at CoNLL\u2013SIGMORPHON 2018: Multilingual In\ufb02ection in Context with Explicit Morphosyntactic Decoding Yova Kementchedjhieva University of Copenhagen yova@di.ku.dk Johannes Bjerva University of Copenhagen bjerva@di.ku.dk Isabelle Augenstein University of Copenhagen augenstein@di.ku.dk Abstract This paper documents the Team Copenhagen system which placed \ufb01rst in the CoNLL\u2013 SIGMORPHON 2018 shared task on univer- sal morphological rein\ufb02ection, Task 2 with an overall accuracy of 49.87. Task 2 focuses on morphological in\ufb02ection in context: generat- ing an in\ufb02ected word form, given the lemma of the word and the context it occurs in. Previous SIGMORPHON shared tasks have focused on context-agnostic in\ufb02ection\u2014the \u201cin\ufb02ection in context\u201d task was introduced this year.",
            "Previous SIGMORPHON shared tasks have focused on context-agnostic in\ufb02ection\u2014the \u201cin\ufb02ection in context\u201d task was introduced this year. We ap- proach this with an encoder-decoder architec- ture over character sequences with three core innovations, all contributing to an improve- ment in performance: (1) a wide context win- dow; (2) a multi-task learning approach with the auxiliary task of MSD prediction; (3) train- ing models in a multilingual fashion. 1 Introduction This paper describes our approach and results for Task 2 of the CoNLL\u2013SIGMORPHON 2018 shared task on universal morphological rein\ufb02ec- tion (Cotterell et al., 2018). The task is to generate an in\ufb02ected word form given its lemma and the context in which it occurs.",
            "The task is to generate an in\ufb02ected word form given its lemma and the context in which it occurs. Morphological (re)in\ufb02ection from context is of particular relevance to the \ufb01eld of computational linguistics: it is compelling to estimate how well a machine-learned system can capture the mor- phosyntactic properties of a word given its con- text, and map those properties to the correct sur- face form for a given lemma. There are two tracks of Task 2 of CoNLL\u2013 SIGMORPHON 2018: in Track 1 the context is given in terms of word forms, lemmas and mor- phosyntactic descriptions (MSD); in Track 2 only word forms are available. See Table 1 for an ex- ample. Task 2 is additionally split in three settings based on data size: high, medium and low, with high-resource datasets consisting of up to 70K in- stances per language, and low-resource datasets consisting of only about 1K instances.",
            "See Table 1 for an ex- ample. Task 2 is additionally split in three settings based on data size: high, medium and low, with high-resource datasets consisting of up to 70K in- stances per language, and low-resource datasets consisting of only about 1K instances. The baseline provided by the shared task organ- isers is a seq2seq model with attention (similar to the winning system for rein\ufb02ection in CoNLL\u2013 SIGMORPHON 2016, Kann and Sch\u00a8utze (2016)), which receives information about context through an embedding of the two words immediately adja- cent to the target form. We use this baseline im- plementation as a starting point and achieve the best overall accuracy of 49.87 on Task 2 by intro- ducing three augmentations to the provided base- line system: (1) We use an LSTM to encode the entire available context; (2) We employ a multi- task learning approach with the auxiliary objective of MSD prediction; and (3) We train the auxiliary component in a multilingual fashion, over sets of two to three languages.",
            "In analysing the performance of our system, we found that encoding the full context improves per- formance considerably for all languages: 11.15 percentage points on average, although it also highly increases the variance in results. Multi-task learning, paired with multilingual training and subsequent monolingual \ufb01netuning, scored high- est for \ufb01ve out of seven languages, improving ac- curacy by another 9.86% on average. 2 System Description Our system is a modi\ufb01cation of the provided CoNLL\u2013SIGMORPHON 2018 baseline system, so we begin this section with a reiteration of the baseline system architecture, followed by a de- scription of the three augmentations we introduce. arXiv:1809.01541v1  [cs.CL]  5 Sep 2018",
            "WORD FORMS We were \u25a1 to feel very welcome . LEMMAS we be make to feel very welcome . MSD TAGS PRO;NOM;PL;1 AUX;IND;PST;FIN \u25a1 PART V;NFIN ADV ADJ PUNCT Table 1: Example input sentence. Context MSD tags and lemmas, marked in gray, are only available in Track 1. The cyan square marks the main objective of predicting the word form made. The magenta square marks the auxiliary objective of predicting the MSD tag V;PST;V.PTCP;PASS. 2.1 Baseline The CoNLL\u2013SIGMORPHON 2018 baseline1 is described as follows: The system is an encoder-decoder on character sequences. It takes a lemma as input and generates a word form. The process is conditioned on the con- text of the lemma [. . . ] The baseline treats the lemma, word form and MSD of the previous and following word as context in track 1. In track 2, the baseline only considers the word forms of the previous and next word.",
            "The process is conditioned on the con- text of the lemma [. . . ] The baseline treats the lemma, word form and MSD of the previous and following word as context in track 1. In track 2, the baseline only considers the word forms of the previous and next word. [...] The baseline system concatenates em- beddings for context word forms, lem- mas and MSDs into a context vector. The baseline then computes character embeddings for each character in the in- put lemma. Each of these is concate- nated with a copy of the context vector. The resulting sequence of vectors is en- coded using an LSTM encoder. Subse- quently, an LSTM decoder generates the characters in the output word form using encoder states and an attention mecha- nism.",
            "Each of these is concate- nated with a copy of the context vector. The resulting sequence of vectors is en- coded using an LSTM encoder. Subse- quently, an LSTM decoder generates the characters in the output word form using encoder states and an attention mecha- nism. To that we add a few details regarding model size and training schedule: \u2022 the number of LSTM layers is one; \u2022 embedding size, LSTM layer size and atten- tion layer size is 100; \u2022 models are trained for 20 epochs; \u2022 on every epoch, training data is subsampled at a rate of 0.3; \u2022 LSTM dropout is applied at a rate 0.3; \u2022 context word forms are randomly dropped at a rate of 0.1; \u2022 the Adam optimiser is used, with a default learning rate of 0.001; and 1Code available at: https:\/\/github.com\/sigmorphon\/conll2018 m a d e We were to  ...  m a k e Main  Task  Auxiliary  Task  V PST  V.PTCP PASS  Figure 1: Schematic representation of our approach.",
            "The focus here is on the prediction of the \ufb01nal char- acter, e, of the word form made. The attention matrix indicates that this character should be based on the \ufb01nal state of the encoder, which contains information about the \ufb01nal character of the input form, and the past and future context. The input and output of the auxiliary decoder are marked in magenta. \u2022 trained models are evaluated on the develop- ment data (the data for the shared task comes already split in train and dev sets). 2.2 Our system Here we compare and contrast our system2 to the baseline system. A diagram of our system is shown in Figure 1. 2.2.1 Entire Context Encoded with LSTMs The idea behind this modi\ufb01cation is to provide the encoder with access to all morpho-syntactic cues present in the sentence. In contrast to the baseline, which only encodes the immediately adjacent con- text of a target word, we encode the entire con- text. All context word forms, lemmas, and MSD 2Code available at: https:\/\/github.com\/ YovaKem\/inflection_in_context",
            "Track 1 Track 2 base our base our DE 64.51 72.40 65.72 64.81 EN 72.91 77.84 70.39 71.90 ES 53.44 56.24 51.05 48.95 high FI 49.05 55.27 34.82 32.40 FR 63.54 70.67 58.45 61.51 RU 71.18 77.91 46.89 49.00 SV 62.23 69.26 54.04 55.96 DE 54.40 62.18 56.93 57.33 EN 60.02 66.67 57.60 66.67 ES 23.14 51.33 41.23 42.50 med.",
            "FI 28.21 35.71 19.19 22.24 FR 45.01 60.29 21.38 45.62 RU 50.30 63.05 30.52 35.94 SV 47.55 57.66 43.09 45.96 DE 0.20 4.85 0.10 18.91 EN 1.81 33.84 2.22 59.42 ES 8.98 31.42 8.98 31.84 low FI 0.76 12.83 0.38 12.33 FR 0.00 34.42 0.00 29.53 RU 0.00 25.90 2.71 22.69 SV 1.17 27.55 0.96 30.96 Table 2: Of\ufb01cial shared task test set results. tags (in Track 1) are embedded in their respective high-dimensional spaces as before, and their em- beddings are concatenated.",
            "tags (in Track 1) are embedded in their respective high-dimensional spaces as before, and their em- beddings are concatenated. However, we now re- duce the entire past context to a \ufb01xed-size vector by encoding it with a forward LSTM, and we sim- ilarly represent the future context by encoding it with a backwards LSTM. 2.2.2 Auxiliary Task: MSD of the Target Form We introduce an auxiliary objective that is meant to increase the morpho-syntactic awareness of the encoder and to regularise the learning process\u2014 the task is to predict the MSD tag of the target form. MSD tag predictions are conditioned on the context encoding, as described in 2.2.1. Tags are generated with an LSTM one component at a time, e.g. the tag PRO;NOM;SG;1 is predicted as a se- quence of four components, \u27e8PRO, NOM, SG, 1\u27e9. For every training instance, we backpropagate the sum of the main loss and the auxiliary loss without any weighting. As MSD tags are only available in Track 1, this augmentation only applies to this track.",
            "For every training instance, we backpropagate the sum of the main loss and the auxiliary loss without any weighting. As MSD tags are only available in Track 1, this augmentation only applies to this track. 2.2.3 Multilinguality The parameters of the entire MSD (auxiliary-task) decoder are shared across languages. Since a grouping of the languages based on lan- guage family would have left several languages in single-member groups (e.g. Russian is the sole representative of the Slavic family), we experi- ment with random groupings of two to three lan- guages. Multilingual training is performed by randomly alternating between languages for every new minibatch. We do not pass any information to the auxiliary decoder as to the source language of the signal it is receiving, as we assume abstract morpho-syntactic features are shared across lan- guages. Finetuning After 20 epochs of multilingual training, we perform 5 epochs of monolingual \ufb01netuning for each language. For this phase, we reduce the learning rate to a tenth of the original learning rate, i.e.",
            "Finetuning After 20 epochs of multilingual training, we perform 5 epochs of monolingual \ufb01netuning for each language. For this phase, we reduce the learning rate to a tenth of the original learning rate, i.e. 0.0001, to ensure that the models are indeed being \ufb01netuned rather than retrained. 2.2.4 Model Size and Training Schedule We keep all hyperparameters the same as in the baseline. Training data is split 90:10 for train- ing and validation. We train our models for 50 epochs, adding early stopping with a tolerance of \ufb01ve epochs of no improvement in the validation loss. We do not subsample from the training data. 2.2.5 Ensemble Prediction We train models for 50 different random com- binations of two to three languages in Track 1, and 50 monolingual models for each language in Track 2.",
            "We do not subsample from the training data. 2.2.5 Ensemble Prediction We train models for 50 different random com- binations of two to three languages in Track 1, and 50 monolingual models for each language in Track 2. Instead of picking the single model that performs best on the development set and thus risking to select a model that highly over\ufb01ts that data, we use an ensemble of the \ufb01ve best mod- els, and make the \ufb01nal prediction for a given target form with a majority vote over the \ufb01ve predictions. 3 Results and Discussion Test results are listed in Table 2. Our system outperforms the baseline for all settings and lan- guages in Track 1 and for almost all in Track 2\u2014 only in the high resource setting is our system not de\ufb01nitively superior to the baseline. Interestingly, our results in the low resource set- ting are often higher for Track 2 than for Track 1, even though contextual information is less explicit",
            "in the Track 2 data and the multilingual multi- tasking approach does not apply to this track. We interpret this \ufb01nding as an indicator that a simpler model with fewer parameters works better in a set- ting of limited training data. Nevertheless, we fo- cus on the low resource setting in the analysis be- low due to time limitations. As our Track 1 results are still substantially higher than the baseline re- sults, we consider this analysis valid and insight- ful. 3.1 Ablation Study We analyse the incremental effect of the differ- ent features in our system, focusing on the low- resource setting in Track 1 and using development data. Entire Context Encoded with LSTMs Encod- ing the entire context with an LSTM highly in- creases the variance of the observed results. So we trained \ufb01fty models for each language and each ar- chitecture. Figure 2 visualises the means and stan- dard deviations over the trained models. In addi- tion, we visualise the average accuracy for the \ufb01ve best models for each language and architecture, as these are the models we use in the \ufb01nal ensemble prediction.",
            "Figure 2 visualises the means and stan- dard deviations over the trained models. In addi- tion, we visualise the average accuracy for the \ufb01ve best models for each language and architecture, as these are the models we use in the \ufb01nal ensemble prediction. Below we refer to these numbers only. The results indicate that encoding the full con- text with an LSTM highly enhances the perfor- mance of the model, by 11.15% on average. This observation explains the high results we obtain also for Track 2. Auxiliary Task: MSD of the Target Form Adding the auxiliary objective of MSD prediction has a variable effect: for four languages (DE, EN, ES, and SV) the effect is positive, while for the rest it is negative. We consider this to be an issue of insuf\ufb01cient data for the training of the auxil- iary component in the low resource setting we are working with. Multilinguality We indeed see results improv- ing drastically with the introduction of multi- lingual training, with multilingual results being 7.96% higher than monolingual ones on average.",
            "Multilinguality We indeed see results improv- ing drastically with the introduction of multi- lingual training, with multilingual results being 7.96% higher than monolingual ones on average. We studied the \ufb01ve best models for each lan- guage as emerging from the multilingual training (listed in Table 3) and found no strong linguistic patterns. The EN\u2013SV pairing seems to yield good models for these languages, which could be ex- plained in terms of their common language family DE FI SV FI, SV RU, FR FR, FI EN RU, SV RU, FI RU,FR SV, ES SV, FR ES DE FI SV, DE SV,EN SV,FR FI DE ES FR, ES EN,RU RU,SV FR SV,EN EN,ES DE,FI SV,EN EN,SV RU SV DE,FR EN,SV SV,FR EN,FI SV EN,DE FI,EN FR,RU ES,EN RU, EN Table 3: Five best multilingual models for each lan- guage. and similar morphology.",
            "and similar morphology. The other natural pair- ings, however, FR\u2013ES, and DE\u2013SV, are not so fre- quent among the best models for these pairs of lan- guages. Finally, monolingual \ufb01netuning improves accu- racy across the board, as one would expect, by 2.72% on average. Overall The \ufb01nal observation to be made based on this breakdown of results is that the multi- tasking approach paired with multilingual train- ing and subsequent monolingual \ufb01netuning out- performs the other architectures for \ufb01ve out of seven languages: DE, EN, FR, RU and SV. For the other two languages in the dataset, ES and FI, the difference between this approach and the approach that emerged as best for them is less than 1%. The overall improvement of the multi- lingual multi-tasking approach over the baseline is 18.30%. 3.2 Error analysis Here we study the errors produced by our sys- tem on the English test set to better understand the remaining shortcomings of the approach.",
            "The overall improvement of the multi- lingual multi-tasking approach over the baseline is 18.30%. 3.2 Error analysis Here we study the errors produced by our sys- tem on the English test set to better understand the remaining shortcomings of the approach. A small portion of the wrong predictions point to an incorrect interpretation of the morpho-syntactic conditioning of the context, e.g. the system pre- dicted plan instead of plans in the context Our include raising private capital. The majority of wrong predictions, however, are nonsensical, like bomb for job, \ufb01fy for \ufb01xing, and gnderrate for un- derstand. This observation suggests that gener- ally the system did not learn to copy the charac- ters of lemma into in\ufb02ected form, which is all it needs to do in a large number of cases. This issue could be alleviated with simple data augmentation techniques that encourage autoencoding (see, e.g., Bergmanis et al., 2017).",
            "DE EN ES FI FR RU SV Language 0 10 20 30 Accuracy Baseline LSTM Enc Multi-task Multi-lingual Finetuned Figure 2: Mean (\u2022) and standard deviation (error bars) over 100 models trained for each language and architecture, and average (\u00d7) over the 5 best models. LSTM Enc refers to a model that encodes the full context with an LSTM; Multi-task builds on LSTM Enc with an auxiliary objective of MSD prediction; Multilingual refers to a model with an auxiliary component trained in a multilingual fashion; Finetuned refers to a multilingual model topped with monolingual \ufb01netuning. Figure 3: Accuracy on the auxiliary task of MSD pre- diction with different models. See the caption of Fig- ure 2 for more details. 3.3 MSD prediction Figure 3 summarises the average MSD-prediction accuracy for the multi-tasking experiments dis- cussed above.3 Accuracy here is generally higher than on the main task, with the multilingual \ufb01ne- tuned setup for Spanish and the monolingual setup for French scoring best: 66.59% and 65.35%, re- spectively.",
            "This observation illustrates the added dif\ufb01culty of generating the correct surface form even when the morphosyntactic description has been identi\ufb01ed correctly. We observe some correlation between these numbers and accuracy on the main task: for DE, EN, RU and SV, the brown, pink and blue bars here pattern in the same way as the correspond- ing \u00d7\u2019s in Figure 2. One notable exception to this 3As MSD tags are not available for target forms in the de- velopment data, the accuracy of MSD prediction is measured over all other nouns, adjectives and verbs in the dataset. pattern is FR where in\ufb02ection gains a lot from mul- tilingual training, while MSD prediction suffers greatly. Notice that the magnitude of change is not always the same, however, even when the gen- eral direction matches: for RU, for example, mul- tilingual training bene\ufb01ts in\ufb02ection much more than in bene\ufb01ts MSD prediction, even though the MSD decoder is the only component that is actu- ally shared between languages.",
            "This observation illustrates the two-fold effect of multi-task train- ing: an auxiliary task can either inform the main task through the parameters the two tasks share, or it can help the main task learning through its regularising effect. 4 Related Work Our system is inspired by previous work on multi- task learning and multi-lingual learning, mainly building on two intuitions: (1) jointly learning re- lated tasks tends to be bene\ufb01cial (Caruana, 1997; S\u00f8gaard and Goldberg, 2016; Plank et al., 2016; Bjerva et al., 2016; Bjerva, 2017b); and (2) jointly learning related languages in an MTL-inspired framework tends to be bene\ufb01cial (Bjerva, 2017a; Johnson et al., 2017; de Lhoneux et al., 2018). In the context of computational morphology, multi- lingual approaches have previously been em- ployed for morphological rein\ufb02ection (Bergma- nis et al., 2017) and for paradigm completion (Kann et al., 2017).",
            "In the context of computational morphology, multi- lingual approaches have previously been em- ployed for morphological rein\ufb02ection (Bergma- nis et al., 2017) and for paradigm completion (Kann et al., 2017). In both of these cases, however, the available datasets covered more lan- guages, 40 and 21, respectively, which allowed for linguistically-motivated language groupings and for parameter sharing directly on the level of char- acters. De Lhoneux et al. (2018) explore param-",
            "eter sharing between related languages for depen- dency parsing, and \ufb01nd that sharing is more bene- \ufb01cial in the case of closely related languages. 5 Conclusions In this paper we described our system for the CoNLL\u2013SIGMORPHON 2018 shared task on Universal Morphological Rein\ufb02ection, Task 2, which achieved the best performance out of all systems submitted, an overall accuracy of 49.87. We showed in an ablation study that this is due to three core innovations, which extend a character- based encoder-decoder model: (1) a wide con- text window, encoding the entire available con- text; (2) multi-task learning with the auxiliary task of MSD prediction, which acts as a regulariser; (3) a multilingual approach, exploiting informa- tion across languages. In future work we aim to gain better understanding of the increase in vari- ance of the results introduced by each of our mod- i\ufb01cations and the reasons for the varying effect of multi-task learning for different languages. Acknowledgements We gratefully acknowledge the support of the NVIDIA Corporation with the donation of the Ti- tan Xp GPU used for this research.",
            "Acknowledgements We gratefully acknowledge the support of the NVIDIA Corporation with the donation of the Ti- tan Xp GPU used for this research. References Toms Bergmanis, Katharina Kann, Hinrich Sch\u00a8utze, and Sharon Goldwater. 2017. Training data aug- mentation for low-resource morphological in\ufb02ec- tion. Proceedings of the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological Rein- \ufb02ection, pages 31\u201339. Johannes Bjerva. 2017a. One Model to Rule them all \u2013 Multitask and Multilingual Modelling for Lexical Analysis. Ph.D. thesis, University of Groningen. Johannes Bjerva. 2017b. Will my auxiliary tagging task help? Estimating Auxiliary Tasks Effectivity in Multi-Task Learning. In NoDaLiDa, pages 216\u2013 220. Johannes Bjerva, Barbara Plank, and Johan Bos. 2016. Semantic tagging with deep residual networks. In COLING, pages 3531\u20133541. Rich Caruana. 1997.",
            "In NoDaLiDa, pages 216\u2013 220. Johannes Bjerva, Barbara Plank, and Johan Bos. 2016. Semantic tagging with deep residual networks. In COLING, pages 3531\u20133541. Rich Caruana. 1997. Multitask learning. Machine Learning, 28 (1):41\u201375. Ryan Cotterell, Christo Kirov, John Sylak-Glassman, G\u00b4eraldine Walther, Ekaterina Vylomova, Arya D. McCarthy, Katharina Kann, Sebastian Mielke, Gar- rett Nicolai, Miikka Silfverberg, David Yarowsky, Jason Eisner, and Mans Hulden. 2018. The CoNLL\u2013 SIGMORPHON 2018 Shared Task: Universal Mor- phological Rein\ufb02ection. In Proceedings of the CoNLL\u2013SIGMORPHON 2018 Shared Task: Univer- sal Morphological Rein\ufb02ection, Brussels, Belgium. Association for Computational Linguistics.",
            "In Proceedings of the CoNLL\u2013SIGMORPHON 2018 Shared Task: Univer- sal Morphological Rein\ufb02ection, Brussels, Belgium. Association for Computational Linguistics. Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi\u00b4egas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2017. Google\u2019s multilingual neural machine translation system: En- abling zero-shot translation. Transactions of the As- sociation for Computational Linguistics, 5:339\u2013351. Katharina Kann, Ryan Cotterell, and Hinrich Sch\u00a8utze. 2017. One-shot neural cross-lingual transfer for paradigm completion. In Proceedings of the 55th Annual Meeting of the Association for Computa- tional Linguistics (Volume 1: Long Papers), pages 1993\u20132003. Association for Computational Linguis- tics. Katharina Kann and Hinrich Sch\u00a8utze. 2016.",
            "Association for Computational Linguis- tics. Katharina Kann and Hinrich Sch\u00a8utze. 2016. MED: The LMU system for the SIGMORPHON 2016 shared task on morphological rein\ufb02ection. In Proceedings of the 14th SIGMORPHON Workshop on Computa- tional Research in Phonetics, Phonology, and Mor- phology, pages 62\u201370. Miryam de Lhoneux, Johannes Bjerva, Isabelle Augen- stein, and Anders S\u00f8gaard. 2018. Parameter sharing between dependency parsers for related languages. In Proceedings of EMNLP. Barbara Plank, Anders S\u00f8gaard, and Yoav Goldberg. 2016. Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss. In Proceedings of ACL (Short Pa- pers). Anders S\u00f8gaard and Yoav Goldberg. 2016. Deep multi-task learning with low level tasks supervised at lower layers. In Proceedings of ACL (Short Pa- pers)."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1809.01541.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 5355.999893188477,
    "avg_doclen_est": 184.6896514892578
}
