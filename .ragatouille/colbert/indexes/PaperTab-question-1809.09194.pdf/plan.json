{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Stochastic Answer Networks for SQuAD 2.0 Xiaodong Liu\u2020, Wei Li\u2020, Yuwei Fang\u2020, Aerin Kim\u2020, Kevin Duh\u2021 and Jianfeng Gao\u2020 \u2020 Microsoft Research, Redmond, WA, USA \u2021 Johns Hopkins University, Baltimore, MD, USA \u2020{xiaodl,wli,yuwfan,ahkim,jfgao}@microsoft.com \u2021kevinduh@cs.jhu.edu Abstract This paper presents an extension of the Stochastic Answer Network (SAN), one of the state-of-the-art machine reading comprehen- sion models, to be able to judge whether a question is unanswerable or not. The extended SAN contains two components: a span detec- tor and a binary classi\ufb01er for judging whether the question is unanswerable, and both compo- nents are jointly optimized. Experiments show that SAN achieves the results competitive to the state-of-the-art on Stanford Question An- swering Dataset (SQuAD) 2.0. To facilitate the research on this \ufb01eld, we release our code: https:\/\/github.com\/kevinduh\/san mrc.",
            "Experiments show that SAN achieves the results competitive to the state-of-the-art on Stanford Question An- swering Dataset (SQuAD) 2.0. To facilitate the research on this \ufb01eld, we release our code: https:\/\/github.com\/kevinduh\/san mrc. 1 Background Teaching machine to read and comprehend a given passage\/paragraph and answer its corresponding questions is a challenging task. It is also one of the long-term goals of natural language understand- ing, and has important applications in e.g., build- ing intelligent agents for conversation and cus- tomer service support. In a real world setting, it is necessary to judge whether the given questions are answerable given the available knowledge, and then generate correct answers for the ones which are able to infer an answer in the passage or an empty answer (as an unanswerable question) oth- erwise.",
            "In a real world setting, it is necessary to judge whether the given questions are answerable given the available knowledge, and then generate correct answers for the ones which are able to infer an answer in the passage or an empty answer (as an unanswerable question) oth- erwise. In comparison with many existing MRC sys- tems (Wang and Jiang, 2016; Liu et al., 2018b; Yu et al., 2018; Seo et al., 2016; Shen et al., 2017), which extract answers by \ufb01nding a sub-string in the passages\/paragraphs, we propose a model that not only extracts answers but also predicts whether such an answer should exist. Using a multi-task learning approach (c.f. (Liu et al., 2015)), we ex- tend the Stochastic Answer Network (SAN) (Liu et al., 2018b) for MRC answer span detector to in- clude a classi\ufb01er that whether the question is unan- Paragraph: The Legend of Zelda: Twilight Princess   is an action-adventure game developed and  published by Nintendo for the GameCube and Wii  home video game consoles.",
            "... The Wii version was  released alongside the console in North America  in November 2006, and in Japan, Europe, and  Australia the following month. The GameCube  version was released worldwide in December  2006. Question 1: What consoles can be used to play  Twilight Princess? Answer: GameCube and Wii Question 2: When was Australia Twilight  launched in North America? Answer: Impossible Figure 1: Examples from SQuAD v2.0. The \ufb01rst question is answerable which indicates its answer high- lighted in blue can be found in the paragraph; while the second question is unanswerable and its plausible an- swer is highlighted in red. swerable. The unanswerable classi\ufb01er is a pair- wise classi\ufb01cation model (Liu et al., 2018a) which predicts a label indicating whether the given pair of a passage and a question is unanswerable. The two models share the same lower layer to save the number of parameters, and separate the top lay- ers for different tasks (the span detector and bi- nary classi\ufb01er).",
            "The two models share the same lower layer to save the number of parameters, and separate the top lay- ers for different tasks (the span detector and bi- nary classi\ufb01er). Our model is pretty simple and intuitive, yet ef\ufb01cient. Without relying on the large pre-trained language models (ELMo) (Peters et al., 2018), the proposed model achieves com- petitive results to the state-of-the-art on Stanford Question Answering Dataset (SQuAD) 2.0. The contribution of this work is summarized as arXiv:1809.09194v1  [cs.CL]  24 Sep 2018",
            "Question Document Word Embedding Surface Feature What ... Twilight Princess? Lexicon Encoding Layer The Legend of Zelda: Twilight \u2026 in December 2006. Lexicon Encoding Layer The Legend of Zelda: Twilight \u2026 in December 2006. Lexicon Encoding Layer Memory  Weighted Sum  Contextual Encoding Layer Contextual Encoding Layer Memory Generation Layer SAN Answer Module  Weighted Sum  Unanswerable Classifier   Figure 2: Architecture of the proposed model for Reading Comprehension: It includes two components: a span detector (the upper left SAN answer module) and an unanswerable classi\ufb01er (the upper right module). It contains two sets of layers: the shared layers including a lexicon encoding layer, contextual encoding layer and memory generation layer; and the task speci\ufb01c layers including the SAN answer module for span detection, and a binary classi\ufb01er determining whether the question is unanswerable. The model is learned jointly. follows. First, we propose a simple yet ef\ufb01cient model for MRC that handles unanswerable ques- tions and is optimized jointly. Second, our model achieves competitive results on SQuAD v2.0.",
            "The model is learned jointly. follows. First, we propose a simple yet ef\ufb01cient model for MRC that handles unanswerable ques- tions and is optimized jointly. Second, our model achieves competitive results on SQuAD v2.0. 2 Model The Machine Reading Comprehension is a task which takes a question Q = {q0, q1, ..., qm\u22121} and a passage\/paragraph P = {p0, p1, ..., pn\u22121} as in- puts, and aims to \ufb01nd an answer span A in P. We assume that if the question is answerable, the an- swer A exists in P as a contiguous text string; oth- erwise, A is an empty string indicating an unan- swerable question. Note that to handle the unan- swerable questions, we manually append a dumpy text string NULL at the end of each corresponding passage\/paragraph. Formally, the answer is for- mulated as A = {abegin, aend}. In case of unan- swerable questions, A points to the last token of the passage.",
            "Formally, the answer is for- mulated as A = {abegin, aend}. In case of unan- swerable questions, A points to the last token of the passage. Our model is a variation of SAN (Liu et al., 2018b), as shown in Figure 2. The main differ- ence is the additional binary classi\ufb01er added in the model justifying whether the question is unan- swerable. Roughly, the model includes two dif- ferent layers: the shared layer and task speci\ufb01c layer. The shared layer is almost identical to the lower layers of SAN, which has a lexicon encod- ing layer, a contextual layer and a memory gen- eration layer. On top of it, there are different an- swer modules for different tasks. We employ the SAN answer module for the span detector and a one-layer feed forward neural network for the bi- nary classi\ufb01cation task. It can also be viewed as a multi-task learning (Caruana, 1997; Liu et al., 2015; Xu et al., 2018).",
            "It can also be viewed as a multi-task learning (Caruana, 1997; Liu et al., 2015; Xu et al., 2018). We will brie\ufb02y describe the model from ground up as follows. Detailed descriptions can be found in (Liu et al., 2018b). Lexicon Encoding Layer. We map the sym- bolic\/surface feature of P and Q into neural space via word embeddings 1, 16-dim part-of-speech 1We use 300-dim GloVe (Pennington et al., 2014) vectors.",
            "(POS) tagging embeddings, 8-dim named-entity embeddings and 4-dim hard-rule features2. Note that we use small embedding size of POS and NER to reduce model size and they mainly serve the role of coarse-grained word clusters. Addi- tionally, we use question enhanced passages word embeddings which can viewwed as soft match- ing between questions and passages. At last, we use two separate two-layer position-wise Feed- Forward Networks (FFN) (Vaswani et al., 2017; Liu et al., 2018b) to map both question and pas- sage encodings into the same dimension. As re- sults, we obtain the \ufb01nal lexicon embeddings for the tokens for Q as a matrix Eq \u2208Rd\u00d7m, and to- kens in P as Eq \u2208Rd\u00d7n. Contextual Encoding Layer. A shared two- layers BiLSTM is used on the top to encode the contextual information of both passages and ques- tions.",
            "Contextual Encoding Layer. A shared two- layers BiLSTM is used on the top to encode the contextual information of both passages and ques- tions. To avoid over\ufb01tting, we concatenate a pre- trained 600-dimensional CoVe vectors3 (McCann et al., 2017) trained on German-English machine translation dataset, with the aforementioned lexi- con embeddings as the \ufb01nal input of the contex- tual encoding layer, and also with the output of the \ufb01rst contextual encoding layer as the input of its second encoding layer. Thus, we obtain the \ufb01- nal representation of the contextual encoding layer by a concatenation of the outputs of two BiLSTM: Hq \u2208R4d\u00d7m for questions and Hp \u2208R4d\u00d7n for passages. Memory Generation Layer. In this layer, we generate a working memory by fusing information from both passages Hp and questions Hq. The attention function (Vaswani et al., 2017) is used to compute the similarity score between passages and questions as: C = dropout \u0010 fattention( \u02c6Hq, \u02c6Hp) \u0011 \u2208Rm\u00d7n.",
            "The attention function (Vaswani et al., 2017) is used to compute the similarity score between passages and questions as: C = dropout \u0010 fattention( \u02c6Hq, \u02c6Hp) \u0011 \u2208Rm\u00d7n. Note that \u02c6 Hq and \u02c6 Hp is transformed from Hq and Hp by one layer neural network ReLU(Wx), re- spectively. A question-aware passage representa- tion is computed as Up = concat(Hp, HqC). Af- ter that, we use the method of (Lin et al., 2017) to apply self attention to the passage: \u02c6Up = Updropdiag(fattention(Up, Up)), where dropdiag means that we only drop diagonal elements on the similarity matrix (i.e., attention 2It includes 3 matching features which are determined based on the original word, lower case, and lemma, respec- tively, and one term sequence feature. 3https:\/\/github.com\/salesforce\/cove with itself).",
            "3https:\/\/github.com\/salesforce\/cove with itself). At last, Up and \u02c6Up are concatenated and are passed through a BiLSTM to form the \ufb01nal memory: M = BiLSTM([Up]; \u02c6Up]). Span detector. We adopt a multi-turn an- swer module for the span detector (Liu et al., 2018b). Formally, at time step t in the range of {1, 2, ..., T \u22121}, the state is de\ufb01ned by st = GRU(st\u22121, xt). The initial state s0 is the sum- mary of the Q: s0 = P j \u03b1jHq j , where \u03b1j = exp(w0\u00b7Hq j ) P j\u2032 exp(w0\u00b7Hq j\u2032). Here, xt is computed from the previous state st\u22121 and memory M: xt = P j \u03b2jMj and \u03b2j = softmax(st\u22121W1M). Fi- nally, a bilinear function is used to \ufb01nd the begin and end point of answer spans at each reasoning step t \u2208{0, 1, . . .",
            "Fi- nally, a bilinear function is used to \ufb01nd the begin and end point of answer spans at each reasoning step t \u2208{0, 1, . . . , T \u22121}: P begin t = softmax(stW2M) (1) P end t = softmax(stW3M)4. (2) The \ufb01nal prediction is the average of each time step: P begin = 1 T P t P begin t , P end = 1 T P t P end t . We randomly apply dropout on the step level in each time step during training, as done in (Liu et al., 2018b). Unanswerable classi\ufb01er. We adopt a one-layer neural network as our unanswerable binary classi- \ufb01er: P u = sigmoid([s0; m0]W4) (3) , where m0 is the summary of the memory: m0 = P j \u03b3jMj, where \u03b3j = exp(w5\u00b7Mj) P j\u2032 exp(w5\u00b7Mj\u2032). P u de- notes the probability of the question which is unanswerable.",
            "P u de- notes the probability of the question which is unanswerable. Objective The objective function of the joint model has two parts: Ljoint = Lspan + \u03bbLclassifier (4) Following (Wang and Jiang, 2016), the span loss function is de\ufb01ned: Lspan = \u2212(log P begin + log P end). (5) The objective function of the binary classi\ufb01er is de\ufb01ned: Lclassifier = \u2212{y ln P u + (1 \u2212y) ln(1 \u2212P u)} (6) where y \u2208{0, 1} is a binary variable: y = 1 in- dicates the question is unanswerable and y = 0 denotes the question is answerable. 4Note that we use a simper formula in Eq 2 as (Liu et al., 2018b).",
            "3 Experiment 3.1 Setup We evaluate our system on SQuAD 2.0 dataset (Rajpurkar et al., 2018), a new MRC dataset which is a combination of Stanford Question Answering Dataset (SQuAD) 1.0 (Rajpurkar et al., 2016) and additional unanswerable question-answer pairs. The answerable pairs are around 100K; while the unanswerable questions are around 53K. This dataset contains about 23K passages and they come from approximately 500 Wikipedia articles. All the questions and answers are obtained by crowd-sourcing. Two evaluation metrics are used: Exact Match (EM) and Macro-averaged F1 score (F1) (Rajpurkar et al., 2018). 3.2 Implementation details We utilize spaCy5 tool to tokenize the both pas- sages and questions, and generate lemma, part- of-speech and named entity tags. The word embeddings are initialized with pre-trained 300- dimensional GloVe (Pennington et al., 2014). A 2-layer BiLSTM is used encoding the contextual information of both questions and passages.",
            "The word embeddings are initialized with pre-trained 300- dimensional GloVe (Pennington et al., 2014). A 2-layer BiLSTM is used encoding the contextual information of both questions and passages. Re- garding the hidden size of our model, we search greedily among {128, 256, 300}. During training, Adamax (Kingma and Ba, 2014) is used as our op- timizer. The min-batch size is set to 32. The learn- ing rate is initialized to 0.002 and it is halved after every 10 epochs. The dropout rate is set to 0.1. To prevent over\ufb01tting, we also randomly set 0.5% words in both passages and questions as unknown words during the training. Here, we use a special token unk to indicate a word which doesn\u2019t appear in GloVe. \u03bb in Eq 4 is set to 1. 4 Results We would like to investigate effectiveness the pro- posed joint model. To do so, the same shared layer\/architecture is employed in the following variants of the proposed model: 1.",
            "\u03bb in Eq 4 is set to 1. 4 Results We would like to investigate effectiveness the pro- posed joint model. To do so, the same shared layer\/architecture is employed in the following variants of the proposed model: 1. SAN: it is standard SAN model 6 (Liu et al., 2018b), which is trained by using Eq 5. 2. Joint SAN: the proposed joint model Eq 4. 5https:\/\/spacy.io 6To handle the unanswerable questions, we append a NULL string at the end of the passages for the unanswerable questions. Single model: EM F1 SAN 67.89 70.68 Joint SAN 69.27 72.20 Joint SAN + Classi\ufb01er 69.54 72.66 Table 1: Performance on the SQuAD 2.0 development dataset. 3. Joint SAN + Classi\ufb01er: the proposed joint model Eq 4 which also use the output infor- mation from the unanswerable binary classi- \ufb01er 7. The results in terms of EM and F1 is summarized in Table 1.",
            "3. Joint SAN + Classi\ufb01er: the proposed joint model Eq 4 which also use the output infor- mation from the unanswerable binary classi- \ufb01er 7. The results in terms of EM and F1 is summarized in Table 1. We observe that Joint SAN outper- forms the SAN baseline with a large margin, e.g., 67.89 vs 69.27 (+1.38) and 70.68 vs 72.20 (+1.52) in terms of EM and F1 scores respectively, so it demonstrates the effectiveness of the joint opti- mization. By incorporating the output information of classi\ufb01er into Joint SAN, it obtains a slight im- provement, e.g., 72.2 vs 72.66 (+0.46) in terms of F1 score. By analyzing the results, we found that in most cases when our model extract an NULL string answer, the classi\ufb01er also predicts it as an unanswerable question with a high probability.",
            "By analyzing the results, we found that in most cases when our model extract an NULL string answer, the classi\ufb01er also predicts it as an unanswerable question with a high probability. SQuAD 2.0 development dataset EM F1 BNA1 59.8 62.6 DocQA1 61.9 64.8 R.M-Reader2 66.9 69.1 R.M-Reader + Veri\ufb01er2 68.5 71.5 Joint SAN 69.3 72.2 SQuAD 2.0 development dataset + ELMo DocQA1 65.1 67.6 R.M-Reader + Veri\ufb01er2 72.3 74.8 SQuAD 2.0 test dataset BNA1 59.2 62.1 DocQA1 59.3 62.3 DocQA + ELMo1 63.4 66.3 R.M-Reader2\u2217 71.7 74.2 Joint SAN# 68.7 71.4 Table 2: Comparison with published results in liter- ature.",
            "1: results are extracted from (Rajpurkar et al., 2018); 2: results are extracted from (Hu et al., 2018). \u2217: it is unclear which model is used. #: we only evalu- ate the Joint SAN in the submission. 7We set the answer to an empty string if the output prob- ability of the classi\ufb01er is larger than 0.5.",
            "Table 2 reports comparison results in literature published 8. Our model achieves state-of-the-art on development dataset in setting without pre- trained large language model (ELMo). Comparing with the much complicated model R.M.-Reader + Veri\ufb01er, which includes several components, our model still outperforms by 0.7 in terms of F1 score. Furthermore, we observe that ELMo gives a great boosting on the performance, e.g., 2.8 points in terms of F1 for DocQA. This encourages us to incorporate ELMo into our model in future. Analysis. To better understand our model, we analyze the accuracy of the classi\ufb01er in our joint model. We obtain 75.3 classi\ufb01cation accuracy on the development with the threshold 0.5. By in- creasing value of \u03bb in Eq 4, the classi\ufb01cation accu- racy reached to 76.8 (\u03bb = 1.5), however the \ufb01nal results of our model only have a small improve- ment (+0.2 in terms of F1 score).",
            "It shows that it is important to make balance between these two components: the span detector and unanswerable classi\ufb01er. 5 Conclusion To sum up, we proposed a simple yet ef\ufb01cient model based on SAN. It showed that the joint learning algorithm boosted the performance on SQuAD 2.0. We also would like to incorporate ELMo into our model in future. Acknowledgments We thank Yichong Xu, Shuohang Wang and Sheng Zhang for valuable discussions and comments. We also thank Robin Jia for the help on SQuAD eval- uations. References Rich Caruana. 1997. Multitask learning. Machine learning 28(1):41\u201375. Minghao Hu, Yuxing Peng, Zhen Huang, Nan Yang, Ming Zhou, et al. 2018. Read+ verify: Machine reading comprehension with unanswerable ques- tions. arXiv preprint arXiv:1808.05759 . Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization.",
            "2018. Read+ verify: Machine reading comprehension with unanswerable ques- tions. arXiv preprint arXiv:1808.05759 . Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 . Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San- tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua 8For the full leaderboard results, please refer to https:\/\/rajpurkar.github.io\/SQuAD-explorer Bengio. 2017. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130 . Xiaodong Liu, Kevin Duh, and Jianfeng Gao. 2018a. Stochastic answer networks for natural language in- ference. arXiv preprint arXiv:1804.07888 . Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-Yi Wang. 2015.",
            "Stochastic answer networks for natural language in- ference. arXiv preprint arXiv:1804.07888 . Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-Yi Wang. 2015. Represen- tation learning using multi-task deep neural net- works for semantic classi\ufb01cation and information re- trieval. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies. Association for Computational Linguis- tics, Denver, Colorado, pages 912\u2013921. http:\/\/ www.aclweb.org\/anthology\/N15-1092. Xiaodong Liu, Yelong Shen, Kevin Duh, and Jian- feng Gao. 2018b. Stochastic answer networks for machine reading comprehension. In Proceed- ings of the 56th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguis- tics, pages 1694\u20131704.",
            "2018b. Stochastic answer networks for machine reading comprehension. In Proceed- ings of the 56th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguis- tics, pages 1694\u20131704. http:\/\/aclweb.org\/ anthology\/P18-1157. Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in transla- tion: Contextualized word vectors. arXiv preprint arXiv:1708.00107 . Jeffrey Pennington, Richard Socher, and Christo- pher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Nat- ural Language Processing (EMNLP). Associa- tion for Computational Linguistics, Doha, Qatar, pages 1532\u20131543. http:\/\/www.aclweb.org\/ anthology\/D14-1162.",
            "In Proceedings of the 2014 Conference on Empirical Methods in Nat- ural Language Processing (EMNLP). Associa- tion for Computational Linguistics, Doha, Qatar, pages 1532\u20131543. http:\/\/www.aclweb.org\/ anthology\/D14-1162. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). volume 1, pages 2227\u20132237. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don\u2019t know: Unanswerable ques- tions for squad. In Proceedings of the 56th Annual Meeting of the Association for Computational Lin- guistics (Volume 2: Short Papers). Association for Computational Linguistics, pages 784\u2013789. http: \/\/aclweb.org\/anthology\/P18-2124.",
            "In Proceedings of the 56th Annual Meeting of the Association for Computational Lin- guistics (Volume 2: Short Papers). Association for Computational Linguistics, pages 784\u2013789. http: \/\/aclweb.org\/anthology\/P18-2124. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text pages 2383\u2013 2392. https:\/\/aclweb.org\/anthology\/ D16-1264.",
            "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Bidirectional attention \ufb02ow for machine comprehension. arXiv preprint arXiv:1611.01603 . Yelong Shen, Xiaodong Liu, Kevin Duh, and Jian- feng Gao. 2017. An empirical analysis of multiple- turn reasoning strategies in reading comprehension tasks. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers). volume 1, pages 957\u2013966. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems. pages 5998\u20136008. Shuohang Wang and Jing Jiang. 2016. Machine com- prehension using match-lstm and answer pointer.",
            "2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems. pages 5998\u20136008. Shuohang Wang and Jing Jiang. 2016. Machine com- prehension using match-lstm and answer pointer. arXiv preprint arXiv:1608.07905 . Yichong Xu, Xiaodong Liu, Yelong Shen, Jingjing Liu, and Jianfeng Gao. 2018. Multi-task learning for ma- chine reading comprehension. arXiv:1809.06963 . Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V. Le. 2018. Qanet: Combining local convolution with global self-attention for reading comprehension."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1809.09194.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 5176.000213623047,
    "avg_doclen_est": 172.53334045410156
}
