{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "arXiv:1909.09484v1  [cs.CL]  17 Sep 2019 Generative Dialogue Policy for Task-oriented Dialogue Systems First Author Af\ufb01liation \/ Address line 1 Af\ufb01liation \/ Address line 2 Af\ufb01liation \/ Address line 3 email@domain Second Author Af\ufb01liation \/ Address line 1 Af\ufb01liation \/ Address line 2 Af\ufb01liation \/ Address line 3 email@domain Abstract There is an increasing demand for task- oriented dialogue systems which can assist users in various activities such as booking tick- ets and restaurant reservations. In order to complete dialogues effectively, dialogue pol- icy plays a key role in task-oriented dialogue systems. As far as we know, the existing task- oriented dialogue systems obtain the dialogue policy through classi\ufb01cation, which can as- sign either a dialogue act and its corresponding parameters or multiple dialogue acts without their corresponding parameters for a dialogue action1. In fact, a good dialogue policy should construct multiple dialogue acts and their cor- responding parameters at the same time. How- ever, it\u2019s hard for existing classi\ufb01cation-based methods to achieve this goal.",
            "In fact, a good dialogue policy should construct multiple dialogue acts and their cor- responding parameters at the same time. How- ever, it\u2019s hard for existing classi\ufb01cation-based methods to achieve this goal. Thus, to ad- dress the issue above, we propose a novel generative dialogue policy learning method. Speci\ufb01cally, the proposed method uses atten- tion mechanism to \ufb01nd relevant segments of given dialogue context and input utterance, and then constructs the dialogue policy by a seq2seq way for task-oriented dialogue sys- tems. Extensive experiments on two bench- mark datasets show that the proposed model signi\ufb01cantly outperforms the state-of-the-art baselines. In addition, we have publicly re- leased our codes2. 1 Introduction Task-oriented dialogue system is an impor- tant tool to build personal virtual assistants, which can help users to complete most of the 1In the dialogue scenario, a dialogue action is generated by the learned dialogue policy in every turn. A dialogue act is the act label in task, such as offer and request. The parameter of a dialogue act is a collection of (slot=value) pairs.",
            "A dialogue act is the act label in task, such as offer and request. The parameter of a dialogue act is a collection of (slot=value) pairs. In pre- vious works, a dialogue action consists of one dialogue act and its parameters or multiple dialogue acts without their pa- rameters. In this work, a dialogue action consists of multiple dialogue acts and their corresponding parameters. 2Github address is xxx (anonymous) daily tasks by interacting with devices via natural language. It\u2019s attracting increasing at- tention of researchers, and lots of works have been proposed in this area (Peng et al., 2018; Eric and Manning, 2017; Lipton et al., 2018; Young et al., 2013; Wen et al., 2016; Lei et al., 2018; Schatzmann et al., 2007b,a).",
            "The existing task-oriented dialogue systems usually consist of four components: (1) natural language understanding (NLU), it tries to iden- tify the intent of a user; (2) dialogue state tracker (DST), it keeps the track of user goals and con- straints in every turn; (3) dialogue policy maker (DP), it aims to generate the next available dia- logue action; and (4) natural language generator (NLG), it generates a natural language response based on the dialogue action. Among the four components, dialogue policy maker plays a key role in order to complete dialogues effectively, be- cause it decides the next dialogue action to be ex- ecuted. As far as we know, the dialogue pol- icy makers in most existing task-oriented di- alogue systems just use the classi\ufb01ers of the prede\ufb01ned acts to obtain dialogue policy (Peng et al., 2018; Lipton et al., 2018; Wen et al., 2016; Liu and Lane, 2017a,b).",
            "The classi\ufb01cation- based dialogue policy learning methods can assign either only a dialogue act and its corresponding parameters (Su et al., 2016; Lipton et al., 2018; Peng et al., 2018) or multiple dialogue acts with- out their corresponding parameters for a dialogue action (Chi et al., 2017). However, all these exist- ing methods cannot obtain multiple dialogue acts and their corresponding parameters for a dialogue action at the same time. Intuitively, it will be more reasonable to con- struct multiple dialogue acts and their correspond- ing parameters for a dialogue action at the same time. For example, it can be shown that there are",
            "Figure 1: The examples in DSTC2 dataset, our pro- posed model can hold more information about dialogue policy than the classi\ufb01cation models mentioned above. \u201cMA, w\/o P\u201d is the model that chooses multiple acts without corresponding parameters during dialogue po- lice modeling, \u201cw\/o MA, P\u201d is the model that chooses only one act and its parameters. 49.4% of turns in the DSTC2 dataset and 61.5% of turns in the Maluuba dataset have multiple di- alogue acts and their corresponding parameters as the dialogue action. If multiple dialogue acts and their corresponding parameters can be ob- tained at the same time, the \ufb01nal response of task- oriented dialogue systems will become more ac- curate and effective. For example, as shown in Figure 1, a user wants to get the name of a cheap french restaurant. The correct dialogue policy should generate three acts in current dialogue turn: offer(name=name slot), inform(food=french) and inform(food=cheap). Thus, the user\u2019s real thought may be: \u201cname slot is a cheap french restaurant\u201d.",
            "The correct dialogue policy should generate three acts in current dialogue turn: offer(name=name slot), inform(food=french) and inform(food=cheap). Thus, the user\u2019s real thought may be: \u201cname slot is a cheap french restaurant\u201d. If losing the act offer, the system may generate a response like \u201cThere are some french restaurants\u201d, which will be far from the user\u2019s goal. To address this challenge, we propose a Gener- ative Dialogue Policy model (GDP) by casting the dialogue policy learning problem as a sequence optimization problem. The proposed model gen- erates a series of acts and their corresponding pa- rameters by the learned dialogue policy. Speci\ufb01- cally, our proposed model uses a recurrent neural network (RNN) as action decoder to construct di- alogue policy maker instead of traditional classi- \ufb01ers. Attention mechanism is used to help the de- coder decode dialogue acts and their correspond- ing parameters, and then the template-based natu- ral language generator uses the results of the dia- logue policy maker to choose an appropriate sen- tence template as the \ufb01nal response to the user.",
            "Attention mechanism is used to help the de- coder decode dialogue acts and their correspond- ing parameters, and then the template-based natu- ral language generator uses the results of the dia- logue policy maker to choose an appropriate sen- tence template as the \ufb01nal response to the user. Extensive experiments conducted on two benchmark datasets verify the effectiveness of our proposed method. Our contributions in this work are three-fold. \u2022 The existing methods cannot construct multi- ple dialogue acts and their corresponding pa- rameters at the same time. In this paper, We propose a novel generative dialogue policy model to solve the problem. \u2022 The extensive experiments demonstrate that the proposed model signi\ufb01cantly outperforms the state-of-the-art baselines on two bench- marks. \u2022 We publicly release the source code. 2 Related Work Usually, the existing task-oriented dialogue sys- tems use a pipeline of four separate modules: natural language understanding, dialogue belief tracker, dialogue policy and natural language gen- erator. Among these four modules, dialogue pol- icy maker plays a key role in task-oriented dia- logue systems, which generates the next dialogue action.",
            "Among these four modules, dialogue pol- icy maker plays a key role in task-oriented dia- logue systems, which generates the next dialogue action. As far as we know, nearly all the existing ap- proaches obtain the dialogue policy by using the classi\ufb01ers of all prede\ufb01ned dialogue acts (Su et al., 2017; Jur\u02c7c\u00b4\u0131\u02c7cek et al., 2011). There are usually two kinds of dialogue policy learning methods. One constructs a dialogue act and its corresponding parameters for a dialogue action. For example, Peng et al. (2018) constructs a simple classi\ufb01er for all the prede\ufb01ned dialogue acts. Lipton et al. (2018) build a complex classi\ufb01er for some pre- de\ufb01ned dialogue acts, addtionally Lipton et al. (2018) adds two acts for each parameter: one to inform its value and the other to request it. The other obtains the dialogue policy by using multi- label classi\ufb01cation to consider multiple dialogue acts without their parameters. Chi et al.",
            "(2018) adds two acts for each parameter: one to inform its value and the other to request it. The other obtains the dialogue policy by using multi- label classi\ufb01cation to consider multiple dialogue acts without their parameters. Chi et al. (2017) performs multi-label multi-class classi\ufb01cation for dialogue policy learning and then the multiple acts can be decided based on a threshold. Based on these classi\ufb01ers, the reinforcement learning can be used to further update the dialogue policy of task- oriented dialogue systems (Young et al., 2013; Cuay\u00b4ahuitl et al., 2015; Liu and Lane, 2017b). In the real scene, an correct dialogue action usu- ally consists of multiple dialogue acts and their corresponding parameters. However, it is very hard for existing classi\ufb01cation-based dialogue pol-",
            "icy maker to achieve this goal. Thus, in this pa- per we propose a novel generative dialogue policy maker to address this issue by casting the dialogue policy learning problem as a sequence optimiza- tion problem. 3 Technical Background 3.1 Encoder-Decoder Seq2Seq Models Seq2Seq model was \ufb01rst introduced by Cho et al. (2014) for statistical machine translation. It uses two recurrent neural networks (RNN) to solve the sequence-to-sequence mapping problem. One called encoder encodes the user utterance into a dense vector representing its semantics, the other called decoder decodes this vector to the target sentence. Now Seq2Seq framework has already been used in task-oriented dialog systems such as (Wen et al., 2016) and (Eric and Manning, 2017), and shows the challenging performance.",
            "Now Seq2Seq framework has already been used in task-oriented dialog systems such as (Wen et al., 2016) and (Eric and Manning, 2017), and shows the challenging performance. In the Seq2Seq model, given the user utterance Q = (x1, x2, ..., xn), the encoder squeezes it into a con- text vector C and then used by decoder to gen- erate the response R = (y1, y2, ..., ym) word by word by maximizing the generation probability of R conditioned on Q. The objective function of Seq2Seq can be written as: p(y1, ..., ym|x1, ..., xn) = p(y1|C) T Y t=2 p(yt|C, y1, ..., yt\u22121) (1) In particular, the encoder RNN produces the con- text vector C by doing calculation below: ht = f(xt, ht\u22121) C = hn (2) The ht is the hidden state of the encoder RNN at time step t and f is the non-linear transforma- tion which can be a long-short term memory unit LSTM (Hochreiter and Schmidhuber, 1997) or a gated recurrent unit GRU (Cho et al., 2014).",
            "In this paper, we implement f by using GRU. The decoder RNN generates each word in reply conditioned on the context vector C. The proba- bility distribution of candidate words at every time step t is calculated as: st = f(yt\u22121, st\u22121, C) yt = softmax(st, yt\u22121) (3) The st is the hidden state of decoder RNN at time step t and yt\u22121 is the generated word in the reply at time t \u22121 calculated by softmax operations. 3.2 Attention Mechanism Attention mechanisms (Bahdanau et al., 2014) have been proved to improved effectively the gen- eration quality for the Seq2Seq framework. In Seq2Seq with attention, each yi corresponds to a context vector Ci which is calculated dynami- cally. It is a weighted average of all hidden states of the encoder RNN.",
            "In Seq2Seq with attention, each yi corresponds to a context vector Ci which is calculated dynami- cally. It is a weighted average of all hidden states of the encoder RNN. Formally, Ci is de\ufb01ned as Ci = Pn j=1 \u03b1ijhj, where \u03b1ij is given by: \u03b1ij = exp(eij) Pn k=1 exp(eik) eij = \u03b7(si\u22121, hj) (4) where si\u22121 is the last hidden state of the de- coder, the \u03b7 is often implemented as a multi-layer- perceptron (MLP) with tanh as the activation func- tion. 4 Generative Dialogue Policy Figure 2 shows the overall system architecture of the proposed GDP model. Our model contains \ufb01ve main components: (1) utterance encoder; (2) dia- logue belief tracker; (3) dialogue policy maker; (4) knowledge base; (5) template-based natural lan- guage generator. Next, we will describe each com- ponent of our proposed GDP model in detail.",
            "Next, we will describe each com- ponent of our proposed GDP model in detail. 4.1 Notations and Task Formulation Given the user utterance Ut at turn t and the di- alogue context Ct\u22121 which contains the result of the dialogue belief tracker at turn t \u22121, the task- oriented dialog system needs to generate user\u2019s in- tents Ct by dialogue belief tracker and then uses this information to get the knowledge base query result kt \u2208Rk. Then the model needs to gener- ate the next dialogue action At based on kt, Ut and Ct. The natural language generator provides the template-based response Rt as the \ufb01nal reply by using At. The Ut and Ct are the sequences, kt is a one-hot vector representing the number of the query results. For baselines, in this paper, the At is the classi\ufb01cation result of the next dialogue action, but in our proposed model it\u2019s a sequence which contains multiple acts and their correspond- ing parameters.",
            "Figure 2: GDP overview. The utterance encoder encodes the user utterance, the dialogue context and the last reply of the systems into the dense vector. As for dialogue belief tracker, we use the approach of Lei et al. (2018) to generate dialogue context. Then this information will be used to search the knowledge base. Based on the user\u2019s intents and query results, dialogue policy maker generates the next dialogue action by using our RNN-based proposed method. 4.2 Utterance Encoder A bidirectional GRU is used to encode the user ut- terance Ut, the last turn response Rt\u22121 made by the system and the dialogue context Ct\u22121 into a continuous representation. The vector is generated by concatenating the last forward and backward GRU states. Ut = (w1, w2, ..., wTm) is the user utterance at turn t. Ct\u22121 = (c1, c2, ..., cTn) is the dialogue context made by dialogue belief tracker at t \u22121 turn. Rt\u22121 is the response made by our task-oriented dialogue system at last turn.",
            "Rt\u22121 is the response made by our task-oriented dialogue system at last turn. Then the words of [Ct\u22121, Rt\u22121, Ut] are \ufb01rstly mapped into an embedding space and further serve as the inputs of each step to the bidirectional GRU. Let n denotes the number of words in the sequence [Ct\u22121, Rt\u22121, Ut]. The \u2212\u2192 hu t\u2032 and \u2190\u2212 hu t\u2032 represent the for- ward and backward GRU state outputs at time step t\u2032. The encoder output of timestep i denote as hu i . Hu = BiGRU(e([Ct\u22121, Rt\u22121, Ut])) hu i = \u2212\u2192 hu i + \u2190\u2212 hu i , hu i \u2208Rdh Hu = {hu 1, hu 2, ..., hun} (5) where e([Ct\u22121, Rt\u22121, Ut]) is the embedding of the input sequence, dh is the hidden size of the GRU. Hu contains the encoder hidden state of each timestep, which will be used by attention mech- anism in dialogue policy maker.",
            "Hu contains the encoder hidden state of each timestep, which will be used by attention mech- anism in dialogue policy maker. 4.3 Dialogue State Tracker Dialogue state tracker maintains the state of a con- versation and collects the user\u2019s goals during the dialogue. Recent work successfully represents this component as discriminative classi\ufb01ers. Lei et al. (2018) veri\ufb01ed that the generation is a better way to model the dialogue state tracker. Speci\ufb01cally, we use a GRU as the generator to decode the Ct of current turn. In order to capture user intent information accurately, the basic atten- tion mechanism is calculated when the decoder de- codes the Ct at each step, which is the same as the Eq. (4).",
            "In order to capture user intent information accurately, the basic atten- tion mechanism is calculated when the decoder de- codes the Ct at each step, which is the same as the Eq. (4). ci = n X j=1 \u03b1ijhu j hd i = GRU(hd i\u22121, e(yd i\u22121)), hd i \u2208Rdh yd i = softmax([hd i , ci]) Hd = {hd 1, hd 2, ..., hd m} Ct = {yd 1, yd 2, ..., yd m} (6) where m is the length of Ct, e(yi) is the embed- ding of the token, dh is the hidden size of the GRU and the hidden state at i timestep of the RNN in dialogue state tracker denote as hd i . The decoded token at step i denotes as yd i . 4.4 Knowledge Base Knowledge base is a database that stores informa- tion about the related task. For example, in the",
            "restaurant reservation, a knowledge base stores the information of all the restaurants, such as location and price. After dialogue belief tracker, the Ct will be used as the constraints to search the results in knowledge base. Then the one-hot vector kt will be produced when the system gets the number of the results. The search result kt has a great in\ufb02uence on di- alogue policy. For example, if the result has multi- ple matches, the system should request more con- straints of the user. In practice, let kt be an one-hot vector of 20 dimensions to represent the number of query results. Then kt will be used as the cue for dialogue policy maker. 4.5 Dialogue Policy Maker In task-oriented dialogue systems, supervised classi\ufb01cation is a straightforward solution for di- alogue policy modeling. However, we observe that classi\ufb01cation cannot hold enough information for dialogue policy modeling. The generative ap- proach is another way to model the dialogue policy maker for task-oriented dialogue systems, which generates the next dialogue acts and their corre- sponding parameters based on the dialogue con- text word by word.",
            "The generative ap- proach is another way to model the dialogue policy maker for task-oriented dialogue systems, which generates the next dialogue acts and their corre- sponding parameters based on the dialogue con- text word by word. Thus the generative approach converts the dialogue policy learning problem into a sequence optimization problem. The dialogue policy maker generates the next dialogue action At based on kt and [Hu, Hd]. Our proposed model uses the GRU as the action de- coder to decode the acts and their parameters for the response. Particularly, at step i, for decoding yp i of At, the decoder GRU takes the embedding of yp i\u22121 to generate a hidden vector hp i . Basic at- tention mechanism is calculated. cu = n X j=1 \u03b1ijhu j ; cd = m X j=1 \u03b1ijhd j hp i = GRU(hp i\u22121, e(yi\u22121)) (7) where e is the embedding of the token, cu is the context vector of the input utterance and cd is the context vector of the dialogue state tracker. hp i is the hidden state of the GRU in dialogue policy maker at i timestep.",
            "hp i is the hidden state of the GRU in dialogue policy maker at i timestep. yp i = softmax(O[hp i , kt, cu, cd]) At = {yp 1, yp 2, ..., yp k} (8) where yp i is the token decoded at i timestep. And the \ufb01nal results of dialogue policy maker denote as At, and the k is the length of it. In our proposed model, the dialogue policy maker can be viewed as a decoder of the seq2seq model conditioned on [Ct\u22121, Rt\u22121, Ut] and kt. 4.6 Nature Language Generator After getting the dialogue action At by the learned dialogue policy maker, the task-oriented dialogue system needs to generate an appropriate response Rt for users. We construct the natural language generator by using template sentences. For each dataset, we extract all the system responses, then we manually modify responses to construct the sentence templates for task-oriented dialogue sys- tems. In our proposed model, the sequence of the acts and parameters At will be used for searching appropriate template.",
            "For each dataset, we extract all the system responses, then we manually modify responses to construct the sentence templates for task-oriented dialogue sys- tems. In our proposed model, the sequence of the acts and parameters At will be used for searching appropriate template. However, the classi\ufb01cation- based baselines use the categories of acts and their corresponding parameters to search the cor- responding template. 4.7 Training In supervised learning, because our proposed model is built in a seq2seq way, the standard cross entropy is adopted as our objective function to train dialogue belief tracker and dialogue policy maker. J = m X j=1 yd j logPj(yd j ) + k X j=1 yp j logPj(yp j ) (9) After supervised learning, the dialogue policy can be further updated by using reinforcement learning. In the context of reinforcement learn- ing, the decoder of dialogue policy maker can be viewed as a policy network, denoted as \u03c0\u03b8(yj) for decoding yj, \u03b8 is the parameters of the decoder. Accordingly, the hidden state created by GRU is the corresponding state, and the choice of the cur- rent token yj is an action3.",
            "Accordingly, the hidden state created by GRU is the corresponding state, and the choice of the cur- rent token yj is an action3. Reward function is also very important for re- inforcement learning when decoding every token. To encourage our policy maker to generate cor- rect acts and their corresponding parameters, we set the reward function as follows: once the dia- logue acts and their parameters are decoded cor- rectly, the reward is 2; otherwise, the reward is -5; 3The action here is different from the dialogue action. It\u2019s a concept of the reinforcement learning.",
            "only the label of the dialogue act is decoded cor- rectly but parameters is wrong, the reward is 1; \u03bb is a decay parameter. More details are shown in Sec 5.3. In our proposed model, rewards can only be obtained at the end of decoding At. In order to get the rewards at each decoding step, we sample some results At after choosing yj, and the reward of yj is set as the average of all the sampled re- sults\u2019 rewards. In order to ensure that the model\u2019s performance is stable during the \ufb01ne-tuning phase of reinforce- ment learning, we freeze the parameters of user utterance and dialogue belief tracker, only the pa- rameters of the dialogue policy maker will be opti- mized by reinforcement learning. Policy gradient algorithm REINFORCE (Williams, 1992) is used for pretrained dialogue policy maker: J = \u22121 m m X j=1 r(yj)\u2202log \u03c0\u03b8(yj) \u2202\u03b8 (10) where the m is the length of the decoded action. The objective function J can be optimized by gra- dient descent.",
            "The objective function J can be optimized by gra- dient descent. 5 Experiments We evaluate the performance of the proposed model in three aspects: (1) the accuracy of the di- alogue state tracker, it aims to show the impact of the dialogue state tracker on the dialogue policy maker; (2) the accuracy of dialogue policy maker, it aims to explain the performance of different methods of constructing dialogue policy; (3) the quality of the \ufb01nal response, it aims to explain the impact of the dialogue policy on the \ufb01nal dialogue response. The evaluation metrics are listed as fol- lows: \u2022 BPRA: Belief Per-Response Accuracy (BPRA) tests the ability to generate the correct user intents during the dialogue. This metric is used to evaluate the accuracy of dialogue belief tracker (Eric and Manning, 2017). \u2022 APRA: Action Per-Response Accuracy (APRA) evaluates the per-turn accuracy of the dialogue actions generated by dia- logue policy maker. For baselines, APRA evaluates the classi\ufb01cation accuracy of the dialogue policy maker.",
            "\u2022 APRA: Action Per-Response Accuracy (APRA) evaluates the per-turn accuracy of the dialogue actions generated by dia- logue policy maker. For baselines, APRA evaluates the classi\ufb01cation accuracy of the dialogue policy maker. But our model actually generates each individual token of Dataset DSTC2 Size Train:1612,Test:506,Dev:1117 Domains restaurant reservation Actions 11. offer, inform, request etc. Slots 8. area, food, price etc. Distinct value 212 Dataset Maluuba Size Train:8621,Test:478,Dev:480 Domains travel booking Actions 16. offer, inform, request etc. Slots 60. startdate, enddate etc. Distinct value inf (continuous values) Table 1: The details of DSTC2 and Maluuba dataset. The Maluuba dataset is more complex than DSTC2, and has some continuous value space such as time and price which is hard to solve for classi\ufb01cation model. actions, and we consider a prediction to be correct only if every token of the model output matches the corresponding token in the ground truth.",
            "actions, and we consider a prediction to be correct only if every token of the model output matches the corresponding token in the ground truth. \u2022 BLEU (Papineni et al., 2002): The metric evaluates the quality of the \ufb01nal response generated by natural language generator. The metric is usually used to measure the perfor- mance of the task-oriented dialogue system. We also choose the following metrics to evalu- ate the ef\ufb01ciency of training the model: \u2022 Timefull: The time for training the whole model, which is important for industry set- tings. \u2022 TimeDP: The time for training the dialogue policy maker in a task-oriented dialogue sys- tem. 5.1 Datasets We adopt the DSTC2 (Henderson et al., 2014) dataset and Maluuba (Asri et al., 2017) dataset to evaluate our proposed model. Both of them are the benchmark datasets for building the task- oriented dialog systems. Speci\ufb01cally, the DSTC2 is a human-machine dataset in the single domain of restaurant searching.",
            "Both of them are the benchmark datasets for building the task- oriented dialog systems. Speci\ufb01cally, the DSTC2 is a human-machine dataset in the single domain of restaurant searching. The Maluuba is a very complex human-human dataset in travel booking domain which contains more slots and values than DSTC2. Detailed slot information in each dataset is shown in Table 1.",
            "Models DSTC2 Maluuba BPRA APRA BLEU T imefull T imeDP BPRA APRA BLEU T imefull T imeDP E2ECM 0.9689 - 0.1782 42.30 m 0.78 m 0.7458 - 0.0797 45.81 m 0.84 m CDM 0.9704 0.2791 0.2039 45.71 m 2.96 m 0.6771 0.1542 0.0704 50.22 m 3.25 m GDP 0.9719 0.5732 0.2847 46.43 m 9.63 m 0.7500 0.4512 0.1156 55.51 m 11.49 m E2ECM+RL 0.9689 - 0.1823 30.01 m 30.01 m 0.7458 - 0.0799 35.13 m 35.13 m CDM+RL 0.9704 0.2873 0.2088 101.0 m 101.",
            "1823 30.01 m 30.01 m 0.7458 - 0.0799 35.13 m 35.13 m CDM+RL 0.9704 0.2873 0.2088 101.0 m 101.0 m 0.6771 0.1625 0.0734 29.00 m 29.00 m GDP+RL 0.9719 0.5766 0.2879 98.07 m 98.07 m 0.7500 0.4521 0.1226 134.8 m 134.8 m Table 2: The performance of baselines and proposed model on DSTC2 and Maluuba dataset. T imefull is the time spent on training the whole model, T imeDP is the time spent on training the dialogue policy maker. 5.2 Baselines For comparison, we choose two state-of-the-art baselines and their variants. \u2022 E2ECM (Chi et al., 2017): In dialogue pol- icy maker, it adopts a classic classi\ufb01cation for skeletal sentence template.",
            "5.2 Baselines For comparison, we choose two state-of-the-art baselines and their variants. \u2022 E2ECM (Chi et al., 2017): In dialogue pol- icy maker, it adopts a classic classi\ufb01cation for skeletal sentence template. In our implement, we construct multiple binary classi\ufb01cations for each act to search the sentence template according to the work proposed by Chi et al. (2017). \u2022 CDM (Su et al., 2016): This approach de- signs a group of classi\ufb01cations (two multi- class classi\ufb01cations and some binary classi- \ufb01cations) to model the dialogue policy. \u2022 E2ECM+RL: It \ufb01ne tunes the classi\ufb01cation parameters of the dialogue policy by REIN- FORCE (Williams, 1992). \u2022 CDM+RL: It \ufb01ne tunes the classi\ufb01cation of the act and corresponding parameters by RE- INFORCE (Williams, 1992).",
            "\u2022 CDM+RL: It \ufb01ne tunes the classi\ufb01cation of the act and corresponding parameters by RE- INFORCE (Williams, 1992). In order to verify the performance of the dia- logue policy maker, the utterance encoder and di- alogue belief tracker of our proposed model and baselines is the same, only dialogue policy maker is different. 5.3 Parameters settings For all models, the hidden size of dialogue be- lief tracker and utterance encoder is 350, and the embedding size demb is set to 300. For our pro- posed model, the hidden size of decoder in dia- logue policy maker is 150. The vocabulary size |V | is 540 for DSTC2 and 4712 for Maluuba. And the size of kt is set to 20. An Adam optimizer (Kingma and Ba, 2014) is used for training our models and baselines, with a learning rate of 0.001 for supervised training and 0.0001 for reinforce- ment learning. In reinforcement learning, the de- cay parameter \u03bb is set to 0.8.",
            "In reinforcement learning, the de- cay parameter \u03bb is set to 0.8. The weight decay is set to 0.001. And early stopping is performed on developing set. 5.4 Experimental Results The experimental results of the proposed model and baselines will be analyzed from the following aspects. BPRA Results: As shown in Table 2, most of the models have similar performance on BPRA on these two datasets, which can guarantee a consis- tent impact on the dialogue policy maker. All the models perform very well in BPRA on DSTC2 dataset. On Maluuba dataset, the BPRA decreases because of the complex domains. We can no- tice that BPRA of CDM is slightly worse than other models on Maluuba dataset, the reason is that the CDM\u2019s dialogue policy maker contains lots of classi\ufb01cations and has the bigger loss than other models because of complex domains, which affects the training of the dialogue belief tracker. APRA Results: Compared with baselines, GDP achieves the best performance in APRA on two datasets. It can be noted that we do not com- pare with the E2ECM baseline in APRA.",
            "APRA Results: Compared with baselines, GDP achieves the best performance in APRA on two datasets. It can be noted that we do not com- pare with the E2ECM baseline in APRA. E2ECM only uses a simple classi\ufb01er to recognize the la- bel of the acts and ignores the parameters infor- mation. In our experiment, APRA of E2ECM is slightly better than our method. Considering the lack of parameters of the acts, it\u2019s unfair for our GDP method. Furthermore, the CDM baseline considers the parameters of the act. But GDP is far better than CDM in supervised learning and re- inforcement learning. BLEU Results: GDP signi\ufb01cantly outperforms the baselines on BLEU. As mentioned above, E2ECM is actually slightly better than GDP in APRA. But in fact, we can \ufb01nd that the lan- guage quality of the response generated by GDP",
            "Dilogue Context Ground Truth GDP E2ECM CDM Inf: cheap, east; sys: name slot is a nice place in the east of town and the price is cheap; user: what\u2019s the address? offer name name slot inform addr addr slot offer name name slot inform addr addr slot inform offer offer name name slot sure, name slot is on addr slot sure, name slot is on addr slot name slot is a nice place in the east of the town name slot is a nice place Table 3: Case Study on DSTC2 dataset. The \ufb01rst column is the Dialogue Context of this case, it contains three parts: (1) Inf is the user\u2019s intent captured by dialogue state tracker; (2) sys is the system response at last turn; (3) user is the user utterance in this turn. The second column to the \ufb01fth column has two rows, above is the action made by the learned dialogue policy maker below is the \ufb01nal response made by template-based generator. is still better than E2ECM, which proves that lack of enough parameters information makes it dif\ufb01- cult to \ufb01nd the appropriate sentence template in NLG.",
            "is still better than E2ECM, which proves that lack of enough parameters information makes it dif\ufb01- cult to \ufb01nd the appropriate sentence template in NLG. It can be found that the BLEU of all mod- els is very poor on Maluuba dataset. The reason is that Maluuba is a human-human task-oriented di- alogue dataset, the utterances are very \ufb02exible, the natural language generator for all methods is dif\ufb01- cult to generate an accurate utterance based on the context. And DSTC2 is a human-machine dialog dataset. The response is very regular so the effec- tiveness of NLG will be better than that of Malu- uba. But from the results, the GDP is still better than the baselines on Maluuba dataset, which also veri\ufb01es that our proposed method is more accurate in modeling dialogue policy on complex domains than the classi\ufb01cation-based methods. E2ECM CDM GDP Model 0 100 200 300 400 Model size (thounsand) Model Size Dialogue Policy Size Figure 3: The number of the parameters.",
            "E2ECM CDM GDP Model 0 100 200 300 400 Model size (thounsand) Model Size Dialogue Policy Size Figure 3: The number of the parameters. GDP has the bigger model size and more dialogue policy parameters because of the RNN-based dialogue policy maker. Time and Model Size: In order to obtain more accurate and complete dialogue policy for task- oriented dialogue systems, the proposed model has more parameters on the dialogue policy maker than baselines. As shown in Figure 3, E2ECM has the minimal dialogue policy parameters because of the simple classi\ufb01cation. It needs minimum training time, but the performance of E2ECM is bad. The number of parameters in the CDM model is slightly larger than E2ECM. However, because both of them are classi\ufb01cation methods, they all lose some important information about dialogue policy. Therefore, we can see from the experi- mental results that the quality of CDM\u2019s dialogue policy is as bad as E2ECM. The number of dia- logue policy maker\u2019s parameters in GDP model is much larger than baselines.",
            "Therefore, we can see from the experi- mental results that the quality of CDM\u2019s dialogue policy is as bad as E2ECM. The number of dia- logue policy maker\u2019s parameters in GDP model is much larger than baselines. Although the pro- posed model need more time to be optimized by supervised learning and reinforcement learning, the performance is much better than all baselines. 5.5 Case Study Table 3 illustrates an example of our proposed model and baselines on DSTC2 dataset. In this ex- ample, a user\u2019s goal is to \ufb01nd a cheap restaurant in the east part of the town. In the current turn, the user wants to get the address of the restaurant. E2ECM chooses the inform and offer acts ac- curately, but the lack of the inform\u2019s parameters makes the \ufb01nal output deviate from the user\u2019s goal. CDM generates the parameters of offer success- fully, but the lack of the information of inform also leads to a bad result. By contrast, the proposed model GDP can generate all the acts and their cor- responding parameters as the dialogue action.",
            "CDM generates the parameters of offer success- fully, but the lack of the information of inform also leads to a bad result. By contrast, the proposed model GDP can generate all the acts and their cor- responding parameters as the dialogue action. In- terestingly, the \ufb01nal result of GDP is exactly the same as the ground truth, which veri\ufb01es that the proposed model is better than the state-of-the-art baselines. 6 Conclusion In this paper, we propose a novel model named GDP. Our proposed model treats the dialogue pol- icy modeling as the generative task instead of the discriminative task which can hold more infor- mation for dialogue policy modeling. We evalu- ate the GDP on two benchmark task-oriented di- alogue datasets. Extensive experiments show that",
            "GDP outperforms the existing classi\ufb01cation-based methods on both action accuracy and BLEU. References Layla El Asri, Hannes Schulz, Shikhar Sharma, Jeremie Zumer, Justin Harris, Emery Fine, Rahul Mehrotra, and Kaheer Suleman. 2017. Frames: A corpus for adding memory to goal-oriented dialogue systems. arXiv preprint arXiv:1704.00057. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473. Ta-Chung Chi, Po-Chun Chen, Shang-Yu Su, and Yun- Nung Chen. 2017. Speaker role contextual model- ing for language understanding and dialogue policy learning. arXiv preprint arXiv:1710.00164.",
            "Ta-Chung Chi, Po-Chun Chen, Shang-Yu Su, and Yun- Nung Chen. 2017. Speaker role contextual model- ing for language understanding and dialogue policy learning. arXiv preprint arXiv:1710.00164. Kyunghyun Cho, Bart Van Merri\u00a8enboer, Caglar Gul- cehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078. Heriberto Cuay\u00b4ahuitl, Simon Keizer, and Oliver Lemon. 2015. Strategic dialogue management via deep reinforcement learning. arXiv preprint arXiv:1511.08099. Mihail Eric and Christopher D Manning. 2017. A copy-augmented sequence-to-sequence architecture gives good performance on task-oriented dialogue. arXiv preprint arXiv:1701.04024. Matthew Henderson, Blaise Thomson, and Jason D Williams.",
            "Mihail Eric and Christopher D Manning. 2017. A copy-augmented sequence-to-sequence architecture gives good performance on task-oriented dialogue. arXiv preprint arXiv:1701.04024. Matthew Henderson, Blaise Thomson, and Jason D Williams. 2014. The second dialog state tracking challenge. In Proceedings of the 15th Annual Meet- ing of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 263\u2013272. Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735\u20131780. Filip Jur\u02c7c\u00b4\u0131\u02c7cek, Blaise Thomson, and Steve Young. 2011. Natural actor and belief critic: Reinforcement algorithm for learning parameters of dialogue sys- tems modelled as pomdps. ACM Transactions on Speech and Language Processing (TSLP), 7(3):6. Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization.",
            "ACM Transactions on Speech and Language Processing (TSLP), 7(3):6. Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Wenqiang Lei, Xisen Jin, Min-Yen Kan, Zhaochun Ren, Xiangnan He, and Dawei Yin. 2018. Sequic- ity: Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures. In Pro- ceedings of the 56th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers), pages 1437\u20131447. Zachary Lipton, Xiujun Li, Jianfeng Gao, Lihong Li, Faisal Ahmed, and Li Deng. 2018. Bbq-networks: Ef\ufb01cient exploration in deep reinforcement learn- ing for task-oriented dialogue systems. In Thirty- Second AAAI Conference on Arti\ufb01cial Intelligence. Bing Liu and Ian Lane. 2017a.",
            "2018. Bbq-networks: Ef\ufb01cient exploration in deep reinforcement learn- ing for task-oriented dialogue systems. In Thirty- Second AAAI Conference on Arti\ufb01cial Intelligence. Bing Liu and Ian Lane. 2017a. An end-to-end train- able neural network model with belief tracking for task-oriented dialog. Proc. Interspeech 2017, pages 2506\u20132510. Bing Liu and Ian Lane. 2017b. Iterative policy learn- ing in end-to-end trainable task-oriented neural dia- log models. In 2017 IEEE Automatic Speech Recog- nition and Understanding Workshop (ASRU), pages 482\u2013489. IEEE. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th annual meeting on association for compu- tational linguistics, pages 311\u2013318. Association for Computational Linguistics.",
            "2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th annual meeting on association for compu- tational linguistics, pages 311\u2013318. Association for Computational Linguistics. Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu, Kam-Fai Wong, and Shang-Yu Su. 2018. Deep dyna-q: Integrating planning for task- completion dialogue policy learning. arXiv preprint arXiv:1801.06176. Jost Schatzmann, Blaise Thomson, Karl Weilhammer, Hui Ye, and Steve Young. 2007a. Agenda-based user simulation for bootstrapping a pomdp dialogue system. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers, pages 149\u2013152. Association for Computational Linguistics. Jost Schatzmann, Blaise Thomson, and Steve Young. 2007b. Statistical user simulation with a hidden agenda.",
            "Association for Computational Linguistics. Jost Schatzmann, Blaise Thomson, and Steve Young. 2007b. Statistical user simulation with a hidden agenda. Proc SIGDial, Antwerp, 273282(9). Pei-Hao Su, Pawel Budzianowski, Stefan Ultes, Mil- ica Gasic, and Steve Young. 2017. Sample-ef\ufb01cient actor-critic reinforcement learning with supervised data for dialogue management. arXiv preprint arXiv:1707.00130. Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas- Barahona, Stefan Ultes, David Vandyke, Tsung- Hsien Wen, and Steve Young. 2016. Continu- ously learning neural dialogue management. arXiv preprint arXiv:1606.02689. Tsung-Hsien Wen, David Vandyke, Nikola Mrksic, Milica Gasic, Lina M Rojas-Barahona, Pei-Hao Su, Stefan Ultes, and Steve Young. 2016.",
            "Tsung-Hsien Wen, David Vandyke, Nikola Mrksic, Milica Gasic, Lina M Rojas-Barahona, Pei-Hao Su, Stefan Ultes, and Steve Young. 2016. A network- based end-to-end trainable task-oriented dialogue system. arXiv preprint arXiv:1604.04562. Ronald J Williams. 1992. Simple statistical gradient- following algorithms for connectionist reinforce- ment learning. Machine learning, 8(3-4):229\u2013256.",
            "Steve Young, Milica Ga\u02c7si\u00b4c, Blaise Thomson, and Ja- son D Williams. 2013. Pomdp-based statistical spo- ken dialog systems: A review. Proceedings of the IEEE, 101(5):1160\u20131179."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1909.09484.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 8843.00015258789,
    "avg_doclen_est": 180.46939086914062
}
