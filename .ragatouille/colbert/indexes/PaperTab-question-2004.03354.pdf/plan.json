{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Inexpensive Domain Adaptation of Pretrained Language Models: Case Studies on Biomedical NER and Covid-19 QA Nina Poerner\u2217\u2020 and Ulli Waltinger\u2020 and Hinrich Sch\u00a8utze\u2217 \u2217Center for Information and Language Processing, LMU Munich, Germany \u2020Corporate Technology Machine Intelligence (MIC-DE), Siemens AG Munich, Germany poerner@cis.uni-muenchen.de | inquiries@cislmu.org Abstract Domain adaptation of Pretrained Language Models (PTLMs) is typically achieved by un- supervised pretraining on target-domain text. While successful, this approach is expensive in terms of hardware, runtime and CO2 emis- sions. Here, we propose a cheaper alterna- tive: We train Word2Vec on target-domain text and align the resulting word vectors with the wordpiece vectors of a general-domain PTLM. We evaluate on eight biomedical Named Entity Recognition (NER) tasks and compare against the recently proposed BioBERT model. We cover over 60% of the BioBERT \u2013 BERT F1 delta, at 5% of BioBERT\u2019s CO2 footprint and 2% of its cloud compute cost.",
            "We evaluate on eight biomedical Named Entity Recognition (NER) tasks and compare against the recently proposed BioBERT model. We cover over 60% of the BioBERT \u2013 BERT F1 delta, at 5% of BioBERT\u2019s CO2 footprint and 2% of its cloud compute cost. We also show how to quickly adapt an existing general- domain Question Answering (QA) model to an emerging domain: the Covid-19 pandemic.1 1 Introduction Pretrained Language Models (PTLMs) such as BERT (Devlin et al., 2019) have spearheaded ad- vances on many NLP tasks. Usually, PTLMs are pretrained on unlabeled general-domain and\/or mixed-domain text, such as Wikipedia, digital books or the Common Crawl corpus. When applying PTLMs to speci\ufb01c domains, it can be useful to domain-adapt them. Domain adap- tation of PTLMs has typically been achieved by pre- training on target-domain text. One such model is BioBERT (Lee et al., 2020), which was initialized from general-domain BERT and then pretrained on biomedical scienti\ufb01c publications.",
            "Domain adap- tation of PTLMs has typically been achieved by pre- training on target-domain text. One such model is BioBERT (Lee et al., 2020), which was initialized from general-domain BERT and then pretrained on biomedical scienti\ufb01c publications. The domain adaptation is shown to be helpful for target-domain tasks such as biomedical Named Entity Recogni- tion (NER) or Question Answering (QA). On the downside, the computational cost of pretraining can be considerable: BioBERTv1.0 was adapted for ten 1www.github.com\/npoe\/covid-qa days on eight large GPUs (see Table 1), which is expensive, environmentally unfriendly, prohibitive for small research labs and students, and may delay prototyping on emerging domains. We therefore propose a fast, CPU-only domain- adaptation method for PTLMs: We train Word2Vec (Mikolov et al., 2013a) on target-domain text and align the resulting word vectors with the wordpiece vectors of an existing general-domain PTLM.",
            "We therefore propose a fast, CPU-only domain- adaptation method for PTLMs: We train Word2Vec (Mikolov et al., 2013a) on target-domain text and align the resulting word vectors with the wordpiece vectors of an existing general-domain PTLM. The PTLM thus gains domain-speci\ufb01c lexi- cal knowledge in the form of additional word vec- tors, but its deeper layers remain unchanged. Since Word2Vec and the vector space alignment are ef\ufb01- cient models, the process requires a fraction of the resources associated with pretraining the PTLM itself, and it can be done on CPU. In Section 4, we use the proposed method to domain-adapt BERT on PubMed+PMC (the data used for BioBERTv1.0) and\/or CORD-19 (Covid- 19 Open Research Dataset). We improve over general-domain BERT on eight out of eight biomed- ical NER tasks, using a fraction of the compute cost associated with BioBERT.",
            "We improve over general-domain BERT on eight out of eight biomed- ical NER tasks, using a fraction of the compute cost associated with BioBERT. In Section 5, we show how to quickly adapt an existing Question Answer- ing model to text about the Covid-19 pandemic, without any target-domain Language Model pre- training or \ufb01netuning. 2 Related work 2.1 The BERT PTLM For our purpose, a PTLM consists of three parts: A tokenizer TLM : L+ \u2192L+ LM, a wordpiece em- bedding lookup function ELM : LLM \u2192RdLM and an encoder function FLM. LLM is a lim- ited vocabulary of wordpieces. All words from the natural language L+ that are not in LLM are tokenized into sequences of shorter word- pieces, e.g., dementia becomes dem ##ent ##ia. Given a sentence S = [w1, . . . , wT ], tokenized arXiv:2004.03354v4  [cs.CL]  27 Jun 2020",
            "size Domain adaptation hardware Power(W) Time(h) CO2(lbs) Google Cloud $ BioBERTv1.0 base 8 NVIDIA v100 GPUs (32GB) 1505 240 544 1421 \u2013 4762 BioBERTv1.1 base 8 NVIDIA v100 GPUs (32GB) 1505 552 1252 3268 \u2013 10952 GreenBioBERT (Section 4) base 12 Intel Xeon E7-8857 CPUs, 30GB RAM 1560 12 28 16 \u2013 76 GreenCovidSQuADBERT (Section 5) large 12 Intel Xeon E7-8857 CPUs, 40GB RAM 1560 24 56 32 \u2013 152 Table 1: Domain adaptation cost. CO2 emissions are calculated according to Strubell et al. (2019). Since our hardware con\ufb01guration is not available on Google Cloud, we take an m1-ultramem-40 instance (40 vCPUs, 961GB RAM) to estimate an upper bound on our Google Cloud cost. as TLM(S) = [TLM(w1); . . .",
            "as TLM(S) = [TLM(w1); . . . ; TLM(wT )], ELM em- beds every wordpiece in TLM(S) into a real-valued, trainable wordpiece vector. The wordpiece vec- tors of the entire sequence are stacked and fed into FLM. Note that we consider position and segment embeddings to be a part of FLM rather than ELM. In the case of BERT, FLM is a Transformer (Vaswani et al., 2017), followed by a \ufb01nal Feed- Forward Net. During pretraining, the Feed- Forward Net predicts the identity of masked word- pieces. When \ufb01netuning on a supervised task, it is usually replaced with a randomly initialized layer. 2.2 Domain-adapted PTLMs Domain adaptation of PTLMs is typically achieved by pretraining on unlabeled target-domain text.",
            "When \ufb01netuning on a supervised task, it is usually replaced with a randomly initialized layer. 2.2 Domain-adapted PTLMs Domain adaptation of PTLMs is typically achieved by pretraining on unlabeled target-domain text. Some examples of such models are BioBERT (Lee et al., 2020), which was pretrained on the PubMed and\/or PubMed Central (PMC) corpora, SciBERT (Beltagy et al., 2019), which was pre- trained on papers from SemanticScholar, Clinical- BERT (Alsentzer et al., 2019; Huang et al., 2019a) and ClinicalXLNet (Huang et al., 2019b), which were pretrained on clinical patient notes, and Adapt- aBERT (Han and Eisenstein, 2019), which was pretrained on Early Modern English text. In most cases, a domain-adapted PTLM is initialized from a general-domain PTLM (e.g., standard BERT), though Beltagy et al. (2019) report better results with a model that was pretrained from scratch with a custom wordpiece vocabulary.",
            "In most cases, a domain-adapted PTLM is initialized from a general-domain PTLM (e.g., standard BERT), though Beltagy et al. (2019) report better results with a model that was pretrained from scratch with a custom wordpiece vocabulary. In this paper, we focus on BioBERT, as its domain adaptation cor- pora are publicly available. 2.3 Word vectors Word vectors are distributed representations of words that are trained on unlabeled text. Con- trary to PTLMs, word vectors are non-contextual, i.e., a word type is always assigned the same vec- tor, regardless of context. In this paper, we use Word2Vec (Mikolov et al., 2013a) to train word vectors. We will denote the Word2Vec lookup func- tion as EW2V : LW2V \u2192RdW2V. 2.4 Word vector space alignment Word vector space alignment has most frequently been explored in the context of cross-lingual word embeddings. For instance, Mikolov et al. (2013b) align English and Spanish Word2Vec spaces by a simple linear transformation. Wang et al.",
            "2.4 Word vector space alignment Word vector space alignment has most frequently been explored in the context of cross-lingual word embeddings. For instance, Mikolov et al. (2013b) align English and Spanish Word2Vec spaces by a simple linear transformation. Wang et al. (2019) use a related method to align cross-lingual word vectors and multilingual BERT wordpiece vectors. 3 Method In the following, we assume access to a general- domain PTLM, as described in Section 2.1, and a corpus of unlabeled target-domain text. 3.1 Creating new input vectors In a \ufb01rst step, we train Word2Vec on the target- domain corpus. In a second step, we take the in- tersection of LLM and LW2V. In practice, the in- tersection mostly contains wordpieces from LLM that correspond to standalone words. It also con- tains single characters and other noise, however, we found that \ufb01ltering them does not improve align- ment quality.",
            "In practice, the in- tersection mostly contains wordpieces from LLM that correspond to standalone words. It also con- tains single characters and other noise, however, we found that \ufb01ltering them does not improve align- ment quality. In a third step, we use the intersec- tion to \ufb01t an unconstrained linear transformation W \u2208RdLM\u00d7dW2V via least squares: argmin W X x\u2208LLM\u2229LW2V ||WEW2V(x) \u2212ELM(x)||2 2 Intuitively, W makes Word2Vec vectors \u201clook like\u201d the PTLM\u2019s native wordpiece vectors, just like cross-lingual alignment makes word vectors from one language \u201clook like\u201d word vectors from another language. In Table 2 (top), we show ex- amples of within-space and cross-space nearest neighbors after alignment. 3.2 Updating the wordpiece embedding layer Next, we rede\ufb01ne the wordpiece embedding layer of the PTLM.",
            "In Table 2 (top), we show ex- amples of within-space and cross-space nearest neighbors after alignment. 3.2 Updating the wordpiece embedding layer Next, we rede\ufb01ne the wordpiece embedding layer of the PTLM. The most radical strategy would be to replace the entire layer with the aligned Word2Vec vectors: \u02c6ELM : LW2V \u2192RdLM ; \u02c6ELM(x) = WEW2V(x)",
            "Query NNs of query in ELM[LLM] NNs of query in WEW2V[LW2V] query \u2208LW2V \u2229LLM Boldface: Training vector pairs surgeon physician, psychiatrist, surgery surgeon, urologist, neurosurgeon surgeon surgeon, physician, researcher neurosurgeon, urologist, radiologist depression Depression, recession, depressed depression, Depression, hopelessness depression depression, anxiety, anxiousness depressive, insomnia, Depression query \u2208LW2V \u2212LLM ventricular cardiac, pulmonary, mitochondrial atrial, ventricle, RV suppressants medications, medicines, medication suppressant, prokinetics, painkillers anesthesiologist surgeon, technician, psychiatrist anesthetist, anaesthesiologist, anaesthetist nephrotoxicity toxicity, in\ufb02ammation, contamination hepatotoxicity, ototoxicity, cardiotoxicity BERT (ref) BioBERTv1.0 (ref) BioBERTv1.1 (ref) GreenBioBERT Biomedical NER task (NER task ID) (Lee et al., 2020) (Lee et al., 2020) (Lee et al.,",
            "0 (ref) BioBERTv1.1 (ref) GreenBioBERT Biomedical NER task (NER task ID) (Lee et al., 2020) (Lee et al., 2020) (Lee et al., 2020) (with standard error of the mean) BC5CDR-disease (Li et al., 2016) (1) 81.97 \/ 82.48 \/ 82.41 85.86 \/ 87.27 \/ 86.56 86.47 \/ 87.84 \/ 87.15 84.88 (.07) \/ 85.29 (.12) \/ 85.08 (.08) NCBI-disease (Do\u02d8gan et al., 2014) (2) 84.12 \/ 87.19 \/ 85.63 89.04 \/ 89.69 \/ 89.36 88.22 \/ 91.25 \/ 89.71 85.49 (.23) \/ 86.41 (.15) \/ 85.94 (.16) BC5CDR-chem (Li et al., 2016) (3) 90.",
            "36 88.22 \/ 91.25 \/ 89.71 85.49 (.23) \/ 86.41 (.15) \/ 85.94 (.16) BC5CDR-chem (Li et al., 2016) (3) 90.94 \/ 91.38 \/ 91.16 93.27 \/ 93.61 \/ 93.44 93.68 \/ 93.26 \/ 93.47 93.82 (.11) \/ 92.35 (.17) \/ 93.08 (.07) BC4CHEMD (Krallinger et al., 2015) (4) 91.19 \/ 88.92 \/ 90.04 92.23 \/ 90.61 \/ 91.41 92.80 \/ 91.92 \/ 92.36 92.80 (.04) \/ 89.78 (.07) \/ 91.26 (.04) BC2GM (Smith et al., 2008) (5) 81.17 \/ 82.42 \/ 81.79 85.16 \/ 83.65 \/ 84.40 84.",
            "78 (.07) \/ 91.26 (.04) BC2GM (Smith et al., 2008) (5) 81.17 \/ 82.42 \/ 81.79 85.16 \/ 83.65 \/ 84.40 84.32 \/ 85.12 \/ 84.72 83.34 (.15) \/ 83.58 (.09) \/ 83.45 (.10) JNLPBA (Kim et al., 2004) (6) 69.57 \/ 81.20 \/ 74.94 72.68 \/ 83.21 \/ 77.59 72.24 \/ 83.56 \/ 77.49 71.93 (.12) \/ 82.58 (.12) \/ 76.89 (.10) LINNAEUS (Gerner et al., 2010) (7) 91.17 \/ 84.30 \/ 87.60 93.84 \/ 86.11 \/ 89.81 90.77 \/ 85.83 \/ 88.24 92.50 (.17) \/ 84.54 (.26) \/ 88.34 (.",
            "17 \/ 84.30 \/ 87.60 93.84 \/ 86.11 \/ 89.81 90.77 \/ 85.83 \/ 88.24 92.50 (.17) \/ 84.54 (.26) \/ 88.34 (.18) Species-800 (Pa\ufb01lis et al., 2013) (8) 69.35 \/ 74.05 \/ 71.63 72.84 \/ 77.97 \/ 75.31 72.80 \/ 75.36 \/ 74.06 73.19 (.26) \/ 75.47 (.33) \/ 74.31 (.24) Table 2: Top: Examples of within-space and cross-space nearest neighbors (NNs) by cosine similarity in Green- BioBERT\u2019s wordpiece embedding layer. Blue: Original wordpiece space. Green: Aligned Word2Vec space. Bot- tom: Biomedical NER test set precision \/ recall \/ F1 (%). \u201c(ref)\u201d: Reference scores from Lee et al. (2020). Boldface: Best model in row. Underlined: Best model without target-domain LM pretraining.",
            "Bot- tom: Biomedical NER test set precision \/ recall \/ F1 (%). \u201c(ref)\u201d: Reference scores from Lee et al. (2020). Boldface: Best model in row. Underlined: Best model without target-domain LM pretraining. In initial experiments, this strategy led to a drop in performance, presumably because func- tion words are not well represented by Word2Vec, and replacing them disrupts BERT\u2019s syntactic abil- ities. To prevent this problem, we leave existing wordpiece vectors intact and only add new ones: \u02c6ELM : LLM \u222aLW2V \u2192RdLM; \u02c6ELM(x) = ( ELM(x) if x \u2208LLM WEW2V(x) otherwise (1) 3.3 Updating the tokenizer In a \ufb01nal step, we update the tokenizer to account for the added words. Let TLM be the standard BERT tokenizer, and let \u02c6TLM be the tokenizer that treats all words in LLM \u222aLW2V as one-wordpiece tokens, while tokenizing any other words as usual.",
            "Let TLM be the standard BERT tokenizer, and let \u02c6TLM be the tokenizer that treats all words in LLM \u222aLW2V as one-wordpiece tokens, while tokenizing any other words as usual. In practice, a given word may or may not bene\ufb01t from being tokenized by \u02c6TLM instead of TLM. To give a concrete example, 82% of the words in the BC5CDR NER dataset that end in the suf\ufb01x -ia are part of a disease entity (e.g., dementia). TLM tok- enizes this word as dem ##ent ##ia, thereby expos- ing this strong orthographic cue to the model. As a result, TLM improves recall on -ia diseases. But there are many cases where wordpiece tokeniza- tion is meaningless or misleading. For instance euthymia (not a disease) is tokenized by TLM as e ##uth ##ym ##ia, making it likely to be classi\ufb01ed as a disease. By contrast, \u02c6TLM gives euthymia a one-wordpiece representation that depends only on distributional semantics.",
            "By contrast, \u02c6TLM gives euthymia a one-wordpiece representation that depends only on distributional semantics. We \ufb01nd that using \u02c6TLM improves precision on -ia diseases. To combine these complementary strengths, we use a 50\/50 mixture of TLM-tokenization and \u02c6TLM- tokenization when \ufb01netuning the PTLM on a task. At test time, we use both tokenizers and mean-pool the outputs. Let o(T (S)) be some output of interest (e.g., a logit), given sentence S tokenized by T . We predict: 1 2[o(TLM(S)) + o( \u02c6TLM(S))] 4 Experiment 1: Biomedical NER In this section, we use the proposed method to create GreenBioBERT, an inexpensive and envi- ronmentally friendly alternative to BioBERT. Re- call that BioBERTv1.0 (biobert v1.0 pubmed pmc) was initialized from general-domain BERT (bert- base-cased) and then pretrained on PubMed+PMC.",
            "Re- call that BioBERTv1.0 (biobert v1.0 pubmed pmc) was initialized from general-domain BERT (bert- base-cased) and then pretrained on PubMed+PMC. 4.1 Domain adaptation We train Word2Vec with vector size dW2V = dLM = 768 on PubMed+PMC (see Appendix for details). Then, we update the wordpiece embed- ding layer and tokenizer of general-domain BERT (bert-base-cased) as described in Section 3. 4.2 Finetuning We \ufb01netune GreenBioBERT on the eight publicly available NER tasks used in Lee et al. (2020). We also do reproduction experiments with general- domain BERT and BioBERTv1.0, using the same setup as our model. See Appendix for details on preprocessing and hyperparameters. Since some of the datasets are sensitive to the random seed, we report mean and standard error over eight runs.",
            "0.00 0.25 0.50 0.75 1.00 1.25 1.50 Test set F1 shifted and scaled (1) (2) (3) (4) (5) (6) (7) (8) NER task ID BERT (ref) BERT (repro) BioBERTv1.0 (ref) BioBERTv1.0 (repro) BioBERTv1.1 (ref) GreenBioBERT Figure 1: NER test set F1, transformed as (x \u2212BERT(ref))\/(BioBERTv1.0(ref) \u2212BERT(ref)). \u201c(ref)\u201d: Reference scores from Lee et al. (2020). \u201c(re- pro)\u201d: Results of our reproduction experiments. Error bars: Standard error of the mean. 4.3 Results and discussion Table 2 (bottom) shows entity-level precision, re- call and F1, as measured by the CoNLL NER scorer. For ease of visualization, Figure 1 shows what por- tion of the BioBERT \u2013 BERT F1 delta is covered.",
            "4.3 Results and discussion Table 2 (bottom) shows entity-level precision, re- call and F1, as measured by the CoNLL NER scorer. For ease of visualization, Figure 1 shows what por- tion of the BioBERT \u2013 BERT F1 delta is covered. On average, we cover between 61% and 70% of the F1 delta (61% for BioBERTv1.0 (ref), 70% for BioBERTv1.1 (ref), and 61% if we take our reproduction experiments as reference). To test whether the improvements over general- domain BERT are due to the aligned Word2Vec vec- tors, or just to the availability of additional vectors in general, we perform an ablation study where we replace the aligned vectors with their non-aligned counterparts (by setting W = 1 in Eq. 1) or with randomly initialized vectors. Table 3 (top) shows that dev set F1 drops under these circumstances, i.e., vector space alignment is important.",
            "1) or with randomly initialized vectors. Table 3 (top) shows that dev set F1 drops under these circumstances, i.e., vector space alignment is important. 5 Experiment 2: Covid-19 QA In this section, we use the proposed method to quickly adapt an existing general-domain QA model to an emerging target domain: Covid-19. Our baseline model is SQuADBERT,2 an exist- ing BERT model that was \ufb01netuned on general- domain SQuAD (Rajpurkar et al., 2016). We evalu- ate on Deepset-AI Covid-QA (M\u00a8oller et al., 2020), a SQuAD-style dataset with 2019 questions about 147 papers from CORD-19 (Covid-19 Open Re- search Dataset). We assume that there is no labeled target-domain data for \ufb01netuning, which is a realis- tic setup for a new domain. 5.1 Domain adaptation We train Word2Vec with vector size dW2V = dLM = 1024 on CORD-19 and\/or PubMed+PMC.",
            "5.1 Domain adaptation We train Word2Vec with vector size dW2V = dLM = 1024 on CORD-19 and\/or PubMed+PMC. 2www.huggingface.co\/bert-large-uncased- whole-word-masking-finetuned-squad NER task ID (1) (2) (3) (4) (5) (6) (7) (8) non-aligned -4.88 -3.50 -4.13 -3.34 -2.34 -0.56 -0.84 -4.63 random init -4.33 -3.60 -3.19 -3.19 -1.92 -0.50 -0.84 -3.58 domain adaptation corpus size EM F1 substr SQuADBERT \u2014\u2014\u2013 33.04 58.24 65.87 GreenCovid- CORD-19 only 2GB 34.62 60.09 68.20 SQuADBERT CORD-19+PubMed+PMC 94GB 34.32 60.23 68.00 Table 3: Top: NER ablation study.",
            "Drop in dev set F1 (w.r.t. GreenBioBERT) when using non-aligned or ran- domly initialized word vectors instead of aligned word vectors. Bottom: Results (%) on Deepset-AI Covid- QA. EM (exact match) and F1 are evaluated with the SQuAD scorer. \u201csubstr\u201d: Predictions that are a sub- string of the gold answer. Much higher than EM, be- cause many gold answers are not minimal answer spans (see Appendix, \u201cNotes on Covid-QA\u201d, for an example). The process takes less than an hour on CORD- 19 and about one day on the combined corpus, again without the need for a GPU. Then, we update SQuADBERT\u2019s wordpiece embedding layer and tokenizer, as described in Section 3. We refer to the resulting model as GreenCovidSQuADBERT. 5.2 Results and discussion Table 3 (bottom) shows that GreenCovidSQuAD- BERT outperforms general-domain SQuADBERT on all measures.",
            "We refer to the resulting model as GreenCovidSQuADBERT. 5.2 Results and discussion Table 3 (bottom) shows that GreenCovidSQuAD- BERT outperforms general-domain SQuADBERT on all measures. Interestingly, the small CORD-19 corpus is enough to achieve this re- sult (compare \u201cCORD-19 only\u201d and \u201cCORD- 19+PubMed+PMC\u201d), presumably because it is spe- ci\ufb01c to the target domain and contains the Covid- QA context papers. 6 Conclusion As a reaction to the trend towards high-resource models, we have proposed an inexpensive, CPU- only method for domain-adapting Pretrained Lan- guage Models: We train Word2Vec vectors on target-domain data and align them with the word- piece vector space of a general-domain PTLM. On eight biomedical NER tasks, we cover over 60% of the BioBERT \u2013 BERT F1 delta, at 5% of BioBERT\u2019s domain adaptation CO2 footprint and 2% of its cloud compute cost.",
            "On eight biomedical NER tasks, we cover over 60% of the BioBERT \u2013 BERT F1 delta, at 5% of BioBERT\u2019s domain adaptation CO2 footprint and 2% of its cloud compute cost. We have also shown how to rapidly adapt an existing BERT QA model to an emerging domain \u2013 the Covid-19 pan- demic \u2013 without the need for target-domain Lan- guage Model pretraining or \ufb01netuning. We hope that our approach will bene\ufb01t practi- tioners with limited time or resources, and that it will encourage environmentally friendlier NLP.",
            "References Emily Alsentzer, John Murphy, William Boag, Wei- Hung Weng, Di Jindi, Tristan Naumann, and Matthew McDermott. 2019. Publicly available clin- ical BERT embeddings. In 2nd Clinical Natural Language Processing Workshop, pages 72\u201378, Min- neapolis, USA. Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB- ERT: A pretrained language model for scienti\ufb01c text. In EMNLP-IJCNLP, pages 3606\u20133611, Hong Kong, China. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In NAACL-HLT, pages 4171\u20134186, Min- neapolis, USA. Jesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, and Noah A Smith. 2019. Show your work: Improved reporting of experimental results.",
            "In NAACL-HLT, pages 4171\u20134186, Min- neapolis, USA. Jesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, and Noah A Smith. 2019. Show your work: Improved reporting of experimental results. In EMNLP-IJCNLP, pages 2185\u20132194, Hong Kong, China. Rezarta Islamaj Do\u02d8gan, Robert Leaman, and Zhiyong Lu. 2014. NCBI disease corpus: a resource for dis- ease name recognition and concept normalization. Journal of biomedical informatics, 47:1\u201310. Martin Gerner, Goran Nenadic, and Casey M Bergman. 2010. LINNAEUS: a species name identi\ufb01cation system for biomedical literature. BMC bioinformat- ics, 11(1):85. Xiaochuang Han and Jacob Eisenstein. 2019. Unsuper- vised domain adaptation of contextualized embed- dings for sequence labeling. In EMNLP-IJCNLP, pages 4229\u20134239, Hong Kong, China.",
            "Xiaochuang Han and Jacob Eisenstein. 2019. Unsuper- vised domain adaptation of contextualized embed- dings for sequence labeling. In EMNLP-IJCNLP, pages 4229\u20134239, Hong Kong, China. Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. 2019a. ClinicalBERT: Modeling clinical notes and predicting hospital readmission. arXiv preprint arXiv:1904.05342. Kexin Huang, Abhishek Singh, Sitong Chen, Ed- ward T Moseley, Chih-ying Deng, Naomi George, and Charlotta Lindvall. 2019b. Clinical XLNet: Modeling sequential clinical notes and predicting prolonged mechanical ventilation. arXiv preprint arXiv:1912.11975. Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka, Yuka Tateisi, and Nigel Collier. 2004. Introduc- tion to the bio-entity recognition task at JNLPBA.",
            "Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka, Yuka Tateisi, and Nigel Collier. 2004. Introduc- tion to the bio-entity recognition task at JNLPBA. In International Joint Workshop on Natural Lan- guage Processing in Biomedicine and its Applica- tions, pages 70\u201375. Martin Krallinger, Obdulia Rabal, Florian Leitner, Miguel Vazquez, David Salgado, Zhiyong Lu, Robert Leaman, Yanan Lu, Donghong Ji, Daniel M Lowe, et al. 2015. The CHEMDNER corpus of chemicals and drugs and its annotation principles. Journal of cheminformatics, 7(1):1\u201317. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2020. BioBERT: A pre- trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234\u20131240.",
            "2020. BioBERT: A pre- trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234\u20131240. Jiao Li, Yueping Sun, Robin J Johnson, Daniela Sci- aky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J Mattingly, Thomas C Wiegers, and Zhiyong Lu. 2016. BioCreative V CDR task corpus: a resource for chemical disease relation extraction. Database, 2016. Ilya Loshchilov and Frank Hutter. 2018. Fixing weight decay regularization in Adam. Tomas Mikolov, Kai Chen, Greg Corrado, and Jef- frey Dean. 2013a. Ef\ufb01cient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013b. Exploiting similarities among languages for ma- chine translation. arXiv preprint arXiv:1309.4168.",
            "Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013b. Exploiting similarities among languages for ma- chine translation. arXiv preprint arXiv:1309.4168. Timo M\u00a8oller, Anthony Reina, Raghavan Jayakumar, and Malte Pietsch. 2020. Covid-qa: A question & answer dataset for covid-19. Evangelos Pa\ufb01lis, Sune P Frankild, Lucia Fanini, Sarah Faulwetter, Christina Pavloudi, Aikaterini Vasileiadou, Christos Arvanitidis, and Lars Juhl Jensen. 2013. The SPECIES and ORGANISMS re- sources for fast and accurate identi\ufb01cation of taxo- nomic names in text. PloS one, 8(6). Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text.",
            "PloS one, 8(6). Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In EMNLP, pages 2383\u20132392, Austin, USA. Larry Smith, Lorraine K Tanabe, Rie Johnson nee Ando, Cheng-Ju Kuo, I-Fang Chung, Chun-Nan Hsu, Yu-Shi Lin, Roman Klinger, Christoph M Friedrich, Kuzman Ganchev, et al. 2008. Overview of BioCreative II gene mention recognition. Genome biology, 9(2):S2. Emma Strubell, Ananya Ganesh, and Andrew McCal- lum. 2019. Energy and policy considerations for deep learning in NLP. In ACL, pages 3645\u20133650, Florence, Italy. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.",
            "In ACL, pages 3645\u20133650, Florence, Italy. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NeurIPS, pages 5998\u20136008, Long Beach, USA. Hai Wang, Dian Yu, Kai Sun, Janshu Chen, and Dong Yu. 2019. Improving pre-trained multilingual mod- els with vocabulary expansion. In CoNLL, pages 316\u2013327, Hong Kong, China.",
            "Inexpensive Domain Adaptation of Pretrained Language Models (Appendix) Word2Vec training We downloaded the PubMed, PMC and CORD-19 corpora from: \u2022 https:\/\/ftp.ncbi.nlm.nih.gov\/pub\/ pmc\/oa_bulk\/ [20 January 2020, 68GB raw text] \u2022 https:\/\/ftp.ncbi.nlm.nih.gov\/pubmed\/ baseline\/ [20 January 2020, 24GB raw text] \u2022 https:\/\/pages.semanticscholar.org\/ coronavirus-research [17 April 2020, 2GB raw text] We extract all abstracts and text bodies and apply the BERT basic tokenizer (a rule-based word tok- enizer that standard BERT uses before wordpiece tokenization). Then, we train CBOW Word2Vec3 with negative sampling. We use default parame- ters except for the vector size (which we set to dW2V = dLM).",
            "Then, we train CBOW Word2Vec3 with negative sampling. We use default parame- ters except for the vector size (which we set to dW2V = dLM). Experiment 1: Biomedical NER Pretrained models General-domain BERT and BioBERTv1.0 were downloaded from: \u2022 www.storage.googleapis.com\/bert_ models\/2018_10_18\/cased_L-12_H- 768_A-12.zip \u2022 www.github.com\/naver\/biobert- pretrained Data We downloaded the NER datasets by follow- ing instructions on www.github.com\/dmis-lab\/ biobert#Datasets. For detailed dataset statistics, see Lee et al. (2020). Preprocessing We use Lee et al. (2020)\u2019s preprocessing strategy: We cut all sentences into chunks of 30 or fewer whitespace-tokenized words (without splitting in- side labeled spans).",
            "For detailed dataset statistics, see Lee et al. (2020). Preprocessing We use Lee et al. (2020)\u2019s preprocessing strategy: We cut all sentences into chunks of 30 or fewer whitespace-tokenized words (without splitting in- side labeled spans). Then, we tokenize every chunk S with T = TLM or T = \u02c6TLM and add special tokens: X = [CLS] T (S) [SEP] Word-initial wordpieces in T (S) are labeled as B(egin), I(nside) or O(utside), while non-word- initial wordpieces are labeled as X(ignore). 3www.github.com\/tmikolov\/word2vec Modeling, training and inference We follow Lee et al. (2020)\u2019s implementation (www.github.com\/dmis-lab\/biobert): We add a randomly initialized softmax classi\ufb01er on top of the last BERT layer to predict the labels. We \ufb01netune the entire model to minimize negative log likelihood, with the AdamW optimizer (Loshchilov and Hutter, 2018) and a linear learning rate sched- uler (10% warmup).",
            "We \ufb01netune the entire model to minimize negative log likelihood, with the AdamW optimizer (Loshchilov and Hutter, 2018) and a linear learning rate sched- uler (10% warmup). All \ufb01netuning runs were done on a GeForce Titan X GPU (12GB). At inference time, we gather the output logits of word-initial wordpieces only. Since the number of word-initial wordpieces is the same for TLM(S) and \u02c6TLM(S), this makes mean-pooling the logits straightforward. Hyperparameters We tune the batch size and peak learning rate on the development set (metric: F1), using the same hyperparameter space as Lee et al. (2020): Batch size: [10, 16, 32, 64]4 Learning rate: [1 \u00b7 10\u22125, 3 \u00b7 10\u22125, 5 \u00b7 10\u22125] We train for 100 epochs, which is the upper end of the 50\u2013100 range recommended by the original authors.",
            "After selecting the best con\ufb01guration for every task and model (see Table 4), we train the \ufb01nal model on the concatenation of training and development set, as was done by Lee et al. (2020). See Figure 2 for expected maximum development set F1 as a function of the number of evaluated hy- perparameter con\ufb01gurations (Dodge et al., 2019). Experiment 2: Covid-19 QA Pretrained model We downloaded the SQuADBERT baseline from: \u2022 www.huggingface.co\/bert-large- uncased-whole-word-masking- finetuned-squad Data We downloaded the Deepset-AI Covid-QA dataset from: \u2022 www.github.com\/deepset-ai\/COVID- QA\/blob\/master\/data\/question- answering\/COVID-QA.json [24 June 2020] 4Since LINNAEUS and BC4CHEM have longer maximum tokenized chunk lengths than the other datasets, our hardware was insuf\ufb01cient to evaluate batch size 64 on them.",
            "At the time of writing, the dataset contains 2019 questions and gold answer spans.5 Every ques- tion is associated with one of 147 research papers (contexts) from CORD-19.6 Since we do not do target-domain \ufb01netuning, we treat the entire dataset as a test set. Preprocessing We tokenize every question-context pair (Q, C) with T = TLM or T = \u02c6TLM, which yields (T (Q), T (C)). Since T (C) is usually too long to be digested in a single forward pass, we de- \ufb01ne a sliding window with width and stride N = \ufb02oor(509\u2212|T (Q)| 2 ). At step n, the \u201cactive\u201d win- dow is between a(l) n = (n \u22121)N + 1 and a(r) n = min(|C|, nN).",
            "At step n, the \u201cactive\u201d win- dow is between a(l) n = (n \u22121)N + 1 and a(r) n = min(|C|, nN). The input is de\ufb01ned as: X(n) = [CLS] T (Q) [SEP] T (C)a(l) n \u2212p(l) n :a(r) n +p(r) n [SEP] p(l) n and p(r) n are chosen such that |X(n)| = 512, and such that the active window is in the center of the input (if possible). Modeling and inference Feeding X(n) into the QA model yields start log- its h\u2032(start,n) \u2208R|X(n)| and end logits h\u2032(end,n) \u2208 R|X(n)|. We extract and concatenate the slices that correspond to the active windows of all steps: h(\u2217) \u2208R|T (C)| h(\u2217) = [h\u2032(\u2217,1) a(l) 1 :a(r) 1 ; . . .",
            "We extract and concatenate the slices that correspond to the active windows of all steps: h(\u2217) \u2208R|T (C)| h(\u2217) = [h\u2032(\u2217,1) a(l) 1 :a(r) 1 ; . . . ; h\u2032(\u2217,n) a(l) n :a(r) n ; . . .] Next, we map the logits from the wordpiece level to the word level. This allows us to mean-pool the outputs of TLM and \u02c6TLM even when |TLM(C)| \u0338= | \u02c6TLM(C)|. Let ci be a word in C and let T (C)j:j+|T (ci)| be the corresponding wordpieces.",
            "Let ci be a word in C and let T (C)j:j+|T (ci)| be the corresponding wordpieces. The start and end logits of ci are: o(\u2217) i = maxj\u2264j\u2032\u2264j+|T (ci)|[h(\u2217) j\u2032 ] Finally, we return the answer span Ck:k\u2032 that maximizes o(start) k + o(end) k\u2032 , subject to the con- straints that k\u2032 does not precede k and the answer contains no more than 500 characters. 5In an earlier version of the paper, we reported results on a preliminary version of Deepset-AI Covid-QA, which contained 1380 questions. 6www.github.com\/deepset-ai\/COVID- QA\/issues\/103 Notes on Covid-QA There are some important differences between Covid-QA and SQuAD, which make the task chal- lenging: \u2022 The Covid-QA contexts are full documents rather than single paragraphs. Thus, the cor- rect answer may appear several times, often with slightly different wordings. But only a single occurrence is annotated as correct, e.g.",
            "Thus, the cor- rect answer may appear several times, often with slightly different wordings. But only a single occurrence is annotated as correct, e.g.: Question: What was the prevalence of Coro- navirus OC43 in community samples in Ilorin, Nigeria? Correct: 13.3% (95% CI 6.9-23.6%) # from main text Predicted: 13.3%, 10\/75 # from abstract \u2022 SQuAD gold answers are de\ufb01ned as the \u201cshortest span in the paragraph that answered the question\u201d (Rajpurkar et al., 2016, p. 4), but many Covid-QA gold answers are longer and contain non-essential context, e.g.: Question: When was the Middle East Res- piratory Syndrome Coronavirus isolated \ufb01rst?",
            "4), but many Covid-QA gold answers are longer and contain non-essential context, e.g.: Question: When was the Middle East Res- piratory Syndrome Coronavirus isolated \ufb01rst? Correct: (MERS-CoV) was \ufb01rst isolated in 2012, in a 60-year-old man who died in Jeddah, KSA due to severe acute pneu- monia and multiple organ failure Predicted: 2012 These differences are part of the reason why the exact match score is lower than the word-level F1 score and the substring score (see Table 3, bottom, main paper).",
            "BERT (repro) BioBERTv1.0 (repro) GreenBioBERT Biomedical NER task (ID) hyperparams dev set F1 hyperparams dev set F1 hyperparams dev set F1 BC5CDR-disease (1) 32, 3 \u00b7 10\u22125 82.12 10, 1 \u00b7 10\u22125 85.15 32, 1 \u00b7 10\u22125 83.90 NCBI-disease (2) 32, 3 \u00b7 10\u22125 87.52 32, 1 \u00b7 10\u22125 87.99 10, 3 \u00b7 10\u22125 88.43 BC5CDR-chem (3) 64, 3 \u00b7 10\u22125 91.00 32, 1 \u00b7 10\u22125 93.36 10, 1 \u00b7 10\u22125 92.59 BC4CHEMD (4) 16, 1 \u00b7 10\u22125 88.02 32, 1 \u00b7 10\u22125 89.35 16, 1 \u00b7 10\u22125 88.53 BC2GM (5) 32,",
            "59 BC4CHEMD (4) 16, 1 \u00b7 10\u22125 88.02 32, 1 \u00b7 10\u22125 89.35 16, 1 \u00b7 10\u22125 88.53 BC2GM (5) 32, 1 \u00b7 10\u22125 83.91 64, 3 \u00b7 10\u22125 85.54 64, 3 \u00b7 10\u22125 84.25 JNLPBA (6) 32, 5 \u00b7 10\u22125 85.18 32, 5 \u00b7 10\u22125 85.30 10, 3 \u00b7 10\u22125 85.10 LINNAEUS (7) 16, 1 \u00b7 10\u22125 96.67 32, 1 \u00b7 10\u22125 97.22 10, 1 \u00b7 10\u22125 96.49 Species-800 (8) 32, 1 \u00b7 10\u22125 72.70 32, 1 \u00b7 10\u22125 77.34 16, 1 \u00b7 10\u22125 75.93 Table 4: Best hyperparameters (batch size,",
            "49 Species-800 (8) 32, 1 \u00b7 10\u22125 72.70 32, 1 \u00b7 10\u22125 77.34 16, 1 \u00b7 10\u22125 75.93 Table 4: Best hyperparameters (batch size, peak learning rate) and best dev set F1 per NER task and model. BERT (repro) and BioBERTv1.0 (repro) refer to our reproduction experiments.",
            "1 \u00b7 10\u22125 75.93 Table 4: Best hyperparameters (batch size, peak learning rate) and best dev set F1 per NER task and model. BERT (repro) and BioBERTv1.0 (repro) refer to our reproduction experiments. 3 6 9 12 82 84 (1) 3 6 9 12 87 88 (2) 3 6 9 12 91 92 93 (3) 3 6 9 87 88 89 (4) 3 6 9 12 84 85 (5) 3 6 9 12 84 85 (6) 3 6 9 95 96 97 (7) 3 6 9 12 72 74 76 (8) 0.0 0.2 0.4 0.6 0.8 1.0 Number of hyperparameter configurations evaluated 0.0 0.2 0.4 0.6 0.8 1.0 Expected max dev set F1 BERT (repro) BioBERTv1.0 (repro) GreenBioBERT Figure 2: Expected maximum F1 on NER development sets as a function of the number of evaluated hyperparam- eter con\ufb01gurations.",
            "Numbers in brackets are NER task IDs (see Table 4)."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-2004.03354.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 8531.000061035156,
    "avg_doclen_est": 164.0576934814453
}
