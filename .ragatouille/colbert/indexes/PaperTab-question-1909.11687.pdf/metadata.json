{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Extremely Small BERT Models from Mixed-Vocabulary Training Sanqiang Zhao*1, Raghav Gupta*2, Yang Song3, and Denny Zhou4 1University of Pittsburgh, Pittsburgh, PA sanqiang.zhao@pitt.edu 2Google Research, Mountain View, CA raghavgupta@google.com 3Kuaishou Technology, Beijing, China yangsong@kuaishou.com 4Google Brain, Mountain View, CA dennyzhou@google.com Abstract Pretrained language models like BERT have achieved good results on NLP tasks, but are impractical on resource-limited devices due to memory footprint. A large fraction of this foot- print comes from the input embeddings with large input vocabulary and embedding dimen- sions. Existing knowledge distillation meth- ods used for model compression cannot be di- rectly applied to train student models with re- duced vocabulary sizes. To this end, we pro- pose a distillation method to align the teacher and student embeddings via mixed-vocabulary training.",
      "Existing knowledge distillation meth- ods used for model compression cannot be di- rectly applied to train student models with re- duced vocabulary sizes. To this end, we pro- pose a distillation method to align the teacher and student embeddings via mixed-vocabulary training. Our method compresses BERTLARGE to a task-agnostic model with smaller vocab- ulary and hidden dimensions, which is an or- der of magnitude smaller than other distilled BERT models and offers a better size-accuracy trade-off on language understanding bench- marks as well as a practical dialogue task. 1 Introduction Recently, pre-trained context-aware language mod- els like ELMo (Peters et al., 2018), GPT (Radford et al., 2019), BERT (Devlin et al., 2018) and XLNet (Yang et al., 2019) have outperformed traditional word embedding models like Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), and achieved strong results on a number of lan- guage understanding tasks.",
      "However, these models are typically too huge to host on mobile/edge de- vices, especially for real-time inference. Recent work has explored, inter alia, knowledge distilla- tion (Ba and Caruana, 2014; Hinton et al., 2015) to train small-footprint student models by implicit transfer of knowledge from a teacher model. Most distillation methods, however, need the stu- dent and teacher output spaces to be aligned. This complicates task-agnostic distillation of BERT to Asterisk (*) denotes equal contribution. Research con- ducted when all authors were at Google. smaller-vocabulary student BERT models since the input vocabulary is also the output space for the masked language modeling (MLM) task used in BERT. This in turn limits these distillation meth- ods\u2019 ability to compress the input embedding ma- trix, that makes up a major proportion of model parameters e.g. the \u223c30K input WordPiece embed- dings of the BERTBASE model make up over 21% of the model size.",
      "the \u223c30K input WordPiece embed- dings of the BERTBASE model make up over 21% of the model size. This proportion is even higher for most distilled BERT models, owing to these distilled models typically having fewer layers than their teacher BERT counterparts. We present a task and model-agnostic distillation approach for training small, reduced-vocabulary BERT models running into a few megabytes. In our setup, the teacher and student models have incompatible vocabularies and tokenizations for the same sequence. We therefore align the stu- dent and teacher WordPiece embeddings by train- ing the teacher on the MLM task with a mix of teacher-tokenized and student-tokenized words in a sequence, and then using these student embed- dings to train smaller student models. Using our method, we train compact 6 and 12-layer reduced- vocabulary student models which achieve competi- tive performance in addition to high compression for benchmark datasets as well as a real-world ap- plication in language understanding for dialogue. 2 Related Work Work in NLP model compression falls broadly into four classes: matrix approximation, weight quanti- zation, pruning/sharing, and knowledge distillation.",
      "2 Related Work Work in NLP model compression falls broadly into four classes: matrix approximation, weight quanti- zation, pruning/sharing, and knowledge distillation. The former two seek to map model parameters to low-rank approximations (Tulloch and Jia, 2017) and lower-precision integers/\ufb02oats (Chen et al., 2015; Zhou et al., 2018; Shen et al., 2019) respec- tively. In contrast, pruning aims to remove/share redundant model weights (Li et al., 2016; Lan et al., 2019). More recently, dropout (Srivastava et al., 2014) has been used to cut inference latency by arXiv:1909.11687v2  [cs.CL]  6 Feb 2021",
      "Figure 1: Depiction of our mixed-vocabulary training approach. (Left) Stage I involving retrained teacher BERT with default con\ufb01g (e.g., 30K vocabulary, 768 hidden dim) and mixed-vocabulary input. (Right) Stage II involving student model with smaller vocabulary (5K) and hidden dims (e.g., 256) and embeddings initialized from stage I. early exit (Fan et al., 2019; Xin et al., 2020). Knowledge distillation focuses on implicit trans- fer of knowledge as soft teacher predictions (Tang et al., 2019), attention distributions (Zagoruyko and Komodakis, 2016) and intermediate outputs (Romero et al., 2014).",
      "Knowledge distillation focuses on implicit trans- fer of knowledge as soft teacher predictions (Tang et al., 2019), attention distributions (Zagoruyko and Komodakis, 2016) and intermediate outputs (Romero et al., 2014). Approaches close to our work rely on similar methods (Sanh et al., 2019; Sun et al., 2019), while others involve combina- tions of layer-wise transfer (Sun et al., 2020), task- speci\ufb01c distillation (Jiao et al., 2019), architecture search (Chen et al., 2020) and layer dropout (Xu et al., 2020); many of these are speci\ufb01c to the trans- former layer (Vaswani et al., 2017). Another highly relevant line of work focuses on reducing the size of the embedding matrix, ei- ther via factorization (Shu and Nakayama, 2018; Lan et al., 2019) or vocabulary selection/pruning (Provilkov et al., 2019; Chen et al., 2019b).",
      "3 Proposed Approach Here, we discuss our rationale behind reducing the student vocabulary size and its challenges, followed by our mixed-vocabulary distillation approach. 3.1 Student Vocabulary WordPiece (WP) tokens (Wu et al., 2016) are sub- word units obtained by applying greedy segmenta- tion to a training corpus. Given such a corpus and a number of desired tokens D, a WordPiece vocab- ulary is generated by selecting D subword tokens such that the resulting corpus is minimal in the num- ber of WordPiece when segmented according to the chosen WordPiece model. The greedy algorithm for this optimization problem is described in more detail in Sennrich et al. (2016). Most published BERT models use a vocabulary of 30522 Word- Pieces, obtained by running the above algorithm on the Wikipedia and BooksCorpus (Zhu et al., 2015) corpora with a desired vocabulary size D of 30000. For our student model, we chose a target vocab- ulary size D of 5000 WordPiece tokens. Using the same WordPiece vocabulary generation algorithm and corpus as above, we obtain a 4928-WordPiece vocabulary for the student model.",
      "For our student model, we chose a target vocab- ulary size D of 5000 WordPiece tokens. Using the same WordPiece vocabulary generation algorithm and corpus as above, we obtain a 4928-WordPiece vocabulary for the student model. This student vo- cabulary includes all ASCII characters as separate tokens, ensuring no out-of-vocabulary words upon tokenization with this vocabulary. Additionally, the 30K teacher BERT vocabulary includes 93.9% of the WP tokens in this 5K student vocabulary but does not subsume it. We explore other strategies to obtain a small student vocabulary in Section 6. For task-agnostic student models, we reuse BERT\u2019s masked language modeling (MLM) task: words in context are randomly masked and pre- dicted given the context via softmax over the model\u2019s WP vocabulary. Thus, the output spaces for our teacher (30K) and student (5K) models are unaligned. This, coupled with both vocabularies tokenizing the same words differently, means exist- ing distillation methods do not apply to our setting.",
      "Thus, the output spaces for our teacher (30K) and student (5K) models are unaligned. This, coupled with both vocabularies tokenizing the same words differently, means exist- ing distillation methods do not apply to our setting. 3.2 Mixed-vocabulary training We propose a two-stage approach for implicit trans- fer of knowledge to the student via the student embeddings, as described below. Stage I (Student Embedding Initialization): We \ufb01rst train the student embeddings with the teacher",
      "model initialized from BERTLARGE. For a given in- put sequence, we mix the vocabularies by randomly selecting (with probability pSV , a hyperparameter) words from the sequence to segment using the stu- dent vocabulary, with the other words segmented using the teacher vocabulary. As in Figure 1 on the left, for input [\u2018I\u2019, \u2018like\u2019, \u2018machine\u2019, \u2018learning\u2019], the words \u2018like\u2019 and \u2018learning\u2019 are segmented using the student vocabulary (in blue), with the others using the teacher vocabulary (in green). Similar to Lample and Conneau (2019), this step seeks to align the student and teacher embeddings for the same tokens: the model learns to predict student tokens using context which is segmented using the teacher vocabulary, and vice versa. Note that since the student embeddings are set to a lower dimension than the teacher embeddings, as they are meant to be used in the smaller student model, we project the student embeddings up to the teacher embedding dimension using a trainable af\ufb01ne layer before these are input to the teacher BERT.",
      "Note that since the student embeddings are set to a lower dimension than the teacher embeddings, as they are meant to be used in the smaller student model, we project the student embeddings up to the teacher embedding dimension using a trainable af\ufb01ne layer before these are input to the teacher BERT. We choose to keep the two embedding ma- trices separate despite the high token overlap: this is partly to keep our approach robust to lower vo- cabulary overlap settings, and partly due to empiri- cal considerations described in Section 6. Let \u03b8s/ebs and \u03b8t/ebt denote the transformer layer and embedding weights for the student and teacher models respectively. The loss de\ufb01ned in Equation 1 is the MLM cross entropy summed over masked positions Mt in the teacher input. yi and ci denote the predicted and true tokens at position i respectively and can belong to either vocabulary. vi\u2208{s,t} denotes the vocabulary used to segment this token.",
      "The loss de\ufb01ned in Equation 1 is the MLM cross entropy summed over masked positions Mt in the teacher input. yi and ci denote the predicted and true tokens at position i respectively and can belong to either vocabulary. vi\u2208{s,t} denotes the vocabulary used to segment this token. Separate softmax layers Pvi are used for token prediction, one for each vocabulary, de- pending on the segmenting vocabulary vi for token i. All teacher parameters (\u03b8t, ebt) and student em- beddings (ebs) are updated in this step. Ls1 = \u2212P i\u2208Mt(logPvi(yi=ci|\u03b8t, ebs, ebt)) (1) Stage II (Student Model Layers): With student embeddings initialized in stage I, we now train the student model normally i.e., using only the student vocabulary and discarding the teacher model. Equa- tion 2 shows the student MLM loss where Ms is the set of positions masked in the student input. All student model parameters (\u03b8s, ebs) are updated.",
      "Equa- tion 2 shows the student MLM loss where Ms is the set of positions masked in the student input. All student model parameters (\u03b8s, ebs) are updated. Ls2 = \u2212P i\u2208Ms logPs(yi=ci|\u03b8s, ebs)) (2) 4 Experiments For evaluation, we \ufb01netune the student model just as one would \ufb01netune the original BERT model i.e., without using the teacher model or any task- speci\ufb01c distillation. We describe our experiments below, with dataset details left to the appendix. 4.1 Evaluation Tasks and Datasets We \ufb01ne-tune and evaluate the distilled student mod- els on two classes of language understanding tasks: GLUE benchmark (Wang et al., 2019): We pick three classi\ufb01cation tasks from GLUE: \u2022 MRPC: Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005), a 2-way sentence pair classi\ufb01cation task with 3.7K train instances.",
      "\u2022 MNLI: Multi-Genre Natural Language Inference (Williams et al., 2018), a 3-way sentence pair classi\ufb01cation task with 393K training instances. \u2022 SST-2: Stanford Sentiment Treebank (Socher et al., 2013), a 2-way sentence classi\ufb01cation task with 67K training instances. Spoken Language Understanding: Since we are also keen on edge device applications, we also eval- uate on spoken language understanding, a practi- cal task in dialogue systems. We use the SNIPS dataset (Coucke et al., 2018) of \u223c14K virtual as- sistant queries, each comprising one of 7 intents and values for one or more of the 39 pre-de\ufb01ned slots. The intent detection and slot \ufb01lling subtasks are modeled respectively as 7-way sentence classi- \ufb01cation and sequence tagging with IOB slot labels. 4.2 Models and Baselines For GLUE, we train student models with 6 and 12 layers, 4 attention heads, and embedding/hidden dimensions \ufb01xed to 256, each using a compact 5K- WP vocabulary.",
      "4.2 Models and Baselines For GLUE, we train student models with 6 and 12 layers, 4 attention heads, and embedding/hidden dimensions \ufb01xed to 256, each using a compact 5K- WP vocabulary. We also evaluate baselines with- out knowledge distillation (NoKD), parameterized identically to the distilled student models (incl. the 5K vocabulary), trained on the MLM teacher ob- jective from scratch. We also compare our models on GLUE with the following approaches: \u2022 DistilBERT (Sanh et al., 2019) distill BERTBASE to 4/6 layers by aligning teacher predictions, \u2022 Patient KD - PKD (Sun et al., 2019) align hidden states to distill BERTBASE to 3/6 layers, \u2022 BERT-of-Theseus (Xu et al., 2020) use a layer dropout method to distill BERTBASE to 6 layers,",
      "Model #Params MRPC MNLI-m/mm SST-2 Average (F1/Acc) (Acc) (Acc) (F1/Acc) BERTBASE (Devlin et al., 2018) 109M 88.9/- 84.6/83.4 93.5 89.0 BERTLARGE (Devlin et al., 2018) 340M 89.3/- 86.7/85.9 94.9 90.3 PKD6 (Sun et al., 2019) 67.0M 85.0/79.9 81.5/81.0 92.0 86.2 PKD3 (Sun et al., 2019) 45.7M 80.7/72.5 76.7/76.3 87.5 81.6 DistilBERT4 (Sanh et al., 2019) 52.2M 82.4/- 78.9/78.0 91.4 84.2 MobileBERT (Sun et al., 2020) 25.3M 88.",
      "6 DistilBERT4 (Sanh et al., 2019) 52.2M 82.4/- 78.9/78.0 91.4 84.2 MobileBERT (Sun et al., 2020) 25.3M 88.8/84.5 83.3/82.6 92.8 88.3 TinyBERT4 (Jiao et al., 2019) 14.5M 82.0*/ - 76.6/77.2* - - TinyBERT4\u2020 (Jiao et al., 2019) 14.5M 86.4/- 82.5/81.8 92.6 87.2 BERT-of-Theseus6\u2020 (Xu et al., 2020) 66M 87.6/83.2 82.4/82.1 92.2 87.4 NoKD Baseline, L-6, H-256 6.2M 81.2/74.1 76.9/76.1 87.0 81.7 Mixed-vocab distilled (ours),",
      "4/82.1 92.2 87.4 NoKD Baseline, L-6, H-256 6.2M 81.2/74.1 76.9/76.1 87.0 81.7 Mixed-vocab distilled (ours), L-6, H-256 84.9/79.3 79.0/78.6 89.1 84.3 NoKD Baseline, L-12, H-256 10.9M 85.1/79.8 79.1/79.0 89.4 84.5 Mixed-vocab distilled (ours), L-12, H-256 87.2/82.6 80.7/80.5 90.6 86.2 * denotes metrics on the development set \u2020 denotes results with task-speci\ufb01c distillation Table 1: Test set accu- racy of distilled models, teacher model and base- lines on the GLUE test sets, along with other pa- rameters. MNLI-m and MNLI-mm refer to the genre-matched and mis- matched test sets.",
      "teacher model and base- lines on the GLUE test sets, along with other pa- rameters. MNLI-m and MNLI-mm refer to the genre-matched and mis- matched test sets. All models other than NoKD and our distilled models use a 30K-WordPiece vo- cabulary. The average is computed using F1 score for MRPC and accuracy for MNLI-m and SST-2. \u2022 TinyBERT (Jiao et al., 2019) apply task speci\ufb01c distillation to BERTBASE and align teacher out- puts, hidden states as well as embeddings, and \u2022 MobileBERT (Sun et al., 2020) combine layer- wise transfer, architecture search and bottleneck structures for an optimized student model. For SNIPS, we shift our focus to smaller, low- latency models for on-device use cases. Here, we train student models with 6 layers and em- bedding/hidden dimensions \u2208{96, 192, 256}. The smaller models here may not be competitive on GLUE but are adequate for practical tasks such as spoken LU.",
      "Here, we train student models with 6 layers and em- bedding/hidden dimensions \u2208{96, 192, 256}. The smaller models here may not be competitive on GLUE but are adequate for practical tasks such as spoken LU. We compare with two strong baselines: \u2022 BERTBASE (Chen et al., 2019a) with intent and IOB slot tags predicted using the [CLS] and the \ufb01rst WP tokens of each word respectively, and \u2022 StackProp (Qin et al., 2019), which uses a series of smaller recurrent and self-attentive encoders. 4.3 Training Details Distillation: For all our models, we train the teacher model with mixed-vocabulary inputs (stage I) for 500K steps, followed by 300K steps of train- ing just the student model (stage II). We utilize the same corpora as the teacher model i.e. BooksCor- pus (Zhu et al., 2015) and English Wikipedia. For both stages, up to 20 input tokens were masked for MLM.",
      "We utilize the same corpora as the teacher model i.e. BooksCor- pus (Zhu et al., 2015) and English Wikipedia. For both stages, up to 20 input tokens were masked for MLM. In stage I, up to 10 of these masked tokens were tokenized using the teacher vocabulary, the rest using the student vocabulary. We optimize the loss using LAMB (You et al., 2019) with a max learning rate of .00125, linear warmup for the \ufb01rst 10% of steps, batch size of 2048 and sequence length of 128. Distillation was done on Cloud TPUs in a 8x8 pod con\ufb01guration. pSV , the probability of segmenting a Stage I input word using the student vocabulary, is set to 0.5. Finetuning: For all downstream task evaluations on GLUE, we \ufb01netune for 10 epochs using LAMB with a learning rate of 0.0001 and batch size of 64. For all experiments on SNIPS, we use ADAM with a learning rate of 0.0001 and a batch size of 64.",
      "For all experiments on SNIPS, we use ADAM with a learning rate of 0.0001 and a batch size of 64. 5 Results GLUE: Table 1 shows results on the downstream GLUE tasks alongside model sizes, for our models, BERTBASE/LARGE, and all baselines. Our proposed models consistently improve upon the identically parameterized NoKD baselines, in- dicating that mixed-vocabulary training is better than training from scratch and avoids a drastic drop in performance from the teacher. Compared with PKD and DistilBERT, our 6-layer model outper- forms PKD3 while being >7x smaller and our 12- layer model performs competitively with PKD6 and DistilBERT4 while being \u223c5-6x smaller. Interestingly, our models do particularly well on the MRPC task: the 6-layer distilled model performs almost as well as PKD6 while being over 10x smaller. This may be due to our smaller models being data-ef\ufb01cient on the smaller MRPC dataset.",
      "Interestingly, our models do particularly well on the MRPC task: the 6-layer distilled model performs almost as well as PKD6 while being over 10x smaller. This may be due to our smaller models being data-ef\ufb01cient on the smaller MRPC dataset. TinyBERT and Bert-of-Theseus are trained in task-speci\ufb01c fashion i.e., a teacher model already \ufb01netuned on the downstream task is used for dis- tillation. TinyBERT\u2019s non-task-speci\ufb01c model re- sults are reported on GLUE dev sets: these results are, therefore, not directly comparable with ours. Even so, our 12-layer model performs credibly",
      "Model #Params Latency Intent Acc Slot F1 BERTBASE (Chen et al., 2019a) 109M 340ms 98.6 97.0 StackProp (Qin et al., 2019) 2.6M >70ms 98.0 94.2 Mixed-vocab distilled, L-6, H-96 1.2M 6ms 98.9 92.8 Mixed-vocab distilled, L-6, H-192 3.6M 14ms 98.8 94.6 Mixed-vocab distilled, L-6, H-256 6.2M 20ms 98.7 95.0 Table 2: Results on the SNIPS dataset. Latency is mea- sured with 4 CPU threads on a Pixel 4 mobile device. compared with the two, presenting a competitive size-accuracy tradeoff, particularly when compared to the 6x larger BERT-of-Theseus. MobileBERT performs strongly for the size while being task-agnostic.",
      "compared with the two, presenting a competitive size-accuracy tradeoff, particularly when compared to the 6x larger BERT-of-Theseus. MobileBERT performs strongly for the size while being task-agnostic. Our 12-layer model, in comparison, retains \u223c98% of its performance with 57% fewer parameters and may thus be better- suited for use on highly resource-limited devices. TinyBERT sees major gains from task-speci\ufb01c data augmentation and distillation, and Mobile- BERT from student architecture search and bot- tleneck layers. Notably, our technique targets the student vocabulary without con\ufb02icting with any of the above methods and can, in fact, be combined with these methods for even smaller models. SNIPS: Table 2 shows results on the SNIPS intent and slot tasks for our models and two state-of-the- art baselines.",
      "SNIPS: Table 2 shows results on the SNIPS intent and slot tasks for our models and two state-of-the- art baselines. Our smallest 6-layer model retains over 95% of the BERTBASE model\u2019s slot \ufb01lling F1 score (Sang and Buchholz, 2000) while being 30x smaller (< 10 MB w/o quantization) and 57x faster on a mobile device, yet task-agnostic. Our other larger distilled models also demonstrate strong per- formance (0.2-0.5% slot F1 higher than the respec- tive NoKD baselines) with small model sizes and latencies low enough for real-time inference. This indicates that small multi-task BERT models (Tsai et al., 2019) present better trade-offs for on-device usage for size, accuracy and latency versus recur- rent encoder-based models such as StackProp. 6 Discussion Impact of vocabulary size: We trained a model from scratch identical to BERTBASE except with our 5K-WP student vocabulary.",
      "6 Discussion Impact of vocabulary size: We trained a model from scratch identical to BERTBASE except with our 5K-WP student vocabulary. On the SST-2 and MNLI-m dev sets, this model obtained 90.9% and 83.7% accuracy respectively - only 1.8% and 0.7% lower respectively compared to BERTBASE. Since embeddings account for a larger fraction of model parameters with fewer layers, we trained another model identical to our 6\u00d7256 model, but with a 30K-WP vocabulary and teacher label dis- tillation. This model showed small gains (0.1% / 0.5% accuracy on SST-2 / MNLI-m dev) over our analogous distilled model, but with 30% more parameters solely due to the larger vocabulary. This suggests that a small WordPiece vocabulary may be almost as effective for sequence classi\ufb01- cation/tagging tasks, especially for smaller BERT models and up to moderately long inputs. Curi- ously, increasing the student vocabulary size to 7K or 10K did not lead to an increase in performance on GLUE.",
      "Curi- ously, increasing the student vocabulary size to 7K or 10K did not lead to an increase in performance on GLUE. We surmise that this may be due to un- der\ufb01tting owing to the embeddings accounting for a larger proportion of the model parameters. Alternative vocabulary pruning: Probing other strategies for a small-vocabulary model, we used the above 6\u00d7256 30K-WP vanilla distilled model to obtain a smaller model by pruning the vocab- ulary to contain the intersection of the 30K and 5K vocabularies (total 4629 WPs). This model is 1.2% smaller than our 4928-WP distilled model, but drops 0.8% / 0.7% on SST-2/MNLI-m dev sets. Furthermore, to exploit the high overlap in vo- cabularies, we tried running our distillation pipeline but with the embeddings for student tokens (after projecting up to the teacher dimension) also present in the teacher vocabulary tied to the teacher em- beddings for those tokens.",
      "Furthermore, to exploit the high overlap in vo- cabularies, we tried running our distillation pipeline but with the embeddings for student tokens (after projecting up to the teacher dimension) also present in the teacher vocabulary tied to the teacher em- beddings for those tokens. This model, however, dropped 0.7% / 0.5% on SST-2/MNLI-m compared to our analogous 6\u00d7256 distilled model. We also tried pretraining BERTLARGE from scratch with the 5K vocabulary and doing vanilla distillation for a 6\u00d7256 student: this model dropped 1.2% / 0.7% for SST-2/MNLI-m over our similar distilled model, indicating the ef\ufb01cacy of mixed-vocabulary training over vanilla distillation. 7 Conclusion We propose a novel approach to knowledge distil- lation for BERT, focusing on using a signi\ufb01cantly smaller vocabulary for the student BERT models. Our mixed-vocabulary training method encourages implicit alignment of the teacher and student Word- Piece embeddings.",
      "7 Conclusion We propose a novel approach to knowledge distil- lation for BERT, focusing on using a signi\ufb01cantly smaller vocabulary for the student BERT models. Our mixed-vocabulary training method encourages implicit alignment of the teacher and student Word- Piece embeddings. Our highly-compressed 6 and 12-layer distilled student models are optimized for on-device use cases and demonstrate competitive performance on both benchmark datasets and prac- tical tasks. Our technique is unique in targeting the student vocabulary size, enabling easy combination with most BERT distillation methods.",
      "References Jimmy Ba and Rich Caruana. 2014. Do deep nets really need to be deep? In Advances in neural information processing systems, pages 2654\u20132662. Daoyuan Chen, Yaliang Li, Minghui Qiu, Zhen Wang, Bofang Li, Bolin Ding, Hongbo Deng, Jun Huang, Wei Lin, and Jingren Zhou. 2020. Ad- abert: Task-adaptive bert compression with differ- entiable neural architecture search. arXiv preprint arXiv:2001.04246. Qian Chen, Zhu Zhuo, and Wen Wang. 2019a. Bert for joint intent classi\ufb01cation and slot \ufb01lling. arXiv preprint arXiv:1902.10909. Wenhu Chen, Yu Su, Yilin Shen, Zhiyu Chen, Xifeng Yan, and William Yang Wang. 2019b. How large a vocabulary does text classi\ufb01cation need? a varia- tional approach to vocabulary selection.",
      "Wenhu Chen, Yu Su, Yilin Shen, Zhiyu Chen, Xifeng Yan, and William Yang Wang. 2019b. How large a vocabulary does text classi\ufb01cation need? a varia- tional approach to vocabulary selection. In Proceed- ings of the 2019 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3487\u20133497. Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin Chen. 2015. Compressing neural networks with the hashing trick. In Inter- national Conference on Machine Learning, pages 2285\u20132294. Alice Coucke, Alaa Saade, Adrien Ball, Th\u00b4eodore Bluche, Alexandre Caulier, David Leroy, Cl\u00b4ement Doumouro, Thibault Gisselbrecht, Francesco Calta- girone, Thibaut Lavril, et al. 2018. Snips voice plat- form: an embedded spoken language understanding system for private-by-design voice interfaces.",
      "2018. Snips voice plat- form: an embedded spoken language understanding system for private-by-design voice interfaces. arXiv preprint arXiv:1805.10190. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of deep bidirectional transformers for language under- standing. arXiv preprint arXiv:1810.04805. William B Dolan and Chris Brockett. 2005. Automati- cally constructing a corpus of sentential paraphrases. In Proceedings of the International Workshop on Paraphrasing. Angela Fan, Edouard Grave, and Armand Joulin. 2019. Reducing transformer depth on demand with struc- tured dropout. arXiv preprint arXiv:1909.11556. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.",
      "arXiv preprint arXiv:1909.11556. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531. Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2019. Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351. Guillaume Lample and Alexis Conneau. 2019. Cross- lingual language model pretraining. arXiv preprint arXiv:1901.07291. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learn- ing of language representations. arXiv preprint arXiv:1909.11942.",
      "2019. Albert: A lite bert for self-supervised learn- ing of language representations. arXiv preprint arXiv:1909.11942. Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. 2016. Pruning \ufb01lters for ef\ufb01- cient convnets. arXiv preprint arXiv:1608.08710. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositional- ity. In Advances in neural information processing systems, pages 3111\u20133119. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word rep- resentation. In Proceedings of the 2014 conference on empirical methods in natural language process- ing (EMNLP), pages 1532\u20131543.",
      "Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word rep- resentation. In Proceedings of the 2014 conference on empirical methods in natural language process- ing (EMNLP), pages 1532\u20131543. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227\u2013 2237. Ivan Provilkov, Dmitrii Emelianenko, and Elena Voita. 2019. Bpe-dropout: Simple and effective subword regularization. arXiv preprint arXiv:1910.13267. Libo Qin, Wanxiang Che, Yangming Li, Haoyang Wen, and Ting Liu. 2019. A stack-propagation frame- work with token-level intent detection for spoken language understanding.",
      "arXiv preprint arXiv:1910.13267. Libo Qin, Wanxiang Che, Yangming Li, Haoyang Wen, and Ting Liu. 2019. A stack-propagation frame- work with token-level intent detection for spoken language understanding. In Proceedings of the 2019 Conference on Empirical Methods in Natu- ral Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2078\u20132087. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Adriana Romero, Nicolas Ballas, Samira Ebrahimi Ka- hou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. 2014. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550. Erik F Tjong Kim Sang and Sabine Buchholz. 2000.",
      "2014. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550. Erik F Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the conll-2000 shared task chunk- ing. In Fourth Conference on Computational Nat- ural Language Learning and the Second Learning Language in Logic Workshop. Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.",
      "Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u2013 1725. Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2019. Q-bert: Hessian based ultra low precision quantization of bert. arXiv preprint arXiv:1909.05840. Raphael Shu and Hideki Nakayama. 2018. Compress- ing word embeddings via deep compositional code learning. In International Conference on Learning Representations. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment tree- bank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u20131642.",
      "2013. Recursive deep models for semantic compositionality over a sentiment tree- bank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u20131642. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from over\ufb01tting. The journal of machine learning research, 15(1):1929\u20131958. Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. Patient knowledge distillation for bert model com- pression. arXiv preprint arXiv:1908.09355. Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. 2020. Mobilebert: a compact task-agnostic bert for resource-limited de- vices. arXiv preprint arXiv:2004.02984.",
      "2020. Mobilebert: a compact task-agnostic bert for resource-limited de- vices. arXiv preprint arXiv:2004.02984. Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. 2019. Distilling task- speci\ufb01c knowledge from bert into simple neural net- works. arXiv preprint arXiv:1903.12136. Henry Tsai, Jason Riesa, Melvin Johnson, Naveen Ari- vazhagan, Xin Li, and Amelia Archer. 2019. Small and practical bert models for sequence labeling. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 3623\u2013 3627. Andrew Tulloch and Yangqing Jia. 2017. High perfor- mance ultra-low-precision convolutions on mobile devices. arXiv preprint arXiv:1712.02427.",
      "Andrew Tulloch and Yangqing Jia. 2017. High perfor- mance ultra-low-precision convolutions on mobile devices. arXiv preprint arXiv:1712.02427. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information pro- cessing systems, pages 5998\u20136008. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. GLUE: A multi-task benchmark and analysis plat- form for natural language understanding. In Inter- national Conference on Learning Representations. Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sen- tence understanding through inference.",
      "GLUE: A multi-task benchmark and analysis plat- form for natural language understanding. In Inter- national Conference on Learning Representations. Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sen- tence understanding through inference. In Proceed- ings of the 2018 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google\u2019s neural machine translation system: Bridging the gap between hu- man and machine translation. arXiv preprint arXiv:1609.08144. Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. 2020. Deebert: Dynamic early exit- ing for accelerating bert inference.",
      "arXiv preprint arXiv:1609.08144. Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. 2020. Deebert: Dynamic early exit- ing for accelerating bert inference. arXiv preprint arXiv:2004.12993. Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou. 2020. Bert-of-theseus: Compress- ing bert by progressive module replacing. arXiv preprint arXiv:2002.02925. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car- bonell, Ruslan Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretrain- ing for language understanding. arXiv preprint arXiv:1906.08237. Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, and Cho-Jui Hsieh.",
      "Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, and Cho-Jui Hsieh. 2019. Large batch optimization for deep learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962. Sergey Zagoruyko and Nikos Komodakis. 2016. Pay- ing more attention to attention: Improving the per- formance of convolutional neural networks via atten- tion transfer. arXiv preprint arXiv:1612.03928. Yiren Zhou, Seyed-Mohsen Moosavi-Dezfooli, Ngai- Man Cheung, and Pascal Frossard. 2018. Adap- tive quantization for deep neural network. In Thirty- Second AAAI Conference on Arti\ufb01cial Intelligence. Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut- dinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler.",
      "In Thirty- Second AAAI Conference on Arti\ufb01cial Intelligence. Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut- dinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE inter- national conference on computer vision, pages 19\u2013 27."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1909.11687.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":8208,
  "avg_doclen":174.6382978723,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1909.11687.pdf"
    }
  }
}