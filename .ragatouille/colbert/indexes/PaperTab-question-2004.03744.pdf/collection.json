[
  "e-SNLI-VE: Corrected Visual-Textual Entailment with Natural Language Explanations Virginie Do1, Oana-Maria Camburu1,2, Zeynep Akata3, and Thomas Lukasiewicz1,2 1University of Oxford, 2Alan Turing Institute, London, 3University of Tuebingen, virginiedo@gmail.com, firstname.lastname@cs.ox.ac.uk, zeynep.akata@uni-tuebingen.de Abstract The recently proposed SNLI-VE corpus for recognising visual-textual entailment is a large, real-world dataset for \ufb01ne-grained multimodal reasoning. However, the automatic way in which SNLI-VE has been assembled (via combin- ing parts of two related datasets) gives rise to a large num- ber of errors in the labels of this corpus. In this paper, we \ufb01rst present a data collection effort to correct the class with the highest error rate in SNLI-VE. Secondly, we re-evaluate an existing model on the corrected corpus, which we call SNLI-VE-2.01, and provide a quantitative comparison with its performance on the non-corrected corpus.",
  "Secondly, we re-evaluate an existing model on the corrected corpus, which we call SNLI-VE-2.01, and provide a quantitative comparison with its performance on the non-corrected corpus. Thirdly, we introduce e-SNLI-VE1, which appends human-written natu- ral language explanations to SNLI-VE-2.0. Finally, we train models that learn from these explanations at training time, and output such explanations at testing time. 1. Introduction Inspired by textual entailment [2], Xie et al. [13] intro- duced the visual-textual entailment (VTE)2 task, which con- siders semantic entailment between a premise image and a textual hypothesis. Semantic entailment consists in deter- mining if the hypothesis can be concluded from the premise, and assigning to each pair of (premise image, textual hy- pothesis) a label among entailment, neutral, and contradic- tion.",
  "Semantic entailment consists in deter- mining if the hypothesis can be concluded from the premise, and assigning to each pair of (premise image, textual hy- pothesis) a label among entailment, neutral, and contradic- tion. In Figure 1, the label for the \ufb01rst image-sentence pair is entailment, because the hypothesis states that \u201ca bunch of people display different \ufb02ags\u201d, which can be clearly de- rived from the image. On the contrary, the second image- IEEE CVPR Workshop on Fair, Data Ef\ufb01cient and Trusted Computer Vision, 2020 1We updated the dataset in our ICCV 2021 paper [6]. It is available at https://github.com/maximek3/e-ViL. sentence pair is labelled as contradiction, because the hy- pothesis stating that \u201cpeople [are] running a marathon\u201d contradicts the image with static people. Xie et al. also propose the SNLI-VE dataset as the \ufb01rst dataset for VTE.",
  "sentence pair is labelled as contradiction, because the hy- pothesis stating that \u201cpeople [are] running a marathon\u201d contradicts the image with static people. Xie et al. also propose the SNLI-VE dataset as the \ufb01rst dataset for VTE. SNLI-VE is built from the textual entail- ment SNLI dataset [2] by replacing textual premises with the Flickr30k images that they originally described [15]. However, images contain more information than their de- scriptions, which may entail or contradict the textual hy- potheses (see Figure 1). As a result, the neutral class in SNLI-VE has substantial labelling errors. Vu et al. [12] es- timated \u223c31% errors in this class, and \u223c1% for the contra- diction and entailment classes. In this work, we \ufb01rst focus on decreasing the error in the neutral class by collecting new labels for the neutral pairs in the validation and test sets of SNLI-VE, using Amazon Mechanical Turk (MTurk).",
  "In this work, we \ufb01rst focus on decreasing the error in the neutral class by collecting new labels for the neutral pairs in the validation and test sets of SNLI-VE, using Amazon Mechanical Turk (MTurk). To ensure high quality anno- tations, we used a series of quality control measures, such as in-browser checks, inserting trusted examples, and col- lecting three annotations per instance. Secondly, we re- evaluate current image-text understanding systems, such as the bottom-up top-down attention network (BUTD) [1] on VTE using our corrected dataset, which we call SNLI-VE- 2.0. Thirdly, we introduce the e-SNLI-VE corpus, which we form by appending human-written natural language expla- nations to SNLI-VE-2.0. These explanations were collected in e-SNLI [3] to support textual entailment for SNLI. For the same reasons as above, we re-annotate the explanations for the neutral pairs in the validation and test sets, while keeping the explanations from e-SNLI for all the rest.",
  "These explanations were collected in e-SNLI [3] to support textual entailment for SNLI. For the same reasons as above, we re-annotate the explanations for the neutral pairs in the validation and test sets, while keeping the explanations from e-SNLI for all the rest. Fi- nally, we extend a current VTE model with the capacity of learning from these explanations at training time and out- 2Xie et al. [13] introduced the VTE task under the name of \u201cvisual en- tailment\u201d, which could imply recognizing entailment between images only. This paper prefers to follow Suzuki et al. [11] and call it \u201cvisual-textual en- tailment\u201d instead, as it involves reasoning on image-sentence pairs. 1 arXiv:2004.03744v3  [cs.CL]  19 Aug 2021",
  "Figure 1. Examples from SNLI-VE-2.0. (a) In red, the neutral la- bel from SNLI-VE is wrong, since the picture clearly shows that the crowd is outdoors. We corrected it to entailment in SNLI- VE-2.0. (b) In green, an ambiguous instance. There is indeed an American \ufb02ag in the background but it is very hard to see, hence the ambiguity between neutral and entailment, and even contra- diction if one cannot spot it. Further, it is not clear whether \u201cthey\u201d implies the whole group or the people visible in the image. putting an explanation for each predicted label at testing time. 2. SNLI-VE-2.0 The goal of VTE is to determine if a textual hypothesis Htext can be concluded, given the information in a premise image Pimage [13]. There are three possible labels: \u2022 Entailment: if there is enough evidence in Pimage to conclude that Htext is true. \u2022 Contradiction: if there is enough evidence in Pimage to conclude that Htext is false.",
  "There are three possible labels: \u2022 Entailment: if there is enough evidence in Pimage to conclude that Htext is true. \u2022 Contradiction: if there is enough evidence in Pimage to conclude that Htext is false. \u2022 Neutral: if neither of the earlier two are true. The SNLI-VE dataset proposed by Xie et al. [13] is the combination of Flickr30k, a popular image dataset for im- age captioning [15] and SNLI, an in\ufb02uential dataset for nat- ural language inference [2]. Textual premises from SNLI are replaced with images from Flickr30k, which is possible, as these premises were originally collected as captions of these images (see Figure 1). However, in practice, a sensible proportion of labels are wrong due to the additional information contained in im- ages. This mostly affects neutral pairs, since images may contain the necessary information to ground a hypothesis for which a simple premise caption was not suf\ufb01cient. An example is shown in Figure 1. Vu et al.",
  "This mostly affects neutral pairs, since images may contain the necessary information to ground a hypothesis for which a simple premise caption was not suf\ufb01cient. An example is shown in Figure 1. Vu et al. [12] report that the label is wrong for \u223c31% of neutral examples, based on a random subset of 171 neutral points from the test set. We also annotated 150 random neutral examples from the test set and found a similar percentage of 30.6% errors.3 3Our annotations are available at https://github.com/ 2.1. Re-annotation details In this work, we only collect new labels for the neu- tral pairs in the validation and test sets of SNLI-VE. While the procedure of re-annotation is generic, we limit our re- annotation to these splits as a \ufb01rst step to verify the differ- ence in performance that current models have when evalu- ated on the corrected test set as well as the effect of model selection on the corrected validation set. We leave for future work re-annotation of the training set, which would likely lead to training better VTE models.",
  "We leave for future work re-annotation of the training set, which would likely lead to training better VTE models. We also chose not to re- annotate entailment and contradiction classes, as their error rates are much lower (<1% as reported by Vu et al. [12]). The main question that we want our dataset to answer is: \u201cWhat is the relationship between the image premise and the sentence hypothesis?\u201d. We provide workers with the de\ufb01nitions of entailment, neutral, and contradiction for image-sentence pairs and one example for each label. As shown in Figure 2, for each image-sentence pair, workers are required to (a) choose a label, (b) highlight words in the sentence that led to their decision, and (c) explain their decision in a comprehensive and concise manner, using at least half of the words that they highlighted. The collected explanations will be presented in more detail in Section 3.2, as we focus here on the label correction.",
  "The collected explanations will be presented in more detail in Section 3.2, as we focus here on the label correction. We point out that it is likely that requiring an explanation at the same time as requiring a label has a positive effect on the correctness of the label, since having to justify in writing the picked label may make workers pay an increased attention. More- over, we implemented additional quality control measures for crowdsourced annotations, such as (a) collecting three annotations for every input, (b) injecting trusted annotations into the task for veri\ufb01cation [10], and (c) restricting to work- ers with at least 90% previous approval rate. Figure 2. MTurk annotation screen. (a) The label contradiction is chosen, (b) the evidence words \u201cman\u201d, \u201cviolin\u201d, and \u201ccrowd\u201d are highlighted, and (c) an explanation is written with these words. First, we noticed that some instances in SNLI-VE are virginie-do/e-SNLI-VE/tree/master/annotations/gt_ labels.csv",
  "ambiguous. We show some examples in Figure 1 and in Appendix 5.3. In order to have a better sense of this am- biguity, three authors of this paper independently annotated 100 random examples. All three authors agreed on 54% of the examples, exactly two authors agreed on 45%, and there was only one example on which all three authors disagreed. We identi\ufb01ed the following three major sources of ambigu- ity: \u2022 mapping an emotion in the hypothesis to a facial ex- pression in the image premise, e.g., \u201cpeople enjoy talk- ing\u201d, \u201cangry people\u201d, \u201csad woman\u201d. Even when the face is seen, it may be subjective to infer an emotion from a static image (see Figure 9 in Appendix 5.3). \u2022 personal taste, e.g., \u201cthe sign is ugly\u201d. \u2022 lack of consensus on terms such as \u201cmany people\u201d or \u201ccrowded\u201d. To account for the ambiguity that the neutral labels seem to present, we considered that an image-sentence pair is too ambiguous and not suitable for a well-de\ufb01ned visual-textual entailment task when three different labels were assigned by the three workers.",
  "To account for the ambiguity that the neutral labels seem to present, we considered that an image-sentence pair is too ambiguous and not suitable for a well-de\ufb01ned visual-textual entailment task when three different labels were assigned by the three workers. Hence, we removed these examples from the validation (5.2%) and test (5.5%) sets. To ensure that our workers are correctly performing the task, we randomly inserted trusted pairs, i.e., pairs among the 54% on which all three authors agreed on the label. For each set of 10 pairs presented to a worker, one trusted pair was introduced at a random location, so that the worker, while being told that there is such a test pair, cannot \ufb01g- ure out which one it is. Via an in-browser check, we only allow workers to submit their answers for each set of 10 in- stances only if the trusted pair was correctly labelled. Other in-browser checks were done for the collection of explana- tions, as we will describe in Section 3.2. More details about the participants and design of the Mechanical Turk task can be found in Appendix 5.2.",
  "Other in-browser checks were done for the collection of explana- tions, as we will describe in Section 3.2. More details about the participants and design of the Mechanical Turk task can be found in Appendix 5.2. After collecting new labels for the neutral instances in the validation and testing sets, we randomly select and an- notate 150 instances from the validation set that were neu- tral in SNLI-VE. Based on this sample, the error rate went down from 31% to 12% in SNLI-VE-2.0. Looking at the 18 instances where we disagreed with the label assigned by MTurk workers, we noticed that 12 were due to ambiguity in the examples, and 6 were due to workers\u2019 errors. Fur- ther investigation into potentially eliminating ambiguous in- stances would likely be bene\ufb01cial. However, we leave it as future work, and we proceed in this work with using our corrected labels, since our error rate is signi\ufb01cantly lower than that of the original SNLI-VE.",
  "However, we leave it as future work, and we proceed in this work with using our corrected labels, since our error rate is signi\ufb01cantly lower than that of the original SNLI-VE. Finally, we note that only about 62% of the originally neutral pairs remain neutral, while 21% become contra- diction and 17% entailment pairs. Therefore, we are now facing an imbalance between the neutral, entailment, and contradiction instances in the validation and testing sets of SNLI-VE-2.0. The neutral class becomes underrepresented and the label distributions in the corrected validation and testing sets both become E / N / C: 39% / 20% / 41%. To account for this, we compute the balanced accuracy, i.e., the average of the three accuracies on each class. 2.2. Re-evaluation of Visual-Textual Entailment Since we decreased the error rate of labels in the valida- tion and test set, we are interested in the performance of a VTE model when using the corrected sets. Model. To tackle SNLI-VE, Xie et al.",
  "Re-evaluation of Visual-Textual Entailment Since we decreased the error rate of labels in the valida- tion and test set, we are interested in the performance of a VTE model when using the corrected sets. Model. To tackle SNLI-VE, Xie et al. [13] used EVE (for \u201cExplainable Visual Entailment\u201d), a modi\ufb01ed version of the BUTD architecture, the winner of the Visual Question An- swering (VQA) challenge in 2017 [1]. Since the EVE im- plementation is not available at the time of this work, we used the original BUTD architecture4, with the same hyper- parameters as reported in [13]. BUTD contains an image processing module and a text processing module. The image processing module encodes each image region proposed by FasterRCNN [9] into a fea- ture vector using a bottom-up attention mechanism. In the text processing module, the text hypothesis is encoded into a \ufb01xed-length vector, which is the last output of a recurrent neural network with 512-GRU units [4].",
  "In the text processing module, the text hypothesis is encoded into a \ufb01xed-length vector, which is the last output of a recurrent neural network with 512-GRU units [4]. To input each to- ken into the recurrent network, we use the pretrained GloVe vectors [8]. Finally, a top-down attention mechanism is used between the hypothesis vector and each of the im- age region vectors to obtain an attention weight for each region. The weighted sum of these image region vectors is then fused with the text hypothesis vector. The multimodal fusion is fed to a multilayer percetron (MLP) with tanh ac- tivations and a \ufb01nal softmax layer to classify the image- sentence relation as entailment, contradiction, or neutral. We use the original training set from SNLI-VE. To see the impact of correcting the validation and test sets, we do the following three experiments: 1. model selection as well as testing are done on the orig- inal uncorrected SNLI-VE.",
  "We use the original training set from SNLI-VE. To see the impact of correcting the validation and test sets, we do the following three experiments: 1. model selection as well as testing are done on the orig- inal uncorrected SNLI-VE. 2. model selection is done on the uncorrected SNLI-VE validation set, while testing is done on the corrected SNLI-VE-2.0 test set. 3. model selection as well as testing are done on the cor- rected SNLI-VE-2.0. Models are trained with cross-entropy loss optimized by the Adam optimizer [7] with batch size 64. The maximum 4Using the implementation from https://github.com/ claudiogreco/coling18-gte.",
  "BUTD val-original val-corrected test-original 73.02% N/A test-corrected 73.18% 72.52% Table 1. Accuracies obtained with BUTD on SNLI-VE (val- original, test-original) and SNLI-VE-2.0 (val-corrected, test- corrected). number of training epochs is set to 100, with early stopping when no improvement is observed on validation accuracy for 3 epochs. The \ufb01nal model checkpoint selected for test- ing is the one with the highest validation accuracy. Results. The results of the three experiments enumerated above are reported in Table 1. Surprisingly, we obtained an accuracy of 73.02% on SNLI-VE using BUTD, which is better than the 71.16% reported by Xie et al. [13] for the EVE system which meant to be an improvement over BUTD. It is also better than their reproduction of BUTD, which gave 68.90%.",
  "[13] for the EVE system which meant to be an improvement over BUTD. It is also better than their reproduction of BUTD, which gave 68.90%. The same BUTD model that achieves 73.02% on the un- corrected SNLI-VE test set, achieves 73.18% balanced ac- curacy when tested on the corrected test set from SNLI-VE- 2.0. Hence, for this model, we do not notice a signi\ufb01cant difference in performance. This could be due to random- ness. Finally, when we run the training loop again, this time doing the model selection on the corrected validation set from SNLI-VE-2.0, we obtain a slightly worse performance of 72.52%, although the difference is not clearly signi\ufb01cant. Finally, we recall that the training set has not been re- annotated, and hence approximately 31% image-sentence pairs are wrongly labelled as neutral, which likely affects the performance of the model. 3.",
  "Finally, we recall that the training set has not been re- annotated, and hence approximately 31% image-sentence pairs are wrongly labelled as neutral, which likely affects the performance of the model. 3. Visual-Textual Entailment with Natural Language Explanations In this work, we also introduce e-SNLI-VE, a dataset combining SNLI-VE-2.0 with human-written explanations from e-SNLI [3], which were originally collected to sup- port textual entailment. We replace the explanations for the neutral pairs in the validation and test sets with new ones collected at the same time as the new labels. We extend a current VTE model with an explanation module able to learn from these explanations at training time and generate an explanation for each predicted label at testing time. 3.1. e-SNLI-VE e-SNLI [3] is an extension of the SNLI corpus with human-annotated natural language explanations for the ground-truth labels. The authors use the explanations to train models to also generate natural language justi\ufb01cations for their predictions.",
  "The authors use the explanations to train models to also generate natural language justi\ufb01cations for their predictions. They collected one explanation for each instance in the training set of SNLI and three explana- tions for each instance in the validation and testing sets. We randomly selected 100 image-sentence pairs in the validation set of SNLI-VE and their corresponding expla- nations in e-SNLI and examined how relevant these expla- nations are for the VTE task. More precisely, we say that an explanation is relevant if it brings information that justi\ufb01es the relationship between the image and the sentence. We restricted the count to correctly labelled inputs and found that 57% explanations were relevant. For example, the ex- planation for entailment in Figure 3 (\u201cCooking in his apart- ment is cooking\u201d) was counted as irrelevant in our statistics, because it would not be the best explanation for an image- sentence pair, even though it is coherent with the textual pair. We investigate whether these explanations improve a VTE model when enhanced with a component that can pro- cess explanations at train time and output them at test time.",
  "We investigate whether these explanations improve a VTE model when enhanced with a component that can pro- cess explanations at train time and output them at test time. To form e-SNLI-VE, we append to SNLI-VE-2.0 the explanations from e-SNLI for all except the neutral pairs in the validation and test sets of SNLI-VE, which we re- place with newly crowdsourced explanations collected at the same time as the labels for these splits (see Figure 3). Statistics of e-SNLI-VE are shown in Appendix 5.1, Table 3. 3.2. Collecting Explanations As mentioned before, in order to submit the annotation of an image-sentence pair, three steps must be completed: workers must choose a label, highlight words in the hypoth- esis, and use at least half of the highlighted words to write an explanation for their decision. The last two steps thus follow the quality control of crowd-sourced explanations in- troduced by Camburu et al. [3]. We also ensured that work- ers do not simply use a copy of the given hypothesis as ex- planation.",
  "The last two steps thus follow the quality control of crowd-sourced explanations in- troduced by Camburu et al. [3]. We also ensured that work- ers do not simply use a copy of the given hypothesis as ex- planation. We ensured all the above via in-browser checks before workers\u2019 submission. An example of collected ex- planations is given in Figure 3. To check the success of our crowdsourcing, we manu- ally assessed the relevance of explanations among a ran- dom subset of 100 examples. A marking scale between 0 and 1 was used, assigning a score of k/n when k required attributes were given in an explanation out of n. We report an 83.5% relevance of explanations from workers. We note that, since our explanations are VTE-speci\ufb01c, they were phrased differently from the ones in e-SNLI, with more speci\ufb01c mentions to the images (e.g., \u201cThere is no labcoat in the picture, just a man wearing a blue shirt.\u201d, \u201cThere are no apples or oranges shown in the picture, only bananas.\u201d).",
  "Therefore, it would likely be bene\ufb01cial to col- lect new explanations for all SNLI-VE-2.0 (not only for the neutral pairs in the validation and test sets) such that mod- els can learn to output convincing explanations for the task at hand. However, we leave this as future work, and we",
  "show in this work the results that one obtains when using the explanations from e-SNLI-VE. Figure 3. Two image-sentence pairs from e-SNLI-VE with (a) at the top, an uninformative explanation from e-SNLI, (b) at the bot- tom, an explanation collected from our crowdsourcing. We only collected new explanations for the neutral class (along with new labels). The SNLI premise is not included in e-SNLI-VE. 3.3. VTE Models with Natural Language Explana- tions This section presents two VTE models that generate nat- ural language explanations for their own decisions. We name them PAE-BUTD-VE and ETP-BUTD-VE, where PAE (resp. ETP) is for PREDICTANDEXPLAIN (resp. EX- PLAINTHENPREDICT), two models with similar principles introduced by Camburu et al. [3]. The \ufb01rst system learns to generate an explanation conditioned on the image premise, textual hypothesis, and predicted label.",
  "EX- PLAINTHENPREDICT), two models with similar principles introduced by Camburu et al. [3]. The \ufb01rst system learns to generate an explanation conditioned on the image premise, textual hypothesis, and predicted label. In contrast, the sec- ond system learns to \ufb01rst generate an explanation condi- tioned on the image premise and textual hypothesis, and subsequently makes a prediction solely based on the expla- nation. 3.3.1 Predict and Explain PAE-BUTD-VE is a system for solving VTE and generat- ing natural language explanations for the predicted labels. The explanations are conditioned on the image premise, the text hypothesis, and the predicted label (ground-truth label at train time), as shown in Figure 4. Figure 4. PAE-BUTD-VE. The generation of explanation is con- ditioned on the image premise, textual hypothesis, and predicted label. Model.",
  "Figure 4. PAE-BUTD-VE. The generation of explanation is con- ditioned on the image premise, textual hypothesis, and predicted label. Model. As described in Section 2.2, in the BUTD model, the hypothesis vector and the image vector were fused in a \ufb01xed-size feature vector f. The vector f was then given as input to an MLP which outputs a probability distribution over the three labels. In PAE-BUTD-VE, in addition to the classi\ufb01cation layer, we add a 512-LSTM [5] decoder to gen- erate an explanation. The decoder takes the feature vector f as initial state. Following Camburu et al. [3], we prepend the label as a token at the beginning of the explanation to condition the explanation on the label. The ground truth la- bel is provided at training time, whereas the predicted label is given at test time. At test time, we use beam search with a beam width of 3 to decode explanations. For memory and time reduction, we replaced words that appeared less than 15 times among explanations with \u201c#UNK#\u201d.",
  "At test time, we use beam search with a beam width of 3 to decode explanations. For memory and time reduction, we replaced words that appeared less than 15 times among explanations with \u201c#UNK#\u201d. This strategy reduces the out- put vocabulary size to approximately 8.6k words. Loss. The training loss is a weighted combination of the classi\ufb01cation loss and the explanation loss, both com- puted using softmax cross entropy: L = \u03b1Llabel + (1 \u2212 \u03b1)Lexplanation ; \u03b1 \u2208[0, 1]. Model selection. In this experiment, we are \ufb01rst inter- ested in examining if a neural network can generate ex- planations at no cost for label accuracy. Therefore, only balanced accuracy on label is used for the model selection criterion. However, future work can investigate other selec- tion criteria involving a combination between the label and explanation performances. We performed hyperparameter search on \u03b1, considering values between 0.2 and 0.8 with a step of 0.2.",
  "However, future work can investigate other selec- tion criteria involving a combination between the label and explanation performances. We performed hyperparameter search on \u03b1, considering values between 0.2 and 0.8 with a step of 0.2. We found \u03b1 = 0.4 to produce the best validation balanced accuracy of 72.81%, while BUTD trained without explanations yielded a similar 72.58% validation balanced accuracy. Results. As summarised in Table 2, we obtain a test bal- anced accuracy for PAE-BUTD-VE of 73%, while the same model trained without explanations obtains 72.52%. This is encouraging, since it shows that one can obtain addi- tional natural language explanations without sacri\ufb01cing per- formance (and eventually even improving the label perfor- mance, however, future work is needed to conclude whether the difference 0.48% improvement in performance is statis- tically signi\ufb01cant). Camburu et al. [3] mentioned that the BLEU score was not an appropriate measure for the quality of explanations and suggested human evaluation instead.",
  "Camburu et al. [3] mentioned that the BLEU score was not an appropriate measure for the quality of explanations and suggested human evaluation instead. We therefore man- ually scored the relevance of 100 explanations that were generated when the model predicted correct labels. We found that only 20% of explanations were relevant. We highlight that the relevance of explanations is in terms of whether the explanation re\ufb02ects ground-truth reasons sup- porting the correct label. This is not to be confused with whether an explanation is correctly illustrating the inner",
  "Label Expl. Expl. (Camburu et al.) PAE-BUTD-VE 73% 20% 34.68% ETP-BUTD-VE 69.40% 35% 49.8% Table 2. Label balanced accuracies and explanation relevance rates of our two explanatory systems on e-SNLI-VE. Comparison with their counterparts in e-SNLI [3]. Without the explanation compo- nent, the balanced accuracy on SNLI-VE-2.0 is 72.52% working of the model, which is left as future work. It is also important to note that on a similar experimental set- ting, Camburu et al. report as low as 34.68% correct expla- nations, training with explanations that were actually col- lected for their task. Lastly, the model selection criterion at validation time was the prediction balanced accuracy, which may contribute to the low quality of explanations. While we show that adding an explanation module does not harm pre- diction performance, more work is necessary to get models that output trustable explanations.",
  "Lastly, the model selection criterion at validation time was the prediction balanced accuracy, which may contribute to the low quality of explanations. While we show that adding an explanation module does not harm pre- diction performance, more work is necessary to get models that output trustable explanations. 3.3.2 Explain Then Predict When assigning a label, an explanation is naturally part of the decision-making process. This motivates the design of a system that explains itself before deciding on a label, called ETP-BUTD-VE. For this system, a \ufb01rst neural net- work is trained to generate an explanation given an image- sentence input. Separately, a second neural network, called EXPLTOLABEL-VE, is trained to predict a label from an explanation (see Figure 5). Model. For the \ufb01rst network, we set \u03b1 = 0 in the train- ing loss of the PAE-BUTD-VE model to obtain a system that only learns to generate an explanation from the image- sentence input, without label prediction. Hence, in this set- ting, no label is prepended before the explanation.",
  "Hence, in this set- ting, no label is prepended before the explanation. For the EXPLTOLABEL-VE model, we use a 512-LSTM followed by an MLP with three 512-layers and ReLU acti- vation, and softmax activation to classify the explanation between entailment, contradiction, and neutral. Figure 5. Architecture of ETP-BUTD-VE. Firstly, an explana- tion is generated, secondly the label is predicted from the expla- nation. The two models (in separate dashed rectangles) are not trained jointly. Model selection. For EXPLTOLABEL-VE, the best model is selected on balanced accuracy at validation time. For ETP-BUTD-VE, perplexity is used to select the best model parameters at validation time. It is computed be- tween the explanations produced by the LSTM and ground truth explanations from the validation set. Results. When we train EXPLTOLABEL-VE on e-SNLI- VE, we obtain a balanced accuracy of 90.55% on the test set.",
  "It is computed be- tween the explanations produced by the LSTM and ground truth explanations from the validation set. Results. When we train EXPLTOLABEL-VE on e-SNLI- VE, we obtain a balanced accuracy of 90.55% on the test set. As reported in Table 2, the overall PAE-BUTD-VE sys- tem achieves 69.40% balanced accuracy on the test set of e- SNLI-VE, which is a 3% decrease from the non-explanatory BUTD counterpart (72.52%). However, by setting \u03b1 to zero and selecting the model that gives the best perplexity per word at validation, the quality of explanation signi\ufb01cantly increased, with 35% relevance, based on manual evaluation. Thus, in our model, generating better explanations involves a small sacri\ufb01ce in label prediction accuracy, implying a trade-off between explanation generation and accuracy. We note that there is room for improvement in our expla- nation generation method. For example, one can implement an attention mechanism similar to Xu et al. [14], so that each generated word relates to a relevant part of the multi- modal feature representation.",
  "We note that there is room for improvement in our expla- nation generation method. For example, one can implement an attention mechanism similar to Xu et al. [14], so that each generated word relates to a relevant part of the multi- modal feature representation. 3.3.3 Qualitative Analysis of Generated Explanations We complement our quantitative results with a qualitative analysis of the explanations generated by our enhanced VTE systems. In Figures 6 and 7, we present examples of the predicted labels and generated explanations. Figure 6 shows an example where the ETP-BUTD-VE model produces both a correct label and a relevant explana- tion. The label is contradiction, because in the image, the students are playing with a soccer ball and not a basketball, thus contradicting the text hypothesis. Given the composi- tion of the generated sentence (\u201cStudents cannot be playing soccer and baseball at the same time.\u201d), EXPLTOLABEL- VE was able to detect a contradiction in the image-sentence input. In comparison, the explanation from e-SNLI-VE is not correct, even if it was valid for e-SNLI when the text premise was given.",
  "In comparison, the explanation from e-SNLI-VE is not correct, even if it was valid for e-SNLI when the text premise was given. This emphasizes the dif\ufb01culty that we are facing with generating proper explanations when train- ing on a noisy dataset. Even when the generated explanations are irrelevant, we noticed that they are on-topic and that most of the time the mistakes come from repetitions of certain sub-phrases. For example, in Figure 7, PAE-BUTD-VE predicts the label neutral, which is correct, but the explanation contains an erroneous repetition of the n-gram \u201care in a car\u201d. How- ever, it appears that the system learns to generate a sentence in the form \u201cJust because ...doesn\u2019t mean ...\u201d, which is",
  "Figure 6. Both systems PAE-BUTD-VE and ETP-BUTD-VE predict the correct label, but only ETP-BUTD-VE generates a relevant explanation. Figure 7. Both systems PAE-BUTD-VE and ETP-BUTD-VE predict the correct label, but generate irrelevant explanations. frequently found for the justi\ufb01cation of neutral pairs in the training set. The explanation generated by ETP-BUTD- VE adopts the same structure, and the EXPLTOLABEL-VE component correctly classi\ufb01es the instance as neutral. How- ever, even if the explanation is semantically correct, it is not relevant for the input and fails to explain the classi\ufb01cation. 4. Conclusion In this paper, we \ufb01rst presented SNLI-VE-2.0, which corrects the neutral instances in the validation and test sets of SNLI-VE. Secondly, we re-evaluated an existing model on the corrected sets in order to update the estimate of its performance on this task.",
  "Secondly, we re-evaluated an existing model on the corrected sets in order to update the estimate of its performance on this task. Thirdly, we introduced e-SNLI- VE, a dataset which extends SNLI-VE-2.0 with natural lan- guage explanations. Finally, we trained two types of models that learn from these explanations at training time, and out- put such explanations at test time, as a stepping stone in ex- plainable arti\ufb01cial intelligence. Our work is a jumping-off point for both the identi\ufb01cation and correction of SNLI-VE, as well as in the extension to explainable VTE. We hope that the community will build on our \ufb01ndings to create more ro- bust as well as explainable multimodal systems. Acknowledgements.",
  "We hope that the community will build on our \ufb01ndings to create more ro- bust as well as explainable multimodal systems. Acknowledgements. This work was supported by the Oxford Internet Institute, a JP Morgan PhD Fellow- ship 2019-2020, an Oxford-DeepMind Graduate Scholar- ship, the Alan Turing Institute under the EPSRC grant EP/N510129/1, and the AXA Research Fund, as well as DFG-EXC-Nummer 2064/1-Projektnummer 390727645 and the ERC under the Horizon 2020 program (grant agree- ment No. 853489). References [1] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. Proc. CVPR, pages 6077\u20136086, 2018. 1, 3 [2] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learn- ing natural language inference. In Proc.",
  "Proc. CVPR, pages 6077\u20136086, 2018. 1, 3 [2] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learn- ing natural language inference. In Proc. EMNLP, 2015. 1, 2 [3] Oana-Maria Camburu, Tim Rockt\u00a8aschel, Thomas Lukasiewicz, and Phil Blunsom. e-SNLI: Natural lan- guage inference with natural language explanations. In Advances in Neural Information Processing Systems, pages 9539\u20139549, 2018. 1, 4, 5, 6 [4] Kyunghyun Cho, Bart Van Merri\u00a8enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.",
  "Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. 3 [5] Sepp Hochreiter and J\u00a8urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997. 5 [6] Maxime Kayser, Oana-Maria Camburu, Leonard Salewski, Cornelius Emde, Virginie Do, Zeynep Akata, and Thomas Lukasiewicz. e-vil: A dataset and benchmark for natural lan- guage explanations in vision-language tasks. arXiv preprint arXiv:2105.03761, 2021. 1 [7] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 3 [8] Jeffrey Pennington, Richard Socher, and Christopher Man- ning. Glove: Global vectors for word representation. In Proc. EMNLP, 2014.",
  "arXiv preprint arXiv:1412.6980, 2014. 3 [8] Jeffrey Pennington, Richard Socher, and Christopher Man- ning. Glove: Global vectors for word representation. In Proc. EMNLP, 2014. 3 [9] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with re- gion proposal networks. In Advances in Neural Information Processing Systems, pages 91\u201399, 2015. 3 [10] Alexander Sorokin and David Forsyth. Utility data annota- tion with Amazon Mechanical Turk. In Proc. CVPR Work- shops, pages 1\u20138, 2008. 2 [11] Riko Suzuki, Hitomi Yanaka, Masashi Yoshikawa, Koji Mi- neshima, and Daisuke Bekki. Multimodal logical infer- ence system for visual-textual entailment. arXiv preprint arXiv:1906.03952, 2019.",
  "Multimodal logical infer- ence system for visual-textual entailment. arXiv preprint arXiv:1906.03952, 2019. 1 [12] Hoa Trong Vu, Claudio Greco, Aliia Erofeeva, Somayeh Ja- faritazehjan, Guido Linders, Marc Tanti, Alberto Testoni, Raffaella Bernardi, and Albert Gatt. Grounded textual en- tailment. arXiv preprint arXiv:1806.05645, 2018. 1, 2",
  "[13] Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. Visual entailment: A novel task for \ufb01ne-grained image understand- ing. arXiv preprint arXiv:1901.06706, 2019. 1, 2, 3, 4 [14] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. Show, attend and tell: Neural im- age caption generation with visual attention. arXiv preprint arXiv:1502.03044, 2015. 6 [15] M. H. Peter Young, Alice Lai, and Julia Hockenmaier. From image descriptions to visual denotations: New simi- larity metrics for semantic inference over event descriptions. TACL, 2:67\u201378, 2014. 1, 2 5. Appendix 5.1.",
  "From image descriptions to visual denotations: New simi- larity metrics for semantic inference over event descriptions. TACL, 2:67\u201378, 2014. 1, 2 5. Appendix 5.1. Statistics of e-SNLI-VE e-SNLI-VE is the combination of SNLI-VE-2.0 with ex- planations from either e-SNLI or our crowdsourced anno- tations where applicable. The statistics of e-SNLI-VE are shown in Table 3. Training Validation Testing #Images 29,783 1,000 1,000 #Entailment 176,932 6,913 6,903 #Neutral 176,045 3,453 3,537 #Contradiction 176,550 7,181 7,134 #Total #Explanations from e-SNLI 529,527 11,888 11,898 #Explanations from None 5,659 5,676 our data collection Vocabulary size5 41,230 8,963 9,197 Table 3.",
  "Summary of e-SNLI-VE (= SNLI-VE-2.0 + explana- tions). Image-sentence pairs labelled as neutral in the training set have not been corrected. 5.2. Details of the Mechanical Turk Task We used Amazon Mechanical Turk (MTurk) to collect new labels and explanations for SNLI-VE. 2,060 workers participated in the annotation effort, with an average of 1.98 assignments per worker and a standard deviation of 5.54. We required the workers to have a previous approval rate above 90%. No restriction was put on the workers\u2019 location. Each assignment consisted of a set of 10 image-sentence pairs. For each pair, the participant was asked to (a) choose a label, (b) highlight words in the sentence that led to their decision, and (c) explain their decision in a comprehensive and concise manner, using a subset of the words that they highlighted. The instructions are shown in Figure 8. Work- ers were also guided with three annotated examples, one for each label.",
  "The instructions are shown in Figure 8. Work- ers were also guided with three annotated examples, one for each label. For each assignment of 10 questions, one trusted annota- tion with gold standard label was inserted at a random posi- tion, as a measure to control the quality of label annotation. 5Including text hypotheses and explanations. Figure 8. Instructions given to workers on Mechanical Turk Each assignment was completed by three different workers. An example of question is shown in Figure 2 in the core paper. 5.3. Ambiguous Examples from SNLI-VE Some examples in SNLI-VE were ambiguous and could \ufb01nd correct justi\ufb01cations for incompatible labels, as shown in Figures 9, 10, and 11. Figure 9. Ambiguous SNLI-VE instance. Some may argue that the woman\u2019s face betrays sadness, but the image is not quite clear. Secondly, even with better resolution, facial expression may not be a strong enough evidence to support the hypothesis about the woman\u2019s emotional state.",
  "Figure 10. Ambiguous SNLI-VE instance. The lack of consensus is on whether the man is \u201cleering\u201d at the woman. While it is likely the case, this interpretation in favour of entailment is subjective, and a cautious annotator would prefer to label the instance as neu- tral. Figure 11. Ambiguous SNLI-VE instance. Some may argue that it is impossible to certify from the image that the children are kinder- garten students, and label the instance as neutral. On the other hand, the furniture may be considered as typical of kindergarten, which would be suf\ufb01cient evidence for entailment."
]