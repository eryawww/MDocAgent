{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Towards Scalable Multi-Domain Conversational Agents: The Schema-Guided Dialogue Dataset Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, Pranav Khaitan Google Research, Mountain View, California, USA {abhirast, xiaoxuez, srinivasksun, raghavgupta, pranavkhaitan}@google.com Abstract Virtual assistants such as Google Assistant, Alexa and Siri provide a conversational interface to a large number of ser- vices and APIs spanning multiple domains. Such systems need to support an ever-increasing number of services with possibly overlapping functionality. Furthermore, some of these services have little to no training data available. Existing public datasets for task-oriented dialogue do not suf\ufb01ciently capture these challenges since they cover few domains and assume a single static ontology per domain. In this work, we introduce the the Schema-Guided Dialogue (SGD) dataset, containing over 16k multi-domain conversations spanning 16 domains. Our dataset exceeds the existing task-oriented dia- logue corpora in scale, while also highlighting the challenges associated with building large-scale virtual assistants.",
      "In this work, we introduce the the Schema-Guided Dialogue (SGD) dataset, containing over 16k multi-domain conversations spanning 16 domains. Our dataset exceeds the existing task-oriented dia- logue corpora in scale, while also highlighting the challenges associated with building large-scale virtual assistants. It pro- vides a challenging testbed for a number of tasks including language understanding, slot \ufb01lling, dialogue state tracking and response generation. Along the same lines, we present a schema-guided paradigm for task-oriented dialogue, in which predictions are made over a dynamic set of intents and slots, provided as input, using their natural language descriptions. This allows a single dialogue system to easily support a large number of services and facilitates simple integration of new services without requiring additional training data. Building upon the proposed paradigm, we release a model for dia- logue state tracking capable of zero-shot generalization to new APIs, while remaining competitive in the regular setting. 1 Introduction Virtual assistants help users accomplish tasks including but not limited to \ufb01nding \ufb02ights, booking restaurants and, more recently, navigating user interfaces, by providing a natural language interface to services and APIs on the web.",
      "1 Introduction Virtual assistants help users accomplish tasks including but not limited to \ufb01nding \ufb02ights, booking restaurants and, more recently, navigating user interfaces, by providing a natural language interface to services and APIs on the web. The re- cent popularity of conversational interfaces and the advent of frameworks like Actions on Google and Alexa Skills, which allow developers to easily add support for new services, has resulted in a major increase in the number of application do- mains and individual services that assistants need to support, following the pattern of smartphone applications. Consequently, recent work has focused on scalable dia- logue systems that can handle tasks across multiple applica- tion domains. Data-driven deep learning based approaches Copyright c\u20dd2020, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved. for multi-domain modeling have shown promise, both for end-to-end and modular systems involving dialogue state tracking and policy learning. This line of work has been facilitated by the release of multi-domain dialogue corpora such as MultiWOZ (Budzianowski et al. 2018), M2M (Shah et al.",
      "This line of work has been facilitated by the release of multi-domain dialogue corpora such as MultiWOZ (Budzianowski et al. 2018), M2M (Shah et al. 2018) and FRAMES (El Asri et al. 2017). However, existing datasets for multi-domain task-oriented dialogue do not suf\ufb01ciently capture a number of challenges that arise with scaling virtual assistants in production. These assistants need to support a large (Kim et al. 2018), con- stantly increasing number of services over a large number of domains. In comparison, existing public datasets cover few domains. Furthermore, they de\ufb01ne a single static API per do- main, whereas multiple services with overlapping function- ality, but heterogeneous interfaces, exist in the real world. To highlight these challenges, we introduce the Schema- Guided Dialogue (SGD) dataset1, which is, to the best of our knowledge, the largest public task-oriented dialogue corpus. It exceeds existing corpora in scale, with over 16000 dia- logues in the training set spanning 26 services belonging to 16 domains (more details in Table 1).",
      "It exceeds existing corpora in scale, with over 16000 dia- logues in the training set spanning 26 services belonging to 16 domains (more details in Table 1). Further, to adequately test the models\u2019 ability to generalize in zero-shot settings, the evaluation sets contain unseen services and domains. The dataset is designed to serve as an effective testbed for intent prediction, slot \ufb01lling, state tracking and language genera- tion, among other tasks in large-scale virtual assistants. We also propose the schema-guided paradigm for task- oriented dialogue, advocating building a single uni\ufb01ed di- alogue model for all services and APIs. Using a service\u2019s schema as input, the model would make predictions over this dynamic set of intents and slots present in the schema. This setting enables effective sharing of knowledge among all services, by relating semantically similar concepts across APIs, and allows the model to handle unseen services and APIs. Under the proposed paradigm, we present a novel ar- chitecture for multi-domain dialogue state tracking. By us- ing large pre-trained models like BERT (Devlin et al.",
      "Under the proposed paradigm, we present a novel ar- chitecture for multi-domain dialogue state tracking. By us- ing large pre-trained models like BERT (Devlin et al. 2019), our model can generalize to unseen services and is robust to API changes, while achieving competitive results on the original and updated MultiWOZ datasets (Eric et al. 2019). 1The dataset has been released at github.com/google-research- datasets/dstc8-schema-guided-dialogue arXiv:1909.05855v2  [cs.CL]  29 Jan 2020",
      "Metric \u2193Dataset \u2192 DSTC2 WOZ2.0 FRAMES M2M MultiWOZ SGD No. of domains 1 1 3 2 7 16 No. of dialogues 1,612 600 1,369 1,500 8,438 16,142 Total no. of turns 23,354 4,472 19,986 14,796 113,556 329,964 Avg. turns per dialogue 14.49 7.45 14.60 9.86 13.46 20.44 Avg. tokens per turn 8.54 11.24 12.60 8.24 13.13 9.75 Total unique tokens 986 2,142 12,043 1,008 23,689 30,352 No. of slots 8 4 61 13 24 214 No. of slot values 212 99 3,871 138 4,510 14,139 Table 1: Comparison of our SGD dataset to existing related datasets for task-oriented dialogue.",
      "of slots 8 4 61 13 24 214 No. of slot values 212 99 3,871 138 4,510 14,139 Table 1: Comparison of our SGD dataset to existing related datasets for task-oriented dialogue. Note that the numbers reported are for the training portions for all datasets except FRAMES, where the numbers for the complete dataset are reported. 2 Related Work Task-oriented dialogue systems have constituted an active area of research for decades. The growth of this \ufb01eld has been consistently fueled by the development of new datasets. Initial datasets were limited to one domain, such as ATIS (Hemphill, Godfrey, and Doddington 1990) for spoken lan- guage understanding for \ufb02ights. The Dialogue State Track- ing Challenges (Williams et al. 2013; Henderson, Thomson, and Williams 2014a; 2014b; Kim et al. 2017) contributed to the creation of dialogue datasets with increasing complex- ity. Other notable related datasets include WOZ2.0 (Wen et al. 2017), FRAMES (El Asri et al.",
      "2017) contributed to the creation of dialogue datasets with increasing complex- ity. Other notable related datasets include WOZ2.0 (Wen et al. 2017), FRAMES (El Asri et al. 2017), M2M (Shah et al. 2018) and MultiWOZ (Budzianowski et al. 2018). These datasets have utilized a variety of data collection techniques, falling within two broad categories: \u2022 Wizard-of-Oz This setup (Kelley 1984) connects two crowd workers playing the roles of the user and the sys- tem. The user is provided a goal to satisfy, and the system accesses a database of entities, which it queries as per the user\u2019s preferences. WOZ2.0, FRAMES and MultiWOZ, among others, have utilized such methods. \u2022 Machine-machine Interaction A related line of work ex- plores simulation-based dialogue generation, where the user and system roles are simulated to generate a com- plete conversation \ufb02ow, which can then be converted to natural language using crowd workers as done in Shah et al. (2018).",
      "\u2022 Machine-machine Interaction A related line of work ex- plores simulation-based dialogue generation, where the user and system roles are simulated to generate a com- plete conversation \ufb02ow, which can then be converted to natural language using crowd workers as done in Shah et al. (2018). Such a framework may be cost-effective and error-resistant since the underlying crowd worker task is simpler, and annotations are obtained automatically. As virtual assistants incorporate diverse domains, recent work has focused on zero-shot modeling (Bapna et al. 2017; Xia et al. 2018; Shah et al. 2019), domain adaptation and transfer learning techniques (Yang, Salakhutdinov, and Co- hen 2017; Rastogi, Hakkani-T\u00a8ur, and Heck 2017; Zhu and Yu 2018). Deep-learning based approaches have achieved state of the art performance on dialogue state tracking tasks. Popular approaches on small-scale datasets estimate the di- alogue state as a distribution over all possible slot-values (Henderson, Thomson, and Young 2014; Wen et al. 2017; Ren et al.",
      "Popular approaches on small-scale datasets estimate the di- alogue state as a distribution over all possible slot-values (Henderson, Thomson, and Young 2014; Wen et al. 2017; Ren et al. 2018) or individually score all slot-value com- binations (Mrk\u02c7si\u00b4c et al. 2017; Zhong, Xiong, and Socher 2018). Such approaches are not practical for deployment in virtual assistants operating over real-world services having a very large and dynamic set of possible values. Address- ing these concerns, approaches utilizing a dynamic vocab- ulary of slot values have been proposed (Rastogi, Gupta, and Hakkani-Tur 2018; Goel, Paul, and Hakkani-T\u00a8ur 2019; Wu et al. 2019). 3 The Schema-Guided Dialogue Dataset An important goal of this work is to create a benchmark dataset highlighting the challenges associated with building large-scale virtual assistants. Table 1 compares our dataset with other public datasets. Our Schema-Guided Dialogue (SGD) dataset exceeds other datasets in most of the met- rics at scale.",
      "Table 1 compares our dataset with other public datasets. Our Schema-Guided Dialogue (SGD) dataset exceeds other datasets in most of the met- rics at scale. The especially larger number of domains, slots, and slot values, and the presence of multiple services per domain, are representative of these scale-related challenges. Furthermore, our evaluation sets contain many services, and consequently slots, which are not present in the training set, to help evaluate model performance on unseen services. The 20 domains present across the train, dev and test splits are listed in Table 2. We create synthetic implementations of a total of 45 services or APIs over these domains. Our simu- lator framework interacts with these services to generate di- alogue outlines, which are a structured representation of di- alogue semantics. We then used a crowd-sourcing procedure to paraphrase these outlines to natural language utterances. Our novel crowd-sourcing procedure preserves all annota- tions obtained from the simulator and does not require any extra annotations after dialogue collection. In this section, we describe these steps in detail and then present analyses of the collected dataset.",
      "Our novel crowd-sourcing procedure preserves all annota- tions obtained from the simulator and does not require any extra annotations after dialogue collection. In this section, we describe these steps in detail and then present analyses of the collected dataset. 3.1 Services and APIs We de\ufb01ne the schema for a service as a combination of in- tents and slots with additional constraints, with an example in Figure 1. We implement all services using a SQL engine. For constructing the underlying tables, we sample a set of entities from Freebase and obtain the values for slots de\ufb01ned in the schema from the appropriate attribute in Freebase. We decided to use Freebase to sample real-world entities instead of synthetic ones since entity attributes are often correlated (e.g, a restaurant\u2019s name is indicative of the cuisine served). Some slots like event dates/times and available ticket counts, which are not present in Freebase, are synthetically sampled. To re\ufb02ect the constraints present in real-world services and APIs, we impose a few other restrictions. First, our dataset does not expose the set of all possible slot values",
      "Domain #Intents #Dialogs Domain #Intents #Dialogs Alarm2,3 2 (1) 324 Movies1,2,3 5 (3) 2339 Banks1,2 4 (2) 1021 Music1,2,3 6 (3) 1833 Buses1,2,3 6 (3) 3135 Payment3 2 (1) 222 Calendar1 3 (1) 1602 RentalCars1,2,3 6 (3) 2510 Events1,2,3 7 (3) 4519 Restaurants1,2,3 4 (2) 3218 Flights1,2,3 10 (4) 3644 RideSharing1,2,3 2 (2) 2223 Homes1,2,3 2 (1) 1273 Services1,2,3 8 (4) 2956 Hotels1,2,3 8 (4) 4992 Train3 2 (1) 350 Media1,2,3 6 (3) 1656 Travel1,2,",
      "2,3 8 (4) 2956 Hotels1,2,3 8 (4) 4992 Train3 2 (1) 350 Media1,2,3 6 (3) 1656 Travel1,2,3 1 (1) 2808 Messaging3 1 (1) 298 Weather1,2,3 1 (1) 1783 Table 2: The total number of intents (services in parentheses) and dialogues for each domain across train1, dev2 and test3 sets. Multi-domain dialogues contribute to counts of each constituent domain. The domain \u2018Service\u2019 includes salons, dentists, doctors etc. The \u2018Alarm\u2019, \u2018Messaging\u2019, \u2018Payment\u2019 and \u2018Train\u2019 domains are only present in the dev or test sets to test generalization to new domains. for some slots. Having such a list is impractical for slots like date or time because they have in\ufb01nitely many possible val- ues or for slots like movie or song names, for which new val- ues are periodically added.",
      "for some slots. Having such a list is impractical for slots like date or time because they have in\ufb01nitely many possible val- ues or for slots like movie or song names, for which new val- ues are periodically added. Our dataset speci\ufb01cally identi\ufb01es such slots as non-categorical and does not provide a set of all possible values for these. We also ensure that the evaluation sets have a considerable fraction of slot values not present in the training set to evaluate the models in the presence of new values. Some slots like gender, number of people, day of the week etc. are de\ufb01ned as categorical and we specify the set of all possible values taken by them. However, these values are not assumed to be consistent across services. E.g., different services may use (\u2018male\u2019, \u2018female\u2019), (\u2018M\u2019, \u2018F\u2019) or (\u2018he\u2019, \u2018she\u2019) as possible values for gender slot. Second, real-world services can only be invoked with a limited number of slot combinations: e.g. restaurant reser- vation APIs do not let the user search for restaurants by date without specifying a location.",
      "Second, real-world services can only be invoked with a limited number of slot combinations: e.g. restaurant reser- vation APIs do not let the user search for restaurants by date without specifying a location. However, existing datasets simplistically allow service calls with any given combina- tion of slot values, thus giving rise to \ufb02ows unsupported by actual services or APIs. As in Figure 1, the different service calls supported by a service are listed as intents. Each in- tent speci\ufb01es a set of required slots and the system is not al- lowed to call this intent without specifying values for these required slots. Each intent also lists a set of optional slots with default values, which the user can override. 3.2 Dialogue Simulator Framework The dialogue simulator interacts with the services to gener- ate dialogue outlines. Figure 2 shows the overall architec- ture of our dialogue simulator framework. It consists of two agents playing the roles of the user and the system. Both agents interact with each other using a \ufb01nite set of actions speci\ufb01ed through dialogue acts over a probabilistic automa- ton designed to capture varied dialogue trajectories.",
      "It consists of two agents playing the roles of the user and the system. Both agents interact with each other using a \ufb01nite set of actions speci\ufb01ed through dialogue acts over a probabilistic automa- ton designed to capture varied dialogue trajectories. These dialogue acts can take a slot or a slot-value pair as argument. Figure 4b shows all dialogue acts supported by the agents. At the start of a conversation, the user agent is seeded with a scenario, which is a sequence of intents to be ful- Figure 1: Example schema for a digital wallet service. \ufb01lled. We identi\ufb01ed over 200 distinct scenarios for the train- ing set, each comprising up to 5 intents. For multi-domain dialogues, we also identify combinations of slots whose val- ues may be transferred when switching intents e.g. the \u2018ad- dress\u2019 slot value in a restaurant service could be transferred to the \u2018destination\u2019 slot for a taxi service invoked right after. The user agent then generates the dialogue acts to be out- put in the next turn. It may retrieve arguments i.e.",
      "the \u2018ad- dress\u2019 slot value in a restaurant service could be transferred to the \u2018destination\u2019 slot for a taxi service invoked right after. The user agent then generates the dialogue acts to be out- put in the next turn. It may retrieve arguments i.e. slot values for some of the generated acts by accessing either the service schema or the SQL backend. The acts, combined with the respective parameters yield the corresponding user actions. Next, the system agent generates the next set of actions using a similar procedure. Unlike the user agent, however, the sys- tem agent has restricted access to the services (denoted by dashed line), e.g. it can only query the services by supplying values for all required slots for some service call. This helps us ensure that all generated \ufb02ows are valid. After an intent is ful\ufb01lled through a series of user and system actions, the user agent queries the scenario to pro- ceed to the next intent. Alternatively, the system may sug- gest related intents e.g. reserving a table after searching for a restaurant. The simulator also allows for multiple intents to be active during a given turn.",
      "Alternatively, the system may sug- gest related intents e.g. reserving a table after searching for a restaurant. The simulator also allows for multiple intents to be active during a given turn. While we skip many imple- mentation details for brevity, it is worth noting that we do not include any domain-speci\ufb01c constraints in the simula- tion automaton. All domain-speci\ufb01c constraints are encoded in the schema and scenario, allowing us to conveniently use the simulator across a wide variety of domains and services. 3.3 Dialogue Paraphrasing The dialogue paraphrasing framework converts the outlines generated by the simulator into a natural conversation. Fig- ure 3a shows a snippet of the dialogue outline generated by the simulator, containing a sequence of user and system ac- tions. The slot values present in these actions are in a canon- ical form because they obtained directly from the service. However, users may refer to these values in various differ-",
      "Figure 2: The overall architecture of the dialogue simulation framework for generating dialogue outlines. ent ways during the conversation, e.g., \u201clos angeles\u201d may be referred to as \u201cLA\u201d or \u201cLAX\u201d. To introduce these natural variations in the slot values, we replace different slot values with a randomly selected variation (kept consistent across user turns in a dialogue) as shown in Figure 3b. Next we de\ufb01ne a set of action templates for converting each action into a utterance. A few examples of such tem- plates are shown below. These templates are used to convert each action into a natural language utterance, and the result- ing utterances for the different actions in a turn are concate- nated together as shown in Figure 3c. The dialogue trans- formed by these steps is then sent to the crowd workers. One crowd worker is tasked with paraphrasing all utterances of a dialogue to ensure naturalness and coherence. REQUEST(location) \u2192Which city are you in? INFORM(location=$x) \u2192I want to eat in $x. OFFER(restaurant=$x) \u2192$x is a nice restaurant.",
      "REQUEST(location) \u2192Which city are you in? INFORM(location=$x) \u2192I want to eat in $x. OFFER(restaurant=$x) \u2192$x is a nice restaurant. In our paraphrasing task, the crowd workers are instructed to exactly repeat the slot values in their paraphrases. This not only helps us verify the correctness of the paraphrases, but also lets us automatically obtain slot spans in the generated utterances by string search. This automatic slot span genera- tion greatly reduced the annotation effort required, with little impact on dialogue naturalness, thus allowing us to collect more data with the same resources. Furthermore, it is im- portant to note that this entire procedure preserves all other annotations obtained from the simulator including the dia- logue state. Hence, no further annotation is needed. 3.4 Dataset Analysis With over 16000 dialogues in the training set, the Schema- Guided Dialogue dataset is the largest publicly available an- notated task-oriented dialogue dataset. The annotations in- clude the active intents and dialogue states for each user ut- terance and the system actions for every system utterance.",
      "The annotations in- clude the active intents and dialogue states for each user ut- terance and the system actions for every system utterance. We have a few other annotations like the user actions but we withhold them from the public release. These annotations enable our dataset to be used as benchmark for tasks like intent detection, dialogue state tracking, imitation learning of dialogue policy, dialogue act to text generation etc. The schemas contain semantic information about the APIs and the constituent intents and slots, in the form of natural lan- guage descriptions and other details (example in Figure 1). The single-domain dialogues in our dataset contain an av- erage of 15.3 turns, whereas the multi-domain ones contain Figure 3: Steps for obtaining paraphrased conversations. To increase the presence of relative dates like tomorrow, next Monday, the current date is assumed to be March 1, 2019. 23 turns on an average. These numbers are also re\ufb02ected in Figure 4a showing the histogram of dialogue lengths on the training set. Table 2 shows the distribution of dialogues across the different domains.",
      "23 turns on an average. These numbers are also re\ufb02ected in Figure 4a showing the histogram of dialogue lengths on the training set. Table 2 shows the distribution of dialogues across the different domains. We note that distribution of di- alogues across the domains and services covered is largely balanced, with the exception domains which are not present in the training set. Figure 4b shows the frequency of dia- logue acts contained in the dataset. Note that all dialogue acts except INFORM, REQUEST and GOODBYE are speci\ufb01c to either the user or the system. 4 The Schema-Guided Approach Virtual assistants aim to support a large number of services available on the web. One possible approach is to de\ufb01ne a large uni\ufb01ed schema for the assistant, to which different service providers can integrate with. However, it is dif\ufb01cult to come up with a common schema covering all use cases. Having a common schema also complicates integration of tail services with limited developer support. We propose the schema-guided approach as an alternative to allow easy in- tegration of new services and APIs.",
      "However, it is dif\ufb01cult to come up with a common schema covering all use cases. Having a common schema also complicates integration of tail services with limited developer support. We propose the schema-guided approach as an alternative to allow easy in- tegration of new services and APIs. Under our proposed approach, each service provides a schema listing the supported slots and intents along with their natural language descriptions (Figure 1 shows an ex- ample). These descriptions are used to obtain a semantic representation of these schema elements. The assistant em- ploys a single uni\ufb01ed model containing no domain or ser- vice speci\ufb01c parameters to make predictions conditioned on these schema elements. For example, Figure 5 shows how dialogue state representation for the same dialogue can vary for two different services. Here, the departure and arrival cities are captured by analogously functioning but differ- ently named slots in both schemas. Furthermore, values for the number stops and direct only slots highlight idiosyn- crasies between services interpreting the same concept. Using a single model facilitates representation and trans- fer of common knowledge across related services. Since the model utilizes semantic representation of schema elements",
      "(a) Histogram of lengths of training set dialogues. (b) Distribution of dialogue acts in training set. Figure 4: Detailed statistics of the SGD dataset. as input, it can interface with unseen services or APIs on which it has not been trained. It is also robust to changes like addition of new intents or slots to the service. 5 Zero-Shot Dialogue State Tracking Models in the schema-guided setting can condition on the pertinent services\u2019 schemas using descriptions of intents and slots. These models, however, also need access to represen- tations for potentially unseen inputs from new services. Re- cent pretrained models like ELMo (Peters et al. 2018) and BERT (Devlin et al. 2019) can help, since they are trained on very large corpora. Building upon these, we present a simple prototype model for zero-shot schema-guided DST. 5.1 Model We use a single model2, shared among all services and do- mains, to make these predictions. We \ufb01rst encode all the in- tents, slots and slot values for categorical slots present in the schema into an embedded representation.",
      "5.1 Model We use a single model2, shared among all services and do- mains, to make these predictions. We \ufb01rst encode all the in- tents, slots and slot values for categorical slots present in the schema into an embedded representation. Since differ- ent schemas can have differing numbers of intents or slots, predictions are made over dynamic sets of schema elements by conditioning them on the corresponding schema embed- dings. This is in contrast to existing models which make pre- dictions over a static schema and are hence unable to share knowledge across domains and services. They are also not robust to changes in schema and require the model to be retrained with new annotated data upon addition of a new intent, slot, or in some cases, a slot value to a service. Schema Embedding This component obtains the embed- ded representations of intents, slots and categorical slot val- 2Our model code is available at github.com/google- research/google-research/tree/master/schema guided dst ues in each service schema. Table 3 shows the sequence pairs used for embedding each schema element.",
      "Schema Embedding This component obtains the embed- ded representations of intents, slots and categorical slot val- 2Our model code is available at github.com/google- research/google-research/tree/master/schema guided dst ues in each service schema. Table 3 shows the sequence pairs used for embedding each schema element. These sequence pairs are fed to a pretrained BERT encoder shown in Fig- ure 6 and the output uCLS is used as the schema embedding. Sequence 1 Sequence 2 Intent service description intent description Slot service description slot description Value slot description value Table 3: Input sequences for the pretrained BERT model to obtain embeddings of different schema elements. For a given service with I intents and S slots, let {ij}, 1 \u2264j \u2264I and {sj}, 1 \u2264j \u2264S be the embeddings of all intents and slots respectively. As a special case, we let {sn j }, 1 \u2264j \u2264N \u2264S denote the embeddings for the N non- categorical slots in the service.",
      "As a special case, we let {sn j }, 1 \u2264j \u2264N \u2264S denote the embeddings for the N non- categorical slots in the service. Also, let {vk j }, 1 \u2264j \u2264V k denote the embeddings for all possible values taken by the kth categorical slot, 1 \u2264k \u2264C, with C being the number of categorical slots and N + C = S. All these embeddings are collectively called schema embeddings. Utterance Encoding Like Chao and Lane (2019), we use BERT to encode the user utterance and the preceding sys- tem utterance to obtain utterance pair embedding u = uCLS and token level representations t1, t2 \u00b7 \u00b7 \u00b7 tM, M being the total number of tokens in the two utterances. The utterance and schema embeddings are used together to obtain model predictions using a set of projections (de\ufb01ned below). Projection Let x, y \u2208Rd. For a task K, we de\ufb01ne l = FK(x, y, p) as a projection transforming x and y into the vector l \u2208Rp using Equations 1-3.",
      "Projection Let x, y \u2208Rd. For a task K, we de\ufb01ne l = FK(x, y, p) as a projection transforming x and y into the vector l \u2208Rp using Equations 1-3. Here, h1, h2 \u2208Rd, W K i and bK i for 1 \u2264i \u22643 are trainable parameters of suitable dimensions and A is the activation function. We use gelu (Hendrycks and Gimpel 2016) activation as in BERT. h1 = A(W K 1 x + bK 1 ) (1) h2 = A(W K 2 (y \u2295h1) + bK 2 ) (2) l = W K 3 h2 + bK 3 (3) Active Intent For a given service, the active intent denotes the intent requested by the user and currently being ful\ufb01lled by the system. It takes the value \u201cNONE\u201d if no intent for the service is currently being processed. Let i0 be a trainable parameter in Rd for the \u201cNONE\u201d intent. We de\ufb01ne the intent network as below.",
      "It takes the value \u201cNONE\u201d if no intent for the service is currently being processed. Let i0 be a trainable parameter in Rd for the \u201cNONE\u201d intent. We de\ufb01ne the intent network as below. lj int = Fint(u, ij, 1), 0 \u2264j \u2264I (4) The logits lj int are normalized using softmax to yield a dis- tribution over all I intents and the \u201cNONE\u201d intent. During inference, we predict the highest probability intent as active. Requested Slots These are the slots whose values are re- quested by the user in the current utterance. Projection Freq predicts logit lj req for the jth slot. Obtained logits are normal- ized using sigmoid to get a score in [0, 1]. During inference, all slots with score > 0.5 are predicted as requested. lj req = Freq(u, sj, 1), 1 \u2264j \u2264S (5)",
      "Figure 5: The predicted dialogue state (shown with dashed edges) for the \ufb01rst two user turns for an example dialogue, showing the active intent and slot assignments, with two related annotation schemas. Note that the dialogue state representation is conditioned on the schema under consideration, which is provided as input, as are the user and system utterances. Figure 6: BERT encoder, taking in two sequences p and q as input and outputs an embedded sequence pair representa- tion uCLS and token level representations {t1 \u00b7 \u00b7 \u00b7 tn+m}. We use BERT to obtain schema element embeddings and encode system and user utterances for dialogue state tracking. User Goal We de\ufb01ne the user goal as the user constraints speci\ufb01ed over the dialogue context till the current user ut- terance. Instead of predicting the entire user goal after each user utterance, we predict the difference between the user goal for the current turn and preceding user turn. During inference, the predicted user goal updates are accumulated to yield the predicted user goal. We predict the user goal updates in two stages.",
      "Instead of predicting the entire user goal after each user utterance, we predict the difference between the user goal for the current turn and preceding user turn. During inference, the predicted user goal updates are accumulated to yield the predicted user goal. We predict the user goal updates in two stages. First, for each slot, a distribution of size 3 denoting the slot status and taking values none, dontcare and active is obtained by normalizing the logits obtained in equation 6 using softmax. If the status of a slot is predicted to be none, its assigned value is assumed to be unchanged. If the prediction is dontcare, then the special dontcare value is assigned to it. Otherwise, a slot value is predicted and assigned to it in the second stage.",
      "If the status of a slot is predicted to be none, its assigned value is assumed to be unchanged. If the prediction is dontcare, then the special dontcare value is assigned to it. Otherwise, a slot value is predicted and assigned to it in the second stage. lj status = Fstatus(u, sj, 3), 1 \u2264j \u2264S (6) lj,k value = Fvalue(u, vk j , 1), 1 \u2264j \u2264V k, 1 \u2264k \u2264C (7) lj,k start = Fstart(tk, sn j , 1), 1 \u2264j \u2264N, 1 \u2264k \u2264M (8) lj,k end = Fend(tk, sn j , 1), 1 \u2264j \u2264N, 1 \u2264k \u2264M (9) In the second stage, equation 7 is used to obtain a logit for each value taken by each categorical slot. Logits for a given categorical slot are normalized using softmax to get a distri- bution over all possible values. The value with the maximum mass is assigned to the slot.",
      "Logits for a given categorical slot are normalized using softmax to get a distri- bution over all possible values. The value with the maximum mass is assigned to the slot. For each non-categorical slot, logits obtained using equations 8 and 9 are normalized using softmax to yield two distributions over all tokens. These two distributions respectively correspond to the start and end in- dex of the span corresponding to the slot. The indices p \u2264q maximizing start[p] + end[q] are predicted to be the span boundary and the corresponding value is assigned to the slot. 5.2 Evaluation We consider the following metrics for evaluation of the dia- logue state tracking task: 1. Active Intent Accuracy: The fraction of user turns for which the active intent has been correctly predicted. 2. Requested Slot F1: The macro-averaged F1 score for requested slots over all eligible turns. Turns with no re- quested slots in ground truth and predictions are skipped. 3. Average Goal Accuracy: For each turn, we predict a sin- gle value for each slot present in the dialogue state. The slots which have a non-empty assignment in the ground truth dialogue state are considered for accuracy.",
      "3. Average Goal Accuracy: For each turn, we predict a sin- gle value for each slot present in the dialogue state. The slots which have a non-empty assignment in the ground truth dialogue state are considered for accuracy. This is the average accuracy of predicting the value of a slot cor- rectly. A fuzzy matching score is used for non-categorical slots to reward partial matches with the ground truth. 4. Joint Goal Accuracy: This is the average accuracy of predicting all slot assignments for a turn correctly. For non-categorical slots a fuzzy matching score is used. Performance on other datasets We evaluate our model on public datasets WOZ2.0 and MultiWOZ 2.1 (Eric et al. 2019). As results in Table 4 show, our model performs com- petitively on these datasets. In these experiments, we omit the use of fuzzy matching scores and use exact match while calculating the goal accuracies to keep our numbers com- parable with other works. Furthermore, for the MultiWOZ 2.1 dataset, we also trained a model incorporating pointer- generator style copying for non-categorical slots, similar to Wu et al.",
      "Furthermore, for the MultiWOZ 2.1 dataset, we also trained a model incorporating pointer- generator style copying for non-categorical slots, similar to Wu et al. (2019), giving us a joint goal accuracy of 0.489, ex- ceeding the best-known result of 0.456 as reported in Eric et al. (2019). We omit the details of this model since it is not the main focus of this work.",
      "Performance on SGD The model performs well for Ac- tive Intent Accuracy and Requested Slots F1 across both seen and unseen services, shown in Table 4. For joint goal and average goal accuracy, the model performs better on seen services compared to unseen ones (Figure 7). The main reason for this performance difference is a signi\ufb01cantly higher OOV rate for slot values of unseen services. Performance on different domains (SGD) The model performance also varies across various domains. The perfor- mance for the different domains is shown in Table 5. We ob- serve that one of the major factors affecting the performance across domains is still the presence of the service in the training data (seen services). In most cases, the performance can be observed to degrade for domains with more unseen services. Among the unseen services, those in the \u2018Rental- Cars\u2019 and \u2018Buses\u2019 domain, have a very high OOV rate for slot values leading to worse performance. A low joint goal accuracy and high average goal accuracy for these two do- mains indicates a possible skew between the performance of different slots.",
      "A low joint goal accuracy and high average goal accuracy for these two do- mains indicates a possible skew between the performance of different slots. Among seen services, \u2018RideSharing\u2019 do- main also exhibits poor performance, since it possesses the largest number of the possible slot values across the dataset. We also notice that for categorical slots, with similar slot val- ues (e.g. \u201cPsychologist\u201d and \u201cPsychiatrist\u201d), there is a very weak signal for the model to distinguish between the differ- ent classes, resulting in inferior performance. Dataset Active Int Acc Req Slot F1 Avg GA Joint GA WOZ2.0 N.A. 0.970 0.920 0.810 MultiWOZ 2.1 N.A. N.A. 0.875 0.434 SGD-S 0.885 0.956 0.684 0.356 SGD-All 0.906 0.965 0.560 0.254 Table 4: Model performance on test sets of the respective datasets. SGD-Single model is trained on single-domain di- alogues only whereas SGD-All model is trained on the en- tire training set.",
      "SGD-Single model is trained on single-domain di- alogues only whereas SGD-All model is trained on the en- tire training set. We also report results on MultiWOZ 2.1 and WOZ2.0. N.A. indicates unavailable tasks. Figure 7: Performance of the model on all services, services seen in training data, services not seen in training data.",
      "We also report results on MultiWOZ 2.1 and WOZ2.0. N.A. indicates unavailable tasks. Figure 7: Performance of the model on all services, services seen in training data, services not seen in training data. Domain Joint GA Avg GA Domain Joint GA Avg GA RentalCars* 0.086 0.480 Restaurants* 0.228 0.558 Buses* 0.097 0.509 Events* 0.235 0.579 Messaging* 0.102 0.200 Flights* 0.239 0.659 Payment* 0.115 0.348 Hotels** 0.289 0.582 Trains* 0.136 0.635 Movies** 0.378 0.686 Music* 0.155 0.399 Services** 0.409 0.721 RideSharing 0.170 0.502 Travel 0.415 0.572 Media* 0.180 0.308 Alarm* 0.577 0.018 Homes 0.189 0.727 Weather 0.620 0.764 Table 5: Model performance per domain (GA: goal accu- racy).",
      "Domains marked with \u2018*\u2019 are those for which the ser- vice in the test set is not present in the training set. Domains like Hotels marked with \u2018**\u2019 has one unseen and one seen service. For other domains, the service in the test set was also seen in the training set. We see that the model generally performs better for domains containing services seen during training. 6 Discussion It is often argued that simulation-based data collection does not yield natural dialogues or suf\ufb01cient coverage, when compared to other approaches such as Wizard-of-Oz. We ar- gue that simulation-based collection is a better alternative for collecting datasets like this owing to the factors below. \u2022 Fewer Annotation Errors: All annotations are automat- ically generated, so these errors are rare. In contrast, Eric et al. (2019) reported annotation errors in 40% of turns in MultiWOZ 2.0 which utilized a Wizard-of-Oz setup. \u2022 Simpler Task: The crowd worker task of paraphrasing a readable utterance for each turn is simple. The error-prone annotation task requiring skilled workers is not needed.",
      "\u2022 Simpler Task: The crowd worker task of paraphrasing a readable utterance for each turn is simple. The error-prone annotation task requiring skilled workers is not needed. Furthermore, Wizard-of-Oz style collection requires do- main speci\ufb01c task de\ufb01nitions and instructions, making the collection of a diverse dataset like ours time consuming. \u2022 Low Cost: The simplicity of the crowd worker task and lack of an annotation task greatly cut data collection costs. \u2022 Better Coverage: A wide variety of dialogue \ufb02ows can be collected and speci\ufb01c usecases can be targeted. To ensure naturalness of the generated conversations, we used the conversational \ufb02ows present in other public datasets like MultiWOZ 2.0 and WOZ2.0 as a guideline while devel- oping the dialogue simulator. It was dif\ufb01cult for us to con- duct a side-by-side comparison with existing datasets since this is the \ufb01rst dataset to cover many new domains at such scale, but we plan to explore it in the future. 7 Conclusions We presented the Schema-Guided Dialogue dataset to en- courage scalable modeling approaches for virtual assistants.",
      "7 Conclusions We presented the Schema-Guided Dialogue dataset to en- courage scalable modeling approaches for virtual assistants. We also introduced the schema-guided paradigm for task- oriented dialogue that simpli\ufb01es the integration of new ser- vices and APIs with large scale virtual assistants. Building upon this paradigm, we present a simplistic model for zero- shot dialogue state tracking achieving competitive results.",
      "Acknowledgements The authors thank Guan-Lin Chao, Amir Fayazi and Maria Wang for their advice and assistance. References Bapna, A.; T\u00a8ur, G.; Hakkani-T\u00a8ur, D.; and Heck, L. P. 2017. To- wards zero-shot frame semantic parsing for domain scaling. In Interspeech 2017, 18th Annual Conference of the International Speech Communication Association, Stockholm, Sweden, August 20-24, 2017. Budzianowski, P.; Wen, T.-H.; Tseng, B.-H.; Casanueva, I.; Ultes, S.; Ramadan, O.; and Gasic, M. 2018. Multiwoz-a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue mod- elling. In Proceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing, 5016\u20135026. Chao, G.-L., and Lane, I. 2019. BERT-DST: Scalable end-to-end dialogue state tracking with bidirectional encoder representations from transformer. In INTERSPEECH.",
      "Chao, G.-L., and Lane, I. 2019. BERT-DST: Scalable end-to-end dialogue state tracking with bidirectional encoder representations from transformer. In INTERSPEECH. Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. Bert: Pre-training of deep bidirectional transformers for language un- derstanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguis- tics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171\u20134186. El Asri, L.; Schulz, H.; Sharma, S.; Zumer, J.; Harris, J.; Fine, E.; Mehrotra, R.; and Suleman, K. 2017. Frames: a corpus for adding memory to goal-oriented dialogue systems. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, 207\u2013 219.",
      "2017. Frames: a corpus for adding memory to goal-oriented dialogue systems. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, 207\u2013 219. Eric, M.; Goel, R.; Paul, S.; Sethi, A.; Agarwal, S.; Gao, S.; and Hakkani-Tur, D. 2019. Multiwoz 2.1: Multi-domain dia- logue state corrections and state tracking baselines. arXiv preprint arXiv:1907.01669. Goel, R.; Paul, S.; and Hakkani-T\u00a8ur, D. 2019. Hyst: A hybrid approach for \ufb02exible and accurate dialogue state tracking. arXiv preprint arXiv:1907.00883. Hemphill, C. T.; Godfrey, J. J.; and Doddington, G. R. 1990. The atis spoken language systems pilot corpus. In Speech and Natu- ral Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania, June 24-27, 1990.",
      "1990. The atis spoken language systems pilot corpus. In Speech and Natu- ral Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania, June 24-27, 1990. Henderson, M.; Thomson, B.; and Williams, J. D. 2014a. The second dialog state tracking challenge. In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), 263\u2013272. Henderson, M.; Thomson, B.; and Williams, J. D. 2014b. The third dialog state tracking challenge. 2014 IEEE Spoken Language Technology Workshop (SLT) 324\u2013329. Henderson, M.; Thomson, B.; and Young, S. 2014. Word-based dialog state tracking with recurrent neural networks. In Proceed- ings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), 292\u2013299. Hendrycks, D., and Gimpel, K. 2016. Gaussian error linear units (gelus).",
      "In Proceed- ings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), 292\u2013299. Hendrycks, D., and Gimpel, K. 2016. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415. Kelley, J. F. 1984. An iterative design methodology for user- friendly natural language of\ufb01ce information applications. ACM Transactions on Information Systems (TOIS) 2(1):26\u201341. Kim, S.; DHaro, L. F.; Banchs, R. E.; Williams, J. D.; and Hen- derson, M. 2017. The fourth dialog state tracking challenge. In Dialogues with Social Robots. Springer. 435\u2013449. Kim, Y.-B.; Kim, D.; Kumar, A.; and Sarikaya, R. 2018. Ef- \ufb01cient large-scale neural domain classi\ufb01cation with personalized attention.",
      "In Dialogues with Social Robots. Springer. 435\u2013449. Kim, Y.-B.; Kim, D.; Kumar, A.; and Sarikaya, R. 2018. Ef- \ufb01cient large-scale neural domain classi\ufb01cation with personalized attention. In Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), 2214\u20132224. Melbourne, Australia: Association for Computational Linguistics. Mrk\u02c7si\u00b4c, N.; S\u00b4eaghdha, D. \u00b4O.; Wen, T.-H.; Thomson, B.; and Young, S. 2017. Neural belief tracker: Data-driven dialogue state track- ing. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, 1777\u20131788. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations.",
      "Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. arXiv preprint arXiv:1802.05365. Rastogi, A.; Gupta, R.; and Hakkani-Tur, D. 2018. Multi-task learning for joint language understanding and dialogue state track- ing. In Proceedings of the 19th Annual SIGdial Meeting on Dis- course and Dialogue, 376\u2013384. Rastogi, A.; Hakkani-T\u00a8ur, D.; and Heck, L. 2017. Scalable multi-domain dialogue state tracking. In 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 561\u2013 568. IEEE. Ren, L.; Xie, K.; Chen, L.; and Yu, K. 2018. Towards universal dialogue state tracking. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2780\u20132786.",
      "IEEE. Ren, L.; Xie, K.; Chen, L.; and Yu, K. 2018. Towards universal dialogue state tracking. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2780\u20132786. Shah, P.; Hakkani-T\u00a8ur, D.; T\u00a8ur, G.; Rastogi, A.; Bapna, A.; Nayak, N.; and Heck, L. 2018. Building a conversational agent overnight with dialogue self-play. arXiv preprint arXiv:1801.04871. Shah, D.; Gupta, R.; Fayazi, A.; and Hakkani-Tur, D. 2019. Robust zero-shot cross-domain slot \ufb01lling with example values. In Pro- ceedings of the 57th Annual Meeting of the Association for Com- putational Linguistics, 5484\u20135490. Florence, Italy: Association for Computational Linguistics.",
      "2019. Robust zero-shot cross-domain slot \ufb01lling with example values. In Pro- ceedings of the 57th Annual Meeting of the Association for Com- putational Linguistics, 5484\u20135490. Florence, Italy: Association for Computational Linguistics. Wen, T.; Vandyke, D.; Mrk\u02c7s\u00b4\u0131c, N.; Ga\u02c7s\u00b4\u0131c, M.; Rojas-Barahona, L.; Su, P.; Ultes, S.; and Young, S. 2017. A network-based end-to-end trainable task-oriented dialogue system. In 15th Conference of the European Chapter of the Association for Computational Linguis- tics, EACL 2017-Proceedings of Conference, volume 1, 438\u2013449. Williams, J.; Raux, A.; Ramachandran, D.; and Black, A. 2013. The dialog state tracking challenge. In Proceedings of the SIGDIAL 2013 Conference, 404\u2013413. Wu, C.-S.",
      "Williams, J.; Raux, A.; Ramachandran, D.; and Black, A. 2013. The dialog state tracking challenge. In Proceedings of the SIGDIAL 2013 Conference, 404\u2013413. Wu, C.-S.; Madotto, A.; Hosseini-Asl, E.; Xiong, C.; Socher, R.; and Fung, P. 2019. Transferable multi-domain state generator for task-oriented dialogue systems. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 808\u2013 819. Florence, Italy: Association for Computational Linguistics. Xia, C.; Zhang, C.; Yan, X.; Chang, Y.; and Yu, P. 2018. Zero-shot user intent detection via capsule neural networks. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 3090\u20133099. Association for Computational Linguis- tics. Yang, Z.; Salakhutdinov, R.; and Cohen, W. W. 2017. Transfer learning for sequence tagging with hierarchical recurrent networks.",
      "Association for Computational Linguis- tics. Yang, Z.; Salakhutdinov, R.; and Cohen, W. W. 2017. Transfer learning for sequence tagging with hierarchical recurrent networks. arXiv preprint arXiv:1703.06345. Zhong, V.; Xiong, C.; and Socher, R. 2018. Global-locally self- attentive encoder for dialogue state tracking. In Proceedings of the 56th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), 1458\u20131467. Melbourne, Aus- tralia: Association for Computational Linguistics. Zhu, S., and Yu, K. 2018. Concept transfer learning for adaptive language understanding. In Proceedings of the 19th Annual SIG- dial Meeting on Discourse and Dialogue, 391\u2013399.",
      "A Appendix User My best friend and his family are coming to visit and I want to take them out while they are here. Is there a concert playing next Friday? System I can look into that but let\u2019s narrow it down a bit \ufb01rst. What city or cities would you like me to search in? User I would prefer it to be fairly local so NY would be my \ufb01rst choice. System Amber Run Brooklyn appears to be playing at Warsaw. User I don\u2019t much care for that one, can you \ufb01nd me a different concert please? System There will also be Chris Botti playing at Sony Hall. User Let\u2019s try some other kind of activity. How about games, preferably baseball. System I was able to \ufb01nd 8 baseball games the \ufb01rst of which is the Mets Vs Cubs playing at Citi Field. User That sounds like something we would all enjoy very much. System Would you like for me to go ahead and purchase tickets for you in advance? User Yes, can you reserve 3 tickets for that game? System You want 3 tickets to the Mets Vs Cubs game taking place in New York next Friday, is that correct?",
      "System Would you like for me to go ahead and purchase tickets for you in advance? User Yes, can you reserve 3 tickets for that game? System You want 3 tickets to the Mets Vs Cubs game taking place in New York next Friday, is that correct? User Yes, that\u2019s what I want. Can you give me the address of the event and the starting time as well? System I was able to book those tickets for you. The game will be held at 123-01 Roosevelt Avenue and it begins at 6 pm. User Thanks for the research and help, that is the only thing I needed for now. System It was my pleasure, have a great day and enjoy the game. Figure A.1: An example single domain dialogue from our dataset in the \u201cEvents\u201d domain. User I wanna change my hair style, help me \ufb01nd a salon System Where are you trying to get your hair done?",
      "System It was my pleasure, have a great day and enjoy the game. Figure A.1: An example single domain dialogue from our dataset in the \u201cEvents\u201d domain. User I wanna change my hair style, help me \ufb01nd a salon System Where are you trying to get your hair done? User Let\u2019s \ufb01nd a place in Oakland System How about 17 Jewels Salon in Oakland, I hear it\u2019s nice User Umm...Doesn\u2019t sound like something I\u2019d like, \ufb01nd me something else System What about this other Oakland salon called Benny Adem Grooming Parlor User What\u2019s their contact info and address? System You can reach them at 510-227-4268 and they\u2019re at 408 14th Street User Actually \ufb01nd me something else System Bibo Salon in Oakland has great reviews User Yes that\u2019 perfect System Shall I book an appointment for you then? User Yeah I want it done ASAP, maybe later today if possible? System What time then?",
      "User Yeah I want it done ASAP, maybe later today if possible? System What time then? User afternoon 12:30 would be great System Con\ufb01rming your 12:30 pm Bibo Salon appointment today User Yes that\u2019s correct System Ok glad I was able to book it User Thanks that\u2019s it for now System Can\u2019t wait to see your new hair! Figure A.2: An example single domain dialogue from our dataset in the \u201cServices\u201d domain.",
      "User I\u2019d like to catch a movie nearby. Can you \ufb01nd me any movie showings? System Sure, where should I look? User Please look around SFO for movie showings. System Okay, I found 10 movies being shown in that area that you might like. What do you think about the movies Ash Is Purest White, Captain Marvel or Citizen Kane? User Citizen Kane sounds like a good one. System How about buying any tickets for the movie? User No thanks, not right now. I\u2019d like to grab something to eat around there too. Can you \ufb01nd any good restau- rants to eat at? System Are you looking for a restaurant that serves a certain kind of food? User Yeah, can you \ufb01nd restaurants that serve Punjabi food and are moderate with their price range? System How about eating at a nice restaurant in San Francisco called August 1 Five? User Great, that sounds perfect. When can I watch the movie you found me earlier? System How about seeing it on March 2nd? User Yeah, that works for me.",
      "System How about eating at a nice restaurant in San Francisco called August 1 Five? User Great, that sounds perfect. When can I watch the movie you found me earlier? System How about seeing it on March 2nd? User Yeah, that works for me. System Okay, there\u2019s 1 showing at 7:30 pm at the Vogue Theatre. User Great, that works. System Anything else I can help you with today? User Yeah, can you book me a table at the restaurant you found for me? System Sure, what time would you like to eat there? User Reserve a table for me at around 5:30 pm, dinner time. System Okay, please con\ufb01rm that you\u2019d like to reserve a table for 2 at August 1 Five located in San Francisco for March 2nd at 5:30 pm. User Yup, that sounds right. Where exactly are they located and do you know if they play live music? System Your table has been successfully booked! Their address is 524 Van Ness Avenue and unfortunately they don\u2019t have live music here. User Okay, can you get me a cab to take me there?",
      "Where exactly are they located and do you know if they play live music? System Your table has been successfully booked! Their address is 524 Van Ness Avenue and unfortunately they don\u2019t have live music here. User Okay, can you get me a cab to take me there? System Sure, do you have a ride preference? I can get you a regular, luxury or other type of ride if you\u2019d like. User Yes, please get me a luxury ride. System Okay, please con\ufb01rm that you want a luxury ride with 2 seats to take you to 524 Van Ness Avenue. User Yeah that sounds right to me. System Okay, your cab has been booked and it\u2019s on its way. User Awesome, thanks! System Anything else I can do for you today? User Nope, that\u2019s all I needed, thanks! System No problem, enjoy your day! Figure A.3: An example multi-domain dialogue from our dataset covering \u201cMovies\u201d, \u201cRestaurants\u201d and \u201cRideSharing\u201d do- mains.",
      "User My buddy is getting married and I need to \ufb01nd a bus ticket. System I can help you with that. Can you give me a little more information like where you are traveling from, where you are traveling to, and when you would like to leave. User I need to go from Las Vegas to Sacramento on the 7th of this month. System Great, I was able to \ufb01nd 4 buss\u2019s matching that criteria. How does a bus with 1 transfer, leaving at 7:20 am, for $88 sound? User That sounds like just what i\u2019m looking for. I will also be needing a rental car until the 11th of this month. System Will this car be from Sacramento from March 7th. User I would like to pick up the car on the 8th around two in the afternoon. System The \ufb01rst of 5 cars I have found for you is a standard Accord, located at SMF International Airport, available for pickup on March 8th. User That one sounds perfect. System Would you like me to proceed with making that reservation for you? User Yes, let\u2019s go ahead with that car.",
      "User That one sounds perfect. System Would you like me to proceed with making that reservation for you? User Yes, let\u2019s go ahead with that car. System Before I reserve this I would like to make sure all the details I have are correct. You would like to pickup a standard vehicle from SMF International Airport on March 8th at 2 pm, and will be returning the car on March 11th. User Yes, that is all correct but can you give me an estimated price too please? System I was successfully able to reserve that with a total cost of $105 for you. User Great, could you also help me \ufb01nd an online movie to watch? System What genre of movie are you looking for? User I am really into detective movies right now. System The 2 movies I found that you may like are Body Double and Citizen Kane. User I would really like to see Body Double now if possible, I heard it\u2019s pretty good. System Okay, so you would like me to begin playing the movie Body Double without subtitles now. Is that correct? User Yes, that would be great. System I\u2019ve went ahead and started playing that movie for you.",
      "System Okay, so you would like me to begin playing the movie Body Double without subtitles now. Is that correct? User Yes, that would be great. System I\u2019ve went ahead and started playing that movie for you. User Thanks so much! System Is there anything else I can help you with for now? User No, that\u2019s all for now. System Have a good day and enjoy your movie! Figure A.4: An example multi-domain dialogue from our dataset covering \u201cBuses\u201d, \u201cRentalCars\u201d and \u201cMedia\u201d domains. Figure A.5: Interface of the dialogue paraphrasing task where the crowd workers are asked to rephrase the dialogue outlines to a more natural expression. The actual interface shows the entire conversation, but only a few utterances have been shown in this \ufb01gure. All non-categorical slot values are highlighted in blue. The task cannot be submitted unless all highlighted values in the outline are also present in the conversational dialogue."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1909.05855.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":12092,
  "avg_doclen":186.0307692308,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1909.05855.pdf"
    }
  }
}