{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Humor Detection: A Transformer Gets the Last Laugh Orion Weller Computer Science Department Brigham Young University orionw@byu.edu Kevin Seppi Computer Science Department Brigham Young University kseppi@byu.edu Abstract Much previous work has been done in attempt- ing to identify humor in text. In this pa- per we extend that capability by proposing a new task: assessing whether or not a joke is humorous. We present a novel way of ap- proaching this problem by building a model that learns to identify humorous jokes based on ratings gleaned from Reddit pages, consist- ing of almost 16,000 labeled instances. Using these ratings to determine the level of humor, we then employ a Transformer architecture for its advantages in learning from sentence context. We demonstrate the effectiveness of this approach and show results that are com- parable to human performance. We further demonstrate our model\u2019s increased capabili- ties on humor identi\ufb01cation problems, such as the previously created datasets for short jokes and puns.",
      "We demonstrate the effectiveness of this approach and show results that are com- parable to human performance. We further demonstrate our model\u2019s increased capabili- ties on humor identi\ufb01cation problems, such as the previously created datasets for short jokes and puns. These experiments show that this method outperforms all previous work done on these tasks, with an F-measure of 93.1% for the Puns dataset and 98.6% on the Short Jokes dataset. 1 Introduction Recent advances in natural language processing and neural network architecture have allowed for widespread application of these methods in Text Summarization (Liu et al., 2018), Natural Lan- guage Generation (Bahuleyan, 2018), and Text Classi\ufb01cation (Yang et al., 2016). Such advances have enabled scientists to study common language practices. One such area, humor, has garnered fo- cus in classi\ufb01cation (Zhang and Liu, 2014; Chen and Soo, 2018), generation (He et al., 2019; Vali- tutti et al., 2013), and in social media (Raz, 2012).",
      "The next question then is, what makes a joke humorous? Although humor is a universal con- struct, there is a wide variety between what each individual may \ufb01nd humorous. We attempt to fo- cus on a subset of the population where we can quantitatively measure reactions: the popular Red- dit r/Jokes thread. This forum is highly popu- lar - with tens of thousands of jokes being posted monthly and over 16 million members. Although larger joke datasets exist, the r/Jokes thread is un- paralleled in the amount of rated jokes it contains. To the best of our knowledge there is no compa- rable source of rated jokes in any other language. These Reddit posts consist of the body of the joke, the punchline, and the number of reactions or up- votes. Although this type of humor may only be most enjoyable to a subset of the population, it is an effective way to measure responses to jokes in a large group setting.1 What enables us to perform such an analysis are the recent improvements in neural network ar- chitecture for natural language processing.",
      "Although this type of humor may only be most enjoyable to a subset of the population, it is an effective way to measure responses to jokes in a large group setting.1 What enables us to perform such an analysis are the recent improvements in neural network ar- chitecture for natural language processing. These breakthroughs started with the Convolutional Neu- ral Network (LeCun et al., 1998) and have recently included the inception (Bahdanau et al., 2015) and progress of the Attention mechanism (Luong et al., 2015; Xu et al., 2015), and the Transformer archi- tecture (Vaswani et al., 2017). 2 Related Work In the related work of joke identi\ufb01cation, we \ufb01nd a myriad of methods employed over the years: sta- tistical and N-gram analysis (Taylor and Mazlack, 2004), Regression Trees (Purandare and Litman, 2006), Word2Vec combined with K-NN Human Centric Features (Yang et al., 2015), and Convo- lutional Neural Networks (Chen and Soo, 2018).",
      "This previous research has gone into many set- tings where humor takes place. Chen and Soo (2018) studied audience laughter compared to tex- tual transcripts in order to identify jokes in con- versation, while much work has also gone into us- 1See the thread (of varied and not safe for work content) at this link. We do not endorse these jokes. arXiv:1909.00252v1  [cs.CL]  31 Aug 2019",
      "Body Punchline Score Man, I was so tired last night; I had a dream I was a muf\ufb02er... and I woke up ex- hausted 276 I told my teenage niece to go get me a newspaper... She laughed at me, and said, \u201dOh uncle you\u2019re so old. Just use my phone.\u201d So I slammed her phone against the wall to kill a spider. 28315 Table 1: Example format of the Reddit Jokes dataset ing and creating datasets like the Pun of the Day (Yang et al., 2015), 16000 One-liners (Mihalcea and Strapparava, 2005), and even Ted Talks (Chen and Soo, 2018). 3 Data We gathered jokes from a variety of sources, each covering a different type of humor. These datasets include jokes of multiple sentences (the Short Jokes dataset), jokes with only one sentence (the Puns dataset), and more mixed jokes (the Reddit dataset). We have made our code and datasets open source for others to use. 2 3.1 Reddit Our Reddit data was gathered using Reddit\u2019s pub- lic API, collecting the most recent jokes.",
      "We have made our code and datasets open source for others to use. 2 3.1 Reddit Our Reddit data was gathered using Reddit\u2019s pub- lic API, collecting the most recent jokes. Every time the scraper ran, it also updated the upvote score of the previously gathered jokes. This data collection occurred every hour through the months of March and April 2019. Since the data was al- ready split into body and punchline sections from Reddit, we created separate datasets containing the body of the joke exclusively and the punchline of the joke exclusively. Additionally, we created a dataset that combined the body and punchline to- gether. Some sample jokes are shown in Table 1, above. The distribution of joke scores varies wildly, rang- ing from 0 to 136,354 upvotes. We found that there is a major jump between the 0-200 upvote range and the 200 range and onwards, with only 6% of jokes scoring between 200-20,000. We used this natural divide as the cutoff to decide what quali\ufb01ed as a funny joke, giving us 13884 not- funny jokes and 2025 funny jokes.",
      "We used this natural divide as the cutoff to decide what quali\ufb01ed as a funny joke, giving us 13884 not- funny jokes and 2025 funny jokes. 3.2 Short Jokes The Short Jokes dataset, found on Kaggle, con- tains 231,657 short jokes scraped from various joke websites with lengths ranging from 10 to 200 2Our code and datasets are publicly available at this link. characters. The previous work by Chen and Soo (2018) combined this dataset with the WMT162 English news crawl. Although their exact com- bined dataset is not publicly available, we used the same method and news crawl source to cre- ate a similar dataset. We built this new Short Jokes dataset by extracting sentences from the WMT162 news crawl that had the same distribution of words and characters as the jokes in the Short Jokes dataset on Kaggle3. This was in order to match the two halves (jokes and non-jokes) as closely as possible. 3.3 Pun of the Day This dataset was scraped by Yang et al.",
      "This was in order to match the two halves (jokes and non-jokes) as closely as possible. 3.3 Pun of the Day This dataset was scraped by Yang et al. (2015) and contains 16001 puns and 16002 not-punny sen- tences. We gratefully acknowledge their help in putting together and giving us use of this dataset. These puns were constructed from the Pun of the Day website while the negative samples were gathered from news websites. 4 Methods In this section we will discuss the methods and model used in our experiments. 4.1 Our Model We have chosen to use the pre-trained BERT (De- vlin et al., 2018) as the base of our model. BERT is a multi-layer bidirectional Transformer encoder and was initially trained on a 3.3 billion word cor- pus. The model can be \ufb01ned-tuned with another additional output layer for a multitude of other tasks. We chose to use this Transformer based model as our initial platform because of its success at recognizing and attending to the most important words in both sentence and paragraph structures. In Figure 1, originally designed by Vaswani et al.",
      "We chose to use this Transformer based model as our initial platform because of its success at recognizing and attending to the most important words in both sentence and paragraph structures. In Figure 1, originally designed by Vaswani et al. (2017), we see the architecture of a Trans- former model: the initial input goes up through an encoder, which has two parts: a multi-headed 3The Short Jokes dataset from Kaggle is available here.",
      "Figure 1: Transformer Model Architecture self attention layer, followed by a feed-forward network. It then outputs the information into the decoder, which includes the previously mentioned layers, plus an additional masked attention step. Afterwords, it is transformed through a softmax into the output. This model\u2019s success is in large part due to the Transformer\u2019s self-attention layers. We chose a learning rate of 2e-05 and a max sequence length of 128. We trained the model for a maximum of 7 epochs, creating checkpoints along the way. 4.2 Training Since our data was unbalanced we decided to up- sample the humorous jokes in training. We split the dataset into a 75/25 percent split, stratifying with the labels. We then upsampled the minority class in the training set until it reached an even 50 percent. This helped our model learn in a more balanced way despite the uneven amount of non- humorous jokes. Our validation and test sets were composed of the remaining 25%, downsampling the data into a 50/50 class split so that the accuracy metric could be balanced and easily understood.",
      "This helped our model learn in a more balanced way despite the uneven amount of non- humorous jokes. Our validation and test sets were composed of the remaining 25%, downsampling the data into a 50/50 class split so that the accuracy metric could be balanced and easily understood. To show how our model compares to the pre- vious work done, we also test on the Short Joke and Pun datasets mentioned in the Data section. For these datasets we will use the metrics (Accu- racy, Precision, Recall, and F1 Score) designated in Chen and Soo (2018) as a comparison. We use Method Body Punchline Full CNN 0.651 0.684 0.688 Transformer 0.661 0.692 0.724 Human (General) 0.493 0.592 0.663 Table 2: Results of Accuracy on Reddit Jokes dataset the same model format as previously mentioned, trained on the Reddit dataset. We then immedi- ately apply the model to predict on the Short Joke and Puns dataset, without further \ufb01ne-tuning, in order to compare the model.",
      "We then immedi- ately apply the model to predict on the Short Joke and Puns dataset, without further \ufb01ne-tuning, in order to compare the model. However, because both the Puns and Short Joke datasets have large and balanced labels, we do so without the upsam- pling and downsampling steps used for the Reddit dataset. 5 Experiments In this section we will introduce the baselines and models used in our experiments. 5.1 Baselines In order to have fair baselines, we used the fol- lowing two models: a CNN with Highway Lay- ers as described by Chen and Soo (2018) and de- veloped by Srivastava et al. (2015), and human performance from a study on Amazon\u2019s Mechan- ical Turk. We wanted to have the general pop- ulation rate these same jokes, thus showing the difference between a general audience and a spe- ci\ufb01c subset of the population, in particular, Red- dit r/Jokes users. Since the Reddit users obvi- ously found these jokes humorous, this experiment would show whether or not a more general popu- lation agreed with those labels.",
      "Since the Reddit users obvi- ously found these jokes humorous, this experiment would show whether or not a more general popu- lation agreed with those labels. We had 199 unique participants rate an average of 30 jokes each with the prompt \u201ddo you \ufb01nd this joke humorous?\u201d If the participant was evaluating a sample from a body or punchline only dataset we prefaced our question with a sentence explaining that context, for example: \u201dBelow is the punch- line of a joke. Based on this punchline, do you think you would \ufb01nd this joke humorous?\u201d Taking these labels, we used the most frequently chosen tag from a majority vote to calculate the percent- ages found in the Human section of Table 2. 5.2 Results In Table 2, we see the results of our experiment with the Reddit dataset. We ran our models on",
      "Previous Work: Accuracy Precision Recall F1 Word2Vec+HCF 0.797 0.776 0.836 0.705 CNN 0.867 0.880 0.859 0.869 CNN+F 0.892 0.886 0.907 0.896 CNN+HN 0.892 0.889 0.903 0.896 CNN+F+HN 0.894 0.866 0.940 0.901 Our Methods: Accuracy Precision Recall F1 Transformer 0.930 0.930 0.931 0.931 Table 3: Comparison of Methods on Pun of the Day Dataset. HCF represents Human Centric Features, F for increasing the number of \ufb01lters, and HN for the use of highway layers in the model. See (Chen and Soo, 2018; Yang et al., 2015) for more details regarding these acronyms. the body of the joke exclusively, the punchline ex- clusively, and both parts together (labeled full in our table).",
      "See (Chen and Soo, 2018; Yang et al., 2015) for more details regarding these acronyms. the body of the joke exclusively, the punchline ex- clusively, and both parts together (labeled full in our table). On the full dataset we found that the Transformer achieved an accuracy of 72.4 percent on the hold out test set, while the CNN was in the high 60\u2019s. We also note that the general human classi\ufb01cation found 66.3% of the jokes to be hu- morous. In order to understand what may be happening in the model, we used the body and punchline only datasets to see what part of the joke was most im- portant for humor. We found that all of the models, including humans, relied more on the punchline of the joke in their predictions (Table 2). Thus, it seems that although both parts of the joke are needed for it to be humorous, the punchline car- ries higher weight than the body.",
      "We found that all of the models, including humans, relied more on the punchline of the joke in their predictions (Table 2). Thus, it seems that although both parts of the joke are needed for it to be humorous, the punchline car- ries higher weight than the body. We hypothesize that this is due to the variations found in the dif- ferent joke bodies: some take paragraphs to set up the joke, while others are less than a sentence. Our experiment with the Short Jokes dataset found the Transformer model\u2019s accuracy and F1 score to be 0.986. This was a jump of 8 percent from the most recent work done with CNNs (Ta- ble 4). The results on the Pun of the Day dataset are shown in Table 3 above. It shows an accuracy of 93 percent, close to 4 percent greater accuracy than the best CNN model proposed. Although the CNN model used a variety of techniques to extract the best features from the dataset, we see that the self-attention layers found even greater success in pulling out the crucial features.",
      "It shows an accuracy of 93 percent, close to 4 percent greater accuracy than the best CNN model proposed. Although the CNN model used a variety of techniques to extract the best features from the dataset, we see that the self-attention layers found even greater success in pulling out the crucial features. 6 Discussion Considering that a joke\u2019s humor value is subjec- tive, the results on the Reddit dataset are surpris- Method Accuracy Precision Recall F1 CNN+F+HN 0.906 0.902 0.946 0.924 Transformer 0.986 0.986 0.986 0.986 Table 4: Results on Short Jokes Identi\ufb01cation ing. The model has used the context of the words to determine, with high probability, what an av- erage Reddit r/Jokes viewer will \ufb01nd humorous. When we look at the general population\u2019s opinion as well, we \ufb01nd a stark difference between their preferences and those of the Reddit users (Table 2). We would hypothesize that our model is learn- ing the speci\ufb01c type of humor enjoyed by those who use the Reddit r/Jokes forum.",
      "We would hypothesize that our model is learn- ing the speci\ufb01c type of humor enjoyed by those who use the Reddit r/Jokes forum. This would suggest that humor can be learned for a speci\ufb01c subset of the population. The model\u2019s high accuracy and F1 scores on the Short Jokes and Pun of the Day dataset show the effectiveness of the model for transfer learning. This result is not terribly surprising. If the model can \ufb01gure out which jokes are funny, it seems to be an easier task to tell when something isn\u2019t a joke at all. Although these results have high potential, de\ufb01ning the absolute truth value for a joke\u2019s humor is a challenging, if not impossible task. However, these results indicate that, at least for a subset of the population, we can \ufb01nd and identify jokes that will be most humorous to them. 7 Conclusion In this paper, we showed a method to de\ufb01ne the measure of a joke\u2019s humor. We explored the idea of using machine learning tools, speci\ufb01cally a Transformer neural network architecture, to dis- cern what jokes are funny and what jokes are not.",
      "7 Conclusion In this paper, we showed a method to de\ufb01ne the measure of a joke\u2019s humor. We explored the idea of using machine learning tools, speci\ufb01cally a Transformer neural network architecture, to dis- cern what jokes are funny and what jokes are not. This proposed model does not require any human",
      "interaction to determine, aside from the text of the joke itself, which jokes are humorous. This archi- tecture can predict the level of humor for a speci\ufb01c audience to a higher degree than a general audi- ence consensus. We also showed that this model has increased capability in joke identi\ufb01cation as a result, with higher accuracy and F1 scores than previous work on this topic. References Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. International Con- ference on Learning Representations. Hareesh Bahuleyan. 2018. Natural Language Genera- tion with Neural Variational Models. arXiv e-prints, page arXiv:1808.09012. Peng-Yu Chen and Von-Wun Soo. 2018. Humor recog- nition using deep learning. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers).",
      "Peng-Yu Chen and Von-Wun Soo. 2018. Humor recog- nition using deep learning. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers). Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Un- derstanding. North American Chapter of the Asso- ciation for Computational Linguistics. He He, Nanyun Peng, and Percy Liang. 2019. Pun generation with surprise. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, Volume 1 (Long and Short Papers), pages 1734\u20131744, Minneapolis, Min- nesota. Association for Computational Linguistics. Yann LeCun, Leon Bottou, Y Bengio, and Patrick Haffner. 1998. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86:2278 \u2013 2324.",
      "Association for Computational Linguistics. Yann LeCun, Leon Bottou, Y Bengio, and Patrick Haffner. 1998. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86:2278 \u2013 2324. Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. 2018. Generating Wikipedia by Summariz- ing Long Sequences. International Conference on Learning Representations. Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective Approaches to Attention- based Neural Machine Translation. Proceedings of the 2015 Conference on Empirical Methods in Natu- ral Language Processing, pages pages 1412 \u2013 1421. Rada Mihalcea and Carlo Strapparava. 2005. Mak- ing computers laugh: Investigations in automatic humor recognition.",
      "Proceedings of the 2015 Conference on Empirical Methods in Natu- ral Language Processing, pages pages 1412 \u2013 1421. Rada Mihalcea and Carlo Strapparava. 2005. Mak- ing computers laugh: Investigations in automatic humor recognition. In Proceedings of the Confer- ence on Human Language Technology and Empiri- cal Methods in Natural Language Processing, HLT \u201905, pages 531\u2013538, Stroudsburg, PA, USA. Associ- ation for Computational Linguistics. Amruta Purandare and Diane Litman. 2006. Hu- mor: Prosody analysis and automatic recognition for f*r*i*e*n*d*s*. Proceedings of the 2006 Con- ference on Empirical Methods in Natural Language Processing, pages 208\u2013215. Yishay Raz. 2012. Automatic humor classi\ufb01cation on twitter. Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies: Student Research Workshop, pages 66\u201370.",
      "Yishay Raz. 2012. Automatic humor classi\ufb01cation on twitter. Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies: Student Research Workshop, pages 66\u201370. Rupesh Kumar Srivastava, Klaus Greff, and J\u00a8urgen Schmidhuber. 2015. Highway Networks. arXiv e- prints, page arXiv:1505.00387. Julia M. Taylor and Lawrence J. Mazlack. 2004. Com- putationally recognizing wordplay in jokes. In Pro- ceedings of CogSci 2004. Alessandro Valitutti, Hannu Toivonen, Antoine Doucet, and Jukka M Toivanen. 2013. let every- thing turn well in your wife: Generation of adult humor using lexical constraints. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pages 243\u2013248.",
      "2013. let every- thing turn well in your wife: Generation of adult humor using lexical constraints. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pages 243\u2013248. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need. 31st Conference on Neural Information Processing Systems, pages 17\u201321. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. 2015. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. arXiv e-prints, page arXiv:1502.03044. Diyi Yang, Alon Lavie, Chris Dyer, and Eduard Hovy. 2015. Humor recognition and humor anchor extrac- tion.",
      "arXiv e-prints, page arXiv:1502.03044. Diyi Yang, Alon Lavie, Chris Dyer, and Eduard Hovy. 2015. Humor recognition and humor anchor extrac- tion. Proceedings of the 2015 Conference on Em- pirical Methods in Natural Language Processing, pages 2367\u20132376. Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alexander J. Smola, and Eduard H. Hovy. 2016. Hi- erarchical attention networks for document classi\ufb01- cation. HLT-NAACL. Renxian Zhang and Naishi Liu. 2014. Recognizing hu- mor on twitter. In Proceedings of the 23rd ACM In- ternational Conference on Conference on Informa- tion and Knowledge Management, CIKM \u201914, pages 889\u2013898, New York, NY, USA. ACM."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1909.00252.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":4894,
  "avg_doclen":181.2592592593,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1909.00252.pdf"
    }
  }
}