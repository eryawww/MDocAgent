[
  "NON-NATIVE SPEAKER VERIFICATION FOR SPOKEN LANGUAGE ASSESSMENT Linlin Wang\u2020, Yu Wang\u2020, Mark J. F. Gales ALTA Institute / Engineering Department, Cambridge University, UK ABSTRACT Automatic spoken language assessment systems are becom- ing more popular in order to handle increasing interests in second language learning. One challenge for these systems is to detect malpractice. Malpractice can take a range of forms, this paper focuses on detecting when a candidate attempts to impersonate another in a speaking test. This form of mal- practice is closely related to speaker veri\ufb01cation, but applied in the speci\ufb01c domain of spoken language assessment. Ad- vanced speaker veri\ufb01cation systems, which leverage deep- learning approaches to extract speaker representations, have been successfully applied to a range of native speaker veri\ufb01- cation tasks. These systems are explored for non-native spo- ken English data in this paper. The data used for speaker en- rolment and veri\ufb01cation is mainly taken from the BULATS test, which assesses English language skills for business.",
  "These systems are explored for non-native spo- ken English data in this paper. The data used for speaker en- rolment and veri\ufb01cation is mainly taken from the BULATS test, which assesses English language skills for business. Per- formance of systems trained on relatively limited amounts of BULATS data, and standard large speaker veri\ufb01cation cor- pora, is compared. Experimental results on large-scale test sets with millions of trials show that the best performance is achieved by adapting the imported model to non-native data. Breakdown of impostor trials across different \ufb01rst languages (L1s) and grades is analysed, which shows that inter-L1 im- postors are more challenging for speaker veri\ufb01cation systems. Index Terms\u2014 speaker veri\ufb01cation, non-native speech 1. INTRODUCTION Automatic spoken assessment systems are becoming increas- ingly popular, especially for English with the high demand around the world for learning of English as a second language [1, 2, 3, 4].",
  "Index Terms\u2014 speaker veri\ufb01cation, non-native speech 1. INTRODUCTION Automatic spoken assessment systems are becoming increas- ingly popular, especially for English with the high demand around the world for learning of English as a second language [1, 2, 3, 4]. In addition to assessing a candidate\u2019s English abil- ity such as \ufb02uency and pronunciation and giving feedback to the candidate, these automatic systems also need to ensure the integrity of the candidate\u2019s score by detecting malprac- tice, as shown in Figure 1. Malpractice is the action by a can- didate that breaks the assessment regulation and potentially This paper reports on research supported by Cambridge Assessment, University of Cambridge. Thanks to Cambridge English Language Assess- ment for support and access to the BULATS and Linguaskill data. \u2020 Both authors contributed equally. The authors would also like to thank Dr Kate Knill and Dr Anton Ragni for valuable discussions during the preparation of this manuscript. threatens the reliability of the exam and associated certi\ufb01ca- tion.",
  "\u2020 Both authors contributed equally. The authors would also like to thank Dr Kate Knill and Dr Anton Ragni for valuable discussions during the preparation of this manuscript. threatens the reliability of the exam and associated certi\ufb01ca- tion. Malpractice can take a range of forms in spoken lan- guage assessment scenarios, such as using or trying to use unauthorised materials, impersonation, speaking irrelevant to prompts/questions, speaking in his/her \ufb01rst language (L1) in- stead of the target language for spoken tests, etc. This work aims to investigate the problem of automatically detecting im- personation, in which a candidate attempts to impersonate an- other in a speaking test. This is closely related to speaker veri\ufb01cation. Fig. 1. Diagram of an automatic spoken language assessment system. Speaker veri\ufb01cation is the process to accept or reject an identity claim by comparing the speaker-speci\ufb01c information extracted from the veri\ufb01cation speech with that from the en- rolment speech of the claimed identity. These approaches can be directly applied to detect impersonation in spoken lan- guage tests.",
  "These approaches can be directly applied to detect impersonation in spoken lan- guage tests. The performance of speaker veri\ufb01cation systems has advanced considerably in the last decade with the de- velopment of i-vector modelling [5], in which a speech seg- ment or a speaker is represented as a low-dimensional fea- ture vector. Extraction of i-vectors is normally based on a Gaussian mixture model (GMM) based universal background model (UBM). This \ufb01xed length representation can then be used with a probabilistic linear discriminant analysis (PLDA) model to produce veri\ufb01cation scores by comparing speaker representations, which are then used to make valid or impos- tor speaker decisions [6, 7, 8, 9]. Recently, with develop- ments in deep learning, performance of speaker veri\ufb01cation systems has been improved by replacing the GMM with a deep neural network (DNN) to derive statistics for extract- ing speaker representations.",
  "Recently, with develop- ments in deep learning, performance of speaker veri\ufb01cation systems has been improved by replacing the GMM with a deep neural network (DNN) to derive statistics for extract- ing speaker representations. This DNN is usually trained to take a \ufb01xed length window of the acoustics and discriminate arXiv:1909.13695v1  [eess.AS]  30 Sep 2019",
  "between speakers using supplied speaker labels as targets. To handle the variable-length nature of the acoustic signal, a pooling layer is used to yield the \ufb01nal \ufb01xed-dimensional speaker representation. In [10], a DNN was trained at the frame level, and pooling was performed by averaging activa- tion vectors of the last hidden layer over all frames of an input utterance. In [11, 12, 13], segment-level embeddings were extracted, which are referred to as x-vectors [13] with data augmentation. By leveraging data augmentation based on background noise and acoustic reverberation, these x-vectors based systems can achieve better performance than i-vector and d-vector based systems on standard speaker veri\ufb01cation tasks. There has been some previous work on tasks related to non-native speech data using speaker veri\ufb01cation approaches, such as detection of non-native speech [14], classi\ufb01cation of native/non-native English [15] and L1 detection [16]. In [17], meta-data (L1) sensitive bottleneck features were employed within the i-vector framework to improve the performance of speaker veri\ufb01cation with non-native speech.",
  "In [17], meta-data (L1) sensitive bottleneck features were employed within the i-vector framework to improve the performance of speaker veri\ufb01cation with non-native speech. In contrast, this paper focuses on making use of the state-of-the-art deep- learning based speaker veri\ufb01cation approaches to detect can- didate impersonation in an English speaking test. As there is limited amounts of data available for the non-native learner task, it is of interest to investigate adapting a standard speaker veri\ufb01cation task to this non-native task. Here a system based on the VoxCeleb dataset [18, 19] is adapted to the BULATS task. Two forms of adaptation are examined: modifying the PLDA distance measure; and adapting the process for extract- ing the speaker representation by \u201c\ufb01ne-tuning\u201d the network to the target domain. Furthermore, detailed analysis of perfor- mance is also done with respect to speaker attributes.",
  "Two forms of adaptation are examined: modifying the PLDA distance measure; and adapting the process for extract- ing the speaker representation by \u201c\ufb01ne-tuning\u201d the network to the target domain. Furthermore, detailed analysis of perfor- mance is also done with respect to speaker attributes. Gender is an important attribute in impostor selection for standard speaker veri\ufb01cation tasks, and for non-native speech, there are two additional speaker attributes: the L1 and the language pro\ufb01ciency level1, which should also be taken into considera- tion for speaker veri\ufb01cation. This paper is organised as follows. Section 2 gives an overview of speaker veri\ufb01cation systems, and Section 3 in- troduces the non-native spoken English corpora used in this work. Experimental setup is described in Section 4, results and analysis are detailed in Section 5, and \ufb01nally, conclusions are drawn in Section 6. 2. SPEAKER VERIFICATION SYSTEMS In this work both i-vector and x-vector representations are used. For the i-vector speaker representation the form de- scribed in [5, 20] is used.",
  "2. SPEAKER VERIFICATION SYSTEMS In this work both i-vector and x-vector representations are used. For the i-vector speaker representation the form de- scribed in [5, 20] is used. This section will just discuss the x-vector speaker representation as this is the form that is adapted to the non-native veri\ufb01cation task. 1Language ability level is referred to as \u201cgrade\u201d in this work. 2.1. Deep neural network embedding extractor There are three blocks to form the DNN for extracting the utterance-level speaker representation, or embedding. The \ufb01rst block of the deep embedding extractor is a frame-level feature extractor. The input to this block is a sequence of acoustic feature vectors {x1, x2, \u00b7 \u00b7 \u00b7 xT } of T frames. This part normally consists of a number of hidden layers such as long short-term memory (LSTM) [21] or time delay neural network (TDNN) layers [12, 13]. The activations of the last hidden layer of this block for the input frames, {h1, h2, \u00b7 \u00b7 \u00b7 hT }, form the input to the second block which is a statistics pooling layer.",
  "The activations of the last hidden layer of this block for the input frames, {h1, h2, \u00b7 \u00b7 \u00b7 hT }, form the input to the second block which is a statistics pooling layer. This layer converts variable- length frame-level features into a \ufb01xed-dimensional vector by calculating the mean vector, \u00b5 and standard deviation vector \u03c3 of the frame-level feature vectors over the T frames. The third block takes the statistics as the input and produces utterance-level representations using a number of stacked fully-connected hidden layers. The output of the DNN ex- tractor is a softmax layer, and each of the nodes corresponds to one speaker identity. This DNN extractor is trained based on a cross-entropy loss function using the supplied speaker la- bels to get the targets. Consider there are N training segments and S speakers, the cross-entropy can be written as F (\u03b8) = \u2212 N X n=1 K X k=1 \u03b4 \u0010 s, s(n) k \u0011 logP \u0010 s|x(n) 1:T , \u03b8 \u0011 , (1) where \u03b8 represents the parameters of the DNN and \u03b4 (\u00b7) rep- resents the Kronecker delta function.",
  "s(n) k represents that the speaker label for segment n is sk. After the DNN is trained, the utterance-level embeddings, ed, are normally extracted from the output of the af\ufb01ne component that is with or without the nonlinear activation function applied of one hidden layer in the third block of the DNN [12, 13]. 2.2. PLDA classi\ufb01er and adaptation After the speaker embeddings are extracted, they are used to train a PLDA model that yields the score (distance) between speaker embeddings. The training of the PLDA models aims to maximise the between-speaker difference and minimise the within-speaker variation, typically using expectation maximi- sation (EM). A number of variants of PLDA models have been introduced into the speaker veri\ufb01cation task based on this \u201cstandard\u201d PLDA [6]: two-covariance PLDA [22] and heavy-tailed PLDA [7]. The variant implemented in the Kaldi toolkit [20], and used in this work, follows [23] and is similar to the two-covariance model.",
  "The variant implemented in the Kaldi toolkit [20], and used in this work, follows [23] and is similar to the two-covariance model. This model can be written as e = y + z, (2) p (y) = N (y; 0, \u0393) , (3) p (e|y) = N (e; y + \u00b5, \u039b) , (4) where e is the speaker embedding. The vector y represents the underlying speaker vector and \u00b5 represents its mean. z",
  "is the Gaussian noise vector. For speaker veri\ufb01cation tasks, estimation of this PLDA model can be performed by estimat- ing the between-speaker covariance matrix, \u0393, and within- speaker covariance matrix, \u039b, using the EM algorithm. PLDA is a powerful approach to classifying speakers given a large amounts of training data with speaker labels [24, 25, 26]. However, large amounts of labelled training data may not be available in the domain of interest such as the one considered in this paper, the non-native speaker veri\ufb01cation. One approach to alleviate this problem is to do adaptation from a pre-trained out-of-domain model to the target domain. There are a number of methods for adapting the PLDA model in both supervised and unsupervised manners [27, 26]. The Kaldi toolkit implements an unsupervised adaptation method which does not require knowledge of speaker labels [20]. This method aims at adapting \u0393 and \u039b of the out-of-domain PLDA model to better match the total covariance of the in- domain adaptation data. 3.",
  "The Kaldi toolkit implements an unsupervised adaptation method which does not require knowledge of speaker labels [20]. This method aims at adapting \u0393 and \u039b of the out-of-domain PLDA model to better match the total covariance of the in- domain adaptation data. 3. NON-NATIVE SPOKEN ENGLISH CORPORA The Business Language Testing Service (BULATS) test of Cambridge Assessment English [28] is a multi-level computer-based English test. It consists of read speech and free-speaking components, with the candidate responding to prompts. The BULATS spoken test has \ufb01ve sections, all with materials appropriate to business scenarios. The \ufb01rst section (A) contains eight questions about the candidate and their work. The second section (B) is a read-aloud section in which the candidates are asked to read eight sentences. The last three sections (C, D and E) have longer utterances of spontaneous speech elicited by prompts. In section C the candidates are asked to talk for one minute about a prompted business related topic. In section D, the candidate has one minute to describe a business situation illustrated in graphs or charts, such as pie or bar charts.",
  "In section C the candidates are asked to talk for one minute about a prompted business related topic. In section D, the candidate has one minute to describe a business situation illustrated in graphs or charts, such as pie or bar charts. The prompt for section E asks the candidate to imagine they are in a speci\ufb01c con- versation and to respond to questions they may be asked in that situation (e.g. advice about planning a conference). This section is made up of 5x 20 seconds responses. Each section is scored between 0 and 6; the overall score is therefore between 0 and 30. This score is then mapped into Common European Framework of Reference (CEFR) [29] language pro\ufb01ciency levels, which is an international stan- dard for describing language ability on a six-level scale. Each candidate is \ufb01nally assigned a \u201cgrade\u201d, ranging from minimal (A1) and basic (A2) command, through limited but effective (B1) and generally effective (B2) command, to good opera- tional (C1) and fully operational (C2) command of the spoken language.",
  "In this work, non-native speech from the BULATS test is used as both training and test data for the speaker veri\ufb01cation systems. To investigate how the systems generalise, data for testing is also taken from the Cambridge Assessment English Linguaskill 2 online test. Like BULATS, this is also a multi- level test and has a similar format composed of the same \ufb01ve sections as described before but assesses general English abil- ity. 4. EXPERIMENTAL SETUP A set of 8,480 candidates from BULATS was used for train- ing. The approximately 280 hours of speech covers a wide range of more than 70 different L1s. There are 15 major L1s with more than 100 candidates for each, including Tamil, Gujarati, Hindi, Telugu, Malayalam, Bengali, Spanish, Rus- sian, Kannada, Portuguese, French, etc.",
  "There are 15 major L1s with more than 100 candidates for each, including Tamil, Gujarati, Hindi, Telugu, Malayalam, Bengali, Spanish, Rus- sian, Kannada, Portuguese, French, etc. Data augmentation was applied to the training set, and each recording was pro- cessed with a randomly selected source from \u201cbabble\u201d, \u201cmu- sic\u201d, \u201cnoise\u201d and \u201creverb\u201d [13], which roughly doubled the size of the original training set. Another set of 8,318 BULATS candidates was used as one test set to evaluate the system per- formance. There are 7 major L1s in this set, each of which has more than 100 candidates: Spanish, Thai, Tamil, Arabic, Vietnamese, Polish and Dutch. There are no overlapping can- didates between the BULATS training and test sets. The other test set of 2,540 candidates came from the Linguaskill test, of which there are 6 major L1s each with more than 100 candi- dates: Hindi, Portuguese, Japanese, Spanish, Thai and Viet- namese.",
  "The other test set of 2,540 candidates came from the Linguaskill test, of which there are 6 major L1s each with more than 100 candi- dates: Hindi, Portuguese, Japanese, Spanish, Thai and Viet- namese. Each of the training set and two test sets was fairly gender balanced, with approximately one third of candidates graded as B1, one third graded as B2, and the rest graded as A1, A2, C1, or C2, according to CEFR ability levels. For each test set candidate, responses from sections A and B were used for speaker enrolment (approximately 180s), while the more challenging free-speaking sections C, D, and E were used for whole section-level veri\ufb01cation (approximately 60s for each section). 5. EXPERIMENTAL RESULTS 5.1. Baseline system performance Gender is generally considered an important speaker attribute, and impostor trials were \ufb01rst selected from the same gender group as the reference speaker, as commonly done in stan- dard speaker veri\ufb01cation tasks.",
  "5. EXPERIMENTAL RESULTS 5.1. Baseline system performance Gender is generally considered an important speaker attribute, and impostor trials were \ufb01rst selected from the same gender group as the reference speaker, as commonly done in stan- dard speaker veri\ufb01cation tasks. This resulted in a total of 104.8 million veri\ufb01cation trials for the BULATS test set and 9.7 million trials for the Linguaskill test set. An i-vector/PLDA system and an x-vector/PLDA sys- tem were \ufb01rst trained on the \u201cin-domain\u201d BULATS training set. For the i-vector system, 13-dimensional perceptual lin- ear predictive (PLP) features were extracted using the HTK toolkit [30] with a frame-length of 25ms. A UBM of 2,048 mixture components was \ufb01rst trained with full-covariance matrices, and then 600-dimensional i-vectors were extracted 2https://www.cambridgeenglish.org/exams-and-tests/linguaskill/",
  "for both training and test sets. For the x-vector system, 40- dimensional \ufb01lterbank features were also extracted using HTK with a frame-length of 25ms. DNN con\ufb01gurations were the same as used in [13], and 512-dimensional x-vectors were extracted from the af\ufb01ne component of the segment-level layer immediately following the statistics pooling layer. Performance of the two baseline systems is shown in Ta- ble 1 in terms of equal error rate (EER). The x-vector system yielded lower EERs on both BULATS and Linguaskill test sets. Table 1. % EER performance of BULATS-trained baseline systems on BULATS and Linguaskill test sets.",
  "The x-vector system yielded lower EERs on both BULATS and Linguaskill test sets. Table 1. % EER performance of BULATS-trained baseline systems on BULATS and Linguaskill test sets. System BULATS Linguaskill BULATS i-vector/PLDA 0.69 0.72 BULATS x-vector/PLDA 0.66 0.70 In addition to the models trained on the BULATS data, it is also interesting to investigate the application of \u201cout- of-the-box\u201d models for standard speaker veri\ufb01cation tasks to this non-native speaker veri\ufb01cation task as there is limited amounts of non-native learner English data that is publicly available. In this paper, the Kaldi-released [20] VoxCeleb x- vector/PLDA system was used as imported models, which was trained on augmented VoxCeleb 1 [18] and VoxCeleb 2 [19]. There are more than 7,000 speakers in the VoxCeleb dataset with more than 2,000 hours of audio data, making it the largest publicly available speaker recognition dataset.",
  "There are more than 7,000 speakers in the VoxCeleb dataset with more than 2,000 hours of audio data, making it the largest publicly available speaker recognition dataset. 30 dimensional mel-frequency cepstral coef\ufb01cients (MFCCs) were used as input features and system con\ufb01gurations were the same as the BULATS x-vector/PLDA one. It can be seen from Table 2 that these out-of-domain models gave worse performance than baseline systems trained on a far smaller amount of BULATS data due to domain mismatch. Thus, two kinds of in-domain adaptation strategies were explored to make use of the BULATS training set: PLDA adaptation and x-vector extractor \ufb01ne-tuning. For PLDA adaptation, x- vectors of the BULATS training set were \ufb01rst extracted using the VoxCeleb-trained x-vector extractor, and then employed to adapt the VoxCeleb-trained PLDA model with their mean and variance.",
  "For PLDA adaptation, x- vectors of the BULATS training set were \ufb01rst extracted using the VoxCeleb-trained x-vector extractor, and then employed to adapt the VoxCeleb-trained PLDA model with their mean and variance. For x-vector extractor \ufb01ne-tuning, with all other layers of the VoxCeleb-trained model kept still, the output layer was re-initialised using the BULATS training set with the number of targets adjusted accordingly, and then all lay- ers were \ufb01ne-tuned on the BULATS training set. Here the PLDA adaptation system is referred to as X1 and the extrac- tor \ufb01ne-tuning system is referred to as X2. Both adaptation approaches can yield good performance gains as can be seen from Table 2. PLDA adaptation is a straightforward yet effec- tive way, while the system with x-vector extractor \ufb01ne-tuning gave slightly lower EERs on both BULATS and Linguaskill test sets by virtue of a relatively \u201cin-domain\u201d extractor prior to the PLDA back-end. Table 2. % EER performance of VoxCeleb-based systems on BULATS and Linguaskill test sets.",
  "Table 2. % EER performance of VoxCeleb-based systems on BULATS and Linguaskill test sets. System BULATS Linguaskill VoxCeleb x-vector/PLDA 0.85 1.44 + PLDA adaptation (X1) 0.55 0.62 + Extractor \ufb01ne-tuning (X2) 0.49 0.55 Detection Error Tradeoff (DET) curves of the four x- vector/PLDA systems on the BULATS test set were illus- trated in Figure 2. It can be seen that, both adaptation sys- tems outperformed the original VoxCeleb-trained system in any threshold of the false alarm (FA) probability and the miss (MS) probability. The extractor \ufb01ne-tuning system only gave higher MS probability than the PLDA adapted one with FA probability below 0.4%, while for a large range of FA probabilities above 0.4%, the extractor \ufb01ne-tuning system outperformed the PLDA adapted one.",
  "The extractor \ufb01ne-tuning system only gave higher MS probability than the PLDA adapted one with FA probability below 0.4%, while for a large range of FA probabilities above 0.4%, the extractor \ufb01ne-tuning system outperformed the PLDA adapted one.   0.1   0.2   0.5    1      2      5      10     20     40   False Alarm probability (in %)   0.1    0.2   0.5     1      2      5      10   Miss probability (in %) BULATS x-vector/PLDA VoxCeleb x-vector/PLDA + PLDA adaptation (X1) + Extractor fine-tuning (X2) Fig. 2. DET curves of the four x-vector/PLDA systems on the BULATS test set with impostors from the same gender group as the reference speaker.",
  "2. DET curves of the four x-vector/PLDA systems on the BULATS test set with impostors from the same gender group as the reference speaker. Furthermore, by leveraging the large-scale VoxCeleb dataset, both adaptation systems produced lower EERs than baseline systems solely trained on BULATS data, especially the extractor \ufb01ne-tuning one, which gave a reduction rate of 26% in EER over the baseline x-vector/PLDA system on the BULATS test set. It can also be seen from Figure 2 that, the extractor \ufb01ne-tuning system gave consistently better perfor- mance than the baseline systems for almost any threshold of FA and MS.",
  "5.2. Impostor attributes analysis As mentioned in Section 5.1, gender is an important at- tribute when selecting impostors. For the non-native English speech data considered in this work, there are two additional attributes that may signi\ufb01cantly impact performance, the can- didate speaking ability (grade) and L1. In this section, the impact of both attributes on veri\ufb01cation performance is anal- ysed on the BULATS test set using the extractor \ufb01ne-tuning system (X2) detailed in Section 5.1 with impostors selected from the same gender group as the reference speaker. Taking EER as the operating threshold, both grade and L1 break- down are investigated with respect to the number of impostor trials resulting in false alarm (FA) errors. As there were only a small number of speakers graded as C1 or C2 in the BULATS test set, the two grade groups were merged into one group as C in the following analysis.",
  "As there were only a small number of speakers graded as C1 or C2 in the BULATS test set, the two grade groups were merged into one group as C in the following analysis. Also for a fair comparison, 200 speakers were randomly selected (roughly gender balanced) for each grade group from the BU- LATS test set, and the grade breakdown is shown in Table 3. For lower grades, impostor trials from the grade group of A1 dominated FA errors as A1 speakers tend to speak short utter- ances, which is more challenging for the systems. For higher grades (B2 and C), impostor trials from the grade group of C constituted a larger portion of FA errors probably due to the fact that C speakers tend to speak long utterances in a more \u201cnative\u201d way and they are also similar to B2 speakers. Table 3. Grade breakdown of the percentage of impostor tri- als with FA errors at the operating threshold of EER for the extractor \ufb01ne-tuning system on a subset of the BULATS test set. Grade Grade of Impostor Spkr. Ref. Spkr.",
  "Table 3. Grade breakdown of the percentage of impostor tri- als with FA errors at the operating threshold of EER for the extractor \ufb01ne-tuning system on a subset of the BULATS test set. Grade Grade of Impostor Spkr. Ref. Spkr. A1 A2 B1 B2 C A1 65.8 27.5 5.8 0.3 0.6 A2 60.9 29.9 7.1 0.9 1.3 B1 46.5 26.8 13.1 7.6 5.9 B2 11.4 11.9 19.2 25.9 31.7 C 17.7 12.0 10.3 24.3 35.6 The numbers of speakers from different L1 groups also varied in the BULATS test set. For a fair comparison, 200 speakers were randomly selected (roughly gender balanced) for each of 6 major L1s.",
  "For a fair comparison, 200 speakers were randomly selected (roughly gender balanced) for each of 6 major L1s. The L1 breakdown is shown in Table 4, where impostor trials from the same L1 group as the reference speaker generally dominated FA errors. English learners from the same L1 group tend to have similar accents when speaking English, which makes them more confusable to speaker veri\ufb01cation systems compared to learners from a different L1 group. Particularly, impostors of Thai L1 con- stitute a considerable portion of FA errors for each L1, as A1 and A2 speakers dominate Thai L1 in the BULATS test set, which is different from other L1s where B1 and B2 speakers dominate. Table 4. L1 breakdown of the percentage of impostor trials with FA errors at the operating threshold of EER for the ex- tractor \ufb01ne-tuning system on a subset of the BULATS test set. L1 L1 of Impostor Spkr. Ref. Spkr. Ara. Pol. Spa. Tam. Tha. Vie. Ara.",
  "L1 L1 of Impostor Spkr. Ref. Spkr. Ara. Pol. Spa. Tam. Tha. Vie. Ara. 74.9 0.0 0.3 0.6 14.7 9.5 Pol. 0.0 76.9 1.3 0.3 21.6 0.0 Spa. 2.1 16.5 44.7 0.0 28.2 8.5 Tam. 0.0 1.7 0.3 62.4 33.9 1.7 Tha. 0.5 2.4 0.4 1.0 92.9 2.8 Vie. 1.2 0.1 1.3 0.6 12.7 84.0 5.3. Overall system performance Based on the analysis in the previous section, the impact of speaker attributes beyond gender, the grade and L1, were used as additional restrictions on the imposter set selection.",
  "Overall system performance Based on the analysis in the previous section, the impact of speaker attributes beyond gender, the grade and L1, were used as additional restrictions on the imposter set selection. The following forms of impostor selection were examined: \u2022 gender, impostors from the same gender group as the reference speaker, as in Section 5.1; \u2022 grade, impostors from the same grade group as the ref- erence speaker; \u2022 >grade, impostors from higher grade groups than the reference speaker if the grade of the reference speaker is lower than C, otherwise from C; this case is of practi- cal interest for impersonation in spoken language tests; \u2022 L1, impostors from the same L1 group as the reference speaker; The number of total veri\ufb01cation trials decreases with further restriction on impostors, which is shown in Table 5. Table 6 shows the impact on EER of restricting the possible set of impostors according to gender, L1 or grade on both BULATS and Linguaskill test sets.",
  "Table 6 shows the impact on EER of restricting the possible set of impostors according to gender, L1 or grade on both BULATS and Linguaskill test sets. Due to the lack of data for each L1 or grade, X1 and X2 systems that are adapted or \ufb01ne-tuned on all of the BULATS training set are used for veri\ufb01cation. As expected, restricting possible impostors according to speaker attributes yielded higher EERs as the percentage of impostors \u201cclose\u201d to the reference speaker increased. Take gender as the starting point, which is the con\ufb01guration used in previous experiments in Section 5.1. Further restricting the set of impostors to L1 again increased EERs agreeing with the results shown in Table 4, similarly to grade. An interesting result in terms of handling imper- sonation is that, if the set of impostors is further restricted to >grade, EERs decrease compared to simply restricted to gender. The highest EER for both systems was achieved by restricted to gender+L1+grade, which indicates that all these are important speaker attributes of non-native data.",
  "The highest EER for both systems was achieved by restricted to gender+L1+grade, which indicates that all these are important speaker attributes of non-native data. The gender+L1+>grade case is more related to practical scenar- ios of impersonation, since it is more likely that a candidate",
  "chooses a substitute from the same gender and L1 group but speak the target language better to impersonate him/herself in order to obtain a higher grade in a spoken language test. Table 5. Number of veri\ufb01cation trials (in millions) with dif- ferent restrictions on impostors for both BULATS and Lin- guaskill test sets. Restrictions BULATS Linguaskill gender 104.8 9.7 + grade 31.6 2.7 + >grade 36.9 3.6 + L1 44.3 2.2 + grade 14.1 0.7 + >grade 16.7 0.8 Table 6. % EER performance of two adapted systems with different restrictions on impostors on both BULATS and Lin- guaskill test sets.",
  "% EER performance of two adapted systems with different restrictions on impostors on both BULATS and Lin- guaskill test sets. Restrictions BULATS Linguaskill X1 X2 X1 X2 gender 0.55 0.49 0.62 0.55 + grade 0.60 0.64 0.66 0.64 + >grade 0.45 0.49 0.55 0.49 + L1 0.65 0.71 0.84 0.98 + grade 0.73 0.79 0.92 1.17 + >grade 0.62 0.68 0.79 0.87 For the impersonation scenario where the impostor tri- als are restricted to gender+L1+>grade, the DET curves for all systems including the unadapted VoxCeleb and BULATS trained systems are shown in Figure 3 for the BULATS test set. This allows the overall distribution of FA and MS er- rors for the aforementioned systems to be evaluated.",
  "This allows the overall distribution of FA and MS er- rors for the aforementioned systems to be evaluated. It can be seen that, compared with the \ufb01ne-tuned X2 system, the PLDA-adapted X1 system had a lower MS probability when the FA probability was low and had a higher MS probability when the FA probability was high. This implies that the X1 system tends to accept imposters as reference speakers while the X2 system tends to reject reference speakers as impostors. For malpractice candidate impersonation in spoken language tests, the X2 system may have a high cost as it may incor- rectly identify malpractice in valid candidates. This would require manual checks to con\ufb01rm this classi\ufb01cation. In con- trast, the X1 system may result in a lower level of security be- cause it has a higher chance of misidentifying the candidate who is impersonating another. Based on these complemen- tary trends, a score-level linear combination of the two sys- tems was performed with weights of 0.7 and 0.3 for X1 and X2 systems, respectively.",
  "Based on these complemen- tary trends, a score-level linear combination of the two sys- tems was performed with weights of 0.7 and 0.3 for X1 and X2 systems, respectively. The combination system gave con- sistently better performance for a wide range of FA and MS probabilities than the aforementioned systems with an EER of 0.58% on the BULATS test set, as demonstrated in Figure 3. The same trend was also observed at these weightings on the Linguaskill test set with an EER of 0.72% for the combination system, approximately 8% relative reduction in EER from the X1 system. Thus, the combination of the two adapted systems making use of both large-scale VoxCeleb data and in-domain BULATS data, can serve as a sensible con\ufb01guration for im- personation detection in spoken language tests.",
  "Thus, the combination of the two adapted systems making use of both large-scale VoxCeleb data and in-domain BULATS data, can serve as a sensible con\ufb01guration for im- personation detection in spoken language tests.   0.1   0.2   0.5    1      2      5      10     20     40   False Alarm probability (in %)   0.1    0.2   0.5     1      2      5      10   Miss probability (in %) BULATS x-vector/PLDA VoxCeleb x-vector/PLDA + PLDA adaptation (X1) + Extractor fine-tuning (X2)   + Combination Fig. 3. DET curves of various systems on the BULATS test set with impostor trials selected from the group of the same gender, same L1 and higher grade as/than the reference speaker. 6. CONCLUSIONS This paper has investigated malpractice in the form of can- didate impersonation for spoken language assessment.",
  "6. CONCLUSIONS This paper has investigated malpractice in the form of can- didate impersonation for spoken language assessment. This task has close relationships to standard speaker veri\ufb01cation, but applied to the domain of non-native speech. Advanced neural network based speaker veri\ufb01cation systems were built on both limited non-native spoken English data from the BU- LATS test, and a large standard corpus VoxCeleb. For the con\ufb01guration used all systems yielded relatively low EERs of less than 1%. Though built with only limited data the sys- tems trained on just BULATS systems outperformed the \u201cout- of-the-box\u201d VoxCeleb based system. However by adapting both the PLDA model and the deep speaker representation, the VoxCeleb-based systems could yield lower EERs. The at- tributes of the \u201cimpostors\u201d was then analysed in terms of both the impostor\u2019s grade and L1. As expected, L1 was the most important attribute of the impostor selected, though the grade did also in\ufb02uence performance.",
  "The at- tributes of the \u201cimpostors\u201d was then analysed in terms of both the impostor\u2019s grade and L1. As expected, L1 was the most important attribute of the impostor selected, though the grade did also in\ufb02uence performance. With the most likely scenario of impersonation by restricting impostors to be from the same gender, same L1, and higher grade group, the combination of the two adapted systems gave consistently better performance for a wide range of FA and MS probabilities, making it a sen- sible con\ufb01guration for impersonation detection.",
  "7. REFERENCES [1] K. Zechner, D. Higgins, X. Xi, and D. M. Williamson, \u201cAutomatic scoring of non-native spontaneous speech in tests of spoken English,\u201d Speech Communication, vol. 51, no. 10, pp. 883\u2013895, 2009. [2] S. M. Witt and S. J. Young, \u201cPhone-level pronunciation scoring and assessment for interactive language learn- ing,\u201d Speech Communication, vol. 30, no. 2, pp. 95\u2013108, 2000. [3] A. Metallinou and J. Cheng, \u201cUsing deep neural net- works to improve pro\ufb01ciency assessment for children English language learners,\u201d in Proc. Interspeech, 2014, pp. 1468\u20131472. [4] Y. Wang, M. J. F. Gales, K. M. Knill, K. Kyriakipoulos, A. Malinin, R. C. van Dalen, and M. Rashid, \u201cTowards automatic assessment of spontaneous spoken English,\u201d Speech Communication, vol. 104, pp.",
  "104, pp. 47 \u2013 56, 2018. [5] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet, \u201cFront-end factor analysis for speaker veri\ufb01- cation,\u201d IEEE Transactions on Audio, Speech, and Lan- guage Processing, vol. 19, no. 4, pp. 788\u2013798, 2011. [6] S. J. D. Prince and J. H. Elder, \u201cProbabilistic lin- ear discriminant analysis for inferences about identity,\u201d in IEEE International Conference on Computer Vision, 2007, pp. 1\u20138. [7] P. Kenny, \u201cBayesian speaker veri\ufb01cation with heavy- tailed priors.,\u201d in Proc. Odyssey: Speaker and Language Recognition Workshop, 2010, vol. 14. [8] D. Garcia-Romero and C. Y. Espy-Wilson, \u201cAnalysis of i-vector length normalization in speaker recognition systems,\u201d in Proc. Interspeech, 2011, pp. 249\u2013252.",
  "14. [8] D. Garcia-Romero and C. Y. Espy-Wilson, \u201cAnalysis of i-vector length normalization in speaker recognition systems,\u201d in Proc. Interspeech, 2011, pp. 249\u2013252. [9] D. Garcia-Romero, X. Zhou, and C. Y. Espy-Wilson, \u201cMulticondition training of gaussian PLDA models in i- vector space for noise and reverberation robust speaker recognition,\u201d in Proc. ICASSP, 2012, pp. 4257\u20134260. [10] E. Variani, X. Lei, E. McDermott, I. L. Moreno, and J. Gonzalez-Dominguez, \u201cDeep neural networks for small footprint text-dependent speaker veri\ufb01cation,\u201d in Proc. ICASSP, 2014, pp. 4052\u20134056.",
  "ICASSP, 2014, pp. 4052\u20134056. [11] D. Snyder, P. Ghahremani, D. Povey, D. Garcia- Romero, Y. Carmiel, and S. Khudanpur, \u201cDeep neu- ral network-based speaker embeddings for end-to-end speaker veri\ufb01cation,\u201d in Proc. SLT Workshop, 2016, pp. 165\u2013170. [12] D. Snyder, D. Garcia-Romero, D. Povey, and S. Khu- danpur, \u201cDeep neural network embeddings for text- independent speaker veri\ufb01cation.,\u201d in Proc. Interspeech, 2017, pp. 999\u20131003. [13] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur, \u201cX-vectors: Robust DNN embeddings for speaker recognition,\u201d in Proc. ICASSP, 2018, pp. 5329\u20135333.",
  "ICASSP, 2018, pp. 5329\u20135333. [14] E. Shriberg, L. Ferrer, S. Kajarekar, N. Scheffer, A. Stol- cke, and M. Akbacak, \u201cDetecting nonnative speech us- ing speaker recognition approaches.,\u201d in Odyssey: The Speaker and Language Recognition Workshop, 2008. [15] B. Tan, Q. Li, and R. Foresta, \u201cAn automatic non- native speaker recognition system,\u201d in IEEE Interna- tional Conference on Technologies for Homeland Secu- rity (HST), 2010, pp. 77\u201383. [16] M. K. Omar and J. Pelecanos, \u201cA novel approach to de- tecting non-native speakers and their native language,\u201d in Pro. ICASSP, 2010, pp. 4398\u20134401. [17] Y. Qian et al., \u201cSelf-Adaptive DNN for Improving Spo- ken Language Pro\ufb01ciency Assessment,\u201d in Proc. Inter- speech, 2016.",
  "ICASSP, 2010, pp. 4398\u20134401. [17] Y. Qian et al., \u201cSelf-Adaptive DNN for Improving Spo- ken Language Pro\ufb01ciency Assessment,\u201d in Proc. Inter- speech, 2016. [18] A. Nagrani, J. S. Chung, and A. Zisserman, \u201cVoxCeleb: A large-scale speaker identi\ufb01cation dataset,\u201d in Proc. Interspeech, 2017, pp. 2616\u20132620. [19] J. S. Chung, A. Nagrani, and A. Zisserman, \u201cVox- Celeb2: Deep speaker recognition,\u201d in Proc. Inter- speech, 2018, pp. 1086\u20131090.",
  "2616\u20132620. [19] J. S. Chung, A. Nagrani, and A. Zisserman, \u201cVox- Celeb2: Deep speaker recognition,\u201d in Proc. Inter- speech, 2018, pp. 1086\u20131090. [20] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, G. Nagendra, M. Hannemann, P. Motl\u00b4\u0131\u02c7cek, Y. Qian, P. Schwarz, J. Silovsk\u00b4y, G. Stemmer, and K. Vesel\u00b4y, \u201cThe Kaldi speech recognition toolkit,\u201d in Proc. ASRU Workshop, 2011, pp. 4141\u20134144. [21] G. Heigold, I. Moreno, S. Bengio, and N. Shazeer, \u201cEnd-to-end text-dependent speaker veri\ufb01cation,\u201d in Proc. ICASSP, 2016, pp. 5115\u20135119.",
  "[21] G. Heigold, I. Moreno, S. Bengio, and N. Shazeer, \u201cEnd-to-end text-dependent speaker veri\ufb01cation,\u201d in Proc. ICASSP, 2016, pp. 5115\u20135119. [22] N. Br\u00a8ummer and E. De Villiers, \u201cThe speaker partition- ing problem.,\u201d in Proc. Odyssey: Speaker and Language Recognition Workshop, 2010. [23] S. Ioffe, \u201cProbabilistic linear discriminant analysis,\u201d in European Conference on Computer Vision. Springer, 2006, pp. 531\u2013542. [24] D. Garcia-Romero, X. Zhang, A. McCree, and D. Povey, \u201cImproving speaker recognition performance in the do- main adaptation challenge using deep neural networks,\u201d in Proc. SLT Workshop, 2014, pp. 378\u2013383.",
  "[25] D. Garcia-Romero, A. McCree, S. Shum, N. Br\u00a8ummer, and C. Vaquero, \u201cUnsupervised domain adaptation for i-vector speaker recognition,\u201d in Proc. Odyssey: The Speaker and Language Recognition Workshop, 2014. [26] J. Villalba and E. Lleida, \u201cUnsupervised adaptation of PLDA by using variational Bayes methods,\u201d in Proc. ICASSP, 2014, pp. 744\u2013748. [27] D. Garcia-Romero and A. McCree, \u201cSupervised domain adaptation for i-vector based speaker recognition,\u201d in Proc. ICASSP, 2014, pp. 4047\u20134051. [28] L. Chambers and K. Ingham, \u201cThe BULATS online speaking test,\u201d Research Notes, vol. 43, pp. 21\u201325, 2011. [29] Council of Europe, Common European Framework of Reference for Languages: learning, teaching, assess- ment, Cambridge University Press, 2001.",
  "43, pp. 21\u201325, 2011. [29] Council of Europe, Common European Framework of Reference for Languages: learning, teaching, assess- ment, Cambridge University Press, 2001. [30] S. Young, G. Evermann, M. J. F. Gales, T. Hain, D. Ker- shaw, X. Liu, G. Moore, J. Odell, D. Ollason, D. Povey, A. Ragni, V. Valtchev, P. C. Woodland, and C. Zhang, The HTK book (for HTK version 3.5), University of Cambridge, 2015."
]