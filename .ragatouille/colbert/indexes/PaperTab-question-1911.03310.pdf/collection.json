[
  "How Language-Neutral is Multilingual BERT? Jind\u02c7rich Libovick\u00b4y1 and Rudolf Rosa2 and Alexander Fraser1 1Center for Information and Language Processing, LMU Munich, Germany 2Faculty of Mathematics and Physics, Charles University, Prague, Czech Republic {libovicky, fraser}@cis.lmu.de rosa@ufal.mff.cuni.cz Abstract Multilingual BERT (mBERT) provides sen- tence representations for 104 languages, which are useful for many multi-lingual tasks. Pre- vious work probed the cross-linguality of mBERT using zero-shot transfer learning on morphological and syntactic tasks. We instead focus on the semantic properties of mBERT. We show that mBERT representations can be split into a language-speci\ufb01c component and a language-neutral component, and that the language-neutral component is suf\ufb01ciently general in terms of modeling semantics to al- low high-accuracy word-alignment and sen- tence retrieval but is not yet good enough for the more dif\ufb01cult task of MT quality estima- tion.",
  "Our work presents interesting challenges which must be solved to build better language- neutral representations, particularly for tasks requiring linguistic transfer of semantics. 1 Introduction Multilingual BERT (mBERT; Devlin et al. 2019) is gaining popularity as a contextual representa- tion for various multilingual tasks, such as de- pendency parsing (Kondratyuk and Straka, 2019; Wang et al., 2019), cross-lingual natural lan- guage inference (XNLI) or named-entity recogni- tion (NER) (Pires et al., 2019; Wu and Dredze, 2019; Kudugunta et al., 2019). Pires et al. (2019) present an exploratory paper showing that mBERT can be used cross-lingually for zero-shot transfer in morphological and syn- tactic tasks, at least for typologically similar lan- guages. They also study an interesting semantic task, sentence-retrieval, with promising initial re- sults. Their work leaves many open questions in terms of how good the cross-lingual mBERT rep- resentation is for semantics, motivating our work.",
  "They also study an interesting semantic task, sentence-retrieval, with promising initial re- sults. Their work leaves many open questions in terms of how good the cross-lingual mBERT rep- resentation is for semantics, motivating our work. In this paper, we directly assess the seman- tic cross-lingual properties of mBERT. To avoid methodological issues with zero-shot transfer (possible language over\ufb01tting, hyper-parameter tuning), we selected tasks that only involve a direct comparison of the representations: cross-lingual sentence retrieval, word alignment, and machine translation quality estimation (MT QE). Addition- ally, we explore how the language is represented in the embeddings by training language identi\ufb01- cation classi\ufb01ers and assessing how the represen- tation similarity corresponds to phylogenetic lan- guage families. Our results show that the mBERT representa- tions, even after language-agnostic \ufb01ne-tuning, are not very language-neutral. However, the identity of the language can be approximated as a constant shift in the representation space.",
  "Our results show that the mBERT representa- tions, even after language-agnostic \ufb01ne-tuning, are not very language-neutral. However, the identity of the language can be approximated as a constant shift in the representation space. An even higher language-neutrality can still be achieved by a lin- ear projection \ufb01tted on a small amount of parallel data. Finally, we present attempts to strengthen the language-neutral component via \ufb01ne-tuning: \ufb01rst, for multi-lingual syntactic and morphological analysis; second, towards language identity re- moval via a adversarial classi\ufb01er. 2 Related Work Since the publication of mBERT (Devlin et al., 2019), many positive experimental results were published. Wang et al. (2019) reached impressive results in zero-shot dependency parsing. However, the representation used for the parser was a bilingual projection of the contextual embeddings based on word-alignment trained on parallel data. Pires et al.",
  "Wang et al. (2019) reached impressive results in zero-shot dependency parsing. However, the representation used for the parser was a bilingual projection of the contextual embeddings based on word-alignment trained on parallel data. Pires et al. (2019) recently examined the cross- lingual properties of mBERT on zero-shot NER and part-of-speech (POS) tagging but the success of zero-shot transfer strongly depends on how ty- pologically similar the languages are. Similarly, arXiv:1911.03310v1  [cs.CL]  8 Nov 2019",
  "Wu and Dredze (2019) trained good multilingual models for POS tagging, NER, and XNLI, but struggled to achieve good results in the zero-shot setup. Pires et al. (2019) assessed mBERT on cross- lingual sentence retrieval between three language pairs. They observed that if they subtract the aver- age difference between the embeddings from the target language representation, the retrieval accu- racy signi\ufb01cantly increases. We systematically study this idea in the later sections. Many experiments show (Wu and Dredze, 2019; Kudugunta et al., 2019; Kondratyuk and Straka, 2019) that downstream task models can extract relevant features from the multilingual represen- tations. But these results do not directly show language-neutrality, i.e., to what extent are similar phenomena are represented similarly across lan- guages. The models can obtain the task-speci\ufb01c information based on the knowledge of the lan- guage, which (as we show later) can be easily identi\ufb01ed.",
  "The models can obtain the task-speci\ufb01c information based on the knowledge of the lan- guage, which (as we show later) can be easily identi\ufb01ed. Our choice of evaluation tasks elimi- nates this risk by directly comparing the represen- tations. Limited success in zero-shot setups and the need for explicit bilingual projection in order to work well (Pires et al., 2019; Wu and Dredze, 2019; R\u00a8onnqvist et al., 2019) also shows limited language neutrality of mBERT. 3 Centering mBERT Representations Following Pires et al. (2019), we hypothesize that a sentence representation in mBERT is composed of a language-speci\ufb01c component, which identi- \ufb01es the language of the sentence, and a language- neutral component, which captures the meaning of the sentence in a language-independent way. We assume that the language-speci\ufb01c component is similar across all sentences in the language.",
  "We assume that the language-speci\ufb01c component is similar across all sentences in the language. We thus try to remove the language-speci\ufb01c in- formation from the representations by centering the representations of sentences in each language so that their average lies at the origin of the vector space. We do this by estimating the language cen- troid as the mean of the mBERT representations for a set of sentences in that language and sub- tracting the language centroid from the contextual embeddings. We then analyze the semantic properties of both the original and the centered representations us- ing a range of probing tasks. For all tasks, we test all layers of the model. For tasks utilizing a single-vector sentence representation, we test both the vector corresponding to the [cls] token and mean-pooled states. 4 Probing Tasks We employ \ufb01ve probing tasks to evaluate the lan- guage neutrality of the representations. Language Identi\ufb01cation. With a representation that captures all phenomena in a language-neutral way, it should be dif\ufb01cult to determine what lan- guage the sentence is written in.",
  "Language Identi\ufb01cation. With a representation that captures all phenomena in a language-neutral way, it should be dif\ufb01cult to determine what lan- guage the sentence is written in. Unlike other tasks, language identi\ufb01cation does require \ufb01tting a classi\ufb01er. We train a linear classi\ufb01er on top of a sentence representation to try to classify the lan- guage of the sentence. Language Similarity. Experiments with POS tagging (Pires et al., 2019) suggest that similar lan- guages tend to get similar representations on av- erage. We quantify that observation by measur- ing how languages tend to cluster by the language families using V-measure over hierarchical clus- tering of the language centeroid (Rosenberg and Hirschberg, 2007). Parallel Sentence Retrieval. For each sentence in a multi-parallel corpus, we compute the cosine distance of its representation with representations of all sentences on the parallel side of the corpus and select the sentence with the smallest distance. Besides the plain and centered [cls] and mean- pooled representations, we evaluate explicit pro- jection into the \u201cEnglish space\u201d.",
  "Besides the plain and centered [cls] and mean- pooled representations, we evaluate explicit pro- jection into the \u201cEnglish space\u201d. For each lan- guage, we \ufb01t a linear regression projecting the rep- resentations into English representation space us- ing a small set of parallel sentences. Word Alignment. While sentence retrieval could be done with keyword spotting, comput- ing bilingual alignment requires resolving detailed correspondence on the word level. We \ufb01nd the word alignment as a minimum weighted edge cover of a bipartite graph. The graph connects the tokens of the sentences in the two languages and edges between them are weighted with the cosine distance of the token representation. Tokens that get split into multi- ple subwords are represented using the average of the embeddings of the subwords. Note that this algorithm is invariant to representation centering which would only change the edge weights by a constant offset.",
  "We evaluate the alignment using the F1 score over both sure and possible alignment links in a manually aligned gold standard. MT Quality Estimation. MT QE assesses the quality of an MT system output without having ac- cess to a reference translation. The standard evaluation metric is the correla- tion with the Human-targeted Translation Error Rate which is the number of edit operations a hu- man translator would need to do to correct the sys- tem output. This is a more challenging task than the two previous ones because it requires captur- ing more \ufb01ne-grained differences in meaning. We evaluate how cosine distance of the repre- sentation of the source sentence and of the MT output re\ufb02ects the translation quality. In addition to plain and centered representations, we also test trained bilingual projection, and a fully supervised regression trained on training data. 5 Experimental Setup We use a pre-trained mBERT model that was made public with the BERT release1. The model dimen- sion is 768, hidden layer dimension 3072, self- attention uses 12 heads, the model has 12 layers.",
  "5 Experimental Setup We use a pre-trained mBERT model that was made public with the BERT release1. The model dimen- sion is 768, hidden layer dimension 3072, self- attention uses 12 heads, the model has 12 layers. It uses a vocabulary of 120k wordpieces that is shared for all languages. To train the language identi\ufb01cation classi\ufb01er, for each of the BERT languages we randomly se- lected 110k sentences of at least 20 characters from Wikipedia, and keep 5k for validation and 5k for testing for each language. The training data are also used for estimating the language centroids. For parallel sentence retrieval, we use a multi- parallel corpus of test data from the WMT14 eval- uation campaign (Bojar et al., 2014) with 3,000 sentences in Czech, English, French, German, Hindi, and Russian. The linear projection exper- iment uses the WMT14 development data. We use manually annotated word alignment datasets to evaluate word alignment between En- glish on one side and Czech (2.5k sent.",
  "The linear projection exper- iment uses the WMT14 development data. We use manually annotated word alignment datasets to evaluate word alignment between En- glish on one side and Czech (2.5k sent.; Mare\u02c7cek, 2016), Swedish (192 sent.; Holmqvist and Ahren- berg, 2011), German (508 sent.), French (447 sent.; Och and Ney, 2000) and Romanian (248 sent.; Mihalcea and Pedersen, 2003) on the other side. We compare the results with FastAlign (Dyer et al., 2013) that was provided with 1M ad- ditional parallel sentences from ParaCrawl (Espl`a 1https://github.com/google-research/bert mBERT UDify lng-free [cls] .935 .938 .796 [cls], cent. .867 .851 .337 mean-pool .919 .896 .230 mean-pool, cent. .285 .243 .247 Table 1: Accuracy of language identi\ufb01cation, values from the best-scoring layers.",
  ".867 .851 .337 mean-pool .919 .896 .230 mean-pool, cent. .285 .243 .247 Table 1: Accuracy of language identi\ufb01cation, values from the best-scoring layers. Afrikaans Albanian Arabic Aragonese Armenian Asturian Azerbaijani Bashkir Basque Bavarian Belarusian Bengali Bishnupriya Manipuri Bosnian Breton Bulgarian Burmese Catalan Cebuano Chechen Chinese Chuvash Croatian Czech Danish Dutch English Estonian Finnish French Galician Georgian German Greek Gujarati Haitian Hebrew Hindi Hungarian Icelandic Ido Indonesian Irish Italian Japanese Javanese Kannada Kazakh Kirghiz Korean Latin Latvian Lithuanian Lombard Low Saxon Luxembourgish Macedonian Malagasy Malay Malayalam Marathi Minangkabau Nepali Newar Norwegian (Bokmal) Norwegian (Nynorsk) Occitan Persian (Farsi) Piedmontese Polish Portuguese Punjabi Romanian Russian Scots Serbian Serbo-Croatian Sicilian Slovak Slovenian South Azerbaijani Spanish Sundanese Swahili Swedish Tagalog Tajik Tamil Tatar Telugu Turkish Ukrainian Urdu Uzbek Vietnamese Volap\u00a8uk Waray-Waray Welsh West Frisian Western Punjabi Yoruba Romance Germanic Slavic Turkic Indic Celtic S.",
  "Dravidian Semitic Figure 1: Language centroids of the mean-pooled rep- resentations from the 8th layer of cased mBERT on a tSNE plot with highlighted language families. et al., 2019) in addition to the test data. For MT QE, we use English-German data pro- vided for the WMT19 QE Shared Task (Fonseca et al., 2019) consisting training and test data with source senteces, their automatic translations, and manually corrections. 6 Results Language Identi\ufb01cation. Table 1 shows that centering the sentence representations consider- ably decreases the accuracy of language identi\ufb01- cation, especially in the case of mean-pooled em- beddings. This indicates that the proposed center- ing procedure does indeed remove the language- speci\ufb01c information to a great extent. Language Similarity. Figure 1 is a tSNE plot (Maaten and Hinton, 2008) of the language cen- troids, showing that the similarity of the centroids tends to correspond to the similarity of the lan-",
  "cased uncased UDify lng-free random 82.42 82.09 80.03 80.59 62.14 Table 2: V-Measure for hierarchical clustering of lan- guage centroids and grouping languages into genealog- ical families for families with at least three languages covered by mBERT. mBERT UDify lng-free [cls] .639 .462 .549 [cls], cent. .684 .660 .686 [cls], proj. .915 .933 .697 mean-pool .776 .314 .755 mean-pool, cent. .838 .564 .828 mean-pool, proj. .983 .906 .983 Table 3: Average accuracy for sentence retrieval over all 30 language pairs. guages. Table 2 con\ufb01rms that the hierarchical clustering of the language centroids mostly corre- sponds to the language families. Parallel Sentence Retrieval. Results in Table 3 reveal that the representation centering dramat- ically improves the retrieval accuracy, showing that it makes the representations more language- neutral.",
  "Parallel Sentence Retrieval. Results in Table 3 reveal that the representation centering dramat- ically improves the retrieval accuracy, showing that it makes the representations more language- neutral. However, an explicitly learned projection of the representations leads to a much greater im- provement, reaching a close-to-perfect accuracy, even though the projection was \ufb01tted on relatively small parallel data. The accuracy is higher for mean-pooled states than for the [cls] embedding and varies according to the layer of mBERT used (see Figure 2). Word Alignment. Table 4 shows that word- alignment based on mBERT representations sur- passes the outputs of the standard FastAlign tool even if it was provided large parallel corpus. This suggests that word-level semantics are well cap- tured by mBERT contextual embeddings. For this task, learning an explicit projection had a negligi- ble effect on the performance.2 MT Quality Estimation. Qualitative results of MT QE are tabulated in Table 5. Unlike sentence retrieval, QE is more sensitive to subtle differences 2We used an expectation-maximization approach that al- ternately aligned the words and learned a linear projection between the representations.",
  "Qualitative results of MT QE are tabulated in Table 5. Unlike sentence retrieval, QE is more sensitive to subtle differences 2We used an expectation-maximization approach that al- ternately aligned the words and learned a linear projection between the representations. This algorithm only brings a negligible improvement of .005 F1 points. 1 2 3 4 5 6 7 8 9 10 11 12 Layer 0 25 50 75 100 Retrieval accuracy % plain centered projected Figure 2: Accuracy of sentence retrieval for mean- pooled contextual embeddings from BERT layers. en- FastAlign mBERT UDify lng-free cs .692 .738 .708 .744 sv .438 .478 .459 .468 de .471 .767 .731 .768 fr .583 .612 .581 .607 ro .690 .703 .696 .704 Table 4: Maximum F1 score for word alignment across layers compared with FastAlign baseline. between sentences. Measuring the distance of the non-centered sentence vectors does not correlate with translation quality at all.",
  "between sentences. Measuring the distance of the non-centered sentence vectors does not correlate with translation quality at all. Centering or explicit projection only leads to a mild correlation, much lower than a supervisedly trained regression;3and even better performance is possible (Fonseca et al., 2019). The results show that the linear projection between the representations only captures a rough semantic correspondence, which does not seem to be suf\ufb01cient for QE, where the most indicative fea- ture appears to be sentence complexity. 7 Fine-tuning mBERT We also considered model \ufb01ne-tuning towards stronger language neutrality. We evaluate two \ufb01ne-tuned versions of mBERT: UDify, tuned for a multi-lingual dependency parser, and lng-free, tuned to jettison the language-speci\ufb01c information from the representations.",
  "We evaluate two \ufb01ne-tuned versions of mBERT: UDify, tuned for a multi-lingual dependency parser, and lng-free, tuned to jettison the language-speci\ufb01c information from the representations. 7.1 UDify The UDify model (Kondratyuk and Straka, 2019) uses mBERT to train a single model for depen- dency parsing and morphological analysis of 75 3Supervised regression using either only the source or only MT output also shows a respectable correlation, which implies that structural features of the sentences are more use- ful than the comparison of the source sentence with MT out- put.",
  "BERT cente- glob. supervised red proj. src MT both cased .005 .163 .362 .352 .419 uncased .027 .204 .367 .390 .425 UDify .039 .167 .368 .375 .413 lng-free .026 .136 .349 .343 .411 Table 5: Correlation of estimated MT quality with HTER for English-to-German translation on WMT19 data. 1 2 3 4 5 6 7 8 9 10 11 12 Layer 0 25 50 75 100 Accuracy % cased UDify lng-free English Figure 3: Language ID accuracy for different layers of mBERT. languages. During the parser training, mBERT is \ufb01ne-tuned, which improves the parser accuracy. Results on zero-shot parsing suggest that the \ufb01ne- tuning leads to more cross-lingual representations with respect to morphology and syntax. However, our analyses show that \ufb01ne-tuning mBERT for multilingual dependency parsing does not remove the language identity information from the representations and actually makes the repre- sentations less semantically cross-lingual.",
  "However, our analyses show that \ufb01ne-tuning mBERT for multilingual dependency parsing does not remove the language identity information from the representations and actually makes the repre- sentations less semantically cross-lingual. 7.2 lng-free In this experiment, we try to make the representa- tions more language-neutral by removing the lan- guage identity from the model using an adversar- ial approach. We continue training mBERT in a multi-task learning setup with the masked LM ob- jective with the same sampling procedure (Devlin et al., 2019) jointly with adversarial language ID classi\ufb01ers (Elazar and Goldberg, 2018). For each layer, we train one classi\ufb01er for the [cls] token and one for the mean-pooled hidden states with the gradient reversal layer (Ganin and Lempitsky, 2015) between mBERT and the classi\ufb01er.",
  "For each layer, we train one classi\ufb01er for the [cls] token and one for the mean-pooled hidden states with the gradient reversal layer (Ganin and Lempitsky, 2015) between mBERT and the classi\ufb01er. The results reveal that the adversarial removal of language information succeeds in dramatically decreasing the accuracy of the language identi\ufb01ca- tion classi\ufb01er; the effect is strongest in deeper lay- ers for which the standard mBERT tend to perform better (see Figure 3). However, other tasksare not affected by the adversarial \ufb01ne-tuning. 8 Conclusions Using a set of semantically oriented tasks that re- quire explicit semantic cross-lingual representa- tions, we showed that mBERT contextual embed- dings do not represent similar semantic phenom- ena similarly and therefore they are not directly usable for zero-shot cross-lingual tasks. Contextual embeddings of mBERT capture sim- ilarities between languages and cluster the lan- guages by their families. Neither cross-lingual \ufb01ne-tuning nor adversarial language identity re- moval breaks this property.",
  "Contextual embeddings of mBERT capture sim- ilarities between languages and cluster the lan- guages by their families. Neither cross-lingual \ufb01ne-tuning nor adversarial language identity re- moval breaks this property. A part of language information is encoded by the position in the em- bedding space, thus a certain degree of cross- linguality can be achieved by centering the repre- sentations for each language. Exploiting this prop- erty allows a good cross-lingual sentence retrieval performance and bilingual word alignment (which is invariant to the shift). A good cross-lingual rep- resentation can be achieved by \ufb01tting a supervised projection on a small parallel corpus. References Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ale\u02c7s Tamchyna. 2014. Findings of the 2014 workshop on statistical machine translation.",
  "2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12\u201358, Baltimore, Maryland, USA. Associa- tion for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics. Chris Dyer, Victor Chahuneau, and Noah A. Smith. 2013. A simple, fast, and effective reparameter- ization of IBM model 2. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, pages 644\u2013648, At- lanta, Georgia. Association for Computational Lin- guistics.",
  "Yanai Elazar and Yoav Goldberg. 2018. Adversarial removal of demographic attributes from text data. In Proceedings of the 2018 Conference on Empiri- cal Methods in Natural Language Processing, pages 11\u201321, Brussels, Belgium. Association for Compu- tational Linguistics. Miquel Espl`a, Mikel Forcada, Gema Ram\u00b4\u0131rez-S\u00b4anchez, and Hieu Hoang. 2019. ParaCrawl: Web-scale par- allel corpora for the languages of the EU. In Pro- ceedings of Machine Translation Summit XVII Vol- ume 2: Translator, Project and User Tracks, pages 118\u2013119, Dublin, Ireland. European Association for Machine Translation. Erick Fonseca, Lisa Yankovskaya, Andr\u00b4e F. T. Martins, Mark Fishel, and Christian Federmann. 2019. Find- ings of the WMT 2019 shared tasks on quality es- timation.",
  "European Association for Machine Translation. Erick Fonseca, Lisa Yankovskaya, Andr\u00b4e F. T. Martins, Mark Fishel, and Christian Federmann. 2019. Find- ings of the WMT 2019 shared tasks on quality es- timation. In Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Pa- pers, Day 2), pages 1\u201310, Florence, Italy. Associa- tion for Computational Linguistics. Yaroslav Ganin and Victor Lempitsky. 2015. Unsu- pervised domain adaptation by backpropagation. In Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 1180\u20131189, Lille, France. PMLR. Maria Holmqvist and Lars Ahrenberg. 2011. A gold standard for English-Swedish word alignment. In Proceedings of the 18th Nordic Conference of Com- putational Linguistics (NODALIDA 2011), pages 106\u2013113, Riga, Latvia.",
  "2011. A gold standard for English-Swedish word alignment. In Proceedings of the 18th Nordic Conference of Com- putational Linguistics (NODALIDA 2011), pages 106\u2013113, Riga, Latvia. Northern European Associa- tion for Language Technology (NEALT). Dan Kondratyuk and Milan Straka. 2019. 75 lan- guages, 1 model: Parsing universal dependencies universally. In Proceedings of the 2019 Confer- ence on Empirical Methods in Natural Language Processing and the 9th International Joint Confer- ence on Natural Language Processing (EMNLP- IJCNLP), pages 2779\u20132795, Hong Kong, China. As- sociation for Computational Linguistics. Sneha Kudugunta, Ankur Bapna, Isaac Caswell, and Orhan Firat. 2019. Investigating multilingual NMT representations at scale.",
  "As- sociation for Computational Linguistics. Sneha Kudugunta, Ankur Bapna, Isaac Caswell, and Orhan Firat. 2019. Investigating multilingual NMT representations at scale. In Proceedings of the 2019 Conference on Empirical Methods in Natu- ral Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1565\u20131575, Hong Kong, China. Association for Computational Linguistics. Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579\u20132605. David Mare\u02c7cek. 2016. Czech-english manual word alignment. LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics ( \u00b4UFAL), Faculty of Mathematics and Physics, Charles Uni- versity. Rada Mihalcea and Ted Pedersen. 2003. An evalua- tion exercise for word alignment.",
  "LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics ( \u00b4UFAL), Faculty of Mathematics and Physics, Charles Uni- versity. Rada Mihalcea and Ted Pedersen. 2003. An evalua- tion exercise for word alignment. In Proceedings of the HLT-NAACL 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Trans- lation and Beyond, pages 1\u201310. Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the Association for Com- putational Linguistics, pages 440\u2013447, Hong Kong. Association for Computational Linguistics. Telmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual BERT? In Pro- ceedings of the 57th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 4996\u2013 5001, Florence, Italy. Association for Computa- tional Linguistics.",
  "2019. How multilingual is multilingual BERT? In Pro- ceedings of the 57th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 4996\u2013 5001, Florence, Italy. Association for Computa- tional Linguistics. Samuel R\u00a8onnqvist, Jenna Kanerva, Tapio Salakoski, and Filip Ginter. 2019. Is multilingual BERT \ufb02u- ent in language generation? In Proceedings of the First NLPL Workshop on Deep Learning for Natural Language Processing, pages 29\u201336, Turku, Finland. Link\u00a8oping University Electronic Press. Andrew Rosenberg and Julia Hirschberg. 2007. V- measure: A conditional entropy-based external clus- ter evaluation measure. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 410\u2013 420, Prague, Czech Republic. Association for Com- putational Linguistics. Yuxuan Wang, Wanxiang Che, Jiang Guo, Yijia Liu, and Ting Liu. 2019.",
  "Association for Com- putational Linguistics. Yuxuan Wang, Wanxiang Che, Jiang Guo, Yijia Liu, and Ting Liu. 2019. Cross-lingual BERT trans- formation for zero-shot dependency parsing. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 5725\u2013 5731, Hong Kong, China. Association for Computa- tional Linguistics. Shijie Wu and Mark Dredze. 2019. Beto, bentz, be- cas: The surprising cross-lingual effectiveness of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 833\u2013844, Hong Kong, China. Association for Com- putational Linguistics."
]