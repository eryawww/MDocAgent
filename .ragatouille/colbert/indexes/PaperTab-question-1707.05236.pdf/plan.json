{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Arti\ufb01cial Error Generation with Machine Translation and Syntactic Patterns Marek Rei Mariano Felice Zheng Yuan Ted Briscoe The ALTA Institute Computer Laboratory University of Cambridge United Kingdom firstname.lastname@cl.cam.ac.uk Abstract Shortage of available training data is hold- ing back progress in the area of auto- mated error detection. This paper inves- tigates two alternative methods for arti\ufb01- cially generating writing errors, in order to create additional resources. We pro- pose treating error generation as a machine translation task, where grammatically cor- rect text is translated to contain errors. In addition, we explore a system for extract- ing textual patterns from an annotated cor- pus, which can then be used to insert er- rors into grammatically correct sentences. Our experiments show that the inclusion of arti\ufb01cially generated errors signi\ufb01cantly improves error detection accuracy on both FCE and CoNLL 2014 datasets. 1 Introduction Writing errors can occur in many different forms \u2013 from relatively simple punctuation and determiner errors, to mistakes including word tense and form, incorrect collocations and erroneous idioms.",
            "1 Introduction Writing errors can occur in many different forms \u2013 from relatively simple punctuation and determiner errors, to mistakes including word tense and form, incorrect collocations and erroneous idioms. Au- tomatically identifying all of these errors is a chal- lenging task, especially as the amount of avail- able annotated data is very limited. Rei and Yan- nakoudakis (2016) showed that while some error detection algorithms perform better than others, it is additional training data that has the biggest im- pact on improving performance. Being able to generate realistic arti\ufb01cial data would allow for any grammatically correct text to be transformed into annotated examples contain- ing writing errors, producing large amounts of ad- ditional training examples. Supervised error gen- eration systems would also provide an ef\ufb01cient method for anonymising the source corpus \u2013 er- ror statistics from a private corpus can be aggre- gated and applied to a different target text, obscur- ing sensitive information in the original examina- tion scripts.",
            "Supervised error gen- eration systems would also provide an ef\ufb01cient method for anonymising the source corpus \u2013 er- ror statistics from a private corpus can be aggre- gated and applied to a different target text, obscur- ing sensitive information in the original examina- tion scripts. However, the task of creating incor- rect data is somewhat more dif\ufb01cult than might initially appear \u2013 naive methods for error genera- tion can create data that does not resemble natural errors, thereby making downstream systems learn misleading or uninformative patterns. Previous work on arti\ufb01cial error generation (AEG) has focused on speci\ufb01c error types, such as prepositions and determiners (Rozovskaya and Roth, 2010, 2011), or noun number errors (Brock- ett et al., 2006). Felice and Yuan (2014) investi- gated the use of linguistic information when gen- erating arti\ufb01cial data for error correction, but also restricting the approach to only \ufb01ve error types.",
            "Felice and Yuan (2014) investi- gated the use of linguistic information when gen- erating arti\ufb01cial data for error correction, but also restricting the approach to only \ufb01ve error types. There has been very limited research on gener- ating arti\ufb01cial data for all types, which is impor- tant for general-purpose error detection systems. For example, the error types investigated by Felice and Yuan (2014) cover only 35.74% of all errors present in the CoNLL 2014 training dataset, pro- viding no additional information for the majority of errors. In this paper, we investigate two supervised approaches for generating all types of arti\ufb01cial errors. We propose a framework for generat- ing errors based on statistical machine translation (SMT), training a model to translate from correct into incorrect sentences. In addition, we describe a method for learning error patterns from an anno- tated corpus and transplanting them into error-free text. We evaluate the effect of introducing arti\ufb01- cial data on two error detection benchmarks.",
            "In addition, we describe a method for learning error patterns from an anno- tated corpus and transplanting them into error-free text. We evaluate the effect of introducing arti\ufb01- cial data on two error detection benchmarks. Our results show that each method provides signi\ufb01cant improvements over using only the available train- ing set, and a combination of both gives an abso- lute improvement of 4.3% in F0.5, without requir- ing any additional annotated data. arXiv:1707.05236v1  [cs.CL]  17 Jul 2017",
            "Original We are a well-mixed class with equal numbers of boys and girls, all about 20 years old. FY14 We am a well-mixed class with equal numbers of boys and girls, all about 20 years old. PAT We are a well-mixed class with equal numbers of boys an girls, all about 20 year old. MT We are a well-mixed class with equals numbers of boys and girls, all about 20 years old. Table 1: Example arti\ufb01cial errors generated by three systems: the error generation method by Felice and Yuan (2014) (FY14), our pattern-based method covering all error types (PAT), and the machine translation approach to arti\ufb01cial error generation (MT). 2 Error Generation Methods We investigate two alternative methods for AEG. The models receive grammatically correct text as input and modify certain tokens to produce incor- rect sequences. The alternative versions of each sentence are aligned using Levenshtein distance, allowing us to identify speci\ufb01c words that need to be marked as errors.",
            "The models receive grammatically correct text as input and modify certain tokens to produce incor- rect sequences. The alternative versions of each sentence are aligned using Levenshtein distance, allowing us to identify speci\ufb01c words that need to be marked as errors. While these alignments are not always perfect, we found them to be suf\ufb01cient for practical purposes, since alternative alignments of similar sentences often result in the same bi- nary labeling. Future work could explore more advanced alignment methods, such as proposed by Felice et al. (2016). In Section 4, this automatically labeled data is then used for training error detection models. 2.1 Machine Translation We treat AEG as a translation task \u2013 given a cor- rect sentence as input, the system would learn to translate it to contain likely errors, based on a training corpus of parallel data. Existing SMT approaches are already optimised for identifying context patterns that correspond to speci\ufb01c out- put sequences, which is also required for gener- ating human-like errors.",
            "Existing SMT approaches are already optimised for identifying context patterns that correspond to speci\ufb01c out- put sequences, which is also required for gener- ating human-like errors. The reverse of this idea, translating from incorrect to correct sentences, has been shown to work well for error correction tasks (Brockett et al., 2006; Ng et al., 2014), and round- trip translation has also been shown to be promis- ing for correcting grammatical errors (Madnani et al., 2012). Following previous work (Brockett et al., 2006; Yuan and Felice, 2013), we build a phrase-based SMT error generation system. During training, error-corrected sentences in the training data are treated as the source, and the original sentences written by language learners as the target. Pi- align (Neubig et al., 2011) is used to create a phrase translation table directly from model prob- abilities. In addition to default features, we add character-level Levenshtein distance to each map- ping in the phrase table, as proposed by Fe- lice et al. (2014).",
            "In addition to default features, we add character-level Levenshtein distance to each map- ping in the phrase table, as proposed by Fe- lice et al. (2014). Decoding is performed us- ing Moses (Koehn et al., 2007) and the language model used during decoding is built from the orig- inal erroneous sentences in the learner corpus. The IRSTLM Toolkit (Federico et al., 2008) is used for building a 5-gram language model with modi\ufb01ed Kneser-Ney smoothing (Kneser and Ney, 1995). 2.2 Pattern Extraction We also describe a method for AEG using pat- terns over words and part-of-speech (POS) tags, extracting known incorrect sequences from a cor- pus of annotated corrections. This approach is based on the best method identi\ufb01ed by Felice and Yuan (2014), using error type distributions; while they covered only 5 error types, we relax this re- striction and learn patterns for generating all types of errors.",
            "This approach is based on the best method identi\ufb01ed by Felice and Yuan (2014), using error type distributions; while they covered only 5 error types, we relax this re- striction and learn patterns for generating all types of errors. The original and corrected sentences in the cor- pus are aligned and used to identify short transfor- mation patterns in the form of (incorrect phrase, correct phrase). The length of each pattern is the affected phrase, plus up to one token of context on both sides. If a word form changes between the incorrect and correct text, it is fully saved in the pattern, otherwise the POS tags are used for matching. For example, the original sentence \u2018We went shop on Saturday\u2019 and the corrected version \u2018We went shopping on Saturday\u2019 would produce the following pattern: (VVD shop VV0 II, VVD shopping VVG II) After collecting statistics from the background corpus, errors can be inserted into error-free text. The learned patterns are now reversed, looking for the correct side of the tuple in the input sentence. We only use patterns with frequency >= 5, which yields a total of 35,625 patterns from our training data.",
            "The learned patterns are now reversed, looking for the correct side of the tuple in the input sentence. We only use patterns with frequency >= 5, which yields a total of 35,625 patterns from our training data. For each input sentence, we \ufb01rst decide how many errors will be generated (using probabilities from the background corpus) and attempt to cre-",
            "ate them by sampling from the collection of appli- cable patterns. This process is repeated until all the required errors have been generated or the sen- tence is exhausted. During generation, we try to balance the distribution of error types as well as keeping the same proportion of incorrect and cor- rect sentences as in the background corpus (Felice, 2016). The required POS tags were generated with RASP (Briscoe et al., 2006), using the CLAWS2 tagset. 3 Error Detection Model We construct a neural sequence labeling model for error detection, following the previous work (Rei and Yannakoudakis, 2016; Rei, 2017). The model receives a sequence of tokens as input and outputs a prediction for each position, indicating whether the token is correct or incorrect in the current con- text. The tokens are \ufb01rst mapped to a distributed vector space, resulting in a sequence of word em- beddings. Next, the embeddings are given as input to a bidirectional LSTM (Hochreiter and Schmid- huber, 1997), in order to create context-dependent representations for every token.",
            "Next, the embeddings are given as input to a bidirectional LSTM (Hochreiter and Schmid- huber, 1997), in order to create context-dependent representations for every token. The hidden states from forward- and backward-LSTMs are concate- nated for each word position, resulting in repre- sentations that are conditioned on the whole se- quence. This concatenated vector is then passed through an additional feedforward layer, and a softmax over the two possible labels (correct and incorrect) is used to output a probability distribu- tion for each token. The model is optimised by minimising categorical cross-entropy with respect to the correct labels. We use AdaDelta (Zeiler, 2012) for calculating an adaptive learning rate dur- ing training, which accounts for a higher baseline performance compared to previous results. 4 Evaluation We trained our error generation models on the public FCE training set (Yannakoudakis et al., 2011) and used them to generate additional arti- \ufb01cial training data.",
            "4 Evaluation We trained our error generation models on the public FCE training set (Yannakoudakis et al., 2011) and used them to generate additional arti- \ufb01cial training data. Grammatically correct text is needed as the starting point for inserting arti\ufb01cial errors, and we used two different sources: 1) the corrected version of the same FCE training set on which the system is trained (450K tokens), and 2) example sentences extracted from the English Vocabulary Pro\ufb01le (270K tokens).1. While there are other text corpora that could be used (e.g., 1http:\/\/www.englishpro\ufb01le.org\/wordlists Wikipedia and news articles), our development ex- periments showed that keeping the writing style and vocabulary close to the target domain gives better results compared to simply including more data. We evaluated our detection models on three benchmarks: the FCE test data (41K tokens) and the two alternative annotations of the CoNLL 2014 Shared Task dataset (30K tokens) (Ng et al., 2014).",
            "We evaluated our detection models on three benchmarks: the FCE test data (41K tokens) and the two alternative annotations of the CoNLL 2014 Shared Task dataset (30K tokens) (Ng et al., 2014). Each arti\ufb01cial error generation system was used to generate 3 different versions of the arti- \ufb01cial data, which were then combined with the original annotated dataset and used for training an error detection system. Table 1 contains example sentences from the error generation systems, high- lighting each of the edits that are marked as errors. The error detection results can be seen in Table 2. We use F0.5 as the main evaluation measure, which was established as the preferred measure for error correction and detection by the CoNLL- 14 shared task (Ng et al., 2014). F0.5 calculates a weighted harmonic mean of precision and re- call, which assigns twice as much importance to precision \u2013 this is motivated by practical appli- cations, where accurate predictions from an er- ror detection system are more important compared to coverage.",
            "F0.5 calculates a weighted harmonic mean of precision and re- call, which assigns twice as much importance to precision \u2013 this is motivated by practical appli- cations, where accurate predictions from an er- ror detection system are more important compared to coverage. For comparison, we also report the performance of the error detection system by Rei and Yannakoudakis (2016), trained using the same FCE dataset. The results show that error detection perfor- mance is substantially improved by making use of arti\ufb01cially generated data, created by any of the described methods. When comparing the er- ror generation system by Felice and Yuan (2014) (FY14) with our pattern-based (PAT) and machine translation (MT) approaches, we see that the latter methods covering all error types consistently im- prove performance. While the added error types tend to be less frequent and more complicated to capture, the added coverage is indeed bene\ufb01- cial for error detection. Combining the pattern- based approach with the machine translation sys- tem (Ann+PAT+MT) gave the best overall perfor- mance on all datasets.",
            "Combining the pattern- based approach with the machine translation sys- tem (Ann+PAT+MT) gave the best overall perfor- mance on all datasets. The two frameworks learn to generate different types of errors, and taking ad- vantage of both leads to substantial improvements in error detection. We used the Approximate Randomisation Test (Noreen, 1989; Cohen, 1995) to calculate statisti- cal signi\ufb01cance and found that the improvement",
            "FCE CoNLL-14 TEST1 CoNLL-14 TEST2 P R F0.5 P R F0.5 P R F0.5 R&Y (2016) 46.10 28.50 41.10 15.40 22.80 16.40 23.60 25.10 23.90 Annotation 53.91 26.88 44.84 16.12 18.42 16.52 25.72 20.92 24.57 Ann+FY14 58.77 25.55 46.54 20.48 14.41 18.88 33.25 16.67 27.72 Ann+PAT 62.47 24.70 47.81 21.07 15.02 19.47 34.04 17.32 28.49 Ann+MT 58.38 28.84 48.37 19.52 20.79 19.73 30.24 22.96 28.39 Ann+PAT+MT 60.67 28.08 49.11 23.28 18.",
            "49 Ann+MT 58.38 28.84 48.37 19.52 20.79 19.73 30.24 22.96 28.39 Ann+PAT+MT 60.67 28.08 49.11 23.28 18.01 21.87 35.28 19.42 30.13 Table 2: Error detection performance when combining manually annotated and arti\ufb01cial training data. for each of the systems using arti\ufb01cial data was signi\ufb01cant over using only manual annotation. In addition, the \ufb01nal combination system is also sig- ni\ufb01cantly better compared to the Felice and Yuan (2014) system, on all three datasets. While Rei and Yannakoudakis (2016) also report separate ex- periments that achieve even higher performance, these models were trained on a considerably larger proprietary corpus. In this paper we compare error detection frameworks trained on the same publicly available FCE dataset, thereby removing the con- founding factor of dataset size and only focusing on the model architectures.",
            "In this paper we compare error detection frameworks trained on the same publicly available FCE dataset, thereby removing the con- founding factor of dataset size and only focusing on the model architectures. The error generation methods can generate al- ternative versions of the same input text \u2013 the pattern-based method randomly samples the er- ror locations, and the SMT system can provide an n-best list of alternative translations. Therefore, we also investigated the combination of multiple error-generated versions of the input \ufb01les when training error detection models. Figure 1 shows the F0.5 score on the development set, as the train- ing data is increased by using more translations from the n-best list of the SMT system. These re- sults reveal that allowing the model to see multiple alternative versions of the same \ufb01le gives a dis- tinct improvement \u2013 showing the model both cor- rect and incorrect variations of the same sentences likely assists in learning a discriminative model. 5 Related Work Our work builds on prior research into AEG. Brockett et al. (2006) constructed regular expres- sions for transforming correct sentences to con- tain noun number errors.",
            "5 Related Work Our work builds on prior research into AEG. Brockett et al. (2006) constructed regular expres- sions for transforming correct sentences to con- tain noun number errors. Rozovskaya and Roth (2010) learned confusion sets from an annotated corpus in order to generate preposition errors. Fos- ter and Andersen (2009) devised a tool for gener- ating errors for different types using patterns pro- vided by the user or collected automatically from 0 2 4 6 8 10 12 0.2 0.3 0.4 0.5 Number of versions F0.5 FCE EVP Figure 1: F0.5 on FCE development set with in- creasing amounts of arti\ufb01cial data from SMT. an annotated corpus. However, their method uses a limited number of edit operations and is thus unable to generate complex errors. Cahill et al. (2013) compared different training methodologies and showed that arti\ufb01cial errors helped correct prepositions.",
            "an annotated corpus. However, their method uses a limited number of edit operations and is thus unable to generate complex errors. Cahill et al. (2013) compared different training methodologies and showed that arti\ufb01cial errors helped correct prepositions. Felice and Yuan (2014) learned er- ror type distributions for generating \ufb01ve types of errors, and the system in Section 2.2 is an exten- sion of this model. While previous work focused on generating a speci\ufb01c subset of error types, we explored two holistic approaches to AEG and showed that they are able to signi\ufb01cantly improve error detection performance. 6 Conclusion This paper investigated two AEG methods, in or- der to create additional training data for error de- tection. First, we explored a method using textual patterns learned from an annotated corpus, which are used for inserting errors into correct input text. In addition, we proposed formulating error gen- eration as an MT framework, learning to translate from grammatically correct to incorrect sentences. The addition of arti\ufb01cial data to the training pro- cess was evaluated on three error detection anno-",
            "tations, using the FCE and CoNLL 2014 datasets. Making use of arti\ufb01cial data provided improve- ments for all data generation methods. By relax- ing the type restrictions and generating all types of errors, our pattern-based method consistently out- performed the system by Felice and Yuan (2014). The combination of the pattern-based method with the machine translation approach gave further sub- stantial improvements and the best performance on all datasets. References Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The Second Release of the RASP System. In Pro- ceedings of the COLING\/ACL on Interactive pre- sentation sessions.. Association for Computational Linguistics, Sydney, Australia, July, pages 77\u201380. https:\/\/doi.org\/10.3115\/1225403.1225423. Chris Brockett, William B. Dolan, and Michael Gamon. 2006. Correcting ESL errors us- ing phrasal SMT techniques.",
            "https:\/\/doi.org\/10.3115\/1225403.1225423. Chris Brockett, William B. Dolan, and Michael Gamon. 2006. Correcting ESL errors us- ing phrasal SMT techniques. Proceedings of the 21st International Conference on Compu- tational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics https:\/\/doi.org\/10.3115\/1220175.1220207. Aoife Cahill, Nitin Madnani, Joel Tetreault, and Di- ane Napolitano. 2013. Robust Systems for Prepo- sition Error Correction Using Wikipedia Revisions. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies. http:\/\/www.aclweb.org\/anthology\/N13-1055. Paul Cohen. 1995. Empirical Methods for Arti\ufb01cial In- telligence. The MIT Press, Cambridge, MA. Marcello Federico, Nicola Bertoldi, and Mauro Cet- tolo. 2008.",
            "Paul Cohen. 1995. Empirical Methods for Arti\ufb01cial In- telligence. The MIT Press, Cambridge, MA. Marcello Federico, Nicola Bertoldi, and Mauro Cet- tolo. 2008. IRSTLM: an open source toolkit for han- dling large scale language models. In Proceedings of the 9th Annual Conference of the International Speech Communication Association. Mariano Felice. 2016. Arti\ufb01cial error generation for translation-based grammatical error correc- tion. Technical Report UCAM-CL-TR-895, University of Cambridge, Computer Laboratory. http:\/\/www.cl.cam.ac.uk\/techreports\/UCAM-CL- TR-895.pdf. Mariano Felice, Christopher Bryant, and Ted Briscoe. 2016. Automatic extraction of learner errors in esl sentences using linguistically enhanced alignments. In Proceedings of COLING 2016, the 26th In- ternational Conference on Computational Linguis- tics: Technical Papers. The COLING 2016 Orga- nizing Committee, Osaka, Japan, pages 825\u2013835.",
            "In Proceedings of COLING 2016, the 26th In- ternational Conference on Computational Linguis- tics: Technical Papers. The COLING 2016 Orga- nizing Committee, Osaka, Japan, pages 825\u2013835. http:\/\/aclweb.org\/anthology\/C16-1079. Mariano Felice and Zheng Yuan. 2014. Generating ar- ti\ufb01cial errors for grammatical error correction. Pro- ceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2014) . Mariano Felice, Zheng Yuan, \u00d8istein E. Andersen, He- len Yannakoudakis, and Ekaterina Kochmar. 2014. Grammatical error correction using hybrid systems and type \ufb01ltering. In Proceedings of the 18th Con- ference on Computational Natural Language Learn- ing: Shared Task. Jennifer Foster and \u00d8istein E. Andersen. 2009. GenERRate: generating errors for use in grammatical error detection.",
            "In Proceedings of the 18th Con- ference on Computational Natural Language Learn- ing: Shared Task. Jennifer Foster and \u00d8istein E. Andersen. 2009. GenERRate: generating errors for use in grammatical error detection. In Proceedings of the Fourth Workshop on Innovative Use of NLP for Building Educational Applications. http:\/\/dl.acm.org\/citation.cfm?id=1609855. Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997. Long Short-term Memory. Neural Computation 9. https:\/\/doi.org\/10.1.1.56.7752. Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for M-gram language modeling. In Pro- ceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing. Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond\u02c7rej Bojar, Alexandra Constantin, and Evan Herbst.",
            "2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions. Nitin Madnani, Joel Tetreault, and Martin Chodorow. 2012. Exploring grammatical error correction with not-so-crummy machine translation. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP. Association for Computa- tional Linguistics, Montr\u00b4eal, Canada, pages 44\u201353. http:\/\/www.aclweb.org\/anthology\/W12-2005. Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shinsuke Mori, and Tatsuya Kawahara. 2011. An unsupervised model for joint phrase alignment and extraction. In Proceedings of the 49th Annual Meet- ing of the Association for Computational Linguis- tics: Human Language Technologies. Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian Hadiwinoto, Raymond Hendy Susanto, and Christo- pher Bryant. 2014.",
            "Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian Hadiwinoto, Raymond Hendy Susanto, and Christo- pher Bryant. 2014. The CoNLL-2014 Shared Task on Grammatical Error Correction. In Proceedings of the Eighteenth Conference on Computa- tional Natural Language Learning: Shared Task. http:\/\/www.aclweb.org\/anthology\/W\/W14\/W14- 1701. Eric W. Noreen. 1989. Computer Intensive Methods for Testing Hypotheses. Wiley, New York.",
            "Marek Rei. 2017. Semi-supervised Multitask Learn- ing for Sequence Labeling. In Proceedings of the 55th Annual Meeting of the Association for Compu- tational Linguistics (ACL-2017). Marek Rei and Helen Yannakoudakis. 2016. Compositional Sequence Labeling Models for Error Detection in Learner Writing. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. https:\/\/aclweb.org\/anthology\/P\/P16\/P16-1112.pdf. Alla Rozovskaya and Dan Roth. 2010. Generating confusion sets for context-sensitive error correc- tion. Proceedings of the 2010 Conference on Em- pirical Methods in Natural Language Processing http:\/\/dl.acm.org\/citation.cfm?id=1870752. Alla Rozovskaya and Dan Roth. 2011. Algorithm Selection and Model Adaptation for ESL Correc- tion Tasks.",
            "Alla Rozovskaya and Dan Roth. 2011. Algorithm Selection and Model Adaptation for ESL Correc- tion Tasks. Proceedings of the 49th Annual Meet- ing of the Association for Computational Linguis- tics (ACL-2011) http:\/\/www.aclweb.org\/anthology- new\/P\/P11\/P11-1093.pdf. Helen Yannakoudakis, Ted Briscoe, and Ben Medlock. 2011. A New Dataset and Method for Automati- cally Grading ESOL Texts. In Proceedings of the 49th Annual Meeting of the Association for Com- putational Linguistics: Human Language Technolo- gies. http:\/\/www.aclweb.org\/anthology\/P11-1019. Zheng Yuan and Mariano Felice. 2013. Constrained grammatical error correction using statistical ma- chine translation. In Proceedings of the 17th Con- ference on Computational Natural Language Learn- ing: Shared Task. Matthew D. Zeiler. 2012. ADADELTA: An Adap- tive Learning Rate Method.",
            "Constrained grammatical error correction using statistical ma- chine translation. In Proceedings of the 17th Con- ference on Computational Natural Language Learn- ing: Shared Task. Matthew D. Zeiler. 2012. ADADELTA: An Adap- tive Learning Rate Method. arXiv preprint arXiv:1212.5701 http:\/\/arxiv.org\/abs\/1212.5701."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1707.05236.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 5610.0,
    "avg_doclen_est": 175.3125
}
