{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Learning to Detect Opinion Snippet for Aspect-Based Sentiment Analysis Mengting Hu1\u2217 Shiwan Zhao2\u2020 Honglei Guo2 Renhong Cheng1 Zhong Su2 1 Nankai University 2 IBM Research - China mthu@mail.nankai.edu.cn, {zhaosw, guohl}@cn.ibm.com chengrh@nankai.edu.cn, suzhong@cn.ibm.com Abstract Aspect-based sentiment analysis (ABSA) is to predict the sentiment polarity towards a partic- ular aspect in a sentence. Recently, this task has been widely addressed by the neural at- tention mechanism, which computes attention weights to softly select words for generating aspect-speci\ufb01c sentence representations. The attention is expected to concentrate on opin- ion words for accurate sentiment prediction. However, attention is prone to be distracted by noisy or misleading words, or opinion words from other aspects. In this paper, we pro- pose an alternative hard-selection approach, which determines the start and end positions of the opinion snippet, and selects the words between these two positions for sentiment pre- diction.",
      "However, attention is prone to be distracted by noisy or misleading words, or opinion words from other aspects. In this paper, we pro- pose an alternative hard-selection approach, which determines the start and end positions of the opinion snippet, and selects the words between these two positions for sentiment pre- diction. Speci\ufb01cally, we learn deep associa- tions between the sentence and aspect, and the long-term dependencies within the sentence by leveraging the pre-trained BERT model. We further detect the opinion snippet by self- critical reinforcement learning. Especially, ex- perimental results demonstrate the effective- ness of our method and prove that our hard- selection approach outperforms soft-selection approaches when handling multi-aspect sen- tences. 1 Introduction Aspect-based sentiment analysis (Pang and Lee, 2008; Liu, 2012) is a \ufb01ne-grained sentiment anal- ysis task which has gained much attention from research and industries. It aims at predicting the sentiment polarity of a particular aspect of the text.",
      "1 Introduction Aspect-based sentiment analysis (Pang and Lee, 2008; Liu, 2012) is a \ufb01ne-grained sentiment anal- ysis task which has gained much attention from research and industries. It aims at predicting the sentiment polarity of a particular aspect of the text. With the rapid development of deep learning, this task has been widely addressed by attention- based neural networks (Wang et al., 2016; Ma et al., 2017; Cheng et al., 2017; Tay et al., 2018; \u2217Work performed while interning at IBM Research - China. \u2020Corresponding author. 0.20 0.12 0.09 0.1 0.04 0.05 0.04 Aspect: place Label: Negative Prediction: Positive the food is usually good but it certainly is not a relaxing place to go Figure 1: Example of attention visualization. The at- tention weights of the aspect place are from the model ATAE-LSTM (Wang et al., 2016), a typical attention mechanism used for soft-selection. Wang et al., 2018a).",
      "The at- tention weights of the aspect place are from the model ATAE-LSTM (Wang et al., 2016), a typical attention mechanism used for soft-selection. Wang et al., 2018a). To name a few, Wang et al. (2016) learn to attend on different parts of the sentence given different aspects, then generates aspect-speci\ufb01c sentence representations for sen- timent prediction. Tay et al. (2018) learn to at- tend on correct words based on associative rela- tionships between sentence words and a given as- pect. These attention-based methods have brought the ABSA task remarkable performance improve- ment. Previous attention-based methods can be cate- gorized as soft-selection approaches since the at- tention weights scatter across the whole sentence and every word is taken into consideration with different weights. This usually results in atten- tion distraction (Li et al., 2018b), i.e., attending on noisy or misleading words, or opinion words from other aspects.",
      "This usually results in atten- tion distraction (Li et al., 2018b), i.e., attending on noisy or misleading words, or opinion words from other aspects. Take Figure 1 as an example, for the aspect place in the sentence \u201cthe food is usually good but it certainly is not a relaxing place to go\u201d, we visualize the attention weights from the model ATAE-LSTM (Wang et al., 2016). As we can see, the words \u201cgood\u201d and \u201cbut\u201d are dominant in at- tention weights. However, \u201cgood\u201d is used to de- scribe the aspect food rather than place, \u201cbut\u201d is not so related to place either. The true opin- ion snippet \u201ccertainly is not a relaxing place\u201d re- ceives low attention weights, leading to the wrong prediction towards the aspect place. Therefore, we propose an alternative hard- selection approach by determining two positions arXiv:1909.11297v1  [cs.CL]  25 Sep 2019",
      "in the sentence and selecting words between these two positions as the opinion expression of a given aspect. This is also based on the observation that opinion words of a given aspect are usually dis- tributed consecutively as a snippet (Wang and Lu, 2018). As a consecutive whole, the opinion snip- pet may gain enough attention weights, avoid be- ing distracted by other noisy or misleading words, or distant opinion words from other aspects. We then predict the sentiment polarity of the given as- pect based on the average of the extracted opin- ion snippet. The explicit selection of the opinion snippet also brings us another advantage that it can serve as justi\ufb01cations of our sentiment predictions, making our model more interpretable. To accurately determine the two positions of the opinion snippet of a particular aspect, we \ufb01rst model the deep associations between the sen- tence and aspect, and the long-term dependen- cies within the sentence by BERT (Devlin et al., 2018), which is a pre-trained language model and achieves exciting results in many natural language tasks.",
      "Second, with the contextual representations from BERT, the two positions are sequentially de- termined by self-critical reinforcement learning. The reason for using reinforcement learning is that we do not have the ground-truth positions of the opinion snippet, but only the polarity of the corre- sponding aspect. Then the extracted opinion snip- pet is used for sentiment classi\ufb01cation. The details are described in the model section. The main contributions of our paper are as fol- lows: \u2022 We propose a hard-selection approach to address the ABSA task. Speci\ufb01cally, our method determines two positions in the sen- tence to detect the opinion snippet towards a particular aspect, and then uses the framed content for sentiment classi\ufb01cation. Our ap- proach can alleviate the attention distrac- tion problem in previous soft-selection ap- proaches. \u2022 We model deep associations between the sen- tence and aspect, and the long-term depen- dencies within the sentence by BERT. We then learn to detect the opinion snippet by self-critical reinforcement learning.",
      "\u2022 We model deep associations between the sen- tence and aspect, and the long-term depen- dencies within the sentence by BERT. We then learn to detect the opinion snippet by self-critical reinforcement learning. \u2022 The experimental results demonstrate the effectiveness of our method and also our approach signi\ufb01cantly outperforms soft- selection approaches on handling multi- aspect sentences. 2 Related Work Traditional machine learning methods for aspect- based sentiment analysis focus on extracting a set of features to train sentiment classi\ufb01ers (Ding et al., 2009; Boiy and Moens, 2009; Jiang et al., 2011), which usually are labor intensive.",
      "2 Related Work Traditional machine learning methods for aspect- based sentiment analysis focus on extracting a set of features to train sentiment classi\ufb01ers (Ding et al., 2009; Boiy and Moens, 2009; Jiang et al., 2011), which usually are labor intensive. With the development of deep learning technologies, neu- ral attention mechanism (Bahdanau et al., 2014) has been widely adopted to address this task (Tang et al., 2015; Wang et al., 2016; Tang et al., 2016; Ma et al., 2017; Chen et al., 2017; Cheng et al., 2017; Li et al., 2018a; Wang et al., 2018a; Tay et al., 2018; Hazarika et al., 2018; Majumder et al., 2018; Fan et al., 2018; Wang et al., 2018b). Wang et al. (2016) propose attention-based LSTM net- works which attend on different parts of the sen- tence for different aspects. Ma et al.",
      "Wang et al. (2016) propose attention-based LSTM net- works which attend on different parts of the sen- tence for different aspects. Ma et al. (2017) utilize the interactive attention to capture the deep asso- ciations between the sentence and the aspect. Hi- erarchical models (Cheng et al., 2017; Li et al., 2018a; Wang et al., 2018a) are also employed to capture multiple levels of emotional expression for more accurate prediction, as the complexity of sentence structure and semantic diversity. Tay et al. (2018) learn to attend based on associative relationships between sentence words and aspect. All these methods use normalized attention weights to softly select words for generating aspect-speci\ufb01c sentence representations, while the attention weights scatter across the whole sentence and can easily result in attention distraction. Wang and Lu (2018) propose a hard-selection method to learn segmentation attention which can effectively capture the structural dependencies between the target and the sentiment expressions with a linear- chain conditional random \ufb01eld (CRF) layer.",
      "Wang and Lu (2018) propose a hard-selection method to learn segmentation attention which can effectively capture the structural dependencies between the target and the sentiment expressions with a linear- chain conditional random \ufb01eld (CRF) layer. How- ever, it can only address aspect-term level senti- ment prediction which requires annotations for as- pect terms. Compared with it, our method can handle both aspect-term level and aspect-category level sentiment prediction by detecting the opinion snippet. 3 Model We \ufb01rst formulate the problem. Given a sen- tence S = {w1, w2, ..., wN} and an aspect A = {a1, a2, ..., aM}, the ABSA task is to predict the",
      "Aspect Sentence TS = T[1:N ] [CLS] \u2026 [SEP] \u2026 E[CLS] E1 EN E[SEP] E1\u2019 EM\u2019 C T1 TN T[SEP] T1\u2019 TM\u2019 \u2026 \u2026 \u2026 \u2026 Word-Aspect Fusion by BERT Hard Selection Sample Sample TS b Argmax Argmax Reward TO = T[l:r] TO b Cross Entropy Reward R Rb L(\u0398) = \u2212(R \u2212Rb)\u22c5(log( p(l)) + log( p(r))) w1 wN a1 aM Figure 2: Network Architecture. We leverage BERT to model the relationships between sentence words and a particular aspect. The sentence and aspect are packed together into a single sequence and fed into BERT, in which E represents the input embedding, and Ti rep- resents the contextual representation of token i. With the contextual representations from BERT, the start and end positions are sequentially sampled and then the framed content is used for sentiment prediction. Re- inforcement learning is adopted for solving the non- differentiable problem of sampling. sentiment of A. In our setting, the aspect can be either aspect terms or an aspect category.",
      "Re- inforcement learning is adopted for solving the non- differentiable problem of sampling. sentiment of A. In our setting, the aspect can be either aspect terms or an aspect category. As as- pect terms, A is a snippet of words in S, i.e., a sub-sequence of the sentence, while as an aspect category, A represents a semantic category with M = 1, containing just an abstract token. In this paper, we propose a hard-selection ap- proach to solve the ABSA task. Speci\ufb01cally, we \ufb01rst learn to detect the corresponding opinion snippet O = {wl, wl+1..., wr}, where 1 \u2264l \u2264 r \u2264N, and then use O to predict the sentiment of the given aspect. The network architecture is shown in Figure 2. 3.1 Word-Aspect Fusion Accurately modeling the relationships between sentence words and an aspect is the key to the success of the ABSA task. Many methods have been developed to model word-aspect relation- ships. Wang et al.",
      "The network architecture is shown in Figure 2. 3.1 Word-Aspect Fusion Accurately modeling the relationships between sentence words and an aspect is the key to the success of the ABSA task. Many methods have been developed to model word-aspect relation- ships. Wang et al. (2016) simply concatenate the aspect embedding with the input word em- beddings and sentence hidden representations for computing aspect-speci\ufb01c attention weights. Ma et al. (2017) learn the aspect and sentence interac- tively by using two attention networks. Tay et al. (2018) adopt circular convolution of vectors for performing the word-aspect fusion. In this paper, we employ BERT (Devlin et al., 2018) to model the deep associations between the sentence words and the aspect. BERT is a power- ful pre-trained model which has achieved remark- able results in many NLP tasks. The architec- ture of BERT is a multi-layer bidirectional Trans- former Encoder (Vaswani et al., 2017), which uses the self-attention mechanism to capture complex interaction and dependency between terms within a sequence.",
      "The architec- ture of BERT is a multi-layer bidirectional Trans- former Encoder (Vaswani et al., 2017), which uses the self-attention mechanism to capture complex interaction and dependency between terms within a sequence. To leverage BERT to model the rela- tionships between the sentence and the aspect, we pack the sentence and aspect together into a sin- gle sequence and then feed it into BERT, as shown in Figure 2. With this sentence-aspect concatena- tion, both the word-aspect associations and word- word dependencies are modeled interactively and simultaneously. With the contextual token repre- sentations TS = T[1:N] \u2208RN\u00d7H of the sentence, where N is the sentence length and H is the hid- den size, we can then determine the start and end positions of the opinion snippet in the sentence. 3.2 Soft-Selection Approach To fairly compare the performance of soft- selection approaches with hard-selection ap- proaches, we use the same word-aspect fusion re- sults TS from BERT. We implement the attention mechanism by adopting the approach similar to the work (Lin et al., 2017).",
      "We implement the attention mechanism by adopting the approach similar to the work (Lin et al., 2017). \u03b1 = softmax(v1tanh(W1TST)) g = \u03b1TS (1) where v1 \u2208RH and W1 \u2208RH\u00d7H are the parame- ters. The normalized attention weights \u03b1 are used to softly select words from the whole sentence and generate the \ufb01nal aspect-speci\ufb01c sentence repre- sentation g. Then we make sentiment prediction as follows: \u02c6y = softmax(W2g + b) (2) where W2 \u2208RC\u00d7H and b \u2208RC are the weight matrix and bias vector respectively. \u02c6y is the prob- ability distribution on C polarities. The polarity with highest probability is selected as the predic- tion. 3.3 Hard-Selection Approach Our proposed hard-selection approach determines the start and end positions of the opinion snippet",
      "and selects the words between these two positions for sentiment prediction. Since we do not have the ground-truth opinion snippet, but only the polarity of the corresponding aspect, we adopt reinforce- ment learning (Williams, 1992) to train our model. To make sure that the end position comes after the start position, we determine the start and end se- quentially as a sequence training problem (Rennie et al., 2017). The parameters of the network, \u0398, de\ufb01ne a policy p\u03b8 and output an action that is the prediction of the position. For simplicity, we only generate two actions for determining the start and end positions respectively. After determining the start position, the \u201cstate\u201d is updated and then the end is conditioned on the start. Speci\ufb01cally, we de\ufb01ne a start vector s \u2208RH and an end vector e \u2208RH. Similar to the prior work (Devlin et al., 2018), the probability of a word being the start of the opinion snippet is com- puted as a dot product between its contextual token representation and s followed by a softmax over all of the words of the sentence.",
      "Similar to the prior work (Devlin et al., 2018), the probability of a word being the start of the opinion snippet is com- puted as a dot product between its contextual token representation and s followed by a softmax over all of the words of the sentence. \u03b2l = softmax(TSs) (3) We then sample the start position l based on the multinomial distribution \u03b2l. To guarantee the end comes after the start, the end is sampled only in the right part of the sentence after the start. Therefore, the state is updated by slicing operation TSr = TS[l :]. Same as the start position, the end position r is also sampled based on the distribution \u03b2r: \u03b2r = softmax(T r Se) (4) Then we have the opinion snippet TO = TS[l : r] to predict the sentiment polarity of the given as- pect in the sentence. The probabilities of the start position at l and the end position at r are p(l) = \u03b2l[l] and p(r) = \u03b2r[r] respectively.",
      "The probabilities of the start position at l and the end position at r are p(l) = \u03b2l[l] and p(r) = \u03b2r[r] respectively. 3.3.1 Reward After we get the opinion snippet TO by the sam- pling of the start and end positions, we compute the \ufb01nal representation go by the average of the opinion snippet, go = avg(TO). Then, equation 2 with different weights is applied for computing the sentiment prediction \u02c6 yo. The cross-entropy loss function is employed for computing the reward. R = \u2212 X c yc log \u02c6 yoc (5) where c is the index of the polarity class and y is the ground truth. 3.3.2 Self-Critical Training In this paper, we use reinforcement learning to learn the start and end positions. The goal of train- ing is to minimize the negative expected reward as shown below.",
      "3.3.2 Self-Critical Training In this paper, we use reinforcement learning to learn the start and end positions. The goal of train- ing is to minimize the negative expected reward as shown below. L(\u0398) = \u2212R \u00b7 p(l) \u00b7 p(r) (6) where \u0398 is all the parameters in our architecture, which includes the base method BERT, the posi- tion selection parameters {s, e}, and the parame- ters for sentiment prediction and then for reward calculation. Therefore, the state in our method is the combination of the sentence and the aspect. For each state, the action space is every position of the sentence. To reduce the variance of the gradient estima- tion, the reward is associated with a reference re- ward or baseline Rb (Rennie et al., 2017). With the likelihood ratio trick, the objective function can be transformed as. L(\u0398) = \u2212(R\u2212Rb)\u00b7(log(p(l))+log(p(r))) (7) The baseline Rb is computed based on the snippet determined by the baseline policy, which selects the start and end positions greedily by the argmax operation on the softmax results.",
      "L(\u0398) = \u2212(R\u2212Rb)\u00b7(log(p(l))+log(p(r))) (7) The baseline Rb is computed based on the snippet determined by the baseline policy, which selects the start and end positions greedily by the argmax operation on the softmax results. As shown in Figure 2, the reward R is calculated by sampling the snippet, while the baseline Rb is computed by greedily selecting the snippet. Note that in the test stage, the snippet is determined by argmax for inference. 4 Experiments In this section, we compare our hard-selection model with various baselines. To assess the ability of alleviating the attention distraction, we further conduct experiments on a simulated multi-aspect dataset in which each sentence contains multiple aspects. 4.1 Datasets We use the same datasets as the work by Tay et al. (2018), which are already processed to token lists and released in Github1. The datasets are from Se- mEval 2014 task 4 (Pontiki et al., 2014), and Se- mEval 2015 task 12 (Pontiki et al., 2015), respec- tively.",
      "The datasets are from Se- mEval 2014 task 4 (Pontiki et al., 2014), and Se- mEval 2015 task 12 (Pontiki et al., 2015), respec- tively. For aspect term level sentiment classi\ufb01ca- tion task (denoted by T), we apply the Laptops and 1https://github.com/vanzytay/ABSA DevSplits",
      "Task Dataset All P N Nu T Laptops Train 1813 767 673 373 T Laptops Dev 500 220 193 87 T Laptops Test 638 341 128 169 T Restaurants Train 3102 685 1886 531 T Restaurants Dev 500 278 120 102 T Restaurants Test 1120 728 196 196 C Restaurants Train 3018 1873 712 433 C Restaurants Dev 500 306 127 67 C Restaurants Test 973 657 222 94 C SE 14+15 Train 3587 1069 2310 208 C SE 14+15 Dev 427 274 134 19 C SE 14+15 Test 1011 455 496 60 Table 1: Dataset statistics. T and C denote the aspect- term and aspect-category tasks, respectively. P, N, and Nu represent the numbers of instances with positive, negative and neutral polarities, and All is the total num- ber of instances. Restaurants datasets from SemEval 2014.",
      "T and C denote the aspect- term and aspect-category tasks, respectively. P, N, and Nu represent the numbers of instances with positive, negative and neutral polarities, and All is the total num- ber of instances. Restaurants datasets from SemEval 2014. For as- pect category level sentiment prediction (denoted by C), we utilize the Restaurants dataset from Se- mEval 2014 and a composed dataset from both Se- mEval 2014 and SemEval 2015. The statistics of the datasets are shown in Table 1. 4.2 Implementation Details Our proposed models are implemented in Py- Torch2. We utilize the bert-base-uncased model, which contains 12 layers and the number of all pa- rameters is 100M. The dimension H is 768. The BERT model is initialized from the pre-trained model, other parameters are initialized by sam- pling from normal distribution N(0, 0.02). In our experiments, the batch size is 32. The reported results are the testing scores that \ufb01ne-tuning 7 epochs with learning rate 5e-5.",
      "In our experiments, the batch size is 32. The reported results are the testing scores that \ufb01ne-tuning 7 epochs with learning rate 5e-5. 4.3 Compared Models \u2022 LSTM: it uses the average of all hidden states as the sentence representation for sentiment prediction. In this model, aspect information is not used. \u2022 TD-LSTM (Tang et al., 2015): it employs two LSTMs and both of their outputs are ap- plied to predict the sentiment polarity. 2https://github.com/huggingface/pytorch-pretrained- BERT \u2022 AT-LSTM (Wang et al., 2016): it uti- lizes the attention mechanism to produce an aspect-speci\ufb01c sentence representation. This method is a kind of soft-selection approach. \u2022 ATAE-LSTM (Wang et al., 2016): it also uses the attention mechanism. The difference with AT-LSTM is that it concatenates the as- pect embedding to each word embedding as the input to LSTM.",
      "This method is a kind of soft-selection approach. \u2022 ATAE-LSTM (Wang et al., 2016): it also uses the attention mechanism. The difference with AT-LSTM is that it concatenates the as- pect embedding to each word embedding as the input to LSTM. \u2022 AF-LSTM(CORR) (Tay et al., 2018): it adopts circular correlation to capture the deep fusion between sentence words and the as- pect, which can learn rich, higher-order re- lationships between words and the aspect. \u2022 AF-LSTM(CONV) (Tay et al., 2018): com- pared with AF-LSTM(CORR), this method applies circular convolution of vectors for performing word-aspect fusion to learn rela- tionships between sentence words and the as- pect. \u2022 BERT-Original: it makes sentiment predic- tion by directly using the \ufb01nal hidden vector C from BERT with the sentence-aspect pair as input.",
      "\u2022 BERT-Original: it makes sentiment predic- tion by directly using the \ufb01nal hidden vector C from BERT with the sentence-aspect pair as input. 4.4 Our Models \u2022 BERT-Soft: as described in Section 3.2, the contextual token representations from BERT are processed by self attention mechanism (Lin et al., 2017) and the attention-weighted sentence representation is utilized for senti- ment classi\ufb01cation. \u2022 BERT-Hard: as described in Section 3.3, it takes the same input as BERT-Soft. It is called a hard-selection approach since it em- ploys reinforcement learning techniques to explicitly select the opinion snippet corre- sponding to a particular aspect for sentiment prediction. 4.5 Experimental Results In this section, we evaluate the performance of our models by comparing them with various baseline models. Experimental results are illustrated in Ta- ble 2, in which 3-way represents 3-class sentiment classi\ufb01cation (positive, negative and neutral) and Binary denotes binary sentiment prediction (posi- tive and negative). The best score of each column is marked in bold.",
      "Term-Level Category-Level Laptops Restaurants Restaurants SemEval 14+15 Model Aspect 3-way Binary 3-way Binary 3-way Binary 3-way Binary Avg LSTM No 61.75 78.25 67.94 82.03 73.38 79.97 75.96 79.92 74.90 TD-LSTM Yes 62.38 79.31 69.73 84.41 79.97 75.96 79.92 74.90 75.63 AT-LSTM Yes 65.83 78.25 74.37 84.74 77.90 84.87 76.16 81.28 77.93 ATAE-LSTM Yes 60.34 74.20 70.71 84.52 77.80 83.85 74.08 78.96 75.56 AF-LSTM(CORR) Yes 64.89 79.96 74.76 86.91 80.47 86.58 74.68 81.60 78.73 AF-LSTM(CONV) Yes 68.81 83.58 75.",
      "56 AF-LSTM(CORR) Yes 64.89 79.96 74.76 86.91 80.47 86.58 74.68 81.60 78.73 AF-LSTM(CONV) Yes 68.81 83.58 75.44 87.78 81.29 87.26 78.44 81.49 80.51 BERT-Original Yes 74.57 88.25 82.66 92.31 88.17 92.37 80.50 86.84 85.71 BERT-Soft Yes 74.92 90.41 82.68 91.98 87.05 91.92 80.02 86.75 85.72 BERT-Hard Yes 74.10 89.55 83.91 92.31 88.17 93.39 81.09 87.89 86.30 Table 2: Experimental results (accuracy %) on all the datasets. Models in the \ufb01rst part are baseline methods.",
      "10 89.55 83.91 92.31 88.17 93.39 81.09 87.89 86.30 Table 2: Experimental results (accuracy %) on all the datasets. Models in the \ufb01rst part are baseline methods. The results in the \ufb01rst part (except BERT-Original) are obtained from the prior work (Tay et al., 2018). Avg column presents macro-averaged results across all the datasets. Firstly, we observe that BERT-Original, BERT-Soft, and BERT-Hard outperform all soft attention baselines (in the \ufb01rst part of Table 2), which demonstrates the effective- ness of \ufb01ne-tuning the pre-trained model on the aspect-based sentiment classi\ufb01cation task.",
      "Particularly, BERT-Original outper- forms AF-LSTM(CONV) by 2.63%\u223c9.57%, BERT-Soft outperforms AF-LSTM(CONV) by 2.01%\u223c9.60% and BERT-Hard improves AF-LSTM(CONV) by 3.38%\u223c11.23% in terms of accuracy. Considering the average score across eight settings, BERT-Original outper- forms AF-LSTM(CONV) by 6.46%, BERT-Soft outperforms AF-LSTM(CONV) by 6.47% and BERT-Hard outperforms AF-LSTM(CONV) by 7.19% respectively. Secondly, we compare the performance of three BERT-related methods. The performance of BERT-Original and BERT-Soft are similar by comparing their average scores. The reason may be that the original BERT has already modeled the deep relationships between the sentence and the aspect. BERT-Original can be thought of as a kind of soft-selection approach as BERT-Soft.",
      "The reason may be that the original BERT has already modeled the deep relationships between the sentence and the aspect. BERT-Original can be thought of as a kind of soft-selection approach as BERT-Soft. We also observe that the snippet selection by re- inforcement learning improves the performance over soft-selection approaches in almost all set- tings. However, the improvement of BERT-Hard over BERT-Soft is marginal. The average score of BERT-Hard is better than BERT-Soft by 0.68%. The improvement percentages are between 0.36% and 1.49%, while on the Laptop dataset, the per- formance of BERT-Hard is slightly weaker than BERT-Soft. The main reason is that the datasets only contain a small portion of multi-aspect sen- tences with different polarities. The distraction of attention will not impact the sentiment prediction much in single-aspect sentences or multi-aspect sentences with the same polarities. 4.6 Experimental Results on Multi-Aspect Sentences On the one hand, the attention distraction issue be- comes worse in multi-aspect sentences.",
      "The distraction of attention will not impact the sentiment prediction much in single-aspect sentences or multi-aspect sentences with the same polarities. 4.6 Experimental Results on Multi-Aspect Sentences On the one hand, the attention distraction issue be- comes worse in multi-aspect sentences. In addi- tion to noisy and misleading words, the attention is also prone to be distracted by opinion words from other aspects of the sentence. On the other hand, the attention distraction impacts the performance of sentiment prediction more in multi-aspect sen- tences than in single-aspect sentences. Hence, we evaluate the performance of our models on a test dataset with only multi-aspect sentences. A multi-aspect sentence can be categorized by two dimensions: the Number of aspects and the Polarity dimension which indicates whether the sentiment polarities of all aspects are the same or not. In the dimension of Number, we categorize the multi-aspect sentences as 2-3 and More. 2-3 refers to the sentences with two or three aspects while More refers to the sentences with more than three aspects. The statistics in the original dataset shows that there are much more sentences with 2- 3 aspects than those with More aspects.",
      "2-3 refers to the sentences with two or three aspects while More refers to the sentences with more than three aspects. The statistics in the original dataset shows that there are much more sentences with 2- 3 aspects than those with More aspects. In the di- mension Polarity, the multi-aspect sentences can be categorized into Same and Diff. Same indicates that all aspects in the sentence have the same senti- ment polarity. Diff indicates that the aspects have",
      "Type Same Diff Total 2-3 More Total 2-3 More Total Number 1665 352 2017 655 327 982 2999 Table 3: Distribution of the multi-aspect test set. Around 67% of the multi-aspect sentences belong to the Same category. Constructed Multi-Aspect Training Set Total Single P N Nu 891 297 297 297 Same Diff Multi 2-asp 2P 2N 2Nu PN PNu NNu 3600 300 300 300 300 300 300 3-asp 3P 3N 3Nu 2P1N 1P2N PNNu 300 300 300 300 300 300 Table 4: Distribution of the multi-aspect training set. 2-asp and 3-asp indicate that the sentence contains two or three aspects respectively. Each multi-aspect sen- tence is categorized as Same or Diff. different polarities. Multi-aspect test set.",
      "2-asp and 3-asp indicate that the sentence contains two or three aspects respectively. Each multi-aspect sen- tence is categorized as Same or Diff. different polarities. Multi-aspect test set. To evaluate the perfor- mance of our models on multi-aspect sentences, we construct a new multi-aspect test set by se- lecting all multi-aspect sentences from the original training, development, and test sets of the Restau- rants term-level task. The details are shown in Ta- ble 3. Multi-aspect training set. Since we use all multi-aspect sentences for testing, we need to generate some \u201cvirtual\u201d multi-aspect sentences for training. The simulated multi-aspect training set includes the original single-aspect sentences and the newly constructed multi-aspect sentences, which are generated by concatenating multiple single-aspect sentences with different aspects. We keep the balance of each subtype in the new train- ing set (see Table 4). The number of Neutral sen- tences is the least among three sentiment polarities in all single-aspect sentences. We randomly se- lect the same number of Positive and Negative sen- tences.",
      "We keep the balance of each subtype in the new train- ing set (see Table 4). The number of Neutral sen- tences is the least among three sentiment polarities in all single-aspect sentences. We randomly se- lect the same number of Positive and Negative sen- tences. Then we construct multi-aspect sentences by combining single-aspect sentences in different combinations of polarities. The naming for dif- ferent combinations is simple. For example, 2P- 1N indicates that the sentence has two positive as- pects and one negative aspect, and P-N-Nu means that the three aspects in the sentence are positive, negative, and neutral respectively. For simplicity, Type Same Diff Total 2-3 More Total BERT-Original 73.33 57.10 60.86 58.35 68.42 BERT-Soft 75.31 57.25 57.19 57.23 69.39 BERT-Hard 76.90 60.15 64.53 61.61 71.89 Table 5: Experimental results (accuracy %) on multi- aspect sentences.",
      "The performance of the 3-way clas- si\ufb01cation on the multi-aspect test set is reported. we only construct 2-asp and 3-asp sentences which are also the majority in the original dataset. Results and Discussions. The results on dif- ferent types of multi-aspect sentences are shown in Table 5. The performance of BERT-Hard is better than BERT-Original and BERT-Soft over all types of multi-aspect sentences. BERT-Hard out- performs BERT-Soft by 2.11% when the aspects have the same sentiment polarities. For multi- aspect sentences with different polarities, the im- provements are more signi\ufb01cant. BERT-Hard out- performs BERT-Soft by 7.65% in total of Diff. The improvements are 5.07% and 12.83% for the types 2-3 and More respectively, which demonstrates the ability of our model on handling sentences with More aspects. Particularly, BERT-Soft has the poorest performance on the subset Diff among the three methods, which proves that soft attention is more likely to cause attention distraction.",
      "Particularly, BERT-Soft has the poorest performance on the subset Diff among the three methods, which proves that soft attention is more likely to cause attention distraction. Intuitively, when multiple aspects in the sen- tence have the same sentiment polarities, even the attention is distracted to other opinion words of other aspects, it can still predict correctly to some extent. In such sentences, the impact of the at- tention distraction is not obvious and dif\ufb01cult to detect. However, when the aspects have differ- ent sentiment polarities, the attention distraction will lead to catastrophic error prediction, which will obviously decrease the classi\ufb01cation accu- racy. As shown in Table 5, the accuracy of Diff is much worse than Same for all three methods. It means that the type of Diff is dif\ufb01cult to han- dle. Even though, the signi\ufb01cant improvement proves that our hard-selection method can alle- viate the attention distraction to a certain extent. For soft-selection methods, the attention distrac- tion is inevitable due to their way in calculating the attention weights for every single word.",
      "Even though, the signi\ufb01cant improvement proves that our hard-selection method can alle- viate the attention distraction to a certain extent. For soft-selection methods, the attention distrac- tion is inevitable due to their way in calculating the attention weights for every single word. The noisy or irrelevant words could seize more atten- tion weights than the ground truth opinion words. Our method considers the opinion snippet as a",
      "1 Multi-Aspect Sentence Aspect Label appetizers Neutral service Negative Prediction Negative Neutral Method BERT-Soft BERT-Hard BERT-Soft BERT-Hard 0.24 0.18 1 0.12 0.15 0.12 0.12 0.12 0.06 Negative Negative the appetizers are ok, but the service is slow the appetizers are ok, but the service is slow the appetizers are ok, but the service is slow the appetizers are ok, but the service is slow Figure 3: Visualization. The attention weights are visualized for BERT-Soft, and the selected opinion snippets are marked for BERT-Hard. The correctness of the predicted results is also marked. consecutive whole, which is more resistant to at- tention distraction. 4.7 Visualization In this section, we visualize the attention weights for BERT-Soft and opinion snippets for BERT- Hard. As demonstrated in Figure 3, the multi- aspect sentence \u201cthe appetizers are OK, but the service is slow\u201d belongs to the category Diff.",
      "4.7 Visualization In this section, we visualize the attention weights for BERT-Soft and opinion snippets for BERT- Hard. As demonstrated in Figure 3, the multi- aspect sentence \u201cthe appetizers are OK, but the service is slow\u201d belongs to the category Diff. Firstly, the attention weights of BERT-Soft scat- ter among the whole sentence and could attend to irrelevant words. For the aspect service, BERT- Soft attends to the word \u201cok\u201d with relatively high score though it does not describe the aspect ser- vice. This problem also exists for the aspect appe- tizers. Furthermore, the attention distraction could cause error prediction. For the aspect appetizers, \u201cbut\u201d and \u201cslow\u201d gain high attention scores and cause the wrong sentiment prediction Negative. Secondly, our proposed method BERT-Hard can detect the opinion snippet for a given aspect. As illustrated in Figure 3, the opinion snippets are se- lected by BERT-Hard accurately.",
      "Secondly, our proposed method BERT-Hard can detect the opinion snippet for a given aspect. As illustrated in Figure 3, the opinion snippets are se- lected by BERT-Hard accurately. In the sentence \u201cthe appetizers are ok, but the service is slow\u201d, BERT-Hard can exactly locate the opinion snip- pets \u201cok\u201d and \u201cslow\u201d for the aspect appetizers and service respectively. At last, we enumerate some opinion snippets detected by BERT-Hard in Table 6. Our method can precisely detect snippets even for latent opin- ion expression and alleviate the in\ufb02uence of noisy words. For instance, \u201ccannot be beat for the qual- ity\u201d is hard to predict using soft attention because the sentiment polarity is transformed by the neg- ative word \u201ccannot\u201d. Our method can select the whole snippet without bias to any word and in this way the attention distraction can be alleviated. We also list some inaccurate snippets in Table 7. Some meaningless words around the true snippet are in- cluded, such as \u201care\u201d, \u201cand\u201d and \u201cat\u201d.",
      "Our method can select the whole snippet without bias to any word and in this way the attention distraction can be alleviated. We also list some inaccurate snippets in Table 7. Some meaningless words around the true snippet are in- cluded, such as \u201care\u201d, \u201cand\u201d and \u201cat\u201d. These Positive Snippets Negative Snippets very good prompt attentive not great bland beautifully presented can not eat this well extremely tasty unbearable conversation as interesting as possible no idea how to use cool and soothing would never go there impressed by not above ordinary cannot be beat for the quality not good Table 6: Examples of accurate opinion snippets de- tected by BERT-Hard. Inaccurate Snippets are very large and and even greater food are not terrible tasty treat at everyone who works the money and said Table 7: Examples of inaccurate opinion snippets de- tected by BERT-Hard. words do not affect the \ufb01nal prediction. A possi- ble explanation to these inaccurate words is that the true snippets are unlabeled and our method predicts them only by the supervisory signal from sentiment labels.",
      "words do not affect the \ufb01nal prediction. A possi- ble explanation to these inaccurate words is that the true snippets are unlabeled and our method predicts them only by the supervisory signal from sentiment labels. 5 Conclusion In this paper, we propose a hard-selection ap- proach for aspect-based sentiment analysis, which determines the start and end positions of the opin- ion snippet for a given input aspect. The deep as- sociations between the sentence and aspect, and the long-term dependencies within the sentence are taken into consideration by leveraging the pre- trained BERT model. With the hard selection of the opinion snippet, our approach can alle- viate the attention distraction problem of tradi- tional attention-based soft-selection methods. Ex- perimental results demonstrate the effectiveness of our method. Especially, our hard-selection ap- proach outperforms soft-selection approaches sig- ni\ufb01cantly when handling multi-aspect sentences with different sentiment polarities.",
      "6 Acknowledgement This work is supported by National Science and Technology Major Project, China (Grant No. 2018YFB0204304). References Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2014. Neural machine translation by jointly learning to align and translate. Computer Science. Erik Boiy and Marie-Francine Moens. 2009. A machine learning approach to sentiment analysis in multilingual web texts. Information retrieval, 12(5):526\u2013558. Peng Chen, Zhongqian Sun, Lidong Bing, and Wei Yang. 2017. Recurrent attention network on mem- ory for aspect sentiment analysis. In Proceedings of the 2017 conference on empirical methods in natural language processing (EMNLP), pages 452\u2013461. Jiajun Cheng, Shenglin Zhao, Jiani Zhang, Irwin King, Xin Zhang, and Hui Wang. 2017. Aspect-level sen- timent classi\ufb01cation with heat (hierarchical atten- tion) network.",
      "Jiajun Cheng, Shenglin Zhao, Jiani Zhang, Irwin King, Xin Zhang, and Hui Wang. 2017. Aspect-level sen- timent classi\ufb01cation with heat (hierarchical atten- tion) network. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Man- agement (CIKM), pages 97\u2013106. ACM. Jacob Devlin, Ming Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. Xiaowen Ding, Bing Liu, and Lei Zhang. 2009. Entity discovery and assignment for opinion mining appli- cations. In Proceedings of the 15th ACM interna- tional conference on Knowledge discovery and data mining (SIGKDD), pages 1125\u20131134. Feifan Fan, Yansong Feng, and Dongyan Zhao. 2018. Multi-grained attention network for aspect-level sentiment classi\ufb01cation.",
      "Feifan Fan, Yansong Feng, and Dongyan Zhao. 2018. Multi-grained attention network for aspect-level sentiment classi\ufb01cation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing (EMNLP), pages 3433\u20133442. Devamanyu Hazarika, Soujanya Poria, Prateek Vij, Gangeshwar Krishnamurthy, Erik Cambria, and Roger Zimmermann. 2018. Modeling inter-aspect dependencies for aspect-based sentiment analysis. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies (NAACL-HLT), pages 266\u2013270. Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent twitter senti- ment classi\ufb01cation. In Proceedings of the 49th An- nual Meeting of the Association for Computational Linguistics (ACL), pages 151\u2013160. Lishuang Li, Yang Liu, and AnQiao Zhou. 2018a.",
      "Target-dependent twitter senti- ment classi\ufb01cation. In Proceedings of the 49th An- nual Meeting of the Association for Computational Linguistics (ACL), pages 151\u2013160. Lishuang Li, Yang Liu, and AnQiao Zhou. 2018a. Hi- erarchical attention based position-aware network for aspect-level sentiment analysis. In Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL), pages 181\u2013189. Xin Li, Lidong Bing, Wai Lam, and Bei Shi. 2018b. Transformation networks for target-oriented senti- ment classi\ufb01cation. In Proceedings of the 56th An- nual Meeting of the Association for Computational Linguistics (ACL). Zhouhan Lin, Minwei Feng, Cicero Nogueira Dos San- tos, Yu Mo, Xiang Bing, Bowen Zhou, and Yoshua Bengio. 2017. A structured self-attentive sentence embedding. In The 5th International Conference on Learning Representations (ICLR). Bing Liu. 2012. Sentiment analysis and opinion min- ing.",
      "2017. A structured self-attentive sentence embedding. In The 5th International Conference on Learning Representations (ICLR). Bing Liu. 2012. Sentiment analysis and opinion min- ing. Synthesis lectures on human language tech- nologies, 5(1):1\u2013167. Dehong Ma, Sujian Li, Xiaodong Zhang, Houfeng Wang, Dehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng Wang. 2017. Interactive attention net- works for aspect-level sentiment classi\ufb01cation. In Twenty-Sixth International Joint Conference on Ar- ti\ufb01cial Intelligence (IJCAI), pages 4068\u20134074. Navonil Majumder, Soujanya Poria, Alexander Gel- bukh, Md Shad Akhtar, Erik Cambria, and Asif Ek- bal. 2018. Iarm: Inter-aspect relation modeling with memory networks in aspect-based sentiment anal- ysis. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3402\u20133411.",
      "2018. Iarm: Inter-aspect relation modeling with memory networks in aspect-based sentiment anal- ysis. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3402\u20133411. Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1- 2):1\u2013135. Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Suresh Manandhar, and Ion Androutsopoulos. 2015. SemEval-2015 task 12: Aspect based sentiment analysis. In SemEval 2015. Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. Semeval-2014 task 4: Aspect based sentiment analysis. (SemEval 2014), pages 27\u201335.",
      "2014. Semeval-2014 task 4: Aspect based sentiment analysis. (SemEval 2014), pages 27\u201335. Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. 2017. Self-critical sequence training for image captioning. In IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR), pages 7008\u20137024. Duyu Tang, Bing Qin, Xiaocheng Feng, and Ting Liu. 2015. Target-dependent sentiment classi\ufb01- cation with long short term memory. CoRR, abs/1512.01100. Duyu Tang, Bing Qin, and Ting Liu. 2016. As- pect level sentiment classi\ufb01cation with deep mem- ory network. Proceedings of the 2016 conference on empirical methods in natural language process- ing (EMNLP).",
      "Yi Tay, Anh Tuan Luu, and Siu Cheung Hui. 2018. Learning to attend via word-aspect associative fu- sion for aspect-based sentiment analysis. In The Thirty-Second Conference on the Association for the Advance of Arti\ufb01cial Intelligence (AAAI). Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information pro- cessing systems (NIPS), pages 5998\u20136008. Bailin Wang and Wei Lu. 2018. Learning latent opin- ions for aspect-level sentiment classi\ufb01cation. In The Thirty-Second Conference on the Association for the Advance of Arti\ufb01cial Intelligence (AAAI). Jingjing Wang, Jie Li, Shoushan Li, Yangyang Kang, Min Zhang, Luo Si, and Guodong Zhou. 2018a.",
      "In The Thirty-Second Conference on the Association for the Advance of Arti\ufb01cial Intelligence (AAAI). Jingjing Wang, Jie Li, Shoushan Li, Yangyang Kang, Min Zhang, Luo Si, and Guodong Zhou. 2018a. As- pect sentiment classi\ufb01cation with both word-level and clause-level attention networks. In Twenty- Seventh International Joint Conference on Arti\ufb01cial Intelligence (IJCAI). Shuai Wang, Sahisnu Mazumder, Bing Liu, Mianwei Zhou, and Yi Chang. 2018b. Target-sensitive mem- ory networks for aspect sentiment classi\ufb01cation. In Proceedings of the 56th Annual Meeting of the Asso- ciation for Computational Linguistics (ACL), pages 957\u2013967. Yequan Wang, Minlie Huang, Li Zhao, et al. 2016. Attention-based lstm for aspect-level sentiment clas- si\ufb01cation. In Proceedings of the 2016 conference on empirical methods in natural language processing (EMNLP), pages 606\u2013615. Ronald J. Williams. 1992.",
      "2016. Attention-based lstm for aspect-level sentiment clas- si\ufb01cation. In Proceedings of the 2016 conference on empirical methods in natural language processing (EMNLP), pages 606\u2013615. Ronald J. Williams. 1992. Simple statistical gradient- following algorithms for connectionist reinforce- ment learning. Machine Learning, 8(3-4):229\u2013256."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1909.11297.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":9681,
  "avg_doclen":176.0181818182,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1909.11297.pdf"
    }
  }
}