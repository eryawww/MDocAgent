{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Message Passing Attention Networks for Document Understanding Giannis Nikolentzos,1 Antoine J.-P. Tixier,1 Michalis Vazirgiannis1,2 1 \u00b4Ecole Polytechnique 2Athens University of Economics and Business {nikolentzos,anti5662,mvazirg}@lix.polytechnique.fr Abstract Graph neural networks have recently emerged as a very effec- tive framework for processing graph-structured data. These models have achieved state-of-the-art performance in many tasks. Most graph neural networks can be described in terms of message passing, vertex update, and readout functions. In this paper, we represent documents as word co-occurrence networks and propose an application of the message passing framework to NLP, the Message Passing Attention network for Document understanding (MPAD). We also propose sev- eral hierarchical variants of MPAD. Experiments conducted on 10 standard text classi\ufb01cation datasets show that our ar- chitectures are competitive with the state-of-the-art. Ablation studies reveal further insights about the impact of the differ- ent components on performance.",
      "Experiments conducted on 10 standard text classi\ufb01cation datasets show that our ar- chitectures are competitive with the state-of-the-art. Ablation studies reveal further insights about the impact of the differ- ent components on performance. Code is publicly available at: https://github.com/giannisnik/mpad. 1 Introduction The concept of message passing over graphs has been around for many years (Weisfeiler and Lehman 1968; Mur- phy, Weiss, and Jordan 1999), as well as that of graph neu- ral networks (GNNs) (Gori, Monfardini, and Scarselli 2005; Scarselli et al. 2008). However, GNNs have only recently started to be closely investigated, following the advent of deep learning. Some notable examples include (Duvenaud et al. 2015; Battaglia et al. 2016; Li et al. 2016; Defferrard, Bresson, and Vandergheynst 2016; Kearnes et al.",
      "Some notable examples include (Duvenaud et al. 2015; Battaglia et al. 2016; Li et al. 2016; Defferrard, Bresson, and Vandergheynst 2016; Kearnes et al. 2016; Kipf and Welling 2016; Hamilton, Ying, and Leskovec 2017; Veli\u02c7ckovi\u00b4c et al. 2017; Xu et al. 2018b). These approaches are known as spectral. Their similarity with message passing (MP) was observed by (Kipf and Welling 2016) and formal- ized by (Gilmer et al. 2017) and (Xu et al. 2018a). The MP framework is based on the core idea of recur- sive neighborhood aggregation. That is, at every iteration, the representation of each vertex is updated based on mes- sages received from its neighbors. The majority of the spec- tral GNNs can be described in terms of the MP framework.",
      "That is, at every iteration, the representation of each vertex is updated based on mes- sages received from its neighbors. The majority of the spec- tral GNNs can be described in terms of the MP framework. GNNs have been applied with great success to bioinfor- matics and social network data, for node classi\ufb01cation, link prediction, and graph classi\ufb01cation. However, a few studies only have focused on the application of the MP framework to representation learning on text. This paper proposes one such application. More precisely, we represent documents as word co-occurrence networks, and develop an expres- sive MP GNN tailored to document understanding, the Mes- sage Passing Attention network for Document understand- ing (MPAD). We also propose several hierarchical variants of MPAD. Evaluation on 10 document classi\ufb01cation datasets shows that our architectures learn representations that are competitive with the state-of-the-art. Furthermore, ablation experiments shed light on the impact of various architectural choices. In what follows, we \ufb01rst provide some background about the MP framework (sec. 2), thoroughly describe and ex- plain MPAD (sec.",
      "Furthermore, ablation experiments shed light on the impact of various architectural choices. In what follows, we \ufb01rst provide some background about the MP framework (sec. 2), thoroughly describe and ex- plain MPAD (sec. 3), present our experimental framework (sec. 4), report and interpret our results (sec. 5), and provide a review of the relevant literature (sec. 6). 2 Message Passing Neural Networks (Gilmer et al. 2017) proposed a MP framework under which many of the recently introduced GNNs can be reformu- lated1. MP consists in an aggregation phase followed by a combination phase (Xu et al. 2018a). More precisely, let G = (V, E) be a graph, and let us consider v \u2208V .",
      "MP consists in an aggregation phase followed by a combination phase (Xu et al. 2018a). More precisely, let G = (V, E) be a graph, and let us consider v \u2208V . At time t + 1, a message vector mt+1 v is computed from the representations of the neighbors N(v) of v: mt+1 v = AGGREGATEt+1\u0000\b ht w | w \u2208N(v) \t\u0001 (1) The new representation ht+1 v of v is then computed by com- bining its current feature vector ht v with the message vector mt+1 v : ht+1 v = COMBINEt+1\u0000ht v, mt+1 v \u0001 (2) Messages are passed for T time steps. Each step is imple- mented by a different layer of the MP network. Hence, iter- ations correspond to network depth. The \ufb01nal feature vector hT v of v is based on messages propagated from all the nodes in the subtree of height T rooted at v. It captures both the topology of the neighborhood of v and the distribution of the vertex representations in it.",
      "Hence, iter- ations correspond to network depth. The \ufb01nal feature vector hT v of v is based on messages propagated from all the nodes in the subtree of height T rooted at v. It captures both the topology of the neighborhood of v and the distribution of the vertex representations in it. If a graph-level feature vector is needed, e.g., for classi- \ufb01cation or regression, a READOUT pooling function, that 1Note that some GNNs, known as spatial, are not based on MP (Niepert, Ahmed, and Kutzkov 2016; Nikolentzos et al. 2018; Tixier et al. 2019). arXiv:1908.06267v2  [cs.CL]  22 Nov 2019",
      "must be invariant to permutations, is applied: hG = READOUT \u0000\b hT v | v \u2208V \t\u0001 (3) Next, we present the MP network we developed for docu- ment understanding. 3 Message Passing Attention network for Document understanding (MPAD) 3.1 Word co-occurrence networks We represent a document as a statistical word co-occurrence network (Mihalcea and Tarau 2004) with a sliding window of size 2 overspanning sentences. Let us denote that graph by G = (V, E). Each unique word in the preprocessed docu- ment is represented by a node in G, and an edge is added be- tween two nodes if they are found together in at least one in- stantiation of the window. G is directed and weighted: edge directions and weights respectively capture text \ufb02ow and co- occurrence counts. G is a compact representation of its document. In G, immediate neighbors are consecutive words in the same sentence2. That is, paths of length 2 correspond to bi- grams.",
      "G is directed and weighted: edge directions and weights respectively capture text \ufb02ow and co- occurrence counts. G is a compact representation of its document. In G, immediate neighbors are consecutive words in the same sentence2. That is, paths of length 2 correspond to bi- grams. Paths of length more than 2 can correspond either to traditional n-grams or to relaxed n-grams, that is, words that never appear in the same sentence but co-occur with the same word(s). Such nodes are linked through common neighbors. Master node. Inspired by (Scarselli et al. 2008), our graph G also includes a special document node, linked to all other nodes via unit weight bi-directional edges. In what follows, let us denote by n the number of nodes in G, including the master node.",
      "Master node. Inspired by (Scarselli et al. 2008), our graph G also includes a special document node, linked to all other nodes via unit weight bi-directional edges. In what follows, let us denote by n the number of nodes in G, including the master node. 3.2 Message passing We formulate our AGGREGATE function as: Mt+1 = MLPt+1\u0000D\u22121AHt\u0001 (4) where Ht \u2208Rn\u00d7d contains node features (d is a hyperpa- rameter3), and A \u2208Rn\u00d7n is the adjacency matrix of G. Since G is directed, A is asymmetric. Also, A has zero diagonal as we choose not to consider the feature of the node itself, only that of its incoming neighbors, when up- dating its representation4. Since G is weighted, the ith row of A contains the weights of the edges incoming on node vi. D \u2208Rn\u00d7n is the diagonal in-degree matrix of G. MLP denotes a multi-layer perceptron, and Mt+1 \u2208Rn\u00d7d is the message matrix.",
      "Since G is weighted, the ith row of A contains the weights of the edges incoming on node vi. D \u2208Rn\u00d7n is the diagonal in-degree matrix of G. MLP denotes a multi-layer perceptron, and Mt+1 \u2208Rn\u00d7d is the message matrix. The use of a MLP was motivated by the observation that for graph classi\ufb01cation, MP neural nets with 1-layer percep- trons are inferior to their MLP counterparts (Xu et al. 2018a). Indeed, 1-layer perceptrons are not universal approximators 2except for words at the end/beginning of two successive sen- tences. 3at t=0, d is equal to the dimensionality of the pretrained word embeddings. 4the feature of the node itself is already taken into account by our GRU-based COMBINE function (see Eq. 5). of multiset functions. Note that like in (Xu et al. 2018a), we use a different MLP at each layer. Renormalization. The rows of D\u22121A sum to 1.",
      "5). of multiset functions. Note that like in (Xu et al. 2018a), we use a different MLP at each layer. Renormalization. The rows of D\u22121A sum to 1. This is equivalent to the renormalization trick of (Kipf and Welling 2016), but using only the in-degrees. That is, instead of com- puting a weighted sum of the incoming neighbors\u2019 feature vectors, we compute a weighted average of them. The co- ef\ufb01cients are proportional to the strength of co-occurrence between words. One should note that by averaging, we lose the ability to distinguish between different neighborhood structures in some special cases, that is, we lose injectiv- ity. Such cases include neighborhoods in which all nodes have the same representations, and neighborhoods of differ- ent sizes containing various representations in equal propor- tions (Xu et al. 2018a). As suggested by the results of an ab- lation experiment, averaging is better than summing in our application (see subsection 5.2).",
      "2018a). As suggested by the results of an ab- lation experiment, averaging is better than summing in our application (see subsection 5.2). Note that instead of simply summing/averaging, we also tried using GAT-like attention (Veli\u02c7ckovi\u00b4c et al. 2017) in early experiments, without ob- taining better results. As far as our COMBINE function, we use the Gated Re- current Unit (Cho et al. 2014; Chung et al.",
      "2017) in early experiments, without ob- taining better results. As far as our COMBINE function, we use the Gated Re- current Unit (Cho et al. 2014; Chung et al. 2014): Ht+1 = GRU(Ht, Mt+1) (5) Omitting biases for readability, we have: Rt+1 = \u03c3(Wt+1 R Mt+1 + Ut+1 R Ht) Zt+1 = \u03c3(Wt+1 Z Mt+1 + Ut+1 Z Ht) \u02dcHt+1 = tanh(Wt+1Mt+1 + Ut+1(Rt+1 \u2299Ht)) Ht+1 = (1 \u2212Zt+1) \u2299Ht + Zt+1 \u2299\u02dcHt+1 (6) where the W and U are trainable weight matrices not shared across time steps, \u03c3(x) = 1/(1 + exp(\u2212x)) is the sigmoid function, and R and Z are the parameters of the reset and up- date gates.",
      "The reset gate controls the amount of information from the previous time step (in Ht) that should propagate to the candidate representations, \u02dcHt+1. The new represen- tations Ht+1 are \ufb01nally obtained by linearly interpolating between the previous and the candidate ones, using the co- ef\ufb01cients returned by the update gate. Interpretation. Updating node representations through a GRU should in principle allow nodes to encode a combi- nation of local and global signals (low and high values of t, resp.), by allowing them to remember about past iterations. In addition, we also explicitly consider node representations at all iterations when reading out (see Eq. 8). 3.3 Readout After passing messages and performing updates for T iter- ations, we obtain a matrix HT \u2208Rn\u00d7d containing the \ufb01- nal vertex representations. Let \u02c6G be graph G without the special document node and its adjacent edges, and matrix \u02c6HT \u2208R(n\u22121)\u00d7d be the corresponding representation ma- trix (i.e., HT without the row of the document node).",
      "Let \u02c6G be graph G without the special document node and its adjacent edges, and matrix \u02c6HT \u2208R(n\u22121)\u00d7d be the corresponding representation ma- trix (i.e., HT without the row of the document node). We use as our READOUT function the concatenation of self-attention applied to \u02c6HT with the \ufb01nal document node representation. More precisely, we apply a global self- attention mechanism (Lin et al. 2017) to the rows of \u02c6HT .",
      "As shown in Eq. 7, \u02c6HT is \ufb01rst passed to a dense layer pa- rameterized by matrix WT A \u2208Rd\u00d7d. An alignment vector \u03b1 is then derived by comparing, via dot products, the rows of the output of the dense layer YT \u2208R(n\u22121)\u00d7d with a train- able vector vT \u2208Rd (initialized randomly) and normalizing with a softmax. The normalized alignment coef\ufb01cients are \ufb01nally used to compute the attentional vector uT \u2208Rd as a weighted sum of the \ufb01nal representations \u02c6HT . YT = tanh( \u02c6HT WT A) \u03b1T i = exp(Yi T \u00b7 vT ) Pn\u22121 j=1 exp(Yj T \u00b7 vT ) uT = n\u22121 X i=1 \u03b1T i \u02c6HT i (7) Note that we tried with multiple context vectors, i.e., with a matrix VT instead of a vector vT , like in (Lin et al. 2017), but results were not convincing, even when adding a regu- larization term to the loss to favor diversity among the rows of VT .",
      "2017), but results were not convincing, even when adding a regu- larization term to the loss to favor diversity among the rows of VT . Master node skip connection. hT G \u2208R2d is obtained by concatenating uT and the \ufb01nal master node representation. That is, the master node vector bypasses the attention mech- anism. This is equivalent to a skip or shortcut connection (He et al. 2016). The reason behind this choice is that we expect the special document node to learn a high-level summary about the document, such as its size, vocabulary, etc. (more details are given in subsection 5.2). Therefore, by making the master node bypass the attention layer, we directly inject global information about the document into its \ufb01nal repre- sentation. Multi-readout. (Xu et al. 2018a), inspired by Jumping Knowledge Networks (Xu et al. 2018b), recommend to not only use the \ufb01nal representations when performing readout, but also that of the earlier steps. Indeed, as one iterates, node features capture more and more global information.",
      "2018a), inspired by Jumping Knowledge Networks (Xu et al. 2018b), recommend to not only use the \ufb01nal representations when performing readout, but also that of the earlier steps. Indeed, as one iterates, node features capture more and more global information. How- ever, retaining more local, intermediary information might be useful too. Thus, instead of applying the readout function only to t = T, we apply it to all time steps and concatenate the results, \ufb01nally obtaining hG \u2208RT \u00d72d : hG = CONCAT \u0000READOUT \u0000Ht\u0001 | t = 1 . . . T \u0001 (8) In effect, with this modi\ufb01cation, we take into account fea- tures based on information aggregated from subtrees of dif- ferent heights (from 1 to T), corresponding to local and global features. 3.4 Hierarchical variants of MPAD Through the successive MP iterations, it could be argued that MPAD implicitly captures some soft notion of the hi- erarchical structure of documents (words \u2192bigrams \u2192 compositions of bigrams, etc.).",
      "3.4 Hierarchical variants of MPAD Through the successive MP iterations, it could be argued that MPAD implicitly captures some soft notion of the hi- erarchical structure of documents (words \u2192bigrams \u2192 compositions of bigrams, etc.). However, it might be ben- e\ufb01cial to explicitly capture document hierarchy. Hierarchi- cal architectures have brought signi\ufb01cant improvements to many NLP tasks, such as language modeling and genera- tion (Lin et al. 2015; Li, Luong, and Jurafsky 2015), sen- timent and topic classi\ufb01cation (Tang, Qin, and Liu 2015; sentence encoder \u2026 MP self att. skip \u2026 ; [ ] s1 sD sentence encoder doc encoder w1 wS \u2026 readout \u2026 dense MP readout MLP +  softmax \u2026 Figure 1: Illustration of MPAD-path (\u229a: master node). Yang et al. 2016), and spoken language understanding (Ra- heja and Tetreault 2019; Shang et al. 2019). Inspired by this line of research, we propose several hierarchical variants of MPAD, detailed in what follows.",
      "Yang et al. 2016), and spoken language understanding (Ra- heja and Tetreault 2019; Shang et al. 2019). Inspired by this line of research, we propose several hierarchical variants of MPAD, detailed in what follows. In all of them, we repre- sent each sentence in the document as a word co-occurrence network, and obtain an embedding for it by applying MPAD as previously described. MPAD-sentence-att. Here, the sentence embeddings are simply combined through self-attention. MPAD-clique. In this variant, we build a complete graph where each node represents a sentence. We then feed that graph to MPAD, where the feature vectors of the nodes are initialized with the sentence embeddings previously ob- tained. MPAD-path. This variant, shown in Fig. 1, is similar to the clique one, except that instead of a complete graph, we build a path according to the natural \ufb02ow of the text. That is, two nodes are linked by a directed edge if the two sentences they represent follow each other in the document.",
      "1, is similar to the clique one, except that instead of a complete graph, we build a path according to the natural \ufb02ow of the text. That is, two nodes are linked by a directed edge if the two sentences they represent follow each other in the document. Note that the sentence graphs in MPAD-clique and MPAD-path do not feature a master node. 4 Experiments 4.1 Datasets We evaluate the quality of the document embeddings learned by MPAD on 10 document classi\ufb01cation datasets, covering the topic identi\ufb01cation, coarse and \ufb01ne sentiment analysis and opinion mining, and subjectivity detection tasks. We brie\ufb02y introduce the datasets next. Their statistics are re- ported in Table 1. (1) Reuters contains stories from the Reuters news agency. We used the ModApte split, removed documents belonging to multiple classes and considered only the 8 classes with the highest number of training examples. (2) BBCSport (Greene and Cunningham 2006) contains sports news articles from the BBC Sport website.",
      "We used the ModApte split, removed documents belonging to multiple classes and considered only the 8 classes with the highest number of training examples. (2) BBCSport (Greene and Cunningham 2006) contains sports news articles from the BBC Sport website. (3) Polarity (Pang and Lee 2005) features positive and neg- ative labeled snippets from Rotten Tomatoes.",
      "Dataset # training # test # classes av. # words max # words voc. size # pretrained examples examples words Reuters 5,485 2,189 8 102.3 964 23,585 15,587 Snippets 10,060 2,280 8 18.0 50 29,257 17,142 BBCSport 737 CV 5 380.5 1,818 14,340 13,390 Polarity 10,662 CV 2 20.3 56 18,777 16,416 Subjectivity 10,000 CV 2 23.3 120 21,335 17,896 MPQA 10,606 CV 2 3.0 36 6,248 6,085 IMDB 25,000 25,000 2 254.3 2,633 141,655 104,391 TREC 5,452 500 6 10.0 37 9,593 9,125 SST-1 157,918 2,210 5 7.4 53 17,833 16,262 SST-2 77,833 1,",
      "391 TREC 5,452 500 6 10.0 37 9,593 9,125 SST-1 157,918 2,210 5 7.4 53 17,833 16,262 SST-2 77,833 1,821 2 9.5 53 17,237 15,756 Yelp2013 301,514 33,504 5 143.7 1,184 48,212 48,212 Table 1: Statistics of the datasets used in our experiments. CV indicates that cross-validation was used. # pretrained words refers to the number of words in the vocabulary having an entry in the Google News word vectors (except for Yelp2013). (4) Subjectivity (Pang and Lee 2004) contains movie review snippets from Rotten Tomatoes (subjective sentences), and IMDB plot summaries (objective sentences). (5) MPQA (Wiebe, Wilson, and Cardie 2005) is made of positive and negative phrases, annotated as part of the sum- mer 2002 NRRC Workshop on Multi-Perspective Question Answering.",
      "(5) MPQA (Wiebe, Wilson, and Cardie 2005) is made of positive and negative phrases, annotated as part of the sum- mer 2002 NRRC Workshop on Multi-Perspective Question Answering. (6) IMDB (Maas et al. 2011) is a collection of highly polar- ized movie reviews (positive/negative). (7) TREC (Li and Roth 2002) consists of questions that are classi\ufb01ed into 6 different categories. (8) SST-1 (Socher et al. 2013) contains the same snippets as Polarity, split into multiple sentences and annotated with \ufb01ne-grained polarity (from very negative to very positive). (9) SST-2 (Socher et al. 2013) is the same as SST-1 but with neutral reviews removed and snippets classi\ufb01ed as positive or negative. (10) Yelp2013 (Tang, Qin, and Liu 2015) features reviews obtained from the 2013 Yelp Dataset Challenge.",
      "2013) is the same as SST-1 but with neutral reviews removed and snippets classi\ufb01ed as positive or negative. (10) Yelp2013 (Tang, Qin, and Liu 2015) features reviews obtained from the 2013 Yelp Dataset Challenge. 4.2 Baselines We evaluate MPAD against multiple state-of-the-art baseline models, including hierarchical ones, to enable fair compari- son with the hierarchical MPAD variants. Doc2vec (Le and Mikolov 2014) is an extension of word2vec that learns vectors for documents in a fully un- supervised manner. Document embeddings are then fed to a logistic regression classi\ufb01er. CNN (Kim 2014). 1D convolutional neural network where the word embeddings are used as channels (depth dimen- sions). DAN (Iyyer et al. 2015). The Deep Averaging Network passes the unweighted average of the embeddings of the in- put words through multiple dense layers and a \ufb01nal softmax.",
      "DAN (Iyyer et al. 2015). The Deep Averaging Network passes the unweighted average of the embeddings of the in- put words through multiple dense layers and a \ufb01nal softmax. Tree-LSTM (Tai, Socher, and Manning 2015) is a general- ization of the standard LSTM architecture to constituency and dependency parse trees. DRNN (Irsoy and Cardie 2014). Recursive neural networks are stacked and applied to parse trees. LSTMN (Cheng, Dong, and Lapata 2016) is an extension of the LSTM model where the memory cell is replaced by a memory network which stores word representations. C-LSTM (Zhou et al. 2015) combines convolutional and re- current neural networks. The region embeddings provided by a CNN are fed to a LSTM. SPGK (Nikolentzos et al. 2017) also models documents as word co-occurrence networks. It computes a graph kernel that compares shortest paths extracted from the word co- occurrence networks and then relies on a SVM. WMD (Kusner et al.",
      "SPGK (Nikolentzos et al. 2017) also models documents as word co-occurrence networks. It computes a graph kernel that compares shortest paths extracted from the word co- occurrence networks and then relies on a SVM. WMD (Kusner et al. 2015) is an application of the well- known Earth Mover\u2019s Distance to text. A k-nearest neighbor classi\ufb01er is used. DiSAN (Shen et al. 2018) uses directional self-attention along with multi-dimensional attention to generate docu- ment representations. LSTM-GRNN (Tang, Qin, and Liu 2015) is a hierarchical model where sentence embeddings are obtained with a CNN and a GRU-RNN is fed the sentence representations to ob- tain a document vector. HN-ATT (Yang et al. 2016) is another hierarchical model, where the same encoder architecture (a bidirectional GRU- RNN) is used for both sentences and documents. Self- attention is applied at each level. 4.3 Model con\ufb01guration and training We preprocess all datasets using the code of (Kim 2014).",
      "Self- attention is applied at each level. 4.3 Model con\ufb01guration and training We preprocess all datasets using the code of (Kim 2014). On Yelp2013, we also replace all tokens appearing strictly less than 6 times with a special UNK token, like in (Yang et al. 2016). We then build a directed word co-occurrence network from each document, with a window of size 2. We use two MP iterations (T=2) for the basic MPAD, and two MP iterations at each level, for the hierarchical variants. The output of the readout goes through a dense layer before reaching the \ufb01nal classi\ufb01cation layer (or the next level, at the \ufb01rst level of MPAD-path and MPAD-clique). We set d to 64, except on IMDB and Yelp on which d = 128, and use a two-layer MLP. The \ufb01nal graph representations are passed through a softmax for classi\ufb01cation. All our dense layers (except in self-attention) use ReLU activation.",
      "We set d to 64, except on IMDB and Yelp on which d = 128, and use a two-layer MLP. The \ufb01nal graph representations are passed through a softmax for classi\ufb01cation. All our dense layers (except in self-attention) use ReLU activation. We train MPAD in an end-to-end fashion by minimizing the cross- entropy loss function with the Adam optimizer (Kingma and Ba 2014) and an initial learning rate of 0.001. To regulate potential differences in magnitude, we apply batch normalization after concatenating the feature vector of the master node with the self-attentional vector, that is,",
      "after the skip connection (see subsection 3.3). To prevent over\ufb01tting, we use dropout (Srivastava et al. 2014) with a rate of 0.5. We select the best epoch, capped at 200, based on the validation accuracy. When cross-validation is used (see 3rd column of Table 1), we construct a validation set by randomly sampling 10% of the training set of each fold. On all datasets except Yelp2013, we use the publicly available5 300-dimensional pre-trained Google News vec- tors (Mikolov et al. 2013) to initialize the node representa- tions H0. On Yelp2013, we follow (Yang et al. 2016) and learn our own word vectors from the training and validation sets with the gensim implementation of word2vec ( \u02c7Reh\u02dau\u02c7rek and Sojka 2010). MPAD was implemented in Python 3.6 us- ing the PyTorch library.",
      "MPAD was implemented in Python 3.6 us- ing the PyTorch library. All experiments were run on a single machine consisting of a 3.4 GHz Intel Core i7 CPU with 16 GB of RAM and an NVidia GeForce Titan Xp GPU. 5 Results and ablations 5.1 Results Experimental results are shown in Table 2. For the base- lines, we provide the scores reported in the original pa- pers. Furthermore, we have evaluated some of the baselines on the rest of our benchmark datasets, and we also report these scores. MPAD reaches best performance on 5 out of 10 datasets, and is close second elsewhere. Moreover, the 5 datasets on which MPAD ranks \ufb01rst widely differ in training set size, number of categories, and prediction task (topic, sentiment, etc.), which indicates that MPAD can perform well in different settings. MPAD vs. hierarchical variants. On 9 datasets out of 10, one or more of the hierarchical variants outperform the vanilla MPAD architecture, highlighting the bene\ufb01t of ex- plicitly modeling the hierarchical nature of documents.",
      "MPAD vs. hierarchical variants. On 9 datasets out of 10, one or more of the hierarchical variants outperform the vanilla MPAD architecture, highlighting the bene\ufb01t of ex- plicitly modeling the hierarchical nature of documents. However, on Subjectivity, standard MPAD outperforms all hierarchical variants. On TREC, it reaches the same ac- curacy. We hypothesize that in some cases, using a different graph to separately encode each sentence might be worse than using one single graph to directly encode the docu- ment. Indeed, in the single document graph, some words that never appear in the same sentence can be connected through common neighbors, as was explained in subsection 3.1. So, this way, some notion of cross-sentence context is captured while learning representations of words, bigrams, etc. at each MP iteration. This creates better informed representa- tions, resulting in a better document embedding. With the hi- erarchical variants, on the other hand, each sentence vector is produced in isolation, without any contextual information about the other sentences in the document.",
      "at each MP iteration. This creates better informed representa- tions, resulting in a better document embedding. With the hi- erarchical variants, on the other hand, each sentence vector is produced in isolation, without any contextual information about the other sentences in the document. Therefore, the \ufb01nal sentence embeddings might be of lower quality, and as a group might also contain redundant/repeated informa- tion. When the sentence vectors are \ufb01nally combined into a document representation, it is too late to take context into account. 5.2 Ablation studies To understand the impact of some hyperparameters on performance, we conducted additional experiments on 5https://code.google.com/archive/p/word2vec the Reuters, Polarity, and IMDB datasets, with the non- hierarchical version of MPAD. Results are shown in Table 3. Number of MP iterations. First, we varied the number of message passing iterations from 1 to 4. We can clearly see in Table 3 that having more iterations improves performance. We attribute this to the fact that we are reading out at each it- eration from 1 to T (see Eq.",
      "First, we varied the number of message passing iterations from 1 to 4. We can clearly see in Table 3 that having more iterations improves performance. We attribute this to the fact that we are reading out at each it- eration from 1 to T (see Eq. 8), which enables the \ufb01nal graph representation to encode a mixture of low-level and high- level features. Indeed, in initial experiments involving read- out at t=T only, setting T \u22652 was always decreasing perfor- mance, despite the GRU-based updates (Eq. 5)6. These re- sults were consistent with that of (Yao, Mao, and Luo 2019) and (Kipf and Welling 2016), who both are reading out only at t=T too. We hypothesize that node features at T \u22652 are too diffuse to be entirely relied upon during readout. More precisely, initially at t=0, node representations capture infor- mation about words, at t=1, about their 1-hop neighborhood (bigrams), at t=2, about compositions of bigrams, etc.",
      "More precisely, initially at t=0, node representations capture infor- mation about words, at t=1, about their 1-hop neighborhood (bigrams), at t=2, about compositions of bigrams, etc. Thus, pretty quickly, node features become general and diffuse. In such cases, considering also the lower-level, more precise features of the earlier iterations when reading out may be necessary. Undirected edges. On Reuters, using an undirected graph leads to better performance, while on Polarity and IMDB, it is the opposite. This can be explained by the fact that Reuters is a topic classi\ufb01cation task, for which the presence or absence of some patterns is important, but not necessar- ily the order in which they appear, while Polarity and IMDB are sentiment analysis tasks. To capture sentiment, modeling word order is crucial, e.g., in detecting negation. No master node. Removing the master node deteriorates performance across all datasets, clearly showing the value of having such a node. We hypothesize that since the special document node is connected to all other nodes, it is able to encode during message passing a summary of the document.",
      "No master node. Removing the master node deteriorates performance across all datasets, clearly showing the value of having such a node. We hypothesize that since the special document node is connected to all other nodes, it is able to encode during message passing a summary of the document. No renormalization. Here, we do not use the renormaliza- tion trick of (Kipf and Welling 2016) during MP (see subsec- tion 3.2). That is, Eq. 4 becomes Mt+1 = MLPt+1\u0000AHt\u0001 . In other words, instead of computing a weighted average of the incoming neighbors\u2019 feature vectors, we compute a weighted sum of them7. Unlike the mean, which captures distributions, the sum captures structural information (Xu et al. 2018a). As shown in Table 3, using sum instead of mean decreases performance everywhere, suggesting that in our application, capturing the distribution of neighbor represen- tations is more important that capturing their structure. We hypothesize that this is the case because statistical word co- occurrence networks tend to have similar structural proper- ties, regardless of the topic, polarity, sentiment, etc.",
      "We hypothesize that this is the case because statistical word co- occurrence networks tend to have similar structural proper- ties, regardless of the topic, polarity, sentiment, etc. of the corresponding documents. Neighbors-only. In this experiment, we replaced the GRU COMBINE function (see Eq. 5) with the identity function. That is, we simply have Ht+1=Mt+1. Since A has zero diagonal, by doing so, we completely ignore the previous feature of the node itself when updating its representation. 6The GRU should in principle enable nodes to retain locality in their representations, by remembering about early iterations. 7Weights are co-occurrence counts, as before.",
      "Model Reut. BBC Pol. Subj. MPQA IMDB TREC SST-1 SST-2 Yelp\u201913 doc2vec (Le and Mikolov 2014) 95.34 98.64 67.30 88.27 82.57 92.5 70.80 48.7 87.8 57.7 CNN (Kim 2014) 97.21 98.37 81.5 93.4 89.5 90.28 93.6 48.0 87.2 64.89 DAN (Iyyer et al.",
      "2015) 94.79 94.30 80.3 92.44 88.91 89.4 89.60 47.7 86.3 61.55 Tree-LSTM (Tai, Socher, and Manning 2015) - - - - - - - 51.0 88.0 - DRNN (Irsoy and Cardie 2014) - - - - - - - 49.8 86.6 - LSTMN (Cheng, Dong, and Lapata 2016) - - - - - - - 47.9 87.0 - C-LSTM (Zhou et al. 2015) - - - - - - 94.6 49.2 87.8 - SPGK (Nikolentzos et al. 2017) 96.39 94.97 77.89 91.48 85.78 OOM 90.69 OOM OOM OOM WMD (Kusner et al.",
      "2017) 96.39 94.97 77.89 91.48 85.78 OOM 90.69 OOM OOM OOM WMD (Kusner et al. 2015) 96.5 98.71 66.42 86.04 83.95 OOM 73.40 OOM OOM OOM DiSAN (Shen et al. 2018) 97.35 96.05 80.38 94.2 90.1 83.25 94.2 51.72 86.76 60.51 LSTM-GRNN (Tang, Qin, and Liu 2015) 96.16 95.52 79.98 92.38 89.08 89.98 89.40 48.09 86.38 65.1 HN-ATT (Yang et al.",
      "2016) 97.25 96.73 80.78 92.92 89.08 90.06 90.80 49.00 86.71 68.2 MPAD 97.07 98.37 80.24 93.46* 90.02 91.30 95.60* 49.09 87.80 66.16 MPAD-sentence-att 96.89 99.32 80.44 93.02 90.12* 91.70 95.60* 49.95* 88.30* 66.47 MPAD-clique 97.57* 99.72* 81.17* 92.82 89.96 91.87* 95.20 48.86 87.91 66.60 MPAD-path 97.44 99.59 80.46 93.31 89.81 91.84 93.80 49.68 87.75 66.80* Table 2: Classi\ufb01cation accuracies. Best performance per column in bold, *best MPAD variant.",
      "Best performance per column in bold, *best MPAD variant. OOM: >16GB RAM. MPAD variant Reut. Pol. IMDB MPAD 1MP 96.57 79.91 90.57 MPAD 2MP* 97.07 80.24 91.30 MPAD 3MP 97.07 80.20 91.24 MPAD 4MP 97.48 80.52 91.30 MPAD 2MP undirected 97.35 80.05 90.97 MPAD 2MP no master node 96.66 79.15 91.09 MPAD 2MP no renormalization 96.02 79.84 91.16 MPAD 2MP neighbors-only 97.12 79.22 89.50 MPAD 2MP no master node skip connection 96.93 80.62 91.12 Table 3: Ablation results. The n in nMP refers to the num- ber of message passing iterations. *vanilla model (MPAD in Table 2).",
      "The n in nMP refers to the num- ber of message passing iterations. *vanilla model (MPAD in Table 2). That is, the update is based entirely on its neighbors. Except on Reuters (almost no change), performance always suffers, stressing the need to take into account the root node during updates and not only its neighborhood. No master node skip connection. Here, the master node does not bypass the attention mechanism and is treated as a normal node. This leads to better performance on Polarity, but slightly worse performance on Reuters and IMDB. 6 Related work (Kipf and Welling 2016; Atwood and Towsley 2016; Veli\u02c7ckovi\u00b4c et al. 2017; Hamilton, Ying, and Leskovec 2017) conduct some node classi\ufb01cation experiments on citation networks, where nodes are scienti\ufb01c papers, i.e., textual data. However, text is only used to derive node feature vec- tors. The external graph structure, which plays a central role in determining node labels, is completely unrelated to text.",
      "However, text is only used to derive node feature vec- tors. The external graph structure, which plays a central role in determining node labels, is completely unrelated to text. On the other hand, (Henaff, Bruna, and LeCun 2015; Defferrard, Bresson, and Vandergheynst 2016) experiment on traditional document classi\ufb01cation tasks. They both build k-nearest neighbor similarity graphs based on the Gaussian diffusion kernel. More precisely, (Henaff, Bruna, and LeCun 2015) build one single graph where nodes are documents and distance is computed in the BoW space. Node features are then used for classi\ufb01cation. Closer to our work, (Deffer- rard, Bresson, and Vandergheynst 2016) represent each doc- ument as a graph. All document graphs are derived from the same underlying structure. Only node features, correspond- ing to the entries of the documents\u2019 BoW vectors, vary. The underlying, shared structure is that of a k-NN graph where nodes are vocabulary terms and similarity is the cosine of the word embedding vectors.",
      "All document graphs are derived from the same underlying structure. Only node features, correspond- ing to the entries of the documents\u2019 BoW vectors, vary. The underlying, shared structure is that of a k-NN graph where nodes are vocabulary terms and similarity is the cosine of the word embedding vectors. (Defferrard, Bresson, and Van- dergheynst 2016) then perform graph classi\ufb01cation. How- ever they found performance to be lower than that of a naive Bayes classi\ufb01er. (Peng et al. 2018) use a GNN for hierarchical classi\ufb01ca- tion into a large taxonomy of topics. This task differs from traditional document classi\ufb01cation. The authors represent documents as unweighted, undirected word co-occurrence networks with word embeddings as node features. They then use the spatial GNN of (Niepert, Ahmed, and Kutzkov 2016) to perform graph classi\ufb01cation. The work closest to ours is probably that of (Yao, Mao, and Luo 2019).",
      "They then use the spatial GNN of (Niepert, Ahmed, and Kutzkov 2016) to perform graph classi\ufb01cation. The work closest to ours is probably that of (Yao, Mao, and Luo 2019). The authors adopt the semi-supervised node classi\ufb01cation approach of (Kipf and Welling 2016). They build one single undirected graph from the entire dataset, with both word and document nodes. Document-word edges are weighted by TF-IDF and word-word edges are weighted by pointwise mutual information derived from co- occurrence within a sliding window. There are no document- document edges. The GNN is trained based on the cross- entropy loss computed only for the labeled nodes, that is, the documents in the training set. When the \ufb01nal node represen- tations are obtained, one can use that of the test documents to classify them and evaluate prediction performance. There are signi\ufb01cant differences between (Yao, Mao, and Luo 2019) and our work. First, our approach is inductive8, not transductive.",
      "There are signi\ufb01cant differences between (Yao, Mao, and Luo 2019) and our work. First, our approach is inductive8, not transductive. Indeed, while the node classi\ufb01cation ap- proach of (Yao, Mao, and Luo 2019) requires all test doc- uments at training time, our graph classi\ufb01cation model is able to perform inference on new, never-seen documents. The downside of representing documents as separate graphs, 8Note that other GNNs used in inductive settings can be found (Hamilton, Ying, and Leskovec 2017; Veli\u02c7ckovi\u00b4c et al. 2017).",
      "however, is that we lose the ability to capture corpus-level dependencies. Also, our directed graphs capture word order- ing, which is ignored by (Yao, Mao, and Luo 2019). Finally, the approach of (Yao, Mao, and Luo 2019) requires com- puting the PMI for every word pair in the vocabulary, which may be prohibitive on datasets with very large vocabular- ies. On the other hand, the complexity of MPAD does not depend on vocabulary size. MPAD is also related to the Transformer\u2019s encoder stack (Vaswani et al. 2017). Speci\ufb01cally, the self-attention layer in each encoder updates the representation of each term based on the representations of all the other terms in the docu- ment, and can thus be thought of as a function performing the AGGREGATE and COMBINE steps. Stacking multiple encoders can also be thought of as performing multiple MP iterations. The main difference is that the Transformer\u2019s self- attention graph is complete, thus ignoring word order and proximity. Also, building that graph requires constructing an adjacency matrix that may become prohibitively large with long documents.",
      "Stacking multiple encoders can also be thought of as performing multiple MP iterations. The main difference is that the Transformer\u2019s self- attention graph is complete, thus ignoring word order and proximity. Also, building that graph requires constructing an adjacency matrix that may become prohibitively large with long documents. 7 Conclusion We proposed an application of the message passing frame- work to NLP, the Message Passing Attention network for Document understanding (MPAD). Experiments show that our architecture is competitive with the state-of-the-art. By processing weighted, directed word co-occurrence net- works, MPAD is sensitive to word order and word-word re- lationship strength. To capture the hierarchical structure of documents, we also proposed three hierarchical variants of MPAD, that bring improvements over the vanilla model. 8 Acknowledgments GN is supported by the project \u201cESIGMA\u201d (ANR-17-CE40- 0028). We thank the NVidia corporation for the donation of a GPU as part of their GPU grant program. References [Atwood and Towsley 2016] Atwood, J., and Towsley, D. 2016. Diffusion-Convolutional Neural Networks.",
      "We thank the NVidia corporation for the donation of a GPU as part of their GPU grant program. References [Atwood and Towsley 2016] Atwood, J., and Towsley, D. 2016. Diffusion-Convolutional Neural Networks. In Advances in Neural Information Processing Systems, 1993\u20132001. [Battaglia et al. 2016] Battaglia, P.; Pascanu, R.; Lai, M.; Rezende, D. J.; et al. 2016. Interaction Networks for Learning about Ob- jects, Relations and Physics. In Advances in Neural Information Processing Systems, 4502\u20134510. [Cheng, Dong, and Lapata 2016] Cheng, J.; Dong, L.; and Lapata, M. 2016. Long Short-Term Memory-Networks for Machine Read- ing. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 551\u2013561. [Cho et al.",
      "2016. Long Short-Term Memory-Networks for Machine Read- ing. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 551\u2013561. [Cho et al. 2014] Cho, K.; van Merrienboer, B.; Gulcehre, C.; Bah- danau, D.; Bougares, F.; Schwenk, H.; and Bengio, Y. 2014. Learn- ing Phrase Representations using RNN Encoder\u2013Decoder for Sta- tistical Machine Translation. In Proceedings of the 2014 Con- ference on Empirical Methods in Natural Language Processing, 1724\u20131734. [Chung et al. 2014] Chung, J.; Gulcehre, C.; Cho, K.; and Bengio, Y. 2014. Empirical Evaluation of Gated Recurrent Neural Net- works on Sequence Modeling. arXiv preprint arXiv:1412.3555.",
      "2014] Chung, J.; Gulcehre, C.; Cho, K.; and Bengio, Y. 2014. Empirical Evaluation of Gated Recurrent Neural Net- works on Sequence Modeling. arXiv preprint arXiv:1412.3555. [Defferrard, Bresson, and Vandergheynst 2016] Defferrard, M.; Bresson, X.; and Vandergheynst, P. 2016. Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering. In Advances in Neural Information Processing Systems, 3844\u20133852. [Duvenaud et al. 2015] Duvenaud, D. K.; Maclaurin, D.; Ipar- raguirre, J.; Bombarell, R.; Hirzel, T.; Aspuru-Guzik, A.; and Adams, R. P. 2015. Convolutional Networks on Graphs for Learn- ing Molecular Fingerprints. In Advances in Neural Information Processing Systems, 2224\u20132232. [Gilmer et al.",
      "2015. Convolutional Networks on Graphs for Learn- ing Molecular Fingerprints. In Advances in Neural Information Processing Systems, 2224\u20132232. [Gilmer et al. 2017] Gilmer, J.; Schoenholz, S. S.; Riley, P. F.; Vinyals, O.; and Dahl, G. E. 2017. Neural Message Passing for Quantum Chemistry. Proceedings of the 34th International Con- ference on Machine Learning 1263\u20131272. [Gori, Monfardini, and Scarselli 2005] Gori, M.; Monfardini, G.; and Scarselli, F. 2005. A New Model for Learning in Graph Do- mains. In Proceedings of the 2005 IEEE International Joint Con- ference on Neural Networks, volume 2, 729\u2013734. [Greene and Cunningham 2006] Greene, D., and Cunningham, P. 2006. Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering.",
      "[Greene and Cunningham 2006] Greene, D., and Cunningham, P. 2006. Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering. In Proceedings of the 23rd Inter- national Conference on Machine Learning, 377\u2013384. [Hamilton, Ying, and Leskovec 2017] Hamilton, W.; Ying, Z.; and Leskovec, J. 2017. Inductive Representation Learning on Large Graphs. In Advances in Neural Information Processing Systems, 1024\u20131034. [He et al. 2016] He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Residual Learning for Image Recognition. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 770\u2013778. [Henaff, Bruna, and LeCun 2015] Henaff, M.; Bruna, J.; and Le- Cun, Y. 2015. Deep Convolutional Networks on Graph-Structured Data. arXiv preprint arXiv:1506.05163.",
      "2015. Deep Convolutional Networks on Graph-Structured Data. arXiv preprint arXiv:1506.05163. [Irsoy and Cardie 2014] Irsoy, O., and Cardie, C. 2014. Deep Re- cursive Neural Networksfor Compositionality in Language. In Ad- vances in Neural Information Processing Systems, 2096\u20132104. [Iyyer et al. 2015] Iyyer, M.; Manjunatha, V.; Boyd-Graber, J.; and Daum\u00b4e III, H. 2015. Deep Unordered Composition Rivals Syn- tactic Methods for Text Classi\ufb01cation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, 1681\u20131691. [Kearnes et al. 2016] Kearnes, S.; McCloskey, K.; Berndl, M.; Pande, V.; and Riley, P. 2016. Molecular graph convolutions: moving beyond \ufb01ngerprints.",
      "[Kearnes et al. 2016] Kearnes, S.; McCloskey, K.; Berndl, M.; Pande, V.; and Riley, P. 2016. Molecular graph convolutions: moving beyond \ufb01ngerprints. Journal of Computer-Aided Molec- ular Design 30(8):595\u2013608. [Kim 2014] Kim, Y. 2014. Convolutional Neural Networks for Sen- tence Classi\ufb01cation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 1746\u20131751. [Kingma and Ba 2014] Kingma, D. P., and Ba, J. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. [Kipf and Welling 2016] Kipf, T. N., and Welling, M. 2016. Semi- supervised classi\ufb01cation with graph convolutional networks. arXiv preprint arXiv:1609.02907. [Kusner et al.",
      "2016. Semi- supervised classi\ufb01cation with graph convolutional networks. arXiv preprint arXiv:1609.02907. [Kusner et al. 2015] Kusner, M.; Sun, Y.; Kolkin, N.; and Wein- berger, K. 2015. From Word Embeddings to Document Distances. In Proceedings of the 32nd International Conference on Machine Learning, 957\u2013966. [Le and Mikolov 2014] Le, Q., and Mikolov, T. 2014. Distributed Representations of Sentences and Documents. In Proceedings of",
      "the 31st International Conference on Machine Learning, 1188\u2013 1196. [Li and Roth 2002] Li, X., and Roth, D. 2002. Learning Question Classi\ufb01ers. In Proceedings of the 19th International Conference on Computational Linguistics, 1\u20137. [Li et al. 2016] Li, Y.; Tarlow, D.; Brockschmidt, M.; and Zemel, R. 2016. Gated graph sequence neural networks. In Proceedings of the 4th International Conference on Learning Representations. [Li, Luong, and Jurafsky 2015] Li, J.; Luong, T.; and Jurafsky, D. 2015. A Hierarchical Neural Autoencoder for Paragraphs and Doc- uments. In Proceedings of the 53rd Annual Meeting of the Associ- ation for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, 1106\u20131115. [Lin et al. 2015] Lin, R.; Liu, S.; Yang, M.; Li, M.; Zhou, M.; and Li, S.",
      "[Lin et al. 2015] Lin, R.; Liu, S.; Yang, M.; Li, M.; Zhou, M.; and Li, S. 2015. Hierarchical Recurrent Neural Network for Document Modeling. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 899\u2013907. [Lin et al. 2017] Lin, Z.; Feng, M.; Santos, C. N. d.; Yu, M.; Xiang, B.; Zhou, B.; and Bengio, Y. 2017. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130. [Maas et al. 2011] Maas, A. L.; Daly, R. E.; Pham, P. T.; Huang, D.; Ng, A. Y.; and Potts, C. 2011. Learning Word Vectors for Sentiment Analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, 142\u2013150.",
      "2011. Learning Word Vectors for Sentiment Analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, 142\u2013150. [Mihalcea and Tarau 2004] Mihalcea, R., and Tarau, P. 2004. Tex- tRank: Bringing Order into Texts. In Proceedings of the 2004 Con- ference on Empirical Methods in Natural Language Processing, 404\u2013411. [Mikolov et al. 2013] Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013. Distributed Representations of Words and Phrases and their Compositionality. In Advances in Neural Information Processing Systems, 3111\u20133119. [Murphy, Weiss, and Jordan 1999] Murphy, K. P.; Weiss, Y.; and Jordan, M. I. 1999. Loopy Belief Propagation for Approximate Inference: An Empirical Study.",
      "[Murphy, Weiss, and Jordan 1999] Murphy, K. P.; Weiss, Y.; and Jordan, M. I. 1999. Loopy Belief Propagation for Approximate Inference: An Empirical Study. In Proceedings of the 15th Confer- ence on Uncertainty in Arti\ufb01cial Intelligence, 467\u2013475. [Niepert, Ahmed, and Kutzkov 2016] Niepert, M.; Ahmed, M.; and Kutzkov, K. 2016. Learning Convolutional Neural Networks for Graphs. In Proceedings of the 33rd International Conference on Machine Learning, 2014\u20132023. [Nikolentzos et al. 2017] Nikolentzos, G.; Meladianos, P.; Rousseau, F.; Stavrakas, Y.; and Vazirgiannis, M. 2017. Shortest- Path Graph Kernels for Document Similarity. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, 1890\u20131900. [Nikolentzos et al.",
      "2017. Shortest- Path Graph Kernels for Document Similarity. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, 1890\u20131900. [Nikolentzos et al. 2018] Nikolentzos, G.; Meladianos, P.; Tixier, A. J.-P.; Skianis, K.; and Vazirgiannis, M. 2018. Kernel Graph Convolutional Neural Networks. In Proceedings of the 27th Inter- national Conference on Arti\ufb01cial Neural Networks, 22\u201332. [Pang and Lee 2004] Pang, B., and Lee, L. 2004. A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts. In Proceedings of the 42nd Annual Meet- ing on Association for Computational Linguistics, 271\u2013278. [Pang and Lee 2005] Pang, B., and Lee, L. 2005. Seeing Stars: Ex- ploiting Class Relationships for Sentiment Categorization with Re- spect to Rating Scales.",
      "[Pang and Lee 2005] Pang, B., and Lee, L. 2005. Seeing Stars: Ex- ploiting Class Relationships for Sentiment Categorization with Re- spect to Rating Scales. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, 115\u2013124. [Peng et al. 2018] Peng, H.; Li, J.; He, Y.; Liu, Y.; Bao, M.; Wang, L.; Song, Y.; and Yang, Q. 2018. Large-Scale Hierarchical Text Classi\ufb01cation with Recursively Regularized Deep Graph-CNN. In Proceedings of the 2018 World Wide Web Conference, 1063\u20131072. [Raheja and Tetreault 2019] Raheja, V., and Tetreault, J. 2019. Dia- logue Act Classi\ufb01cation with Context-Aware Self-Attention. arXiv preprint arXiv:1904.02594.",
      "2019. Dia- logue Act Classi\ufb01cation with Context-Aware Self-Attention. arXiv preprint arXiv:1904.02594. [ \u02c7Reh\u02dau\u02c7rek and Sojka 2010] \u02c7Reh\u02dau\u02c7rek, R., and Sojka, P. 2010. Soft- ware Framework for Topic Modelling with Large Corpora. In Pro- ceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, 45\u201350. [Scarselli et al. 2008] Scarselli, F.; Gori, M.; Tsoi, A. C.; Hagen- buchner, M.; and Monfardini, G. 2008. The Graph Neural Network Model. IEEE Transactions on Neural Networks 20(1):61\u201380. [Shang et al. 2019] Shang, G.; Tixier, A. J.-P.; Vazirgiannis, M.; and Lorr\u00b4e, J.-P. 2019.",
      "[Shang et al. 2019] Shang, G.; Tixier, A. J.-P.; Vazirgiannis, M.; and Lorr\u00b4e, J.-P. 2019. Energy-based Self-attentive Learning of Ab- stractive Communities for Spoken Language Understanding. arXiv preprint arXiv:1904.09491. [Shen et al. 2018] Shen, T.; Zhou, T.; Long, G.; Jiang, J.; Pan, S.; and Zhang, C. 2018. DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding. In Proceedings of the 32nd AAAI Conference on Arti\ufb01cial Intelligence, 5446\u20135455. [Socher et al. 2013] Socher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning, C. D.; Ng, A.; and Potts, C. 2013. Recursive Deep Mod- els for Semantic Compositionality Over a Sentiment Treebank.",
      "2013] Socher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning, C. D.; Ng, A.; and Potts, C. 2013. Recursive Deep Mod- els for Semantic Compositionality Over a Sentiment Treebank. In Proceedings of the 2013 Conference on Empirical Methods in Nat- ural Language Processing, 1631\u20131642. [Srivastava et al. 2014] Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; and Salakhutdinov, R. 2014. Dropout: A Simple Way to Prevent Neural Networks from Over\ufb01tting. The Journal of Machine Learning Research 15(1):1929\u20131958. [Tai, Socher, and Manning 2015] Tai, K. S.; Socher, R.; and Man- ning, C. D. 2015. Improved Semantic Representations From Tree- Structured Long Short-Term Memory Networks.",
      "[Tai, Socher, and Manning 2015] Tai, K. S.; Socher, R.; and Man- ning, C. D. 2015. Improved Semantic Representations From Tree- Structured Long Short-Term Memory Networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, 1556\u20131566. [Tang, Qin, and Liu 2015] Tang, D.; Qin, B.; and Liu, T. 2015. Document Modeling with Gated Recurrent Neural Network for Sentiment Classi\ufb01cation. In Proceedings of the 2015 conference on Empirical Methods in Natural Language Processing, 1422\u20131432. [Tixier et al. 2019] Tixier, A. J.-P.; Nikolentzos, G.; Meladianos, P.; and Vazirgiannis, M. 2019. Graph Classi\ufb01cation with 2D Convo- lutional Neural Networks. In Proceedings of the 28th International Conference on Arti\ufb01cial Neural Networks, 578\u2013593.",
      "2019. Graph Classi\ufb01cation with 2D Convo- lutional Neural Networks. In Proceedings of the 28th International Conference on Arti\ufb01cial Neural Networks, 578\u2013593. [Vaswani et al. 2017] Vaswani, A.; Shazeer, N.; Parmar, N.; Uszko- reit, J.; Jones, L.; Gomez, A. N.; Kaiser, \u0141.; and Polosukhin, I. 2017. Attention Is All You Need. In Advances in Neural Informa- tion Processing Systems, 5998\u20136008. [Veli\u02c7ckovi\u00b4c et al. 2017] Veli\u02c7ckovi\u00b4c, P.; Cucurull, G.; Casanova, A.; Romero, A.; Lio, P.; and Bengio, Y. 2017. Graph attention net- works. arXiv preprint arXiv:1710.10903. [Weisfeiler and Lehman 1968] Weisfeiler, B., and Lehman, A. A. 1968.",
      "2017. Graph attention net- works. arXiv preprint arXiv:1710.10903. [Weisfeiler and Lehman 1968] Weisfeiler, B., and Lehman, A. A. 1968. A reduction of a graph to a canonical form and an algebra arising during this reduction. Nauchno-Technicheskaya Informat- sia 2(9):12\u201316. [Wiebe, Wilson, and Cardie 2005] Wiebe, J.; Wilson, T.; and Cardie, C. 2005. Annotating Expressions of Opinions and Emo- tions in Language. Language Resources and Evaluation 39(2- 3):165\u2013210. [Xu et al. 2018a] Xu, K.; Hu, W.; Leskovec, J.; and Jegelka, S. 2018a. How Powerful are Graph Neural Networks? arXiv preprint arXiv:1810.00826.",
      "[Xu et al. 2018b] Xu, K.; Li, C.; Tian, Y.; Sonobe, T.; Kawarabayashi, K.-i.; and Jegelka, S. 2018b. Representa- tion Learning on Graphs with Jumping Knowledge Networks. In Proceedings of the 35th International Conference on Machine Learning, 5449\u20135458. [Yang et al. 2016] Yang, Z.; Yang, D.; Dyer, C.; He, X.; Smola, A.; and Hovy, E. 2016. Hierarchical Attention Networks for Document Classi\ufb01cation. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguis- tics: Human Language Technologies, 1480\u20131489. [Yao, Mao, and Luo 2019] Yao, L.; Mao, C.; and Luo, Y. 2019. Graph Convolutional Networks for Text Classi\ufb01cation. In Pro- ceedings of the 33rd AAAI Conference on Arti\ufb01cial Intelligence, 7370\u20137377. [Zhou et al.",
      "2019. Graph Convolutional Networks for Text Classi\ufb01cation. In Pro- ceedings of the 33rd AAAI Conference on Arti\ufb01cial Intelligence, 7370\u20137377. [Zhou et al. 2015] Zhou, C.; Sun, C.; Liu, Z.; and Lau, F. 2015. A C-LSTM Neural Network for Text Classi\ufb01cation. arXiv preprint arXiv:1511.08630."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1908.06267.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":12165,
  "avg_doclen":166.6438356164,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1908.06267.pdf"
    }
  }
}