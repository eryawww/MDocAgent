{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "A Multi-Task Learning Framework for Extracting Drugs and Their Interactions from Drug Labels Tung Tran1, Ramakanth Kavuluru1,2, and Halil Kilicoglu3 1Department of Computer Science, University of Kentucky, Lexington, KY, USA 2Division of Biomedical Informatics, Department of Internal Medicine, University of Kentucky, Lexington, KY, USA 3Lister Hill National Center for Biomedical Communications, National Library of Medicine, Bethesda, MD, USA Abstract Preventable adverse drug reactions as a result of medical errors present a growing concern in modern medicine. As drug-drug interactions (DDIs) may cause adverse re- actions, being able to extracting DDIs from drug labels into machine-readable form is an important e\ufb00ort in e\ufb00ectively deploy- ing drug safety information. The DDI track of TAC 2018 introduces two large hand-annotated test sets for the task of extracting DDIs from structured product labels with linkage to standard terminolo- gies. Herein, we describe our approach to tackling tasks one and two of the DDI track, which corresponds to named entity recognition (NER) and sentence-level rela- tion extraction respectively.",
      "Herein, we describe our approach to tackling tasks one and two of the DDI track, which corresponds to named entity recognition (NER) and sentence-level rela- tion extraction respectively. Namely, our approach resembles a multi-task learning framework designed to jointly model var- ious sub-tasks including NER and inter- action type and outcome prediction. On NER, our system ranked second (among eight teams) at 33.00% and 38.25% F1 on Test Sets 1 and 2 respectively. On rela- tion extraction, our system ranked second (among four teams) at 21.59% and 23.55% on Test Sets 1 and 2 respectively. 1 Introduction Preventable adverse drug reactions (ADRs) in- troduce a growing concern in the modern health- care system as they represent a large fraction of hospital admissions and play a signi\ufb01cant role in increased health care costs [1].",
      "1 Introduction Preventable adverse drug reactions (ADRs) in- troduce a growing concern in the modern health- care system as they represent a large fraction of hospital admissions and play a signi\ufb01cant role in increased health care costs [1]. Based on a study examining hospital admission data, it is estimated that approximately three to four per- cent of hospital admissions are caused by adverse events [2]; moreover, it is estimated that between 53% and 58% of these events were due to med- ical errors [3] (and are therefore considered pre- ventable). Such preventable adverse events have been cited as the eighth leading cause of death in the U.S., with an estimated fatality rate of between 44,000 and 98,000 each year [4]. As drug-drug interactions (DDIs) may lead to pre- ventable ADRs, being able to extract DDIs from structured product labeling (SPL) documents for prescription drugs is an important e\ufb00ort toward e\ufb00ective dissemination of drug safety informa- tion.",
      "As drug-drug interactions (DDIs) may lead to pre- ventable ADRs, being able to extract DDIs from structured product labeling (SPL) documents for prescription drugs is an important e\ufb00ort toward e\ufb00ective dissemination of drug safety informa- tion. The Text Analysis Conference (TAC) is a series of workshops aimed at encouraging re- search in natural language processing (NLP) and related applications by providing large test col- lections along with a standard evaluation pro- cedure. The Drug-Drug Interaction Extraction from Drug Labels track of TAC 2018 [5], orga- nized by the U.S. Food and Drug Administration (FDA) and U.S. National Library of Medicine (NLM), is established with the goal of transform- ing the contents of SPLs into a machine-readable format with linkage to standard terminologies. We focus on the \ufb01rst two tasks of the DDI track involving named entity recognition (NER) and relation extraction (RE). Task 1 is focused on identifying mentions in the text correspond- ing to precipitants, interaction triggers, and in- teraction e\ufb00ects.",
      "We focus on the \ufb01rst two tasks of the DDI track involving named entity recognition (NER) and relation extraction (RE). Task 1 is focused on identifying mentions in the text correspond- ing to precipitants, interaction triggers, and in- teraction e\ufb00ects. Precipitants are de\ufb01ned as sub- stances, drugs, or a drug class involved in an interaction. Task 2 is focused on identifying sentence-level interactions; concretely, the goal is 1 arXiv:1905.07464v1  [cs.CL]  17 May 2019",
      "Labeled Drug Precipitant Trigger E\ufb00ect The use of Adenocard in patients receiving  digitalis may be rarely associated with  ventricular \ufb01brillation (see Warnings ). Pharmacodynamic Interaction Adenocard digitalis ventricular \ufb01brillation INTERACTION TYPE INTERACTION EFFECT MENTIONS INTERACTION Figure 1: An example illustrating the DDI task to identify the interacting precipitant, the type of the interaction, and outcome of the interac- tion. The interaction outcome depends on the interaction type as follows. Pharmacodynamic (PD) interactions are associated with a speci\ufb01ed e\ufb00ect corresponding to a span within the text that describes the outcome of the interaction. Naturally, it is possible for a precipitant to be involved in multiple PD interactions. Pharma- cokinetic (PK) interactions are associated with a label from a \ufb01xed vocabulary of National Can- cer Institute (NCI) Thesaurus codes indicating various levels of increase/decrease in functional measurements.",
      "Pharma- cokinetic (PK) interactions are associated with a label from a \ufb01xed vocabulary of National Can- cer Institute (NCI) Thesaurus codes indicating various levels of increase/decrease in functional measurements. For example, consider the sen- tence: \u201cThere is evidence that treatment with phenytoin leads to to decrease intestinal absorp- tion of furosemide, and consequently to lower peak serum furosemide concentrations.\u201d Here, phenytoin is involved in a PK interaction with the label drug, furosemide, and the type of PK interaction is indicated by the NCI Thesaurus code C54615 which describes a decrease in the maximum serum concentration (Cmax) of the la- bel drug. Lastly, unspeci\ufb01ed (UN) interactions are interactions with an outcome that is not ex- plicitly stated in the text and usually indicated through cautionary statements.",
      "Lastly, unspeci\ufb01ed (UN) interactions are interactions with an outcome that is not ex- plicitly stated in the text and usually indicated through cautionary statements. Figure 1 fea- tures a simple example of a PD interaction that is extracted from the drug label for Adenocard, where the precipitant is digitalis and the e\ufb00ect is \u201cventricular \ufb01brillation.\u201d 2 Materials and Methods Herein, we describe the training and testing data involved in this task and the metrics used for evaluation. In Section 2.3, we describe our mod- eling approach, our deep learning architecture, and our training procedure. 2.1 Datasets Each drug label is a collection of sections (e.g., DOSAGE & ADMINISTRATION, CON- TRAINDICATIONS, and WARNINGS) where each section contains one or more sentences. Each sentence is annotated with a list of zero or more mentions and interactions. The train- ing data released for this task contains 22 drug labels, referred to as Training-22, with gold stan- dard annotations.",
      "Each sentence is annotated with a list of zero or more mentions and interactions. The train- ing data released for this task contains 22 drug labels, referred to as Training-22, with gold stan- dard annotations. Two test sets of 57 and 66 drug labels, referred to as Test Set 1 and 2 respectively, with gold standard annotations are used to evaluate participating systems. As Training-22 is a relatively small dataset, we ad- ditionally utilize an external dataset with 180 an- notated drug labels dubbed NLM-180 [6] (more later). We provide summary statistics about these datasets in Table 1. Test Set 1 closely resembles Training-22 with respect to the sec- tions that are annotated. However, Test Set 1 is more sparse in the sense that there are more sen- tences per drug label (144 vs. 27), with a smaller proportion of those sentences having gold anno- tations (23% vs. 51%).",
      "However, Test Set 1 is more sparse in the sense that there are more sen- tences per drug label (144 vs. 27), with a smaller proportion of those sentences having gold anno- tations (23% vs. 51%). Test Set 2 is unique in that it contains annotations from only two sections, namely DRUG INTERACTIONS and CLINICAL PHARMACOLOGY, the latter of which is not represented in Training-22 (nor Test Set 1). Lastly, Training-22, Test Set 1, and Test Set 2 all vary with respect to the distribution of interaction types, with Training-22, Test Set 1, and Test Set 2 containing a higher proportion of PD, UN, and PK interactions respectively. 2.2 Evaluation Metrics We used the o\ufb03cial evaluation metrics for NER and relation extraction based on the standard precision, recall, and F1 micro-averaged over ex- actly matched entity/relation annotations. For either task, there are two matching criteria: pri- mary and relaxed. For entity recognition, relaxed matching considers only entity bounds while pri- mary matching considers entity bounds as well as the type of the entity.",
      "For either task, there are two matching criteria: pri- mary and relaxed. For entity recognition, relaxed matching considers only entity bounds while pri- mary matching considers entity bounds as well as the type of the entity. For relation extrac- tion, relaxed matching only considers precipitant drug (and their bounds) while primary match- 2",
      "NLM-180 * Training-22 Test Set 1 Test Set 2 Number of Drug Labels 180 22 57 66 Mean number of sentences per Drug Label 32 27 144 64 Mean number of words per sentence 23 24 22 23 Proportion of annotated sentences 27% 51% 23% 23% Mean number of mentions per annotated sentence 4.0 3.8 3.7 3.",
      "0 3.8 3.7 3.6 Proportion of mentions that are Precipitant 57% 53% 56% 55% Proportion of mentions that are Trigger 20% 28% 30% 33% Proportion of mentions that are Speci\ufb01cInteraction 23% 19% 14% 12% Proportion of interactions that are Pharmacodynamic 47% 49% 33% 28% Proportion of interactions that are Pharmacokinetic 25% 21% 28% 47% Proportion of interactions that are Unspeci\ufb01ed 28% 30% 39% 25% Table 1: Characteristics of datasets * Statistics for NLM-180 were computed on mapped examples (based on our own annotation mapping scheme) and not based on the original dataset. ing comprehensively considers precipitant drugs and, for each, the corresponding interaction type and interaction outcome. As relation extraction evaluation takes into account the bounds of con- stituent entity predictions, relation extraction performance is heavily reliant on entity recog- nition performance.",
      "ing comprehensively considers precipitant drugs and, for each, the corresponding interaction type and interaction outcome. As relation extraction evaluation takes into account the bounds of con- stituent entity predictions, relation extraction performance is heavily reliant on entity recog- nition performance. On the other hand, we note that while NER evaluation considers trig- ger mentions, triggers are ignored when evaluat- ing relation extraction performance. 2.3 Methodology We propose a multi-task learning framework for extracting drug-drug interactions from drug la- bels. The framework involves branching paths for each training objective (corresponding to sub-tasks) such that parameters of earlier layers (i.e., the context encoder) are shared. Modeling Approach. Since only drugs in- volved in an interaction (precipitants) are an- notated in the ground truth, we model the task of precipitant recognition and interaction type prediction jointly. We accomplish this by re- ducing the problem to a sequence tagging prob- lem via a novel NER tagging scheme. That is, for each precipitant drug, we additionally en- code the associated interaction type.",
      "We accomplish this by re- ducing the problem to a sequence tagging prob- lem via a novel NER tagging scheme. That is, for each precipitant drug, we additionally en- code the associated interaction type. Hence, there are \ufb01ve possible tags: T for trigger, E for e\ufb00ects, and D, K, and U for precipitants with pharmacodynamic, pharmacokinetic, and unspeci\ufb01ed interactions respectively. As a pre- processsing step, we identify the label drug in the sentence, if it is mentioned, and bind it to a generic entity token (e.g. \u201cLABELDRUG\u201d). We additionally account for label drug aliases, such as the generic version of a brand-name drug, and bind them to the same entity token. Ta- ble 2 shows how the tagging scheme is applied to the simple example in Figure 1. A drawback is that simplifying assumptions must be made that will hamper recall; e.g., we only consider non-overlapping mentions (more later).",
      "Ta- ble 2 shows how the tagging scheme is applied to the simple example in Figure 1. A drawback is that simplifying assumptions must be made that will hamper recall; e.g., we only consider non-overlapping mentions (more later). O O O O O O O B-D The use of LABELDRUG in patients receiving digitalis O O O B-T I-T B-E I-E O may be rarely associated with ventricular \ufb01brillation . Table 2: Example of the tagging scheme Once we have identi\ufb01ed the precipitant o\ufb00sets (as well as of triggers/e\ufb00ects) and the interac- tion type for each precipitant, we subsequently predict the outcome or consequence of the in- teraction (if any). To that end, we consider all entity spans annotated with K tags and assign them a label from a static vocabulary of 20 NCI concept codes corresponding to PK consequence (i.e., multiclass classi\ufb01cation) based on sentence- context. Likewise, we consider all entity spans annotated with D tags and link them to mention spans annotated with E tags; we accomplish this 3",
      "via binary classi\ufb01cation of all pairwise combina- tions. For entity spans annotated with U tags, no outcome prediction is made. Neural Network Architecture. Our pro- posed deep neural network is illustrated in Fig- ure 2. We utilize Bi-directional Long Short- Term Memory networks (Bi-LSTMs) and con- volutional neural networks (CNNs) designed for natural language processing as building blocks for our architecture [7, 8]. Entity recognition and outcome prediction share common parame- ters via a Bi-LSTM context encoder that com- poses a context representation at each timestep based on input words mapped to dense embed- dings and character-CNN composed representa- tions. We use the same character-CNN represen- tation as described in a prior work [9]; however, in this work, we omit the character type embed- ding. A Bi-LSTM component is used to annotate IOB tags for joint entity recognition and interac- tion type prediction (or, NER prediction) while a CNN with two separate dense output layers (one for PK and one for PD interactions) is used for outcome prediction.",
      "A Bi-LSTM component is used to annotate IOB tags for joint entity recognition and interac- tion type prediction (or, NER prediction) while a CNN with two separate dense output layers (one for PK and one for PD interactions) is used for outcome prediction. We consider NER predic- tion to be the main objective with outcome pre- diction playing a secondary role. When predict- ing outcome, the contextual input is arranged such that candidate entity (and e\ufb00ect) mentions are bound to generic tokens; the resulting rep- resentation is referred to as \u201centity-bound word embeddings\u201d in Figure 2. Notation. We denote BiLSTM(\u00b7) : Rn\u00d7din 7\u2192 Rn\u00d7dout as an abstract function, representing a standard bi-directional recurrent neural net- work with LSTM units, where n is the number of input vector representations (e.g., word em- beddings) in the sequence and din and dout are the dimensionality of the input and output rep- resentations respectively.",
      "We similarity denote CNN[h1,...,hk](\u00b7) : Rn\u00d7din 7\u2192Rdout to represent a standard CNN that maps an n \u00d7 din matrix to a vector representation of length dout, where [h1, . . . , hk] is a list of window (or kernel) sizes that are used in the convolution. Context Encoder. Let the input be a sen- tence of length n represented as a matrix S \u2208 Rn\u00d7d, where each row corresponds to a word embedding of length d. Moreover, let W i \u2208 Rm\u00d7dchar represent the word at position i of the sentence such that each of the m rows corre- spond to a character embedding of length dchar. The purpose of the context encoder is to encode each word of the input with surrounding linguis- tic features and long-distance dependency infor- mation. To that end, we employ the use of a Bi-LSTM network to encode S as a context ma- trix C \u2208Rn\u00d7dcontext where dcontext is a hyper- parameter of the network.",
      "To that end, we employ the use of a Bi-LSTM network to encode S as a context ma- trix C \u2208Rn\u00d7dcontext where dcontext is a hyper- parameter of the network. Concretely, C = BiLSTM \uf8eb \uf8ec \uf8ed S1 \u2225CNN[3](W 1) ... Sn \u2225CNN[3](W n) \uf8f6 \uf8f7 \uf8f8 (1) where Si denotes the ith row of S and \u2225is the vec- tor concatenation operator. Essentially, for each word, we compose character representations us- ing a CNN with a window size of three and con- catenate them to pre-trained word embeddings; we stack the concatenated vectors as rows of a new matrix that is ultimately fed as input to the Bi-LSTM context encoder. The ith row of C, denoted as Ci, represents the entire context centered at the ith word. As an implementation detail, we chose n and m to be the maximum sentence and word length (according to the train- ing data) respectively and pad shorter examples with zero vectors. NER Objective.",
      "The ith row of C, denoted as Ci, represents the entire context centered at the ith word. As an implementation detail, we chose n and m to be the maximum sentence and word length (according to the train- ing data) respectively and pad shorter examples with zero vectors. NER Objective. The network for the NER objective manifests as a stacked Bi-LSTM ar- chitecture when we consider both the context encoder and the entity recognition component. Borrowing from residual networks [10], we re- inforce the input by concatenating word embed- dings to the intermediate context vectors be- fore feeding it to the second Bi-LSTM layer. Concretely, the \ufb01nal entity recognition matrix R \u2208Rn\u00d7dner is composed such that R = BiLSTM \uf8eb \uf8ec \uf8ed C1 \u2225S1 ... Cn \u2225Sn \uf8f6 \uf8f7 \uf8f8. (2) 4",
      "BI-LSTM LAYER BI-LSTM LAYER CONTEXT ENCODER CHARACTER-BASED  WORD EMBEDDINGS WORD EMBEDDINGS COMPONENT FOR  ENTITY RECOGNITION CNN LAYER CNN LAYER COMPONENT FOR  OUTCOME PREDICTION CONTEXT EMBEDDINGS ENTITY-BOUND  WORD EMBEDDINGS SENTENCE INPUT ENTITY SPAN  AND  INTERACTION  TYPE  (IOB ANNOTATOR) PHARMACOKINETIC  OUTCOME  (MULTICLASS CLASSIFIER) PHARMACODYNAMIC  OUTCOME  (BINARY CLASSIFIER) FINAL WORD-LEVEL REPRESENTATION FINAL OUTCOME REPRESENTATION CONTEXT  EMBEDDINGS Figure 2: The multi-task neural network for DDI extraction The output at each position i = 1, . . . , n is qi = W nerRi + bner where Ri is the ith row of R and W ner \u2208R\u2113\u00d7dner and bner \u2208R\u2113are network parameters such that \u2113= 11 denotes the number of possible IOB tags such as O, B-K, I-K and so on.",
      "In order to obtain a categorical distribution, we apply the SoftMax function to qi such that pi = SoftMax(qi) where pi is the vector of probability estimates serving as a categorical distribution over \u2113tags for the word at position i. We optimize by com- puting the standard categorical cross-entropy loss for each of the n individual tag predictions. The \ufb01nal loss to be optimized is the mean over all n individually-computed losses. A stacked Bi-LSTM architecture improves over a single Bi-LSTM architecture given its ca- pacity to learn deep contextualized embeddings. While we showed that the stacked approach is better for this particular task in Section 3.1, it is not necessarily the case that a stacked approach is better in general. We o\ufb00er an alternative ex- planation and motivation for using a stacked ar- chitecture for this particular problem based on our initial intuition as follows. First, we note that a standalone Bi-LSTM is not able to han- dle the inference aspect of NER, which entails learning IOB constraints.",
      "First, we note that a standalone Bi-LSTM is not able to han- dle the inference aspect of NER, which entails learning IOB constraints. As an example, in the IOB encoding scheme, it is not possible for a I- D tag to immediately follow a B-E tag; in this way, the prediction of a tag is directly depen- dent on the prediction of neighboring tags. This inference aspect is typically handled by a linear- chain CRF. We believe that a stacked Bi-LSTM at least partially handles this aspect in the sense that the \ufb01rst Bi-LSTM (the context encoder) is given the opportunity to form independent pre- liminary decisions while the second Bi-LSTM is tasked with to making \ufb01nal decisions (based on preliminary ones) that are more globally consis- tent with respect to IOB constraints. Outcome Objective. To predict outcome, we construct a secondary branch in the network path that involves convolving over the word and 5",
      "context embeddings made available in earlier lay- ers. We \ufb01rst de\ufb01ne a relation representation v \u2208Rdrel that is produced by convolving with window sizes 3, 4, and 5 over the context vec- tors concatenated to entity-bound1 versions of the original input; concretely, v = CNN[3,4,5] \uf8eb \uf8ec \uf8ed C1 \u2225S \u2032 1 ... Cn \u2225S \u2032 n \uf8f6 \uf8f7 \uf8f8. where S \u2032 is the entity-bound version of S. Based on this outcome representation, we compose two separate softmax outputs: one for PK interac- tions and one for PD interactions. Concretely, the output layers are pPK = SoftMax(W PKv + bPK) and pPD = SoftMax(W PDv + bPD) where pPK \u2208R\u2113PK and pPD \u2208R\u2113PD are probabil- ity estimates serving as a categorical distribution over the outcome label space for PD and PK re- spectively and W PD, W PK, bPK, and bPD are parameters of the network.",
      "For PK, \u2113PK = 20 given there are 20 possible NCI Thesaurus codes corresponding to PK outcomes. For PD, \u2113PD = 2 as it is a binary classi\ufb01cation problem to assess whether the precipitant and e\ufb00ect pair encoded by S \u2032 are linked. We optimize using the standard categorical cross-entropy loss on both objectives. Training Data. In NLM-180, there is no dis- tinction between triggers and e\ufb00ects; moreover, PK e\ufb00ects are limited to coarse-grained (binary) labels corresponding to increase or decrease in function measurements. Hence, a direct map- ping from NLM-180 to Training-22 is impossi- ble. As a compromise, NLM-180 \u201ctriggers\u201d were mapped to Training-22 triggers in the case of un- speci\ufb01ed and PK interactions.",
      "Hence, a direct map- ping from NLM-180 to Training-22 is impossi- ble. As a compromise, NLM-180 \u201ctriggers\u201d were mapped to Training-22 triggers in the case of un- speci\ufb01ed and PK interactions. For PD interac- tions, we instead mapped NLM-180 \u201ctriggers\u201d to 1We refer to the process of generating examples for relation classi\ufb01cation, wherein mentions of candidate en- tities in the context are replaced with generic tokens that are also learned during back-propagation, as \u201centity- binding.\u201d Training-22 e\ufb00ects, which we believe to be appro- priate based on our manual analysis of the data. Since we do not have both trigger and e\ufb00ect for every PD interaction, we opted to ignore trigger mentions altogether in the case of PD interac- tions to avoid introducing mixed signals. While trigger recognition has no bearing on relation ex- traction performance, this policy has the e\ufb00ect of reducing the recall upperbound on NER by about 25% (more later on upperbound).",
      "While trigger recognition has no bearing on relation ex- traction performance, this policy has the e\ufb00ect of reducing the recall upperbound on NER by about 25% (more later on upperbound). To over- come the lack of \ufb01ne-grained annotations for PK outcome in NLM-180, we deploy the well-known bootstrapping approach [11] to incrementally an- notate NLM-180 PK outcomes using Training-22 annotations as seed examples. To mitigate the problem of semantic drift, in each bootstrap cy- cle, we re-annotated by hand predictions that were not consistent with the original NLM-180 coarse annotations (i.e., active learning [12]). Training Procedure. We train the three ob- jective losses (NER, PK outcome, and PD out- come) in an interleaved fashion at the mini- batch [13] level. We use word embeddings of size 200 pre-trained on the PubMed corpus [14] as input to the network; these are further modi\ufb01ed during back-propagation.",
      "We use word embeddings of size 200 pre-trained on the PubMed corpus [14] as input to the network; these are further modi\ufb01ed during back-propagation. For the character-level CNN, we set the character embedding size to 24 with 50 \ufb01lters over a window size of 3; the \ufb01- nal character-CNN composition is therefore of length 50. For each Bi-LSTM, the hidden size is set to 100 such that context vectors are 200 in length. For outcome prediction, we used win- dow sizes of 3, 4, and 5 with 50 \ufb01lters per window size; the \ufb01nal vector representation for outcome prediction is therefore 150 in length. A held-out development set of 4 drug labels is used for tuning and validation. The models are trained for 30 epochs with check-pointing; only the check-point with the best performance on the development set is kept for testing.",
      "A held-out development set of 4 drug labels is used for tuning and validation. The models are trained for 30 epochs with check-pointing; only the check-point with the best performance on the development set is kept for testing. We dynamically set the mini-batch size Nb as a func- tion of the number of examples N such that the number of training iterations is roughly 300 per epoch (and also constant regardless of training data size); concretely, Nb = \u230aN/300\u230b+ 1. As a form of regularization, we apply dropout [15] 6",
      "at a rate of 50% on the hidden representations immediately after a Bi-LSTM or CNN compo- sition. The outcome objectives are trained such that the gradients of the context encoder weights are downscaled by an order of magnitude (i.e., one tenth) to encourage learning at the later lay- ers. When learning on the NER objective \u2013 the main branch of the network \u2013 the gradients are not downscaled in the same manner. Moreover, when training on the NER objective, we up- weight the loss penalty on \u201crelation\u201d tags (non-O tags) by a factor of 10, which forces the model to prioritize di\ufb00erentiation between di\ufb00erent types of interactions over span segmentation. We ad- ditionally upweight the loss penalty by a factor of 3 on Training-22 examples compared to NLM- 180 examples. We optimize using the Adam [16] optimization method. These hyper-parameters were tuned during initial experiments. 3 Results and Discussion In this section, we present and discuss the results of our cross-validation experiments.",
      "We optimize using the Adam [16] optimization method. These hyper-parameters were tuned during initial experiments. 3 Results and Discussion In this section, we present and discuss the results of our cross-validation experiments. We then de- scribe the \u201cruns\u201d that were submitted as chal- lenge entries and present our o\ufb03cial challenge results. We discuss these results in Section 3.3. 3.1 Validation Results We present the results of our initial experiments in Table 3. Evaluations were produced as as re- sult of 11-fold cross-validation over Training-22 with two drug labels per fold. Instead of macro- averaging over folds, and thereby weighting each fold equally, we evaluate on the union of all 11 test-fold predictions. The upperbound in Table 3 is produced by reducing Training-22 (with gold labels) to our sequence-tagging format and then reverting it back to the original o\ufb03cial XML format. Low- ered recall is mostly due to simplifying assump- tions; e.g., we only consider non-overlapping mentions.",
      "Low- ered recall is mostly due to simplifying assump- tions; e.g., we only consider non-overlapping mentions. For coordinated disjoint cases such as \u201cX and Y inducers\u201d, we only considered \u201cY inducers\u201d in our simplifying assumption. Im- perfect precision is due to discrepancies between the tokenization scheme used by our method and that used to produce gold annotations; this leads to the occasional mismatch in entity o\ufb00sets dur- ing evaluation. Using a stacked Bi-LSTM trained on the orig- inal 22 training examples (Table 3; row 1) as our baseline, we make the following observa- tions. Incorporating NLM-180 resulted in a sig- ni\ufb01cant boost of more than 20 F1-points in re- lation extraction performance and more than 10 F1-points in NER performance (Table 3; row 2), despite the lowered upperbound on NER recall as mentioned in Section 2.3. Adding character- CNN based word representations improved per- formance marginally, more so for NER than re- lation extraction (Table 3; row 3).",
      "Adding character- CNN based word representations improved per- formance marginally, more so for NER than re- lation extraction (Table 3; row 3). We also im- plemented several tweaks to the pre-processing and post-processing aspects of the model based on preliminary error analysis including (1) using drug class mentions (e.g., \u201cdiuretics\u201d) as prox- ies if the drug label is not mentioned directly; (2) removing modi\ufb01ers such as moderate, strong, and potent so that output conforms to o\ufb03cial annotation guidelines; and (3) purging predicted mentions with only stopwords or generic terms such as \u201cdrugs\u201d or \u201cagents.\u201d These tweaks im- proved performance by more than two F1-points across both metrics (Table 3; row 4). Stacked architecture. Based on early exper- iments with simpler models tuned on relaxed matching (not shown in Table 3 and not directly comparable to results displayed in Table 3), we found that a stacked Bi-LSTM architecture im- proves over a single Bi-LSTM by approximately four F1-points on relation extraction (55.59% vs.",
      "51.55% F1 tuned on the relaxed matching crite- ria). We moreover found that omitting word em- beddings as input at the second Bi-LSTM results in worse performance at 52.91% F1. Temporal Convolution Networks. We also experimented with using Temporal Convolution Networks (TCNs) [17] as a \u201cdrop-in\u201d replace- ment for Bi-LSTMs. Our attempts involved re- placing only the second Bi-LSTM with a TCN (Table 3; row 4) as well as replacing both Bi- LSTMs with TCNs (Table 3; row 5). The re- 7",
      "Entity (Primary) Relation (Primary) Model / Data P (%) R (%) F (%) P (%) R (%) F (%) Stacked Bi-LSTM / Training-22 37.22 40.74 38.90 18.76 23.50 20.86 Stacked Bi-LSTM / Training-22 + NLM-180 49.45 49.79 49.62 42.54 43.56 43.05 Char-CNN + Stacked Bi-LSTM / Training-22 + NLM-180 51.63 50.97 51.30 43.09 44.31 43.69 Char-CNN + Stacked Bi-LSTM + Tweaks / Training-22 + NLM-180 53.72 53.76 53.74 46.35 45.66 46.00 Char-CNN + Bi-LSTM + TCN / Training-22 + NLM-180 48.82 50.72 49.75 39.68 41.17 39.68 Char-CNN + Stacked TCN / Training-22 + NLM-180 41.46 48.44 44.68 32.",
      "82 50.72 49.75 39.68 41.17 39.68 Char-CNN + Stacked TCN / Training-22 + NLM-180 41.46 48.44 44.68 32.15 36.68 34.27 Upperbound due to simplifying assumptions 99.21 74.56 85.14 97.49 81.44 88.74 Table 3: Preliminary results based on 11-fold cross validation over Training-22 with two held-out drug labels per fold. When NLM-180 is incorporated, the training data used for each fold consists of 20 non-held out drug labels from Training-22 and all 180 drug labels from NLM-180. sults of these early experiments were not promis- ing and further \ufb01ne-tuning may be necessary for better performance. 3.2 O\ufb03cial Test Results Our \ufb01nal system submission is based on a stacked Bi-LSTM network with character-CNNs trained on both Training-22 and NLM-180 (correspond- ing to row 4 of Table 3).",
      "3.2 O\ufb03cial Test Results Our \ufb01nal system submission is based on a stacked Bi-LSTM network with character-CNNs trained on both Training-22 and NLM-180 (correspond- ing to row 4 of Table 3). We submitted the fol- lowing three runs based on this architecture: 1. A single model. 2. An ensemble over ten models each trained with randomly initialized weights and a ran- dom development split. Intuitively, models collectively \u201cvote\u201d on predicted annotations that are kept and annotations that are dis- carded. A unique annotation (entity or re- lation) has one vote for each time it ap- pears in one of the ten model prediction sets. In terms of implementation, unique annotations are incrementally added (to the \ufb01nal prediction set) in order of descend- ing vote count; subsequent annotations that con\ufb02ict (i.e., overlap based on character o\ufb00sets) with existing annotations are dis- carded. Hence, we loosely refer to this ap- proach as \u201cvoting-based\u201d ensembling. 3.",
      "Hence, we loosely refer to this ap- proach as \u201cvoting-based\u201d ensembling. 3. A single model with pre/post-processing rules to handle modi\ufb01er coordinations; for example, \u201cX and Y inducers\u201d would be cor- rectly identi\ufb01ed as two distinct entities cor- responding to \u201cX inducers\u201d and \u201cY induc- ers.\u201d Here, we essentially encoded \u201cX and Y inducers\u201d as a single entity when training the NER objective; during test time, we use simple rules based on pattern matching to split the joint \u201centity\u201d into its constituents. Eight teams participated in task 1 while four teams participated in task 2. We record the rel- ative performance of our system (among others in the top 5) on the two o\ufb03cial test sets in Ta- ble 4. For each team, we only display the perfor- mance of the best run for a particular test set. Methods are grouped by the data used for train- ing and ranked in ascending order of primary relation extraction performance followed by en- tity recognition performance.",
      "For each team, we only display the perfor- mance of the best run for a particular test set. Methods are grouped by the data used for train- ing and ranked in ascending order of primary relation extraction performance followed by en- tity recognition performance. We also included a single model trained solely on Training-22, that was not submitted, for comparison. Our voting- based ensemble performed best among the three systems submitted by our team on both NER and relation extraction. In the o\ufb03cial challenge, this model placed second overall on both NER and relation extraction. Tang et al. [21] boasts the top performing sys- tem on both tasks. In addition to Training-22 and NLM-180, the team trained and validated their models on a set of 1148 sentences sampled from DailyMed labels that were manually anno- tated according to o\ufb03cial annotation guidelines. Hence, strictly speaking, their method is not di- rectly comparable to ours given the signi\ufb01cant di\ufb00erence in available training data. 8",
      "Entity (Primary) Relation (Relaxed) Relation (Primary) Training Data Method P (%) R (%) F (%) P (%) R (%) F (%) P (%) R (%) F (%) Training-22 Ours (Single model)1 21.74 33.84 26.47 32.95 34.57 33.74 15.49 15.42 15.45 Training-22 + NLM-180 Zhang and Kordjamshidi [18] 17.00 15.86 16.41 - - - - - - Akhtyamova and Cardi\ufb00[19] 37.96 20.39 26.53 - - - - - - Ours (Modi\ufb01er Coordination) 28.63 27.48 28.04 38.97 30.62 34.30 21.94 16.57 18.88 Ours (Single model) 29.64 31.58 30.58 38.16 33.80 35.85 21.28 18.09 19.55 Dandala et al.",
      "[20] 41.94 23.19 29.87 46.60 29.78 36.34 25.24 16.10 19.66 Ours (Ensemble) 29.50 37.45 33.00 40.55 38.36 39.43 22.08 21.13 21.59 Training-22 + NLM-180 + HS2 Tang et al.",
      "[21] 55.23 38.32 45.25 71.70 45.46 55.64 54.43 32.76 40.90 (a) System performance on O\ufb03cial Test Set 1 Entity (Primary) Relation (Relaxed) Relation (Primary) Training Data Method P (%) R (%) F (%) P (%) R (%) F (%) P (%) R (%) F (%) Training-22 Ours (Single model)1 27.77 33.31 30.29 38.38 40.85 39.58 16.17 15.39 15.77 Training-22 + NLM-180 Zhang and Kordjamshidi [18] 17.13 21.89 19.22 - - - - - - Akhtyamova and Cardi\ufb00[19] 37.76 24.07 29.40 - - - - - - Ours (Modi\ufb01er Coordination) 34.92 30.33 32.46 46.72 36.13 40.75 21.39 15.67 18.09 Dandala et al.",
      "[20] 44.61 29.31 35.38 50.07 36.86 42.46 22.99 16.83 19.43 Ours (Single model) 35.29 33.47 34.36 44.93 37.64 40.96 22.51 17.71 19.82 Ours (Ensemble) 36.68 40.02 38.28 49.51 44.27 46.74 22.53 21.13 23.55 Training-22 + NLM-180 + HS2 Tang et al. [21] 51.23 42.39 46.39 66.99 49.58 56.98 48.92 34.49 40.46 (b) System performance on O\ufb03cial Test Set 2 Table 4: Comparison of our method with that of other teams in the top 5. Only the best perform- ing method of each team is shown; methods are grouped by available training data and ranked in ascending order by relation extraction (primary) performance followed by entity recognition performance.",
      "Only the best perform- ing method of each team is shown; methods are grouped by available training data and ranked in ascending order by relation extraction (primary) performance followed by entity recognition performance. 1This model was not submitted and is shown for reference only 2HS refers to a private dataset of 1148 sentences manually-annotated by Tang et al. [21] according to o\ufb03cial guidelines 3.3 Discussion While precision was similar between the three systems (with exceptions), we observed that our ensemble-based system bene\ufb01ted mostly from improved recall. This aligns with our initial ex- pectation (based on prior experience with deep learning models) that an ensemble-based ap- proach would improve stability and accuracy with deep neural models. Although including NLM-180 as training data resulted in signi\ufb01cant performance gains during 11-fold cross valida- tion, we \ufb01nd that the same improvements were not as dramatic on either test sets despite the 800% gain in training data. As such, we o\ufb00er the following analysis.",
      "As such, we o\ufb00er the following analysis. First, we suspect that there may be a semantic or annotation drift be- tween these datasets as annotation guidelines evolve over time and as annotators become more experienced. To our knowledge, the datasets were annotated in the following order: NLM-180, Training-22, and \ufb01nally Test Sets 1 and 2; more- over, Test Sets 1 and 2 were annotated by sep- arate groups of annotators. Second, having few but higher quality examples may be more ad- vantageous than having many but lower quality examples, at least for this particular task where evaluation is based on matching exact character o\ufb00sets. Finally, we note that the top perform- ing system exhibits superior performance on Test Set 1 compared to Test Set 2; interestingly, we observe an inverse of the scenario in our own sys- tem. This may be an indicator that our system struggles with data that is more \u201csparse\u201d (as pre- viously de\ufb01ned in Section 2.1). 9",
      "4 Conclusion We presented a method for jointly extracting precipitants and their interaction types as part of a multi-task framework that additionally de- tects interaction outcome. Among three \u201cruns\u201d, a ten model voting-ensemble was our best per- former. In future e\ufb00orts, we will experiment with Graph Convolution Networks [22] over depen- dency trees as a \u201cdrop-in\u201d replace for Bi-LSTMs to assess its suitability for this task. Acknowledgements This research was conducted during TT\u2019s par- ticipation in the Lister Hill National Center for Biomedical Communications (LHNCBC) Re- search Program in Medical Informatics for Grad- uate students at the U.S. National Library of Medicine, National Institutes of Health. HK is supported by the intramural research program at the U.S. National Library of Medicine, National Institutes of Health. RK and TT are also sup- ported by the U.S. National Library of Medicine through grant R21LM012274. References [1] Patrick J McDonnell and Michael R Ja- cobs. Hospital admissions resulting from preventable adverse drug reactions.",
      "RK and TT are also sup- ported by the U.S. National Library of Medicine through grant R21LM012274. References [1] Patrick J McDonnell and Michael R Ja- cobs. Hospital admissions resulting from preventable adverse drug reactions. An- nals of Pharmacotherapy, 36(9):1331\u20131336, 2002. [2] Troyen A Brennan, Lucian L Leape, Nan M Laird, Liesi Hebert, A Russell Localio, Ann G Lawthers, Joseph P Newhouse, Paul C Weiler, and Howard H Hiatt. In- cidence of adverse events and negligence in hospitalized patients: results of the harvard medical practice study i. New England jour- nal of medicine, 324(6):370\u2013376, 1991. [3] Eric J Thomas, David M Studdert, Joseph P Newhouse, Brett IW Zbar, K Mason Howard, Elliott J Williams, and Troyen A Brennan. Costs of medical injuries in utah and colorado. Inquiry, pages 255\u2013 264, 1999.",
      "Costs of medical injuries in utah and colorado. Inquiry, pages 255\u2013 264, 1999. [4] Molla S Donaldson, Janet M Corrigan, Linda T Kohn, et al. To err is human: building a safer health system, volume 6. National Academies Press, 2000. [5] U.S. National Institute of Standards and Technology. Text Analysis Conference (TAC) 2018. https://tac.nist.gov/ 2018/index.html. [6] National Library of Medicine. NLM-DDI CD corpus: DailyMed cardiovascular prod- uct labels annotated with drug-drug in- teractions. https://lhce-brat.nlm.nih. gov/NLMDDICorpus.htm. [7] Yoon Kim. Convolutional neural networks for sentence classi\ufb01cation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746\u20131751, Doha, Qatar, October 2014. Association for Computational Lin- guistics.",
      "Convolutional neural networks for sentence classi\ufb01cation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746\u20131751, Doha, Qatar, October 2014. Association for Computational Lin- guistics. [8] Tung Tran and Ramakanth Kavuluru. Pre- dicting mental conditions based on \u201chistory of present illness\u201d in psychiatric notes with deep neural networks. Journal of Biomedi- cal Informatics, pages S138\u2013S148, 2017. [9] Tung Tran and Ramakanth Kavuluru. An end-to-end deep learning architecture for extracting protein-protein interactions af- fected by genetic mutations. Journal of Bio- logical Databases and Curation (Database), 2018:1\u201313, 2018. [10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
      "[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016. [11] Rosie Jones, Andrew McCallum, Kamal Nigam, and Ellen Rilo\ufb00. Bootstrapping for text learning tasks. In IJCAI-99 Workshop on Text Mining: Foundations, Techniques and Applications, volume 1, 1999. 10",
      "[12] Burr Settles. Active learning. Synthesis Lec- tures on Arti\ufb01cial Intelligence and Machine Learning, 6(1):1\u2013114, 2012. [13] Quoc V Le, Jiquan Ngiam, Adam Coates, Abhik Lahiri, Bobby Prochnow, and An- drew Y Ng. On optimization methods for deep learning. In Proceedings of the 28th In- ternational Conference on Machine Learn- ing, pages 265\u2013272. Omnipress, 2011. [14] Sampo Pyysalo, Filip Ginter, Hans Moen, Tapio Salakoski, and Sophia Ananiadou. Distributional semantics resources for biomedical text processing. In Proceed- ings of 5th International Symposium on Languages in Biology and Medicine, pages 39\u201344, 2013. [15] Nitish Srivastava, Geo\ufb00rey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from over\ufb01tting.",
      "[15] Nitish Srivastava, Geo\ufb00rey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from over\ufb01tting. The Journal of Machine Learning Research, 15(1):1929\u20131958, 2014. [16] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the 3rd International Con- ference for Learning Representations (ICLR 2015), 2015. [17] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent net- works for sequence modeling. arXiv preprint arXiv:1803.01271, 2018. [18] Yue Zhang and Parisa Kordjamshidi. PE- TU participation at TAC 2018 drug-drug interaction extraction from drug labels. In Proceedings of the 2018 Text Analysis Con- ference (TAC 2018), 2018.",
      "[18] Yue Zhang and Parisa Kordjamshidi. PE- TU participation at TAC 2018 drug-drug interaction extraction from drug labels. In Proceedings of the 2018 Text Analysis Con- ference (TAC 2018), 2018. [19] Liliya Akhtyamova and John Cardi\ufb00. Extracting drug-drug interactions with character-level and dependency-based em- beddings. In Proceedings of the 2018 Text Analysis Conference (TAC 2018), 2018. [20] Bharath Dandala, Diwakar Mahajan, and Ananya Poddar. IBM Research system at TAC 2018: Deep learning architectures for drug-drug interaction extraction from struc- tured product labels. In Proceedings of the 2018 Text Analysis Conference (TAC 2018), 2018. [21] Siliang Tang, Qi Zhang, Tianpeng Zheng, Mengdi Zhou, Zhan Chen, Lixing Shen, Xi- ang Ren, Yueting Zhuang, Shiliang Pu, and Fei Wu Wu. Two step joint model for drug drug interaction extraction.",
      "[21] Siliang Tang, Qi Zhang, Tianpeng Zheng, Mengdi Zhou, Zhan Chen, Lixing Shen, Xi- ang Ren, Yueting Zhuang, Shiliang Pu, and Fei Wu Wu. Two step joint model for drug drug interaction extraction. In Proceedings of the 2018 Text Analysis Conference (TAC 2018), 2018. [22] Yuhao Zhang, Peng Qi, and Christopher D Manning. Graph convolution over pruned dependency trees improves relation extrac- tion. In Proceedings of 2018 Conference on Empirical Methods in Natural Language Processing, 2018. 11"
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1905.07464.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":9419,
  "avg_doclen":171.2545454545,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1905.07464.pdf"
    }
  }
}