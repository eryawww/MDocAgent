{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Tosin P. Adewumi*, Foteini Liwicki, and Marcus Liwicki Word2Vec: Optimal hyper-parameters and their impact on NLP downstream tasks Abstract: Word2Vec is a prominent model for natural lan- guage processing (NLP) tasks. Similar inspiration is found in distributed embeddings for new state-of-the-art (SotA) deep neural networks. However, wrong combination of hyper-parameters can produce poor quality vectors. The objective of this work is to empirically show optimal com- bination of hyper-parameters exists and evaluate various combinations. We compare them with the released, pre- trained original word2vec model. Both intrinsic and ex- trinsic (downstream) evaluations, including named entity recognition (NER) and sentiment analysis (SA) were car- ried out. The downstream tasks reveal that the best model is usually task-specific, high analogy scores don\u2019t necessar- ily correlate positively with F1 scores and the same applies to the focus on data alone. Increasing vector dimension size after a point leads to poor quality or performance.",
            "The downstream tasks reveal that the best model is usually task-specific, high analogy scores don\u2019t necessar- ily correlate positively with F1 scores and the same applies to the focus on data alone. Increasing vector dimension size after a point leads to poor quality or performance. If ethical considerations to save time, energy and the envi- ronment are made, then reasonably smaller corpora may do just as well or even better in some cases. Besides, us- ing a small corpus, we obtain better WordSim scores, cor- responding Spearman correlation and better downstream performances (with significance tests) compared to the original model, trained on a 100 billion-word corpus. Keywords: Named Entity Recognition, Sentiment Analysis, Hyperparameters 1 Introduction There have been many implementations of the word2vec model in either of the two architectures it provides: contin- uous skipgram and continuous bag-of-words (CBoW) [1]. Similar distributed models of word or subword embeddings (or vector representations) find usage in SotA, deep neu- ral networks like bidirectional encoder representations from transformers (BERT) and its successors [2, 3, 4].",
            "Similar distributed models of word or subword embeddings (or vector representations) find usage in SotA, deep neu- ral networks like bidirectional encoder representations from transformers (BERT) and its successors [2, 3, 4]. BERT generates contextual representations of words after been trained for extended periods on large corpora, unsuper- *Corresponding author: Tosin P. Adewumi, SRT Department, EISLAB, Lule\u00e5 University of Technology, 97187, Sweden, E-mail: tosin.adewumi@ltu.se Foteini Liwicki, SRT Department, EISLAB, Lule\u00e5 University of Technology, 97187, Sweden, E-mail: foteini.liwicki@ltu.se Marcus Liwicki, SRT Department, EISLAB, Lule\u00e5 University of Technology, 97187, Sweden, E-mail: marcus.liwicki@ltu.se vised, using the attention mechanisms [5]. Unsupervised learning provides feature representations using large unla- belled corpora [6].",
            "Unsupervised learning provides feature representations using large unla- belled corpora [6]. It has been observed that various hyper-parameter combinations have been used in different research involv- ing word2vec, after its release, with the possibility of many of them being sub-optimal [7, 8, 9]. Therefore, the authors seek to address the research question: what is the opti- mal combination of word2vec hyper-parameters for intrin- sic and extrinsic NLP purposes, specifically NER and SA? There are astronomically high numbers of combinations of hyper-parameters possible for neural networks, even with just a few layers [10]. Hence, the scope of our extensive, empirical work over three English corpora is on dimension size, training epochs, window size and vocabulary size for the training algorithms (hierarchical softmax and negative sampling) of both skipgram and CBoW. The objective of this work is to determine the optimal combinations of word2vec hyper-parameters for intrinsic evaluation (semantic and syntactic analogies) and a few extrinsic evaluation tasks [11, 12]. It is not our objective in this work to set new SotA results.",
            "The objective of this work is to determine the optimal combinations of word2vec hyper-parameters for intrinsic evaluation (semantic and syntactic analogies) and a few extrinsic evaluation tasks [11, 12]. It is not our objective in this work to set new SotA results. Some main contributions of this research are the empirical establishment of optimal combinations of word2vec hyper-parameters for NLP tasks, discovering the behaviour of quality of vectors vis-a-vis in- creasing dimensions and the confirmation of embeddings performance being task-specific for the downstream. The rest of this paper is organised as follows: related work, ma- terials and methods used, experimental that describes ex- periments performed, results and discussion that present final results, and conclusion. 2 Related Work Breaking away from the non-distributed (high-dimensional, sparse) representations of words, typical of traditional bag- of-words or one-hot-encoding [13], [1] created word2vec. Word2Vec consists of two shallow neural network archi- tectures: continuous skipgram and CBoW. It uses dis- tributed (low-dimensional, dense) representations of words that group similar words.",
            "Word2Vec consists of two shallow neural network archi- tectures: continuous skipgram and CBoW. It uses dis- tributed (low-dimensional, dense) representations of words that group similar words. This new model traded the com- plexity of deep neural network architectures, by other re- searchers, for more efficient training over large corpora. Its arXiv:2003.11645v3  [cs.CL]  17 Apr 2021",
            "2 Tosin P. Adewumi, Foteini Liwicki, and Marcus Liwicki, Article title architectures have two training algorithms: negative sam- pling and hierarchical softmax [14]. The released model was trained on Google news dataset of 100 billion words. Im- plementations of the model have been undertaken by re- searchers in the programming languages Python and C++, though the original was done in C [15]. The Python imple- mentations are slower to train, being an interpreted lan- gauge [16, 17]. Continuous skipgram predicts (by maximizing classi- fication of) words before and after the center word, for a given range. Since distant words are less connected to a center word in a sentence, less weight is assigned to such distant words in training. CBoW, on the other hand, uses words from the history and future in a sequence, with the objective of correctly classifying the target word in the middle. It works by projecting all history or future words within a chosen window into the same position, averaging their vectors. Hence, the order of words in the history or future does not influence the averaged vector.",
            "It works by projecting all history or future words within a chosen window into the same position, averaging their vectors. Hence, the order of words in the history or future does not influence the averaged vector. This is sim- ilar to the traditional bag-of-words. A log-linear classifier is used in both architectures [1]. In further work, they ex- tended the model to be able to do phrase representations and subsample frequent words [14]. Earlier models like la- tent dirichlet allocation (LDA) and latent semantic analysis (LSA) exist and effectively achieve low dimensional vectors by matrix factorization [18, 10]. It\u2019s been shown that word vectors are beneficial for NLP tasks [13], such as SA and NER. Besides, [1] showed with vector space algebra that relationships among words can be evaluated, expressing the quality of vectors pro- duced from the model. The famous, semantic example: vec- tor(\"King\") - vector(\"Man\") + vector(\"Woman\") \u2248vec- tor(\"Queen\") can be verified using cosine distance. Syn- tactic relationship examples include plural verbs and past tense, among others.",
            "The famous, semantic example: vec- tor(\"King\") - vector(\"Man\") + vector(\"Woman\") \u2248vec- tor(\"Queen\") can be verified using cosine distance. Syn- tactic relationship examples include plural verbs and past tense, among others. WordSimilarity-353 (WordSim) test set is another analysis tool for word vectors [19]. Unlike Google analogy score, which is based on vector space alge- bra, WordSim is based on human expert-assigned semantic similarity on two sets of English word pairs. Both tools measure embedding quality, with a scaled score of 1 being the highest (very much similar or exact, in Google analogy case). Like word embeddings, subword representations have proven to be helpful when dealing with out-of-vocabulary (OOV) words and [20] used such embeddings to guide the parsing of OOV words in their work on meaning represen- tation for robots. Despite their success, word embeddings display biases (as one of their shortcomings) seen in the data they are trained on [21].",
            "Despite their success, word embeddings display biases (as one of their shortcomings) seen in the data they are trained on [21]. Intrinsic tests, in the form of word similarity or analogy tests, reveal meaningful rela- tions among words in embeddings, given the relationship among words in context [1, 22]. However, it is inappropriate to assume such intrinsic tests are sufficient in themselves, just as it is inappropriate to assume one particular down- stream test is sufficient to generalise the performance of embeddings on all NLP tasks [23, 24, 25]. [1] tried various hyper-parameters with both architec- tures of their model, ranging from 50 to 1,000 dimensions, 30,000 to 3,000,000 vocabulary sizes, 1 to 3 epochs, among others. In our work, we extended research to 3,000 dimen- sions and epochs of 5 and 10. Different observations were noticed from the many trials. They observed diminishing returns after a certain point, despite additional dimensions or larger, unstructured training data. However, quality in- creased when both dimensions and data size were increased together.",
            "Different observations were noticed from the many trials. They observed diminishing returns after a certain point, despite additional dimensions or larger, unstructured training data. However, quality in- creased when both dimensions and data size were increased together. Although they pointed out that choice of optimal hyper-parameter configurations depends on the NLP prob- lem at hand, they identified the most important factors as architecture, dimension size, subsampling rate, and the window size. In addition, it has been observed that larger datasets improve the quality of word vectors and, poten- tially, performance on downstream tasks [26, 1] . 3 Materials and methods 3.1 Datasets The corpora used for word embeddings are the 2019 En- glish Wiki News Abstract by [27] of about 15MB, 2019 En- glish Simple Wiki (SW) Articles by [28] of about 711MB and the Billion Word (BW) of 3.9GB by [29].",
            "The corpus used for sentiment analysis is the internet movie database (IMDb) of movie reviews by [30] while that for NER is the Groningen Meaning Bank (GMB) by [31], containing 47,959 sentence samples. The IMDb dataset used has a to- tal of 25,000 sentences with half being positive sentiments and the other half being negative sentiments. The GMB dataset has 17 labels, with 9 main labels and 2 context tags. Google (semantic and syntactic) analogy test set by [1] and WordSimilarity-353 (with Spearman correlation) by [19] were chosen for intrinsic evaluations. 3.2 Embeddings The hyper-parameters tuned in a grid search for the em- beddings are given in table 1. The models were generated in a shared cluster running Ubuntu 16 with 32 CPUs of 32x Intel Xeon 4110 at 2.1GHz. Gensim [15] Python library implementation of word2vec was used. This is because of",
            "Tosin P. Adewumi, Foteini Liwicki, and Marcus Liwicki, Article title 3 Tab. 1: Upstream hyper-parameter choices Hyper-parameter Values Dimension size 300, 1200, 1800, 2400, 3000 Window size (w) 4, 8 Architecture Skipgram (s1), CBoW (s0) Algorithm H. Softmax (h1), N. Sampling (h0) Epochs 5, 10 its relative stability, popular support and to minimize the time required in writing and testing a new implementa- tion in Python from scratch. Our models are available for confirmation and source codes are available on github.1 3.3 Downstream Architectures The downstream experiments were run on a Tesla GPU on a shared DGX cluster running Ubuntu 18. Pytorch deep learning framework was used. A long short term memory network (LSTM) was trained on the GMB dataset for NER. A BiLSTM network was trained on the IMDb dataset for SA. The BiLSTM in- cludes an additional hidden linear layer before the output layer.",
            "A long short term memory network (LSTM) was trained on the GMB dataset for NER. A BiLSTM network was trained on the IMDb dataset for SA. The BiLSTM in- cludes an additional hidden linear layer before the output layer. Hyper-parameter details of the two networks for the downstream tasks are given in table 2. The metrics for ex- trinsic evaluation include F1, precision, recall and accuracy scores (in the case of SA). Tab. 2: Downstream network hyper-parameters Archi Epochs Hidden Dim LR Loss LSTM 40 128 0.01 Cross Entropy BiLSTM 20 128 * 2 0.0001 BCELoss 4 Experimental To form the vocabulary for the embeddings, words occur- ring less than 5 times in the corpora were dropped, stop words removed using the natural language toolkit (NLTK) [32] and additional data pre-processing carried out. Table 1 describes most hyper-parameters explored for each dataset and notations used.",
            "Table 1 describes most hyper-parameters explored for each dataset and notations used. In all, 80 runs (of about 160 minutes) were conducted for the 15MB Wiki Abstract dataset with 80 serialized models totaling 15.136GB while 80 runs (for 1 https:\/\/github.com\/tosingithub\/sdesk Fig. 1: Network architecture for NER Fig. 2: Network architecture for SA",
            "4 Tosin P. Adewumi, Foteini Liwicki, and Marcus Liwicki, Article title over 320 hours) were conducted for the 711MB SW dataset, with 80 serialized models totaling over 145GB. Experi- ments for all combinations for 300 dimensions were con- ducted on the 3.9GB training set of the BW corpus and additional runs for other dimensions for the window size 8 + skipgram + hierarchical softmax combination to ver- ify the trend of quality of word vectors as dimensions are increased. Preferably, more than one training instance would have been run per combination for a model and an average taken, however, the long hours involved made this pro- hibitive. Despite this, we randomly ran a few combinations more than once and confirmed the difference in intrinsic scores were negligible. For both downstream tasks, the default Pytorch em- bedding was tested before being replaced by the original (100B) pre-trained embedding and ours. In each case, the dataset was shuffled before training and split in the ratio 70:15:15 for training, dev and test sets. Batch size of 64 was used and Adam as optimizer.",
            "In each case, the dataset was shuffled before training and split in the ratio 70:15:15 for training, dev and test sets. Batch size of 64 was used and Adam as optimizer. For each task, experi- ments for each embedding was conducted four times and an average value calculated. 5 Results and Discussion The WordSim result output file from the Gensim Python program always has more than one value reported, includ- ing the Spearman correlation. The first value is reported as WordSim score1 in the relevant table. Table 3 summarizes key results from the intrinsic evaluations for 300 dimen- sions2. Table 4 reveals the training time (in hours) and av- erage embedding loading time (in seconds) representative of the various models used. Tables 5 and 6 summarize key results for the extrinsic evaluations.",
            "Table 4 reveals the training time (in hours) and av- erage embedding loading time (in seconds) representative of the various models used. Tables 5 and 6 summarize key results for the extrinsic evaluations. Figures 3, 4, 5, 6 and 7 present line graph of the eight combinations for different dimension sizes for SW, the trend of SW and BW corpora over several dimension sizes, analogy score comparison for models across datasets, NER mean F1 scores on the GMB dataset and SA mean F1 scores on the IMDb dataset, re- spectively. Results for the smallest dataset (Wiki Abstract) are so poor, because of the tiny file size (15MB), there\u2019s no reason reporting them here. Hence, we have focused on re- sults from the SW and BW corpora. Best combination in terms of analogy sometimes changes when corpus size increases, as will be noticed from table 3. In terms of analogy score, for 10 epochs, w8s0h0 performs best while w8s1h0 performs best in terms of 2 The results are to 3 decimal places WordSim and corresponding Spearman correlation for SW.",
            "In terms of analogy score, for 10 epochs, w8s0h0 performs best while w8s1h0 performs best in terms of 2 The results are to 3 decimal places WordSim and corresponding Spearman correlation for SW. Meanwhile, increasing the corpus size to BW, w4s1h0 per- forms best in terms of analogy score while w8s1h0 main- tains its position as the best in terms of WordSim and Spearman correlation. Besides considering quality metrics, it can be observed from table 4 that comparative ratio of values between the models is not commensurate with the results in intrinsic or extrinsic values, especially when we consider the amount of time and energy spent, since more training time results in more energy consumption [17]. Information on the length of training time for the orig- inal 100B model is not readily available. However, it\u2019s in- teresting to note that it is a skipgram-negative sampling (s1h0) model. Its analogy score, which we tested and re- port, is confirmed in the original paper [1].",
            "However, it\u2019s in- teresting to note that it is a skipgram-negative sampling (s1h0) model. Its analogy score, which we tested and re- port, is confirmed in the original paper [1]. It beats our best models in only analogy score (even for SW), perform- ing worse in others, despite using a much bigger corpus of 3,000,000 vocabulary size and 100 billion words while SW had vocabulary size of 367,811 and is 711MB. It is very likely our analogy scores will improve when we use a much larger corpus, as can be observed from table 3, which in- volves just one billion words. Fig. 3: Simple Wiki: Analogy Scores for 10 Epochs (color needed) With regards to increasing dimension, though the two best combinations in analogy (w8s0h0 & w4s0h0) for SW, as shown in fig. 3, decreased only slightly compared to oth- ers, the increased training time and much larger serialized model size render any possible minimal score advantage with higher dimensions undesirable. As can be observed in fig.",
            "3, decreased only slightly compared to oth- ers, the increased training time and much larger serialized model size render any possible minimal score advantage with higher dimensions undesirable. As can be observed in fig. 4, from 100 dimensions, scores improve but start to drop after over 300 dimensions for SW and after over 400 dimensions for BW, confirming the observation by [1]. This trend is true for all combinations for all tests. Polyno- mial interpolation may be used to determine the optimal dimension in both corpora. With regards to NER, most pretrained embeddings outperformed the default Pytorch embedding, with our",
            "Tosin P. Adewumi, Foteini Liwicki, and Marcus Liwicki, Article title 5 Tab. 3: Scores for 300 dimensions for 10 epochs for SW, BW & 100B corpora. w8s1h1 w8s0h1 w8s0h0 w8s1h0 w4s1h1 w4s0h1 w4s0h0 w4s1h0 Simple Wiki Analogy 0.461 0.269 0.502 0.439 0.446 0.243 0.478 0.407 WordSim score1 0.636 0.611 0.654 0.655 0.635 0.608 0.620 0.635 Spearman 0.670 0.648 0.667 0.695 0.668 0.648 0.629 0.682 Billion Word Analogy 0.587 0.376 0.638 0.681 0.556 0.363 0.629 0.684 WordSim score1 0.614 0.",
            "695 0.668 0.648 0.629 0.682 Billion Word Analogy 0.587 0.376 0.638 0.681 0.556 0.363 0.629 0.684 WordSim score1 0.614 0.511 0.599 0.644 0.593 0.508 0.597 0.635 Spearman 0.653 0.535 0.618 0.681 0.629 0.527 0.615 0.677 Google News - 100B (s1h0) Analogy: 0.740 WordSim: 0.624 Spearman: 0.659 Fig. 4: Analogy Scores for w4s1h1 of SW for 5 Epochs & w8s1h1 of BW for 10 epochs (not drawn to scale from 400) (color needed) Fig. 5: Comparison of 300 dimension models for 10 epochs for SW & BW corpora Tab.",
            "5: Comparison of 300 dimension models for 10 epochs for SW & BW corpora Tab. 4: Training & embedding loading time for w8s1h0, w8s1h1 & 100B Model Training (hours) Loading Time (s) SW w8s1h0 5.44 1.93 BW w8s1h1 27.22 4.89 GoogleNews (100B) - 97.73 Tab.",
            "5: NER Dev and Test sets Mean Results Metric Default 100B w8 s0 h0 w8 s1 h0 BW w4 s1 h0 Dev, Test Dev, Test Dev, Test Dev, Test Dev, Test F1 0.661, 0.661 0.679, 0.676 0.668, 0.669 0.583, 0.676 0.679, 0.677 Precision 0.609, 0.608 0.646, 0.642 0.636, 0.637 0.553, 0.642 0.644, 0.642 Recall 0.723, 0.724 0.716, 0.714 0.704, 0.706 0.618, 0.715 0.717, 0.717 Tab. 6: Sentiment Analysis Dev and Test sets Mean Results Metric Default 100B w8 s0 h0 w8 s1 h0 BW w4 s1 h0 Dev, Test Dev, Test Dev, Test Dev, Test Dev, Test F1 0.810, 0.805 0.",
            "6: Sentiment Analysis Dev and Test sets Mean Results Metric Default 100B w8 s0 h0 w8 s1 h0 BW w4 s1 h0 Dev, Test Dev, Test Dev, Test Dev, Test Dev, Test F1 0.810, 0.805 0.384, 0.386 0.798, 0.799 0.548, 0.553 0.498, 0.390 Precision 0.805, 0.795 0.6, 0.603 0.814, 0.811 0.510, 0.524 0.535, 0.533 Recall 0.818, 0.816 0.303, 0.303 0.788, 0.792 0.717, 0.723 0.592, 0.386 Accuracy 0.807, 0.804 0.549, 0.55 0.801, 0.802 0.519, 0.522 0.519, 0.",
            "792 0.717, 0.723 0.592, 0.386 Accuracy 0.807, 0.804 0.549, 0.55 0.801, 0.802 0.519, 0.522 0.519, 0.517 BW w4s1h0 model (which is best in BW analogy score) performing best in F1 score and closely followed by the 100B model. On the other hand, with regards to SA, Py- torch embedding outperformed the pretrained embeddings but was closely followed by our SW w8s0h0 model (which also had the best SW analogy score). 100B performed sec- ond worst of all, despite originating from a very huge cor- pus. The combinations w8s0h0 & w4s0h0 of SW performed reasonably well in both extrinsic tasks, just as the default Pytorch embedding did.",
            "100B performed sec- ond worst of all, despite originating from a very huge cor- pus. The combinations w8s0h0 & w4s0h0 of SW performed reasonably well in both extrinsic tasks, just as the default Pytorch embedding did. Significance tests using bootstrap, based on [33], on the results of the differences in means of the 100B & BW w4s1h0 models for NER shows a 95% confidence interval (CI) of [-0.008, 0.01] but [0.274, 0.504] for 100B & SW w8s0h0 for SA. Since one algorithm is involved in the com- parisons in each case, unlike multiple algorithms [34], the applied bootstrap approach is adequate. The CI interval for",
            "6 REFERENCES NER includes 0, thus we can conclude the difference was likely due to chance and fail to reject the null hypothesis but the CI for SA does not include 0, thus the difference is unlikely due to chance so we reject the null hypothesis. Fig. 6: Named Entity Recognition (NER) Mean F1 Scores on GMB Dataset Fig. 7: Sentiment Analysis (SA) Mean F1 Scores on IMDb Dataset 6 Conclusions This work analyses, empirically, optimal combinations of hyper-parameters for embeddings, specifically for word2vec. It further shows that for downstream tasks, like NER and SA, there\u2019s no silver bullet! However, some combinations show strong performance across tasks. Per- formance of embeddings is task-specific and high analogy scores do not necessarily correlate positively with perfor- mance on downstream tasks. This point on correlation is somewhat similar to results by [35] and [12]. It was dis- covered that increasing embedding dimension size depreci- ates performance after a point. If strong considerations of saving time, energy and the environment are made, then reasonably smaller corpora may suffice or even be better in some cases.",
            "It was dis- covered that increasing embedding dimension size depreci- ates performance after a point. If strong considerations of saving time, energy and the environment are made, then reasonably smaller corpora may suffice or even be better in some cases. The on-going drive by many researchers to use ever-growing data to train deep neural networks can bene- fit from the findings of this work. Indeed, hyper-parameter choices are very important in neural network systems [10]. Future work that may be investigated are the perfor- mance of other architectures of word or sub-word embed- dings in SotA networks like BERT (based on a matrix of hyper-parameters), the performance and comparison of em- beddings applied to other less-explored languages, and how these embeddings perform in other downstream tasks. Funding This work was supported partially by Vinnova under the project number 2019-02996 \u2019Spr\u00e5kmodeller f\u00f6r svenska myndigheter\u2019. They, however, had no involvement in any stage of this work, including study design, interpretation of data and report writing. References [1] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.",
            "They, however, had no involvement in any stage of this work, including study design, interpretation of data and report writing. References [1] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. [2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidi- rectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [3] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.",
            "Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. [4] Colin Raffel, Noam Shazeer, Adam Roberts, Kather- ine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of trans- fer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019. [5] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998\u20136008, 2017. [6] Martin L\u00e4ngkvist, Lars Karlsson, and Amy Loutfi. A review of unsupervised feature learning and deep learning for time-series modeling. Pattern Recognition Letters, 42:11\u201324, 2014.",
            "REFERENCES 7 [7] Bhuwan Dhingra, Hanxiao Liu, Ruslan Salakhutdi- nov, and William W Cohen. A comparative study of word embeddings for reading comprehension. arXiv preprint arXiv:1703.00993, 2017. [8] Marwa Naili, Anja Habacha Chaibi, and Henda Haj- jami Ben Ghezala. Comparative study of word embed- ding methods in topic segmentation. Procedia com- puter science, 112:340\u2013349, 2017. [9] Yanshan Wang, Sijia Liu, Naveed Afzal, Majid Rastegar-Mojarad, Liwei Wang, Feichen Shen, Paul Kingsbury, and Hongfang Liu. A comparison of word embeddings for the biomedical natural language pro- cessing. Journal of biomedical informatics, 87:12\u201320, 2018. [10] Omer Levy, Yoav Goldberg, and Ido Dagan. Improv- ing distributional similarity with lessons learned from word embeddings.",
            "Journal of biomedical informatics, 87:12\u201320, 2018. [10] Omer Levy, Yoav Goldberg, and Ido Dagan. Improv- ing distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics, 3:211\u2013225, 2015. [11] Yijia Zhang, Qingyu Chen, Zhihao Yang, Hongfei Lin, and Zhiyong Lu. Biowordvec, improving biomedi- cal word embeddings with subword information and mesh. Scientific data, 6(1):1\u20139, 2019. [12] Bin Wang, Angela Wang, Fenxiao Chen, Yuncheng Wang, and C-C Jay Kuo. Evaluating word embedding models: methods and experimental results. APSIPA Transactions on Signal and Information Processing, 8, 2019. [13] Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384\u2013394.",
            "[13] Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384\u2013394. Association for Computa- tional Linguistics, 2010. [14] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111\u20133119, 2013. [15] Radim \u0158eh\u016f\u0159ek and Petr Sojka. Software Frame- work for Topic Modelling with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u201350, Val- letta, Malta, May 2010. ELRA. http:\/\/is.muni.cz\/ publication\/884893\/en. [16] Tosin P Adewumi. Inner loop program construct: A faster way for program execution.",
            "ELRA. http:\/\/is.muni.cz\/ publication\/884893\/en. [16] Tosin P Adewumi. Inner loop program construct: A faster way for program execution. Open Computer Science, 8(1):115\u2013122, 2018. [17] Tosin P Adewumi and Marcus Liwicki. Inner for-loop for speeding up blockchain mining. Open Computer Science, 2019. [18] Scott Deerwester, Susan T Dumais, George W Fur- nas, Thomas K Landauer, and Richard Harshman. Indexing by latent semantic analysis. Journal of the American society for information science, 41(6):391\u2013 407, 1990. [19] Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. Placing search in context: The concept re- visited. ACM Transactions on information systems, 20(1):116\u2013131, 2002.",
            "Placing search in context: The concept re- visited. ACM Transactions on information systems, 20(1):116\u2013131, 2002. [20] Jesse Thomason, Aishwarya Padmakumar, Jivko Sinapov, Nick Walker, Yuqian Jiang, Harel Yedid- sion, Justin Hart, Peter Stone, and Raymond Mooney. Jointly improving parsing and perception for natu- ral language commands through human-robot dialog. Journal of Artificial Intelligence Research, 67:327\u2013374, 2020. [21] Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In Advances in neural in- formation processing systems, pages 4349\u20134357, 2016. [22] Jeffrey Pennington, Richard Socher, and Christo- pher D Manning. Glove: Global vectors for word rep- resentation.",
            "In Advances in neural in- formation processing systems, pages 4349\u20134357, 2016. [22] Jeffrey Pennington, Richard Socher, and Christo- pher D Manning. Glove: Global vectors for word rep- resentation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543, 2014. [23] Albert Gatt and Emiel Krahmer. Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. Journal of Artificial In- telligence Research, 61:65\u2013170, 2018. [24] Tosin P Adewumi, Foteini Liwicki, and Marcus Li- wicki. Corpora compared: The case of the swedish gigaword & wikipedia corpora. arXiv preprint arXiv:2011.03281, 2020. [25] Tosin P Adewumi, Foteini Liwicki, and Marcus Li- wicki. The challenge of diacritics in yoruba embed- dings.",
            "arXiv preprint arXiv:2011.03281, 2020. [25] Tosin P Adewumi, Foteini Liwicki, and Marcus Li- wicki. The challenge of diacritics in yoruba embed- dings. arXiv preprint arXiv:2011.07605, 2020. [26] Tosin P Adewumi, Foteini Liwicki, and Marcus Li- wicki. Conversational systems in machine learning from the point of view of the philosophy of sci- ence\u2014using alime chat and related studies. Philoso- phies, 4(3):41, 2019. [27] Wikipedia. Wiki news abstract. 2019. [28] Wikipedia. Simple wiki articles. 2019. [29] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. One billion word benchmark for measuring progress in statistical language modeling. Technical report, Google, 2013.",
            "[29] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. One billion word benchmark for measuring progress in statistical language modeling. Technical report, Google, 2013. [30] Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts.",
            "8 REFERENCES Learning word vectors for sentiment analysis. In Pro- ceedings of the 49th annual meeting of the associa- tion for computational linguistics: Human language technologies-volume 1, pages 142\u2013150. Association for Computational Linguistics, 2011. [31] Johan Bos, Valerio Basile, Kilian Evang, Noortje J Venhuizen, and Johannes Bjerva. The groningen meaning bank. In Handbook of linguistic annotation, pages 463\u2013496. Springer, 2017. [32] Edward Loper and Steven Bird. Nltk: the natural language toolkit. arXiv preprint cs\/0205028, 2002. [33] Guillaume Calmettes, Gordon B Drummond, and Sarah L Vowler. Making do with what we have: use your bootstraps. Advances in physiology education, 36(3):177\u2013180, 2012. [34] Janez Dem\u0161ar. Statistical comparisons of classifiers over multiple data sets. Journal of Machine learning research, 7(Jan):1\u201330, 2006.",
            "Advances in physiology education, 36(3):177\u2013180, 2012. [34] Janez Dem\u0161ar. Statistical comparisons of classifiers over multiple data sets. Journal of Machine learning research, 7(Jan):1\u201330, 2006. [35] Billy Chiu, Anna Korhonen, and Sampo Pyysalo. In- trinsic evaluation of word vectors fails to predict ex- trinsic performance. In Proceedings of the 1st work- shop on evaluating vector-space representations for NLP, pages 1\u20136, 2016."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-2003.11645.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 7399.000213623047,
    "avg_doclen_est": 176.1666717529297
}
