[
  "Learning Multilingual Word Embeddings Using Image-Text Data Karan Singhal Stanford University ksinghal@cs.stanford.edu Karthik Raman Google AI karthikraman@google.com Balder ten Cate Google AI balder@google.com Abstract There has been signi\ufb01cant interest recently in learning multilingual word embeddings \u2013 in which semantically similar words across lan- guages have similar embeddings. State-of- the-art approaches have relied on expensive labeled data, which is unavailable for low- resource languages, or have involved post- hoc uni\ufb01cation of monolingual embeddings. In the present paper, we investigate the ef\ufb01- cacy of multilingual embeddings learned from weakly-supervised image-text data. In particu- lar, we propose methods for learning multilin- gual embeddings using image-text data, by en- forcing similarity between the representations of the image and that of the text. Our ex- periments reveal that even without using any expensive labeled data, a bag-of-words-based embedding model trained on image-text data achieves performance comparable to the state- of-the-art on crosslingual semantic similarity tasks.",
  "Our ex- periments reveal that even without using any expensive labeled data, a bag-of-words-based embedding model trained on image-text data achieves performance comparable to the state- of-the-art on crosslingual semantic similarity tasks. 1 Introduction Recent advances in learning distributed represen- tations for words (i.e., word embeddings) have resulted in improvements across numerous natu- ral language understanding tasks (Mikolov et al., 2013c; Pennington et al., 2014). These methods use unlabeled text corpora to model the seman- tic content of words using their co-occurring con- text words. Key to this is the observation that semantically similar words have similar contexts (Sahlgren, 2008), thus leading to similar word embeddings. A limitation of these word embed- ding approaches is that they only produce mono- lingual embeddings. This is because word co- occurrences are very likely to be limited to being within language rather than across language in text corpora. Hence semantically similar words across languages are unlikely to have similar word em- beddings.",
  "This is because word co- occurrences are very likely to be limited to being within language rather than across language in text corpora. Hence semantically similar words across languages are unlikely to have similar word em- beddings. To remedy this, there has been recent work on learning multilingual word embeddings, in which semantically similar words within and across lan- guages have similar word embeddings (Ruder, 2017). Multilingual embeddings are not just in- teresting as an interlingua between multiple lan- guages; they are useful in many downstream ap- plications. For example, one application of mul- tilingual embeddings is to \ufb01nd semantically simi- lar words and phrases across languages (Ammar et al., 2016). Another use of multilingual em- beddings is in enabling zero-shot learning on un- seen languages, just as monolingual word embed- dings enable predictions on unseen words (Artetxe and Schwenk, 2018). In other words, a classi\ufb01er using pretrained multilingual word embeddings can generalize to other languages even if train- ing data is only in English.",
  "In other words, a classi\ufb01er using pretrained multilingual word embeddings can generalize to other languages even if train- ing data is only in English. Interestingly, multilin- gual embeddings have also been shown to improve monolingual task performance (Faruqui and Dyer, 2014b; Kiela et al., 2014). Consequently, multilingual embeddings can be very useful for low-resource languages \u2013 they al- low us to overcome the scarcity of data in these languages. However, as detailed in Section 2, most work on learning multilingual word embeddings so far has heavily relied on the availability of ex- pensive resources such as word-aligned / sentence- aligned parallel corpora or bilingual lexicons. Un- fortunately, this data can be prohibitively expen- sive to collect for many languages. Furthermore even for languages with such data available, the coverage of the data is a limiting factor that re- stricts how much of the semantic space can be aligned across languages. Overcoming this data bottleneck is a key contribution of our work. We investigate the use of cheaply available, weakly-supervised image-text data for learning multilingual embeddings.",
  "Overcoming this data bottleneck is a key contribution of our work. We investigate the use of cheaply available, weakly-supervised image-text data for learning multilingual embeddings. Images are a rich, language-agnostic medium that can provide a arXiv:1905.12260v1  [cs.CL]  29 May 2019",
  "bridge across languages. For example, the En- glish word \u201ccat\u201d might be found on webpages con- taining images of cats. Similarly, the German word \u201ckatze\u201d (meaning cat) is likely to be found on other webpages containing similar (or perhaps identical) images of cats. Thus, images can be used to learn that these words have similar seman- tic content. Importantly, image-text data is gener- ally available on the internet even for low-resource languages. As image data has proliferated on the inter- net, tools for understanding images have ad- vanced considerably. Convolutional neural net- works (CNNs) have achieved roughly human-level or better performance on vision tasks, particularly classi\ufb01cation (Russakovsky et al., 2014; Szegedy et al., 2015; He et al., 2016). During classi\ufb01ca- tion of an image, CNNs compute intermediate out- puts that have been used as generic image features that perform well across a variety of vision tasks (Sharif Razavian et al., 2014).",
  "During classi\ufb01ca- tion of an image, CNNs compute intermediate out- puts that have been used as generic image features that perform well across a variety of vision tasks (Sharif Razavian et al., 2014). We use these image features to enforce that words associated with sim- ilar images have similar embeddings. Since words associated with similar images are likely to have similar semantic content, even across languages, our learned embeddings capture crosslingual sim- ilarity. There has been other recent work on reducing the amount of supervision required to learn multi- lingual embeddings (cf. Section 2). These meth- ods take monolingual embeddings learned using existing methods and align them post-hoc in a shared embedding space. A limitation with post- hoc alignment of monolingual embeddings, \ufb01rst noticed by Duong et al. (2017), is that doing train- ing of monolingual embeddings and alignment separately may lead to worse results than joint training of embeddings in one step.",
  "A limitation with post- hoc alignment of monolingual embeddings, \ufb01rst noticed by Duong et al. (2017), is that doing train- ing of monolingual embeddings and alignment separately may lead to worse results than joint training of embeddings in one step. Since the monolingual embedding objective is distinct from the multilingual embedding objective, monolin- gual embeddings are not required to capture all in- formation helpful for post-hoc multilingual align- ment. Post-hoc alignment loses out on some in- formation, whereas joint training does not. Duong et al. (2017) observe improved results using a joint training method compared to a similar post-hoc method. Thus, a joint training approach is de- sirable. To our knowledge, no previous method jointly learns multilingual word embeddings using weakly-supervised data available for low-resource languages. To summarize: In this paper we propose an ap- proach for learning multilingual word embeddings using image-text data jointly across all languages. We demonstrate that even a bag-of-words based embedding approach achieves performance com- petitive with the state-of-the-art on crosslingual semantic similarity tasks.",
  "To summarize: In this paper we propose an ap- proach for learning multilingual word embeddings using image-text data jointly across all languages. We demonstrate that even a bag-of-words based embedding approach achieves performance com- petitive with the state-of-the-art on crosslingual semantic similarity tasks. We present experiments for understanding the effect of using pixel data as compared to co-occurrences alone. We also pro- vide a method for training and making predictions on multilingual word embeddings even when the language of the text is unknown. 2 Related Work Most work on producing multilingual embeddings has relied on crosslingual human-labeled data, such as bilingual lexicons (Mikolov et al., 2013b; Ammar et al., 2016; Faruqui and Dyer, 2014b; Xing et al., 2015) or parallel/aligned corpora (Kle- mentiev et al., 2012; Ammar et al., 2016; Luong et al., 2015; Vuli\u00b4c and Moens, 2015).",
  "These works are also largely bilingual due to either limitations of methods or the requirement for data that exists only for a few language pairs. Bilingual embed- dings are less desirable because they do not lever- age the relevant resources of other languages. For example, in learning bilingual embeddings for En- glish and French, it may be useful to leverage re- sources in Spanish, since French and Spanish are closely related. Bilingual embeddings are also limited in their applications to just one language pair. For instance, Luong et al. (2015) propose BiSkip, a model that extends the skip-gram ap- proach of Mikolov et al. (2013a) to a bilingual par- allel corpus. The embedding for a word is trained to predict not only its own context, but also the contexts for corresponding words in a second cor- pus in a different language. Ammar et al. (2016) extend this approach further to multiple languages. This method, called MultiSkip, is compared to our methods in Section 5.",
  "Ammar et al. (2016) extend this approach further to multiple languages. This method, called MultiSkip, is compared to our methods in Section 5. There has been some recent work on reducing the amount of human-labeled data required to learn multilingual embeddings, enabling work on low-resource languages (Smith et al., 2017; Artetxe et al., 2017; Conneau et al., 2017). These methods take monolingual embeddings learned using existing methods and align them post-hoc in a shared embedding space, exploiting the structural similarity of monolingual embedding",
  "Figure 1: Our high-level approach for constraining query and image representations to be similar. The English query \u201ccat with big ears\u201d is mapped to Q, while the corresponding image example is mapped to I. We use the cosine similarity of these representations as input to a softmax loss function. The model task can be understood as predicting if an image is relevant to a given query. spaces \ufb01rst noticed by Mikolov et al. (2013b). As discussed in Section 1, post-hoc alignment of monolingual embeddings is inherently subopti- mal. For example, Smith et al. (2017) and Artetxe et al. (2017) use human-labeled data, along with shared surface forms across languages, to learn an alignment in the bilingual setting. Conneau et al. (2017) build on this for the multilingual setting, using no human-labeled data and instead using an adversarial approach to maximize alignment between monolingual embedding spaces given their structural similarities. This method (MUSE) outperforms previous approaches and represents the state-of-the-art. We compare it to our methods in Section 5.",
  "This method (MUSE) outperforms previous approaches and represents the state-of-the-art. We compare it to our methods in Section 5. There has been other work using image-text data to improve image and caption representa- tions for image tasks and to learn word transla- tions (Karpathy and Fei-Fei, 2015; Frome et al., 2013; Gella et al., 2017; Calixto et al., 2017; He- witt et al., 2018), but no work using images to learn competitive multilingual word-level embed- dings. 3 Data We experiment using a dataset derived from Google Images search results1. The dataset con- sists of queries and the corresponding image search results. For example, one (query, image) pair might be \u201ccat with big ears\u201d and an image of a cat. Each (query, image) pair also has a weight corresponding to a relevance score of the image for the query. The dataset includes 3 billion (query, image, weight) triples, with 900 million 1https://images.google.com unique images and 220 million unique queries.",
  "Each (query, image) pair also has a weight corresponding to a relevance score of the image for the query. The dataset includes 3 billion (query, image, weight) triples, with 900 million 1https://images.google.com unique images and 220 million unique queries. The data was prepared by \ufb01rst taking the query- image set, \ufb01ltering to remove any personally iden- ti\ufb01able information and adult content, and tok- enizing the remaining queries by replacing special characters with spaces and trimming extraneous whitespace. Rare tokens (those that do not appear in queries at least six times) are \ufb01ltered out. Each token in each query is given a language tag based on the user-set home language of the user making the search on Google Images. For example, if the query \u201cback pain\u201d is made by a user with English as her home language, then the query is stored as \u201cen:back en:pain\u201d. The dataset includes queries in about 130 languages. Though the speci\ufb01c dataset we use is propri- etary, Hewitt et al.",
  "The dataset includes queries in about 130 languages. Though the speci\ufb01c dataset we use is propri- etary, Hewitt et al. (2018) have obtained a similar dataset, using the Google Images search interface, that comprises queries in 100 languages. 4 Methods We present a series of experiments to investigate the usefulness of multimodal image-text data in learning multilingual embeddings. The crux of our method involves enforcing that for each query- image pair, the query representation (Q) is similar to the image representation (I). The query rep- resentation is a function of the word embeddings for each word in a (language-tagged) query, so en- forcing this constraint on the query representation also has the effect of constraining the correspond- ing multilingual word embeddings. Given some Q and some I, we enforce that the representations are similar by maximizing their cosine similarity. We use a combination of co-",
  "sine similarity and softmax objective to produce our loss. This high-level approach is illustrated in Figure 1. In particular, we calculate unweighted loss as follows for a query q and a corresponding image i: loss(Query q, Image i) = \u2212log e QTq Ii |Qq||Ii| P j e QTq Ij |Qq||Ij| where Qq is the query representation for query q; Ii is the image representation corresponding to im- age i; j ranges over all images in the corpus; and QT q Ii is the dot product of the vectors Qq and Ii. Note that this requires that Qq and Ij have identi- cal dimensionality. If a weight w is provided for the (query, image) pair, the loss is multiplied by the weight. Observe that Q and I remain unspec- i\ufb01ed for now: we detail different experiments in- volving different representations below.",
  "If a weight w is provided for the (query, image) pair, the loss is multiplied by the weight. Observe that Q and I remain unspec- i\ufb01ed for now: we detail different experiments in- volving different representations below. In practice, given the size of our dataset, calcu- lating the full denominator of the loss for a query, image pair would involve iterating through each image for each query, which is O(n2) in the num- ber of training examples. To remedy this, we cal- culated the loss within each batch separately. That is, the denominator of the loss only involved sum- ming over images in the same batch as the query. We used a batch size of 1000 for all experiments. In principle, the negative sampling approach used by Mikolov et al. (2013c) could be used instead to prevent quadratic time complexity. We can interpret this loss function as producing a softmax classi\ufb01cation task for queries and im- ages: given a query, the model needs to predict the image relevant to that query.",
  "(2013c) could be used instead to prevent quadratic time complexity. We can interpret this loss function as producing a softmax classi\ufb01cation task for queries and im- ages: given a query, the model needs to predict the image relevant to that query. The cosine simi- larity between the image representation Ii and the query representation Qq is normalized under soft- max to produce a \u201cbelief\u201d that the image i is the image relevant to the query q. This is analogous to the skip-gram model proposed by Mikolov et al. (2013a), although we use cosine similarity instead of dot product. Just as the skip-gram model en- sures the embeddings of words are predictive of their contexts, our model ensures the embeddings of queries (and their constituent words) are predic- tive of images relevant to them. 4.1 Leveraging Image Understanding Given the natural co-occurrence of images and text on the internet and the availability of powerful generic features, a \ufb01rst approach is to use generic image features as the foundation for the image rep- resentation I. We apply two fully-connected lay- ers to learn a transformation from image features to the \ufb01nal representation.",
  "We can compute the image representation Ii for image i as: Ii = ReLU(U \u2217ReLU(V fi + b1) + b2) where fi is a d-dimensional column vector repre- senting generic image features for image i, V is a m\u00d7d matrix, b1 is an m-dimensional column vec- tor, U is a n\u00d7m matrix, and b2 is an n-dimensional column vector. We use a recti\ufb01ed linear unit acti- vation function after each fully-connected layer. We use 64-dimensional image features derived from image-text data using an approach similar to that used by Juan et al. (2019), who train image features to discriminate between \ufb01ne-grained se- mantic image labels. We run two experiments with m and n: in the \ufb01rst, m = 200 and n = 100 (pro- ducing 100-dimensional embeddings), and in the second, m = 300 and n = 300 (producing 300- dimensional embeddings). For the query representation, we use a simple approach. The query representation is just the av- erage of its constituent multilingual embeddings.",
  "For the query representation, we use a simple approach. The query representation is just the av- erage of its constituent multilingual embeddings. Then, as the query representation is constrained to be similar to corresponding image representa- tions, the multilingual embeddings (randomly ini- tialized) are also constrained. Note that each word in each query is pre\ufb01xed with the language of the query. For example, the English query \u201cback pain\u201d is treated as \u201cen:back en:pain\u201d, and the multilingual embeddings that are averaged are those for \u201cen:back\u201d and \u201cen:pain\u201d. This means that words in different languages with shared surface forms are given separate embed- dings. We experiment with shared embeddings for words with shared surface forms in Section 4.3. In practice, we use a \ufb01xed multilingual vocab- ulary for the word embeddings, given the size of the dataset. Out-of-vocabulary words are handled by hashing them to a \ufb01xed number of embedding buckets (we use 1,000,000).",
  "In practice, we use a \ufb01xed multilingual vocab- ulary for the word embeddings, given the size of the dataset. Out-of-vocabulary words are handled by hashing them to a \ufb01xed number of embedding buckets (we use 1,000,000). That is, there are 1,000,000 embeddings for all out-of-vocabulary words, and the assignment of embedding for each word is determined by a hash function. Our approach for leveraging image understand- ing is shown in Figure 2. 4.2 Co-Occurrence Only Another approach for generating query and image representations is treating images as a black box.",
  "Figure 2: Our \ufb01rst method for calculating query and image representations, as presented in Section 4.1. To calculate the query representation, the multilingual embeddings for each language-pre\ufb01xed token are averaged. To calculate the image representation, d-dimensional generic image features are passed through two fully-connected layers with m and n neurons. Without using pixel data, how well can we do? Given the statistics of our dataset (3B query, image pairs with 220M unique queries and 900M unique images), we know that different queries co-occur with the same images. Intuitively, if a query q1 co- occurs with many of the same images as query q2, then q1 and q2 are likely to be semantically simi- lar, regardless of the visual content of the shared images. Thus, we can use a method that uses only co-occurrence statistics to better understand how well we can capture relationships between queries. This method serves as a baseline to our initial ap- proach leveraging image understanding.",
  "Thus, we can use a method that uses only co-occurrence statistics to better understand how well we can capture relationships between queries. This method serves as a baseline to our initial ap- proach leveraging image understanding. In this setting, we keep query representations the same, and we modify image representations as follows: the image representation for an image is a randomly initialized, trainable vector (of the same dimensionality as the query representation, to en- sure the cosine similarity can be calculated). The intuition for this approach is that if two queries are both associated with an image, their query repre- sentations will both be constrained to be similar to the same vector, and so the query representations themselves are constrained to be similar. This ap- proach is a simple way to adapt our method to make use of only co-occurrence statistics. One concern with this approach is that many queries may not have signi\ufb01cant image co- occurrences with other queries. In particular, there are likely many images associated with only a sin- gle query.",
  "One concern with this approach is that many queries may not have signi\ufb01cant image co- occurrences with other queries. In particular, there are likely many images associated with only a sin- gle query. These isolated images pull query rep- resentations toward their respective random im- age representations (adding noise), but do not pro- vide any information about the relationships be- tween queries. Additionally, even for images as- sociated with multiple queries, if these queries are all within language, then they may not be very helpful for learning multilingual embeddings. Consequently, we run two experiments: one with the original dataset and one with a subset of the dataset that contains only images associated with queries in at least two different languages. This subset of the dataset has 540 million query, image pairs (down from 3 billion). For both experiments, we use m = 200 and n = 100 and produce 100- dimensional embeddings.",
  "This subset of the dataset has 540 million query, image pairs (down from 3 billion). For both experiments, we use m = 200 and n = 100 and produce 100- dimensional embeddings. 4.3 Language Unaware Query Representation In Section 4.1, our method for computing query representations involved prepending language pre- \ufb01xes to each token, ensuring that the multilingual embedding for the English word \u201cpain\u201d is distinct from that for the French word \u201cpain\u201d (meaning bread). These query representations are language aware, meaning that a language tag is required for each query during both training and prediction. In the weakly-supervised setting, we may want to re- lax this requirement, as language-tagged data is not always readily available. In our language unaware setting, language tags are not necessary. Each surface form in each query has a distinct embedding, and words with shared surface forms across languages (e.g., En- glish \u201cpain\u201d and French \u201cpain\u201d) have a shared em- bedding. In this sense, shared surface forms are used as a bridge between languages. This is il- lustrated in Figure 3.",
  "In this sense, shared surface forms are used as a bridge between languages. This is il- lustrated in Figure 3. This may be helpful in cer- tain cases, as for English \u201cactor\u201d and Spanish \u201cac- tor\u201d. The image representations leverage generic",
  "Figure 3: In our language unaware approach, language tags are not prepended to each token, so the word \u201cpain\u201d in English and French share an embedding. image features, exactly as in Section 4.1. In our language-unaware experiment, we use m = 200 and n = 100 and produce 100-dimensional em- beddings. 4.4 Evaluation We evaluate our learned multilingual embeddings using six crosslingual semantic similarity tasks, two multilingual document classi\ufb01cation tasks, and 13 monolingual semantic similarity tasks. We adapt code from Ammar et al. (2016) and Faruqui and Dyer (2014a) for evaluation. Crosslingual Semantic Similarity This task measures how well multilingual embeddings cap- ture semantic similarity of words, as judged by human raters. The task consists of a series of crosslingual word pairs. For each word pair in the task, human raters judge how semantically simi- lar the words are. The model also predicts how similar the words are, using the cosine similarity between the embeddings.",
  "The task consists of a series of crosslingual word pairs. For each word pair in the task, human raters judge how semantically simi- lar the words are. The model also predicts how similar the words are, using the cosine similarity between the embeddings. The score on the task is the Spearman correlation between the human rat- ings and the model predictions. The speci\ufb01c six subtasks we use are part of the Rubenstein-Goodenough dataset (Rubenstein and Goodenough, 1965) and detailed by Ammar et al. (2016). We also include an additional task aggre- gating the six subtasks. Multilingual Document Classi\ufb01cation In this task, a classi\ufb01er built on top of learned multilin- gual embeddings is trained on the RCV corpus of newswire text as in Klementiev et al. (2012) and Ammar et al. (2016). The corpus consists of doc- uments in seven languages on four topics, and the classi\ufb01er predicts the topic. The score on the task is test accuracy.",
  "(2012) and Ammar et al. (2016). The corpus consists of doc- uments in seven languages on four topics, and the classi\ufb01er predicts the topic. The score on the task is test accuracy. Note that each document is mono- lingual, so this task measures performance within languages for multiple languages (as opposed to crosslingual performance). Monolingual Semantic Similarity This task is the same as the crosslingual semantic similarity task described above, but all word pairs are in En- glish. We use this to understand how monolingual performance differs across methods. We present an average score across the 13 subtasks provided by Faruqui and Dyer (2014a). Coverage Evaluation tasks also report a cover- age, which is the fraction of the test data that a set of multilingual embeddings is able to make predic- tions on. This is needed because not every word in the evaluation task has a corresponding learned multilingual embedding. Thus, if coverage is low, scores are less likely to be reliable.",
  "This is needed because not every word in the evaluation task has a corresponding learned multilingual embedding. Thus, if coverage is low, scores are less likely to be reliable. 5 Results and Conclusions We \ufb01rst present results on the crosslingual se- mantic similarity and multilingual document clas- si\ufb01cation for our previously described experi- ments. We compare against the multiSkip ap- proach by Ammar et al. (2016) and the state-of- the-art MUSE approach by Conneau et al. (2017). Results for crosslingual semantic similarity are presented in Table 1, and results for multilingual document classi\ufb01cation are presented in Table 2. Our experiments corresponding to Section 4.1 are titled ImageVec 100-Dim and ImageVec 300- Dim in Tables 1 and 2. Both experiments sig- ni\ufb01cantly outperform the multiSkip experiments in all crosslingual semantic similarity subtasks, and the 300-dimensional experiment slightly out- performs MUSE as well. Note that coverage scores are generally around 0.8 for these exper- iments.",
  "Both experiments sig- ni\ufb01cantly outperform the multiSkip experiments in all crosslingual semantic similarity subtasks, and the 300-dimensional experiment slightly out- performs MUSE as well. Note that coverage scores are generally around 0.8 for these exper- iments. In multilingual document classi\ufb01cation, MUSE achieves the best scores, and while our 300-dimensional experiment outperforms the mul- tiSkip 40-dimensional experiment, it does not per- form as well as the 512-dimensional experiment. Note that coverage scores are lower on these tasks. One possible explanation for the difference in performance across the crosslingual semantic sim- ilarity task and multilingual document classi\ufb01ca- tion task is that the former measures crosslingual performance, whereas the latter measures mono- lingual performance in multiple languages, as de- scribed in Section 4.4. We brie\ufb02y discuss further evidence that our models perform less well in the monolingual context below.",
  "en+es en+de en+fr de+es de+fr fr+es all ImageVec 100-Dim .75 [.87] .77 [.87] .84 [.74] .80 [.83] .76 [.77] .77 [.73] .79 [.81] ImageVec 300-Dim .79 [.87] .81 [.87] .86 [.74] .81 [.83] .77 [.77] .80 [.73] .82 [.81] ImageVec Baseline .10 [.87] .03 [.87] .14 [.74] -.25 [.83] .07 [.77] .15 [.73] .08 [.81] ImageVec Baseline 2 Lang. .27 [.87] .38 [.79] .23 [.74] .26 [.75] .16 [.75] .27 [.73] .28 [.78] ImageVec Lang.",
  ".27 [.87] .38 [.79] .23 [.74] .26 [.75] .16 [.75] .27 [.73] .28 [.78] ImageVec Lang. Unaware .59 [.87] .62 [.87] .79 [.74] .63 [.83] .73 [.77] .73 [.73] .67 [.81] multiSkip 40-Dim .51 [.83] .67 [.75] .44 [.70] .39 [.63] .29 [.56] .43 [.60] .49 [.68] multiSkip 512-Dim .43 [.83] .73 [.76] .62 [.70] .43 [.63] .24 [.56] .48 [.60] .50 [.69] MUSE 300-Dim .76 [.87] .85 [.86] .79 [.74] .83 [.81] .73 [.77] .74 [.73] .79 [.80] Table 1: Crosslingual semantic similarity scores (Spearman\u2019s \u03c1) across six subtasks for ImageVec (our method) and previous work. Coverage is in brackets. The last column indicates the combined score across all subtasks.",
  "Coverage is in brackets. The last column indicates the combined score across all subtasks. Best scores on each subtask are bolded. en+da+it 7 Lang. ImageVec 100-Dim .74 [.60] .79 [.52] ImageVec 300-Dim .80 [.60] .84 [.52] ImageVec Baseline .60 [.60] .59 [.52] ImageVec Baseline 2 Lang. .65 [.45] .65 [.36] ImageVec Lang. Unaware .73 [.60] .78 [.52] multiSkip 40-Dim .77 [.45] .82 [.44] multiSkip 512-Dim .87 [.48] .91 [.46] MUSE 300-Dim .87 [.54] .91 [.51] Table 2: Multilingual document classi\ufb01cation accuracy scores across two subtasks for ImageVec (our method) and previous work. Coverage is in brackets. Best scores are bolded (ties broken by coverage). Is Image Understanding Necessary?",
  "Coverage is in brackets. Best scores are bolded (ties broken by coverage). Is Image Understanding Necessary? Compar- ing the experiments leveraging image understand- ing to our co-occurrence-only baseline experi- ments ImageVec Baseline and ImageVec Base- line 2 Lang described in Section 4.2, we see that performance is signi\ufb01cantly degraded without pixel data (note that both experiments use 100- dimensional embeddings). Still, the results for multilingual document classi\ufb01cation, in particular, show that we are able to learn multilingual word embeddings using co-occurrence between queries and images alone. Interestingly, we can see that performance in the experiment in which images are \ufb01ltered to be as- sociated with at least two languages appears better than the baseline experiment on the full dataset (al- though coverage is low for multilingual document classi\ufb01cation). As mentioned in Section 4.2, this may be because images without multiple queries degrade performance by introducing noise to the optimization problem.",
  "As mentioned in Section 4.2, this may be because images without multiple queries degrade performance by introducing noise to the optimization problem. We also experimented with the same \ufb01ltering on the experiments using im- age understanding to see if this could further boost performance (results not shown), but this reduced performance to a similar extent as random data \ufb01l- tering. This is likely because even isolated images (with just one query associated with an image) are still helpful for the task in this case, since the use of generic image features still constrains queries associated with similar images to have similar rep- resentations. Even in the \ufb01ltered baseline, results for both tasks are signi\ufb01cantly lower than the methods leveraging image understanding, indicating that while co-occurrence data alone is useful, pixel data may be needed to learn competitive multilin- gual embeddings using our method. Language Unaware Learning The language unaware setting only differs from the language aware one when words share a common surface form. In some cases, words sharing a common sur- face form have the same meaning across languages (i.e., cognates). An example is \u201cactor\u201d in English and Spanish.",
  "Language Unaware Learning The language unaware setting only differs from the language aware one when words share a common surface form. In some cases, words sharing a common sur- face form have the same meaning across languages (i.e., cognates). An example is \u201cactor\u201d in English and Spanish. In these cases, the language unaware setting may boost performance, as the embedding for \u201cactor\u201d effectively has more training data be- hind it. In other cases, words sharing a common surface form have different meanings across lan- guages (i.e., false cognates). An example is \u201cpain\u201d in English and French. In these cases, we expect language unawareness to reduce performance, es- pecially if the meanings of false cognates are very different. Our results for our 100-dimensional language unaware embeddings are presented in Tables 1 and 2 as ImageVec Lang. Unaware. We can see",
  "avg. score ImageVec 100-Dim .48 [.98] ImageVec 300-Dim .48 [.98] ImageVec Baseline .24 [.98] ImageVec Baseline 2 Lang. .33 [.95] ImageVec Lang. Unaware .42 [.98] multiSkip 40-Dim .44 [.94] multiSkip 512-Dim .44 [.96] MUSE 300-Dim .62 [.97] Table 3: Average monolingual semantic similarity score (Spearman\u2019s \u03c1) across 13 subtasks for ImageVec (our method) and previous work. Average coverage is in brackets. Best score is bolded. that this experiment performs worse on crosslin- gual semantic similarity but about the same on multilingual document classi\ufb01cation as the 100-dimensional language aware experiment (Im- ageVec 100-Dim). Still, on crosslingual semantic similarity, it signi\ufb01cantly outperforms both mul- tiSkip experiments. Thus, in applications where language unaware training or prediction is impor- tant, our method produces multilingual embed- dings competitive with other language aware ap- proaches.",
  "Still, on crosslingual semantic similarity, it signi\ufb01cantly outperforms both mul- tiSkip experiments. Thus, in applications where language unaware training or prediction is impor- tant, our method produces multilingual embed- dings competitive with other language aware ap- proaches. Effect of Embedding Size In these experi- ments, embeddings with higher dimensionalities generally perform better in both evaluation tasks. 300-dimensional embeddings produced using our method slightly outperform 100-dimensional ones in every subtask for both tasks. Monolingual Embedding Quality As men- tioned earlier in Section 5, we suspect that the dif- ference in performance (as compared to MUSE) on crosslingual semantic similarity and multilin- gual document classi\ufb01cation for our experiments might be due to reduced monolingual perfor- mance. After all, other methods train by leverag- ing word contexts (and subword information, in the case of MUSE) in a large monolingual cor- pus, whereas we use only images as a bridge be- tween words within and across languages.",
  "After all, other methods train by leverag- ing word contexts (and subword information, in the case of MUSE) in a large monolingual cor- pus, whereas we use only images as a bridge be- tween words within and across languages. Es- pecially for words representing abstract concepts without obvious image associations (consider the word \u201cdemocracy\u201d), it is likely that our method would produce lower quality within-language em- beddings than text-only methods. This is not un- expected: Hewitt et al. (2018) found that word translations learned via images are worse for more abstract words and Kiela et al. (2014) found that using image data is unhelpful for improving the quality of representations for some concepts. It stands to reason then that our method would produce weaker monolingual performance. To test this, we ran 13 English monolingual semantic similarity tasks on each experiment. We present average scores in Table 3. We can see that 300-dimensional embeddings produced using our method fare signi\ufb01cantly worse than MUSE em- beddings, although they perform similarly to the multiSkip embeddings.",
  "We present average scores in Table 3. We can see that 300-dimensional embeddings produced using our method fare signi\ufb01cantly worse than MUSE em- beddings, although they perform similarly to the multiSkip embeddings. For comparison, competi- tive English word embeddings achieve results sim- ilar to MUSE. This suggests that there is signif- icant room for improvement within language (at least for English) in the quality of our learned multilingual embeddings. Improving monolingual performance would also likely boost scores across other tasks, motivating future work in this direc- tion. 6 Discussion We demonstrated how to learn competitive mul- tilingual word embeddings using image-text data \u2013 which is available for low-resource languages. We have presented experiments for understand- ing the effect of using pixel data as compared to co-occurrences alone. We have also proposed a method for training and making predictions on multilingual word embeddings even when lan- guage tags for words are unavailable. Using a simple bag-of-words approach, we achieve per- formance competitive with the state-of-the-art on crosslingual semantic similarity tasks.",
  "We have also proposed a method for training and making predictions on multilingual word embeddings even when lan- guage tags for words are unavailable. Using a simple bag-of-words approach, we achieve per- formance competitive with the state-of-the-art on crosslingual semantic similarity tasks. We have also identi\ufb01ed a direction for future work: within language performance is weaker than the state-of-the-art, likely because our work leveraged only image-text data rather than a large monolingual corpus. Fortunately, our joint train- ing approach provides a simple extension of our method for future work: multi-task joint training. For example, in a triple-task setting, we can si- multaneously (1) constrain query and relevant im- age representations to be similar and (2) constrain word embeddings to be predictive of context in large monolingual corpora and (3) constrain repre- sentations for parallel text across languages to be similar. For the second task, implementing recent advances in producing monolingual embeddings, such as using subword information, is likely to",
  "improve results. Multilingual embeddings learned in a multi-task setting would reap both the bene- \ufb01ts of our methods and existing methods for pro- ducing word embeddings. For example, while our method is likely to perform worse for more ab- stract words, when combined with existing ap- proaches it is likely to achieve more consistent per- formance. An interesting effect of our approach is that queries and images are embedded into a shared space through the query and image representa- tions. This setup enables a range of future re- search directions and applications, including bet- ter image features, better monolingual text rep- resentations (especially for visual tasks), nearest- neighbor search for text or images given one modality (or both), and joint prediction using text and images. Acknowledgments We thank Tom Duerig, Ev- geniy Gabrilovich, Dan Gillick, Raphael Hoff- mann, Zhen Li, Alessandro Presta, Aleksei Tim- ofeev, Radu Soricut, and our reviewers for their insightful feedback and comments.",
  "Acknowledgments We thank Tom Duerig, Ev- geniy Gabrilovich, Dan Gillick, Raphael Hoff- mann, Zhen Li, Alessandro Presta, Aleksei Tim- ofeev, Radu Soricut, and our reviewers for their insightful feedback and comments. References Waleed Ammar, George Mulcaire, Yulia Tsvetkov, Guillaume Lample, Chris Dyer, and Noah A. Smith. 2016. Massively multilingual word embeddings. CoRR, abs/1602.01925. Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017. Learning bilingual word embeddings with (almost) no bilingual data. In Proceedings of the 55th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), volume 1, pages 451\u2013462. Mikel Artetxe and Holger Schwenk. 2018. Mas- sively multilingual sentence embeddings for zero- shot cross-lingual transfer and beyond. CoRR, abs/1812.10464.",
  "Mikel Artetxe and Holger Schwenk. 2018. Mas- sively multilingual sentence embeddings for zero- shot cross-lingual transfer and beyond. CoRR, abs/1812.10464. Iacer Calixto, Qun Liu, and Nick Campbell. 2017. Multilingual multi-modal embeddings for natural language processing. CoRR, abs/1702.01101. Alexis Conneau, Guillaume Lample, Marc\u2019Aurelio Ranzato, Ludovic Denoyer, and Herv\u00b4e J\u00b4egou. 2017. Word translation without parallel data. CoRR, abs/1710.04087. Long Duong, Hiroshi Kanayama, Tengfei Ma, Steven Bird, and Trevor Cohn. 2017. Multilingual training of crosslingual word embeddings. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, volume 1, pages 894\u2013904. Manaal Faruqui and Chris Dyer. 2014a.",
  "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, volume 1, pages 894\u2013904. Manaal Faruqui and Chris Dyer. 2014a. Community evaluation and exchange of word vectors at word- vectors.org. In Proceedings of ACL: System Demon- strations. Manaal Faruqui and Chris Dyer. 2014b. Improving vector space word representations using multilingual correlation. In Proceedings of the 14th Conference of the European Chapter of the Association for Com- putational Linguistics, pages 462\u2013471. Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Tomas Mikolov, et al. 2013. De- vise: A deep visual-semantic embedding model. In Advances in neural information processing systems, pages 2121\u20132129. Spandana Gella, Rico Sennrich, Frank Keller, and Mirella Lapata. 2017. Image pivoting for learn- ing multilingual multimodal representations.",
  "In Advances in neural information processing systems, pages 2121\u20132129. Spandana Gella, Rico Sennrich, Frank Keller, and Mirella Lapata. 2017. Image pivoting for learn- ing multilingual multimodal representations. CoRR, abs/1707.07601. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recog- nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013 778. John Hewitt, Daphne Ippolito, Brendan Callahan, Reno Kriz, Derry Tanti Wijaya, and Chris Callison-Burch. 2018. Learning translations via images with a mas- sively multilingual image dataset. In Proceedings of the 56th Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers), volume 1, pages 2566\u20132576.",
  "2018. Learning translations via images with a mas- sively multilingual image dataset. In Proceedings of the 56th Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers), volume 1, pages 2566\u20132576. Da-Cheng Juan, Chun-Ta Lu, Zhen Li, Futang Peng, Aleksei Timofeev, Yi-Ting Chen, Yaxi Gao, Tom Duerig, Andrew Tomkins, and Sujith Ravi. 2019. Graph-rise: Graph-regularized image semantic em- bedding. arXiv preprint arXiv:1902.10814. Andrej Karpathy and Li Fei-Fei. 2015. Deep visual- semantic alignments for generating image descrip- tions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3128\u20133137. Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen Clark. 2014. Improving multi-modal representa- tions using image dispersion: Why less is sometimes more.",
  "In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3128\u20133137. Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen Clark. 2014. Improving multi-modal representa- tions using image dispersion: Why less is sometimes more. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Vol- ume 2: Short Papers), volume 2, pages 835\u2013841. Alexandre Klementiev, Ivan Titov, and Binod Bhat- tarai. 2012. Inducing crosslingual distributed repre- sentations of words. Proceedings of COLING 2012, pages 1459\u20131474. Thang Luong, Hieu Pham, and Christopher D Man- ning. 2015. Bilingual word representations with monolingual quality in mind. In Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing, pages 151\u2013159.",
  "Tomas Mikolov, Kai Chen, Greg Corrado, and Jef- frey Dean. 2013a. Ef\ufb01cient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013b. Exploiting similarities among languages for machine translation. CoRR, abs/1309.4168. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013c. Distributed representa- tions of words and phrases and their compositional- ity. In Advances in neural information processing systems, pages 3111\u20133119. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 confer- ence on empirical methods in natural language pro- cessing (EMNLP), pages 1532\u20131543. Herbert Rubenstein and John B Goodenough. 1965.",
  "Glove: Global vectors for word representation. In Proceedings of the 2014 confer- ence on empirical methods in natural language pro- cessing (EMNLP), pages 1532\u20131543. Herbert Rubenstein and John B Goodenough. 1965. Contextual correlates of synonymy. Communica- tions of the ACM, 8(10):627\u2013633. Sebastian Ruder. 2017. A survey of cross-lingual em- bedding models. CoRR, abs/1706.04902. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, An- drej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Fei-Fei Li. 2014. Imagenet large scale visual recognition challenge. CoRR, abs/1409.0575. Magnus Sahlgren. 2008. The distributional hypothesis. Italian Journal of Disability Studies, 20:33\u201353.",
  "2014. Imagenet large scale visual recognition challenge. CoRR, abs/1409.0575. Magnus Sahlgren. 2008. The distributional hypothesis. Italian Journal of Disability Studies, 20:33\u201353. Ali Sharif Razavian, Hossein Azizpour, Josephine Sul- livan, and Stefan Carlsson. 2014. Cnn features off- the-shelf: an astounding baseline for recognition. In Proceedings of the IEEE conference on computer vi- sion and pattern recognition workshops, pages 806\u2013 813. Samuel L Smith, David HP Turban, Steven Hamblin, and Nils Y Hammerla. 2017. Of\ufb02ine bilingual word vectors, orthogonal transformations and the inverted softmax. arXiv preprint arXiv:1702.03859. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du- mitru Erhan, Vincent Vanhoucke, and Andrew Ra- binovich. 2015. Going deeper with convolutions.",
  "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du- mitru Erhan, Vincent Vanhoucke, and Andrew Ra- binovich. 2015. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vi- sion and pattern recognition, pages 1\u20139. Ivan Vuli\u00b4c and Marie-Francine Moens. 2015. Bilin- gual word embeddings from non-parallel document- aligned data applied to bilingual lexicon induction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Lan- guage Processing (Volume 2: Short Papers), vol- ume 2, pages 719\u2013725. Chao Xing, Dong Wang, Chao Liu, and Yiye Lin. 2015. Normalized word embedding and orthogonal trans- form for bilingual word translation. In Proceed- ings of the 2015 Conference of the North Ameri- can Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1006\u20131011."
]