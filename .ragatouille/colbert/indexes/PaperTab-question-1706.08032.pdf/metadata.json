{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "A Deep Neural Architecture for Sentence-level Sentiment Classi\ufb01cation in Twitter Social Networking Huy Nguyen\u2217and Minh-Le Nguyen\u2217 \u2217Japan Advanced Institute of Science and Technology Ishikawa, Japan {huy.nguyen, nguyenml}@jaist.ac.jp Abstract\u2014This paper introduces a novel deep learning framework including a lexicon-based approach for sentence- level prediction of sentiment label distribution. We propose to \ufb01rst apply semantic rules and then use a Deep Convolutional Neural Network (DeepCNN) for character-level embeddings in order to increase information for word-level embedding. After that, a Bidirectional Long Short-Term Memory network (Bi- LSTM) produces a sentence-wide feature representation from the word-level embedding. We evaluate our approach on three twitter sentiment classi\ufb01cation datasets. Experimental results show that our model can improve the classi\ufb01cation accuracy of sentence-level sentiment analysis in Twitter social networking. Keywords-Twitter; Sentiment classi\ufb01cation, Opinion mining. I. INTRODUCTION Twitter sentiment classi\ufb01cation have intensively re- searched in recent years [1][2].",
      "Keywords-Twitter; Sentiment classi\ufb01cation, Opinion mining. I. INTRODUCTION Twitter sentiment classi\ufb01cation have intensively re- searched in recent years [1][2]. Different approaches were developed for Twitter sentiment classi\ufb01cation by using ma- chine learning such as Support Vector Machine (SVM) with rule-based features [3] and the combination of SVMs and Naive Bayes (NB) [4]. In addition, hybrid approaches combining lexicon-based and machine learning methods also achieved high performance described in [5]. However, a problem of traditional machine learning is how to de\ufb01ne a feature extractor for a speci\ufb01c domain in order to extract important features. Deep learning models are different from traditional ma- chine learning methods in that a deep learning model does not depend on feature extractors because features are extracted during training progress. The use of deep learning methods becomes to achieve remarkable results for sentiment analysis [6][7][8]. Some researchers used Convo- lutional Neural Network (CNN) for sentiment classi\ufb01cation. CNN models have been shown to be effective for NLP.",
      "The use of deep learning methods becomes to achieve remarkable results for sentiment analysis [6][7][8]. Some researchers used Convo- lutional Neural Network (CNN) for sentiment classi\ufb01cation. CNN models have been shown to be effective for NLP. For example, [7] proposed various kinds of CNN to learn sentiment-bearing sentence vectors, [6] adopted two CNNs in character-level to sentence-level representation for senti- ment analysis. [8] constructs experiments on a character- level CNN for several large-scale datasets. In addition, Long Short-Term Memory (LSTM) is another state-of-the- art semantic composition model for sentiment classi\ufb01cation with many variants described in [9]. The studies reveal that using a CNN is useful in extracting information and \ufb01nding feature detectors from texts. In addition, a LSTM can be good in maintaining word order and the context of words. However, in some important aspects, the use of CNN or LSTM separately may not capture enough information.",
      "The studies reveal that using a CNN is useful in extracting information and \ufb01nding feature detectors from texts. In addition, a LSTM can be good in maintaining word order and the context of words. However, in some important aspects, the use of CNN or LSTM separately may not capture enough information. Inspired by the models above, the goal of this research is using a Deep Convolutional Neural Network (DeepCNN) to exploit the information of characters of words in order to support word-level embedding. A Bi-LSTM produces a sentence-wide feature representation based on these embed- dings. The Bi-LSTM is a version of [10] with Full Gradient described in [11]. In addition, the rules-based approach also effects classi\ufb01cation accuracy by focusing on important sub- sentences expressing the main sentiment of a tweet while removing unnecessary parts of a tweet. The paper makes the following contributions: \u2022 We construct a tweet processor removing unnecessary sub-sentences from tweets in order that the model learns important information in a tweet. We share ideas with [1] and [12], however, our tweet processor keeps emoticons in tweets and only uses rules to remove non- essential parts for handling negation.",
      "We share ideas with [1] and [12], however, our tweet processor keeps emoticons in tweets and only uses rules to remove non- essential parts for handling negation. \u2022 We train DeepCNN on top of character embeddings to produce feature maps which capture the morphological and shape information of a word. The morphological and shape information illustrate how words are formed, and their relationship to other words. DeepCNN trans- forms the character-level embeddings into global \ufb01xed- sized feature vectors at higher abstract level. Such character feature vectors contribute enriching the in- formation of words in a sentence. \u2022 We create an integration of global \ufb01xed-size character feature vectors and word-level embedding for the Bi- LSTM. The Bi-LSTM connects the information of words in a sequence and maintains the order of words for sentence-level representation. The organization of the present paper is as follows: In section 2, we describe the model architecture which introduces the structure of the model. We explain the basic idea of model and the way of constructing the model.",
      "The organization of the present paper is as follows: In section 2, we describe the model architecture which introduces the structure of the model. We explain the basic idea of model and the way of constructing the model. Section 3 show results and analysis and section 4 summarize this arXiv:1706.08032v1  [cs.CL]  25 Jun 2017",
      "paper. II. MODEL ARCHITECTURE A. Basic idea Our proposed model consists of a deep learning clas- si\ufb01er and a tweet processor. The deep learning classi\ufb01er is a combination of DeepCNN and Bi-LSTM. The tweet processor standardizes tweets and then applies semantic rules on datasets. We construct a framework that treats the deep learning classi\ufb01er and the tweet processor as two distinct components. We believe that standardizing data is an important step to achieve high accuracy. To formulate our problem in increasing the accuracy of the classi\ufb01er, we illustrate our model in Figure. 1 as follows: 1) Tweets are \ufb01rstly considered via a processor based on preprocessing steps [1] and the semantic rules- based method [12] in order to standardize tweets and capture only important information containing the main sentiment of a tweet. 2) We use DeepCNN with Wide convolution for character-level embeddings. A wide convolution can learn to recognize speci\ufb01c n-grams at every position in a word that allows features to be extracted indepen- dently of these positions in the word. These features maintain the order and relative positions of characters.",
      "A wide convolution can learn to recognize speci\ufb01c n-grams at every position in a word that allows features to be extracted indepen- dently of these positions in the word. These features maintain the order and relative positions of characters. A DeepCNN is constructed by two wide convolution layers and the need of multiple wide convolution layers is widely accepted that a model constructing by multiple processing layers have the ability to learn representations of data with higher levels of abstrac- tion [13]. Therefore, we use DeepCNN for character- level embeddings to support morphological and shape information for a word. The DeepCNN produces N global \ufb01xed-sized feature vectors for N words. 3) A combination of the global \ufb01xed-size feature vectors and word-level embedding is fed into Bi-LSTM. The Bi-LSTM produces a sentence-level representation by maintaining the order of words. Our work is philosophically similar to [6]. However, our model is distinguished with their approaches in two aspects: \u2022 Using DeepCNN with two wide convolution layers to increase representation with multiple levels of abstrac- tion.",
      "The Bi-LSTM produces a sentence-level representation by maintaining the order of words. Our work is philosophically similar to [6]. However, our model is distinguished with their approaches in two aspects: \u2022 Using DeepCNN with two wide convolution layers to increase representation with multiple levels of abstrac- tion. \u2022 Integrating global character \ufb01xed-sized feature vectors with word-level embedding to extract a sentence-wide feature set via Bi-LSTM. This deals with three main problems: (i) Sentences have any different size; (ii) The semantic and the syntactic of words in a sentence are captured in order to increase information for a word; (iii) Important information of characters that can appear at any position in a word are extracted. In sub-section B, we introduce various kinds of dataset. The modules of our model are constructed in other sub- sections. B. Data Preparation \u2022 Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from [1]. [1] constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small.",
      "B. Data Preparation \u2022 Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from [1]. [1] constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks [1] [6] [14]. \u2022 Sanders - Twitter Sentiment Corpus: This dataset con- sists of hand-classi\ufb01ed tweets collected by using search terms: #apple, #google, #microsoft and #twitter. We construct the dataset as [15] for binary classi\ufb01cation. \u2022 Health Care Reform (HCR): This dataset was con- structed by crawling tweets containing the hashtag #hcr [16]. Task is to predict positive/negative tweets [15]. C. Preprocessing We \ufb01rstly take unique properties of Twitter in order to reduce the feature space such as Username, Usage of links, None, URLs and Repeated Letters. We then process retweets, stop words, links, URLs, mentions, punctuation and accentuation.",
      "C. Preprocessing We \ufb01rstly take unique properties of Twitter in order to reduce the feature space such as Username, Usage of links, None, URLs and Repeated Letters. We then process retweets, stop words, links, URLs, mentions, punctuation and accentuation. For emoticons, [1] revealed that the training process makes the use of emoticons as noisy labels and they stripped the emoticons out from their training dataset because [1] believed that if we consider the emoticons, there is a negative impact on the accuracies of classi\ufb01ers. In addition, removing emoticons makes the classi\ufb01ers learns from other features (e.g. unigrams and bi-grams) presented in tweets and the classi\ufb01ers only use these non-emoticon features to predict the sentiment of tweets. However, there is a problem is that if the test set contains emoticons, they do not in\ufb02uence the classi\ufb01ers because emoticon features do not contain in its training data. This is a limitation of [1], because the emoticon features would be useful when classifying test data.",
      "This is a limitation of [1], because the emoticon features would be useful when classifying test data. Therefore, we keep emoticon features in the datasets because deep learning models can capture more information from emoticon features for increasing classi\ufb01cation accuracy. D. Semantic Rules (SR) In Twitter social networking, people express their opinions containing sub-sentences. These sub-sentences using speci\ufb01c PoS particles (Conjunction and Conjunctive adverbs), like \u201dbut, while, however, despite, however\u201d have different po- larities. However, the overall sentiment of tweets often focus on certain sub-sentences. For example: \u2022 @lonedog bwahahah...you are amazing! However, it was quite the letdown. \u2022 @kirstiealley my dentist is great but she\u2019s expen- sive...=( In two tweets above, the overall sentiment is negative. However, the main sentiment is only in the sub-sentences following but and however. This inspires a processing step to remove unessential parts in a tweet. Rule-based approach",
      "Figure 1. The overview of a deep learning system. Table I SEMANTIC RULES [12] Rule Semantic rules Example - STS Corpus Output R11 If a sentence contains \u201dbut\u201d, disregard all previous sentiment and only take the sentiment of the part after \u201dbut\u201d. @kirstiealley my dentist is great but she\u2019s expensive...=( she\u2019s expensive...=( R12 If a sentence contains \u201ddespite\u201d, only take sentiment of the part before \u201ddespite\u201d. I\u2019m not dead despite rumours to the con- trary. I\u2019m not dead R13 If a sentence contains \u201dunless\u201d, and \u201dunless\u201d is followed by a negative clause, disregard the \u201dunless\u201d clause. laptop charger is broken - unless a little cricket set up home inside it overnight. typical at the worst possible time. laptop charger is broken R14 If a sentence contains \u201dwhile\u201d, disregard the sentence following the \u201dwhile\u201d and take the sentiment only of the sentence that follows the one after the \u201dwhile\u201d. My throat is killing me, and While I got a decent night\u2019s sleep last night, I still feel like I\u2019m about to fall over.",
      "My throat is killing me, and While I got a decent night\u2019s sleep last night, I still feel like I\u2019m about to fall over. I still feel like I\u2019m about to fall over R15 If the sentence contains \u201dhowever\u201d, disregard the sentence preceding the \u201dhowever\u201d and take the sentiment only of the sentence that follows the \u201dhowever\u201d. @lonedog bwahahah...you are amazing! However, it was quite the letdown. it was quite the letdown. Table II THE NUMBER OF TWEETS ARE PROCESSED BY USING SEMANTIC RULES Dataset Set # Sentences/ tweets STS Corpus Train 138703 Test 25 Sanders Train 39 Dev 74 Test 54 HCR Train 164 can assists these problems in handling negation and dealing with speci\ufb01c PoS particles led to effectively affect the \ufb01nal output of classi\ufb01cation [12][17]. [12] summarized a full presentation of their semantic rules approach and devised ten semantic rules in their hybrid approach based on the presentation of [17].",
      "[12] summarized a full presentation of their semantic rules approach and devised ten semantic rules in their hybrid approach based on the presentation of [17]. We use \ufb01ve rules in the semantic rules set because other \ufb01ve rules are only used to compute polarity of words after POS tagging or Parsing steps. We follow the same naming convention for rules utilized by [12] to represent the rules utilized in our proposed method. The rules utilized in the proposed method are displayed in Table I in which is included examples from STS Corpus and output after using the rules. Table II illustrates the number of processed sentences on each dataset. E. Representation Levels To construct embedding inputs for our model, we use a \ufb01xed-sized word vocabulary V word and a \ufb01xed-sized char- acter vocabulary V char. Given a word wi is composed from characters {c1, c2, ..., cM}, the character-level embeddings are encoded by column vectors ui in the embedding matrix W char \u2208Rdchar\u00d7|V char|, where V char is the size of the character vocabulary. For word-level embedding rword, we use a pre-trained word-level embedding with dimension 200 or 300.",
      "For word-level embedding rword, we use a pre-trained word-level embedding with dimension 200 or 300. A pre-trained word-level embedding can capture the syntactic and semantic information of words [18]. We build every word wi into an embedding vi = [ri; ei] which is constructed by two sub-vectors: the word-level embedding ri \u2208Rdword and the character \ufb01xed-size feature vector ei \u2208Rl of wi where l is the length of the \ufb01lter of wide convolutions. We have N character \ufb01xed-size feature vectors corresponding to word-level embedding in a sentence.",
      "Figure 2. Deep Convolutional Neural Network (DeepCNN) for the sequence of character embeddings of a word. For example with 1 region size is 2 and 4 feature maps in the \ufb01rst convolution and 1 region size is 3 with 3 feature maps in the second convolution. F. Deep Learning Module DeepCNN in the deep learning module is illustrated in Figure. 2. The DeepCNN has two wide convolution layers. The \ufb01rst layer extract local features around each character windows of the given word and using a max pooling over character windows to produce a global \ufb01xed-sized feature vector for the word. The second layer retrieves important context characters and transforms the representation at pre- vious level into a representation at higher abstract level. We have N global character \ufb01xed-sized feature vectors for N words. In the next step of Figure. 1, we construct the vector vi = [ri, ei] by concatenating the word-level embedding with the global character \ufb01xed-size feature vectors. The input of Bi- LSTM is a sequence of embeddings {v1, v2, ..., vN}.",
      "In the next step of Figure. 1, we construct the vector vi = [ri, ei] by concatenating the word-level embedding with the global character \ufb01xed-size feature vectors. The input of Bi- LSTM is a sequence of embeddings {v1, v2, ..., vN}. The use of the global character \ufb01xed-size feature vectors increases the relationship of words in the word-level embedding. The purpose of this Bi-LSTM is to capture the context of words in a sentence and maintain the order of words toward to extract sentence-level representation. The top of the model is a softmax function to predict sentiment label. We describe in detail the kinds of CNN and LSTM that we use in next sub-part 1 and 2.",
      "The top of the model is a softmax function to predict sentiment label. We describe in detail the kinds of CNN and LSTM that we use in next sub-part 1 and 2. 1) Convolutional Neural Network: The one-dimensional convolution called time-delay neural net has a \ufb01lter vector m and take the dot product of \ufb01lter m with each m-grams in the sequence of characters si \u2208R of a word in order to obtain a sequence c: cj = mT sj\u2212m+1:j (1) Based on Equation 1, we have two types of convolutions that depend on the range of the index j. The narrow type of convolution requires that s \u2265m and produce a sequence c \u2208Rs\u2212m+1. The wide type of convolution does not require on s or m and produce a sequence c \u2208Rs+m\u22121. Out-of- range input values si where i < 1 or i > s are taken to be zero. We use wide convolution for our model.",
      "The wide type of convolution does not require on s or m and produce a sequence c \u2208Rs+m\u22121. Out-of- range input values si where i < 1 or i > s are taken to be zero. We use wide convolution for our model. Wide Convolution: Given a word wi composed of M characters {c1, c2, ..., cM}, we take a character embedding ui \u2208Rd for each character ci and construct a character matrix W char \u2208Rd\u00d7|V chr| as following Equation. 2: W char = \uf8ee \uf8f0 | | | u1 ... uM | | | \uf8f9 \uf8fb (2) The values of the embeddings ui are parameters that are optimized during training. The trained weights in the \ufb01lter m correspond to a feature detector which learns to recognize a speci\ufb01c class of n-grams. The n-grams have size n \u2265m. The use of a wide convolution has some advantages more than a narrow convolution because a wide convolution ensures that all weights of \ufb01lter reach the whole characters of a word at the margins. The resulting matrix has dimension d \u00d7 (s + m \u22121).",
      "The use of a wide convolution has some advantages more than a narrow convolution because a wide convolution ensures that all weights of \ufb01lter reach the whole characters of a word at the margins. The resulting matrix has dimension d \u00d7 (s + m \u22121). 2) Long Short-Term Memory: Long Short-Term Memory networks usually called LSTMs are a improved version of RNN. The core idea behind LSTMs is the cell state which can maintain its state over time, and non-linear gating units which regulate the information \ufb02ow into and out of the cell. The LSTM architecture that we used in our proposed model is described in [10]. A single LSTM memory cell is implemented by the following composite function: it = \u03c3(Wxixt + Whiht\u22121 + Wcict\u22121 + bi) (3) ft = \u03c3(Wxfxt + Whfht\u22121 + Wcfct\u22121 + bf) (4) ct = ftct\u22121 + ittanh(Wxcxt + Whcht\u22121 + bc) (5) ot = \u03c3(Wxoxt + Whoht\u22121 + Wcoct + bo) (6)",
      "ht = ottanh(ct) (7) where \u03c3 is the logistic sigmoid function, i, f, o and c are the input gate, forget gate, output gate, cell and cell input activation vectors respectively. All of them have a same size as the hidden vector h. Whi is the hidden-input gate matrix, Wxo is the input-output gate matrix. The bias terms which are added to i, f, c and o have been omitted for clarity. In addition, we also use the full gradient for calculating with full backpropagation through time (BPTT) described in [11]. A LSTM gradients using \ufb01nite differences could be checked and making practical implementations more reliable. G. Regularization For regularization, we use a constraint on l2 \u2212norms of the weight vectors [19]. III. RESULTS AND ANALYSIS A. Experimental setups For the Stanford Twitter Sentiment Corpus, we use the number of samples as [6]. The training data is selected 80K tweets for a training data and 16K tweets for the development set randomly from the training data of [1]. We conduct a binary prediction for STS Corpus.",
      "The training data is selected 80K tweets for a training data and 16K tweets for the development set randomly from the training data of [1]. We conduct a binary prediction for STS Corpus. For Sander dataset, we use standard 10-fold cross valida- tion as [15]. We construct the development set by selecting 10% randomly from 9-fold training data. In Health Care Reform Corpus, we also select 10% randomly for the development set in a training set and construct as [15] for comparison. We describe the summary of datasets in Table III. Table III SUMMARY STATISTICS FOR THE DATASETS AFTER USING SEMANTIC RULES. c: THE NUMBER OF CLASSES. N: THE NUMBER OF TWEETS. lw: MAXIMUM SENTENCE LENGTH. lc: MAXIMUM CHARACTER LENGTH. |Vw|: WORD ALPHABET SIZE. |Vc|: CHARACTER ALPHABET SIZE.",
      "c: THE NUMBER OF CLASSES. N: THE NUMBER OF TWEETS. lw: MAXIMUM SENTENCE LENGTH. lc: MAXIMUM CHARACTER LENGTH. |Vw|: WORD ALPHABET SIZE. |Vc|: CHARACTER ALPHABET SIZE. Data Set N c lw lc |Vw| |Vc| Test STS Train 80K 2 33 110 67083 134 - Dev 16K 28 48 Test 359 21 16 Sanders Train 991 2 31 33 3379 84 CV Dev 110 27 47 Test 122 28 21 HCR Train 621 2 25 70 3100 60 - Dev 636 26 16 Test 665 20 16 1) Hyperparameters: for all datasets, the \ufb01lter window size (h) is 7 with 6 feature maps each for the \ufb01rst wide convolution layer, the second wide convolution layer has a \ufb01lter window size of 5 with 14 feature maps each.",
      "Dropout rate (p) is 0.5, l2 constraint, learning rate is 0.1 and momentum of 0.9. Mini-batch size for STS Corpus is 100 and others are 4. In addition, training is done through stochastic gradient descent over shuf\ufb02ed mini-batches with Adadelta update rule [20]. 2) Pre-trained Word Vectors: we use the publicly avail- able Word2Vec trained from 100 billion words from Google and TwitterGlove1 of Stanford is performed on aggregated global word-word co-occurrence statistics from a corpus. Word2Vec has dimensionality of 300 and Twitter Glove have dimensionality of 200. Words that do not present in the set of pre-train words are initialized randomly. Table IV ACCURACY OF DIFFERENT MODELS FOR BINARY CLASSIFICATION Model STS Sanders HCR CharSCNN/ pre-training [6] 86.4 - - CharSCNN/ random [6] 81.9 - - SCNN/ pre-training [6] 85.2 - - SCNN/ random [6] 82.2 - - MaxEnt [1] 83.",
      "4 - - CharSCNN/ random [6] 81.9 - - SCNN/ pre-training [6] 85.2 - - SCNN/ random [6] 82.2 - - MaxEnt [1] 83.0 - - NB [1] 82.7 - - SVM [1] 82.2 - - SVM-BoW - 82.43 73.99 SVM-BoW + lex - 83.98 75.94 RF-BoW - 79.24 70.83 RF-BoW + lex - 82.35 72.93 LR-BoW - 77.45 73.83 LR-BoW + lex - 79.49 74.73 MNB-BoW - 79.82 72.48 MNB-BoW + lex - 83.41 75.33 ENS (RF + MNB + LR) - BoW - - 75.19 ENS (SVM + RF + MNB + LR) - BoW + lex - - 76.99 ENS (SVM + RF + MNB + LR) - BoW - 82.",
      "33 ENS (RF + MNB + LR) - BoW - - 75.19 ENS (SVM + RF + MNB + LR) - BoW + lex - - 76.99 ENS (SVM + RF + MNB + LR) - BoW - 82.76 - ENS (SVM + RF + MNB) - BoW + lex - 84.89 - DeepCNN + SR + Glove 85.23 62.38 76.84 Bi-LSTM + SR + Glove 85.79 84.32 78.49 (DeepCNN + Bi-LSTM) + SR + Glove 86.63 85.14 79.55 (DeepCNN + Bi-LSTM) + SR + GoogleW2V 86.35 85.05 80.9 (DeepCNN + Bi-LSTM) + GoogleW2V 86.07 84.23 80.75 B. Experimental results Table IV shows the result of our model for sentiment classi\ufb01cation against other models. We compare our model performance with the approaches of [1][6] on STS Corpus.",
      "07 84.23 80.75 B. Experimental results Table IV shows the result of our model for sentiment classi\ufb01cation against other models. We compare our model performance with the approaches of [1][6] on STS Corpus. [1] reported the results of Maximum Entropy (MaxEnt), NB, SVM on STS Corpus having good performance in previous time. The model of [6] is a state-of-the-art so far by using a CharSCNN. As can be seen, 86.63 is the best prediction accuracy of our model so far for the STS Corpus. For Sanders and HCR datasets, we compare results with the model of [15] that used a ensemble of multiple base classi\ufb01ers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). The ENS model is combined with bag-of-words (BoW), feature hashing (FH) and lexi- cons. The model of [15] is a state-of-the-art on Sanders and HCR datasets. Our models outperform the model of [15] for the Sanders dataset and HCR dataset.",
      "The model of [15] is a state-of-the-art on Sanders and HCR datasets. Our models outperform the model of [15] for the Sanders dataset and HCR dataset. C. Analysis As can be seen, the models with SR outperforms the model with no SR. Semantic rules is effective in order to 1https://nlp.stanford.edu/projects/glove/",
      "increase classi\ufb01cation accuracy. We evaluate the ef\ufb01ciency of SR for the model in Table V of our full paper 2. We also conduct two experiments on two separate models: DeepCNN and Bi-LSTM in order to show the effectiveness of combi- nation of DeepCNN and Bi-LSTM. In addition, the model using TwitterGlove outperform the model using GoogleW2V because TwitterGlove captures more information in Twitter than GoogleW2V. These results show that the character-level information and SR have a great impact on Twitter Data. The pre-train word vectors are good, universal feature extractors. The difference between our model and other approaches is the ability of our model to capture important features by using SR and combine these features at high bene\ufb01t. The use of DeepCNN can learn a representation of words in higher abstract level. The combination of global character \ufb01xed-sized feature vectors and a word embedding helps the model to \ufb01nd important detectors for particles such as \u2019not\u2019 that negate sentiment and potentiate sentiment such as \u2019too\u2019, \u2019so\u2019 standing beside expected features.",
      "The combination of global character \ufb01xed-sized feature vectors and a word embedding helps the model to \ufb01nd important detectors for particles such as \u2019not\u2019 that negate sentiment and potentiate sentiment such as \u2019too\u2019, \u2019so\u2019 standing beside expected features. The model not only learns to recognize single n-grams, but also patterns in n- grams lead to form a structure signi\ufb01cance of a sentence. IV. CONCLUSIONS In the present work, we have pointed out that the use of character embeddings through a DeepCNN to enhance information for word embeddings built on top of Word2Vec or TwitterGlove improves classi\ufb01cation accuracy in Tweet sentiment classi\ufb01cation. Our results add to the well-establish evidence that character vectors are an important ingredient for word-level in deep learning for NLP. In addition, se- mantic rules contribute handling non-essential sub-tweets in order to improve classi\ufb01cation accuracy. REFERENCES [1] A. Go, R. Bhayani, and L. Huang, \u201cTwitter sentiment clas- si\ufb01cation using distant supervision,\u201d CS224N Project Report, Stanford, vol. 1, no. 12, 2009.",
      "REFERENCES [1] A. Go, R. Bhayani, and L. Huang, \u201cTwitter sentiment clas- si\ufb01cation using distant supervision,\u201d CS224N Project Report, Stanford, vol. 1, no. 12, 2009. [2] P. Nakov, A. Ritter, S. Rosenthal, F. Sebastiani, and V. Stoy- anov, \u201cSemEval-2016 task 4: Sentiment analysis in twitter,\u201d Proceedings of SemEval, pp. 1\u201318, 2016. [3] J. Silva, L. Coheur, A. C. Mendes, and A. Wichert, \u201cFrom symbolic to sub-symbolic information in question classi\ufb01ca- tion,\u201d Arti\ufb01cial Intelligence Review, vol. 35, no. 2, pp. 137\u2013 154, 2011. [4] S. Wang and C. D. Manning, \u201cBaselines and bigrams: Simple, good sentiment and topic classi\ufb01cation,\u201d in Proceedings of the 50th Annual Meeting of the Association for Computa- tional Linguistics: Short Papers-Volume 2.",
      "[4] S. Wang and C. D. Manning, \u201cBaselines and bigrams: Simple, good sentiment and topic classi\ufb01cation,\u201d in Proceedings of the 50th Annual Meeting of the Association for Computa- tional Linguistics: Short Papers-Volume 2. Association for Computational Linguistics, 2012, pp. 90\u201394. [5] A. Muhammad, N. Wiratunga, and R. Lothian, \u201cContextual sentiment analysis for social media genres,\u201d Knowledge- Based Systems, vol. 108, pp. 92\u2013101, 2016. 2https://github.com/huynt-plus/papers/blob/master/deepnn-full-paper.pdf [6] C. N. Dos Santos and M. Gatti, \u201cDeep convolutional neural networks for sentiment analysis of short texts.\u201d in COLING, 2014, pp. 69\u201378. [7] Y. Kim, \u201cConvolutional neural networks for sentence classi\ufb01cation,\u201d CoRR, vol. abs/1408.5882, 2014. [Online].",
      "69\u201378. [7] Y. Kim, \u201cConvolutional neural networks for sentence classi\ufb01cation,\u201d CoRR, vol. abs/1408.5882, 2014. [Online]. Available: http://arxiv.org/abs/1408.5882 [8] X. Zhang and Y. LeCun, \u201cText understanding from scratch,\u201d CoRR, vol. abs/1502.01710, 2015. [Online]. Available: http://arxiv.org/abs/1502.01710 [9] F. A. Gers and J. Schmidhuber, \u201cRecurrent nets that time and count,\u201d in Neural Networks, 2000. IJCNN 2000, Proceedings of the IEEE-INNS-ENNS International Joint Conference on, vol. 3. IEEE, 2000, pp. 189\u2013194. [10] A. Graves, \u201cGenerating sequences with recurrent neural net- works,\u201d arXiv preprint arXiv:1308.0850, 2013.",
      "3. IEEE, 2000, pp. 189\u2013194. [10] A. Graves, \u201cGenerating sequences with recurrent neural net- works,\u201d arXiv preprint arXiv:1308.0850, 2013. [11] A. Graves and J. Schmidhuber, \u201cFramewise phoneme classi- \ufb01cation with bidirectional LSTM and other neural network architectures,\u201d Neural Networks, vol. 18, no. 5, pp. 602\u2013610, 2005. [12] O. Appel, F. Chiclana, J. Carter, and H. Fujita, \u201cA hybrid approach to the sentiment analysis problem at the sentence level,\u201d Knowledge-Based Systems, vol. 108, pp. 110\u2013124, 2016. [13] Y. LeCun, Y. Bengio, and G. Hinton, \u201cDeep learning,\u201d Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.",
      "108, pp. 110\u2013124, 2016. [13] Y. LeCun, Y. Bengio, and G. Hinton, \u201cDeep learning,\u201d Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015. [14] F. Bravo-Marquez, M. Mendoza, and B. Poblete, \u201cCombining strengths, emotions and polarities for boosting twitter senti- ment analysis,\u201d in Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining. ACM, 2013, p. 2. [15] N. F. F. Da Silva, E. R. Hruschka, and E. R. Hruschka, \u201cTweet sentiment analysis with classi\ufb01er ensembles,\u201d Decision Sup- port Systems, vol. 66, pp. 170\u2013179, 2014.",
      "66, pp. 170\u2013179, 2014. [16] M. Speriosu, N. Sudan, S. Upadhyay, and J. Baldridge, \u201cTwit- ter polarity classi\ufb01cation with label propagation over lexical links and the follower graph,\u201d in Proceedings of the First workshop on Unsupervised Learning in NLP. Association for Computational Linguistics, 2011, pp. 53\u201363. [17] Y. Xie, Z. Chen, K. Zhang, Y. Cheng, D. K. Honbo, A. Agrawal, and A. N. Choudhary, \u201cMuSES: Multilingual sentiment elicitation system for social media data,\u201d IEEE Intelligent Systems, vol. 29, no. 4, pp. 34\u201342, 2014. [18] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, \u201cDistributed representations of words and phrases and their compositionality,\u201d in Advances in Neural Information Pro- cessing Systems 26. Curran Associates, Inc., 2013, pp.",
      "Curran Associates, Inc., 2013, pp. 3111\u2013 3119. [19] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, \u201cImproving neural networks by preventing co-adaptation of feature detectors,\u201d arXiv preprint arXiv:1207.0580, 2012. [20] M. D. Zeiler, \u201cADADELTA: an adaptive learning rate method,\u201d arXiv preprint arXiv:1212.5701, 2012."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1706.08032.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":6892,
  "avg_doclen":181.3684210526,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1706.08032.pdf"
    }
  }
}