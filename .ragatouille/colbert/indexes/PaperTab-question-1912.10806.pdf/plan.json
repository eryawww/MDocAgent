{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "DP-LSTM: Differential Privacy-inspired LSTM for Stock Prediction Using Financial News Xinyi Li1, Yinchuan Li2,1, Hongyang Yang1, Liuqing Yang1, Xiao-Yang Liu1 1Columbia University, 2Beijing Institute of Technology {xl2717, yl3923, hy2500, ly2335, xl2427}@columbia.edu Abstract Stock price prediction is important for value investments in the stock market. In particular, short-term prediction that exploits \ufb01nancial news articles is promising in recent years. In this paper, we propose a novel deep neural network DP-LSTM for stock price prediction, which incorporates the news articles as hidden information and integrates difference news sources through the differential privacy mechanism. First, based on the autoregressive moving average model (ARMA), a sentiment- ARMA is formulated by taking into consideration the information of \ufb01nancial news articles in the model. Then, an LSTM-based deep neural network is designed, which consists of three components: LSTM, VADER model and differential privacy (DP) mechanism. The proposed DP-LSTM scheme can reduce prediction errors and increase the robustness.",
            "Then, an LSTM-based deep neural network is designed, which consists of three components: LSTM, VADER model and differential privacy (DP) mechanism. The proposed DP-LSTM scheme can reduce prediction errors and increase the robustness. Extensive experiments on S&P 500 stocks show that (i) the proposed DP-LSTM achieves 0.32% improvement in mean MPA of prediction result, and (ii) for the prediction of the market index S&P 500, we achieve up to 65.79% improvement in MSE. 1 Introduction Stock prediction is crucial for quantitative analysts and investment companies. Stocks\u2019 trends, however, are affected by a lot of factors such as interest rates, in\ufb02ation rates and \ufb01nancial news [12]. To predict stock prices accurately, one must use these variable information. In particular, in the banking industry and \ufb01nancial services, analysts\u2019 armies are dedicated to pouring over, analyzing, and attempting to quantify qualitative data from news. A large amount of stock trend information is extracted from the large amount of text and quantitative information that is involved in the analysis.",
            "In particular, in the banking industry and \ufb01nancial services, analysts\u2019 armies are dedicated to pouring over, analyzing, and attempting to quantify qualitative data from news. A large amount of stock trend information is extracted from the large amount of text and quantitative information that is involved in the analysis. Investors may judge on the basis of technical analysis, such as charts of a company, market indices, and on textual information such as news blogs or newspapers. It is however dif\ufb01cult for investors to analyze and predict market trends according to all of these information [22]. A lot of arti\ufb01cial intelligence approaches have been investigated to automatically predict those trends [3]. For instance, investment simulation analysis with arti\ufb01cial markets or stock trend analysis with lexical cohesion based metric of \ufb01nancial news\u2019 sentiment polarity. Quantitative analysis today is heavily dependent on data. However, the majority of such data is unstructured text that comes from sources like \ufb01nancial news articles. The challenge is not only the amount of data that are involved, but also the kind of language that is used in them to express sentiments, which means emoticons.",
            "However, the majority of such data is unstructured text that comes from sources like \ufb01nancial news articles. The challenge is not only the amount of data that are involved, but also the kind of language that is used in them to express sentiments, which means emoticons. Sifting through huge volumes of this text data is dif\ufb01cult as well as time-consuming. It also requires a great deal of resources and expertise to analyze all of that [4]. To solve the above problem, in this paper we use sentiment analysis to extract information from textual information. Sentiment analysis is the automated process of understanding an opinion about a given subject from news articles [5]. The analyzed data quanti\ufb01es reactions or sentiments of the general public toward people, ideas or certain products and reveal the information\u2019s contextual 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada. arXiv:1912.10806v1  [q-fin.ST]  20 Dec 2019",
            "polarity. Sentiment analysis allows us to understand if newspapers are talking positively or negatively about the \ufb01nancial market, get key insights about the stock\u2019s future trend market. We use valence aware dictionary and sentiment reasoner (VADER) to extract sentiment scores. VADER is a lexicon and rule-based sentiment analysis tool attuned to sentiments that are expressed in social media speci\ufb01cally [6]. VADER has been found to be quite successful when dealing with NY Times editorials and social media texts. This is because VADER not only tells about the negativity score and positively but also tells us about how positive or negative a sentiment is. However, news reports are not all objective. We may increase bias because of some non-objective reports, if we rely on the information that is extracted from the news for prediction fully. Therefore, in order to enhance the prediction model\u2019s robustness, we will adopt differential privacy (DP) method. DP is a system for sharing information about a dataset publicly by describing groups\u2019 patterns within the dataset while withholding information about individuals in the dataset. DP can be achieved if the we are willing to add random noise to the result.",
            "DP is a system for sharing information about a dataset publicly by describing groups\u2019 patterns within the dataset while withholding information about individuals in the dataset. DP can be achieved if the we are willing to add random noise to the result. For example, rather than simply reporting the sum, we can inject noise from a Laplace or gaussian distribution, producing a result that\u2019s not quite exact, that masks the contents of any given row. In the last several years a promising approach to private data analysis has emerged, based on DP, which ensures that an analysis outcome is \"roughly as likely\" to occur independent of whether any individual opts in to, or to opts out of, the database. In consequence, any one individual\u2019s speci\ufb01c data can never greatly affect the results. General techniques for ensuring DP have now been proposed, and a lot of datamining tasks can be carried out in a DP method, frequently with very accurate results [21]. We proposed a DP-LSTM neural network, which increase the accuracy of prediction and robustness of model at the same time. The remainder of the paper is organized as follows. In Section 2, we introduce stock price model, the sentiment analysis and differential privacy method.",
            "We proposed a DP-LSTM neural network, which increase the accuracy of prediction and robustness of model at the same time. The remainder of the paper is organized as follows. In Section 2, we introduce stock price model, the sentiment analysis and differential privacy method. In Section 3, we develop the different privacy- inspired LSTM (DP-LSTM) deep neural network and present the training details. Prediction results are provided in Section 4. Section 5 concludes the paper. 2 Problem Statement In this section, we \ufb01rst introduce the background of the stock price model, which is based on the autoregressive moving average (ARMA) model. Then, we present the sentiment analysis details of the \ufb01nancial news and introduce how to use them to improve prediction performance. At last, we introduce the differential privacy framework and the loss function. 2.1 ARMA Model The ARMA model, which is one of the most widely used linear models in time series prediction [17], where the future value is assumed as a linear combination of the past errors and past values. ARMA is used to set the stock midterm prediction problem up.",
            "2.1 ARMA Model The ARMA model, which is one of the most widely used linear models in time series prediction [17], where the future value is assumed as a linear combination of the past errors and past values. ARMA is used to set the stock midterm prediction problem up. Let XA t be the variable based on ARMA at time t, then we have XA t = f1({Xt\u2212i}p i=1) = \u00b5 + p X i=1 \u03c6iXt\u2212i \u2212 q X i=1 \u03c8j\u03f5t\u2212j + \u03f5t, (1) where Xt\u2212i denotes the past value at time t \u2212i; \u03f5t denotes the random error at time t; \u03c6i and \u03c8j are the coef\ufb01cients; \u00b5 is a constant; p and q are integers that are often referred to as autoregressive and moving average polynomials, respectively. 2.2 Sentiment Analysis Another variable highly related to stock price is the textual information from news, whose changes may be a precursor to price changes. In our paper, news refers to a news article\u2019s title on a given trading day.",
            "2.2 Sentiment Analysis Another variable highly related to stock price is the textual information from news, whose changes may be a precursor to price changes. In our paper, news refers to a news article\u2019s title on a given trading day. It has been used to infer whether an event had informational content and whether investors\u2019 interpretations of the information were positive, negative or neutral. We hence use sentiment analysis to identify and extract opinions within a given text. Sentiment analysis aims at gauging the attitude, sentiments, evaluations and emotions of a speaker or writer based on subjectivity\u2019s computational treatment in a text [19]-[20]. 2",
            "Figure 1: NLTK processing. For preprocessing, each news title will be tokenized into individual words. Then applying SentimentIntensityAnalyzer from NLTK vadar to calculate the polarity score. Figure 2: Positive wordcloud (left) and negative wordcloud (right). We divide the news based on their compound score. For both positive news and negative news, we count all the words and rank them to create the wordcloud. The larger the word, more frequently it has appeared in the source. Figure 1 shows an example of the sentiment analysis results obtained from \ufb01nancial news titles that were based on VADER. VADER uses a combination of a sentiment lexicon which are generally labelled according to their semantic orientation as either negative or positive. VADER has been found to be quite successful when dealing with news reviews. It is fully open-sourced under the MIT License. The result of VADER represent as sentiment scores, which include the positive, negative and neutral scores represent the proportion of text that falls in these categories. This means all these three scores should add up to 1.",
            "It is fully open-sourced under the MIT License. The result of VADER represent as sentiment scores, which include the positive, negative and neutral scores represent the proportion of text that falls in these categories. This means all these three scores should add up to 1. Besides, the Compound score is a metric that calculates the sum of all the lexicon ratings which have been normalized between -1(most extreme negative) and +1 (most extreme positive). Figure 2 shows the positive and negative wordcloud, which is an intuitive analysis of the number of words in the news titles. 2.3 Sentiment-ARMA Model and Loss Function To take the sentiment analysis results of the \ufb01nancial news into account, we introduce the sentiment- ARMA model as follows \u02c6Xt = \u03b1XA t + \u03bbSA t + c = \u03b1XA t + \u03bbf2({St\u2212i}p i=1 | {z } Sentiment ) + c, (2) where \u03b1 and \u03bb are weighting factors; c is a constant; and f2(\u00b7) is similar to f1(\u00b7) in the ARMA model (1) and is used to describe the prediction problem.",
            "In this paper, the LSTM neural network is used to predict the stock price, the input data is the previous stock price and the sentiment analysis results. Hence, the sentiment based LSTM neural network 3",
            "Figure 3: LSTM procedure (named sentiment-LSTM) is aimed to minimize the following loss function: L = min p+T X t=p+1    Xt \u2212\u02c6Xt    2 2 , (3) where T denotes the number of prediction time slots, i.e., t = 1, ..., p are the observations (training input data), t = p + 1, ..., p + T are the predicts (training output data); and \u02c6Xt is given in (2). 2.4 Overview of LSTM Denote X train t = {Xt\u2212i, St\u2212i}p i=1 as the training input data. Figure 3 shows the LSTM\u2019s structure network, which comprises one or more hidden layers, an output layer and an input layer [16]. LSTM networks\u2019 main advantage is that the hidden layer comprises memory cells.",
            "Figure 3 shows the LSTM\u2019s structure network, which comprises one or more hidden layers, an output layer and an input layer [16]. LSTM networks\u2019 main advantage is that the hidden layer comprises memory cells. Each memory cell recurrently has a core self-connected linear unit called \u201c Constant Error Carousel (CEC)\u201d [13], which provides short-term memory storage and has three gates: \u2022 Input gate, which controls the information from a new input to the memory cell, is given by it = \u03c3(Wi \u00d7 [ht\u22121, X train t ] + bi), (4) \u02c6ct = tanh(Wc \u00d7 [ht\u22121, X train t ] + bc), (5) where ht\u22121 is the hidden state at the time step t \u22121; it is the output of the input gate layer at the time step t; \u02c6ct is the candidate value to be added to the output at the time step t; bi and bc are biases of the input gate layer and the candidate value computation, respectively; Wi and Wc are weights of the input gate and the candidate value computation, respectively; and \u03c3(x) = 1\/(1 + e\u2212x) is the pointwise nonlinear activation function.",
            "\u2022 Forget gate, which controls the limit up to which a value is saved in the memory, is given by ft = \u03c3(Wf \u00d7 [ht\u22121, X train t ] + bf), (6) where ft is the forget state at the time step t, Wf is the weight of the forget gate; and bf is the bias of the forget gate. \u2022 Output gate, which controls the information output from the memory cell, is given by ct = ft \u00d7 ct\u22121 + it \u00d7 \u02c6ct, (7) ot = \u03c3(Wo \u00d7 [ht\u22121, X train t ] + bo), (8) ht = ot \u00d7 tanh(ct), (9) where new cell states ct are calculated based on the results of the previous two steps; ot is the output at the time step t; Wo is the weight of the output gate; and bo is the bias of the output gate [14]. 2.5 De\ufb01nition of Differential Privacy Differential privacy is one of privacy\u2019s most popular de\ufb01nitions today, which is a system for publicly sharing information about a dataset by describing the patterns of groups within the dataset while withholding information about individuals in the dataset.",
            "2.5 De\ufb01nition of Differential Privacy Differential privacy is one of privacy\u2019s most popular de\ufb01nitions today, which is a system for publicly sharing information about a dataset by describing the patterns of groups within the dataset while withholding information about individuals in the dataset. It intuitively requires that the mechanism that outputs information about an underlying dataset is robust to one sample\u2019s any change, thus 4",
            "protecting privacy. A mechanism f is a random function that takes a dataset N as input, and outputs a random variable f(N). For example, suppose N is a news articles dataset, then the function that outputs compound score of articles in N plus noise from the standard normal distribution is a mechanism [7]. Although differential privacy was originally developed to facilitate secure analysis over sensitive data, it can also enhance the robustness of the data. Note that \ufb01nance data, especially news data and stock data, is unstable with a lot of noise, with a more robust data the accuracy of prediction will be improved. Since we predict stock price by fusing news come from different sources, which might include fake news. Involving differential privacy in the training to improve the robustness of the \ufb01nance news is meaningful. 3 Training DP-LSTM Neural Network It is known that it is risky to predict stocks by considering news factors, because news can\u2019t guarantee full notarization and objectivity, many times extreme news will have a big impact on prediction models. To solve this problem, we consider entering the idea of the differential privacy when training.",
            "To solve this problem, we consider entering the idea of the differential privacy when training. In this section, our DP-LSTM deep neural network training strategy is presented. The input data consists of three components: stock price, sentiment analysis compound score and noise. 3.1 Data Preprocessing and Normalization 3.1.1 Data Preprocessing The data for this project are two parts, the \ufb01rst part is the historical S&P 500 component stocks, which are downloaded from the Yahoo Finance. We use the data over the period of from 12\/07\/2017 to 06\/01\/2018. The second part is the news article from \ufb01nancial domain are collected with the same time period as stock data. Since our paper illustrates the relationship between the sentiment of the news articles and stocks\u2019 price. Hence, only news article from \ufb01nancial domain are collected. The data is mainly taken from Webhose archived data, which consists of 306242 news articles present in JSON format, dating from December 2017 up to end of June 2018.",
            "Hence, only news article from \ufb01nancial domain are collected. The data is mainly taken from Webhose archived data, which consists of 306242 news articles present in JSON format, dating from December 2017 up to end of June 2018. The former 85% of the dataset is used as the training data and the remainder 15% is used as the testing data. The News publishers for this data are CNBC.com, Reuters.com, WSJ.com, Fortune.com. The Wall Street Journal is one of the largest newspapers in the United States, which coverage of breaking news and current headlines from the US and around the world include top stories, photos, videos, detailed analysis and in-depth thoughts; CNBC primarily carries business day coverage of U.S. and international \ufb01nancial markets, which following the end of the business day and on non-trading days; Fortune is an American multinational business magazine; Reuters is an international news organization. We preprocess the raw article body and use NLTK sentiment package alence Aware Dictionary and Sentiment Reasoner (VADER) to extract sentiment scores.",
            "We preprocess the raw article body and use NLTK sentiment package alence Aware Dictionary and Sentiment Reasoner (VADER) to extract sentiment scores. The stocks with missing data are deleted, and the dataset we used eventually contains 451 stocks and 4 news resources (CNBC.com, Reuters.com, WSJ.comFortune.com.). Each stock records the adjust close price and news compound scores of 121 trading days. Training input: 92 days Training output: 92 days 12\/07\/2017 04\/20\/2018 05\/04\/2018 05\/18\/2018 12\/08\/2017 04\/23\/2018 05\/07\/2018 05\/21\/2018 Testing output: 9 days Testing input: 9 days Figure 4: Schematic diagram of rolling window. A rolling window with size 10 is used to separate data, that is, We predict the stock price of the next trading day based on historical data from the previous 10 days, hence resulting in a point-by-point prediction [15]. In particular, the training window is initialized with all real training data.",
            "A rolling window with size 10 is used to separate data, that is, We predict the stock price of the next trading day based on historical data from the previous 10 days, hence resulting in a point-by-point prediction [15]. In particular, the training window is initialized with all real training data. Then we shift the window and add the next real point to the last point of training window to predict the next point and so forth. Then, according to the length of the window, the training data is divided into 92 sets of training input data (each set length 10) and training output data (each set length 1). The testing data is divided into input and output data of 9 windows (see Figure 4). 5",
            "3.1.2 Normalization To detect stock price pattern, it is necessary to normalize the stock price data. Since the LSTM neural network requires the stock patterns during training, we use \u201cmin-max\u201d normalization method to reform dataset, which keeps the pattern of the data [11], as follow: Xn t = Xt \u2212min(Xt) max(Xt) \u2212min(Xt), (10) where Xn t denotes the data after normalization. Accordingly, de-normalization is required at the end of the prediction process to get the original price, which is given by \u02c6Xt = \u02c6Xn t [max(Xt) \u2212min(Xt)] + min(Xt), (11) where \u02c6Xn t denotes the predicted data and \u02c6Xt denotes the predicted data after de-normalization. Note that compound score is not normalized, since the compound score range from -1 to 1, which means all the compound score data has the same scale, so it is not require the normalization processing. 3.2 Adding Noise We consider the differential privacy as a method to improve the robustness of the LSTM predictions [8].",
            "3.2 Adding Noise We consider the differential privacy as a method to improve the robustness of the LSTM predictions [8]. We explore the interplay between machine learning and differential privacy, and found that differential privacy has several properties that make it particularly useful in application such as robustness to extract textual information [9]. The robustness of textual information means that accuracy is guaranteed to be unaffected by certain false information [10]. The input data of the model has 5 dimensions, which are the stock price and four compound scores as (Xt, St 1, St 2, St 3, St 4), t = 1, ..., T, where Xt represents the stock price and St i, i = 1, ..., 4 respectively denote the mean compound score calculated from WSJ, CNBC, Fortune and Reuters.",
            "According to the process of differential privacy, we add Gaussian noise with different variances to the news according to the variance of the news, i.e., the news compound score after adding noise is given by eSt i = St i + N(0, \u03bbvar(Si)), i = 1, ..., 4, (12) where var(\u00b7) is the variance operator, \u03bb is a weighting factor and N(\u00b7) denotes the random Gaussian process with zero mean and variance \u03bbvar(Si). We used python to crawl the news from the four sources of each trading day, perform sentiment analysis on the title of the news, and get the compound score.",
            "We used python to crawl the news from the four sources of each trading day, perform sentiment analysis on the title of the news, and get the compound score. After splitting the data into training sets and test sets, we separately add noise to each of four news sources of the training set, then, for n-th stock, four sets of noise-added data (Xn t , eSt 1, St 2, St 3, St 4), (Xn t , St 1, eSt 2, St 3, St 4), (Xn t , St 1, St 2, eSt 3, St 4), (Xn t , St 1, St 2, St 3, eSt 4) are combined into a new training data through a rolling window. The stock price is then combined with the new compound score training data as input data for our DP-LSTM neural network. 3.3 Training Setting The LSTM model in \ufb01gure 3 has six layers, followed by an LSTM layer, a dropout layer, an LSTM layer, an LSTM layer, a dropout layer, a dense layer, respectively.",
            "3.3 Training Setting The LSTM model in \ufb01gure 3 has six layers, followed by an LSTM layer, a dropout layer, an LSTM layer, an LSTM layer, a dropout layer, a dense layer, respectively. The dropout layers (with dropout rate 0.2) prevent the network from over\ufb01tting. The dense layer is used to reshape the output. Since a network will be dif\ufb01cult to train if it contains a large number of LSTM layers [16], we use three LSTM layers here. In each LSTM layer, the loss function is the mean square error (MSE), which is the sum of the squared distances between our target variable and the predicted value. In addition, the ADAM [17] is used as optimizer, since it is straightforward to implement, computationally ef\ufb01cient and well suited for problems with large data set and parameters. There are many methods and algorithms to implement sentiment analysis systems. In this paper, we use rule-based systems that perform sentiment analysis based on a set of manually crafted rules.",
            "There are many methods and algorithms to implement sentiment analysis systems. In this paper, we use rule-based systems that perform sentiment analysis based on a set of manually crafted rules. Usually, rule-based approaches de\ufb01ne a set of rules in some kind of scripting language that identify subjectivity, polarity, or the subject of an opinion. We use VADER, a simple rule-based model for general sentiment analysis. 6",
            "Figure 5: NLTK result. Method Mean MPA LSTM without news 0.978305309 LSTM with news 0.978366682 DP-LSTM 0.981582666 Table 1: Predicted Mean MPA results. 4 Performance Evaluation In this section, we validate our DP-LSTM based on the S&P 500 stocks. We calculate the mean prediction accuracy (MPA) to evaluate the proposed methods, which is de\ufb01ned as MPAt = 1 \u22121 L L X \u2113=1 |Xt,\u2113\u2212\u02c6Xt,\u2113| Xt,\u2113 , (13) where Xt,\u2113is the real stock price of the \u2113-th stock on the t-th day, L is the number of stocks and \u02c6Xt,\u2113 is the corresponding prediction result. Figure 5 plots the average score for all news on the same day over the period. The compound score is \ufb02uctuating between -0.3 and 0.15, indicating an overall neutral to slightly negative sentiment. The Positive, Negative and Neutral scores represent the proportion of text that falls in these categories.",
            "The compound score is \ufb02uctuating between -0.3 and 0.15, indicating an overall neutral to slightly negative sentiment. The Positive, Negative and Neutral scores represent the proportion of text that falls in these categories. The Compound score is a metric that calculates the sum of all the lexicon ratings which have been normalized between -1 (most extreme negative) and +1 (most extreme positive). Figure 6: Mean prediction accuracies of the DP-LSTM and vanilla LSTM. Figure 6 shows the MPAs of the proposed DP-LSTM and vanilla LSTM for comparison. In Table 1, we give the mean MPA results for the prediction prices, which shows the accuracy performance of DP-LSTM is 0.32% higer than the LSTM with news. The result means the DP framework can make the prediction result more accuracy and robustness. Note that the results are obtained by running many trials, since we train stocks separately and predict each price individually due to the different patterns and scales of stock prices. This in total adds up to 7",
            "Metrics LSTM without news LSTM with news DP-LSTM MSE 580.9226827 536.6306251 198.7500672 Accuracy 0.99263803 0.99292492 0.99582651 Mean error percent 0.00736197 0.00707508 0.00417349 Table 2: S&P 500 predicted results. 451 runs. The results shown in Table 1 is the average of these 451 runs. Furthermore, we provide results for 9 duration over a period in Figure 6. The performance of our DP-LSTM is always better than the LSTM with news. Based on the sentiment-ARMA model and adding noise for training, the proposed DP-LSTM is more robust. The investment risk based on this prediction results is reduced. Figure 7: Prediction result of LSTM based on price. In Figure 7, we can see the prediction results of DP-LSTM with is closer to the real S&P 500 index price line than other methods. The two lines (prediction results of LSTM with news and LSTM without news) almost coincide in Figure 7.",
            "In Figure 7, we can see the prediction results of DP-LSTM with is closer to the real S&P 500 index price line than other methods. The two lines (prediction results of LSTM with news and LSTM without news) almost coincide in Figure 7. We can tell the subtle differences from the Table 2, that DP-LSTM is far ahead, and LSTM with news is slightly better than LSTM without news. 5 Conclusion In this paper, we integrated the deep neural network with the famous NLP models (VADER) to identify and extract opinions within a given text, combining the stock adjust close price and compound score to reduce the investment risk. We \ufb01rst proposed a sentiment-ARMA model to represent the stock price, which incorporates in\ufb02uential variables (price and news) based on the ARMA model. Then, a DP- LSTM deep neural network was proposed to predict stock price according to the sentiment-ARMA model, which combines the LSTM, compound score of news articles and differential privacy method. News are not all objective. If we rely on the information extracted from the news for prediction fully, we may increase bias because of some non-objective reports.",
            "News are not all objective. If we rely on the information extracted from the news for prediction fully, we may increase bias because of some non-objective reports. Therefore, the DP-LSTM enhance robustness of the prediction model. Experiment results based on the S&P 500 stocks show that the proposed DP-LSTM network can predict the stock price accurately with robust performance, especially for S&P 500 index that re\ufb02ects the general trend of the market. S&P 500 prediction results show that the differential privacy method can signi\ufb01cantly improve the robustness and accuracy. References [1] X. Li, Y. Li, X.-Y. Liu, D. Wang, \u201cRisk management via anomaly circumvent: mnemonic deep learning for midterm stock prediction.\u201d in Proceedings of 2nd KDD Workshop on Anomaly Detection in Finance (Anchorage \u201919), 2019.",
            "Liu, D. Wang, \u201cRisk management via anomaly circumvent: mnemonic deep learning for midterm stock prediction.\u201d in Proceedings of 2nd KDD Workshop on Anomaly Detection in Finance (Anchorage \u201919), 2019. [2] P. Chang, C. Fan, and C. Liu, \u201cIntegrating a piece-wise linear representation method and a neural network model for stock trading points prediction.\u201d IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 39, 1 (2009), 80\u201392. [3] Akita, Ryo, et al. \u201cDeep learning for stock prediction using numerical and textual information.\u201d IEEE\/ACIS 15th International Conference on Computer and Information Science (ICIS). IEEE, 2016. 8",
            "[4] Li, Xiaodong, et al. \u201cDoes summarization help stock prediction? A news impact analysis.\u201d IEEE Intelligent Systems 30.3 (2015): 26-34. [5] Ding, Xiao, et al. \u201cDeep learning for event-driven stock prediction.\u201d Twenty-fourth International Joint Conference on Arti\ufb01cial Intelligence. 2015. [6] Hutto, Clayton J., and Eric Gilbert. \u201cVader: A parsimonious rule-based model for sentiment analysis of social media text.\u201d Eighth International AAAI Conference on Weblogs and Social Media, 2014. [7] Ji, Zhanglong, Zachary C. Lipton, and Charles Elkan. \u201cDifferential privacy and machine learning: a survey and review.\u201d arXiv preprint arXiv:1412.7584 (2014). [8] Abadi, Martin, et al. \u201cDeep learning with differential privacy.\u201d Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, ACM, 2016. [9] McMahan, H. Brendan, and Galen Andrew.",
            "[8] Abadi, Martin, et al. \u201cDeep learning with differential privacy.\u201d Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, ACM, 2016. [9] McMahan, H. Brendan, and Galen Andrew. \u201cA general approach to adding differential privacy to iterative training procedures.\u201d arXiv preprint arXiv:1812.06210 (2018). [10] Lecuyer, Mathias, et al. \u201cCerti\ufb01ed robustness to adversarial examples with differential privacy.\u201d arXiv preprint arXiv:1802.03471 (2018). [11] Hafezi, Reza, Jamal Shahrabi, and Esmaeil Hadavandi. \u201cA bat-neural network multi-agent system (BN- NMAS) for stock price prediction: Case study of DAX stock price.\u201d Applied Soft Computing, 29 (2015): 196-210. [12] Chang, Pei-Chann, Chin-Yuan Fan, and Chen-Hao Liu.",
            "[12] Chang, Pei-Chann, Chin-Yuan Fan, and Chen-Hao Liu. \u201cIntegrating a piecewise linear representation method and a neural network model for stock trading points prediction.\u201d IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 39.1 (2008): 80-92. [13] Gers, Felix A., Nicol N. Schraudolph, and J\u00fcrgen Schmidhuber. \u201cLearning precise timing with LSTM recurrent networks.\u201d Journal of Machine Learning Research 3.Aug (2002): 115-143. [14] Qin, Yao, et al. \u201cA dual-stage attention-based recurrent neural network for time series prediction.\u201d arXiv preprint arXiv:1704.02971 (2017). [15] Malhotra, Pankaj, et al. \u201cLong short term memory networks for anomaly detection in time series.\u201d Proceedings. Presses universitaires de Louvain, 2015. [16] Sak, Ha\u00b8sim, Andrew Senior, and Fran\u00e7oise Beaufays.",
            "\u201cLong short term memory networks for anomaly detection in time series.\u201d Proceedings. Presses universitaires de Louvain, 2015. [16] Sak, Ha\u00b8sim, Andrew Senior, and Fran\u00e7oise Beaufays. \u201cLong short-term memory recurrent neural network architectures for large scale acoustic modeling.\u201d Fifteenth annual conference of the international speech communication association, 2014. [17] Kingma, Diederik P., and Jimmy Ba. \u201cAdam: A method for stochastic optimization.\u201d arXiv preprint arXiv:1412.6980 (2014). [18] Box, George EP, et al. Time series analysis: forecasting and control. John Wiley & Sons, 2015. [19] Pang, Bo, and Lillian Lee. \u201cOpinion mining and sentiment analysis.\u201d Foundations and Trends in Information Retrieval 2.1\u20132 (2008): 1-135. [20] Cambria, Erik. \u201cAffective computing and sentiment analysis.\u201d IEEE Intelligent Systems 31.2 (2016): 102-107. [21] Dwork C, Lei J. Differential privacy and robust statistics\/\/STOC.",
            "[20] Cambria, Erik. \u201cAffective computing and sentiment analysis.\u201d IEEE Intelligent Systems 31.2 (2016): 102-107. [21] Dwork C, Lei J. Differential privacy and robust statistics\/\/STOC. 2009, 9: 371-380. [22] X. Li, Y. Li, Y. Zhan, and X.-Y. Liu. \u201cOptimistic bull or pessimistic bear: adaptive deep reinforcement learning for stock portfolio allocation.\u201d in Proceedings of the 36th International Conference on Machine Learning, 2019. 9"
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1912.10806.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 6672.000061035156,
    "avg_doclen_est": 175.57894897460938
}
