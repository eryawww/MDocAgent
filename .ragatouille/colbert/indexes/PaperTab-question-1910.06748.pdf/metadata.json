{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Language Identi\ufb01cation on Massive Datasets of Short Message using an Attention Mechanism CNN Duy Tin Vo * 1 Richard Khoury * 2 Abstract Language Identi\ufb01cation (LID) is a challenging task, especially when the input texts are short and noisy such as posts and statuses on social media or chat logs on gaming forums. The task has been tackled by either designing a feature set for a traditional classi\ufb01er (e.g. Naive Bayes) or applying a deep neural network classi\ufb01er (e.g. Bi-directional Gated Recurrent Unit, Encoder- Decoder). These methods are usually trained and tested on a huge amount of private data, then used and evaluated as off-the-shelf packages by other researchers using their own datasets, and conse- quently the various results published are not di- rectly comparable. In this paper, we \ufb01rst create a new massive labelled dataset based on one year of Twitter data. We use this dataset to test sev- eral existing language identi\ufb01cation systems, in order to obtain a set of coherent benchmarks, and we make our dataset publicly available so that others can add to this set of benchmarks.",
      "We use this dataset to test sev- eral existing language identi\ufb01cation systems, in order to obtain a set of coherent benchmarks, and we make our dataset publicly available so that others can add to this set of benchmarks. Fi- nally, we propose a shallow but ef\ufb01cient neural LID system, which is a ngram-regional convolu- tion neural network enhanced with an attention mechanism. Experimental results show that our architecture is able to predict tens of thousands of samples per second and surpasses all state-of- the-art systems with an improvement of 5%. 1. Introduction Language Identi\ufb01cation (LID) is the Natural Language Processing (NLP) task of automatically recognizing the language that a document is written in. While this task was called \u201dsolved\u201d by some authors over a decade ago, it has seen a resurgence in recent years thanks to the rise in pop- ularity of social media (Jauhiainen et al., 2018; Jaech et al., 2016), and the corresponding daily creation of millions of new messages in dozens of different languages including rare ones that are not often included in language identi- \ufb01cation systems.",
      "Moreover, these messages are typically very short (Twitter messages were until recently limited to 140 characters) and very noisy (including an abundance of spelling mistakes, non-word tokens like URLs, emoticons, or hashtags, as well as foreign-language words in messages of another language), whereas LID was solved using long and clean documents. Indeed, several studies have shown that LID systems trained to a high accuracy on traditional documents suffer signi\ufb01cant drops in accuracy when ap- plied to short social-media texts (Lui & Baldwin, 2012; Carter et al., 2013). Given its massive scale, multilingual nature, and popular- ity, Twitter has naturally attracted the attention of the LID research community. Several attempts have been made to construct LID datasets from that resource. However, a ma- jor challenge is to assign each tweet in the dataset to the correct language among the more than 70 languages used on the platform.",
      "Several attempts have been made to construct LID datasets from that resource. However, a ma- jor challenge is to assign each tweet in the dataset to the correct language among the more than 70 languages used on the platform. The three commonly-used approaches are to rely on human labeling (Lui & Baldwin, 2014; Tromp & Pechenizkiy, 2011), machine detection (Tromp & Pech- enizkiy, 2011; Jurgens et al., 2017), or user geolocation (Carter et al., 2013; Blodgett et al., 2017; Bergsma et al., 2012). Human labeling is an expensive process in terms of workload, and it is thus infeasible to apply it to create a massive dataset and get the full bene\ufb01t of Twitter\u2019s scale. Automated LID labeling of this data creates a noisy and im- perfect dataset, which is to be expected since the purpose of these datasets is to create new and better LID algorithms.",
      "Automated LID labeling of this data creates a noisy and im- perfect dataset, which is to be expected since the purpose of these datasets is to create new and better LID algorithms. And user geolocation is based on the assumption that users in a geographic region use the language of that region; an assumption that is not always correct, which is why this technique is usually paired with one of the other two. Our \ufb01rst contribution in this paper is to propose a new approach to build and automatically label a Twitter LID dataset, and to show that it scales up well by building a dataset of over 18 million labeled tweets. Our hope is that our new Twitter dataset will become a benchmarking standard in the LID literature. Traditional LID models (Lui & Baldwin, 2012; Carter et al., 2013; Gamallo et al., 2014) proposed different ideas to design a set of useful features. This set of features is then passed to traditional machine learning algorithms such as Naive Bayes (NB). The resulting systems are capable of la- beling thousands of inputs per second with moderate accu- racy.",
      "This set of features is then passed to traditional machine learning algorithms such as Naive Bayes (NB). The resulting systems are capable of la- beling thousands of inputs per second with moderate accu- racy. Meanwhile, neural network models (Kocmi & Bojar, arXiv:1910.06748v1  [cs.CL]  15 Oct 2019",
      "Language Identi\ufb01cation on Massive Datasets of Short Message using an Attention Mechanism CNN Table 1. Summary of literature results Paper Input Algorithm Metric Results langid.py Tromp & Pechenizkiy (2011) Character n-grams Graph Accuracy 0.975 0.941 Carter et al. (2013) Social network information Prior probabilities Accuracy 0.972 0.886 Gamallo et al. (2014) Words Dictionary F1-score 0.733 N/A Jaech et al. (2016) Words LSTM F1-score 0.912 0.879 Kocmi & Bojar (2017) Character n-grams GRU Accuracy 0.955 0.912 Jurgens et al. (2017) Character n-grams Encoder-decoder Accuracy 0.982 0.960 2017; Jurgens et al., 2017) approach the problem by design- ing a deep and complex architecture like gated recurrent unit (GRU) or encoder-decoder net. These models use the message text itself as input using a sequence of character embeddings, and automatically learn its hidden structure via a deep neural network.",
      "These models use the message text itself as input using a sequence of character embeddings, and automatically learn its hidden structure via a deep neural network. Consequently, they obtain better results in the task but with an ef\ufb01ciency trade-off. To allevi- ate these drawbacks, our second contribution in this paper is to propose a shallow but ef\ufb01cient neural LID algorithm. We followed previous neural LID (Kocmi & Bojar, 2017; Jurgens et al., 2017) in using character embeddings as in- puts. However, instead of using a deep neural net, we pro- pose to use a shallow ngram-regional convolution neural network (CNN) with an attention mechanism to learn input representation. We experimentally prove that the ngram- regional CNN is the best choice to tackle the bottleneck problem in neural LID. We also illustrate the behaviour of the attention structure in focusing on the most impor- tant features in the text for the task. Compared with other benchmarks on our Twitter datasets, our proposed model consistently achieves new state-of-the-art results with an improvement of 5% in accuracy and F1 score and a com- petitive inference time.",
      "Compared with other benchmarks on our Twitter datasets, our proposed model consistently achieves new state-of-the-art results with an improvement of 5% in accuracy and F1 score and a com- petitive inference time. The rest of this paper is structured as follows. After a back- ground review in the next section, we will present our Twit- ter dataset in Section 3. Our novel LID algorithm will be the topic of Section 4. We will then present and analyze some experiments we conducted with our algorithm in Section 5, along with benchmarking tests of popular and literature LID systems, before drawing some concluding remarks in Section 6. Our Twitter dataset and our LID algorithm\u2019s source code are publicly available1. 2. Related Work In this section, we will consider recent advances on the spe- ci\ufb01c challenge of language identi\ufb01cation in short text mes- sages. Readers interested in a general overview of the area of LID, including older work and other challenges in the area, are encouraged to read the thorough survey of (Jauhi- ainen et al., 2018).",
      "Readers interested in a general overview of the area of LID, including older work and other challenges in the area, are encouraged to read the thorough survey of (Jauhi- ainen et al., 2018). 1https://github.com/duytinvo/LID_NN 2.1. Probabilistic LID One of the \ufb01rst, if not the \ufb01rst, systems for LID special- ized for short text messages is the graph-based method of (Tromp & Pechenizkiy, 2011). Their graph is composed of vertices, or character n-grams (n = 3) observed in messages in all languages, and of edges, or connections between suc- cessive n-grams weighted by the observed frequency of that connection in each language. Identifying the language of a new message is then done by identifying the most probable path in the graph that generates that message. Their method achieves an accuracy of 0.975 on their own Twitter corpus. Carter, Weerkamp, and Tsagkias proposed an approach for LID that exploits the very nature of social media text (Carter et al., 2013).",
      "Their method achieves an accuracy of 0.975 on their own Twitter corpus. Carter, Weerkamp, and Tsagkias proposed an approach for LID that exploits the very nature of social media text (Carter et al., 2013). Their approach computes the prior probability of the message being in a given language in- dependently of the content of the message itself, in \ufb01ve dif- ferent ways: by identifying the language of external content linked to by the message, the language of previous mes- sages by the same user, the language used by other users explicitly mentioned in the message, the language of pre- vious messages in the on-going conversation, and the lan- guage of other messages that share the same hashtags. They achieve a top accuracy of 0.972 when combining these \ufb01ve priors with a linear interpolation. One of the most popular language identi\ufb01cation packages is the langid.py library proposed in (Lui & Baldwin, 2012), thanks to the fact it is an open-source, ready-to-use li- brary written in the Python programming language.",
      "One of the most popular language identi\ufb01cation packages is the langid.py library proposed in (Lui & Baldwin, 2012), thanks to the fact it is an open-source, ready-to-use li- brary written in the Python programming language. It is a multinomial Na\u00a8\u0131ve Bayes classi\ufb01er trained on character n-grams (1 \u2264n \u22644) from 97 different languages. The training data comes from longer document sources, both formal ones (government publications, software documen- tation, and newswire) and informal ones (online encyclope- dia articles and websites). While their system is not special- ized for short messages, the authors claim their algorithm can generalize across domains off-the-shelf, and they con- ducted experiments using the Twitter datasets of (Tromp & Pechenizkiy, 2011) and (Carter et al., 2013) that achieved accuracies of 0.941 and 0.886 respectively, which is weaker than the specialized short-message LID systems of (Tromp & Pechenizkiy, 2011) and (Carter et al., 2013).",
      "Starting from the basic observation of Zipf\u2019s Law, that each language has a small number of words that occur very fre-",
      "Language Identi\ufb01cation on Massive Datasets of Short Message using an Attention Mechanism CNN quently in most documents, the authors of (Gamallo et al., 2014) created a dictionary-based algorithm they called Quelingua. This algorithm includes ranked dictionaries of the 1,000 most popular words of each language it is trained to recognize. Given a new message, recognized words are given a weight based on their rank in each language, and the identi\ufb01ed language is the one with the highest sum of word weights. Quelingua achieves an F1-score of 0.733 on the TweetLID competition corpus (Zubiaga et al., 2014), a narrow improvement over a trigram Na\u00a8\u0131ve Bayes classi\ufb01er which achieves an F1-Score of 0.727 on the same corpus, but below the best results achieved in the competition. 2.2.",
      "2.2. Neural Network LID Neural network models have been applied on many NLP problems in recent years with great success, achieving ex- cellent performance on challenges ranging from text clas- si\ufb01cation (Vo et al., 2018) to sequence labeling (Yang & Zhang, 2018). In LID, the authors of (Jaech et al., 2016) built a hierarchical system of two neural networks. The \ufb01rst level is a Convolutional Neural Network (CNN) that converts white-space-delimited words into a word vector. The second level is a Long-Short-Term Memory (LSTM) network (a type of recurrent neural network (RNN)) that takes in sequences of word vectors outputted by the \ufb01rst level and maps them to language labels. They trained and tested their network on Twitter\u2019s of\ufb01cial Twitter70 dataset2, and achieved an F-score of 0.912, compared to langid.py\u2019s performance of 0.879 on the same dataset.",
      "They trained and tested their network on Twitter\u2019s of\ufb01cial Twitter70 dataset2, and achieved an F-score of 0.912, compared to langid.py\u2019s performance of 0.879 on the same dataset. They also trained and tested their system using the TweetLID corpus and achieved an F1-score of 0.762, above the system of (Gamallo et al., 2014) presented earlier, and above the top system of the TweetLID competition, the SVM LID sys- tem of (Hurtado et al., 2014) which achieved an F1-score of 0.752. The authors of (Kocmi & Bojar, 2017) also used a RNN system, but preferred the Gated Recurrent Unit (GRU) ar- chitecture to the LSTM, indicating it performed slightly better in their experiments. Their system breaks the text into non-overlapping 200-character segments, and feeds character n-grams (n = 8) into the GRU network to clas- sify each letter into a probable language.",
      "Their system breaks the text into non-overlapping 200-character segments, and feeds character n-grams (n = 8) into the GRU network to clas- sify each letter into a probable language. The segment\u2019s language is simply the most probable language over all letters, and the text\u2019s language is the most probable lan- guage over all segments. The authors tested their system on short messages, but not on tweets; they built their own corpus of short messages by dividing their data into 200- character segments. On that corpus, they achieve an accu- racy of 0.955, while langid.py achieves 0.912. 2https://blog.twitter.com/ engineering/en_us/a/2015/ evaluating-language-identification-performance. html Table 2. Twitter corpus distribution by language label. Lang P(%) Lang P(%) Lang P(%) EN 36.458 EL 0.086 LV, BG, JA 23.750 SV 0.046 UR, TA 10\u22123 ES 9.964 FA 0.027 MR, BN, AR 7.627 VI 0.021 MR, BN, PT 6.839 FI 0.",
      "458 EL 0.086 LV, BG, JA 23.750 SV 0.046 UR, TA 10\u22123 ES 9.964 FA 0.027 MR, BN, AR 7.627 VI 0.021 MR, BN, PT 6.839 FI 0.020 IN, KN, KO 5.559 CS 0.015 ET, SL, TH 2.965 UK 0.015 GU, CY, FR 2.180 HI 0.013 ZH, CKB, TR 2.152 DA 0.007 IS, LT, RU 0.948 HU 0.006 ML, SI, IT 0.490 NO 0.005 IW, NE, DE 0.356 RO 0.003 KM, MY, PL 0.251 SR 0.003 TL, KA, NL 0.187 EU 0.002 BO < 10\u22123 The authors of (Jurgens et al., 2017) also created a character-level LID network using a GRU architecture, in the form of a three-layer encoder-decoder RNN.",
      "003 TL, KA, NL 0.187 EU 0.002 BO < 10\u22123 The authors of (Jurgens et al., 2017) also created a character-level LID network using a GRU architecture, in the form of a three-layer encoder-decoder RNN. They trained and tested their system using their own Twitter dataset, and achieved an F1-score of 0.982, while langid.py achieved 0.960 on the same dataset. To summarize, we present the key results of the papers re- viewed in this section in Table 1, along with the results langid.py obtained on the same datasets as benchmark. 3. Our Twitter LID Datasets 3.1. Source Data and Language Labeling Unlike other authors who built Twitter datasets, we chose not to mine tweets from Twitter directly through their API, but instead use tweets that have already been downloaded and archived on the Internet Archive3.",
      "3. Our Twitter LID Datasets 3.1. Source Data and Language Labeling Unlike other authors who built Twitter datasets, we chose not to mine tweets from Twitter directly through their API, but instead use tweets that have already been downloaded and archived on the Internet Archive3. This has two im- portant bene\ufb01ts: this site makes its content freely avail- able for research purposes, unlike Twitter which comes with restrictions (especially on distribution), and the tweets are backed-up permanently, as opposed to Twitter where tweets may be deleted at any time and become unavail- able for future research or replication of past studies. The Internet Archive has made available a set of 1.7 billion tweets collected over the year of 2017 in a 600GB JSON \ufb01le which includes all tweet metadata attributes4. Five of these attributes are of particular importance to us.",
      "The Internet Archive has made available a set of 1.7 billion tweets collected over the year of 2017 in a 600GB JSON \ufb01le which includes all tweet metadata attributes4. Five of these attributes are of particular importance to us. They are tweet.id, tweet.user.id, tweet.text, tweet.lang, and tweet.user.lang, corresponding respectively to the unique tweet ID number, the unique user ID number, the text con- 3https://archive.org/details/ twitterstream 4https://developer.twitter.com/en/ docs/tweets/data-dictionary/overview/ intro-to-tweet-json.html",
      "Language Identi\ufb01cation on Massive Datasets of Short Message using an Attention Mechanism CNN tent of the tweet in UTF-8 characters, the tweet\u2019s language as determined by Twitter\u2019s automated LID software, and the user\u2019s self-declared language. We begin by \ufb01ltering the corpus to keep only those tweets where the user\u2019s self-declared language and the tweet\u2019s detected language correspond; that language becomes the tweet\u2019s correct language label. This operation cuts out roughly half the tweets, and leaves us with a corpus of about 900 million tweets in 54 different languages. Table 2 shows the distribution of languages in that corpus. Unsur- prisingly, it is a very imbalanced distribution of languages, with English and Japanese together accounting for 60% of all tweets. This is consistent with other studies and statis- tics of language use on Twitter, going as far back as 20135. It does however make it very dif\ufb01cult to use this corpus to train a LID system for other languages, especially for one of the dozens of seldom-used languages. This was our mo- tivation for creating a balanced Twitter dataset. 3.2.",
      "It does however make it very dif\ufb01cult to use this corpus to train a LID system for other languages, especially for one of the dozens of seldom-used languages. This was our mo- tivation for creating a balanced Twitter dataset. 3.2. Our Balanced Datasets When creating a balanced Twitter LID dataset, we face a design question: should our dataset seek to maximize the number of languages present, to make it more interesting and challenging for the task of LID, but at the cost of hav- ing fewer tweets per language to include seldom-used lan- guages. Or should we maximize the number of tweets per language to make the dataset more useful for training deep neural networks, but at the cost of having fewer languages present and eliminating the seldom-used languages. To cir- cumvent this issue, we propose to build three datasets: a small-scale one with more languages but fewer tweets, a large-scale one with more tweets but fewer languages, and a medium-scale one that is a compromise between the two extremes.",
      "To cir- cumvent this issue, we propose to build three datasets: a small-scale one with more languages but fewer tweets, a large-scale one with more tweets but fewer languages, and a medium-scale one that is a compromise between the two extremes. Moreover, since we plan for our datasets to be- come standard benchmarking tools, we have subdivided the tweets of each language in each dataset into training, vali- dation, and testing sets. \u2022 Small-scale dataset: This dataset is composed of 28 languages with 13,000 tweets per language, subdi- vided into 7,000 training set tweets, 3,000 validation set tweets, and 3,000 testing set tweets. There is thus a total of 364,000 tweets in this dataset. Referring to Table 2, this dataset includes every language that rep- resents 0.002% or more of the Twitter corpus. To be sure, it is possible to create a smaller dataset with all 54 languages but much fewer tweets per language, but we feel that this is the lower limit to be useful for train- ing LID deep neural systems.",
      "To be sure, it is possible to create a smaller dataset with all 54 languages but much fewer tweets per language, but we feel that this is the lower limit to be useful for train- ing LID deep neural systems. \u2022 Medium scale dataset: This dataset keeps 22 of the 5https://www.statista.com/statistics/267129/most-used- languages-on-twitter/ 28 languages of the small-scale dataset, but has 10 times as many tweets per language. In other words, each language has a 70,000-tweet training set, a 30,000-tweet validation set, and a 30,000-tweet test- ing set, for a total of 2,860,000 tweets. \u2022 Large-scale dataset: Once again, we increased ten- fold the number of tweets per language, and kept the 14 languages that had suf\ufb01cient tweets in our ini- tial 900 million tweet corpus. This gives us a dataset where each language has 700,000 tweets in its training set, 300,000 tweets in its validation set, and 300,000 tweets in its testing set, for a total 18,200,000 tweets.",
      "This gives us a dataset where each language has 700,000 tweets in its training set, 300,000 tweets in its validation set, and 300,000 tweets in its testing set, for a total 18,200,000 tweets. Referring to Table 2, this dataset includes every lan- guage that represents 0.1% or more of the Twitter cor- pus. 4. Proposed Model Since many languages have unclear word boundaries, char- acter n-grams, rather than words, have become widely used as input in LID systems (Lui & Baldwin, 2012; Kocmi & Bojar, 2017; Tromp & Pechenizkiy, 2011; Ju- rgens et al., 2017).",
      "With this in mind, the LID problem can be de\ufb01ned as such: given a tweet tw consisting of n ordered characters (tw = [ch1, ch2, ..., chn]) selected within the vocabulary set char of V unique characters (char = {ch1, ch2, ..., chV }) and a set l of L languages (l = {l1, l2, ..., lL}) , the aim is to predict the language \u02c6l present in tw using a classi\ufb01er: \u02c6l = argmaxli\u2208lScore(li|tw), (1) where Score(li|tw) is a scoring function quantifying how likely language li was used given the observed message tw. Most statistical LID systems follow the model of (Lui & Baldwin, 2012). They start off by using what is called a one-hot encoding technique, which represents each charac- ter chi as a one-hot vector xoh i \u2208ZV 2 according to the index of this character in char.",
      "They start off by using what is called a one-hot encoding technique, which represents each charac- ter chi as a one-hot vector xoh i \u2208ZV 2 according to the index of this character in char. This transforms tw into a matrix Xoh: Xoh = [xoh 1 , xoh 2 , ..., xoh n ] \u2208ZV \u00d7n 2 where xi oh[j] = ( 1, if chi = char j 0, otherwise (2) The vector Xoh is passed to a feature extraction function, for example row-wise sum or tf-idf weighting, to obtain a feature vector h. h is \ufb01nally fed to a classi\ufb01er model for either discriminative scoring (e.g. Support Vector Machine) or generative scoring (e.g. Na\u00a8\u0131ve Bayes).",
      "Language Identi\ufb01cation on Massive Datasets of Short Message using an Attention Mechanism CNN (a) Neural network LID baseline. (b) Our attentional-based CNN model. Figure 1. Neural network classi\ufb01er architectures. Unlike statistical methods, a typical neural network LID system, as illustrated in Figure 1a, \ufb01rst pass this input through an embedding layer to map each character chi \u2208 tw to a low-dense vector xi \u2208Rd, where d denotes the dimension of character embedding. Given an input tweet tw, after passing through the embedding layer, we obtain an embedded matrix: X = [ x1 x2 ...xn ] \u2208Rd\u00d7n, (3) The embedded matrix X is then fed through a neural net- work architecture, which transforms it into an output vector h = f(X) of length L that represents the likelihood of each language, and which is passed through a Softmax function. This updates equation 1 as: \u02c6l = argmaxli\u2208lSoftmax i(h). (4) Tweets in particular are noisy messages which can contain a mix of multiple languages.",
      "This updates equation 1 as: \u02c6l = argmaxli\u2208lSoftmax i(h). (4) Tweets in particular are noisy messages which can contain a mix of multiple languages. To deal with this challenge, most previous neural network LID systems used deep se- quence neural layers, such as an encoder-decoder (Jurgens et al., 2017) or a GRU (Kocmi & Bojar, 2017), to ex- tract global representations at a high computational cost. By contrast, we propose to employ a shallow (single-layer) convolution neural network (CNN) to locally learn region- based features. In addition, we propose to use an atten- tion mechanism to proportionally merge together these lo- cal features for an entire tweet tw. We hypothesize that the attention mechanism will effectively capture which local features of a particular language are the dominant features of the tweet.",
      "In addition, we propose to use an atten- tion mechanism to proportionally merge together these lo- cal features for an entire tweet tw. We hypothesize that the attention mechanism will effectively capture which local features of a particular language are the dominant features of the tweet. There are two major advantages of our pro- posed architecture: \ufb01rst, the use of the CNN, which has the least number of parameters among other neural networks, simpli\ufb01es the neural network model and decreases the in- ference latency; and second, the use of the attention mecha- nism makes it possible to model the mix of languages while maintaining a competitive performance. 4.1. ngam-regional CNN Model To begin, we present a traditional CNN with an ngam- regional constrain as our baseline. CNNs have been widely used in both image processing (LeCun et al., 1998) and NLP (Collobert et al., 2011). The convolution operation of a \ufb01lter with a region size m is parameterized by a weight matrix Wcnn \u2208Rdcnn\u00d7md and a bias vector bcnn \u2208 Rdcnn, where dcnn is the dimension of the CNN.",
      "The convolution operation of a \ufb01lter with a region size m is parameterized by a weight matrix Wcnn \u2208Rdcnn\u00d7md and a bias vector bcnn \u2208 Rdcnn, where dcnn is the dimension of the CNN. The inputs are a sequence of m consecutive input columns in X, rep- resented by a concatenated vector X[i : i + m \u22121] \u2208Rmd. The region-based feature vector ci is computed as follows: X[i : i + m \u22121] = xi \u2295... \u2295xi+m\u22121, ci = g(Wcnn \u00b7 X[i : i + m \u22121] + bcnn), (5) where \u2295denotes a concatenation operation and g is a non- linear function. The region \ufb01lter is slid from the beginning to the end of X to obtain a convolution matrix C: C = [c1, ..., cn\u2212m+1] \u2208Rdcnn\u00d7(n\u2212m+1).",
      "The region \ufb01lter is slid from the beginning to the end of X to obtain a convolution matrix C: C = [c1, ..., cn\u2212m+1] \u2208Rdcnn\u00d7(n\u2212m+1). (6) The \ufb01rst novelty of our CNN is that we add a zero-padding constrain at both sides of X to ensure that the number of columns in C is equal to the number of columns in X. Consequently, each ci feature vector corresponds to an xi input vector at the same index position i, and is learned from concatenating the surrounding m-gram embeddings.",
      "Language Identi\ufb01cation on Massive Datasets of Short Message using an Attention Mechanism CNN Particularly: 2p + (n \u2212m + 1) = n p = m \u22121 2 , (7) where p is the number of zero-padding columns. Finally, in a normal CNN, a row-wise max-pooling function is applied on C to extract the dcnn most salient features, as shown in Equation 8. However, one weakness of this approach is that it extracts the most salient features out of sequence. h = fCNN (X) = hcnn = poolingmax(C) \u2208Rdcnn (8) 4.2. Attention Mechanism Instead of the traditional pooling function of Equation 8, a second important innovation of our CNN model is to use an attention mechanism to model the interaction between region-based features from the beginning to the end of an input. Figure 1b illustrates our proposed model.",
      "Attention Mechanism Instead of the traditional pooling function of Equation 8, a second important innovation of our CNN model is to use an attention mechanism to model the interaction between region-based features from the beginning to the end of an input. Figure 1b illustrates our proposed model. Given a sequence of regional feature vectors C = [c1, c2, ..., cn] as computed in Equation 6, we pass it through a fully- connected hidden layer to learn a sequence of regional hid- den vectors H = [h1, h2, ..., hn] \u2208Rdhd\u00d7n using Equa- tion 9. hi = g2(Whd \u00b7 ci + bhd) \u2208Rdhd, (9) where g2 is a non-linear activation function, Whd and bhd denote model parameters, and dhd is the dimension of the hidden layer. We followed Yang et al. (Yang et al., 2016) in employing a regional context vector u \u2208Rdhd to measure the importance of each window-based hidden vector. The regional importance factors are computed by: t = H \u00d7 u \u2208Rn.",
      "We followed Yang et al. (Yang et al., 2016) in employing a regional context vector u \u2208Rdhd to measure the importance of each window-based hidden vector. The regional importance factors are computed by: t = H \u00d7 u \u2208Rn. (10) The importance factors are then fed to a Softmax layer to obtain the normalized weight: \u03b1i = ti P i\u2032 ti\u2032 . (11) The \ufb01nal representation of a given input is computed by a weighted sum of its regional feature vectors: h = X i \u03b1ici (12) 5. Experimental Results 5.1. Benchmarks For the benchmarks, we selected \ufb01ve systems. We picked \ufb01rst the langid.py6 library which is frequently used to com- 6https://github.com/saffsd/langid.py Table 3. Parameter settings Parameter our CNN our att CNN d 50 50 g relu relu g2 n.a. relu dcnn 100 100 m 5 5 p 2 2 dhd n.a.",
      "Parameter settings Parameter our CNN our att CNN d 50 50 g relu relu g2 n.a. relu dcnn 100 100 m 5 5 p 2 2 dhd n.a. 100 lr 0.001 0.001 decay rate 0.05 0.05 max epochs 512 512 patience 64 64 clip rate 5 5 pare systems in the literature. Since our work is in neural- network LID, we selected two neural network systems from the literature, speci\ufb01cally the encoder-decoder EquiLID7 system of (Jurgens et al., 2017) and the GRU neural net- work LanideNN8 system of (Kocmi & Bojar, 2017).",
      "Fi- nally, we included CLD29 and CLD310, two implementa- tions of the Na\u00a8\u0131ve Bayes LID software used by Google in their Chrome web browser (Lui & Baldwin, 2014; Jauhi- ainen et al., 2018; Bergsma et al., 2012) and sometimes used as a comparison system in the LID literature (Blod- gett et al., 2017; Jurgens et al., 2017; Bergsma et al., 2012; Lui & Baldwin, 2012; Kocmi & Bojar, 2017). We obtained publicly-available implementations of each of these algo- rithms, and test them all against our three datasets. In Table 4, we report each algorithm\u2019s accuracy and F1 score, the two metrics usually reported in the LID literature. We also included precision and recall values, which are necessary for computing F1 score. And \ufb01nally we included the speed in number of messages handled per second.",
      "In Table 4, we report each algorithm\u2019s accuracy and F1 score, the two metrics usually reported in the LID literature. We also included precision and recall values, which are necessary for computing F1 score. And \ufb01nally we included the speed in number of messages handled per second. This metric is not often discussed in the LID literature, but is of particu- lar importance when dealing with a massive dataset such as ours or a massive streaming source such as Twitter. We compare these benchmarks to our two models: the im- proved CNN as described in Section 4.1 and our proposed CNN model with an attention mechanism of Section 4.2. These are labelled CNN and Attention CNN in Table 4. In both models, we \ufb01lter out characters that appear less than 5 times and apply a dropout approach with a dropout rate of 0.5. ADAM optimization algorithm and early stopping techniques are employed during training. The full list of parameters and settings is given in Table 3. It is worth not- ing that we randomly select this con\ufb01guration without any tuning process.",
      "ADAM optimization algorithm and early stopping techniques are employed during training. The full list of parameters and settings is given in Table 3. It is worth not- ing that we randomly select this con\ufb01guration without any tuning process. 7https://github.com/davidjurgens/equilid 8https://github.com/tomkocmi/LanideNN 9https://github.com/CLD2Owners/cld2 10https://github.com/google/cld3",
      "Language Identi\ufb01cation on Massive Datasets of Short Message using an Attention Mechanism CNN Table 4. Benchmarking results. Model Small-scale dataset Medium-scale dataset Large-scale dataset Acc P R F1 Speed Acc P R F1 Speed Acc P R F1 Speed langid.py 0.9229 0.9290 0.9229 0.9240 3710.96 0.9449 0.9475 0.9449 0.9454 3797.34 0.9483 0.9502 0.9483 0.9486 4630.59 CLD2 0.8670 0.9624 0.8670 0.8997 43308.31 0.8784 0.9638 0.8784 0.9067 40287.01 0.8711 0.9527 0.8711 0.8952 42297.95 CLD3 0.7284 0.8686 0.7284 0.7456 5911.94 0.7131 0.8792 0.",
      "8711 0.9527 0.8711 0.8952 42297.95 CLD3 0.7284 0.8686 0.7284 0.7456 5911.94 0.7131 0.8792 0.7131 0.7333 6265.21 0.6976 0.9133 0.6976 0.7326 6139.86 LanideNN 0.9304 0.9052 0.8984 0.9003 11.47 0.9414 0.9064 0.9005 0.9021 9.38 0.9370 0.8811 0.8745 0.8763 7.08 EquiLID 0.9244 0.9516 0.9244 0.9325 7.53 0.9430 0.9616 0.9430 0.9484 7.64 0.9489 0.9648 0.9489 0.9532 7.05 langid.",
      "9244 0.9325 7.53 0.9430 0.9616 0.9430 0.9484 7.64 0.9489 0.9648 0.9489 0.9532 7.05 langid.py (retrained) 0.9013 0.9030 0.9013 0.9018 460.00 0.7866 0.8068 0.7866 0.7918 459.25 0.6731 0.7990 0.6731 0.6918 459.60 CNN 0.9675 0.9677 0.9675 0.9675 39562.69 0.9832 0.9834 0.9832 0.9832 38427.42 0.9866 0.9867 0.9866 0.9866 31647.77 Attention CNN 0.9712 0.9716 0.9712 0.9713 31782.44 0.9841 0.9842 0.",
      "9866 0.9867 0.9866 0.9866 31647.77 Attention CNN 0.9712 0.9716 0.9712 0.9713 31782.44 0.9841 0.9842 0.9841 0.9842 24828.28 0.9915 0.9915 0.9915 0.9915 35266.82 5.2. Analysis The \ufb01rst thing that appears from these results is the speed difference between algorithms. CLD3 and langid.py both can process several thousands of messages per second, and CLD2 is even an order of magnitude better, but the two neural network software have considerably worse perfor- mances, at less than a dozen messages per second. This is the ef\ufb01ciency trade-off of neural-network LID systems we mentioned in Section 1; although to be fair, we should also point out that those two systems are research prototypes and thus may not have been fully optimized. In terms of accuracy and F1 score, langid.py, LanideNN, and EquiLID have very similar performances.",
      "In terms of accuracy and F1 score, langid.py, LanideNN, and EquiLID have very similar performances. All three consistently score above 0.90, and each achieves the best accuracy or the best F1 score at some point, if only by 0.002. By contrast, CLD2 and CLD3 have weaker perfor- mances; signi\ufb01cantly so in the case of CLD3. In all cases, using our small-, medium-, or large-scale test set does not signi\ufb01cantly affect the results. All the benchmark systems were tested using the pre- trained models they come with. For comparison purposes, we retrained langid.py from scratch using the training and validation portion of our datasets, and ran the tests again. Surprisingly, we \ufb01nd that the results are worse for all metrics compared to using their pre-trained model, and moreover that using the medium- and large-scale datasets give signi\ufb01cantly worse results than using the small-scale dataset.",
      "Surprisingly, we \ufb01nd that the results are worse for all metrics compared to using their pre-trained model, and moreover that using the medium- and large-scale datasets give signi\ufb01cantly worse results than using the small-scale dataset. This may be a result of the fact the corpus the langid.py software was trained with and optimized for orig- inally is drastically different from ours: it is a imbalanced dataset of 18,269 tweets in 9 languages. Our larger cor- pora, being more drastically different from the original, give increasingly worse performances. This observation may also explain the almost 10% variation in performance of langid.py reported in the literature and reproduced in Ta- ble 1. The fact that the message handling performance of the library drops massively compared to its pre-trained re- sults further indicates how the software was optimized to use its corpus. Based on this initial result, we decided not to retrain the other benchmark systems. The last two lines of Table 4 report the results of our basic CNN and our attention CNN LID systems.",
      "Based on this initial result, we decided not to retrain the other benchmark systems. The last two lines of Table 4 report the results of our basic CNN and our attention CNN LID systems. It can be seen that both of them outperform the benchmark systems in ac- curacy, precision, recall, and F1 score in all experiments. Moreover, the attention CNN outperforms the basic CNN in every metric (we will explore the bene\ufb01t of the attention mechanism in the next subsection). In terms of processing speed, only the CLD2 system surpasses ours, but it does so at the cost of a 10% drop in accuracy and F1 score. Looking at the choice of datasets, it can be seen that training with the large-scale dataset leads to a nearly 1% improvement com- pared to the medium-sized dataset, which also gives a 1% improvement compared to the small-scale dataset. While it is expected that using more training data will lead to a better system and better results, the small improvement in- dicates that even our small-scale dataset has suf\ufb01cient mes- sages to allow the network training to converge. 5.3.",
      "While it is expected that using more training data will lead to a better system and better results, the small improvement in- dicates that even our small-scale dataset has suf\ufb01cient mes- sages to allow the network training to converge. 5.3. Impact of Attention Mechanism We can further illustrate the impact of our attention mecha- nism by displaying the importance factor \u03b1i corresponding to each character chi in selected tweets. Table 5 shows a set of tweets that were correctly identi\ufb01ed by the attention CNN but misclassi\ufb01ed by the regular CNN in three differ- ent languages: English, French, and Vietnamese. The color intensity of a letter\u2019s cell is proportional to the attention mechanism\u2019s normalized weight \u03b1i, or on the focus the net- work puts on that character. In order words, the attention CNN puts more importance on the features that have the darkest color. The case studies of Table 5 show the noise-tolerance that comes from the attention mechanism. It can be seen that the system puts virtually no weight on URL links (e.g. twen1, twfr2, twvi2), on hashtags (e.g.",
      "The case studies of Table 5 show the noise-tolerance that comes from the attention mechanism. It can be seen that the system puts virtually no weight on URL links (e.g. twen1, twfr2, twvi2), on hashtags (e.g. twen3), or on usernames (e.g. twen2, twfr1, twvi1). We should emphasize that our system does not implement any text preprocessing steps; the input tweets are kept as-is. Despite that, the network learned to distinguish between words and non-words, and to focus mainly on the former. In fact, when the network does put attention on these elements, it is when they appear to use real words (e.g. \u201cstar\u201d and \u201cseed\u201d in the username of twen2, \u201cmother\u201d and \u201cnone\u201d in the hashtag of twen3). This also illustrates how the attention mechanism can pick out",
      "Language Identi\ufb01cation on Massive Datasets of Short Message using an Attention Mechanism CNN Table 5. Tweets misclassi\ufb01ed by the CNN but recognized by the Attention CNN IDs Tweets with attention values att. CNN CNN twen1 en de twen2 en de twen3 en fr twfr1 fr de twfr2 fr ro twfr3 fr es twvi1 vi es twvi2 vi hu twvi3 vi ko \ufb01ne-grained features within noisy text: in those examples, it was able to focus on real-word components of longer non- word strings. The examples of Table 5 also show that the attention CNN learns to focus on common words to recognize lan- guages. Some of the highest-weighted characters in the ex- ample tweets are found in common determiners, adverbs, and verbs of each language. These include \u201cin\u201d (twen1), \u201cdes\u201d (twfr1), \u201cle\u201d (twfr2), \u201cest\u201d (twfr3), \u201cqu\u00e1\" (twvi2), and \u201cnh\u1ea5t\" (twvi3).",
      "These include \u201cin\u201d (twen1), \u201cdes\u201d (twfr1), \u201cle\u201d (twfr2), \u201cest\u201d (twfr3), \u201cqu\u00e1\" (twvi2), and \u201cnh\u1ea5t\" (twvi3). These letters and words signi\ufb01cantly contribute in identifying the language of a given input. Finally, when multiple languages are found within a tweet, the network successfully captures all of them. For example, twfr3 switches from French to Spanish and twvi2 mixes both English and Vietnamese. In both cases, the network identi\ufb01es features of both languages; it focuses strongly on \u201cest\u201d and \u201cy\u201d in twfr3, and on \u201cDon\u2019t\u201d and \u201cb\u00e0i\" in twvi2. The message of twvi3 mixes three languages, Vietnamese, English, and Korean, and the network focuses on all three parts, by picking out \u201cnh\u1eadt\" and \u201cm\u1eebng\" in Vietnamese, \u201c# \uc0dd\uc77c\ucd95\ud558\ud574\u201d and \u201c#\ud0dc\ud615\uc0dd\uc77c\u201d in Korean, and \u201chave\u201d in English.",
      "Since our system is setup to classify each tweet into a single language, the strongest feature of each tweet wins out and the message is classi\ufb01ed in the corresponding language. Nonetheless, it is signi\ufb01cant to see that features of all languages present in the tweet are picked out, and a future version of our system could successfully decompose the tweets into portions of each language. 6. Conclusion In this paper, we \ufb01rst demonstrated how to build balanced, automatically-labelled, and massive LID datasets. These datasets are taken from Twitter, and are thus composed of real-world and noisy messages. We applied our technique to build three datasets ranging from hundreds of thousands to tens of millions of short texts. Next, we proposed our new neural LID system, a CNN-based network with an at- tention mechanism to mitigate the performance bottleneck issue while still maintaining a state-of-the-art performance. The results obtained by our system surpassed \ufb01ve bench- mark LID systems by 5% to 10%.",
      "Next, we proposed our new neural LID system, a CNN-based network with an at- tention mechanism to mitigate the performance bottleneck issue while still maintaining a state-of-the-art performance. The results obtained by our system surpassed \ufb01ve bench- mark LID systems by 5% to 10%. Moreover, our analy- sis of the attention mechanism shed some light on the in- ner workings of the typically-black-box neural network, and demonstrated how it helps pick out the most impor- tant linguistic features while ignoring noise. All of our datasets and source code are publicly available at https: //github.com/duytinvo/LID_NN.",
      "Language Identi\ufb01cation on Massive Datasets of Short Message using an Attention Mechanism CNN References Bergsma, S., McNamee, P., Bagdouri, M., Fink, C., and Wilson, T. Language identi\ufb01cation for creating language-speci\ufb01c twitter collections. In Proceedings of the second workshop on language in social media, pp. 65\u201374. Association for Computational Linguistics, 2012. Blodgett, S. L., Wei, J., and O\u2019Connor, B. A dataset and classi\ufb01er for recognizing social media english. In Pro- ceedings of the 3rd Workshop on Noisy User-generated Text, pp. 56\u201361, 2017. Carter, S., Weerkamp, W., and Tsagkias, M. Microblog language identi\ufb01cation: Overcoming the limitations of short, unedited and idiomatic text. Language Resources and Evaluation, 47(1):195\u2013215, 2013.",
      "Carter, S., Weerkamp, W., and Tsagkias, M. Microblog language identi\ufb01cation: Overcoming the limitations of short, unedited and idiomatic text. Language Resources and Evaluation, 47(1):195\u2013215, 2013. Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. Natural language pro- cessing (almost) from scratch. JMLR, 2011. Gamallo, P., Garcia, M., Sotelo, S., and Campos, J. R. P. Comparing ranking-based and naive bayes approaches to language detection on tweets. In TweetLID@ SEPLN, pp. 12\u201316, 2014. Hurtado, L. F., Pla, F., Gim\u00b4enez, M., and Arnal, E. S. Elirf- upv en tweetlid: Identi\ufb01caci\u00b4on del idioma en twitter. In TweetLID@ SEPLN, pp. 35\u201338, 2014.",
      "In TweetLID@ SEPLN, pp. 35\u201338, 2014. Jaech, A., Mulcaire, G., Hathi, S., Ostendorf, M., and Smith, N. A. Hierarchical character-word mod- els for language identi\ufb01cation. arXiv preprint arXiv:1608.03030, 2016. Jauhiainen, T., Lui, M., Zampieri, M., Baldwin, T., and Lind\u00b4en, K. Automatic language identi\ufb01cation in texts: A survey. arXiv preprint arXiv:1804.08186, 2018. Jurgens, D., Tsvetkov, Y., and Jurafsky, D. Incorporating dialectal variability for socially equitable language iden- ti\ufb01cation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pp. 51\u201357, 2017.",
      "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pp. 51\u201357, 2017. Kocmi, T. and Bojar, O. Lanidenn: Multilingual lan- guage identi\ufb01cation on character window. arXiv preprint arXiv:1701.03338, 2017. LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient- based learning applied to document recognition. Pro- ceedings of the IEEE, pp. 2278\u20132324, 1998. Lui, M. and Baldwin, T. langid. py: An off-the-shelf lan- guage identi\ufb01cation tool. In Proceedings of the ACL 2012 system demonstrations, pp. 25\u201330. Association for Computational Linguistics, 2012. Lui, M. and Baldwin, T. Accurate language identi\ufb01cation of twitter messages. In Proceedings of the 5th workshop on language analysis for social media (LASM), pp.",
      "25\u201330. Association for Computational Linguistics, 2012. Lui, M. and Baldwin, T. Accurate language identi\ufb01cation of twitter messages. In Proceedings of the 5th workshop on language analysis for social media (LASM), pp. 17\u2013 25, 2014. Tromp, E. and Pechenizkiy, M. Graph-based n-gram lan- guage identi\ufb01cation on short texts. In Proc. 20th Ma- chine Learning conference of Belgium and The Nether- lands, pp. 27\u201334, 2011. Vo, D.-T., Zhang, Y., and Zhu, X. Shallow network with rich features for text classi\ufb01cation. In Proceedings of CICLing, 2018. Yang, J. and Zhang, Y. Ncrf++: An open-source neural se- quence labeling toolkit. In Proceedings of the 56th An- nual Meeting of the Association for Computational Lin- guistics, 2018. URL https://arxiv.org/pdf/ 1806.05626.pdf.",
      "In Proceedings of the 56th An- nual Meeting of the Association for Computational Lin- guistics, 2018. URL https://arxiv.org/pdf/ 1806.05626.pdf. Yang, Z., Yang, D., Dyer, C., He, X., Smola, A., and Hovy, E. Hierarchical attention networks for document classi- \ufb01cation. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technologies, pp. 1480\u20131489. Association for Computational Linguis- tics, 2016. doi: 10.18653/v1/N16-1174. URL http: //aclweb.org/anthology/N16-1174. Zubiaga, A., San Vicente, I., Gamallo, P., Campos, J. R. P., Loinaz, I. A., Aranberri, N., Ezeiza, A., and Fresno- Fern\u00b4andez, V. Overview of tweetlid: Tweet language identi\ufb01cation at sepln 2014.",
      "In TweetLID@ SEPLN, pp. 1\u201311, 2014."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1910.06748.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":10635,
  "avg_doclen":177.25,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1910.06748.pdf"
    }
  }
}