[
  "Answering Complex Questions Using Open Information Extraction Tushar Khot Allen Institute for AI tushark@allenai.org Ashish Sabharwal Allen Institute for AI ashishs@allenai.org Peter Clark Allen Institute for AI peterc@allenai.org Abstract While there has been substantial progress in factoid question-answering (QA), an- swering complex questions remains chal- lenging, typically requiring both a large body of knowledge and inference tech- niques. Open Information Extraction (Open IE) provides a way to generate semi-structured knowledge for QA, but to date such knowledge has only been used to answer simple questions with retrieval- based methods. We overcome this limita- tion by presenting a method for reasoning with Open IE knowledge, allowing more complex questions to be handled. Using a recently proposed support graph optimiza- tion framework for QA, we develop a new inference model for Open IE, in particu- lar one that can work effectively with mul- tiple short facts, noise, and the relational structure of tuples.",
  "Using a recently proposed support graph optimiza- tion framework for QA, we develop a new inference model for Open IE, in particu- lar one that can work effectively with mul- tiple short facts, noise, and the relational structure of tuples. Our model signi\ufb01- cantly outperforms a state-of-the-art struc- tured solver on complex questions of vary- ing dif\ufb01culty, while also removing the re- liance on manually curated knowledge. 1 Introduction Effective question answering (QA) systems have been a long-standing quest of AI research. Struc- tured curated KBs have been used successfully for this task (Berant et al., 2013; Berant and Liang, 2014). However, these KBs are expensive to build and typically domain-speci\ufb01c. Automatically con- structed open vocabulary (subject; predicate; ob- ject) style tuples have broader coverage, but have only been used for simple questions where a single tuple suf\ufb01ces (Fader et al., 2014; Yin et al., 2015).",
  "Automatically con- structed open vocabulary (subject; predicate; ob- ject) style tuples have broader coverage, but have only been used for simple questions where a single tuple suf\ufb01ces (Fader et al., 2014; Yin et al., 2015). Our goal in this work is to develop a QA system that can perform reasoning with Open IE (Banko et al., 2007) tuples for complex multiple-choice questions that require tuples from multiple sen- tences. Such a system can answer complex ques- tions in resource-poor domains where curated knowledge is unavailable. Elementary-level sci- ence exams is one such domain, requiring com- plex reasoning (Clark, 2015). Due to the lack of a large-scale structured KB, state-of-the-art sys- tems for this task either rely on shallow reasoning with large text corpora (Clark et al., 2016; Cheng et al., 2016) or deeper, structured reasoning with a small amount of automatically acquired (Khot et al., 2015) or manually curated (Khashabi et al., 2016) knowledge.",
  "Consider the following question from an Alaska state 4th grade science test: Which object in our solar system re\ufb02ects light and is a satellite that orbits around one planet? (A) Earth (B) Mercury (C) the Sun (D) the Moon This question is challenging for QA systems be- cause of its complex structure and the need for multi-fact reasoning. A natural way to answer it is by combining facts such as (Moon; is; in the solar system), (Moon; re\ufb02ects; light), (Moon; is; satellite), and (Moon; orbits; around one planet). A candidate system for such reasoning, and which we draw inspiration from, is the TABLEILP system of Khashabi et al. (2016). TABLEILP treats QA as a search for an optimal subgraph that connects terms in the question and answer via rows in a set of curated tables, and solves the optimization problem using Integer Linear Programming (ILP). We similarly want to search for an optimal subgraph.",
  "(2016). TABLEILP treats QA as a search for an optimal subgraph that connects terms in the question and answer via rows in a set of curated tables, and solves the optimization problem using Integer Linear Programming (ILP). We similarly want to search for an optimal subgraph. However, a large, automatically extracted tuple KB makes the reasoning context different on three fronts: (a) unlike reasoning with tables, chaining tuples is less important and reliable as join rules aren\u2019t arXiv:1704.05572v1  [cs.AI]  19 Apr 2017",
  "available; (b) conjunctive evidence becomes paramount, as, unlike a long table row, a single tuple is less likely to cover the entire question; and (c) again, unlike table rows, tuples are noisy, making combining redundant evidence essen- tial. Consequently, a table-knowledge centered inference model isn\u2019t the best \ufb01t for noisy tuples. To address this challenge, we present a new ILP-based model of inference with tuples, im- plemented in a reasoner called TUPLEINF. We demonstrate that TUPLEINF signi\ufb01cantly outper- forms TABLEILP by 11.8% on a broad set of over 1,300 science questions, without requiring manu- ally curated tables, using a substantially simpler ILP formulation, and generalizing well to higher grade levels. The gains persist even when both solvers are provided identical knowledge. This demonstrates for the \ufb01rst time how Open IE based QA can be extended from simple lookup questions to an effective system for complex questions.",
  "The gains persist even when both solvers are provided identical knowledge. This demonstrates for the \ufb01rst time how Open IE based QA can be extended from simple lookup questions to an effective system for complex questions. 2 Related Work We discuss two classes of related work: retrieval- based web question-answering (simple reason- ing with large scale KB) and science question- answering (complex reasoning with small KB). Web QA: There exist several systems for retrieval-based Web QA problems (Ferrucci et al., 2010; Brill et al., 2002). While structured KBs such as Freebase have been used in many (Berant et al., 2013; Berant and Liang, 2014; Kwiatkowski et al., 2013), such approaches are limited by the coverage of the data. QA systems using semi- structured Open IE tuples (Fader et al., 2013, 2014; Yin et al., 2015) or automatically extracted web tables (Sun et al., 2016; Pasupat and Liang, 2015) have broader coverage but are limited to simple questions with a single query.",
  "Science QA: Elementary-level science QA tasks require reasoning to handle complex questions. Markov Logic Networks (Richardson and Domin- gos, 2006) have been used to perform probabilistic reasoning over a small set of logical rules (Khot et al., 2015). Simple IR techniques have also been proposed for science tests (Clark et al., 2016) and Gaokao tests (equivalent to the SAT exam in China) (Cheng et al., 2016). The work most related to TUPLEINF is the aforementioned TABLEILP solver. This approach focuses on building inference chains using man- ually de\ufb01ned join rules for a small set of curated tables. While it can also use open vocabulary tu- ples (as we assess in our experiments), its ef\ufb01cacy is limited by the dif\ufb01culty of de\ufb01ning reliable join rules for such tuples. Further, each row in some complex curated tables covers all relevant contex- tual information (e.g., each row of the adaptation table contains (animal, adaptation, challenge, ex- planation)), whereas recovering such information requires combining multiple Open IE tuples.",
  "Further, each row in some complex curated tables covers all relevant contex- tual information (e.g., each row of the adaptation table contains (animal, adaptation, challenge, ex- planation)), whereas recovering such information requires combining multiple Open IE tuples. 3 Tuple Inference Solver We \ufb01rst describe the tuples used by our solver. We de\ufb01ne a tuple as (subject; predicate; objects) with zero or more objects. We refer to the subject, pred- icate, and objects as the \ufb01elds of the tuple. 3.1 Tuple KB We use the text corpora (S) from Clark et al. (2016) to build our tuple KB.",
  "We refer to the subject, pred- icate, and objects as the \ufb01elds of the tuple. 3.1 Tuple KB We use the text corpora (S) from Clark et al. (2016) to build our tuple KB. For each test set, we use the corresponding training questions Qtr to retrieve domain-relevant sentences from S. Speci\ufb01cally, for each multiple-choice question (q, A) \u2208Qtr and each choice a \u2208A, we use all non-stopword tokens in q and a as an Elastic- Search1 query against S. We take the top 200 hits, run Open IE v4,2 and aggregate the resulting tu- ples over all a \u2208A and over all questions in Qtr to create the tuple KB (T).3 3.2 Tuple Selection Given a multiple-choice question qa with question text q and answer choices A={ai}, we select the most relevant tuples from T and S as follows. Selecting from Tuple KB: We use an in- verted index to \ufb01nd the 1,000 tuples that have the most overlapping tokens with question tokens tok(qa).4.",
  "Selecting from Tuple KB: We use an in- verted index to \ufb01nd the 1,000 tuples that have the most overlapping tokens with question tokens tok(qa).4. We also \ufb01lter out any tuples that over- lap only with tok(q) as they do not support any answer. We compute the normalized TF-IDF score treating the question, q as a query and each tuple, t as a document: tf(x, q) = 1 if x \u2208q; idf(x) = log(1 + N/nx) tf-idf(t, q) = X x\u2208t\u2229q idf(x) 1https://www.elastic.co/products/elasticsearch 2http://knowitall.github.io/openie 3Available at http://anonymized 4All tokens are stemmed and stop-word \ufb01ltered",
  "Figure 1: An example support graph linking a question (top), two tuples from the KB (colored) and an answer option (nitrogen). where N is the number of tuples in the KB and nx are the number of tuples containing x. We nor- malize the tf-idf score by the number of tokens in t and q. We \ufb01nally take the 50 top-scoring tuples Tqa 5. On-the-\ufb02y tuples from text: To handle ques- tions from new domains not covered by the train- ing set, we extract additional tuples on the \ufb02y from S (similar to Sharma et al. (2015)). We perform the same ElasticSearch query described earlier for building T. We ignore sentences that cover none or all answer choices as they are not discriminative. We also ignore long sentences (>300 characters) and sentences with negation6 as they tend to lead to noisy inference. We then run Open IE on these sentences and re-score the resulting tuples using the Jaccard score7 due to the lossy nature of Open IE, and \ufb01nally take the 50 top-scoring tuples T \u2032 qa.",
  "We then run Open IE on these sentences and re-score the resulting tuples using the Jaccard score7 due to the lossy nature of Open IE, and \ufb01nally take the 50 top-scoring tuples T \u2032 qa. 3.3 Support Graph Search Similar to TABLEILP, we view the QA task as searching for a graph that best connects the terms in the question (qterms) with an answer choice via the knowledge; see Figure 1 for a simple il- lustrative example. Unlike standard alignment models used for tasks such as Recognizing Tex- tual Entailment (RTE) (Dagan et al., 2010), how- ever, we must score alignments between a set Tqa \u222aT \u2032 qa of structured tuples and a (potentially multi-sentence) multiple-choice question qa. The qterms, answer choices, and tuples \ufb01elds form the set of possible vertices, V, of the support graph.",
  "The qterms, answer choices, and tuples \ufb01elds form the set of possible vertices, V, of the support graph. Edges connecting qterms to tuple \ufb01elds and tuple \ufb01elds to answer choices form the set of pos- sible edges, E. The support graph, G(V, E), is a subgraph of G(V, E) where V and E denote \u201cac- tive\u201d nodes and edges, resp.",
  "We de\ufb01ne the desired behavior of an optimal support graph via an ILP 5Available at http://allenai.org/data.html 6containing not, \u2019nt, or except 7| tok(t) \u2229tok(qa) | / | tok(t) \u222atok(qa) | Active \ufb01eld must have < w1 connected edges Active choice must have < w2 edges Active qterm must have < w3 edges Support graph must have < w4 active tuples Active tuple must have \u2265w5 active \ufb01elds Active tuple must have an edge to some qterm Active tuple must have an edge to some choice Active tuple must have active subject If a tuple predicate aligns to q, the subject (object) must align to a term preceding (following, resp.) q Table 1: High-level ILP constraints; we report re- sults for \u20d7w = (2, 4, 4, 4, 2); the model can be im- proved with more careful parameter selection model as follows8. Objective Function Similar to TABLEILP, we score the support graph based on the weight of the active nodes and edges.",
  "Objective Function Similar to TABLEILP, we score the support graph based on the weight of the active nodes and edges. Each edge e(t, h) is weighted based on a word- overlap score.9 While TABLEILP used Word- Net (Miller, 1995) paths to compute the weight, this measure results in unreliable scores when faced with longer phrases found in Open IE tuples. Compared to a curated KB, it is easy to \ufb01nd Open IE tuples that match irrelevant parts of the questions. To mitigate this issue, we improve the scoring of qterms in our ILP objective to focus on important terms. Since the later terms in a ques- tion tend to provide the most critical information, we scale qterm coef\ufb01cients based on their position. Also, qterms that appear in almost all of the se- lected tuples tend not to be discriminative as any tuple would support such a qterm. Hence we scale the coef\ufb01cients by the inverse frequency of the to- kens in the selected tuples.",
  "Also, qterms that appear in almost all of the se- lected tuples tend not to be discriminative as any tuple would support such a qterm. Hence we scale the coef\ufb01cients by the inverse frequency of the to- kens in the selected tuples. Constraints Since Open IE tuples do not come with schema and join rules, we can de\ufb01ne a substantially sim- pler model compared to TABLEILP. This reduces the reasoning capability but also eliminates the re- liance on hand-authored join rules and regular ex- pressions used in TABLEILP. We discovered (see empirical evaluation) that this simple model can achieve the same score as TABLEILP on the Re- gents test (target test set used by TABLEILP) and generalizes better to different grade levels. We de\ufb01ne active vertices and edges using ILP constraints: an active edge must connect two ac- tive vertices and an active vertex must have at least one active edge. To avoid positive edge coef\ufb01- 8c.f. Appendix A for more details 9w(t, h) = |tok(t) \u2229tok(h)|/|tok(h)|",
  "cients in the objective function resulting in spuri- ous edges in the support graph, we limit the num- ber of active edges from an active tuple, question choice, tuple \ufb01elds, and qterms (\ufb01rst group of con- straints in Table 1). Our model is also capable of using multiple tuples to support different parts of the question as illustrated in Figure 1. To avoid spurious tuples that only connect with the question (or choice) or ignore the relation being expressed in the tuple, we add constraints that require each tuple to connect a qterm with an answer choice (second group of constraints in Table 1). We also de\ufb01ne new constraints based on the Open IE tuple structure. Since an Open IE tu- ple expresses a fact about the tuple\u2019s subject, we require the subject to be active in the support graph. To avoid issues such as (Planet; orbit; Sun) matching the sample question in the introduction (\u201cWhich object. . .orbits around a planet\u201d), we also add an ordering constraint (third group in Table 1). Its worth mentioning that TUPLEINF only com- bines parallel evidence i.e.",
  ". .orbits around a planet\u201d), we also add an ordering constraint (third group in Table 1). Its worth mentioning that TUPLEINF only com- bines parallel evidence i.e. each tuple must con- nect words in the question to the answer choice. For reliable multi-hop reasoning using OpenIE tu- ples, we can add inter-tuple connections to the support graph search, controlled by a small num- ber of rules over the OpenIE predicates. Learning such rules for the Science domain is an open prob- lem and potential avenue of future work. 4 Experiments Comparing our method with two state-of-the-art systems for 4th and 8th grade science exams, we demonstrate that (a) TUPLEINF with only au- tomatically extracted tuples signi\ufb01cantly outper- forms TABLEILP with its original curated knowl- edge as well as with additional tuples, and (b) TU- PLEINF\u2019s complementary approach to IR leads to an improved ensemble. Numbers in bold indicate statistical signi\ufb01cance based on the Binomial ex- act test (Howell, 2012) at p = 0.05.",
  "Numbers in bold indicate statistical signi\ufb01cance based on the Binomial ex- act test (Howell, 2012) at p = 0.05. We consider two question sets. (1) 4th Grade set (1220 train, 1304 test) is a 10x larger superset of the NY Regents questions (Clark et al., 2016), and includes professionally written licensed ques- tions. (2) 8th Grade set (293 train, 282 test) con- tains 8th grade questions from various states.10 We consider two knowledge sources.",
  "(2) 8th Grade set (293 train, 282 test) con- tains 8th grade questions from various states.10 We consider two knowledge sources. The Sen- tence corpus (S) consists of domain-targeted 80K sentences and 280 GB of plain text extracted from 10http://allenai.org/data/science-exam-questions.html Solvers 4th Grade 8th Grade TABLEILP(C) 39.9 34.1 TUPLEINF(T+T\u2019) 51.7 51.6 TABLEILP(C+T) 42.1 37.9 TUPLEINF(C+T) 47.5 48.0 Table 2: TUPLEINF is signi\ufb01cantly better at struc- tured reasoning than TABLEILP Solvers 4th Grade 8th Grade IR(S) 52.0 52.8 IR(S) + TABLEILP(C) 53.3 54.5 IR(S) + TUPLEINF(T+T\u2019) 55.3 55.1 Table 3: TUPLEINF is complementarity to IR, re- sulting in a strong ensemble web pages used by Clark et al.",
  "(2016). This cor- pus is used by the IR solver and also used to create the tuple KB T and on-the-\ufb02y tuples T \u2032 qa. Addi- tionally, TABLEILP uses \u223c70 Curated tables (C) designed for 4th grade NY Regents exams. We compare TUPLEINF with two state-of-the- art baselines. IR is a simple yet powerful information-retrieval baseline (Clark et al., 2016) that selects the answer option with the best match- ing sentence in a corpus. TABLEILP is the state- of-the-art structured inference baseline (Khashabi et al., 2016) developed for science questions. 4.1 Results Table 2 shows that TUPLEINF, with no curated knowledge, outperforms TABLEILP on both ques- tion sets by more than 11%. The lower half of the table shows that even when both solvers are given the same knowledge (C+T),11 the improved selec- tion and simpli\ufb01ed model of TUPLEINF12 results in a statistically signi\ufb01cant improvement.",
  "The lower half of the table shows that even when both solvers are given the same knowledge (C+T),11 the improved selec- tion and simpli\ufb01ed model of TUPLEINF12 results in a statistically signi\ufb01cant improvement. Our simple model, TUPLEINF(C + T), also achieves scores comparable to TABLEILP on the latter\u2019s target Regents questions (61.4% vs TABLEILP\u2019s reported 61.5%) without any specialized rules. Table 3 shows that while TUPLEINF achieves similar scores as the IR solver, the approaches are complementary (structured lossy knowledge reasoning vs. lossless sentence retrieval). The two solvers, in fact, differ on 47.3% of the train- ing questions. To exploit this complementarity, 11See Appendix B for how tables (and tuples) are used by TUPLEINF (and TABLEILP). 12On average, TABLEILP (TUPLEINF) has 3,403 (1,628, resp.) constraints and 982 (588, resp.) variables.",
  "12On average, TABLEILP (TUPLEINF) has 3,403 (1,628, resp.) constraints and 982 (588, resp.) variables. TUPLE- INF\u2019s ILP can be solved in half the time taken by TABLEILP, resulting in 68.6% reduction in overall question answering time.",
  "we train an ensemble system (Clark et al., 2016) which, as shown in the table, provides a substan- tial boost over the individual solvers. Further, IR + TUPLEINF is consistently better than IR + TABLEILP. Finally, in combination with IR and the statistical association based PMI solver (that scores 54.1% by itself) of Clark et al. (2016), TU- PLEINF achieves a score of 58.2% as compared to TABLEILP\u2019s ensemble score of 56.7% on the 4th grade set, again attesting to TUPLEINF\u2019s strength. 5 Error Analysis We describe four classes of failures that we ob- served, and the future work they suggest. Missing Important Words: Which material will spread out to completely \ufb01ll a larger con- tainer? (A)air (B)ice (C)sand (D)water In this question, we have tuples that support water will spread out and \ufb01ll a larger container but miss the critical word \u201ccompletely\u201d. An approach capa- ble of detecting salient question words could help avoid that.",
  "(A)air (B)ice (C)sand (D)water In this question, we have tuples that support water will spread out and \ufb01ll a larger container but miss the critical word \u201ccompletely\u201d. An approach capa- ble of detecting salient question words could help avoid that. Lossy IE: Which action is the best method to separate a mixture of salt and water? ... The IR solver correctly answers this question by using the sentence: Separate the salt and water mixture by evaporating the water. However, TU- PLEINF is not able to answer this question as Open IE is unable to extract tuples from this impera- tive sentence. While the additional structure from Open IE is useful for more robust matching, con- verting sentences to Open IE tuples may lose im- portant bits of information. Bad Alignment: Which of the following gases is necessary for humans to breathe in order to live?(A) Oxygen(B) Carbon dioxide(C) Helium(D) Water vapor TUPLEINF returns \u201cCarbon dioxide\u201d as the answer because of the tuple (humans; breathe out; carbon dioxide).",
  "Bad Alignment: Which of the following gases is necessary for humans to breathe in order to live?(A) Oxygen(B) Carbon dioxide(C) Helium(D) Water vapor TUPLEINF returns \u201cCarbon dioxide\u201d as the answer because of the tuple (humans; breathe out; carbon dioxide). The chunk \u201cto breathe\u201d in the question has a high alignment score to the \u201cbreathe out\u201d relation in the tuple even though they have com- pletely different meanings. Improving the phrase alignment can mitigate this issue. Out of scope: Deer live in forest for shelter. If the forest was cut down, which situation would most likely happen?... Such questions that require modeling a state pre- sented in the question and reasoning over the state are out of scope of our solver. 6 Conclusion We presented a new QA system, TUPLEINF, that can reason over a large, potentially noisy tuple KB to answer complex questions. Our results show that TUPLEINF is a new state-of-the-art struc- tured solver for elementary-level science that does not rely on curated knowledge and generalizes to higher grades.",
  "Our results show that TUPLEINF is a new state-of-the-art struc- tured solver for elementary-level science that does not rely on curated knowledge and generalizes to higher grades. Errors due to lossy IE and misalign- ments suggest future work in incorporating con- text and distributional measures. References Tobias Achterberg. 2009. SCIP: solving constraint in- teger programs. Math. Prog. Computation 1(1):1\u2013 41. Michele Banko, Michael J. Cafarella, Stephen Soder- land, Matthew Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In IJ- CAI. J. Berant, A. Chou, R. Frostig, and P. Liang. 2013. Se- mantic parsing on Freebase from question-answer pairs. In Empirical Methods in Natural Language Processing (EMNLP). Jonathan Berant and Percy Liang. 2014. Semantic parsing via paraphrasing. In ACL. Eric Brill, Susan Dumais, and Michele Banko. 2002.",
  "In Empirical Methods in Natural Language Processing (EMNLP). Jonathan Berant and Percy Liang. 2014. Semantic parsing via paraphrasing. In ACL. Eric Brill, Susan Dumais, and Michele Banko. 2002. An analysis of the AskMSR question-answering sys- tem. In Proceedings of EMNLP. pages 257\u2013264. Gong Cheng, Weixi Zhu, Ziwei Wang, Jianghui Chen, and Yuzhong Qu. 2016. Taking up the gaokao chal- lenge: An information retrieval approach. In IJCAI. Peter Clark. 2015. Elementary school science and math tests as a driver for AI: take the Aristo challenge! In 29th AAAI/IAAI. Austin, TX, pages 4019\u20134021. Peter Clark, Oren Etzioni, Tushar Khot, Ashish Sab- harwal, Oyvind Tafjord, Peter Turney, and Daniel Khashabi. 2016. Combining retrieval, statistics, and inference to answer elementary science questions.",
  "Peter Clark, Oren Etzioni, Tushar Khot, Ashish Sab- harwal, Oyvind Tafjord, Peter Turney, and Daniel Khashabi. 2016. Combining retrieval, statistics, and inference to answer elementary science questions. In 30th AAAI. Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. 2010. Recognizing textual entailment: Ra- tional, evaluation and approaches\u2013erratum. Natural Language Engineering 16(01):105\u2013105. Anthony Fader, Luke S. Zettlemoyer, and Oren Etzioni. 2013. Paraphrase-driven learning for open question answering. In ACL. Anthony Fader, Luke S. Zettlemoyer, and Oren Etzioni. 2014. Open question answering over curated and extracted knowledge bases. In KDD.",
  "David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A Kalyanpur, Adam Lally, J William Murdock, Eric Nyberg, John Prager, et al. 2010. Building Watson: An overview of the DeepQA project. AI Magazine 31(3):59\u201379. David Howell. 2012. Statistical methods for psychol- ogy. Cengage Learning. Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Pe- ter Clark, Oren Etzioni, and Dan Roth. 2016. Ques- tion answering via integer programming over semi- structured knowledge. In IJCAI. Tushar Khot, Niranjan Balasubramanian, Eric Gribkoff, Ashish Sabharwal, Peter Clark, and Oren Etzioni. 2015. Exploring Markov logic networks for question answering. In EMNLP. Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke S. Zettlemoyer. 2013.",
  "2015. Exploring Markov logic networks for question answering. In EMNLP. Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke S. Zettlemoyer. 2013. Scaling semantic parsers with on-the-\ufb02y ontology matching. In EMNLP. George Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM 38(11):39\u2013 41. Panupong Pasupat and Percy Liang. 2015. Composi- tional semantic parsing on semi-structured tables. In ACL. Matthew Richardson and Pedro Domingos. 2006. Markov logic networks. Machine learning 62(1\u2013 2):107\u2013136. Arpit Sharma, Nguyen Ha Vo, Somak Aditya, and Chitta Baral. 2015. Towards addressing the wino- grad schema challenge - building and using a seman- tic parser and a knowledge hunting module. In IJ- CAI.",
  "Arpit Sharma, Nguyen Ha Vo, Somak Aditya, and Chitta Baral. 2015. Towards addressing the wino- grad schema challenge - building and using a seman- tic parser and a knowledge hunting module. In IJ- CAI. Huan Sun, Hao Ma, Xiaodong He, Wen tau Yih, Yu Su, and Xifeng Yan. 2016. Table cell search for question answering. In WWW. Pengcheng Yin, Nan Duan, Ben Kao, Jun-Wei Bao, and Ming Zhou. 2015. Answering questions with com- plex semantic constraints on open knowledge bases. In CIKM.",
  "A Appendix: ILP Model Details To build the ILP model, we \ufb01rst need to get the questions terms (qterm) from the question by chunking the question using an in-house chunker based on the postagger from FACTORIE. 13 Variables The ILP model has an active vertex variable for each qterm (xq), tuple (xt), tuple \ufb01eld (xf) and question choice (xa). Table 4 describes the co- ef\ufb01cients of these active variables. For example, the coef\ufb01cient of each qterm is a constant value (0.8) scaled by three boosts. The idf boost, idfB for a qterm, x is calculated as log(1 + (|Tqa| + |T \u2032 qa|)/nx) where nx is the number of tuples in Tqa \u222aT \u2032 qa containing x. The science term boost, scienceB boosts coef\ufb01cients of qterms that are valid science terms based on a list of 9K terms.",
  "The location boost, locB of a qterm at index i in the question is given by i/tok(q) (where i=1 for the \ufb01rst term). Similarly each edge, e has an associated active edge variable with the word overlap score as its coef\ufb01cient, ce. For ef\ufb01ciency, we only create qterm\u2192\ufb01eld edge and \ufb01eld\u2192choice edge if the coef\ufb01cient is greater than a certain threshold (0.1 and 0.2, respectively). Finally the objective function of our ILP model can be written as: X q\u2208qterms cqxq + X t\u2208tuples ctxt + X e\u2208edges cexe Description Var. Coef\ufb01cient (c) Qterm xq 0.8\u00b7idfB\u00b7scienceB\u00b7locB Tuple xt -1 + jaccardScore(t, qa) Tuple Field xf 0 Choice xa 0 Table 4: Coef\ufb01cients for active variables. Constraints Next we describe the constraints in our model. We have basic de\ufb01nitional constraints over the active variables.",
  "Constraints Next we describe the constraints in our model. We have basic de\ufb01nitional constraints over the active variables. Active variable must have an active edge Active edge must have an active source node Active edge must have an active target node Exactly one answer choice must be active Active \ufb01eld implies tuple must be active 13http://factorie.cs.umass.edu/ Apart from the constraints described in Table 1, we also use the which-term boosting constraints de\ufb01ned by TABLEILP (Eqns. 44 and 45 in Table 13 (Khashabi et al., 2016)). As described in Sec- tion B, we create a tuple from table rows by setting pairs of cells as the subject and object of a tuple. For these tuples, apart from requiring the subject to be active, we also require the object of the tuple. This would be equivalent to requiring at least two cells of a table row to be active. B Experiment Details We use the SCIP ILP optimization engine (Achter- berg, 2009) to optimize our ILP model.",
  "This would be equivalent to requiring at least two cells of a table row to be active. B Experiment Details We use the SCIP ILP optimization engine (Achter- berg, 2009) to optimize our ILP model. To get the score for each answer choice ai, we force the active variable for that choice xai to be one and use the objective function value of the ILP model as the score. For evaluations, we use a 2-core 2.5 GHz Amazon EC2 linux machine with 16 GB RAM. To evaluate TABLEILP and TUPLEINF on curated tables and tuples, we converted them into the expected format of each solver as follows. B.1 Using curated tables with TUPLEINF For each question, we select the 7 best matching tables using the tf-idf score of the table w.r.t. the question tokens and top 20 rows from each table using the Jaccard similarity of the row with the question. (same as Khashabi et al. (2016)). We then convert the table rows into the tuple structure using the relations de\ufb01ned by TABLEILP.",
  "the question tokens and top 20 rows from each table using the Jaccard similarity of the row with the question. (same as Khashabi et al. (2016)). We then convert the table rows into the tuple structure using the relations de\ufb01ned by TABLEILP. For ev- ery pair of cells connected by a relation, we cre- ate a tuple with the two cells as the subject and primary object with the relation as the predicate. The other cells of the table are used as additional objects to provide context to the solver. We pick top-scoring 50 tuples using the Jaccard score. B.2 Using Open IE tuples with TABLEILP We create an additional table in TABLEILP with all the tuples in T. Since TABLEILP uses \ufb01xed-length (subject; predicate; object) triples, we need to map tuples with multiple objects to this format. For each object, Oi in the input Open IE tuple (S; P; O1; O2 . . .), we add a triple (S; P; Oi) to this table."
]