{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Dynamic Compositionality in Recursive Neural Networks with Structure-aware Tag Representations \u2020Taeuk Kim, \u2020Jihun Choi, \u2021Daniel Edmiston, \u2020Sanghwan Bae, \u2020Sang-goo Lee \u2020Department of Computer Science and Engineering, Seoul National University, Seoul, Korea {taeuk, jhchoi, sanghwan, sglee}@europa.snu.ac.kr \u2021Department of Linguistics, University of Chicago, Chicago, IL, USA danedmiston@uchicago.edu Abstract Most existing recursive neural network (RvNN) architectures utilize only the structure of parse trees, ignoring syntactic tags which are provided as by-products of parsing. We present a novel RvNN architecture that can provide dynamic composi- tionality by considering comprehensive syntactic information derived from both the structure and linguistic tags. Speci\ufb01- cally, we introduce a structure-aware tag representation con- structed by a separate tag-level tree-LSTM. With this, we can control the composition function of the existing word- level tree-LSTM by augmenting the representation as a sup- plementary input to the gate functions of the tree-LSTM.",
            "With this, we can control the composition function of the existing word- level tree-LSTM by augmenting the representation as a sup- plementary input to the gate functions of the tree-LSTM. In extensive experiments, we show that models built upon the proposed architecture obtain superior or competitive perfor- mance on several sentence-level tasks such as sentiment anal- ysis and natural language inference when compared against previous tree-structured models and other sophisticated neu- ral models. 1 Introduction One of the most fundamental topics in natural language pro- cessing is how best to derive high-level representations from constituent parts, as natural language meanings are a func- tion of their constituent parts. How best to construct a sen- tence representation from distributed word embeddings is an example domain of this larger issue. Even though sequen- tial neural models such as recurrent neural networks (RNN) (Elman 1990) and their variants including Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber 1997) and Gated Recurrent Unit (GRU) (Cho et al.",
            "2014) have become the de-facto standard for condensing sentence-level informa- tion from a sequence of words into a \ufb01xed vector, there have been many lines of research towards better sentence repre- sentation using other neural architectures, e.g. convolutional neural networks (CNN) (Kim 2014) or self-attention based models (Shen et al. 2018). From a linguistic point of view, the underlying tree structure\u2014as expressed by its constituency and dependency trees\u2014of a sentence is an integral part of its meaning. In- spired by this fact, some recursive neural network (RvNN1) Copyright c\u20dd2019, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved. 1To avoid confusion, we call recursive neural networks (or tree- structured NNs) RvNNs to distinguish them from recurrent neural models are designed to re\ufb02ect the syntactic tree structure, achieving impressive results on several sentence-level tasks such as sentiment analysis (Socher et al. 2012; Socher et al. 2013), machine translation (Yang et al.",
            "2012; Socher et al. 2013), machine translation (Yang et al. 2017), natural lan- guage inference (Bowman et al. 2016), and discourse rela- tion classi\ufb01cation (Wang et al. 2017). However, some recent works have (Yogatama et al. 2017; Choi, Yoo, and Lee 2018) proposed latent tree models, which learn to construct task-speci\ufb01c tree structures with- out explicit supervision, bringing into question the value of linguistically-motivated recursive neural models. Witness- ing the surprising performance of the latent tree models on some sentence-level tasks, there arises a natural question: Are linguistic tree structures the optimal way of composing sentence representations for NLP tasks? In this paper, we demonstrate that linguistic priors are in fact useful for devising effective neural models for sentence representations, showing that our novel architecture based on constituency trees and their tag2 information obtains su- perior performance on several sentence-level tasks, includ- ing sentiment analysis and natural language inference.",
            "In this paper, we demonstrate that linguistic priors are in fact useful for devising effective neural models for sentence representations, showing that our novel architecture based on constituency trees and their tag2 information obtains su- perior performance on several sentence-level tasks, includ- ing sentiment analysis and natural language inference. A chief novelty of our approach is that we introduce a small separate tag-level tree-LSTM to control the composi- tion function of the existing word-level tree-LSTM, which is in charge of extracting helpful syntactic signals for mean- ingful semantic composition of constituents by considering both the structures and linguistic tags of constituency trees simultaneously. In addition, we demonstrate that applying a typical LSTM to preprocess the leaf nodes of a tree-LSTM greatly improves the performance of the tree models. More- over, we propose a clustered tag set to replace the existing tags on the assumption that the original syntactic tags are too \ufb01ned-grained to be useful in neural models. In short, our contributions in this work are as follows: \u2022 We propose a new linguistically-motivated neural model which generates high-quality sentence representations by considering all the information extracted from con- stituency parse trees.",
            "In short, our contributions in this work are as follows: \u2022 We propose a new linguistically-motivated neural model which generates high-quality sentence representations by considering all the information extracted from con- stituency parse trees. \u2022 In addition, we demonstrate the superiority of the pro- networks RNNs, following the convention of some previous works. 2In this work, we refer to both part-of-speech (POS) tags (e.g. DT-determiner, JJ-adjective) for words and phrase-level tags (e.g. NP-noun phrase, VP-verb phrase) simply as \u2018tags\u2019. arXiv:1809.02286v2  [cs.CL]  26 Nov 2018",
            "posed models achieving new state-of-the-art performance within the similar model class on 4 out of 5 sentence clas- si\ufb01cation benchmarks, as well as showing competitive re- sults compared to other types of neural models. \u2022 We empirically show that another key point to the success of tree-structured models is to contextualize input word embeddings so that the corresponding input for each word in a sentence can better re\ufb02ect the meaning of the whole sentence. 2 Related Work Recursive neural networks (RvNN) are a kind of neural ar- chitecture which model sentences by exploiting syntactic structure. While earlier RvNN models proposed utilizing di- verse composition functions, including feed-forward neural networks (Socher et al. 2011), matrix-vector multiplication (Socher et al. 2012), and tensor computation (Socher et al. 2013), tree-LSTMs (Tai, Socher, and Manning 2015) remain the standard for several sentence-level tasks. Even though classic RvNNs have demonstrated superior performance on a variety of tasks, their in\ufb02exibility, i.e.",
            "2013), tree-LSTMs (Tai, Socher, and Manning 2015) remain the standard for several sentence-level tasks. Even though classic RvNNs have demonstrated superior performance on a variety of tasks, their in\ufb02exibility, i.e. their inability to handle dynamic compositionality for different syntactic con\ufb01gurations, is a considerable weakness. For in- stance, it would be desirable if our model could distinguish e.g. adjective-noun composition from that of verb-noun or preposition-noun composition, as models failing to make such a distinction ignore real-world syntactic considerations such as \u2018-arity\u2019 of function words (i.e. types), and the ad- junct\/argument distinction. To enable dynamic compositionality in recursive neural networks, many previous works (Hashimoto et al. 2013; Dong et al. 2014; Qian et al. 2015; Wang et al. 2017; Liu, Qiu, and Huang 2017b; Huang, Qian, and Zhu 2017; Teng and Zhang 2017) have proposed various methods.",
            "2014; Qian et al. 2015; Wang et al. 2017; Liu, Qiu, and Huang 2017b; Huang, Qian, and Zhu 2017; Teng and Zhang 2017) have proposed various methods. One main direction of research leverages tag information, which is produced as a by-product of parsing. In detail, Qian et al. (2015) suggested TG-RNN, a model employing differ- ent composition functions according to POS tags, and TE- RNN\/TE-RNTN, models which leverage tag embeddings as additional inputs for the existing tree-structured models. De- spite the novelty of utilizing tag information, the explosion of the number of parameters (in case of the TG-RNN) and the limited performance of the original models (in case of the TE-RNN\/TE-RNTN) have prevented these models from being widely adopted. Meanwhile, Wang et al. (2017) and Huang, Qian, and Zhu (2017) proposed models based on a tree-LSTM which also uses the tag vectors to control the gate functions of the tree-LSTM.",
            "Meanwhile, Wang et al. (2017) and Huang, Qian, and Zhu (2017) proposed models based on a tree-LSTM which also uses the tag vectors to control the gate functions of the tree-LSTM. In spite of their impressive results, there is a limitation that the trained tag embeddings are too simple to re\ufb02ect the rich information which tags pro- vide in different syntactic structures. To alleviate this prob- lem, we introduce structure-aware tag representations in the next section. Another way of building dynamic compositionality into RvNNs is to take advantage of a meta-network (or hyper- network). Inspired by recent works on dynamic parameter prediction, DC-TreeLSTMs (Liu, Qiu, and Huang 2017b) dynamically create the parameters for compositional func- tions in a tree-LSTM. Speci\ufb01cally, the model has two sep- arate tree-LSTM networks whose architectures are similar, but the smaller of the two is utilized to calculate the weights of the bigger one.",
            "Speci\ufb01cally, the model has two sep- arate tree-LSTM networks whose architectures are similar, but the smaller of the two is utilized to calculate the weights of the bigger one. A possible problem for this model is that it may be easy to be trained such that the role of each tree- LSTM is ambiguous, as they share the same input, i.e. word information. Therefore, we design two disentangled tree- LSTMs in our model so that one focuses on extracting use- ful features from only syntactic information while the other composes semantic units with the aid of the features. Fur- thermore, our model reduces the complexity of computation by utilizing typical tree-LSTM frameworks instead of com- puting the weights for each example. Finally, some recent works (Yogatama et al. 2017; Choi, Yoo, and Lee 2018) have proposed latent tree-structured models that learn how to formulate tree structures from only sequences of tokens, without the aid of syntactic trees or lin- guistic information.",
            "Finally, some recent works (Yogatama et al. 2017; Choi, Yoo, and Lee 2018) have proposed latent tree-structured models that learn how to formulate tree structures from only sequences of tokens, without the aid of syntactic trees or lin- guistic information. The latent tree models have the advan- tage of being able to \ufb01nd the optimized task-speci\ufb01c order of composition rather than a sequential or syntactic one. In experiments, we compare our model with not only syntactic tree-based models but also latent tree models, demonstrating that modeling with explicit linguistic knowledge can be an attractive option. 3 Model In this section, we introduce a novel RvNN architec- ture, called SATA Tree-LSTM3 (Structure-Aware Tag Augmented Tree-LSTM). This model is similar to typi- cal Tree-LSTMs, but provides dynamic compositionality by augmenting a separate tag-level tree-LSTM which produces structure-aware tag representations for each node in a tree.",
            "This model is similar to typi- cal Tree-LSTMs, but provides dynamic compositionality by augmenting a separate tag-level tree-LSTM which produces structure-aware tag representations for each node in a tree. In other words, our model has two independent tree-structured modules based on the same constituency tree, one of which (word-level tree-LSTM) is responsible for constructing sen- tence representations given a sequence of words as usual, while the other (tag-level tree-LSTM) provides supplemen- tary syntactic information to the former. In section 3.1, we \ufb01rst review tree-LSTM architectures. Then in section 3.2, we introduce a tag-level tree-LSTM and structure-aware tag representations. In section 3.3, we dis- cuss an additional technique to boost the performance of tree-structured models, and in section 3.4, we describe the entire architecture of our model in detail. 3.1 Tree-LSTM The LSTM (Hochreiter and Schmidhuber 1997) architecture was \ufb01rst introduced as an extension of the RNN architecture to mitigate the vanishing and exploding gradient problems.",
            "3.1 Tree-LSTM The LSTM (Hochreiter and Schmidhuber 1997) architecture was \ufb01rst introduced as an extension of the RNN architecture to mitigate the vanishing and exploding gradient problems. In addition, several works have discovered that applying the LSTM cell into tree structures can be an effective means of modeling sentence representations. To be formal, the composition function of the cell in a 3The implementation of our model and supplemental materials are available at https:\/\/github.com\/galsang\/SATA-Tree-LSTM.",
            "tree-LSTM can be formulated as follows: \uf8ee \uf8ef\uf8ef\uf8ef\uf8f0 i fl fr o g \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fb= \uf8ee \uf8ef\uf8ef\uf8ef\uf8f0 \u03c3 \u03c3 \u03c3 \u03c3 tanh \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fb   W \u0014 hl hr \u0015 + b ! (1) c = fl \u2299cl + fr \u2299cr + i \u2299g (2) h = o \u2299tanh (c) (3) where h, c \u2208Rd indicate the hidden state and cell state of the LSTM cell, and hl, hr, cl, cr \u2208Rd the hidden states and cell states of a left and right child. g \u2208Rd is the newly composed input for the cell and i, fl, fr, o \u2208Rd represent an input gate, two forget gates (left, right), and an output gate respectively. W \u2208R5d\u00d72d and b \u2208R5d are trainable pa- rameters.",
            "W \u2208R5d\u00d72d and b \u2208R5d are trainable pa- rameters. \u03c3 corresponds to the sigmoid function, tanh to the hyperbolic tangent, and \u2299to element-wise multiplication. Note the equations assume that there are only two chil- dren for each node, i.e. binary or binarized trees, following the standard in the literature. While RvNN models can be constructed on any tree structure, in this work we only con- sider constituency trees as inputs. In spite of the obvious upside that recursive models have in being so \ufb02exible, they are known for being dif\ufb01cult to fully utilize with batch computations as compared to other neural architectures because of the diversity of structure found across sentences. To alleviate this problem, Bowman et al. (2016) proposed the SPINN model, which brings a shift-reduce algorithm to the tree-LSTM. As SPINN sim- pli\ufb01es the process of constructing a tree into only two op- erations, i.e. shift and reduce, it can support more effective parallel computations while enjoying the advantages of tree structures.",
            "As SPINN sim- pli\ufb01es the process of constructing a tree into only two op- erations, i.e. shift and reduce, it can support more effective parallel computations while enjoying the advantages of tree structures. For ef\ufb01ciency, our model also starts from our own SPINN re-implementation, whose function is exactly the same as that of the tree-LSTM. 3.2 Structure-aware Tag Representation In most previous works using linguistic tag information (Qian et al. 2015; Wang et al. 2017; Huang, Qian, and Zhu 2017), tags are usually represented as simple low- dimensional dense vectors, similar to word embeddings. This approach seems reasonable in the case of POS tags that are attached to the corresponding words, but phrase-level constituent tags (e.g. NP, VP, ADJP) vary greatly in size and shape, making them less amenable to uniform treatment. For instance, even the same phrase tags within different syntactic contexts can vary greatly in size and internal structure, as the case of NP tags in Figure 1 shows.",
            "NP, VP, ADJP) vary greatly in size and shape, making them less amenable to uniform treatment. For instance, even the same phrase tags within different syntactic contexts can vary greatly in size and internal structure, as the case of NP tags in Figure 1 shows. Here, the NP consisting of DT[the]-NN[stories] has a different internal structure than the NP consisting of NP[the \ufb01lm \u2019s]-NNS[shortcomings]. One way of deriving structure-aware tag representations from the original tag embeddings is to introduce a sepa- rate tag-level tree-LSTM which accepts the typical tag em- beddings at each node of a tree and outputs the computed structure-aware tag representations for the nodes. Note that the module concentrates on extracting useful syntactic fea- tures by considering only the tags and structures of the trees, excluding word information. ROOT PP IN Despite NP NP DT the @NP NN film POS 's NNS shortcomings @S , , @S NP DT the NNS stories @S VP @VP VBP are ADVP quietly VP moving . . Figure 1: A constituency tree example from Stanford Senti- ment Treebank.",
            ". Figure 1: A constituency tree example from Stanford Senti- ment Treebank. Formally, we denote a tag embedding for the tag attached to each node in a tree as e \u2208RdT. Then, the function of each cell in the tag tree-LSTM is de\ufb01ned in the following way. Leaf nodes are de\ufb01ned by the following: \u0014\u02c6c \u02c6h \u0015 = tanh (UTe + aT) (4) while non-leaf nodes are de\ufb01ned by the following: \uf8ee \uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 \u02c6i \u02c6fl \u02c6fr \u02c6o \u02c6g \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fa\uf8fb = \uf8ee \uf8ef\uf8ef\uf8ef\uf8f0 \u03c3 \u03c3 \u03c3 \u03c3 tanh \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fb   WT \uf8ee \uf8f0 \u02c6hl \u02c6hr e \uf8f9 \uf8fb+ bT !",
            "(5) \u02c6c = \u02c6fl \u2299\u02c6cl + \u02c6fr \u2299\u02c6cr +\u02c6i \u2299\u02c6g (6) \u02c6h = \u02c6o \u2299tanh (\u02c6c) (7) where \u02c6h, \u02c6c \u2208RdT represent the hidden state and cell state of each node in the tag tree-LSTM. We regard the hid- den state (\u02c6h) as a structure-aware tag representation for the node. UT \u2208R2dT\u00d7dT, aT \u2208R2dT, WT \u2208R5dT\u00d73dT, and bT \u2208R5dT are trainable parameters. The rest of the nota- tion follows equations 1, 2, and 3. In case of leaf nodes, the states are computed by a simple non-linear transformation. Meanwhile, the composition function in a non-leaf node ab- sorbs the tag embedding (e) as an additional input as well as the hidden states of the two children nodes.",
            "In case of leaf nodes, the states are computed by a simple non-linear transformation. Meanwhile, the composition function in a non-leaf node ab- sorbs the tag embedding (e) as an additional input as well as the hidden states of the two children nodes. The bene- \ufb01t of revising tag representations according to the internal structure is that the derived embedding is a function of the corresponding makeup of the node, rather than a monolithic, categorical tag. With regard to the tags themselves, we conjecture that the taxonomy of the tags currently in use in many NLP systems is too complex to be utilized effectively in deep neural mod- els, considering the speci\ufb01city of many tag sets and the lim- ited amount of data with which to train. Thus, we cluster POS (word-level) tags into 12 groups following the universal POS tagset (Petrov, Das, and McDonald 2012) and phrase- level tags into 11 groups according to criteria analogous to the case of words, resulting in 23 tag categories in total. In this work, we use the revised coarse-grained tags instead of the original ones. For more details, we refer readers to the supplemental materials.",
            "3.3 Leaf-LSTM An inherent shortcoming of RvNNs relative to sequential models is that each intermediate representation in a tree is unaware of its external context until all the information is gathered together at the root node. In other words, each com- position process is prone to be locally optimized rather than globally optimized. To mitigate this problem, we propose using a leaf-LSTM following the convention of some previous works (Eriguchi, Hashimoto, and Tsuruoka 2016; Yang et al. 2017; Choi, Yoo, and Lee 2018), which is a typical LSTM that accepts a sequence of words in order. Instead of leveraging word embeddings directly, we can use each hidden state and cell state of the leaf-LSTM as input tokens for leaf nodes in a tree-LSTM, anticipating the proper contextualization of the input sequence.",
            "Instead of leveraging word embeddings directly, we can use each hidden state and cell state of the leaf-LSTM as input tokens for leaf nodes in a tree-LSTM, anticipating the proper contextualization of the input sequence. Formally, we denote a sequence of words in an input sen- tence as w1:n (n: the length of the sentence), and the corre- sponding word embeddings as x1:n. Then, the operation of the leaf-LSTM at time t can be formulated as, \uf8ee \uf8ef\uf8ef\uf8f0 \u02dci \u02dcf \u02dco \u02dcg \uf8f9 \uf8fa\uf8fa\uf8fb= \uf8ee \uf8ef\uf8f0 \u03c3 \u03c3 \u03c3 tanh \uf8f9 \uf8fa\uf8fb   WL \u0014\u02dcht\u22121 xt \u0015 + bL !",
            "(8) \u02dcct = \u02dcf \u2299\u02dcct\u22121 +\u02dci \u2299\u02dcg (9) \u02dcht = \u02dco \u2299tanh (\u02dcct) (10) where xt \u2208Rdw indicates an input word vector and \u02dcht, \u02dcct \u2208 Rdh represent the hidden and cell state of the LSTM at time t (\u02dcht\u22121 corresponds to the hidden state at time t-1). WL and bL are learnable parameters. The remaining notation follows that of the tree-LSTM above. In experiments, we demonstrate that introducing a leaf- LSTM fares better at processing the input words of a tree- LSTM compared to using a feed-forward neural network. We also explore the possibility of its bidirectional setting in ablation study. 3.4 SATA Tree-LSTM In this section, we de\ufb01ne SATA Tree-LSTM (Structure- Aware Tag Augmented Tree-LSTM, see Figure 2) which joins a tag-level tree-LSTM (section 3.2), a leaf-LSTM (sec- tion 3.3), and the original word tree-LSTM together.",
            "As above we denote a sequence of words in an input sen- tence as w1:n and the corresponding word embeddings as x1:n. In addition, a tag embedding for the tag attached to each node in a tree is denoted by e \u2208RdT. Then, we derive the \ufb01nal sentence representation for the input sentence with our model in two steps. First, we compute structure-aware tag representations (\u02c6h) for each node of a tree using the tag tree-LSTM (the right side of Figure 2) as follows: \u0014\u02c6c \u02c6h \u0015 = ( Tag-Tree-LSTM(e) if a leaf node Tag-Tree-LSTM(\u02c6hl, \u02c6hr, e) otherwise (11) I this loved film . Word tree-LSTM NP DT VBD NN . NP VP @S ROOT Tag tree-LSTM Leaf-LSTM Word Embedding Fully-connected Tag Embedding Figure 2: A diagram of SATA Tree-LSTM.",
            "Word tree-LSTM NP DT VBD NN . NP VP @S ROOT Tag tree-LSTM Leaf-LSTM Word Embedding Fully-connected Tag Embedding Figure 2: A diagram of SATA Tree-LSTM. The model has two separate tree-LSTM modules, the right of which (tag tree-LSTM) extracts a structure-aware tag represen- tation to control the composition function of the remain- ing tree-LSTM (word tree-LSTM). Fully-connected: one- layered non-linear transformation. where Tag-Tree-LSTM indicates the module we described in section 3.2. Second, we combine semantic units recursively on the word tree-LSTM in a bottom-up fashion. For leaf nodes, we leverage the Leaf-LSTM (the bottom-left of Figure 2, ex- plained in section 3.3) to compute \u02dcct and \u02dcht in sequential order, with the corresponding input xt.",
            "For leaf nodes, we leverage the Leaf-LSTM (the bottom-left of Figure 2, ex- plained in section 3.3) to compute \u02dcct and \u02dcht in sequential order, with the corresponding input xt. \u0014\u02dcct \u02dcht \u0015 = Leaf-LSTM(\u02dcht\u22121, xt) (12) Then, the \u02dcct and \u02dcht can be utilized as input tokens to the word tree-LSTM, with the left (right) child of the target node corresponding to the tth word in the input sentence.",
            "\u0014\u02c7c{l,r} \u02c7h{l,r} \u0015 = \u0014\u02dcct \u02dcht \u0015 (13) In the non-leaf node case, we calculate phrase represen- tations for each node in the word tree-LSTM (the upper-left of Figure 2) recursively as follows: \u02c7g = tanh \u0012 Uw \u0014\u02c7hl \u02c7hr \u0015 + aw \u0013 (14) \uf8ee \uf8ef\uf8ef\uf8f0 \u02c7i \u02c7fl \u02c7fr \u02c7o \uf8f9 \uf8fa\uf8fa\uf8fb= \uf8ee \uf8ef\uf8f0 \u03c3 \u03c3 \u03c3 \u03c3 \uf8f9 \uf8fa\uf8fb   Ww \uf8ee \uf8f0 \u02c7hl \u02c7hr \u02c6h \uf8f9 \uf8fb+ bw ! (15) \u02c7c = \u02c7fl \u2299\u02c7cl + \u02c7fr \u2299\u02c7cr +\u02c7i \u2299\u02c7g (16) \u02c7h = \u02c7o \u2299tanh (\u02c7c) (17)",
            "where \u02c7h, \u02c7c \u2208Rdh represent the hidden and cell state of each node in the word tree-LSTM. Uw \u2208Rdh\u00d72dh, Ww \u2208 R4dh\u00d7(2dh+dT), aw \u2208Rdh, bw \u2208R4dh are learned param- eters. The remaining notation follows those of the previous sections. Note that the structure-aware tag representations (\u02c6h) are only utilized to control the gate functions of the word tree-LSTM in the form of additional inputs, and are not in- volved in the semantic composition (\u02c7g) directly. Finally, the hidden state of the root node (\u02c7hroot) in the word-level tree-LSTM becomes the \ufb01nal sentence represen- tation of the input sentence. 4 Experiment and Discussion 4.1 Quantitative Analysis Sentence classi\ufb01cation tasks One of the most basic ap- proaches to evaluate a sentence encoder is to measure the classi\ufb01cation performance with the sentence representations made by the encoder. Thus, we conduct experiments on the following \ufb01ve datasets. (Summary statistics for the datasets are reported in the supplemental materials.)",
            "Thus, we conduct experiments on the following \ufb01ve datasets. (Summary statistics for the datasets are reported in the supplemental materials.) \u2022 MR: A group of movie reviews with binary (positive \/ negative) classes. (Pang and Lee 2005) \u2022 SST-2: Stanford Sentiment Treebank (Socher et al. 2013). Similar to MR, but each review is provided in the form of a binary parse tree whose nodes are annotated with nu- meric sentiment values. For SST-2, we only consider bi- nary (positive \/ negative) classes. \u2022 SST-5: Identical to SST-2, but the reviews are grouped into \ufb01ne-grained (very negative, negative, neutral, posi- tive, very positive) classes. \u2022 SUBJ: Sentences grouped as being either subjective or objective (binary classes). (Pang and Lee 2004) \u2022 TREC: A dataset which groups questions into six differ- ent question types (classes). (Li and Roth 2002) As a preprocessing step, we construct parse trees for the sentences in the datasets using the Stanford PCFG parser (Klein and Manning 2003).",
            "(Li and Roth 2002) As a preprocessing step, we construct parse trees for the sentences in the datasets using the Stanford PCFG parser (Klein and Manning 2003). Because syntactic tags are by- products of constituency parsing, we do not need further pre- processing. To classify the sentence given our sentence representa- tion (\u02c7hroot), we use one fully-connected layer with a ReLU activation, followed by a softmax classi\ufb01er. The \ufb01nal pre- dicted probability distribution of the class y given the sen- tence w1:n is de\ufb01ned as follows, s = ReLU(Ws\u02c7hroot + bs) (18) p(y|w1:n) = softmax(Wcs + bc) (19) where s \u2208Rds is the computed task-speci\ufb01c sentence repre- sentation for the classi\ufb01er, and Ws \u2208Rds\u00d7dh, Wc \u2208Rdc\u00d7ds, bs \u2208Rds, bc \u2208Rdc are trainable parameters. As an objec- tive function, we use the cross entropy of the predicted and true class distributions.",
            "As an objec- tive function, we use the cross entropy of the predicted and true class distributions. The results of the experiments on the \ufb01ve datasets are shown in table 1. In this table, we report the test accuracy of our model and various other models on each dataset in terms of percentage. To consider the effects of random ini- tialization, we report the best numbers obtained from each several runs with hyper-parameters \ufb01xed. Compared with the previous syntactic tree-based models as well as other neural models, our SATA Tree-LSTM shows superior or competitive performance on all tasks. Speci\ufb01- cally, our model achieves new state-of-the-art results within the tree-structured model class on 4 out of 5 sentence clas- si\ufb01cation tasks\u2014SST-2, SST-5, MR, and TREC. The model shows its strength, in particular, when the datasets provide phrase-level supervision to facilitate tree structure learning (i.e. SST-2, SST-5).",
            "The model shows its strength, in particular, when the datasets provide phrase-level supervision to facilitate tree structure learning (i.e. SST-2, SST-5). Moreover, the numbers we report for SST-5 and TREC are competitive to the existing state-of- the-art results including ones from structurally pre-trained models such as ELMo (Peters et al. 2018), proving our model\u2019s superiority. Note that the SATA Tree-LSTM also outperforms the recent latent tree-based model, indicating that modeling a neural model with explicit linguistic knowl- edge can be an attractive option. On the other hand, a remaining concern is that our SATA Tree-LSTM is not robust to random seeds when the size of a dataset is relatively small, as tag embeddings are randomly initialized rather than relying on pre-trained ones in contrast with the case of words. From this observation, we could \ufb01nd out there needs a direction of research towards pre-trained tag embeddings. Natural language inference To estimate the performance of our model beyond the tasks requiring only one sentence at a time, we conduct an experiment on the Stanford Natural Language Inference (Bowman et al.",
            "From this observation, we could \ufb01nd out there needs a direction of research towards pre-trained tag embeddings. Natural language inference To estimate the performance of our model beyond the tasks requiring only one sentence at a time, we conduct an experiment on the Stanford Natural Language Inference (Bowman et al. 2015) dataset, each ex- ample of which consists of two sentences, the premise and the hypothesis. Our objective given the data is to predict the correct relationship between the two sentences among three options\u2014 contradiction, neutral, or entailment. We use the siamese architecture to encode both the premise (p1:m) and hypothesis (h1:n) following the stan- dard of sentence-encoding models in the literature. (Specif- ically, p1:m is encoded as \u02c7hp root \u2208Rdh and h1:n is encoded as \u02c7hh root \u2208Rdh with the same encoder.) Then, we leverage some heuristics (Mou et al. 2016), followed by one fully- connected layer with a ReLU activation and a softmax clas- si\ufb01er.",
            "Then, we leverage some heuristics (Mou et al. 2016), followed by one fully- connected layer with a ReLU activation and a softmax clas- si\ufb01er. Speci\ufb01cally, z = \u0002\u02c7hp root; \u02c7hh root; |\u02c7hp root \u2212\u02c7hh root|; \u02c7hp root \u2299\u02c7hh root \u0003 (20) s = ReLU(Wsz + bs) (21) p(y|p1:m, h1:n) = softmax(Wcs + bc) (22) where z \u2208R4dh, s \u2208Rds are intermediate features for the classi\ufb01er and Ws \u2208Rds\u00d74dh, Wc \u2208Rdc\u00d7ds, bs \u2208Rds, bc \u2208Rdc are again trainable parameters. Our experimental results on the SNLI dataset are shown in table 2. In this table, we report the test accuracy and number of trainable parameters for each model. Our SATA- LSTM again demonstrates its decent performance compared against the neural models built on both syntactic trees and latent trees, as well as the non-tree models.",
            "In this table, we report the test accuracy and number of trainable parameters for each model. Our SATA- LSTM again demonstrates its decent performance compared against the neural models built on both syntactic trees and latent trees, as well as the non-tree models. (Latent Syn- tax Tree-LSTM: Yogatama et al. (2017), Tree-based CNN: Mou et al. (2016), Gumbel Tree-LSTM: Choi, Yoo, and Lee",
            "Models SST-2 SST-5 MR SUBJ TREC Tree-structured models RNTN (Socher et al. 2013) 85.4 45.7 - - - AdaMC-RNTN (Dong et al. 2014) 88.5 46.7 - - - TE-RNTN (Qian et al. 2015) 87.7 49.8 - - - TBCNN (Mou et al.",
            "2014) 88.5 46.7 - - - TE-RNTN (Qian et al. 2015) 87.7 49.8 - - - TBCNN (Mou et al. 2015) 87.9 51.4 - - 96.0 Tree-LSTM (Tai, Socher, and Manning 2015) 88.0 51.0 - - - AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017a) 87.8 50.2 81.9 94.1 - DC-TreeLSTM (Liu, Qiu, and Huang 2017b) 87.8 - 81.7 93.7 93.8 TE-LSTM (Huang, Qian, and Zhu 2017) 89.6 52.6 82.2 - - BiConTree (Teng and Zhang 2017) 90.3 53.5 - - 94.8 Gumbel Tree-LSTM\u22c6(Choi, Yoo, and Lee 2018) 90.7 53.7 - - - TreeNet (Cheng et al.",
            "2018) - - 83.6 95.9 96.1 SATA Tree-LSTM (Ours) 91.3 54.4 83.8 95.4 96.2 Other neural models CNN (Kim 2014) 88.1 48.0 81.5 93.4 93.6 AdaSent (Zhao, Lu, and Poupart 2015) - - 83.1 95.5 92.4 LSTM-CNN (Zhou et al. 2016) 89.5 52.4 82.3 94.0 96.1 byte-mLSTM\u2020 (Radford, Jozefowicz, and Sutskever 2017) 91.8 52.9 86.9 94.6 - BCN + Char + CoVe\u2020 (McCann et al. 2017) 90.3 53.7 - - 95.8 BCN + Char + ELMo\u2020 (Peters et al.",
            "2017) 90.3 53.7 - - 95.8 BCN + Char + ELMo\u2020 (Peters et al. 2018) - 54.7\u00b10.5 - - - Table 1: The comparison of various models on different sentence classi\ufb01cation tasks. We report the test accuracy of each model in percentage. Our SATA Tree-LSTM shows superior or competitive performance on all tasks, compared to previous tree- structured models as well as other sophisticated models. \u22c6: Latent tree-structured models. \u2020: Models which are pre-trained with large external corpora. (2018), NSE: Munkhdalai and Yu (2017), Reinforced Self- Attention Network: Shen et al. (2018), Residual stacked en- coders: Nie and Bansal (2017), BiLSTM with generalized pooling: Chen, Ling, and Zhu (2018).) Note that the num- ber of learned parameters in our model is also comparable to other sophisticated models, showing the ef\ufb01ciency of our model.",
            "Note that the num- ber of learned parameters in our model is also comparable to other sophisticated models, showing the ef\ufb01ciency of our model. Even though our model has proven its mettle, the effect of tag information seems relatively weak in the case of SNLI, which contains a large amount of data compared to the oth- ers. One possible explanation is that neural models may learn some syntactic rules from large amounts of text when the text size is large enough, reducing the necessity of ex- ternal linguistic knowledge. We leave the exploration of the effectiveness of tags relative to data size for future work. Experimental details Here we go over the settings com- mon across our models during experimentation. For more task-speci\ufb01c details, refer to the supplemental materials. For our input embeddings, we used 300 dimensional 840B GloVe (Pennington, Socher, and Manning 2014) as pre-trained word embeddings, and tag representations were randomly sampled from the uniform distribution [-0.005, 0.005]. Tag vectors are revised during training while the \ufb01ne-tuning of the word embedding depends on the task.",
            "Tag vectors are revised during training while the \ufb01ne-tuning of the word embedding depends on the task. Our models were trained using the Adam (Kingma and Ba 2014) or Adadelta (Zeiler 2012) optimizer, depending on task. For regularization, weight decay is added to the loss function ex- cept for SNLI following Loshchilov and Hutter (2017) and Dropout (Srivastava et al. 2014) is also applied for the word embeddings and task-speci\ufb01c classi\ufb01ers. Moreover, batch normalization (Ioffe and Szegedy 2015) is adopted for the classi\ufb01ers. As a default, all the weights in the model are ini- tialized following He et al. (2015) and the biases are set to 0. The total norm of the gradients of the parameters is clipped Models Acc.",
            "As a default, all the weights in the model are ini- tialized following He et al. (2015) and the biases are set to 0. The total norm of the gradients of the parameters is clipped Models Acc. # Params Tree-structured models 100D Latent Syntax Tree-LSTM\u22c6 80.5 500K 300D Tree-based CNN 82.1 3.5M 300D SPINN-PI 83.2 3.7M 300D Gumbel Tree-LSTM\u22c6 85.6 2.9M 300D SATA Tree-LSTM (Ours) 85.9 3.3M Other neural models 300D NSE 84.6 3.0M 300D Reinforced Self-Attention Network 86.3 3.1M 600D Residual stacked encoders 86.0 29M 600D BiLSTM with generalized pooling 86.6 65M Table 2: The accuracy of diverse models on Stanford Natu- ral Language Inference. For fair comparison, we only con- sider sentence-encoding based models.",
            "For fair comparison, we only con- sider sentence-encoding based models. Our model achieves a comparable result with a moderate number of parameters. \u22c6: Latent tree models. not to be over 5 during training. Our best models for each dataset were chosen by valida- tion accuracy in cases where a validation set was provided as a part of the dataset. Otherwise, we perform a grid search on probable hyper-parameter settings, or run 10-fold cross- validation in cases where even a test set does not exist. 4.2 Ablation Study In this section, we design an ablation study on the core mod- ules of our model to explore their effectiveness. The dataset used in this experiment is SST-2. To conduct the experi- ment, we only replace the target module with other candi- dates while maintaining the other settings. To be speci\ufb01c, we focus on two modules, the leaf-LSTM and structure-aware tag embeddings (tag-level tree-LSTM). In the \ufb01rst case, the leaf-LSTM is replaced with a fully-connected layer with a tanh activation or Bi-LSTM. In the second case, we replace",
            "88.9  90.0  90.9  90.0  91.3  91.3  85 86 87 88 89 90 91 92 Accuracy (%) Figure 3: An ablation study on the core modules of our model. The test accuracy of each model on SST-2 is re- ported. The results demonstrate that the modules play an im- portant role for achieving the superior performance of our model. FC: A fully connected-layer with a tanh function. w\/o tags: Tag embeddings are not used. w\/ tags: The naive tag embeddings are directly inserted into each node of a tree. the structure-aware tag embeddings with naive tag embed- dings or do not employ them at all. The experimental results are depicted in Figure 3. As the chart shows, our model outperforms all the other op- tions we have considered. In detail, the left part of the chart shows that the leaf-LSTM is the most effective option com- pared to its competitors.",
            "The experimental results are depicted in Figure 3. As the chart shows, our model outperforms all the other op- tions we have considered. In detail, the left part of the chart shows that the leaf-LSTM is the most effective option com- pared to its competitors. Note that the sequential leaf-LSTM is somewhat superior or competitive than the bidirectional leaf-LSTM when both have a comparable number of param- eters. We conjecture this may because a backward LSTM does not add additional useful knowledge when the struc- ture of a sentence is already known. In conclusion, we use the uni-directional LSTM as a leaf module because of its simplicity and remarkable performance. Meanwhile, the right part of the \ufb01gure demonstrates that our newly introduced structure-aware embeddings have a real impact on improving the model performance. Interest- ingly, employing the naive tag embeddings made no differ- ence in terms of the test accuracy, even though the abso- lute validation accuracy increased (not reported in the \ufb01g- ure). This result supports our assumption that tag informa- tion should be considered in the structure.",
            "This result supports our assumption that tag informa- tion should be considered in the structure. 4.3 Qualitative Analysis In previous sections, we have numerically demonstrated that our model is effective in encouraging useful composition of semantic units. Here, we directly investigate the computed representations for each node of a tree, showing that the re- markable performance of our model is mainly due to the gradual and recursive composition of the intermediate rep- resentations on the syntactic structure. To observe the phrase-level embeddings at a glance, we draw a scatter plot in which a point represents the corre- sponding intermediate representation. We utilize PCA (Prin- cipal Component Analysis) to project the representations Figure 4: A scatter plot whose points represent the interme- diate representations for each node of the tree in Figure 1. From this \ufb01gure, we can see the tendency of constructing the representations recursively from the low to the high level. into a two-dimensional vector space. As a target parse tree, we reuse the one seen in Figure 1. The result is shown in Figure 4.",
            "From this \ufb01gure, we can see the tendency of constructing the representations recursively from the low to the high level. into a two-dimensional vector space. As a target parse tree, we reuse the one seen in Figure 1. The result is shown in Figure 4. From this \ufb01gure, we con\ufb01rm that the intermediate repre- sentations have a hierarchy in the semantic space, which is very similar to that of the parse tree. In other words, as many tree-structured models pursue, we can see the tendency of constructing the representations from the low-level (the bot- tom of the \ufb01gure) to the high-level (the top-left and top-right of the \ufb01gure), integrating the meaning of the constituents recursively. An interesting thing to note is that the \ufb01nal sen- tence representation is near that of the phrase \u2018, the stories are quietly moving.\u2019 rather than that of \u2018Despite the \ufb01lm\u2019s shortcomings\u2019, catching the main meaning of the sentence. 5 Conclusion We have proposed a novel RvNN architecture to fully utilize linguistic priors.",
            "5 Conclusion We have proposed a novel RvNN architecture to fully utilize linguistic priors. A newly introduced tag-level tree-LSTM demonstrates that it can effectively control the composition function of the corresponding word-level tree-LSTM. In ad- dition, the proper contextualization of the input word vectors results in signi\ufb01cant performance improvements on several sentence-level tasks. For future work, we plan to explore a new way of exploiting dependency trees effectively, similar to the case of constituency trees. Acknowledgments We thank anonymous reviewers for their constructive and fruitful comments. This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (NRF2016M3C4A7952587). References [2015] Bowman, S. R.; Angeli, G.; Potts, C.; and Manning, C. D. 2015. A large annotated corpus for learning natural language in- ference. In EMNLP.",
            "[2016] Bowman, S. R.; Gauthier, J.; Rastogi, A.; Gupta, R.; Man- ning, C. D.; and Potts, C. 2016. A fast uni\ufb01ed model for parsing and sentence understanding. In ACL, 1466\u20131477. [2018] Chen, Q.; Ling, Z.-H.; and Zhu, X. 2018. Enhancing sen- tence embedding with generalized pooling. In COLING, 1815\u2013 1826. [2018] Cheng, Z.; Yuan, C.; Li, J.; and Yang, H. 2018. Treenet: Learning sentence representations with unconstrained tree struc- ture. In IJCAI. [2014] Cho, K.; van Merrienboer, B.; Gulcehre, C.; Bahdanau, D.; Bougares, F.; Schwenk, H.; and Bengio, Y. 2014. Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation. In EMNLP, 1724\u20131734.",
            "2014. Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation. In EMNLP, 1724\u20131734. [2018] Choi, J.; Yoo, K. M.; and Lee, S.-g. 2018. Learning to compose task-speci\ufb01c tree structures. In AAAI. [2014] Dong, L.; Wei, F.; Zhou, M.; and Xu, K. 2014. Adaptive multi-compositionality for recursive neural models with applica- tions to sentiment analysis. In AAAI, 1537\u20131543. [1990] Elman, J. L. 1990. Finding structure in time. Cognitive science 14(2):179\u2013211. [2016] Eriguchi, A.; Hashimoto, K.; and Tsuruoka, Y. 2016. Tree- to-sequence attentional neural machine translation. In ACL, 823\u2013 833. [2013] Hashimoto, K.; Miwa, M.; Tsuruoka, Y.; and Chikayama, T. 2013.",
            "2016. Tree- to-sequence attentional neural machine translation. In ACL, 823\u2013 833. [2013] Hashimoto, K.; Miwa, M.; Tsuruoka, Y.; and Chikayama, T. 2013. Simple customization of recursive neural networks for semantic relation classi\ufb01cation. In EMNLP, 1372\u20131376. [2015] He, K.; Zhang, X.; Ren, S.; and Sun, J. 2015. Delving deep into recti\ufb01ers: Surpassing human-level performance on imagenet classi\ufb01cation. In ICCV, 1026\u20131034. [1997] Hochreiter, S., and Schmidhuber, J. 1997. Long short-term memory. Neural computation 9(8):1735\u20131780. [2017] Huang, M.; Qian, Q.; and Zhu, X. 2017. Encoding syntactic knowledge in neural networks for sentiment classi\ufb01cation. ACM Transactions on Information Systems (TOIS) 35(3):26.",
            "[2017] Huang, M.; Qian, Q.; and Zhu, X. 2017. Encoding syntactic knowledge in neural networks for sentiment classi\ufb01cation. ACM Transactions on Information Systems (TOIS) 35(3):26. [2015] Ioffe, S., and Szegedy, C. 2015. Batch normalization: accel- erating deep network training by reducing internal covariate shift. In ICML, 448\u2013456. [2014] Kim, Y. 2014. Convolutional neural networks for sentence classi\ufb01cation. In EMNLP, 1746\u20131751. [2014] Kingma, D. P., and Ba, J. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. [2003] Klein, D., and Manning, C. D. 2003. Accurate unlexicalized parsing. In ACL, 423\u2013430. [2002] Li, X., and Roth, D. 2002.",
            "[2003] Klein, D., and Manning, C. D. 2003. Accurate unlexicalized parsing. In ACL, 423\u2013430. [2002] Li, X., and Roth, D. 2002. Learning question classi\ufb01ers. In COLING, 1\u20137. [2017a] Liu, P.; Qiu, X.; and Huang, X. 2017a. Adaptive semantic compositionality for sentence modelling. In IJCAI, 4061\u20134067. [2017b] Liu, P.; Qiu, X.; and Huang, X. 2017b. Dynamic composi- tional neural networks over tree structure. In IJCAI, 4054\u20134060. [2017] Loshchilov, I., and Hutter, F. 2017. Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101. [2017] McCann, B.; Bradbury, J.; Xiong, C.; and Socher, R. 2017.",
            "2017. Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101. [2017] McCann, B.; Bradbury, J.; Xiong, C.; and Socher, R. 2017. Learned in translation: Contextualized word vectors. In NIPS, 6294\u20136305. [2015] Mou, L.; Peng, H.; Li, G.; Xu, Y.; Zhang, L.; and Jin, Z. 2015. Discriminative neural sentence modeling by tree-based con- volution. In EMNLP, 2315\u20132325. [2016] Mou, L.; Men, R.; Li, G.; Xu, Y.; Zhang, L.; Yan, R.; and Jin, Z. 2016. Natural language inference by tree-based convolution and heuristic matching. In ACL, 130. [2017] Munkhdalai, T., and Yu, H. 2017. Neural semantic encoders. In ACL, 397\u2013407.",
            "2016. Natural language inference by tree-based convolution and heuristic matching. In ACL, 130. [2017] Munkhdalai, T., and Yu, H. 2017. Neural semantic encoders. In ACL, 397\u2013407. [2017] Nie, Y., and Bansal, M. 2017. Shortcut-stacked sentence encoders for multi-domain inference. In RepEval, 41\u201345. [2004] Pang, B., and Lee, L. 2004. A sentimental education: Senti- ment analysis using subjectivity summarization based on minimum cuts. In ACL, 271. [2005] Pang, B., and Lee, L. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In ACL, 115\u2013124. [2014] Pennington, J.; Socher, R.; and Manning, C. 2014. Glove: Global vectors for word representation. In EMNLP, 1532\u20131543.",
            "In ACL, 115\u2013124. [2014] Pennington, J.; Socher, R.; and Manning, C. 2014. Glove: Global vectors for word representation. In EMNLP, 1532\u20131543. [2018] Peters, M.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. In NAACL, 2227\u20132237. [2012] Petrov, S.; Das, D.; and McDonald, R. 2012. A universal part-of-speech tagset. In LREC. [2015] Qian, Q.; Tian, B.; Huang, M.; Liu, Y.; Zhu, X.; and Zhu, X. 2015. Learning tag embeddings and tag-speci\ufb01c composition functions in recursive neural network. In ACL, volume 1, 1365\u2013 1374. [2017] Radford, A.; Jozefowicz, R.; and Sutskever, I.",
            "2015. Learning tag embeddings and tag-speci\ufb01c composition functions in recursive neural network. In ACL, volume 1, 1365\u2013 1374. [2017] Radford, A.; Jozefowicz, R.; and Sutskever, I. 2017. Learn- ing to generate reviews and discovering sentiment. arXiv preprint arXiv:1704.01444. [2018] Shen, T.; Tianyi, Z.; Guodong, L.; Jing, J.; Sen, W.; and Chengqi, Z. 2018. Reinforced self-attention network: a hybrid of hard and soft attention for sequence modeling. arXiv preprint arXiv:1801.10296. [2011] Socher, R.; Lin, C. C.-Y.; Ng, A. Y.; and Manning, C. D. 2011. Parsing natural scenes and natural language with recursive neural networks. In ICML, 129\u2013136. [2012] Socher, R.; Huval, B.; Manning, C. D.; and Ng, A. Y.",
            "2011. Parsing natural scenes and natural language with recursive neural networks. In ICML, 129\u2013136. [2012] Socher, R.; Huval, B.; Manning, C. D.; and Ng, A. Y. 2012. Semantic compositionality through recursive matrix-vector spaces. In EMNLP, 1201\u20131211. [2013] Socher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning, C. D.; Ng, A.; and Potts, C. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, 1631\u20131642. [2014] Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from over\ufb01tting. JMLR 15(1):1929\u20131958. [2015] Tai, K. S.; Socher, R.; and Manning, C. D.",
            "2014. Dropout: A simple way to prevent neural networks from over\ufb01tting. JMLR 15(1):1929\u20131958. [2015] Tai, K. S.; Socher, R.; and Manning, C. D. 2015. Im- proved semantic representations from tree-structured long short- term memory networks. In ACL, 1556\u20131566. [2017] Teng, Z., and Zhang, Y. 2017. Head-lexicalized bidirectional tree lstms. TACL 5:163\u2013177. [2017] Wang, Y.; Li, S.; Yang, J.; Sun, X.; and Wang, H. 2017. Tag- enhanced tree-structured neural networks for implicit discourse re- lation classi\ufb01cation. In IJCNLP, 496\u2013505. [2017] Yang, B.; Wong, D. F.; Xiao, T.; Chao, L. S.; and Zhu, J. 2017. Towards bidirectional hierarchical representations for attention-based neural machine translation.",
            "In IJCNLP, 496\u2013505. [2017] Yang, B.; Wong, D. F.; Xiao, T.; Chao, L. S.; and Zhu, J. 2017. Towards bidirectional hierarchical representations for attention-based neural machine translation. In EMNLP, 1432\u2013 1441. [2017] Yogatama, D.; Blunsom, P.; Dyer, C.; Grefenstette, E.; and Ling, W. 2017. Learning to compose words into sentences with reinforcement learning. In ICLR.",
            "[2012] Zeiler, M. D. 2012. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701. [2015] Zhao, H.; Lu, Z.; and Poupart, P. 2015. Self-adaptive hier- archical sentence model. In IJCAI, 4069\u20134076. [2016] Zhou, P.; Qi, Z.; Zheng, S.; Xu, J.; Bao, H.; and Xu, B. 2016. Text classi\ufb01cation improved by integrating bidirectional lstm with two-dimensional max pooling. In COLING, 3485\u20133495."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1809.02286.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 10549.999694824219,
    "avg_doclen_est": 170.16128540039062
}
