[
  "Canonicalizing Knowledge Base Literals Jiaoyan Chen1, Ernesto Jim\u00b4enez-Ruiz2,3, and Ian Horrocks1,2 1 Department of Computer Science, University of Oxford, United Kingdom 2 The Alan Turing Institute, United Kingdom 3 Department of Informatics, University of Oslo, Norway Abstract. Ontology-based knowledge bases (KBs) like DBpedia are very valu- able resources, but their usefulness and usability is limited by various quality issues. One such issue is the use of string literals instead of semantically typed entities. In this paper we study the automated canonicalization of such literals, i.e., replacing the literal with an existing entity from the KB or with a new entity that is typed using classes from the KB. We propose a framework that combines both reasoning and machine learning in order to predict the relevant entities and types, and we evaluate this framework against state-of-the-art baselines for both semantic typing and entity matching.",
  "We propose a framework that combines both reasoning and machine learning in order to predict the relevant entities and types, and we evaluate this framework against state-of-the-art baselines for both semantic typing and entity matching. Keywords: Knowledge Base Correction \u00b7 Literal Canonicalization \u00b7 Knowledge- based Learning \u00b7 Recurrent Neural Network 1 Introduction Ontology-based knowledge bases (KBs) like DBpedia [2] are playing an increasingly important role in domains such knowledge management, data analysis and natural lan- guage understanding. Although they are very valuable resources, the usefulness and usability of such KBs is limited by various quality issues [31,10,22]. One such issue is the use of string literals (both explicitly typed and plain literals) instead of semantically typed entities; for example in the triple \u27e8River Thames, passesArea, \u201cPort Meadow, Oxford\u201d\u27e9. This weakens the KB as it does not capture the semantics of such literals. If, in contrast, the object of the triple were an entity, then this entity could, e.g., be typed as Wetland and Park, and its location given as Oxford.",
  "This weakens the KB as it does not capture the semantics of such literals. If, in contrast, the object of the triple were an entity, then this entity could, e.g., be typed as Wetland and Park, and its location given as Oxford. This problem is pervasive and hence results in a signi\ufb01cant loss of information: according to statistics from Gunaratna et al. [14] in 2016, the DBpedia property dbp:location has over 105,000 unique string literals that could be matched with entities. Besides DBpedia, such literals can also be found in some other KBs from encyclopedias (e.g., zhishi.me [21]), in RDF graphs transformed from tabular data (e.g., LinkedGeoData [3]), in aligned or evolving KBs, etc. One possible remedy for this problem is to apply automated semantic typing and en- tity matching (AKA canonicalization4) to such literals. To the best of our knowledge, semantic typing of KB literals has rarely been studied. Gunaratna et al.",
  "One possible remedy for this problem is to apply automated semantic typing and en- tity matching (AKA canonicalization4) to such literals. To the best of our knowledge, semantic typing of KB literals has rarely been studied. Gunaratna et al. [14] used se- mantic typing in their entity summarization method, \ufb01rst identifying the so called focus term of a phrase via grammatical structure analysis, and then matching the focus term 4 Note this is different from canonical mapping of literal values in the RDF standard by W3C. arXiv:1906.11180v1  [cs.AI]  26 Jun 2019",
  "2 Jiaoyan Chen, Ernesto Jim\u00b4enez-Ruiz, and Ian Horrocks with both KB types and entities. Their method is, however, rather simplistic: it neither utilizes the literal\u2019s context, such as the associated property and subject, nor captures the contextual meaning of the relevant words. What has been widely studied is the se- mantic annotation of KB entities [13,23,28] and of noun phrases outside the KB (e.g., from web tables) [18,9,4]; in such cases, however, the context is very different, and en- tity typing can, for example, exploit structured information such as the entity\u2019s linked Wikipedia page [13] and the domain and range of properties that the entity is associated with [23]. With the development of deep learning, semantic embedding and feature learning have been widely adopted for exploring different kinds of contextual semantics in pre- diction, with Recurrent Neural Network (RNN) being a state-of-the-art method for deal- ing with structured data and text. One well known example is word2vec \u2014 an RNN language model which can represent words in a vector space that retains their meaning [20].",
  "One well known example is word2vec \u2014 an RNN language model which can represent words in a vector space that retains their meaning [20]. Another example is a recent study by Kartsaklis et al. [15], which maps text to KB entities with a Long-short Term Memory RNN for textual feature learning. These methods offer the potential for developing accurate prediction-based methods for KB literal typing and entity matching where the contextual semantics is fully exploited. In this study, we investigate KB literal canonicalization using a combination of RNN-based learning and semantic technologies. We \ufb01rst predict the semantic types of a literal by: (i) identifying candidate classes via lexical entity matching and KB queries; (ii) automatically generating positive and negative examples via KB sampling, with external semantics (e.g., from other KBs) injected for improved quality; (iii) training classi\ufb01ers using relevant subject-predicate-literal triples embedded in an attentive bidi- rectional RNN (AttBiRNN); and (iv) using the trained classi\ufb01ers and KB class hierarchy to predict candidate types.",
  "The novelty of our framework lies in its knowledge-based learning; this includes automatic candidate class extraction and sampling from the KB, triple embedding with different importance degrees suggesting different semantics, and using the predicted types to identify a potential canonical entity from the KB. We have evaluated our framework using a synthetic literal set (S-Lite) and a real literal set (R- Lite) from DBpedia [2]. The results are very promising, with signi\ufb01cant improvements over several baselines, including the existing state-of-the-art. 2 Method 2.1 Problem Statement In this study we consider a knowledge base (KB) that includes both ontological axioms that induce (at least) a hierarchy of semantic types (i.e., classes), and assertions that describe concrete entities (individuals). Each such assertion is assumed to be in the form of an RDF triple \u27e8s, p, o\u27e9, where s is an entity, p is a property and o can be either an entity or a literal (i.e., a typed or untyped data value such as a string or integer).",
  "Each such assertion is assumed to be in the form of an RDF triple \u27e8s, p, o\u27e9, where s is an entity, p is a property and o can be either an entity or a literal (i.e., a typed or untyped data value such as a string or integer). We focus on triples of the form \u27e8s, p, l\u27e9, where l is a string literal; such literals can be identi\ufb01ed by regular expressions, as in [14], or by data type inference as in [8]. Our aim is to cononicalize l by \ufb01rst identifying the type of l, i.e., a set of classes Cl that an entity corresponding to l should be an instance of, and then determining",
  "Canonicalizing Knowledge Base Literals 3 if such an entity already exists in the KB. The \ufb01rst subtask is modeled as a machine learning classi\ufb01cation problem where a real value score in [0, 1] is assigned to each class c occurring in the KB, and Cl is the set of classes determined by the assigned score with strategies e.g., adopting a class if its score exceeds some threshold. The second subtask is modeled as an entity lookup problem constrained by Cl. It is important to note that: (i) When we talk about a literal l we mean the occurrence of l in a triple \u27e8s, p, l\u27e9. Lexically equivalent literals might be treated very differently depending on their triple contexts. (ii) If the KB is an OWL DL ontology, then the set of object properties (which con- nect two entities) and data properties (which connect an entity to a literal) should be disjoint. In practice, however, KBs such as DBpedia often don\u2019t respect this constraint. In any case, we avoid the issue by simply computing the relevant typ- ing and canonicalization information, and leaving it up to applications as to how they want to exploit it.",
  "In practice, however, KBs such as DBpedia often don\u2019t respect this constraint. In any case, we avoid the issue by simply computing the relevant typ- ing and canonicalization information, and leaving it up to applications as to how they want to exploit it. (iii) We assume that no manual annotations or external labels are given \u2014 the classi\ufb01er is automatically trained using the KB. 2.2 Technical Framework The technical framework for the classi\ufb01cation problem is shown in Fig. 1. It involves three main steps: (i) candidate class extraction; (ii) model training and prediction; and (iii) literal typing and canonicalization.",
  "2.2 Technical Framework The technical framework for the classi\ufb01cation problem is shown in Fig. 1. It involves three main steps: (i) candidate class extraction; (ii) model training and prediction; and (iii) literal typing and canonicalization. <Subject, Property, Literal>\u2019s -, passesArea, \u201cPort Meadow, Oxford\u201d, -, passesArea, \u201cLondon, England\u201d \u2026 Entities \ud835\udc38\" Entities \ud835\udc38# Classes \ud835\udc36\" Classes \ud835\udc36# Candidate  Classes \ud835\udc36\"# Literal  Matching Property  Objects Query For each class \ud835\udc50in \ud835\udc36\"# Sampling Sample: (triple < \ud835\udc60, \ud835\udc5d, \ud835\udc59>, label) Particular & General samples  Sample refinement Neural Network (Classifier) Triple < \ud835\udc60, \ud835\udc5d, \ud835\udc59> to Sequence  [\ud835\udc64\ud835\udc5c\ud835\udc5f\ud835\udc511, \u2026 , \ud835\udc64\ud835\udc5c\ud835\udc5f\ud835\udc514] Bidirectional RNNs + Attention Layer Training For each literal, predict  types, lookup entities  Literal Typing &  Canonicalization \u201cPort Meadow, Oxford\u201d \u2013 Park and Wetland (Types) \u201cLondon, England\u201d \u2013 London (Entity) Query Merge Fig.",
  "1. The technical framework for KB literal canonicalization. Candidate class extraction Popular KBs like DBpedia often contain a large number of classes. For ef\ufb01ciency reasons, and to reduce noise in the learning process, we \ufb01rst identify a subset of candidate classes. This selection should be rather inclusive so as to maximize potential recall. In order to achieve this we pool the candidate classes for all",
  "4 Jiaoyan Chen, Ernesto Jim\u00b4enez-Ruiz, and Ian Horrocks literals occurring in triples with a given property; i.e., to compute the candidate classes for a literal l occurring in a triple \u27e8s, p, l\u27e9, we consider all triples that use property p. Note that, as discussed above, in practice such triples may include both literals and entities as their objects. We thus use two techniques for identifying candidate classes from the given set of triples. In the case where the object of the triple is an entity, the candidates are just the set of classes that this entity is an instance of. In practice we identify the candidates for the set of all such entities, which we denote EP , via a SPARQL query to the KB, with the resulting set of classes being denoted CP . In the case where the object of the triple is a literal, we \ufb01rst match the literal to entities using a lexical index which is built based on the entity\u2019s name, labels and anchor text (description).",
  "In the case where the object of the triple is a literal, we \ufb01rst match the literal to entities using a lexical index which is built based on the entity\u2019s name, labels and anchor text (description). To maximize recall, the literal, its tokens (words) and its sub-phrases are used to retrieve entities by lexical matching; this technique is particularly effective when the literal is a long phrase. As in the \ufb01rst case, we identify all relevant entities, which we denote EM, and then retrieve the relevant classes CM using a SPARQL query. The candidate class set is simply the union of CP and CM, denoted as CP M. Model training and prediction. We adopt the strategy of training one binary classi\ufb01er for each candidate class, instead of multi-class classi\ufb01cation, so as to facilitate dealing with the class hierarchy [27]. The classi\ufb01er architecture includes an input layer with word embedding, an encoding layer with bidirectional RNNs, an attention layer and a fully connected (FC) layer for modeling the contextual semantics of the literal.",
  "The classi\ufb01er architecture includes an input layer with word embedding, an encoding layer with bidirectional RNNs, an attention layer and a fully connected (FC) layer for modeling the contextual semantics of the literal. To train a classi\ufb01er, both positive and negative entities (samples), including those from EM (particular samples) and those outside EM (general samples) are extracted from the KB, with external KBs and logical constraints being used to improve sample quality. The trained classi\ufb01ers are used to compute a score for each candidate class. Literal Typing and Canonicalization The \ufb01nal stage is to semantically type and, where possible, canonicalise literals. For a given literal, two strategies, independent and hierarchical, are used to determine its types (classes), with a score for each type. We then use these types and scores to try to identify an entity in the KB that could reasonably be substituted for the literal.",
  "For a given literal, two strategies, independent and hierarchical, are used to determine its types (classes), with a score for each type. We then use these types and scores to try to identify an entity in the KB that could reasonably be substituted for the literal. 2.3 Prediction Model Given a phrase literal l and its associated RDF triple \u27e8s, p, l\u27e9, our neural network model aims at utilizing the semantics of s, p and l for the classi\ufb01cation of l. The architecture is shown in Fig. 2. It \ufb01rst separately parses the subject label, the property label and the literal into three word (token) sequences whose lengths, denoted as Ts, Tp and Tl, are \ufb01xed to the maximum subject, property and literal sequence lengths from the training data by padding shorter sequences with null words. We then concatenate the three sequences into a single word sequence (wordt, t \u2208[1, T]), where T = Ts + Tp + Tl. Each word is then encoded into a vector via word embedding (null is encoded into a zero vector), and the word sequence is transformed into a vector sequence (xt, t \u2208 [1, T]).",
  "Each word is then encoded into a vector via word embedding (null is encoded into a zero vector), and the word sequence is transformed into a vector sequence (xt, t \u2208 [1, T]). Note that this preserves information about the position of words in s, p and l.",
  "Canonicalizing Knowledge Base Literals 5 \u201cRiver\u201d \u201cThames\u201d Subject \ud835\udc60 \u201cpasses\u201d \u201cArea\u201d Predicate \ud835\udc5d \u201cPort\u201d \u201cMeadow\u201d \u201cOxford\u201d Literal \ud835\udc59 Word Vectors Bidirectional RNNs Attention Layer Word Attentions FC Layer +  Logistic Regression \ud835\udc53(\ud835\udc60,\ud835\udc5d,\ud835\udc59) Query \ud835\udc65), \ud835\udc61\u2208[1, \ud835\udc47] \u210e), \ud835\udc61\u2208[1, \ud835\udc47] \u210e), \ud835\udc61\u2208[\ud835\udc47, 1] \ud835\udc622 \ud835\udefc),\ud835\udc61\u2208[1,\ud835\udc47] \ud835\udc63 Fig. 2. The architecture of the neural network. The semantics of forward and backward surrounding words is effective in predicting a word\u2019s semantics. For example, \u201cPort\u201d and \u201cMeadow\u201d are more likely to indicate a place as they appear after \u201cArea\u201d and before \u201cOxford\u201d.",
  "2. The architecture of the neural network. The semantics of forward and backward surrounding words is effective in predicting a word\u2019s semantics. For example, \u201cPort\u201d and \u201cMeadow\u201d are more likely to indicate a place as they appear after \u201cArea\u201d and before \u201cOxford\u201d. To embed such contextual semantics into a feature vector, we stack a layer composed of bidirectional Recurrent Neural Networks (BiRNNs) with Gated Recurrent Unit (GRU) [5]. Within each RNN, a reset gate rt is used to control the contribution of the past word, and an update gate zt is used to balance the contributions of the past words and the new words.",
  "Within each RNN, a reset gate rt is used to control the contribution of the past word, and an update gate zt is used to balance the contributions of the past words and the new words. The hidden state (embedding) at position t is computed as \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 ht = (1 \u2212zt) \u2299ht\u22121 + zt \u2299\u02dcht, \u02dcht = \u03c4(Whxt + rt \u2299(Uhht\u22121) + bh), zt = \u03c3(Wzxt + Uzht\u22121 + bz), rt = \u03c3(Wrxt + Urht\u22121 + br), (1) where \u2299denotes the Hadamard product, \u03c3 and \u03c4 denote the activation function of sigmod and tanh respectively, and Wh, Uh, bh, Wz, Uz, bz, Wr, Ur and br are pa- rameters to learn.",
  "With the two bidirectional RNNs, one forward hidden state and one backward hidden state are calculated for the sequence, denoted as (\u2212\u2192 ht, t \u2208[1, T]) and (\u2190\u2212 ht, t \u2208[T, 1]) respectively. They are concatenated as the output of the RNN layer: ht = h\u2212\u2192 ht, \u2190\u2212 ht i , t \u2208[1, T]. We assume different words are differently informative towards the type of the literal. For example, the word \u201cport\u201d is more important than the other words in distinguishing the type Wetland from other concrete types of Place. To this end, an attention layer is further stacked. Given the input from the RNN layer (ht, t \u2208[1, T]), the attention layer outputs ha = [\u03b1tht] , t \u2208[1, T], where \u03b1t is the normalized weight of the word at",
  "6 Jiaoyan Chen, Ernesto Jim\u00b4enez-Ruiz, and Ian Horrocks position t and is calculated as \uf8f1 \uf8f2 \uf8f3 \u03b1t = exp(uT t uw) P t\u2208[1,T ] exp(uT t uw) ut = \u03c4(Wwht + bw), (2) where uw, Ww and bw are parameters to learn. Speci\ufb01cally, uw denotes the general informative degrees of all the words, while \u03b1t denotes the attention of the word at position t w.r.t. other words in the sequence. Note that the attention weights can also be utilized to justify a prediction. In order to exploit information about the location of a word in the subject, property or literal, we do not calculate the weighted sum of the BiRNN output but concatenate the weighted vectors. The dimension of each RNN hidden state (i.e., \u2190\u2212 ht and \u2212\u2192 ht), denoted as dr, and the dimension of each attention layer output (i.e., \u03b1tht), denoted as da, are two hyper parameters of the network architecture.",
  "The dimension of each RNN hidden state (i.e., \u2190\u2212 ht and \u2212\u2192 ht), denoted as dr, and the dimension of each attention layer output (i.e., \u03b1tht), denoted as da, are two hyper parameters of the network architecture. A fully connected (FC) layer and a logistic regression layer are \ufb01nally stacked for modeling the nonlinear relationship and calculating the output score respectively: f(s, p, l) = \u03c3(Wfha + bf), (3) where Wf and bf are the parameters to learn, \u03c3 denotes the sigmod function, and f denotes the function of the whole network. 2.4 Sampling and Training We \ufb01rst extract both particular samples and general samples from the KB using SPARQL queries and reasoning; we then improve sample quality by detecting and repairing wrong and missing entity classi\ufb01cations with the help of external KBs; and \ufb01nally we train the classi\ufb01ers. Particular Sample Particular samples are based on the entities EM that are lexically matched by the literals.",
  "Particular Sample Particular samples are based on the entities EM that are lexically matched by the literals. For each literal candidate class c in CM, its particular samples are generated by: (i) Extracting its positive particular entities: Ec M = {e|e \u2208EM, e is an instance of c}; (ii) Generating its positive particular samples as P+ c = \u222ae\u2208Ec M {\u27e8s, p, l\u27e9|s \u2208S(p, e), l \u2208L(e)} , (4) where S(p, e) denotes the set of entities occurring in the subject position in a triple of the form \u27e8s, p, e\u27e9, and L(e) denotes all the labels (text phrases) of the entity e; (iii) Extracting its negative particular entities Eec M as those entities in EM that are instances of some sibling class of c and not instances of c;5 (iv) Generating its negative particular samples P\u2212 c with Eec M using the same approach as for positive samples. 5 We use sibling classes to generate negative examples as, in practice, sibling classes are often disjoint.",
  "Canonicalizing Knowledge Base Literals 7 General Sample Given that the literal matched candidate classes CM are only a part of all the candidate classes CP M, and that the size of particular samples may be too small to train the neural network, we additionally generate general samples based on common KB entities. For each candidate class c in CP M, all its entities in the KB, denoted as Ec, are extracted and then its positive general samples, denoted as G+ c , are generated from Ec using the same approach as for particular samples. Similarly, entities of the sibling classes of c, denoted as Eec, are extracted, and general negative samples, denoted as G\u2212 c , are generated from Eec. As for negative particular entities, we check each entity in Eec and remove those that are not instances of c. Unlike the particular samples, the positive and negative general samples are bal- anced. This means that we reduce the size of G+ c and G\u2212 c to the minimum of #(G+ c ), #(G\u2212 c ) and N0, where #() denotes set cardinality, and N0 is a hyper parameter for sampling. Size reduction is implemented via random sampling.",
  "This means that we reduce the size of G+ c and G\u2212 c to the minimum of #(G+ c ), #(G\u2212 c ) and N0, where #() denotes set cardinality, and N0 is a hyper parameter for sampling. Size reduction is implemented via random sampling. Sample Re\ufb01nement Many KBs are quite noisy, with wrong or missing entity classi- \ufb01cations. For example, when using the SPARQL endpoint of DBpedia, dbr:Scotland is classi\ufb01ed as dbo:MusicalArtist instead of as dbo:Country, while dbr:Afghan appears without a type. We have corrected and complemented the sample generation by combin- ing the outputs of more than one KB. For example, the DBpedia endpoint suggestions are compared against Wikidata and the DBpedia lookup service. Most DBpedia entities are mapped to Wikidata entities whose types are used to validate and complement the suggested types from the DBpedia endpoint. In addition, the lookup service, although incomplete, typically provides very precise types that can also con\ufb01rm the validity of the DBpedia endpoint types.",
  "Most DBpedia entities are mapped to Wikidata entities whose types are used to validate and complement the suggested types from the DBpedia endpoint. In addition, the lookup service, although incomplete, typically provides very precise types that can also con\ufb01rm the validity of the DBpedia endpoint types. The validation is performed by identifying if the types suggested by one KB are compatible with those returned by other KBs, that is, if the relevant types belong to the same branch of the hierarchy (e.g., the DBpedia taxonomy). With the new entity classi\ufb01cations, the samples are revised accordingly. Training We train a binary classi\ufb01er f c for each class c in CP M. It is \ufb01rst pre-trained with general samples G+ c \u222aG\u2212 c , and then \ufb01ne tuned with particular samples P+ c \u222aP\u2212 c . Pre-training deals with the shortage of particular samples, while \ufb01ne-tuning bridges the gap between common KB entities and the entities associated with the literals, which is also known as domain adaptation.",
  "Pre-training deals with the shortage of particular samples, while \ufb01ne-tuning bridges the gap between common KB entities and the entities associated with the literals, which is also known as domain adaptation. Given that pre-training is the most time consuming step, but is task agnostic, classi\ufb01ers for all the classes in a KB could be pre-trained in advance to accelerate a speci\ufb01c literal canonicalization task. 2.5 Independent and Hierarchical Typing In prediction, the binary classi\ufb01er for class c, denoted as f c, outputs a score yc l indicat- ing the probability that a literal l belongs to class c: yc l = f c(l), yc l \u2208[0, 1]. With the predicted scores, we adopt two strategies \u2013 independent and hierarchical to determine the types. In the independent strategy, the relationship between classes is not consid- ered. A class c is selected as a type of l if its score yc l \u2265\u03b8, where \u03b8 is a threshold hyper parameter in [0, 1].",
  "8 Jiaoyan Chen, Ernesto Jim\u00b4enez-Ruiz, and Ian Horrocks The hierarchical strategy considers the class hierarchy and the disjointness between sibling classes. We \ufb01rst calculate a hierarchical score for each class with the predicted scores of itself and its descendents: sc l = max n yc\u2032 l |c\u2032 \u2291c, c\u2032 \u2208CP M o , (5) where \u2291denotes the subclass relationship between two classes, CP M is the set of can- didate classes for l, and max denotes the maximum value of a set. For a candidate class c\u2032 in CP M, we denote all disjoint candidate classes as D(CP M, c\u2032). They can be de- \ufb01ned as sibling classes of both c\u2032 and its ancestors, or via logical constraints in the KB. A class c is selected as a type of l if (i) its hierarchical score sc l \u2265\u03b8, and (ii) it satis\ufb01es the following soft exclusion condition: sc l \u2212max n sc\u2032 l |c\u2032 \u2208D(CP M, c) o \u2265\u03ba, (6) where \u03ba is a relaxation hyper parameter.",
  "The exclusion of disjoint classes is hard if \u03ba is set to 0, and relaxed if \u03ba is set to a negative \ufb02oat with a small absolute value e.g., \u22120.1. Finally, for a given literal l, we return the set of all selected classes as its types Cl. 2.6 Canonicalization Given a literal l, we use Cl to try to identify an associated entity. A set of candidate entities are \ufb01rst retrieved using the lexical index that is built on the entity\u2019s name, label, anchor text, etc. Unlike candidate class extraction, here we use the whole text phrase of the literal, and rank the candidate entities according to their lexical similarities. Those entities that are not instances of any classes in Cl are then \ufb01ltered out, and the most simi- lar entity among the remainder is selected as the associated entity for l. If no entities are retrieved, or all the retrieved entities are \ufb01ltered out, then the literal could be associated with a new entity whose types are those most speci\ufb01c classes in Cl.",
  "In either case we can improve the quality of our results by checking that the resulting entities would be consistent if added to the KB, and discarding any entity associations that would lead to inconsistency. 3 Evaluation 3.1 Experiment Setting Data Sets In the experiments, we adopt a real literal set (R-Lite) and a synthetic literal set (S-Lite)6 , both of which are extracted from DBpedia. R-Lite is based on the prop- erty and literal pairs published by Gunaratna et al. in 2016 [14]. We re\ufb01ne the data by (i) removing literals that no longer exist in the current version of DBpedia; (ii) extracting new literals from DBpedia for properties whose existing literals were all removed in step (i); (iii) extending each property and literal pair with an associated subject; and (iv) 6 Data and codes: https://github.com/ChenJiaoyan/KG_Curation",
  "Canonicalizing Knowledge Base Literals 9 manually adding ground truth types selected from classes de\ufb01ned in the DBpedia On- tology (DBO)7. To fully evaluate the study with more data, we additionally constructed S-Lite from DBpedia by repeatedly: (i) selecting a DBpedia triple of the form \u27e8s, p, e\u27e9, where e is an entity; (ii) replacing e with it\u2019s label l to give a triple \u27e8s, p, l\u27e9; (iii) elim- inating the entity e from DBpedia; and (iv) adding as ground truth types the DBpedia classes of which e is (implicitly) an instance. More data details are shown in Table 1. Properties # Literals # Ground Truth Types # (per Literal) Characters (Tokens) # per Literal S-Lite 41 1746 256 (2.94) 16.66 (2.40) R-Lite 142 820 123 (3.11) 19.44 (3.25) Table 1. Statistics of S-Lite and R-Lite. Metrics In evaluating the typing performance, Precision, Recall and F1 Score are used.",
  "Statistics of S-Lite and R-Lite. Metrics In evaluating the typing performance, Precision, Recall and F1 Score are used. For a literal l, the computed types Cl are compared with the ground truths Cgt l , and the following micro metrics are calculated: Pl = #(Cl \u2229Cgt l )/#(Cl), Rl = #(Cl \u2229Cgt l )/#(Cgt l ), and F1l = (2 \u00d7 Pl \u00d7 Rl)/(Pl + Rl). They are then averaged over all the literals as the \ufb01nal Precision, Recall and F1 Score of a literal set. Although F1 Score measures the overall performance with both Precision and Recall considered, it depends on the threshold hyper parameter \u03b8 as with Precision and Recall. Thus we let \u03b8 range from 0 to 1 with a step of 0.01, and calculate the average of all the F1 Scores (AvgF1@all) and top 5 highest F1 Scores (AvgF1@top5).",
  "Thus we let \u03b8 range from 0 to 1 with a step of 0.01, and calculate the average of all the F1 Scores (AvgF1@all) and top 5 highest F1 Scores (AvgF1@top5). AvgF1@all measures the overall pattern recognition capability, while AvgF1@top5 is relevant in real applications where we often use a validation data set to \ufb01nd a \u03b8 setting that is close to the optimum. We also use the highest (top) Precision in evaluating the sample re\ufb01nement. In evaluating entity matching performance, Precision is measured by manually check- ing whether the identi\ufb01ed entity is correct or not. S-Lite is not used for entity matching evaluation as the corresponding entities for all its literals are assumed to be excluded from the KB. We are not able to measure recall for entity matching as we do not have the ground truths; instead, we have evaluated entity matching with different con\ufb01dence thresholds and compared the number of correct results. Baselines and Settings The evaluation includes three aspects.",
  "We are not able to measure recall for entity matching as we do not have the ground truths; instead, we have evaluated entity matching with different con\ufb01dence thresholds and compared the number of correct results. Baselines and Settings The evaluation includes three aspects. We \ufb01rst compare differ- ent settings of the typing framework, analyzing the impacts of sample re\ufb01nement, \ufb01ne tuning by particular samples, BiRNN and the attention mechanism. We also compare the independent and hierarchical typing strategies. We then compare the overall typing performance of our framework with (i) Gunaratna et al. [14], which matches the literal to both classes and entities; (ii) an entity lookup based method; and (iii) a probabilis- tic property range estimation method. Finally, we analyze the performance of entity matching with and without the predicted types. 7 Classes with the pre\ufb01x of http://dbpedia.org/ontology/.",
  "10 Jiaoyan Chen, Ernesto Jim\u00b4enez-Ruiz, and Ian Horrocks The DBpedia lookup service, which is based on the Spotlight index [19], is used for entity lookup (retrieval). The DBpedia SPARQL endpoint is used for query answering and reasoning. The reported results are based on the following settings: the Adam opti- mizer together with cross-entropy loss are used for network training; dr and da are set to 200 and 50 respectively; N0 is set to 1200; word2vec trained with the latest Wikipedia article dump is adopted for word embedding; and (Ts, Tp, Tl) are set to (12, 4, 12) for S- Lite and (12, 4, 15) for R-Lite. The experiments are run on a workstation with Intel(R) Xeon(R) CPU E5-2670 @2.60GHz, with programs implemented by Tensor\ufb02ow.",
  "The experiments are run on a workstation with Intel(R) Xeon(R) CPU E5-2670 @2.60GHz, with programs implemented by Tensor\ufb02ow. 3.2 Results on Framework Settings We \ufb01rst evaluate the impact of the neural network architecture, \ufb01ne tuning and different typing strategies, with their typing results on S-Lite shown in Table 2 and Fig. 3. Our \ufb01ndings are supported by comparable results on R-Lite. We further evaluate sample re\ufb01nement, with some statistics of the re\ufb01nement operations as well as performance improvements shown in Fig. 4. Network Architecture and Fine Tuning According to Table 2, we \ufb01nd BiRNN sig- ni\ufb01cantly outperforms Multiple Layer Perceptron (MLP), a basic but widely used neu- ral network model, while stacking an attention layer (AttBiRNN) further improves AvgF1@all and AvgF1@top5, for example by 3.7% and 3.1% respectively with hierar- chical typing (\u03ba = \u22120.1).",
  "The result is consistent for both pre-trained models and \ufb01ne tuned models, using both independent and hierarchical typing strategies. This indicates the effectiveness of our neural network architecture. Meanwhile, the performance of all the models is signi\ufb01cantly improved after they are \ufb01ne tuned by the particular samples, as expected. For example, when the independent typing strategy is used, AvgF1@all and AvgF1@top5 of AttBiRNN are improved by 54.1% and 35.2% respectively. Framework Independent Hierarchical (\u03ba = \u22120.1) Hierarchical (\u03ba = 0) Settings AvgF1@all AvgF1@top5 AvgF1@all AvgF1@top5 AvgF1@all AvgF1@top5 Pre-training MLP 0.4102 0.4832 0.5060 0.5458 0.5916 0.5923 BiRNN 0.4686 0.5566 0.5295 0.5649 0.5977 0.5985 AttBiRNN 0.4728 0.5590 0.5420 0.",
  "5916 0.5923 BiRNN 0.4686 0.5566 0.5295 0.5649 0.5977 0.5985 AttBiRNN 0.4728 0.5590 0.5420 0.5912 0.6049 0.6052 Fine tuning MLP 0.6506 0.6948 0.6859 0.6989 0.6429 0.6626 BiRNN 0.7008 0.7434 0.7167 0.7372 0.6697 0.6850 AttBiRNN 0.7286 0.7557 0.7429 0.7601 0.6918 0.7070 Table 2. Typing performance of our framework on S-Lite under different settings. Independent and Hierarchical Typing The impact of independent and hierarchical typing strategies is more complex.",
  "7557 0.7429 0.7601 0.6918 0.7070 Table 2. Typing performance of our framework on S-Lite under different settings. Independent and Hierarchical Typing The impact of independent and hierarchical typing strategies is more complex. As shown in Table 2, when the classi\ufb01er is weak (e.g., pre-trained BiRNN), hierarchical typing with both hard exclusion (\u03ba = 0) and relaxed exclusion (\u03ba = \u22120.1) has higher AvgF1@all and AvgF1@top5 than indepen- dent typing. However, when a strong classi\ufb01er (e.g., \ufb01ne tuned AttBiRNN) is used, AvgF1@all and AvgF1@top5 of hierarchical typing with relaxed exclusion are close",
  "Canonicalizing Knowledge Base Literals 11 to independent typing, while hierarchical typing with hard exclusion has worse per- formance. We further analyze Precision, Recall and F1 Score of both typing strategies under varying threshold (\u03b8) values, as shown in Fig. 3. In comparison with indepen- dent typing, hierarchical typing achieves (i) more stable Precision, Recall and F1 Score curves; and (ii) signi\ufb01cantly higher Precision, especially when \u03b8 is small. Meanwhile, as with the results in Table 2, relaxed exclusion outperforms hard exclusion in hierarchical typing except for Precision when \u03b8 is between 0 and 0.05.",
  "Meanwhile, as with the results in Table 2, relaxed exclusion outperforms hard exclusion in hierarchical typing except for Precision when \u03b8 is between 0 and 0.05. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 P (I)  R (I)  F1 (I)  P (H, \ud835\udecb=-0.1)  R (H, \ud835\udecb=-0.1)  F1 (H, \ud835\udecb=-0.1)  P (H, \ud835\udecb=0.0)  R (H, \ud835\udecb=0.0)  F1 (H, \ud835\udecb=0.0)  P / R / F1 \ud835\udf03 Fig.",
  "3. (P)recision, (R)ecall and (F1) Score of independent (I) and hierarchical (H) typing for S-Lite, with the scores predicted by the \ufb01ne tuned AttBiRNN. Sample Re\ufb01nement Fig. 4 [Right] shows the ratio of positive and negative particular samples that are deleted and added during sample re\ufb01nement. The AttBiRNN classi\ufb01ers \ufb01ne tuned by the re\ufb01ned particular samples are compared with those \ufb01ne tuned by the original particular samples. The improvements on AvgF1@all, AvgF1@top5 and top Precision, which are based on the average of the three above typing settings, are shown in Fig. 4 [Left]. On the one hand, we \ufb01nd sample re\ufb01nement bene\ufb01ts both S-Lite and R-Lite, as expected.",
  "4 [Left]. On the one hand, we \ufb01nd sample re\ufb01nement bene\ufb01ts both S-Lite and R-Lite, as expected. On the other hand, we \ufb01nd the improvement on S-Lite is limited, while the improvement on R-Lite is quite signi\ufb01cant: F1@all and top Precision, e.g., are improved by around 0.8% and 1.8% respectively on S-Lite, but 4.3% and 7.4% respectively on R-Lite. This may be due to two factors: (i) the ground truths of S-Lite are the entities\u2019 class and super classes inferred from the KB itself, while the ground truths of R-Lite are manually labeled; (ii) sample re\ufb01nement deletes many more noisy positive and negative samples (which are caused by wrong entity classi\ufb01cations of the KB) on R-Lite than on S-Lite, as shown in Fig. 4 [Right]. 3.3 Results on Semantic Typing Table 3 displays the overall semantic typing performance of our method and the base- lines.",
  "4 [Right]. 3.3 Results on Semantic Typing Table 3 displays the overall semantic typing performance of our method and the base- lines. Results for two optimum settings are reported for each method. The baseline",
  "12 Jiaoyan Chen, Ernesto Jim\u00b4enez-Ruiz, and Ian Horrocks 0.00%  1.00%  2.00%  3.00%  4.00%  5.00%  6.00%  7.00%  S-Lite R-Lite F1@all F1@top5 top Precision 0.00%  2.00%  4.00%  6.00%  8.00%  10.00%  12.00%  (Positive, Add) (Negative, Add) (Positive, Del) (Negative, Del) S-Lite R-Lite Fig. 4. [Left] Performance improvement (%) by sample re\ufb01nement; [Right] Ratio (%) of added (deleted) positive (negative) particular sample per classi\ufb01er during sample re\ufb01nement. Entity-Lookup retrieves one or several entities using the whole phrase of the literal, and uses their classes and super classes as the types.",
  "Entity-Lookup retrieves one or several entities using the whole phrase of the literal, and uses their classes and super classes as the types. Gunaratna [14] matches the literal\u2019s focus term (head word) to an exact class, then an exact entity, and then a class with the highest similarity score. It stops as soon as some classes or entities are matched. We ex- tend its original \u201cexact entity match\u201d setting with \u201crelaxed entity match\u201d which means multiple entities are retrieved. Property Range Estimation gets the classes and super classes from the entity objects of the property, and calculates the score of each class as the ratio of entity objects that belong to that class. (H/I, \u03ba, \u00b7)@top-P (F1) denotes the setting where the highest Precision (F1 Score) is achieved. Methods with S-Lite R-Lite Their Settings Precision Recall F1 Score Precision Recall F1 Score Gunaratna exact entity match 0.3825 0.4038 0.3773 0.4761 0.5528 0.4971 relaxed entity match 0.4176 0.5816 0.4600 0.3865 0.",
  "3825 0.4038 0.3773 0.4761 0.5528 0.4971 relaxed entity match 0.4176 0.5816 0.4600 0.3865 0.6526 0.4469 Entity-Lookup top-1 entity 0.2765 0.2620 0.2623 0.3994 0.4407 0.4035 top-3 entities 0.2728 0.3615 0.2962 0.3168 0.5201 0.3655 Property Range (H/I, \u03ba, \u03b8)@top-P 0.7563 0.5583 0.6210 0.5266 0.4015 0.4364 Estimation (H/I, \u03ba, \u03b8)@top-F1 0.6874 0.7166 0.6773 0.4520 0.5069 0.4632 AttBiRNN (H/I, \u03ba, \u03b8)@top-P 0.8320 0.7325 0.7641 0.",
  "6874 0.7166 0.6773 0.4520 0.5069 0.4632 AttBiRNN (H/I, \u03ba, \u03b8)@top-P 0.8320 0.7325 0.7641 0.7466 0.5819 0.6340 (H/I, \u03ba, \u03b8)@top-F1 0.8179 0.7546 0.7708 0.6759 0.6451 0.6386 Table 3. Overall typing performance of our method and the baselines on S-Lite and R-Lite. As we can see, AttBiRNN achieves much higher performance than all three base- lines on both S-Lite and R-Lite. For example, the F1 Score of AttBiRNN is 67.6%, 160.2% and 13.8% higher than those of Gunaratna, Entity-Lookup and Property Range Estimation respectively on S-Lite, and 28.5%, 58.3% and 37.9% higher respectively on R-Lite.",
  "AttBiRNN also has signi\ufb01cantly higher Precision and Recall, even when the setting is adjusted for the highest F1 Score. This is as expected, because our neural net- work, which learns the semantics (statistical correlation) from both word vector corpus and KB, models and utilizes the contextual meaning of the literal and its associated triple, while Gunaratna and Entity-Lookup are mostly based on lexical similarity. The",
  "Canonicalizing Knowledge Base Literals 13 performance of Property Range Estimation is limited because the object annotation in DBpedia usually does not follow the property range, especially for those properties in R-Lite. For example, objects of the property dbp:of\ufb01ce have 35 DBO classes, ranging from dbo:City and dbo:Country to dbo:Company. It is also notable that AttBiRNN and Property Range Estimation perform better on S-Lite than on R-Lite. The top F1 Score is 20.7% and 46.2% higher respectively, while the top Precision is 11.4% and 43.6% higher respectively. This is because R- Lite is more noisy, with longer literals, and has more ground truth types on average (cf. Table 1), while S-Lite has fewer properties, and each property has a large number of entity objects, which signi\ufb01cantly bene\ufb01ts Property Range Estimation.",
  "Table 1), while S-Lite has fewer properties, and each property has a large number of entity objects, which signi\ufb01cantly bene\ufb01ts Property Range Estimation. In contrast, the two entity matching based methods, Gunaratna and Entity-Lookup, perform worse on S-Lite than on R-Lite; this is because the construction of S-Lite removes those KB entities from which literals were derived. Gunaratna outperforms Entity-Lookup as it extracts the head word and matches it to both entities and classes. Note that the head word is also included in our candidate class extraction with lookup. 3.4 Results on Entity Matching Table 4 displays the number of correct matched entities and the Precision of entity matching on R-Lite. The types are predicted by the \ufb01ne-tuned AttBiRNN with inde- pendent typing and two threshold settings. We can see that Precision is improved when the retrieved entities that do not belong to any of the predicted types are \ufb01ltered out. The improvement is 6.1% and 5.8% when \u03b8 is set to 0.15 and 0.01 respectively.",
  "We can see that Precision is improved when the retrieved entities that do not belong to any of the predicted types are \ufb01ltered out. The improvement is 6.1% and 5.8% when \u03b8 is set to 0.15 and 0.01 respectively. Meanwhile, although the total number of matches may decrease because of the \ufb01ltering, the number of correct matches still increases from 396 to 404 (\u03b8 = 0.01). This means that Recall is also improved. Metrics Pure Lookup Lookup-Type (\u03b8 = 0.15) Lookup-Type (\u03b8 = 0.01) Correct Matches # 396 400 404 Precision 0.6781 0.7194 0.7176 Table 4. Overall performance of entity matching on R-Lite with and without type constraint. 4 Related Work Work on KB quality issues can can be divided into KB quality assessment [10,31], and KB quality improvement/re\ufb01nement [22].",
  "Overall performance of entity matching on R-Lite with and without type constraint. 4 Related Work Work on KB quality issues can can be divided into KB quality assessment [10,31], and KB quality improvement/re\ufb01nement [22]. The former includes error and anomaly detection methods, such as test-driven and query template based approaches [16,11], with statistical methods [6] and consistency reasoning [24] also being applied to assess KB quality with different kinds of metric. The latter includes (i) KB completion, such as entity classi\ufb01cation [13,23,28], relation prediction [17] and data typing [8]; and (ii) KB diagnosis and repair, such as abnormal value detection [11], erroneous identity link detection [26] and data mapping (e.g., links to Wikipedia pages) correction [7].",
  "14 Jiaoyan Chen, Ernesto Jim\u00b4enez-Ruiz, and Ian Horrocks KB canonicalization refers to those re\ufb01nement works that deal with redundant and ambiguous KB components as well as poorly expressed knowledge with limited reason- ing potential. Some works in open information extraction (IE) [12,29,30] aim to iden- tify synonymous noun phrases and relation phrases of open KBs which are composed of triple assertions extracted from text without any ontologies. For example, the recently proposed CESI method [29] utilizes both learned KB embeddings and side information like WordNet to \ufb01nd synonyms via clustering. Other works analyze synonyms for on- tological KBs. Abedjan et al. [1] discovered synonymously used predicates for query expansion on DBpedia. Pujara et al. [25] identi\ufb01ed coreferent entities of NELL with on- tological constraints considered. These clustering, embedding, or entity linking based methods in open IE however can not be directly applied or do not work well for our KB literal canonicalization. The utilization of these techniques will be in our future work.",
  "These clustering, embedding, or entity linking based methods in open IE however can not be directly applied or do not work well for our KB literal canonicalization. The utilization of these techniques will be in our future work. String literals in ontological KBs such as DBpedia often represent poorly expressed knowledge, with semantic types and coreferent entities missed. As far as we known, canonicalization of such literals has been little studied. Gunaratna et al. [14] typed the literal by matching its head term to ontology classes and KB entities, but the literal context (e.g., the associated subject and property) and semantic meaning of the com- position words were not utilized. Some ideas of entity classi\ufb01cation can be borrowed for literal typing but will become ineffective as the context differs. For example, the baseline Property Range Estimation in our experiments uses the idea of SDType [23] \u2014 utilizing the statistical distribution of types in the subject position and object position of properties to estimate an entity\u2019s type probabilities. As a literal is associated with only one property, such probabilistic estimation becomes inaccurate (cf. results in Table 3).",
  "As a literal is associated with only one property, such probabilistic estimation becomes inaccurate (cf. results in Table 3). Our literal classi\ufb01cation model is in some degree inspired by those natural language understanding and web table annotation works that match external noun phrases to KB types and entities [15,18,4] using neural networks and semantic embeddings for mod- eling the contextual semantics. For example, Luo et al. [18] learned features from the surrounding cells of a target cell to predict its entity association. However the context in those works is very different, i.e., a simple regular structure of rows/columns with lim- ited (table) metadata. In contrast, KBs have a complex irregular structure and rich meta data (the knowledge captured in the KB). Differently from these works, we developed different methods, e.g., candidate class extraction and high quality sampling, to learn the network from the KB with its assertions, terminologies and reasoning capability. 5 Discussion and Outlook In this paper we present our study on KB literal canonicalization \u2014 an important prob- lem on KB quality that has been little studied. A new technical framework is proposed with neural network and knowledge-based learning.",
  "5 Discussion and Outlook In this paper we present our study on KB literal canonicalization \u2014 an important prob- lem on KB quality that has been little studied. A new technical framework is proposed with neural network and knowledge-based learning. It (i) extracts candidate classes as well as their positive and negative samples from the KB by lookup and query answer- ing, with their quality improved using an external KB; (ii) trains classi\ufb01ers that can ef- fectively learn a literal\u2019s contextual features with BiRNNs and an attention mechanism; (iii) identi\ufb01es types and matches entity for canonicalization. We use a real data set and a synthetic data set, both extracted from DBpedia, for evaluation. It achieves much higher",
  "Canonicalizing Knowledge Base Literals 15 performance than the baselines that include the state-of-the-art. We discuss below some more subjective observations and possible directions for future work. Neural Network and Prediction Justi\ufb01cation The network architecture aims to learn features from a literal\u2019s context. In our AttBiRNN, a triple is modeled as a word se- quence with three size-\ufb01xed segments allocated for the subject, object and literal re- spectively. The cooccurrence of words and the importance of each word are learned by BiRNNs and the attention mechanism respectively, where word position (including whether it is in the subject, property or literal) is signi\ufb01cant. The effectiveness of such a design has been validated in Section 3.2. However, the current design does not exploit further semantics of the subject, such as its relation to other entities. We believe that this will provide limited indication of the literal\u2019s semantic type, but this could be ex- plored using graph embedding methods such as random walks and Graph Convolutional Networks.",
  "However, the current design does not exploit further semantics of the subject, such as its relation to other entities. We believe that this will provide limited indication of the literal\u2019s semantic type, but this could be ex- plored using graph embedding methods such as random walks and Graph Convolutional Networks. We believe that it would be interesting to explore the possible use of the learned attention weights (\u03b1t) in justifying the predictions. For example, considering the lit- eral in triple \u27e8dbr:Byron White, dbp:battles, \u201cWorld War II\u201d\u27e9and the classi\ufb01er of type dbo:MilitaryCon\ufb02ict, \u201cWar\u201d gets a dominant attention weight of 0.919, \u201cbattles\u201d and \u201cII\u201d get attention weights 0.051 and 0.025 respectively, while the attention weights of other words and the padded empty tokens are all less than 0.0015. Similarly, in the triple \u27e8dbr:Larry Bird, dbp:statsLeague, \u201cNBA\u201d \u27e9, the total attention weights of the subject, property and literal are 0.008, 0.801 and 0.191 respectively w.r.t.",
  "Similarly, in the triple \u27e8dbr:Larry Bird, dbp:statsLeague, \u201cNBA\u201d \u27e9, the total attention weights of the subject, property and literal are 0.008, 0.801 and 0.191 respectively w.r.t. the classi- \ufb01er of dbo:Organisation, but become 0.077, 0.152 and 0.771 w.r.t. the classi\ufb01er of dbo:BasketballLeague, where the signal of basketball is focused. Knowledge-based Learning We developed some strategies to fully train our neural networks with the supervision of the KB itself. One strategy is the separated extrac- tion of general samples and particular samples. It (i) eliminates the time consuming pre-training step from a speci\ufb01c task, reducing for example the total typing time per literal of S-Lite from 10.5 seconds to 2.5 seconds (training and prediction are run with at most 10 parallel threads), and (ii) adapts the domain of the classi\ufb01er toward the tar- get literals through \ufb01ne tuning, which signi\ufb01cantly improves the accuracy as shown in Table 2.",
  "Another strategy that has been evaluated in Section 3.2 is sample re\ufb01nement by validating entity classi\ufb01cations with external knowledge from Wikidata. However, we believe that this could be further extended with more external KBs, as well as with logical constraints and rules. Entity Matching We currently search for the corresponding entity of a literal by lex- ical lookup, and \ufb01lter out those that are not instances of any of the predicted types. The extension with prediction does improve the performance in comparison with pure lookup (cf. Section 3.4), but not as signi\ufb01cantly as semantic typing, especially on the metric of the number of correct matches. One reason is that entity matching itself has relatively few ground truths as many literals in R-Lite have no corresponding entities in the KB. Another reason is that we post-process the entities from lookup instead of",
  "16 Jiaoyan Chen, Ernesto Jim\u00b4enez-Ruiz, and Ian Horrocks directly predicting the correspondence. This means that those missed by pure lookup are still missed. In the future we plan to explore direct prediction of the matching entity probably using semantic embedding and graph feature learning. Acknowledgments The work is supported by the AIDA project (U.K. Government\u2019s Defence & Security Programme in support of the Alan Turing Institute), the SIRIUS Centre for Scalable Data Access (Research Council of Norway, project 237889), the Royal Society, EPSRC projects DBOnto, MaSI3 and ED3. References 1. Abedjan, Z., Naumann, F.: Synonym analysis for predicate expansion. In: Extended semantic web conference. pp. 140\u2013154. Springer (2013) 2. Auer, S., Bizer, C., Kobilarov, G., Lehmann, J., Cyganiak, R., Ives, Z.: Dbpedia: A nucleus for a web of open data. In: The semantic web, pp. 722\u2013735. Springer (2007) 3.",
  "In: The semantic web, pp. 722\u2013735. Springer (2007) 3. Auer, S., Lehmann, J., Hellmann, S.: Linkedgeodata: Adding a spatial dimension to the web of data. In: International Semantic Web Conference. pp. 731\u2013746. Springer (2009) 4. Chen, J., Jimenez-Ruiz, E., Horrocks, I., Sutton, C.: Colnet: Embedding the semantics of web tables for column type prediction. In: AAAI (2019) 5. Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., Ben- gio, Y.: Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation. In: Proceedings of the 2014 Conference on Empirical Methods in Natural Lan- guage Processing. pp. 1724\u20131734 (2014) 6.",
  "In: Proceedings of the 2014 Conference on Empirical Methods in Natural Lan- guage Processing. pp. 1724\u20131734 (2014) 6. Debattista, J., Londo\u02dcno, S., Lange, C., Auer, S.: Quality assessment of linked datasets using probabilistic approximation. In: European semantic web conference. pp. 221\u2013236. Springer (2015) 7. Dimou, A., Kontokostas, D., Freudenberg, M., Verborgh, R., Lehmann, J., Mannens, E., Hellmann, S., Van de Walle, R.: Assessing and re\ufb01ning mappingsto rdf to improve dataset quality. In: International Semantic Web Conference. pp. 133\u2013149. Springer (2015) 8. Dongo, I., Cardinale, Y., Al-Khalil, F., Chbeir, R.: Semantic web datatype inference: Towards better rdf matching. In: International Conference on Web Information Systems Engineering. pp. 57\u201374 (2017) 9.",
  "Dongo, I., Cardinale, Y., Al-Khalil, F., Chbeir, R.: Semantic web datatype inference: Towards better rdf matching. In: International Conference on Web Information Systems Engineering. pp. 57\u201374 (2017) 9. Efthymiou, V., Hassanzadeh, O., Rodriguez-Muro, M., Christophides, V.: Matching web ta- bles with knowledge base entities: from entity lookups to entity embeddings. In: International Semantic Web Conference. pp. 260\u2013277. Springer (2017) 10. F\u00a8arber, M., Bartscherer, F., Menne, C., Rettinger, A.: Linked data quality of dbpedia, free- base, opencyc, wikidata, and yago. Semantic Web 9(1), 77\u2013129 (2018) 11. Fleischhacker, D., Paulheim, H., Bryl, V., V\u00a8olker, J., Bizer, C.: Detecting errors in numerical linked data using cross-checked outlier detection. In: International Semantic Web Confer- ence.",
  "Fleischhacker, D., Paulheim, H., Bryl, V., V\u00a8olker, J., Bizer, C.: Detecting errors in numerical linked data using cross-checked outlier detection. In: International Semantic Web Confer- ence. pp. 357\u2013372 (2014) 12. Gal\u00b4arraga, L., Heitz, G., Murphy, K., Suchanek, F.M.: Canonicalizing open knowledge bases. In: Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management. pp. 1679\u20131688 (2014) 13. Gangemi, A., Nuzzolese, A.G., Presutti, V., Draicchio, F., Musetti, A., Ciancarini, P.: Au- tomatic typing of dbpedia entities. In: International Semantic Web Conference. pp. 65\u201381. Springer (2012)",
  "Canonicalizing Knowledge Base Literals 17 14. Gunaratna, K., Thirunarayan, K., Sheth, A., Cheng, G.: Gleaning types for literals in rdf triples with application to entity summarization. In: European Semantic Web Conference. pp. 85\u2013100 (2016) 15. Kartsaklis, D., Pilehvar, M.T., Collier, N.: Mapping text to knowledge graph entities using multi-sense lstms. In: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 1959\u20131970 (2018) 16. Kontokostas, D., Westphal, P., Auer, S., Hellmann, S., Lehmann, J., Cornelissen, R., Zaveri, A.: Test-driven evaluation of linked data quality. In: Proceedings of the 23rd international conference on World Wide Web. pp. 747\u2013758. ACM (2014) 17. Krompa\u00df, D., Baier, S., Tresp, V.: Type-constrained representation learning in knowledge graphs.",
  "In: Proceedings of the 23rd international conference on World Wide Web. pp. 747\u2013758. ACM (2014) 17. Krompa\u00df, D., Baier, S., Tresp, V.: Type-constrained representation learning in knowledge graphs. In: International Semantic Web Conference. pp. 640\u2013655. Springer (2015) 18. Luo, X., Luo, K., Chen, X., Zhu, K.Q.: Cross-lingual entity linking for web tables. In: AAAI. pp. 362\u2013369 (2018) 19. Mendes, P.N., Jakob, M., Garc\u00b4\u0131a-Silva, A., Bizer, C.: Dbpedia spotlight: shedding light on the web of documents. In: Proceedings of the 7th international conference on semantic systems. pp. 1\u20138. ACM (2011) 20. Mikolov, T., Chen, K., Corrado, G., Dean, J.: Ef\ufb01cient estimation of word representations in vector space.",
  "pp. 1\u20138. ACM (2011) 20. Mikolov, T., Chen, K., Corrado, G., Dean, J.: Ef\ufb01cient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 (2013) 21. Niu, X., Sun, X., Wang, H., Rong, S., Qi, G., Yu, Y.: Zhishi. me-weaving chinese linking open data. In: International Semantic Web Conference. pp. 205\u2013220. Springer (2011) 22. Paulheim, H.: Knowledge graph re\ufb01nement: A survey of approaches and evaluation methods. Semantic web 8(3), 489\u2013508 (2017) 23. Paulheim, H., Bizer, C.: Type inference on noisy rdf data. In: International semantic web conference. pp. 510\u2013525 (2013) 24. Paulheim, H., Gangemi, A.: Serving dbpedia with dolce\u2013more than just adding a cherry on top. In: International Semantic Web Conference. pp.",
  "In: International semantic web conference. pp. 510\u2013525 (2013) 24. Paulheim, H., Gangemi, A.: Serving dbpedia with dolce\u2013more than just adding a cherry on top. In: International Semantic Web Conference. pp. 180\u2013196. Springer (2015) 25. Pujara, J., Miao, H., Getoor, L., Cohen, W.: Knowledge graph identi\ufb01cation. In: International Semantic Web Conference. pp. 542\u2013557. Springer (2013) 26. Raad, J., Beek, W., Van Harmelen, F., Pernelle, N., Sa\u00a8\u0131s, F.: Detecting erroneous identity links on the web using network metrics. In: International Semantic Web Conference. pp. 391\u2013407. Springer (2018) 27. Silla, C.N., Freitas, A.A.: A survey of hierarchical classi\ufb01cation across different application domains. Data Mining and Knowledge Discovery 22(1-2), 31\u201372 (2011) 28.",
  "Springer (2018) 27. Silla, C.N., Freitas, A.A.: A survey of hierarchical classi\ufb01cation across different application domains. Data Mining and Knowledge Discovery 22(1-2), 31\u201372 (2011) 28. Sleeman, J., Finin, T., Joshi, A.: Entity type recognition for heterogeneous semantic graphs. AI Magazine 36(1), 75\u201386 (2015) 29. Vashishth, S., Jain, P., Talukdar, P.: Cesi: Canonicalizing open knowledge bases using em- beddings and side information. In: Proceedings of the 2018 World Wide Web Conference on World Wide Web. pp. 1317\u20131327 (2018) 30. Wu, T.H., Wu, Z., Kao, B., Yin, P.: Towards practical open knowledge base canonicalization. In: Proceedings of the 27th ACM International Conference on Information and Knowledge Management. pp. 883\u2013892 (2018) 31.",
  "Wu, T.H., Wu, Z., Kao, B., Yin, P.: Towards practical open knowledge base canonicalization. In: Proceedings of the 27th ACM International Conference on Information and Knowledge Management. pp. 883\u2013892 (2018) 31. Zaveri, A., Rula, A., Maurino, A., Pietrobon, R., Lehmann, J., Auer, S.: Quality assessment for linked data: A survey. Semantic Web 7(1), 63\u201393 (2016)"
]