[
  "arXiv:1712.00991v1  [cs.CL]  4 Dec 2017 Mining Supervisor Evaluation and Peer Feedback in Performance Appraisals Girish Keshav Palshikar, Sachin Pawar, Saheb Chourasia, Nitin Ramrakhiyani TCS Research, Tata Consultancy Services Limited, 54B Hadapsar Industrial Estate, Pune 411013, India. {gk.palshikar, sachin7.p, saheb.c, nitin.ramrakhiyani}@tcs.com Abstract. Performance appraisal (PA) is an important HR process to periodically measure and evaluate every employee\u2019s performance vis-a- vis the goals established by the organization. A PA process involves pur- poseful multi-step multi-modal communication between employees, their supervisors and their peers, such as self-appraisal, supervisor assessment and peer feedback. Analysis of the structured data and text produced in PA is crucial for measuring the quality of appraisals and tracking actual improvements. In this paper, we apply text mining techniques to produce insights from PA text.",
  "Analysis of the structured data and text produced in PA is crucial for measuring the quality of appraisals and tracking actual improvements. In this paper, we apply text mining techniques to produce insights from PA text. First, we perform sentence classi\ufb01ca- tion to identify strengths, weaknesses and suggestions of improvements found in the supervisor assessments and then use clustering to discover broad categories among them. Next we use multi-class multi-label classi- \ufb01cation techniques to match supervisor assessments to prede\ufb01ned broad perspectives on performance. Finally, we propose a short-text summa- rization technique to produce a summary of peer feedback comments for a given employee and compare it with manual summaries. All tech- niques are illustrated using a real-life dataset of supervisor assessment and peer feedback text produced during the PA of 4528 employees in a large multi-national IT company. 1 Introduction Performance appraisal (PA) is an important HR process, particularly for modern organizations that crucially depend on the skills and expertise of their workforce. The PA process enables an organization to periodically measure and evaluate every employee\u2019s performance. It also provides a mechanism to link the goals established by the organization to its each employee\u2019s day-to-day activities and performance.",
  "The PA process enables an organization to periodically measure and evaluate every employee\u2019s performance. It also provides a mechanism to link the goals established by the organization to its each employee\u2019s day-to-day activities and performance. Design and analysis of PA processes is a lively area of research within the HR community [13], [22], [10], [20]. The PA process in any modern organization is nowadays implemented and tracked through an IT system (the PA system) that records the interactions that happen in various steps. Availability of this data in a computer-readable database opens up opportunities to analyze it using automated statistical, data- mining and text-mining techniques, to generate novel and actionable insights / patterns and to help in improving the quality and e\ufb00ectiveness of the PA pro- cess [15], [19], [1]. Automated analysis of large-scale PA data is now facilitated",
  "2 by technological and algorithmic advances, and is becoming essential for large organizations containing thousands of geographically distributed employees han- dling a wide variety of roles and tasks. A typical PA process involves purposeful multi-step multi-modal communica- tion between employees, their supervisors and their peers. In most PA processes, the communication includes the following steps: (i) in self-appraisal, an employee records his/her achievements, activities, tasks handled etc.; (ii) in supervisor as- sessment, the supervisor provides the criticism, evaluation and suggestions for improvement of performance etc.; and (iii) in peer feedback (aka 360\u25e6view), the peers of the employee provide their feedback. There are several business ques- tions that managers are interested in. Examples: 1. For my workforce, what are the broad categories of strengths, weaknesses and suggestions of improvements found in the supervisor assessments? 2. For my workforce, how many supervisor comments are present for each of a given \ufb01xed set of perspectives (which we call attributes), such as FUNC- TIONAL EXCELLENCE, CUSTOMER FOCUS, BUILDING EFFECTIVE TEAMS etc.? 3.",
  "2. For my workforce, how many supervisor comments are present for each of a given \ufb01xed set of perspectives (which we call attributes), such as FUNC- TIONAL EXCELLENCE, CUSTOMER FOCUS, BUILDING EFFECTIVE TEAMS etc.? 3. What is the summary of the peer feedback for a given employee? In this paper, we develop text mining techniques that can automatically produce answers to these questions. Since the intended users are HR executives, ideally, the techniques should work with minimum training data and experimentation with parameter setting. These techniques have been implemented and are being used in a PA system in a large multi-national IT company. The rest of the paper is organized as follows. Section 2 summarizes related work. Section 3 summarizes the PA dataset used in this paper. Section 4 ap- plies sentence classi\ufb01cation algorithms to automatically discover three impor- tant classes of sentences in the PA corpus viz., sentences that discuss strengths, weaknesses of employees and contain suggestions for improving her performance. Section 5 considers the problem of mapping the actual targets mentioned in strengths, weaknesses and suggestions to a \ufb01xed set of attributes.",
  "Section 5 considers the problem of mapping the actual targets mentioned in strengths, weaknesses and suggestions to a \ufb01xed set of attributes. In Section 6, we discuss how the feedback from peers for a particular employee can be sum- marized. In Section 7 we draw conclusions and identify some further work. 2 Related Work We \ufb01rst review some work related to sentence classi\ufb01cation. Semantically classi- fying sentences (based on the sentence\u2019s purpose) is a much harder task, and is gaining increasing attention from linguists and NLP researchers. McKnight and Srinivasan [12] and Yamamoto and Takagi [23] used SVM to classify sentences in biomedical abstracts into classes such as INTRODUCTION, BACKGROUND, PUR- POSE, METHOD, RESULT, CONCLUSION. Cohen et al. [3] applied SVM and other techniques to learn classi\ufb01ers for sentences in emails into classes, which are speech acts de\ufb01ned by a verb-noun pair, with verbs such as request, propose, amend, commit, deliver and nouns such as meeting, document, committee; see also [2].",
  "3 Khoo et al. [9] uses various classi\ufb01ers to classify sentences in emails into classes such as APOLOGY, INSTRUCTION, QUESTION, REQUEST, SALUTATION, STATE- MENT, SUGGESTION, THANKING etc. Qadir and Rilo\ufb00[17] proposes several \ufb01lters and classi\ufb01ers to classify sentences on message boards (community QA systems) into 4 speech acts: COMMISSIVE (speaker commits to a future action), DIRECTIVE (speaker expects listener to take some action), EXPRESSIVE (speaker expresses his or her psychological state to the listener), REPRESENTATIVE (represents the speaker\u2019s belief of something). Hachey and Grover [7] used SVM and maximum entropy classi\ufb01ers to classify sentences in legal documents into classes such as FACT, PROCEEDINGS, BACKGROUND, FRAMING, DISPOSAL; see also [18]. Desh- pande et al. [5] proposes unsupervised linguistic patterns to classify sentences into classes SUGGESTION, COMPLAINT.",
  "Desh- pande et al. [5] proposes unsupervised linguistic patterns to classify sentences into classes SUGGESTION, COMPLAINT. There is much work on a closely related problem viz., classifying sentences in dialogues through dialogue-speci\ufb01c categories called dialogue acts [21], which we will not review here. Just as one example, Cotterill [4] classi\ufb01es ques- tions in emails into the dialogue acts of YES NO QUESTION, WH QUESTION, ACTION REQUEST, RHETORICAL, MULTIPLE CHOICE etc. We could not \ufb01nd much work related to mining of performance appraisals data. Pawar et al. [16] uses kernel-based classi\ufb01cation to classify sentences in both performance appraisal text and product reviews into classes SUGGESTION, AP- PRECIATION, COMPLAINT. Apte et al. [1] provides two algorithms for matching the descriptions of goals or tasks assigned to employees to a standard template of model goals. One algorithm is based on the co-training framework and uses goal descriptions and self-appraisal comments as two separate perspectives.",
  "Apte et al. [1] provides two algorithms for matching the descriptions of goals or tasks assigned to employees to a standard template of model goals. One algorithm is based on the co-training framework and uses goal descriptions and self-appraisal comments as two separate perspectives. The second approach uses semantic similarity under a weak supervision framework. Ramrakhiyani et al. [19] proposes label propagation algorithms to discover as- pects in supervisor assessments in performance appraisals, where an aspect is modelled as a verb-noun pair (e.g. conduct training, improve coding). 3 Dataset In this paper, we used the supervisor assessment and peer feedback text produced during the performance appraisal of 4528 employees in a large multi-national IT company. The corpus of supervisor assessment has 26972 sentences. The sum- mary statistics about the number of words in a sentence is: min:4 max:217 av- erage:15.5 STDEV:9.2 Q1:9 Q2:14 Q3:19. 4 Sentence Classi\ufb01cation The PA corpus contains several classes of sentences that are of interest.",
  "4 Sentence Classi\ufb01cation The PA corpus contains several classes of sentences that are of interest. In this paper, we focus on three important classes of sentences viz., sentences that discuss strengths (class STRENGTH), weaknesses of employees (class WEAK- NESS) and suggestions for improving her performance (class SUGGESTION). The strengths or weaknesses are mostly about the performance in work carried out, but sometimes they can be about the working style or other personal",
  "4 qualities. The classes WEAKNESS and SUGGESTION are somewhat overlapping; e.g., a suggestion may address a perceived weakness. Following are two example sentences in each class. STRENGTH: \u2013 Excellent technology leadership and delivery capabilities along with ability to groom technology champions within the team. \u2013 He can drive team to achieve results and can take pressure. WEAKNESS: \u2013 Sometimes exhibits the quality that he knows more than the others in the room which puts off others. \u2013 Tends to stretch himself and team a bit too hard. SUGGESTION: \u2013 X has to attune himself to the vision of the business unit and its goals a little more than what is being currently exhibited. \u2013 Need to improve on business development skills, articulation of business and solution benefits. Several linguistic aspects of these classes of sentences are apparent. The sub- ject is implicit in many sentences. The strengths are often mentioned as either noun phrases (NP) with positive adjectives (Excellent technology leadership) or positive nouns (engineering strength) or through verbs with positive po- larity (dedicated) or as verb phrases containing positive adjectives (delivers innovative solutions).",
  "The strengths are often mentioned as either noun phrases (NP) with positive adjectives (Excellent technology leadership) or positive nouns (engineering strength) or through verbs with positive po- larity (dedicated) or as verb phrases containing positive adjectives (delivers innovative solutions). Similarly for weaknesses, where negation is more fre- quently used (presentations are not his forte), or alternatively, the polarities of verbs (avoid) or adjectives (poor) tend to be negative. However, sometimes the form of both the strengths and weaknesses is the same, typically a stand- alone sentiment-neutral NP, making it di\ufb03cult to distinguish between them; e.g., adherence to timing or timely closure. Suggestions often have an imperative mood and contain secondary verbs such as need to, should, has to. Sugges- tions are sometimes expressed using comparatives (better process compliance). We built a simple set of patterns for each of the 3 classes on the POS-tagged form of the sentences. We use each set of these patterns as an unsupervised sentence classi\ufb01er for that class.",
  "We built a simple set of patterns for each of the 3 classes on the POS-tagged form of the sentences. We use each set of these patterns as an unsupervised sentence classi\ufb01er for that class. If a particular sentence matched with patterns for multiple classes, then we have simple tie-breaking rules for picking the \ufb01nal class. The pattern for the STRENGTH class looks for the presence of positive words / phrases like takes ownership, excellent, hard working, commitment, etc. Similarly, the pattern for the WEAKNESS class looks for the presence of negative words / phrases like lacking, diffident, slow learner, less focused, etc. The SUGGESTION pattern not only looks for keywords like should, needs to but also for POS based pattern like \u201ca verb in the base form (VB) in the beginning of a sentence\u201d.",
  "5 We randomly selected 2000 sentences from the supervisor assessment cor- pus and manually tagged them (dataset D1). This labelled dataset contained 705, 103, 822 and 370 sentences having the class labels STRENGTH, WEAKNESS, SUGGESTION or OTHER respectively. We trained several multi-class classi\ufb01ers on this dataset. Table 1 shows the results of 5-fold cross-validation experiments on dataset D1. For the \ufb01rst 5 classi\ufb01ers, we used their implementation from the SciKit Learn library in Python (scikit-learn.org). The features used for these classi\ufb01ers were simply the sentence words along with their frequencies. For the last 2 classi\ufb01ers (in Table 1), we used our own implementation. The overall accu- racy for a classi\ufb01er is de\ufb01ned as A = #correct predictions #data points , where the denominator is 2000 for dataset D1. Note that the pattern-based approach is unsupervised i.e., it did not use any training data.",
  "The overall accu- racy for a classi\ufb01er is de\ufb01ned as A = #correct predictions #data points , where the denominator is 2000 for dataset D1. Note that the pattern-based approach is unsupervised i.e., it did not use any training data. Hence, the results shown for it are for the entire dataset and not based on cross-validation. Table 1. Results of 5-fold cross validation for sentence classi\ufb01cation on dataset D1. STRENGTH WEAKNESS SUGGESTION Classi\ufb01er P R F P R F P R F A Logistic Regression 0.715 0.759 0.736 0.309 0.204 0.246 0.788 0.749 0.768 0.674 Multinomial Naive Bayes 0.719 0.723 0.721 0.246 0.155 0.190 0.672 0.790 0.723 0.646 Random Forest 0.681 0.688 0.685 0.286 0.039 0.068 0.",
  "723 0.721 0.246 0.155 0.190 0.672 0.790 0.723 0.646 Random Forest 0.681 0.688 0.685 0.286 0.039 0.068 0.730 0.734 0.732 0.638 AdaBoost 0.522 0.888 0.657 0.265 0.087 0.131 0.825 0.618 0.707 0.604 Linear SVM 0.718 0.698 0.708 0.357 0.194 0.252 0.744 0.759 0.751 0.651 SVM with ADWSK [16] 0.789 0.847 0.817 0.491 0.262 0.342 0.844 0.871 0.857 0.771 Pattern-based 0.825 0.687 0.749 0.976 0.494 0.656 0.835 0.828 0.832 0.698 4.",
  "342 0.844 0.871 0.857 0.771 Pattern-based 0.825 0.687 0.749 0.976 0.494 0.656 0.835 0.828 0.832 0.698 4.1 Comparison with Sentiment Analyzer We also explored whether a sentiment analyzer can be used as a baseline for identifying the class labels STRENGTH and WEAKNESS. We used an implemen- tation of sentiment analyzer from TextBlob1 to get a polarity score for each sentence. Table 2 shows the distribution of positive, negative and neutral sen- timents across the 3 class labels STRENGTH, WEAKNESS and SUGGESTION. It can be observed that distribution of positive and negative sentiments is almost similar in STRENGTH as well as SUGGESTION sentences, hence we can conclude that the information about sentiments is not much useful for our classi\ufb01cation problem. 4.2 Discovering Clusters within Sentence Classes After identifying sentences in each class, we can now answer question (1) in Section 1.",
  "4.2 Discovering Clusters within Sentence Classes After identifying sentences in each class, we can now answer question (1) in Section 1. From 12742 sentences predicted to have label STRENGTH, we extract 1 https://textblob.readthedocs.io/en/dev/",
  "6 Table 2. Results of TextBlob sentiment analyzer on the dataset D1 Sentence Class Positive Negative Neutral STRENGTH 544 44 117 WEAKNESS 44 24 35 SUGGESTION 430 52 340 Table 3. 5 representative clusters in strengths. Strength cluster Count motivation expertise knowledge talent skill 1851 coaching team coach 1787 professional career job work working training practice 1531 opportunity focus attention success future potential impact result change 1431 sales retail company business industry marketing product 1251 nouns that indicate the actual strength, and cluster them using a simple clus- tering algorithm which uses the cosine similarity between word embeddings2 of these nouns. We repeat this for the 9160 sentences with predicted label WEAK- NESS or SUGGESTION as a single class. Tables 3 and 4 show a few represen- tative clusters in strengths and in weaknesses, respectively. We also explored clustering 12742 STRENGTH sentences directly using CLUTO [8] and Carrot2 Lingo [14] clustering algorithms. Carrot2 Lingo3 discovered 167 clusters and also assigned labels to these clusters.",
  "We also explored clustering 12742 STRENGTH sentences directly using CLUTO [8] and Carrot2 Lingo [14] clustering algorithms. Carrot2 Lingo3 discovered 167 clusters and also assigned labels to these clusters. We then generated 167 clusters using CLUTO as well. CLUTO does not generate cluster labels automatically, hence we used 5 most frequent words within the cluster as its labels. Table 5 shows the largest 5 clusters by both the algorithms. It was observed that the clusters created by CLUTO were more meaningful and informative as compared to those by Carrot2 Lingo. Also, it was observed that there is some correspondence between noun clusters and sentence clusters. E.g. the nouns cluster motivation expertise knowledge talent skill (Table 3) corresponds to the CLUTO sentence clus- ter skill customer management knowledge team (Table 5). But overall, users found the nouns clusters to be more meaningful than the sentence clusters.",
  "E.g. the nouns cluster motivation expertise knowledge talent skill (Table 3) corresponds to the CLUTO sentence clus- ter skill customer management knowledge team (Table 5). But overall, users found the nouns clusters to be more meaningful than the sentence clusters. 2 We used 100 dimensional word vectors trained on Wikipedia 2014 and Gigaword 5 corpus, available at: https://nlp.stanford.edu/projects/glove/ 3 We used the default parameter settings for Carrot2 Lingo algorithm as mentioned at: http://download.carrot2.org/head/manual/index.html Table 4. 5 representative clusters in weaknesses and suggestions. Weakness cluster Count motivation expertise knowledge talent skill 1308 market sales retail corporate marketing commercial industry business 1165 awareness emphasis focus 1165 coaching team coach 1149 job work working task planning 1074",
  "7 Table 5. Largest 5 sentence clusters within 12742 STRENGTH sentences Algorithm Cluster #Sentences CLUTO performance performer perform years team 510 skill customer management knowledge team 325 role delivery work place show 289 delivery manage management manager customer 259 knowledge customer business experience work 250 Carrot2 manager manage 1824 team team 1756 delivery management 451 manage team 376 customer management 321 5 PA along Attributes In many organizations, PA is done from a prede\ufb01ned set of perspectives, which we call attributes. Each attribute covers one speci\ufb01c aspect of the work done by the employees. This has the advantage that we can easily compare the performance of any two employees (or groups of employees) along any given attribute. We can correlate various performance attributes and \ufb01nd dependencies among them. We can also cluster employees in the workforce using their supervisor ratings for each attribute to discover interesting insights into the workforce. The HR managers in the organization considered in this paper have de\ufb01ned 15 attributes (Table 6). Each attribute is essentially a work item or work category described at an abstract level.",
  "The HR managers in the organization considered in this paper have de\ufb01ned 15 attributes (Table 6). Each attribute is essentially a work item or work category described at an abstract level. For example, FUNCTIONAL EXCELLENCE covers any tasks, goals or activities related to the software engineering life-cycle (e.g., requirements analysis, design, coding, testing etc.) as well as technologies such as databases, web services and GUI. In the example in Section 4, the \ufb01rst sentence (which has class STRENGTH) can be mapped to two attributes: FUNCTIONAL EXCELLENCE and BUILD- ING EFFECTIVE TEAMS. Similarly, the third sentence (which has class WEAK- NESS) can be mapped to the attribute INTERPERSONAL EFFECTIVENESS and so forth. Thus, in order to answer the second question in Section 1, we need to map each sentence in each of the 3 classes to zero, one, two or more attributes, which is a multi-class multi-label classi\ufb01cation problem. We manually tagged the same 2000 sentences in Dataset D1 with attributes, where each sentence may get 0, 1, 2, etc.",
  "We manually tagged the same 2000 sentences in Dataset D1 with attributes, where each sentence may get 0, 1, 2, etc. up to 15 class labels (this is dataset D2). This labelled dataset contained 749, 206, 289, 207, 91, 223, 191, 144, 103, 80, 82, 42, 29, 15, 24 sentences having the class labels listed in Table 6 in the same order. The number of sentences having 0, 1, 2, or more than 2 attributes are: 321, 1070, 470 and 139 respectively. We trained several multi-class multi-label classi\ufb01ers on this dataset. Table 7 shows the results of 5-fold cross-validation experiments on dataset D2. Precision, Recall and F-measure for this multi-label classi\ufb01cation are com- puted using a strategy similar to the one described in [6]. Let Pi be the set of",
  "8 Table 6. Strengths, Weaknesses and Suggestions along Performance Attributes Performance Attributes #Strengths #Weaknesses #Suggestions FUNCTIONAL EXCELLENCE 321 26 284 BUILDING EFFECTIVE TEAMS 80 6 89 INTERPERSONAL EFFECTIVENESS 151 16 97 CUSTOMER FOCUS 100 5 76 INNOVATION MANAGEMENT 22 4 53 EFFECTIVE COMMUNICATION 53 17 124 BUSINESS ACUMEN 39 10 103 TAKING OWNERSHIP 47 3 81 PEOPLE DEVELOPMENT 31 8 57 DRIVE FOR RESULTS 37 4 30 STRATEGIC CAPABILITY 8 4 51 WITHSTANDING PRESSURE 16 6 16 DEALING WITH AMBIGUITIES 4 8 12 MANAGING VISION AND PURPOSE 3 0 9 TIMELY DECISION MAKING 6 2 10 Table 7. Results of 5-fold cross validation for multi-class multi-label classi\ufb01cation on dataset D2.",
  "Results of 5-fold cross validation for multi-class multi-label classi\ufb01cation on dataset D2. Classi\ufb01er Precision P Recall R F Logistic Regression 0.715 0.711 0.713 Multinomial Naive Bayes 0.664 0.588 0.624 Random Forest 0.837 0.441 0.578 AdaBoost 0.794 0.595 0.680 Linear SVM 0.722 0.672 0.696 Pattern-based 0.750 0.679 0.713 predicted labels and Ai be the set of actual labels for the ith instance. Precision and recall for this instance are computed as follows: Precisioni = |Pi \u2229Ai| |Pi| , Recalli = |Pi \u2229Ai| |Ai| It can be observed that Precisioni would be unde\ufb01ned if Pi is empty and simi- larly Recalli would be unde\ufb01ned when Ai is empty. Hence, overall precision and recall are computed by averaging over all the instances except where they are unde\ufb01ned.",
  "Hence, overall precision and recall are computed by averaging over all the instances except where they are unde\ufb01ned. Instance-level F-measure can not be computed for instances where ei- ther precision or recall are unde\ufb01ned. Therefore, overall F-measure is computed using the overall precision and recall. 6 Summarization of Peer Feedback using ILP The PA system includes a set of peer feedback comments for each employee. To answer the third question in Section 1, we need to create a summary of all the",
  "9 peer feedback comments about a given employee. As an example, following are the feedback comments from 5 peers of an employee. 1. vast knowledge on different technologies 2. His experience and wast knowledge mixed with his positive attitude, willingness to teach and listen and his humble nature. 3. Approachable, Knowlegeable and is of helping nature. 4. Dedication, Technical expertise and always supportive 5. Effective communication and team player The individual sentences in the comments written by each peer are \ufb01rst identi\ufb01ed and then POS tags are assigned to each sentence. We hypothesize that a good summary of these multiple comments can be constructed by identifying a set of important text fragments or phrases. Initially, a set of candidate phrases is extracted from these comments and a subset of these candidate phrases is chosen as the \ufb01nal summary, using Integer Linear Programming (ILP). The details of the ILP formulation are shown in Table 8. As an example, following is the summary generated for the above 5 peer comments.",
  "The details of the ILP formulation are shown in Table 8. As an example, following is the summary generated for the above 5 peer comments. humble nature, effective communication, technical expertise, always supportive, vast knowledge Following rules are used to identify candidate phrases: \u2013 An adjective followed by in which is followed by a noun phrase (e.g. good in customer relationship) \u2013 A verb followed by a noun phrase (e.g. maintains work life balance) \u2013 A verb followed by a preposition which is followed by a noun phrase (e.g. engage in discussion) \u2013 Only a noun phrase (e.g. excellent listener) \u2013 Only an adjective (e.g. supportive) Various parameters are used to evaluate a candidate phrase for its importance. A candidate phrase is more important: \u2013 if it contains an adjective or a verb or its headword is a noun having WordNet lexical category noun.attribute (e.g. nouns such as dedication, sincerity) \u2013 if it contains more number of words \u2013 if it is included in comments of multiple peers \u2013 if it represents any of the performance attributes such as Innovation, Cus- tomer, Strategy etc.",
  "nouns such as dedication, sincerity) \u2013 if it contains more number of words \u2013 if it is included in comments of multiple peers \u2013 if it represents any of the performance attributes such as Innovation, Cus- tomer, Strategy etc. A complete list of parameters is described in detail in Table 8. There is a trivial constraint C0 which makes sure that only K out of N candi- date phrases are chosen. A suitable value of K is used for each employee depend- ing on number of candidate phrases identi\ufb01ed across all peers (see Algorithm 1). Another set of constraints (C1 to C10) make sure that at least one phrase is selected for each of the leadership attributes. The constraint C11 makes sure that multiple phrases sharing the same headword are not chosen at a time. Also,",
  "10 single word candidate phrases are chosen only if they are adjectives or nouns with lexical category noun.attribute. This is imposed by the constraint C12. It is important to note that all the constraints except C0 are soft constraints, i.e. there may be feasible solutions which do not satisfy some of these constraints. But each constraint which is not satis\ufb01ed, results in a penalty through the use of slack variables. These constraints are described in detail in Table 8. The objective function maximizes the total importance score of the selected candidate phrases. At the same time, it also minimizes the sum of all slack variables so that the minimum number of constraints are broken. Data: N: No. of candidate phrases Result: K: No. of phrases to select as part of summary if N \u226410 then K \u2190\u230aN \u22170.5\u230b; else if N \u226420 then K \u2190\u230agetNoOfPhrasesT oSelect(10) + (N \u221210) \u22170.4\u230b;",
  "5\u230b; else if N \u226420 then K \u2190\u230agetNoOfPhrasesT oSelect(10) + (N \u221210) \u22170.4\u230b; else if N \u226430 then K \u2190\u230agetNoOfPhrasesT oSelect(20) + (N \u221220) \u22170.3\u230b; else if N \u226450 then K \u2190\u230agetNoOfPhrasesT oSelect(30) + (N \u221230) \u22170.2\u230b; else K \u2190\u230agetNoOfPhrasesT oSelect(50) + (N \u221250) \u22170.1\u230b; end if K < 4 and N \u22654 then K \u21904 else if K < 4 then K \u2190N else if K > 20 then K \u219020 end Algorithm 1: getNoOfPhrasesT oSelect (For determining number of phrases to select to include in summary) 6.1 Evaluation of auto-generated summaries We considered a dataset of 100 employees, where for each employee multiple peer comments were recorded. Also, for each employee, a manual summary was generated by an HR personnel.",
  "1 Evaluation of auto-generated summaries We considered a dataset of 100 employees, where for each employee multiple peer comments were recorded. Also, for each employee, a manual summary was generated by an HR personnel. The summaries generated by our ILP-based approach were compared with the corresponding manual summaries using the ROUGE [11] unigram score. For comparing performance of our ILP-based sum- marization algorithm, we explored a few summarization algorithms provided by the Sumy package4. A common parameter which is required by all these 4 https://github.com/miso-belica/sumy",
  "11 Table 8. Integer Linear Program (ILP) formulation Parameters: \u2013 N: No. of phrases \u2013 K: No. of phrases to be chosen for inclusion in the \ufb01nal summary \u2013 Freq: Array of size N, Freqi = no. of distinct peers mentioning the ith phrase \u2013 Adj: Array of size N, Adji = 1 if the ith phrase contains any adjective \u2013 V erb: Array of size N, V erbi = 1 if the ith phrase contains any verb \u2013 NumW ords: Array of size N, NumW ordsi = 1 no.",
  "of words in the ith phrase \u2013 NounCat: Array of size N, NounCati = 1 if lexical category (WordNet) of headword of the ith phrase is noun.attribute \u2013 InvalidSingleNoun: Array of size N, InvalidSingleNouni = 1 if the ith phrase is single word phrase which is neither an adjective nor a noun having lexical category (WordNet) noun.attribute \u2013 Leadership, T eam, Innovation, Communication, Knowledge, Delivery, Ownership, Customer, Strategy,Personal: Indicator arrays of size N each, representing whether any phrase corresponds to a particular performance attribute, e.g. Customeri = 1 indicates that ith phrase is of type Customer \u2013 S: Matrix of dimensions N \u00d7 N, where Sij = 1 if headwords of ith and jth phrase are same Variables: \u2013 X: Array of N binary variables, where Xi = 1 only when ith phrase is chosen to be the part of \ufb01nal summary \u2013 S1, S2, \u00b7 \u00b7 \u00b7 S12: Integer slack variables Objective: Maximize PN i=1 ((NounCati + Adji + V erbi + 1) \u00b7 Freqi \u00b7 NumW ordsi \u00b7 Xi) \u221210000",
  "S2, \u00b7 \u00b7 \u00b7 S12: Integer slack variables Objective: Maximize PN i=1 ((NounCati + Adji + V erbi + 1) \u00b7 Freqi \u00b7 NumW ordsi \u00b7 Xi) \u221210000 \u00b7 P12 j=1 Sj Constraints: C0: PN i=1 Xi = K (Exactly K phrases should be chosen) C1: PN i=1(Leadershipi \u00b7 Xi) + S1 \u22651 C2: PN i=1(T eami \u00b7 Xi) + S2 \u22651 C3: PN i=1(Knowledgei \u00b7 Xi) + S3 \u22651 C4: PN i=1(Deliveryi \u00b7 Xi) + S4 \u22651 C5: PN i=1(Ownershipi \u00b7 Xi) + S5 \u22651 C6: PN i=1(Innovationi \u00b7 Xi) + S6 \u22651 C7: PN i=1(Communicationi \u00b7 Xi) + S7 \u22651 C8: PN i=1(Customeri \u00b7 Xi) + S8 \u22651 C9: PN i=1(Strategyi \u00b7 Xi) + S9 \u22651 C10: PN i=1(Personali \u00b7 Xi) +",
  "PN i=1(Communicationi \u00b7 Xi) + S7 \u22651 C8: PN i=1(Customeri \u00b7 Xi) + S8 \u22651 C9: PN i=1(Strategyi \u00b7 Xi) + S9 \u22651 C10: PN i=1(Personali \u00b7 Xi) + S10 \u22651 (At least one phrase should be chosen to represent each leadership attribute) C11: PN i=1 PN j=1,s.t.i\u0338=j(Sij \u00b7 (Xi + Xj \u22121)) + S11 <= 0 (No duplicate phrases should be chosen) C12: PN i=1(InvalidSingleNouni \u00b7 Xi) \u2212S12 <= 0 (Single word noun phrases are not preferred if they are not noun.attribute)",
  "12 algorithms is number of sentences keep in the \ufb01nal summary. ILP-based summa- rization requires a similar parameter K, which is automatically decided based on number of total candidate phrases. Assuming a sentence is equivalent to roughly 3 phrases, for Sumy algorithms, we set number of sentences parameter to the ceiling of K/3. Table 9 shows average and standard deviation of ROUGE un- igram f1 scores for each algorithm, over the 100 summaries. The performance of ILP-based summarization is comparable with the other algorithms, as the two sample t-test does not show statistically signi\ufb01cant di\ufb00erence. Also, human evaluators preferred phrase-based summary generated by our approach to the other sentence-based summaries. Table 9. Comparative performance of various summarization algorithms Algorithm ROUGE unigram F1 Average Std.",
  "Also, human evaluators preferred phrase-based summary generated by our approach to the other sentence-based summaries. Table 9. Comparative performance of various summarization algorithms Algorithm ROUGE unigram F1 Average Std. Deviation LSA 0.254 0.146 TextRank 0.254 0.146 LexRank 0.258 0.148 ILP-based summary 0.243 0.15 7 Conclusions and Further Work In this paper, we presented an analysis of the text generated in Performance Appraisal (PA) process in a large multi-national IT company. We performed sentence classi\ufb01cation to identify strengths, weaknesses and suggestions for im- provements found in the supervisor assessments and then used clustering to discover broad categories among them. As this is non-topical classi\ufb01cation, we found that SVM with ADWS kernel [16] produced the best results. We also used multi-class multi-label classi\ufb01cation techniques to match supervisor assessments to prede\ufb01ned broad perspectives on performance. Logistic Regression classi\ufb01er was observed to produce the best results for this topical classi\ufb01cation.",
  "We also used multi-class multi-label classi\ufb01cation techniques to match supervisor assessments to prede\ufb01ned broad perspectives on performance. Logistic Regression classi\ufb01er was observed to produce the best results for this topical classi\ufb01cation. Finally, we proposed an ILP-based summarization technique to produce a summary of peer feedback comments for a given employee and compared it with manual summaries. The PA process also generates much structured data, such as supervisor ratings. It is an interesting problem to compare and combine the insights from discovered from structured data and unstructured text. Also, we are planning to automatically discover any additional performance attributes to the list of 15 attributes currently used by HR. References 1. M. Apte, S. Pawar, S. Patil, S. Baskaran, A. Shrivastava, and G.K. Palshikar. Short text matching in performance management. In Proceedings of the 21st International",
  "13 Conference on Management of Data (COMAD 2016), pages 13\u201323, 2016. 2. Vitor R. Carvalho and William W. Cohen. Improving \u201demail speech acts\u201d anal- ysis via n-gram selection. In Proceedings of the HLT-NAACL 2006 Workshop on Analyzing Conversations in Text and Speech, ACTS \u201909, pages 35\u201341, 2006. 3. W.W. Cohen, V.R. Carvalho, and T.M. Mitchell. Learning to classify email into \u201dspeech acts\u201d. In Proc. Empirical Methods in Natural Language Processing (EMNLP-2004), pages 309\u2013316, 2004. 4. Rachel Cotterill. Question classi\ufb01cation for email. In Proc. Ninth Int. Conf. Com- putational Semantics (IWCS 2011), 2011. 5. S. Deshpande, G.K. Palshilkar, and G Athiappan. An unsupervised approach to sentence classi\ufb01cation.",
  "Conf. Com- putational Semantics (IWCS 2011), 2011. 5. S. Deshpande, G.K. Palshilkar, and G Athiappan. An unsupervised approach to sentence classi\ufb01cation. In Proc. Int. Conf. on Management of Data (COMAD 2010), pages 88\u201399, 2010. 6. Shantanu Godbole and Sunita Sarawagi. Discriminative methods for multi-labeled classi\ufb01cation. In PAKDD 2004, pages 22\u201330, 2004. 7. B. Hachey and C. Grover. Sequence modelling for sentence classi\ufb01cation in a legal summarisation system. In Proc. 2005 ACM Symposium on Applied Computing, 2005. 8. George Karypis. Cluto-a clustering toolkit. Technical report, DTIC Document, 2002. 9. A. Khoo, Y. Marom, and D. Albrecht. Experiments with sentence classi\ufb01cation.",
  "8. George Karypis. Cluto-a clustering toolkit. Technical report, DTIC Document, 2002. 9. A. Khoo, Y. Marom, and D. Albrecht. Experiments with sentence classi\ufb01cation. In Proc. 2006 Australasian Language Technology Workshop (ALTW2006), pages 18\u201325, 2006. 10. P.E. Levy and J.R. Williams. The social context of performance appraisal: a review and framework for the future. Journal of Management, 30(6):881\u2013905, 2004. 11. Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out: Proceedings of the ACL-04 workshop, volume 8. Barcelona, Spain, 2004. 12. L. McKnight and P. Srinivasan. Categorization of sentence types in medical ab- stracts. In Proc. American Medical Informatics Association Annual Symposium, pages 440\u2013444, 2003. 13. K.R.",
  "12. L. McKnight and P. Srinivasan. Categorization of sentence types in medical ab- stracts. In Proc. American Medical Informatics Association Annual Symposium, pages 440\u2013444, 2003. 13. K.R. Murphy and J. Cleveland. Understanding Performance Appraisal: Social, Organizational and Goal-Based Perspective. Sage Publishers, 1995. 14. Stanislaw Osinski, Jerzy Stefanowski, and Dawid Weiss. Lingo: Search results clus- tering algorithm based on singular value decomposition. In Intelligent Information Processing and Web Mining, Proceedings of the International IIS: IIPWM\u201904 Con- ference held in Zakopane, Poland, May 17-20, 2004, pages 359\u2013368, 2004. 15. G.K. Palshikar, S. Deshpande, and S. Bhat. Quest: Discovering insights from survey responses. In Proceedings of the 8th Australasian Data Mining Conf. (AusDM09), pages 83\u201392, 2009. 16.",
  "Palshikar, S. Deshpande, and S. Bhat. Quest: Discovering insights from survey responses. In Proceedings of the 8th Australasian Data Mining Conf. (AusDM09), pages 83\u201392, 2009. 16. S. Pawar, N. Ramrakhiyani, G. K. Palshikar, and S. Hingmire. Deciphering review comments: Identifying suggestions, appreciations and complaints. In Proc. 20th Int. Conf. on Applications of Natural Language to Information Systems (NLDB 2015), LNCS 9103, pages 204\u2013211, 2015. 17. Ashequl Qadir and Ellen Rilo\ufb00. Classifying sentences as speech acts in message board posts. In Proc. Empirical Methods in Natural Language Processing (EMNLP- 2011), 2011. 18. N. Ramrakhiyani, S. Pawar, and G.K. Palshikar. A system for classi\ufb01cation of propositions of the indian supreme court judgements.",
  "18. N. Ramrakhiyani, S. Pawar, and G.K. Palshikar. A system for classi\ufb01cation of propositions of the indian supreme court judgements. In Proc. 5th 2013 Forum on Information Retrieval Evaluation (FIRE 2013), pages 1\u20134, 2013.",
  "14 19. N. Ramrakhiyani, S. Pawar, G.K. Palshikar, and M. Apte. Aspects from appraisals: A label propagation with prior induction approach. In Proceedings of the 21st Inter- national Conference on Applications of Natural Language to Information Systems (NLDB 2016), volume LNCS 9612, pages 301\u2013309, 2016. 20. M. Schraeder, J. Becton, and R. Portis. A critical examination of performance appraisals. The Journal for Quality and Participation, (spring):20\u201325, 2007. 21. Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor, Rachel Martin, Carol Van Ess-Dykema, and Marie Meteer. Dialogue act modeling for automatic tagging and recognition of conversa- tional speech. Computational Linguistics, 26(3), 2000. 22. C. Viswesvaran.",
  "Dialogue act modeling for automatic tagging and recognition of conversa- tional speech. Computational Linguistics, 26(3), 2000. 22. C. Viswesvaran. Assessment of individual job performance: a review of the past century and a look ahead. In N. Anderson, D.S. Ones, H.K. Sinangil, and C. Viswes- varan, editors, Handbook of Industrial, Work and Organizational Psychology. Sage Publishers, 2001. 23. Y. Yamamoto and T. Takagi. A sentence classi\ufb01cation system for multi biomedical literature summarization. In Proc. 21st International Conference on Data Engi- neering Workshops, pages 1163\u20131168, 2005."
]