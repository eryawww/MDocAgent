[
  "Minimally Supervised Learning of Affective Events Using Discourse Relations Jun Saito Yugo Murawaki Graduate School of Informatics, Kyoto University Yoshida-honmachi, Sakyo-ku, Kyoto, 606-8501, Japan {saito, murawaki, kuro}@nlp.ist.i.kyoto-u.ac.jp Sadao Kurohashi Abstract Recognizing affective events that trigger pos- itive or negative sentiment has a wide range of natural language processing applications but remains a challenging problem mainly be- cause the polarity of an event is not neces- sarily predictable from its constituent words. In this paper, we propose to propagate affec- tive polarity using discourse relations. Our method is simple and only requires a very small seed lexicon and a large raw corpus. Our experiments using Japanese data show that our method learns affective events effec- tively without manually labeled data. It also improves supervised learning results when la- beled data are small. 1 Introduction Affective events (Ding and Riloff, 2018) are events that typically affect people in positive or negative ways.",
  "It also improves supervised learning results when la- beled data are small. 1 Introduction Affective events (Ding and Riloff, 2018) are events that typically affect people in positive or negative ways. For example, getting money and playing sports are usually positive to the experi- encers; catching cold and losing one\u2019s wallet are negative. Understanding affective events is impor- tant to various natural language processing (NLP) applications such as dialogue systems (Shi and Yu, 2018), question-answering systems (Oh et al., 2012), and humor recognition (Liu et al., 2018). In this paper, we work on recognizing the polarity of an affective event that is represented by a score ranging from \u22121 (negative) to 1 (positive). Learning affective events is challenging be- cause, as the examples above suggest, the po- larity of an event is not necessarily predictable from its constituent words. Combined with the unbounded combinatorial nature of language, the non-compositionality of affective polarity entails the need for large amounts of world knowledge, which can hardly be learned from small annotated data.",
  "Combined with the unbounded combinatorial nature of language, the non-compositionality of affective polarity entails the need for large amounts of world knowledge, which can hardly be learned from small annotated data. In this paper, we propose a simple and effective method for learning affective events that only re- quires a very small seed lexicon and a large raw corpus. As illustrated in Figure 1, our key idea is that we can exploit discourse relations (Prasad et al., 2008) to ef\ufb01ciently propagate polarity from seed predicates that directly report one\u2019s emotions (e.g., \u201cto be glad\u201d is positive). Suppose that events x1 are x2 are in the discourse relation of CAUSE (i.e., x1 causes x2). If the seed lexicon suggests x2 is positive, x1 is also likely to be positive because it triggers the positive emotion. The fact that x2 is known to be negative indicates the negative po- larity of x1.",
  "If the seed lexicon suggests x2 is positive, x1 is also likely to be positive because it triggers the positive emotion. The fact that x2 is known to be negative indicates the negative po- larity of x1. Similarly, if x1 and x2 are in the dis- course relation of CONCESSION (i.e., x2 in spite of x1), the reverse of x2\u2019s polarity can be prop- agated to x1. Even if x2\u2019s polarity is not known in advance, we can exploit the tendency of x1 and x2 to be of the same polarity (for CAUSE) or of the reverse polarity (for CONCESSION) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polar- ity of a given event. We trained the models using a Japanese web corpus. Given the minimum amount of supervi- sion, they performed well. In addition, the combi- nation of annotated and unannotated data yielded a gain over a purely supervised baseline when la- beled data were small.",
  "We trained the models using a Japanese web corpus. Given the minimum amount of supervi- sion, they performed well. In addition, the combi- nation of annotated and unannotated data yielded a gain over a purely supervised baseline when la- beled data were small. 2 Related Work Learning affective events is closely related to sen- timent analysis. Whereas sentiment analysis usu- ally focuses on the polarity of what are described (e.g., movies), we work on how people are typ- ically affected by events. In sentiment analysis, much attention has been paid to compositional- ity. Word-level polarity (Takamura et al., 2005; arXiv:1909.00694v2  [cs.CL]  28 Dec 2019",
  "Type Former event Latter event Relation AL \u8a66\u5408\u306b\u52dd\u3064([I] win the game) \u5b09\u3057\u3044([I] am glad) CAUSE AL \u30d4\u30af\u30cb\u30c3\u30af\u306b\u2f8f\u304f([I] go to a picnic) \u5929\u6c17\u304c\u2f3c\u914d\u3060([I] am worried about the weather) CONCESSION CA \u6696\u623f\u304c\u306a\u3044(There is no heating) \u5bd2\u3044([I] am cold) CAUSE CO \u8996\u2f12\u304c\u826f\u3044([I] have good eyes) \u3088\u304f\u2f92\u3048\u306a\u3044([I] cannot see [it] well) CONCESSION \u22121 Propagate the same polarity Propagate the reverse polarity Encourage them to have the same polarity +1 Encourage them to have the reverse polarity +1 +1 Figure 1: An overview of our method. We focus on pairs of events, the former events and the latter events, which are connected with a discourse relation, CAUSE or CONCESSION. Dropped pronouns are indicated by brackets in English translations. We divide the event pairs into three types: AL, CA, and CO.",
  "We focus on pairs of events, the former events and the latter events, which are connected with a discourse relation, CAUSE or CONCESSION. Dropped pronouns are indicated by brackets in English translations. We divide the event pairs into three types: AL, CA, and CO. In AL, the polarity of a latter event is automatically identi\ufb01ed as either positive or negative, according to the seed lexicon (the positive word is colored red and the negative word blue). We propagate the latter event\u2019s polarity to the former event. The same polarity as the latter event is used for the discourse relation CAUSE, and the reversed polarity for CONCESSION. In CA and CO, the latter event\u2019s polarity is not known. Depending on the discourse relation, we encourage the two events\u2019 polarities to be the same (CA) or reversed (CO). Details are given in Section 3.2.",
  "In CA and CO, the latter event\u2019s polarity is not known. Depending on the discourse relation, we encourage the two events\u2019 polarities to be the same (CA) or reversed (CO). Details are given in Section 3.2. Wilson et al., 2005; Baccianella et al., 2010) and the roles of negation and intensi\ufb01cation (Reitan et al., 2015; Wilson et al., 2005; Zhu et al., 2014) are among the most important topics. In contrast, we are more interested in recognizing the senti- ment polarity of an event that pertains to common- sense knowledge (e.g., getting money and catch- ing cold). Label propagation from seed instances is a com- mon approach to inducing sentiment polarities. While Takamura et al. (2005) and Turney (2002) worked on word- and phrase-level polarities, Ding and Riloff (2018) dealt with event-level polarities. Takamura et al.",
  "While Takamura et al. (2005) and Turney (2002) worked on word- and phrase-level polarities, Ding and Riloff (2018) dealt with event-level polarities. Takamura et al. (2005) and Turney (2002) linked instances using co-occurrence information and/or phrase-level coordinations (e.g., \u201cA and B\u201d and \u201cA but B\u201d). We shift our scope to event pairs that are more complex than phrase pairs, and con- sequently exploit discourse connectives as event- level counterparts of phrase-level conjunctions. Ding and Riloff (2018) constructed a network of events using word embedding-derived similar- ities. Compared with this method, our discourse relation-based linking of events is much simpler and more intuitive. Some previous studies made use of document structure to understand the sentiment. Shimizu et al. (2018) proposed a sentiment-speci\ufb01c pre- training strategy using unlabeled dialog data (tweet-reply pairs). Kaji and Kitsuregawa (2006) proposed a method of building a polarity-tagged corpus (ACP Corpus).",
  "Shimizu et al. (2018) proposed a sentiment-speci\ufb01c pre- training strategy using unlabeled dialog data (tweet-reply pairs). Kaji and Kitsuregawa (2006) proposed a method of building a polarity-tagged corpus (ACP Corpus). They automatically gath- ered sentences that had positive or negative opin- ions utilizing HTML layout structures in addition to linguistic patterns. Our method depends only on raw texts and thus has wider applicability. 3 Proposed Method 3.1 Polarity Function Our goal is to learn the polarity function p(x), which predicts the sentiment polarity score of an event x. We approximate p(x) by a neural net- work with the following form: p(x) = tanh(Linear(Encoder(x))). (1) Encoder outputs a vector representation of the event x. Linear is a fully-connected layer and transforms the representation into a scalar. tanh is the hyperbolic tangent and transforms the scalar into a score ranging from \u22121 to 1. In Sec- tion 4.2, we consider two speci\ufb01c implementations of Encoder.",
  "tanh is the hyperbolic tangent and transforms the scalar into a score ranging from \u22121 to 1. In Sec- tion 4.2, we consider two speci\ufb01c implementations of Encoder. 3.2 Discourse Relation-Based Event Pairs Our method requires a very small seed lexicon and a large raw corpus. We assume that we can au- tomatically extract discourse-tagged event pairs, (xi1, xi2) (i = 1, \u00b7 \u00b7 \u00b7 ) from the raw corpus. We refer to xi1 and xi2 as former and latter events, respectively. As shown in Figure 1, we limit our scope to two discourse relations: CAUSE and CONCESSION. The seed lexicon consists of positive and neg- ative predicates. If the predicate of an extracted",
  "event is in the seed lexicon and does not in- volve complex phenomena like negation, we as- sign the corresponding polarity score (+1 for pos- itive events and \u22121 for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types. AL (Automatically Labeled Pairs) The seed lexicon matches (1) the latter event but (2) not the former event, and (3) their discourse relation type is CAUSE or CONCESSION. If the discourse rela- tion type is CAUSE, the former event is given the same score as the latter. Likewise, if the discourse relation type is CONCESSION, the former event is given the opposite of the latter\u2019s score. They are used as reference scores during training. CA (CAUSE Pairs) The seed lexicon matches neither the former nor the latter event, and their discourse relation type is CAUSE. We assume the two events have the same polarities.",
  "They are used as reference scores during training. CA (CAUSE Pairs) The seed lexicon matches neither the former nor the latter event, and their discourse relation type is CAUSE. We assume the two events have the same polarities. CO (CONCESSION Pairs) The seed lexicon matches neither the former nor the latter event, and their discourse relation type is CONCESSION. We assume the two events have the reversed polarities. 3.3 Loss Functions Using AL, CA, and CO data, we optimize the pa- rameters of the polarity function p(x). We de\ufb01ne a loss function for each of the three types of event pairs and sum up the multiple loss functions. We use mean squared error to construct loss functions. For the AL data, the loss function is de\ufb01ned as: LAL = 1 NAL NAL X i=1 (ri2 \u2212p(xi2))2 + \u03bbAL 1 NAL NAL X i=1 (ri1 \u2212p(xi1))2, (2) where xi1 and xi2 are the i-th pair of the AL data.",
  "ri1 and ri2 are the automatically-assigned scores of xi1 and xi2, respectively. NAL is the total num- ber of AL pairs, and \u03bbAL is a hyperparameter. For the CA data, the loss function is de\ufb01ned as: LCA = \u03bbCA 1 NCA NCA X i=1 (p(yi1) \u2212p(yi2))2 + \u00b5 1 NCA NCA X i=1 X u\u2208{yi1,yi2} (1 \u2212p(u)2). (3) yi1 and yi2 are the i-th pair of the CA pairs. NCA is the total number of CA pairs. \u03bbCA and \u00b5 are hyperparameters. The \ufb01rst term makes the scores of the two events closer while the second term pre- vents the scores from shrinking to zero.",
  "NCA is the total number of CA pairs. \u03bbCA and \u00b5 are hyperparameters. The \ufb01rst term makes the scores of the two events closer while the second term pre- vents the scores from shrinking to zero. The loss function for the CO data is de\ufb01ned analogously: LCO = \u03bbCO 1 NCO NCO X i=1 (p(zi1) + p(zi2))2 + \u00b5 1 NCO NCO X i=1 X u\u2208{zi1,zi2} (1 \u2212p(u)2). (4) The difference is that the \ufb01rst term makes the scores of the two events distant from each other. 4 Experiments 4.1 Dataset 4.1.1 AL, CA, and CO As a raw corpus, we used a Japanese web cor- pus that was compiled through the procedures pro- posed by Kawahara and Kurohashi (2006). To ex- tract event pairs tagged with discourse relations, we used the Japanese dependency parser KNP1 and in-house postprocessing scripts (Saito et al., 2018).",
  "To ex- tract event pairs tagged with discourse relations, we used the Japanese dependency parser KNP1 and in-house postprocessing scripts (Saito et al., 2018). KNP used hand-written rules to segment each sentence into what we conventionally called clauses (mostly consecutive text chunks), each of which contained one main predicate. KNP also identi\ufb01ed the discourse relations of event pairs if explicit discourse connectives (Prasad et al., 2008) such as \u201c\u306e\u3067\u201d (because) and \u201c\u306e\u306b\u201d (in spite of) were present. We treated Cause/Reason (\u539f \u56e0\u30fb\u7406\u7531) and Condition (\u6761\u4ef6) in the original tagset (Kawahara et al., 2014) as CAUSE and Con- cession (\u9006\u63a5)2 as CONCESSION, respectively. Here is an example of event pair extraction. 1http://nlp.ist.i.kyoto-u.ac.jp/EN/ index.php?KNP 2 To be precise, this discourse type is semantically broader than Concession and extends to the area of Contrast.",
  "Type of pairs # of pairs AL (Automatically Labeled Pairs) 1,000,000 CA (CAUSE Pairs) 5,000,000 CO (CONCESSION Pairs) 5,000,000 Table 1: Statistics of the AL, CA, and CO datasets. (1) \u91cd\u5927\u306a\u5931\u6557\u3092\u72af\u3057\u305f\u306e\u3067\u3001\u4ed5\u4e8b\u3092\u30af\u30d3 \u306b\u306a\u3063\u305f\u3002 Because [I] made a serious mistake, [I] got \ufb01red. From this sentence, we extracted the event pair of \u201c\u91cd\u5927\u306a\u5931\u6557\u3092\u72af\u3059\u201d ([I] make a serious mis- take) and \u201c\u4ed5\u4e8b\u3092\u30af\u30d3\u306b\u306a\u308b\u201d ([I] get \ufb01red), and tagged it with CAUSE. We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section A.1. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO.",
  "We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section A.1. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was \ufb01ve times larger than AL. The results are shown in Table 1. 4.1.2 ACP (ACP Corpus) We used the latest version3 of the ACP Cor- pus (Kaji and Kitsuregawa, 2006) for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, re- spectively: (2) \u4f5c\u696d\u304c\u697d\u3060\u3002 The work is easy. (3) \u99d0\u8eca\u5834\u304c\u306a\u3044\u3002 There is no parking lot.",
  "For example, the following two sentences were labeled positive and negative, re- spectively: (2) \u4f5c\u696d\u304c\u697d\u3060\u3002 The work is easy. (3) \u99d0\u8eca\u5834\u304c\u306a\u3044\u3002 There is no parking lot. Although the ACP corpus was originally con- structed in the context of sentiment analysis, we found that it could roughly be regarded as a col- lection of affective events. We parsed each sen- tence and extracted the last clause in it. The train/dev/test split of the data is shown in Table 2. The objective function for supervised training is: LACP = 1 NACP NACP X i=1 (Ri \u2212p(vi))2, (5) 3The dataset was obtained from Nobuhiro Kaji via per- sonal communication. Dataset Event polarity # of events Train Positive 299,834 Negative 300,164 Dev Positive 50,118 Negative 49,882 Test Positive 50,046 Negative 49,954 Table 2: Details of the ACP dataset.",
  "Dataset Event polarity # of events Train Positive 299,834 Negative 300,164 Dev Positive 50,118 Negative 49,882 Test Positive 50,046 Negative 49,954 Table 2: Details of the ACP dataset. where vi is the i-th event, Ri is the reference score of vi, and NACP is the number of the events of the ACP Corpus. To optimize the hyperparameters, we used the dev set of the ACP Corpus. For the evaluation, we used the test set of the ACP Corpus. The model output was classi\ufb01ed as positive if p(x) > 0 and negative if p(x) \u22640. 4.2 Model Con\ufb01gurations As for Encoder, we compared two types of neu- ral networks: BiGRU and BERT. GRU (Cho et al., 2014) is a recurrent neural network sequence en- coder. BiGRU reads an input sequence forward and backward and the output is the concatenation of the \ufb01nal forward and backward hidden states.",
  "GRU (Cho et al., 2014) is a recurrent neural network sequence en- coder. BiGRU reads an input sequence forward and backward and the output is the concatenation of the \ufb01nal forward and backward hidden states. BERT (Devlin et al., 2019) is a pre-trained multi-layer bidirectional Transformer (Vaswani et al., 2017) encoder. Its output is the \ufb01nal hid- den state corresponding to the special classi\ufb01ca- tion tag ([CLS]). For the details of Encoder, see Sections A.2. We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corre- sponding objective functions were: LAL, LAL + LCA + LCO, LACP, and LACP + LAL + LCA + LCO. 4.3 Results and Discussion Table 3 shows accuracy. As the Random baseline suggests, positive and negative labels were dis- tributed evenly.",
  "4.3 Results and Discussion Table 3 shows accuracy. As the Random baseline suggests, positive and negative labels were dis- tributed evenly. The Random+Seed baseline made use of the seed lexicon and output the correspond- ing label (or the reverse of it for negation) if the event\u2019s predicate is in the seed lexicon. We can see that the seed lexicon itself had practically no impact on prediction. The models in the top block performed consid- erably better than the random baselines. The per- formance gaps with their (semi-)supervised coun- terparts, shown in the middle block, were less than",
  "Training dataset Encoder Acc AL BiGRU 0.843 BERT 0.863 AL+CA+CO BiGRU 0.866 BERT 0.835 ACP BiGRU 0.919 BERT 0.933 ACP+AL+CA+CO BiGRU 0.917 BERT 0.913 Random 0.500 Random+Seed 0.503 Table 3: Performance of various models on the ACP test set. Training dataset Encoder Acc ACP (6K) BERT 0.876 +AL 0.886 ACP (6K) BiGRU 0.830 +AL+CA+CO 0.879 Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data. 7%. This demonstrates the effectiveness of dis- course relation-based label propagation. Comparing the model variants, we obtained the highest score with the BiGRU encoder trained with the AL+CA+CO dataset. BERT was compet- itive but its performance went down if CA and CO were used in addition to AL.",
  "Comparing the model variants, we obtained the highest score with the BiGRU encoder trained with the AL+CA+CO dataset. BERT was compet- itive but its performance went down if CA and CO were used in addition to AL. We conjecture that BERT was more sensitive to noises found more frequently in CA and CO. Contrary to our expectations, supervised mod- els (ACP) outperformed semi-supervised models (ACP+AL+CA+CO). This suggests that the train- ing set of 0.6 million events is suf\ufb01ciently large for training the models. For comparison, we trained the models with a subset (6,000 events) of the ACP dataset. As the results shown in Table 4 demon- strate, our method is effective when labeled data are small. The result of hyperparameter optimization for the BiGRU encoder was as follows: \u03bbAL = 1, \u03bbCA = 0.35, \u03bbCO = 1, \u00b5 = 0.5. As the CA and CO pairs were equal in size (Table 1), \u03bbCA and \u03bbCO were comparable values.",
  "As the CA and CO pairs were equal in size (Table 1), \u03bbCA and \u03bbCO were comparable values. \u03bbCA was about one-third of \u03bbCO, and this indicated that the CA pairs were noisier than the CO pairs. A major type of CA pairs that violates our assump- tion was in the form of \u201cproblemnegative causes Input event Polarity \u9053\u306b\u8ff7\u3046([I] get lost) -0.771 \u9053\u306b\u8ff7\u308f\u306a\u3044([I] don\u2019t get lost) 0.835 \u7b11\u3046([I] laugh) 0.624 \u7b11\u308f\u308c\u308b([I] am laughed at) -0.687 \u8102\u80aa\u3092\u843d\u3068\u3059([I] lose body fat) 0.452 \u80a9\u3092\u843d\u3068\u3059([I] feel disappointed) -0.653 Table 5: Examples of polarity scores predicted by the BiGRU model trained with AL+CA+CO.",
  "solutionpositive\u201d: (4) (\u60aa\u3044\u3068\u3053\u308d\u304c\u3042\u308b, \u3088\u304f\u306a\u308b\u3088\u3046\u306b\u52aa\u529b \u3059\u308b) (there is a bad point, [I] try to improve [it]) The polarities of the two events were reversed in spite of the CAUSE relation, and this lowered the value of \u03bbCA. Some examples of model outputs are shown in Table 5. The \ufb01rst two examples suggest that our model successfully learned negation without ex- plicit supervision. Similarly, the next two exam- ples differ only in voice but the model correctly recognized that they had opposite polarities. The last two examples share the predicate \u201c\u843d\u3068\u3059\u201d (drop) and only the objects are different. The sec- ond event \u201c\u80a9\u3092\u843d\u3068\u3059\u201d (lit. drop one\u2019s shoul- ders) is an idiom that expresses a disappointed feeling. The examples demonstrate that our model correctly learned non-compositional expressions. 5 Conclusion In this paper, we proposed to use discourse rela- tions to effectively propagate polarities of affec- tive events from seeds.",
  "The examples demonstrate that our model correctly learned non-compositional expressions. 5 Conclusion In this paper, we proposed to use discourse rela- tions to effectively propagate polarities of affec- tive events from seeds. Experiments show that, even with a minimal amount of supervision, the proposed method performed well. Although event pairs linked by discourse analy- sis are shown to be useful, they nevertheless con- tain noises. Adding linguistically-motivated \ufb01lter- ing rules would help improve the performance. Acknowledgments We thank Nobuhiro Kaji for providing the ACP Corpus and Hirokazu Kiyomaru and Yudai Kishi- moto for their help in extracting event pairs. This work was partially supported by Yahoo! Japan Corporation.",
  "References Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas- tiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In Proceedings of the seventh edition of the Lan- guage Resources and Evaluation Conference. Kyunghyun Cho, Bart van Merrienboer, Caglar Gul- cehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Nat- ural Language Processing (EMNLP), pages 1724\u2013 1734, Doha, Qatar. Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing.",
  "Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics. Haibo Ding and Ellen Riloff. 2018. Weakly supervised induction of affective events by optimizing semantic consistency. In Proceedings of the 32nd AAAI Con- ference on Arti\ufb01cial Intelligence. Nobuhiro Kaji and Masaru Kitsuregawa. 2006. Auto- matic construction of polarity-tagged corpus from html documents. In Proceedings of the COL- ING/ACL on Main Conference Poster Sessions, COLING-ACL \u201906, pages 452\u2013459, Stroudsburg, PA, USA. Association for Computational Linguis- tics. Daisuke Kawahara and Sadao Kurohashi. 2006.",
  "Association for Computational Linguis- tics. Daisuke Kawahara and Sadao Kurohashi. 2006. Case frame compilation from the web using high- performance computing. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC\u201906), Genoa, Italy. European Language Resources Association (ELRA). Daisuke Kawahara, Yuichiro Machida, Tomohide Shi- bata, Sadao Kurohashi, Hayato Kobayashi, and Manabu Sassano. 2014. Rapid development of a corpus with discourse annotations using two-stage crowdsourcing. In Proceedings of COLING 2014, the 25th International Conference on Computa- tional Linguistics: Technical Papers, pages 269\u2013 278, Dublin, Ireland. Dublin City University and As- sociation for Computational Linguistics. Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. International Conference on Learning Representations. Lizhen Liu, Donghai Zhang, and Wei Song. 2018. Modeling sentiment association in discourse for hu- mor recognition.",
  "Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. International Conference on Learning Representations. Lizhen Liu, Donghai Zhang, and Wei Song. 2018. Modeling sentiment association in discourse for hu- mor recognition. In Proceedings of the 56th An- nual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 586\u2013 591, Melbourne, Australia. Association for Compu- tational Linguistics. Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto, Takuya Kawada, Stijn De Saeger, Jun\u2019ichi Kazama, and Yiou Wang. 2012. Why question answering us- ing sentiment analysis and word classes. In Pro- ceedings of the 2012 Conference on Empirical Meth- ods on Natural Language Processing and Computa- tional Natural Language Learning. Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt- sakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008.",
  "Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt- sakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn discourse TreeBank 2.0. In Proceedings of the 2012 Conference on Empiri- cal Methods on Natural Language Processing and Computational Natural Language Learning. Johan Reitan, J\u00f8rgen Faret, Bj\u00a8orn Gamb\u00a8ack, and Lars Bungum. 2015. Negation scope detection for twitter sentiment analysis. In Proceedings of the 9th Work- shop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis. Jun Saito, Tomohiro Sakaguchi, Tomohide Shibata, Daisuke Kawahara, and Sadao Kurohashi. 2018. Design and visualization of linguistic information unit based on predicate-argument structure. In Pro- ceedings of the 24th Annual Meeting of Natural Lan- guage Processing (in Japanese). Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.",
  "2018. Design and visualization of linguistic information unit based on predicate-argument structure. In Pro- ceedings of the 24th Annual Meeting of Natural Lan- guage Processing (in Japanese). Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u2013 1725, Berlin, Germany. Association for Computa- tional Linguistics. Weiyan Shi and Zhou Yu. 2018. Sentiment adaptive end-to-end dialog systems. In Proceedings of the 56th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 1509\u20131519, Melbourne, Australia. Association for Computational Linguistics. Toru Shimizu, Nobuyuki Shimizu, and Hayato Kobayashi. 2018. Pretraining sentiment classi\ufb01ers with unlabeled dialog data.",
  "Association for Computational Linguistics. Toru Shimizu, Nobuyuki Shimizu, and Hayato Kobayashi. 2018. Pretraining sentiment classi\ufb01ers with unlabeled dialog data. In Proceedings of the 56th Annual Meeting of the Association for Com- putational Linguistics (Volume 2: Short Papers), pages 764\u2013770, Melbourne, Australia. Association for Computational Linguistics. Ilya Sutskever, James Martens, George Dahl, and Ge- offrey Hinton. 2013. On the importance of initial- ization and momentum in deep learning. In Pro- ceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 1139\u20131147, At- lanta, Georgia, USA. PMLR. Hiroya Takamura, Takashi Inui, and Manabu Okumura. 2005. Extracting semantic orientations of words us-",
  "ing spin model. In Proceedings of the 43rd An- nual Meeting on Association for Computational Lin- guistics, ACL \u201905, pages 133\u2013140, Stroudsburg, PA, USA. Association for Computational Linguistics. Peter D. Turney. 2002. Thumbs up or thumbs down?: Semantic orientation applied to unsupervised classi- \ufb01cation of reviews. In Proceedings of the 40th An- nual Meeting on Association for Computational Lin- guistics, ACL \u201902, pages 417\u2013424, Stroudsburg, PA, USA. Association for Computational Linguistics. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need.",
  "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar- nett, editors, Advances in Neural Information Pro- cessing Systems 30, pages 5998\u20136008. Curran As- sociates, Inc. Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase- level sentiment analysis. In Proceedings of the Con- ference on Human Language Technology and Em- pirical Methods in Natural Language Processing, HLT \u201905, pages 347\u2013354, Stroudsburg, PA, USA. Association for Computational Linguistics. Xiaodan Zhu, Hongyu Guo, Saif Mohammad, and Svetlana Kiritchenko. 2014.",
  "Association for Computational Linguistics. Xiaodan Zhu, Hongyu Guo, Saif Mohammad, and Svetlana Kiritchenko. 2014. An empirical study on the effect of negation words on sentiment. In Pro- ceedings of the 52nd Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers), pages 304\u2013313, Baltimore, Maryland. As- sociation for Computational Linguistics.",
  "A Appendices A.1 Seed Lexicon Positive Words \u559c\u3076(rejoice), \u5b09\u3057\u3044(be glad), \u697d\u3057\u3044(be pleasant), \u5e78\u305b(be happy), \u611f \u52d5(be impressed), \u8208\u596e(be excited), \u61d0\u304b\u3057\u3044 (feel nostalgic), \u597d\u304d(like), \u5c0a\u656c(respect), \u5b89\u5fc3 (be relieved), \u611f\u5fc3(admire), \u843d\u3061\u7740\u304f(be calm), \u6e80\u8db3(be satis\ufb01ed), \u7652\u3055\u308c\u308b(be healed), and \u30b9\u30c3\u30ad\u30ea(be refreshed).",
  "Negative Words \u6012\u308b(get angry), \u60b2\u3057\u3044(be sad), \u5bc2\u3057\u3044(be lonely), \u6016\u3044(be scared), \u4e0d\u5b89 (feel anxious), \u6065\u305a\u304b\u3057\u3044(be embarrassed), \u5acc (hate), \u843d\u3061\u8fbc\u3080(feel down), \u9000\u5c48(be bored), \u7d76\u671b(feel hopeless), \u8f9b\u3044(have a hard time), \u56f0 \u308b(have trouble), \u6182\u9b31(be depressed), \u5fc3\u914d(be worried), and \u60c5\u3051\u306a\u3044(be sorry). A.2 Settings of Encoder BiGRU The dimension of the embedding layer was 256. The embedding layer was initialized with the word embeddings pretrained using the Web corpus. The input sentences were segmented into words by the morphological analyzer Ju- man++.4 The vocabulary size was 100,000. The number of hidden layers was 2. The dimension of hidden units was 256. The optimizer was Momen- tum SGD (Sutskever et al., 2013). The mini-batch size was 1024.",
  "The number of hidden layers was 2. The dimension of hidden units was 256. The optimizer was Momen- tum SGD (Sutskever et al., 2013). The mini-batch size was 1024. We ran 100 epochs and selected the snapshot that achieved the highest score for the dev set. BERT We used a Japanese BERT model5 pre- trained with Japanese Wikipedia. The input sen- tences were segmented into words by Juman++, and words were broken into subwords by apply- ing BPE (Sennrich et al., 2016). The vocabulary size was 32,000. The maximum length of an input sequence was 128. The number of hidden layers was 12. The dimension of hidden units was 768. The number of self-attention heads was 12. The optimizer was Adam (Kingma and Ba, 2014). The mini-batch size was 32. We ran 1 epoch.",
  "The number of hidden layers was 12. The dimension of hidden units was 768. The number of self-attention heads was 12. The optimizer was Adam (Kingma and Ba, 2014). The mini-batch size was 32. We ran 1 epoch. 4http://nlp.ist.i.kyoto-u.ac.jp/EN/ index.php?JUMAN++ 5http://nlp.ist.i.kyoto-u.ac.jp/ index.php?BERT%E6%97%A5%E6%9C%AC%E8%AA% 9EPretrained%E3%83%A2%E3%83%87%E3%83%AB (in Japanese)"
]