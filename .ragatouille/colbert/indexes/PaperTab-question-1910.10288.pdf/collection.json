[
  "LOCATION-RELATIVE ATTENTION MECHANISMS FOR ROBUST LONG-FORM SPEECH SYNTHESIS Eric Battenberg RJ Skerry-Ryan Soroosh Mariooryad Daisy Stanton David Kao Matt Shannon Tom Bagby Google Research ABSTRACT Despite the ability to produce human-level speech for in-domain text, attention-based end-to-end text-to-speech (TTS) systems suf- fer from text alignment failures that increase in frequency for out- of-domain text. We show that these failures can be addressed us- ing simple location-relative attention mechanisms that do away with content-based query/key comparisons. We compare two families of attention mechanisms: location-relative GMM-based mechanisms and additive energy-based mechanisms. We suggest simple modi- \ufb01cations to GMM-based attention that allow it to align quickly and consistently during training, and introduce a new location-relative attention mechanism to the additive energy-based family, called Dy- namic Convolution Attention (DCA).",
  "We suggest simple modi- \ufb01cations to GMM-based attention that allow it to align quickly and consistently during training, and introduce a new location-relative attention mechanism to the additive energy-based family, called Dy- namic Convolution Attention (DCA). We compare the various mech- anisms in terms of alignment speed and consistency during training, naturalness, and ability to generalize to long utterances, and con- clude that GMM attention and DCA can generalize to very long utterances, while preserving naturalness for shorter, in-domain ut- terances. Index Terms\u2014 Speech synthesis, attention, sequence-to- sequence models 1. INTRODUCTION Sequence-to-sequence models that use an attention mechanism to align the input and output sequences [1, 2] are currently the predom- inant paradigm in end-to-end TTS. Approaches based on the seminal Tacotron system [3] have demonstrated naturalness that rivals that of human speech for certain domains [4].",
  "Approaches based on the seminal Tacotron system [3] have demonstrated naturalness that rivals that of human speech for certain domains [4]. Despite these successes, there are sometimes complaints of a lack of robustness in the align- ment procedure that leads to missing or repeating words, incomplete synthesis, or an inability to generalize to longer utterances [5, 6, 7]. The original Tacotron system [3] used the content-based atten- tion mechanism introduced in [2] to align the target text with the output spectrogram. This mechanism is purely content-based and does not exploit the monotonicity and locality properties of TTS alignment, making it one of the least stable choices. The Tacotron 2 system [4] used the improved hybrid location-sensitive mechanism from [8] that combines content-based and location-based features, allowing generalization to utterances longer than those seen during training. The hybrid mechanism still has occasional alignment issues which led a number of authors to develop attention mechanisms that directly exploit monotonicity [9, 5, 6]. These monotonic alignment mechanisms have demonstrated properties like increased alignment speed during training, improved stability, enhanced naturalness, and a virtual elimination of synthesis errors.",
  "These monotonic alignment mechanisms have demonstrated properties like increased alignment speed during training, improved stability, enhanced naturalness, and a virtual elimination of synthesis errors. Downsides of these meth- ods include decreased ef\ufb01ciency due to a reliance on recursion to marginalize over possible alignments, the necessity of training hacks to ensure learning doesn\u2019t stall or become unstable, and decreased quality when operating in a more ef\ufb01cient hard alignment mode during inference. Separately, some authors [10] have moved back toward the purely location-based GMM attention introduced by Graves in [1], and some have proposed stabilizing GMM attention by using soft- plus nonlinearities in place of the exponential function [11, 12]. However, there has been no systematic comparison of these design choices. In this paper, we compare the content-based and location- sensitive mechanisms used in Tacotron 1 and 2 with a variety of simple location-relative mechanisms in terms of alignment speed and consistency, naturalness of the synthesized speech, and ability to generalize to long utterances.",
  "In this paper, we compare the content-based and location- sensitive mechanisms used in Tacotron 1 and 2 with a variety of simple location-relative mechanisms in terms of alignment speed and consistency, naturalness of the synthesized speech, and ability to generalize to long utterances. We show that GMM-based mecha- nisms are able to generalize to very long (potentially in\ufb01nite-length) utterances, and we introduce simple modi\ufb01cations that result in improved speed and consistency of alignment during training. We also introduce a new location-relative mechanism called Dynamic Convolution Attention that modi\ufb01es the hybrid location-sensitive mechanism from Tacotron 2 to be purely location-based, allowing it to generalize to very long utterances as well. 2. TWO FAMILIES OF ATTENTION MECHANISMS 2.1. Basic Setup The system that we use in this paper is based on the original Tacotron system [3] with architectural modi\ufb01cations from the baseline model detailed in the appendix of [12].",
  "2. TWO FAMILIES OF ATTENTION MECHANISMS 2.1. Basic Setup The system that we use in this paper is based on the original Tacotron system [3] with architectural modi\ufb01cations from the baseline model detailed in the appendix of [12]. We use the CBHG encoder from [3] to produce a sequence of encoder outputs, {hj}L j=1, from a length- L input sequence of target phonemes, {xj}L j=1. Then an attention RNN, (2), produces a sequence of states, {si}T i=1, that the atten- tion mechanism uses to compute \u03b1i, the alignment at decoder step i. Additional arguments to the attention function in (3) depend on the speci\ufb01c attention mechanism (e.g., whether it is content-based, location-based, or both). The context vector, ci, that is fed to the decoder RNN is computed using the alignment, \u03b1i, to produce a weighted average of encoder states.",
  "The context vector, ci, that is fed to the decoder RNN is computed using the alignment, \u03b1i, to produce a weighted average of encoder states. The decoder is fed both the con- text vector and the current attention RNN state, and an output func- tion produces the decoder output, yi, from the decoder RNN state, di. {hj}L j=1 = Encoder({xj}L j=1) (1) si = RNNAtt(si\u22121, ci\u22121, yi\u22121) (2) \u03b1i = Attention(si, . . . ) ci = X j \u03b1i,jhj (3) di = RNNDec(di\u22121, ci, si) yi = fo(di) (4) arXiv:1910.10288v2  [cs.CL]  22 Apr 2020",
  "2.2. GMM-Based Mechanisms An early sequence-to-sequence attention mechanism was proposed by Graves in [1]. This approach is a purely location-based mecha- nism that uses an unnormalized mixture of K Gaussians to produce the attention weights, \u03b1i, for each encoder state. The general form of this type of attention is shown in (5), where wi, Zi, \u2206i, and \u03c3i are computed from the attention RNN state. The mean of each Gaussian component is computed using the recurrence relation in (6), which makes the mechanism location-relative and potentially monotonic if \u2206i is constrained to be positive. \u03b1i,j = K X k=1 wi,k Zi,k exp \u0012 \u2212(j \u2212\u00b5i,k)2 2(\u03c3i,k)2 \u0013 (5) \u00b5i = \u00b5i\u22121 + \u2206i (6) In order to compute the mixture parameters, intermediate parame- ters ( \u02c6wi, \u02c6\u2206i, \u02c6\u03c3i) are \ufb01rst computed using the MLP in (7) and then converted to the \ufb01nal parameters using the expressions in Table 1.",
  "( \u02c6wi, \u02c6\u2206i, \u02c6\u03c3i) = V tanh(Wsi + b) (7) The version 0 (V0) row in Table 1 corresponds to the original mecha- nism proposed in [1]. V1 adds normalization of the mixture weights and components and uses the exponential function to compute the mean offset and variance. V2 uses the softplus function to compute the mean offset and standard deviation. Another modi\ufb01cation we test is the addition of initial biases to the intermediate parameters \u02c6\u2206i and \u02c6\u03c3i in order to encourage the \ufb01nal parameters \u2206i and \u03c3i to take on useful values at initialization. In our experiments, we test versions of V1 and V2 GMM attention that use biases that target a value of \u2206i = 1 for the initial forward movement and \u03c3i = 10 for the initial standard deviation (taking into account the different nonlinearities used to compute the parameters). Table 1. Conversion of intermediate parameters computed in (7) to \ufb01nal mixture parameters for the three tested GMM-based attention mechanisms.",
  "Table 1. Conversion of intermediate parameters computed in (7) to \ufb01nal mixture parameters for the three tested GMM-based attention mechanisms. Smax(\u00b7) is the softmax function, while S+(\u00b7) is the soft- plus function. Zi wi \u2206i \u03c3i V0 [1] 1 exp( \u02c6wi) exp( \u02c6\u2206i) p exp(\u2212\u02c6\u03c3i)/2 V1 p 2\u03c0\u03c32 i Smax( \u02c6wi) exp( \u02c6\u2206i) p exp(\u02c6\u03c3i) V2 p 2\u03c0\u03c32 i Smax( \u02c6wi) S+( \u02c6\u2206i) S+(\u02c6\u03c3i) 2.3. Additive Energy-Based Mechanisms A separate family of attention mechanisms use an MLP to compute attention energies, ei, that are converted to attention weights, \u03b1i, using the softmax function. This family includes the content-based mechanism introduced in [2] and the hybrid location-sensitive mech- anism from [8]. A generalized formulation of this family is shown in (8).",
  "This family includes the content-based mechanism introduced in [2] and the hybrid location-sensitive mech- anism from [8]. A generalized formulation of this family is shown in (8). ei,j = v\u22batanh(Wsi + V hj + Ufi,j + Tgi,j + b) + pi,j (8) \u03b1i = Smax(ei) (9) fi = F \u2217\u03b1i\u22121 (10) gi = G(si) \u2217\u03b1i\u22121, G(si) = VG tanh(WGsi + bG) (11) pi = log(P \u2217\u03b1i\u22121) (12) Here we see the content-based terms, Wsi and V hj, that repre- sent query/key comparisons and the location-sensitive term, Ufi,j, that uses convolutional features computed from the previous atten- tion weights as in (10) [8]. Also present are two new terms, Tgi,j and pi,j, that are unique to our proposed Dynamic Convolution At- tention.",
  "Also present are two new terms, Tgi,j and pi,j, that are unique to our proposed Dynamic Convolution At- tention. The Tgi,j term is very similar to Ufi,j except that it uses dynamic \ufb01lters that are computed from the current attention RNN state as in (11). The pi,j term is the output of a \ufb01xed prior \ufb01lter that biases the mechanism to favor certain types of alignment. Ta- ble 2 shows which of the terms are present in the three energy-based mechanisms we compare in this paper. Table 2. The terms from (8) that are present in each of the three energy-based attention mechanisms we test. Wsi V hj Ufi,j Tgi,j pi,j Content-Based [2] \u0013 \u0013 - - - Location-Sensitive [8] \u0013 \u0013 \u0013 - - Dynamic Convolution - - \u0013 \u0013 \u0013 2.4. Dynamic Convolution Attention In designing Dynamic Convolution Attention (DCA), we were moti- vated by location-relative mechanisms like GMM attention, but de- sired fully normalized attention weights.",
  "Dynamic Convolution Attention In designing Dynamic Convolution Attention (DCA), we were moti- vated by location-relative mechanisms like GMM attention, but de- sired fully normalized attention weights. Despite the fact that GMM attention V1 and V2 use normalized mixture weights and compo- nents, the attention weights still end up unnormalized because they are sampled from a continuous probability density function. This can lead to occasional spikes or dropouts in the alignment, and attempt- ing to directly normalize GMM attention weights results in unstable training. Attention normalization isn\u2019t a signi\ufb01cant problem in \ufb01ne- grained output-to-text alignment, but becomes more of an issue for coarser-grained alignment tasks where the attention window needs to gradually move to the next index (for example in variable-length prosody transfer applications [13]). Because DCA is in the energy- based attention family, it is normalized by default and should work well for a variety of monotonic alignment tasks. Another issue with GMM attention is that because it uses a mix- ture of distributions with in\ufb01nite support, it isn\u2019t necessarily mono- tonic.",
  "Another issue with GMM attention is that because it uses a mix- ture of distributions with in\ufb01nite support, it isn\u2019t necessarily mono- tonic. At any time, the mechanism could choose to emphasize a component whose mean is at an earlier point in the sequence, or it could expand the variance of a component to look backward in time, potentially hurting alignment stability. To address monotonicity issues, we make modi\ufb01cations to the hybrid location-sensitive mechanism. First we remove the content- based terms, Wsi and Whi, which prevents the alignment from moving backward due to a query/key match at a past timestep. Do- ing this prevents the mechanism from adjusting its alignment trajec- tory as it is only left with a set of static \ufb01lters, Ufi,j, that learn to bias the alignment to move forward by a certain \ufb01xed amount. To remedy this, we add a set of learned dynamic \ufb01lters, Tgi,j, that are computed from the attention RNN state as in (11). These \ufb01lters serve to dynamically adjust the alignment relative to the alignment at the previous step.",
  "To remedy this, we add a set of learned dynamic \ufb01lters, Tgi,j, that are computed from the attention RNN state as in (11). These \ufb01lters serve to dynamically adjust the alignment relative to the alignment at the previous step. In order to prevent the dynamic \ufb01lters from moving things back- ward, we use a single \ufb01xed prior \ufb01lter to bias the alignment toward short forward steps. Unlike the static and dynamic \ufb01lters, the prior \ufb01lter is a causal \ufb01lter that only allows forward progression of the alignment. In order to enforce the monotonicity constraint, the out- put of the \ufb01lter is converted to the logit domain via the log function",
  "before being added to the energy function in (8) (we also \ufb02oor the prior logits at \u2212106 to prevent under\ufb02ow). We set the taps of the prior \ufb01lter using values from the beta- binomial distribution, which is a two-parameter discrete distribution with \ufb01nite support. p(k) =   n k ! B(k + \u03b1, n \u2212k + \u03b2) B(\u03b1, \u03b2) , k \u2208{0, . . . , n} (13) where B(\u00b7) is the beta function. For our experiments we use the pa- rameters \u03b1 = 0.1 and \u03b2 = 0.9 to set the taps on a length-11 prior \ufb01lter (n = 10). Repeated application of the prior \ufb01lter encourages an average forward movement of 1 encoder step per decoder step (E[k] = \u03b1n/(\u03b1 + \u03b2)) with the uncertainty in the prior alignment increasing after each step.",
  "Repeated application of the prior \ufb01lter encourages an average forward movement of 1 encoder step per decoder step (E[k] = \u03b1n/(\u03b1 + \u03b2)) with the uncertainty in the prior alignment increasing after each step. The prior parameters could be tailored to re\ufb02ect the phonemic rate of each dataset in order to optimize align- ment speed during training, but for simplicity we use the same val- ues for all experiments. Figure 1 shows the prior \ufb01lter along with the alignment weights every 20 decoder steps when ignoring the contri- bution from other terms in (8). 0.0 0.5 Prior \ufb01lter 0 1 0 steps 0.000 0.025 20 steps 0.000 0.025 40 steps 0.00 0.02 60 steps 0 20 40 60 80 100 120 Encoder Steps 0.00 0.02 80 steps Initial Alignment Via Repeated Application of Prior Filter Fig. 1. Initial alignment encouraged by the prior \ufb01lter (ignoring the contribution of other term in (8)).",
  "1. Initial alignment encouraged by the prior \ufb01lter (ignoring the contribution of other term in (8)). The attention weights are shown every 20 decoders steps with the prior \ufb01lter itself shown at the top. 3. EXPERIMENTS 3.1. Experiment Setup In our experiments we compare the GMM and additive energy-based families of attention mechanisms enumerated in Tables 1 and 2. We use the Tacotron architecture described in Section 2.1 and only vary the attention function used to compute the attention weights, \u03b1i. The decoder produces two 128-bin, 12.5ms-hop mel spectrogram frames per step. We train each model using the Adam optimizer for 300,000 steps with a gradient clipping threshold of 5 and a batch size of 256, spread across 32 Google Cloud TPU cores. We use an initial learning rate of 10\u22123 that is reduced to 5 \u00d7 10\u22124, 3 \u00d7 10\u22124, 10\u22124, and 5\u00d710\u22125 at 50k, 100k, 150k, and 200k steps, respectively.",
  "To convert the mel spectrograms produced by the models into audio samples, we use a separately-trained WaveRNN [14] for each speaker. For all attention mechanisms, we use a size of 128 for all tanh hidden layers. For the GMM mechanisms, we use K = 5 mixture components. For location-sensitive attention (LSA), we use 32 static \ufb01lters, each of length 31. For DCA, we use 8 static \ufb01lters and 8 dynamic \ufb01lters (all of length 21), and a length-11 causal prior \ufb01lter as described in Section 2.4. We run experiments using two different single-speaker datasets. The \ufb01rst (which we refer to as the Lessac dataset) comprises audio- book recordings from Catherine Byers, the speaker from the 2013 Blizzard Challenge. For this dataset, we train on a 49,852-utterance (37-hour) subset, consisting of utterances up to 5 seconds long, and evaluate on a separate 935-utterance subset.",
  "For this dataset, we train on a 49,852-utterance (37-hour) subset, consisting of utterances up to 5 seconds long, and evaluate on a separate 935-utterance subset. The second is the LJ Speech dataset [15], a public dataset consisting of audiobook record- ings that are segmented into utterances of up to 10 seconds. We train on a 12,764-utterance subset (23 hours) and evaluate on a separate 130-utterance subset. 3.2. Alignment Speed and Consistency To test the alignment speed and consistency of the various mecha- nisms, we run 10 identical trials of 10,000 training steps and plot the MCD-DTW between a ground truth holdout set and the output of the model during training. The MCD-DTW is an objective similar- ity metric that uses dynamic time warping (DTW) to \ufb01nd the min- imum mel cepstral distortion (MCD) [16] between two sequences. The faster a model is able to align with the text, the faster it will start producing reasonable spectrograms that produce a lower MCD- DTW.",
  "The faster a model is able to align with the text, the faster it will start producing reasonable spectrograms that produce a lower MCD- DTW. 8 10 MCD-DTW Content-Based Location-Sensitive DCA GMMv2 + Bias 0 5k 10k Training Steps 8 10 MCD-DTW GMMv0 0 5k 10k Training Steps GMMv1 0 5k 10k Training Steps GMMv1 + Bias 0 5k 10k Training Steps GMMv2 Alignment Trials: Lessac < 5sec 7.5 10.0 12.5 MCD-DTW Content-Based Location-Sensitive DCA GMMv2 + Bias 0 5k 10k Training Steps 7.5 10.0 12.5 MCD-DTW GMMv0 0 5k 10k Training Steps GMMv1 0 5k 10k Training Steps GMMv1 + Bias 0 5k 10k Training Steps GMMv2 Alignment Trials: LJ < 10sec Fig. 2.",
  "2. Alignment trials for 8 different mechanisms (10 runs each) trained on the Lessac (top) and LJ (bottom) datasets. The validation set MCD-DTW drops down after alignment has occurred. Figure 2 shows these trials for 8 different mechanisms for both the Lessac and LJ datasets. Content-based (CBA), location-sensitive (LSA), and DCA are the three energy-based mechanisms from Ta- ble 2, and the 3 GMM varieties are shown in Table 1. We also test the V1 and V2 GMM mechanisms with an initial parameter bias as described in Section 2.2 (abbreviated as GMMv1b and GMMv2b). Looking at the plots for the Lessac dataset (top of Figure 2), we see that the mechanisms on the top row (the energy-based family and GMMv2b) all align consistently, with DCA and GMMv2b aligning the fastest. The GMM mechanisms on the bottom row don\u2019t fare",
  "as well, and while they typically align more often than not, there are a signi\ufb01cant number of failures or cases of delayed alignment. It\u2019s interesting to note that adding a bias to the GMMv1 mechanism actually hurts its consistency while adding a bias to GMMv2 helps it. Looking at the plots for the LJ dataset at the bottom of Figure 2, we \ufb01rst see that the dataset is more dif\ufb01cult in terms of alignment. This is likely due to the higher maximum and average length of the utterances in the training data (most utterances in the LJ dataset are longer than 5 seconds) but could also be caused by an increased pres- ence of intra-utterance pauses and overall lower audio quality. Here, the top row doesn\u2019t fare as well: CBA has trouble aligning within the \ufb01rst 10k steps, while DCA and GMMv2b both fail to align once. LSA succeeds on all 10 trials but tends to align more slowly than DCA and GMMv2b when they succeed. With these consistency re- sults in mind, we will only be testing the top row of mechanisms in subsequent evaluations.",
  "LSA succeeds on all 10 trials but tends to align more slowly than DCA and GMMv2b when they succeed. With these consistency re- sults in mind, we will only be testing the top row of mechanisms in subsequent evaluations. 3.3. In-Domain Naturalness We evaluate CBA, LSA, DCA, and GMMv2b using mean opinion score (MOS) naturalness judgments produced by a crowd-sourced pool of raters. Scores range from 1 to 5, with 5 representing \u201ccom- pletely natural speech\u201d. The Lessac and LJ models are evaluated on their respective test sets (hence in-domain), and the results are shown in Table 3. We see that for these utterances, the LSA, DCA, and GMMV2b mechanisms all produce equivalent scores around 4.3, while the content-based mechanism is a bit lower due to occasional catastrophic attention failures. Table 3. MOS naturalness results along with 95% con\ufb01dence inter- vals for the Lessac and LJ datasets.",
  "Table 3. MOS naturalness results along with 95% con\ufb01dence inter- vals for the Lessac and LJ datasets. Lessac LJ Content-Based 4.07 \u00b1 0.08 4.19 \u00b1 0.06 Location-Sensitive 4.31 \u00b1 0.06 4.34 \u00b1 0.06 GMMv2b 4.32 \u00b1 0.06 4.29 \u00b1 0.06 DCA 4.31 \u00b1 0.06 4.33 \u00b1 0.06 Ground Truth 4.64 \u00b1 0.04 4.55 \u00b1 0.04 3.4. Generalization to Long Utterances Now we evaluate our models on long utterances taken from two chapters of the Harry Potter novels. We use 1034 utterances that vary between 58 and 1648 characters (10 and 299 words). Google Cloud Speech-To-Text1 is used to produce transcripts of the resulting audio output, and we compute the character errors rate (CER) between the produced transcripts and the target transcripts.",
  "We use 1034 utterances that vary between 58 and 1648 characters (10 and 299 words). Google Cloud Speech-To-Text1 is used to produce transcripts of the resulting audio output, and we compute the character errors rate (CER) between the produced transcripts and the target transcripts. Figure 3 shows the CER results as a function of utterance length for the Lessac mod- els (trained on up to 5 second utterances) and LJ models (trained on up to 10 second utterances). The plots show that CBA fares the worst, with the CER shooting up when the test length exceeds the max training length. LSA shoots up soon after at around 3x the max training length, while the two location-relative mechanisms, DCA and GMMv2b, are both able to generalize to the whole range of ut- terance lengths tested.",
  "LSA shoots up soon after at around 3x the max training length, while the two location-relative mechanisms, DCA and GMMv2b, are both able to generalize to the whole range of ut- terance lengths tested. 1https://cloud.google.com/speech-to-text 200 12s 400 24s 600 36s 800 48s 1000 60s 1200 72s 1400 84s Utterance Length [chars, \u223csecs] 0 20 40 60 80 100 Character Error Rate [%] Length Robustness: Lessac < 5sec Content-Based Location-Sensitive GMMv2b DCA 200 12s 400 24s 600 36s 800 48s 1000 60s 1200 72s 1400 84s Utterance Length [chars, \u223csecs] 0 20 40 60 80 Character Error Rate [%] Length Robustness: LJ < 10sec Content-Based Location-Sensitive GMMv2b DCA Fig. 3.",
  "3. Utterance length robustness for models trained on the Lessac (top) and LJ (bottom) datasets. 4. DISCUSSION We have shown that Dynamic Convolution Attention (DCA) and V2 GMM attention with initial bias (GMMv2b) are able to generalize to utterances much longer than those seen during training, while pre- serving naturalness on shorter utterances. This opens the door for synthesis of entire paragraphs or long sentences (e.g., for book or news reading applications), which can improve naturalness and con- tinuity compared to synthesizing each sentence or clause separately and then stitching them together. These two location-relative mechanisms are simple to imple- ment and do not rely on dynamic programming to marginalize over alignments. They also tend to align very quickly during training, which makes the occasional alignment failure easy to detect so train- ing can be restarted. In our alignment trials, despite being slower to align on average, LSA seemed to have an edge in terms of align- ment consistency; however, we have noticed that slower alignment can sometimes lead to worse quality models, probably because the other model components are being optimized in an unaligned state for longer.",
  "In our alignment trials, despite being slower to align on average, LSA seemed to have an edge in terms of align- ment consistency; however, we have noticed that slower alignment can sometimes lead to worse quality models, probably because the other model components are being optimized in an unaligned state for longer. Compared to GMMv2b, DCA can more easily bound its recep- tive \ufb01eld (because its prior \ufb01lter numerically disallows backward or excessive forward movement), which makes it easier to incorporate hard windowing optimizations in production. Another advantage of DCA over GMM attention is that its attention weights are normal- ized, which helps to stabilize the alignment, especially for coarse- grained alignment tasks. For monotonic alignment tasks like TTS and speech recogni- tion, location-relative attention mechanisms have many advantages and warrant increased consideration and further study. Supplemental materials, including audio examples, are available on the web2. 2https://google.github.io/tacotron/publications/ location_relative_attention",
  "5. REFERENCES [1] Alex Graves, \u201cGenerating Sequences With Recurrent Neural Networks,\u201d arXiv.org, Aug. 2013. [2] Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio, \u201cNeural Machine Translation by Jointly Learning to Align and Translate,\u201d in International Conference on Learning Represen- tations, 2015. [3] Yuxuan Wang, R.J. Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J. Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc Le, Yannis Agiomyrgian- nakis, Rob Clark, and Rif A. Saurous, \u201cTacotron: Towards End-to-End Speech Synthesis,\u201d in Proc. Interspeech 2017, 2017, pp. 4006\u20134010.",
  "Interspeech 2017, 2017, pp. 4006\u20134010. [4] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, R. Skerrv-Ryan, R. A. Saurous, Y. Agiomvrgiannakis, and Y. Wu, \u201cNatural TTS Synthesis by Conditioning Wavenet on Mel Spectrogram Predictions,\u201d in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), April 2018, pp. 4779\u20134783. [5] J. Zhang, Z. Ling, and L. Dai, \u201cForward attention in sequence- to-sequence acoustic modeling for speech synthesis,\u201d in 2018 IEEE International Conference on Acoustics, Speech and Sig- nal Processing (ICASSP), April 2018, pp. 4789\u20134793.",
  "4789\u20134793. [6] Mutian He, Yan Deng, and Lei He, \u201cRobust Sequence-to- Sequence Acoustic Modeling with Stepwise Monotonic Atten- tion for Neural TTS,\u201d in Proc. Interspeech 2019, 2019, pp. 1293\u20131297. [7] Peng Liu, Xixin Wu, Shiyin Kang, Guangzhi Li, Dan Su, and Dong Yu, \u201cMaximizing Mutual Information for Tacotron,\u201d arXiv.org, Aug. 2019. [8] Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, \u201cAttention-based Mod- els for Speech Recognition,\u201d in Proceedings of the 28th In- ternational Conference on Neural Information Processing Sys- tems - Volume 1, Cambridge, MA, USA, 2015, NIPS\u201915, pp. 577\u2013585, MIT Press.",
  "577\u2013585, MIT Press. [9] Colin Raffel, Minh-Thang Luong, Peter J. Liu, Ron J. Weiss, and Douglas Eck, \u201cOnline and Linear-time Attention by En- forcing Monotonic Alignments,\u201d in Proceedings of the 34th International Conference on Machine Learning - Volume 70. 2017, ICML\u201917, pp. 2837\u20132846, JMLR.org. [10] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron Weiss, Rob Clark, and Rif A. Saurous, \u201cTowards End-to-End Prosody Transfer for Expres- sive Speech Synthesis with Tacotron,\u201d in Proceedings of the 35th International Conference on Machine Learning, Jennifer Dy and Andreas Krause, Eds., Stockholmsmssan, Stockholm Sweden, 10\u201315 Jul 2018, vol. 80 of Proceedings of Machine Learning Research, pp. 4693\u20134702, PMLR.",
  "80 of Proceedings of Machine Learning Research, pp. 4693\u20134702, PMLR. [11] K. Kastner, J. F. Santos, Y. Bengio, and A. Courville, \u201cRepre- sentation Mixing for TTS Synthesis,\u201d in ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Sig- nal Processing (ICASSP), May 2019, pp. 5906\u20135910. [12] E Battenberg, Soroosh Mariooryad, Daisy Stanton, R J Skerry- Ryan, Matt Shannon, David Kao, and Tom Bagby, \u201cEffective Use of Variational Embedding Capacity in Expressive End-to- End Speech Synthesis,\u201d arXiv.org, June 2019. [13] Y. Lee and T. Kim, \u201cRobust and Fine-grained Prosody Con- trol of End-to-end Speech Synthesis,\u201d in ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Sig- nal Processing (ICASSP), May 2019, pp. 5911\u20135915.",
  "5911\u20135915. [14] Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stim- berg, Aaron van den Oord, Sander Dieleman, and Koray Kavukcuoglu, \u201cEf\ufb01cient Neural Audio Synthesis,\u201d in Proceed- ings of the 35th International Conference on Machine Learn- ing, Jennifer Dy and Andreas Krause, Eds., Stockholmsmssan, Stockholm Sweden, 10\u201315 Jul 2018, vol. 80 of Proceedings of Machine Learning Research, pp. 2410\u20132419, PMLR. [15] Keith Ito, \u201cThe LJ Speech Dataset,\u201d https://keithito. com/LJ-Speech-Dataset/, 2017. [16] R Kubichek, \u201cMel-cepstral distance measure for objective speech quality assessment,\u201d in Communications, Computers and Signal Processing, 1993., IEEE Paci\ufb01c Rim Conference on. IEEE, 1993, vol. 1, pp. 125\u2013128."
]