{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation Liliang Ren, Jianmo Ni and Julian McAuley University of California, San Diego La Jolla, CA92093 {lren,jin018,jmcauley}@ucsd.edu Abstract Existing approaches to dialogue state track- ing rely on pre-de\ufb01ned ontologies consisting of a set of all possible slot types and values. Though such approaches exhibit promising performance on single-domain benchmarks, they suffer from computational complexity that increases proportionally to the number of pre-de\ufb01ned slots that need tracking. This is- sue becomes more severe when it comes to multi-domain dialogues which include larger numbers of slots. In this paper, we investi- gate how to approach DST using a generation framework without the pre-de\ufb01ned ontology list. Given each turn of user utterance and system response, we directly generate a se- quence of belief states by applying a hierarchi- cal encoder-decoder structure. In this way, the computational complexity of our model will be a constant regardless of the number of pre- de\ufb01ned slots.",
      "Given each turn of user utterance and system response, we directly generate a se- quence of belief states by applying a hierarchi- cal encoder-decoder structure. In this way, the computational complexity of our model will be a constant regardless of the number of pre- de\ufb01ned slots. Experiments on both the multi- domain and the single domain dialogue state tracking dataset show that our model not only scales easily with the increasing number of pre-de\ufb01ned domains and slots but also reaches the state-of-the-art performance. 1 Introduction A Dialogue State Tracker (DST) is a core compo- nent of a modular task-oriented dialogue system (Young et al., 2013). For each dialogue turn, a DST module takes a user utterance and the dia- logue history as input, and outputs a belief esti- mate of the dialogue state. Then a machine action is decided based on the dialogue state according to a dialogue policy module, after which a machine response is generated. Traditionally, a dialogue state consists of a set of requests and joint goals, both of which are rep- resented by a set of slot-value pairs (e.g.",
      "Then a machine action is decided based on the dialogue state according to a dialogue policy module, after which a machine response is generated. Traditionally, a dialogue state consists of a set of requests and joint goals, both of which are rep- resented by a set of slot-value pairs (e.g. (request, phone), (area, north), (food, Japanese)) (Hender- son et al., 2014). In a recently proposed multi- DST Models ITC NBT-CNN (Mrksic et al., 2017) O(mn) MD-DST (Rastogi et al., 2017) O(n) GLAD (Zhong et al., 2018) O(mn) StateNet PSI (Ren et al., 2018) O(n) TRADE (Wu et al., 2019) O(n) HyST (Goel et al., 2019) O(n) DSTRead (Gao et al., 2019) O(n) Table 1: The Inference Time Complexity (ITC) of pre- vious DST models.",
      "The ITC is calculated based on how many times inference must be performed to complete a prediction of the belief state in a dialogue turn, where m is the number of values in a pre-de\ufb01ned ontology list and n is the number of slots. domain dialogue state tracking dataset, MultiWoZ (Budzianowski et al., 2018), a representation of dialogue state consists of a hierarchical structure of domain, slot, and value is proposed. This is a more practical scenario since dialogues often in- clude multiple domains simultaneously. Many recently proposed DSTs (Zhong et al., 2018; Ramadan et al., 2018) are based on pre- de\ufb01ned ontology lists that specify all possible slot values in advance. To generate a distribution over the candidate set, previous works often take each of the slot-value pairs as input for scoring. How- ever, in real-world scenarios, it is often not prac- tical to enumerate all possible slot value pairs and perform scoring from a large dynamically chang- ing knowledge base (Xu and Hu, 2018).",
      "How- ever, in real-world scenarios, it is often not prac- tical to enumerate all possible slot value pairs and perform scoring from a large dynamically chang- ing knowledge base (Xu and Hu, 2018). To tackle this problem, a popular direction is to build a \ufb01xed-length candidate set that is dynamically up- dated throughout the dialogue development. Ta- ble 1 brie\ufb02y summaries the inference time com- plexity of multiple state-of-the-art DST models following this direction. Since the inference com- plexity of all of previous model is at least pro- arXiv:1909.00754v2  [cs.AI]  18 Oct 2019",
      "portional to the number of the slots, these mod- els will struggle to scale to multi-domain datasets with much larger numbers of pre-de\ufb01ned slots. In this work, we formulate the dialogue state tracking task as a sequence generation problem, instead of formulating the task as a pair-wise pre- diction problem as in existing work. We pro- pose the COnditional MEmory Relation Network (COMER), a scalable and accurate dialogue state tracker that has a constant inference time complex- ity. 1 Speci\ufb01cally, our model consists of an encoder- decoder network with a hierarchically stacked de- coder to \ufb01rst generate the slot sequences in the be- lief state and then for each slot generate the cor- responding value sequences. The parameters are shared among all of our decoders for the scalabil- ity of the depth of the hierarchical structure of the belief states. COMER applies BERT contextual- ized word embeddings (Devlin et al., 2018) and BPE (Sennrich et al., 2016) for sequence encod- ing to ensure the uniqueness of the representations of the unseen words.",
      "COMER applies BERT contextual- ized word embeddings (Devlin et al., 2018) and BPE (Sennrich et al., 2016) for sequence encod- ing to ensure the uniqueness of the representations of the unseen words. The word embeddings for sequence generation are initialized with the static word embeddings generated from BERT . 2 Motivation Figure 1 shows a multi-domain dialogue in which the user wants the system to \ufb01rst help book a train and then reserve a hotel. For each turn, the DST will need to track the slot-value pairs (e.g. (ar- rive by, 20:45)) representing the user goals as well as the domain that the slot-value pairs belongs to (e.g. train, hotel). Instead of representing the be- lief state via a hierarchical structure, one can also combine the domain and slot together to form a combined slot-value pair (e.g. (train; arrive by, 20:45) where the combined slot is \u201ctrain; arrive by\u201d), which ignores the subordination relationship between the domain and the slots.",
      "(train; arrive by, 20:45) where the combined slot is \u201ctrain; arrive by\u201d), which ignores the subordination relationship between the domain and the slots. A typical fallacy in dialogue state tracking datasets is that they make an assumption that the slot in a belief state can only be mapped to a sin- gle value in a dialogue turn. We call this the sin- gle value assumption. Figure 2 shows an exam- ple of this fallacy from the WoZ2.0 dataset: Based on the belief state label (food, seafood), it will be impossible for the downstream module in the di- alogue system to generate sample responses that 1The code is released at https://github.com/ renll/ComerNet Figure 1: An example dialogue from the multi-domain dataset, MultiWOZ. At each turn, the DST needs to output the belief state, a nested tuple of (DOMAIN, (SLOT, VALUE)), immediately after the user utterance ends. The belief state is accumulated as the dialogue proceeds. Turns are separated by black lines. Figure 2: An example in the WoZ2.0 dataset that inval- idates the single value assumption.",
      "The belief state is accumulated as the dialogue proceeds. Turns are separated by black lines. Figure 2: An example in the WoZ2.0 dataset that inval- idates the single value assumption. It is impossible for the system to generate the sample response about the Chinese restaurant with the original belief state (food, seafood). A correction could be made as (food, seafood > chinese) which has multiple values and a logical op- erator \u201c>\u201d. return information about Chinese restaurants. A correct representation of the belief state could be (food, seafood > chinese). This would tell the system to \ufb01rst search the database for information about seafood and then Chinese restaurants. The logical operator \u201c>\u201d indicates which retrieved in- formation should have a higher priority to be re- turned to the user. Thus we are interested in build- ing DST modules capable of generating structured sequences, since this kind of sequence representa- tion of the value is critical for accurately capturing the belief states of a dialogue. 3 Hierarchical Sequence Generation for DST Given a dialogue D which consists of T turns of user utterances and system actions, our target is to predict the state at each turn. Different from pre-",
      "Figure 3: The general model architecture of the Hierarchical Sequence Generation Network. The Conditional Memory Relation (CMR) decoders (gray) share all of their parameters. vious methods which formulate multi-label state prediction as a collection of binary prediction problems, COMER adapts the task into a sequence generation problem via a Seq2Seq framework. As shown in Figure 3, COMER consists of three encoders and three hierarchically stacked de- coders. We propose a novel Conditional Memory Relation Decoder (CMRD) for sequence decod- ing. Each encoder includes an embedding layer and a BiLSTM. The encoders take in the user ut- terance, the previous system actions, and the pre- vious belief states at the current turn, and encodes them into the embedding space. The user encoder and the system encoder use the \ufb01xed BERT model as the embedding layer.",
      "The encoders take in the user ut- terance, the previous system actions, and the pre- vious belief states at the current turn, and encodes them into the embedding space. The user encoder and the system encoder use the \ufb01xed BERT model as the embedding layer. Since the slot value pairs are un-ordered set el- ements of a domain in the belief states, we \ufb01rst order the sequence of domain according to their frequencies as they appear in the training set (Yang et al., 2018), and then order the slot value pairs in the domain according to the slot\u2019s frequencies of as they appear in a domain. After the sorting of the state elements, We represent the belief states following the paradigm: (Domain1- Slot1, Value1; Slot2, Value2; ... Domain2- Slot1, Value1; ...) for a more concise representation compared with the nested tuple representation. All the CMRDs take the same representations from the system encoder, user encoder and the be- lief encoder as part of the input.",
      "All the CMRDs take the same representations from the system encoder, user encoder and the be- lief encoder as part of the input. In the proce- dure of hierarchical sequence generation, the \ufb01rst CMRD takes a zero vector for its condition input c, and generates a sequence of the domains, D, as well as the hidden representation of domains HD. For each d in D, the second CMRD then takes the corresponding hd as the condition input and gen- erates the slot sequence Sd, and representations, HS,d. Then for each s in S, the third CMRD gen- erates the value sequence Vd,s based on the corre- sponding hs,d. We update the belief state with the new (d, (sd, Vd,s)) pairs and perform the proce- dure iteratively until a dialogue is completed. All the CMR decoders share all of their parameters.",
      "We update the belief state with the new (d, (sd, Vd,s)) pairs and perform the proce- dure iteratively until a dialogue is completed. All the CMR decoders share all of their parameters. Since our model generates domains and slots in- stead of taking pre-de\ufb01ned slots as inputs, and the number of domains and slots generated each turn is only related to the complexity of the contents covered in a speci\ufb01c dialogue, the inference time complexity of COMER is O(1) with respect to the number of pre-de\ufb01ned slots and values. 3.1 Encoding Module Let X represent a user utterance or system transcript consisting of a sequence of words {w1, . . . , wT }. The encoder \ufb01rst passes the se- quence {[CLS], w1, . . . , wT , [SEP]} into a pre- trained BERT model and obtains its contextual embeddings EX. Speci\ufb01cally, we leverage the output of all layers of BERT and take the average to obtain the contextual embeddings.",
      "For each domain/slot appeared in the training set, if it has more than one word, such as \u2018price range\u2019, \u2018leave at\u2019, etc., we feed it into BERT and take the average of the word vectors to form the extra slot embedding Es. In this way, we map each domain/slot to a static embedding, which allows us to generate a domain/slot as a whole instead of a token at each time step of domain/slot sequence decoding. We also construct a static vocabulary embedding Ev by feeding each token in the BERT vocabulary into BERT. The \ufb01nal static word em- bedding E is the concatenation of the Ev and Es. After we obtain the contextual embeddings for the user utterance, system action, and the static embeddings for the previous belief state, we feed each of them into a Bidirectional LSTM (Hochre- iter and Schmidhuber, 1997).",
      "After we obtain the contextual embeddings for the user utterance, system action, and the static embeddings for the previous belief state, we feed each of them into a Bidirectional LSTM (Hochre- iter and Schmidhuber, 1997). hat = BiLSTM(eXat, hat\u22121) hut = BiLSTM(eXut, hut\u22121) hbt = BiLSTM(eXbt, hbt\u22121) ha0 = hu0 = hb0 = c0, (1) where c0 is the zero-initialized hidden state for the BiLSTM. The hidden size of the BiLSTM is dm/2. We concatenate the forward and the back- ward hidden representations of each token from the BiLSTM to obtain the token representation hkt \u2208Rdm, k \u2208{a, u, b} at each time step t. The hidden states of all time steps are concate- nated to obtain the \ufb01nal representation of Hk \u2208 RT\u00d7dm, k \u2208{a, u, B}. The parameters are shared between all of the BiLSTMs.",
      "The parameters are shared between all of the BiLSTMs. 3.2 Conditional Memory Relation Decoder Inspired by Residual Dense Networks (Zhang et al., 2018), End-to-End Memory Networks (Sukhbaatar et al., 2015) and Relation Networks (Santoro et al., 2017), we here propose the Condi- tional Memory Relation Decoder (CMRD). Given a token embedding, ex, CMRD outputs the next token, s, and the hidden representation, hs, with the hierarchical memory access of different en- coded information sources, HB, Ha, Hu, and the relation reasoning under a certain given condition c, s, hs = CMRD(ex, c, HB, Ha, Hu), the \ufb01nal output matrices S, Hs \u2208Rls\u00d7dm are con- catenations of all generated s and hs (respectively) along the sequence length dimension, where dm is the model size, and ls is the generated sequence length. The general structure of the CMR decoder is shown in Figure 4.",
      "The general structure of the CMR decoder is shown in Figure 4. Note that the CMR decoder can support additional memory sources by adding the residual connection and the attention block, but here we only show the structure with three sources: belief state representation (HB), system transcript representation (Ha), and user utterance representation (Hu), corresponding to a dialogue state tracking scenario. Since we share the pa- rameters between all of the decoders, thus CMRD is actually a 2-dimensional auto-regressive model with respect to both the condition generation and the sequence generation task. At each time step t, the CMR decoder \ufb01rst em- beds the token xt with a \ufb01xed token embedding E \u2208Rde\u00d7dv, where de is the embedding size and dv is the vocabulary size. The initial token x0 is \u201c[CLS]\u201d. The embedded vector ext is then en- coded with an LSTM, which emits a hidden repre- sentation h0 \u2208Rdm, h0 = LSTM(ext, qt\u22121). where qt is the hidden state of the LSTM.",
      "The initial token x0 is \u201c[CLS]\u201d. The embedded vector ext is then en- coded with an LSTM, which emits a hidden repre- sentation h0 \u2208Rdm, h0 = LSTM(ext, qt\u22121). where qt is the hidden state of the LSTM. q0 is initialized with an average of the hidden states of the belief encoder, the system encoder and the user encoder which produces HB, Ha, Hu respectively.",
      "where qt is the hidden state of the LSTM. q0 is initialized with an average of the hidden states of the belief encoder, the system encoder and the user encoder which produces HB, Ha, Hu respectively. h0 is then summed (element-wise) with the con- dition representation c \u2208Rdm to produce h1, which is (1) fed into the attention module; (2) used for residual connection; and (3) concatenated with other hi, (i > 1) to produce the concatenated working memory, r0, for relation reasoning, h1 = h0 + c, h2 = h1 + Attnbelief(h1, He), h3 = h2 + Attnsys(h2, Ha), h4 = h3 + Attnusr(h3, Hu), r = h1 \u2295h2 \u2295h3 \u2295h4 \u2208R4dm, where Attnk (k \u2208{belief, sys, usr}) are the atten- tion modules applied respectively to HB, Ha, Hu, and \u2295means the concatenation operator.",
      "The gra- dients are blocked for h1, h2, h3 during the back- propagation stage, since we only need them to work as the supplementary memories for the re- lation reasoning followed. The attention module takes a vector, h \u2208Rdm, and a matrix, H \u2208Rdm\u00d7l as input, where l is the sequence length of the representation, and outputs",
      "Figure 4: The general structure of the Conditional Memory Relation Decoder. The decoder output, s (e.g. \u201cfood\u201d), is re\ufb01lled to the LSTM for the decoding of the next step. The blue lines in the \ufb01gure means that the gradients are blocked during the back propagation stage. ha, a weighted sum of the column vectors in H. a = W T 1 h + b1 \u2208Rdm, c = softmax(HT a) \u2208Rl, h = Hc \u2208Rdm, ha = W T 2 h + b2 \u2208Rdm, where the weights W1 \u2208 Rdm\u00d7dm, W2 \u2208 Rdm\u00d7dm and the bias b1 \u2208Rdm, b2 \u2208Rdm are the learnable parameters. The order of the attention modules, i.e., \ufb01rst at- tend to the system and the user and then the be- lief, is decided empirically.",
      "The order of the attention modules, i.e., \ufb01rst at- tend to the system and the user and then the be- lief, is decided empirically. We can interpret this hierarchical structure as the internal order for the memory processing, since from the daily life expe- rience, people tend to attend to the most contem- porary memories (system/user utterance) \ufb01rst and then attend to the older history (belief states). All of the parameters are shared between the attention modules. The concatenated working memory, r0, is then fed into a Multi-Layer Perceptron (MLP) with four layers, r1 = \u03c3(W T 1 r0 + b1), r2 = \u03c3(W T 2 r1 + b2), r3 = \u03c3(W T 3 r2 + b3), hk = \u03c3(W T 4 r3 + b4), where \u03c3 is a non-linear activation, and the weights W1 \u2208R4dm\u00d7dm, Wi \u2208Rdm\u00d7dm and the bias b1 \u2208Rdm, bi \u2208Rdm are learnable parameters, and 2 \u2264i \u22644.",
      "The number of layers for the MLP is decided by the grid search. The hidden representation of the next token, hk, is \ufb01rst fed into a dropout layer with drop rate p, and then (1) emitted out of the decoder as a repre- sentation; and (2) fed into a linear layer to generate the next token, hs = dropout(hk) \u2208Rdm, ho = W T k hs + bs \u2208Rde, ps = softmax(ET ho) \u2208Rdv, s = argmax(ps) \u2208R, where the weight Wk \u2208Rdm\u00d7de and the bias bk \u2208Rde are learnable parameters. Since de is the embedding size and the model parameters are independent of the vocabulary size, the CMR de- coder can make predictions on a dynamic vocabu- lary and implicitly supports the generation of un- seen words. When training the model, we min- imize the cross-entropy loss between the output probabilities, ps, and the given labels. 4 Experiments 4.1 Experimental Setting We \ufb01rst test our model on the single domain dataset, WoZ2.0 (Wen et al., 2017). It consists",
      "Metric WoZ2.0 MultiWoZ Avg. # turns, t 7.45 13.68 Avg. tokens / turn, s 11.24 13.18 Number of Slots, n 3 35 Number of Values, m 99 4510 Training set size 600 8438 Validation set size 200 1000 Test set size 400 1000 Table 2: The statistics of the WoZ2.0 and the Multi- WoZ datasets. of 1,200 dialogues from the restaurant reservation domain with three pre-de\ufb01ned slots: food, price range, and area. Since the name slot rarely occurs in the dataset, it is not included in our experiments, following previous literature (Ren et al., 2018; Liu and Perez, 2017). Our model is also tested on the multi-domain dataset, MultiWoZ (Budzianowski et al., 2018). It has a more complex ontology with 7 domains and 25 prede\ufb01ned slots.",
      "Our model is also tested on the multi-domain dataset, MultiWoZ (Budzianowski et al., 2018). It has a more complex ontology with 7 domains and 25 prede\ufb01ned slots. Since the com- bined slot-value pairs representation of the belief states has to be applied for the model with O(n) ITC, the total number of slots is 35. The statistics of these two datsets are shown in Table 2. Based on the statistics from these two datasets, we can calculate the theoretical Inference Time Multiplier (ITM), K, as a metric of scalability.",
      "The statistics of these two datsets are shown in Table 2. Based on the statistics from these two datasets, we can calculate the theoretical Inference Time Multiplier (ITM), K, as a metric of scalability. Given the inference time complexity, ITM mea- sures how many times a model will be slower when being transferred from the WoZ2.0 dataset, d1, to the MultiWoZ dataset, d2, K = h(t)h(s)h(n)h(m) h(x) = ( 1 O(x) = O(1), xd2 xd1 otherwise, where O(x) means the Inference Time Complex- ity (ITC) of the variable x. For a model having an ITC of O(1) with respect to the number of slots n, and values m, the ITM will be a multiplier of 2.15x, while for an ITC of O(n), it will be a mul- tiplier of 25.1, and 1,143 for O(mn). As a convention, the metric of joint goal ac- curacy is used to compare our model to previ- ous work.",
      "As a convention, the metric of joint goal ac- curacy is used to compare our model to previ- ous work. The joint goal accuracy regards the model making a successful belief state prediction if all of the slots and values predicted are exactly matched with the labels provided. Since our model also generate un-ordered domains and slots, we only need to require the predicted belief dictio- nary to be exactly matched with the labels dur- ing model evaluation. We also did a simple post- processing to remove the generated (DOMAIN, SLOT, VALUE) triplets which have empty ele- ments. In this work, the model with the highest joint accuracy on the validation set is evaluated on the test set for the test joint accuracy measurement. 4.2 Implementation Details We use the BERTlarge model for both contextual and static embedding generation. All LSTMs in the model are stacked with 2 layers, and only the output of the last layer is taken as a hidden repre- sentation.",
      "4.2 Implementation Details We use the BERTlarge model for both contextual and static embedding generation. All LSTMs in the model are stacked with 2 layers, and only the output of the last layer is taken as a hidden repre- sentation. ReLU non-linearity is used for the acti- vation function, \u03c3. The hyper-parameters of our model are identical for both the WoZ2.0 and the MultiwoZ datasets: dropout rate p = 0.5, model size dm = 512, em- bedding size de = 1024. For training on WoZ2.0, the model is trained with a batch size of 32 and the ADAM optimizer (Kingma and Ba, 2015) for 150 epochs, while for MultiWoZ, the AMSGrad optimizer (Reddi et al., 2018) and a batch size of 16 is adopted for 15 epochs of training. For both optimizers, we use a learning rate of 0.0005 with a gradient clip of 2.0.",
      "For both optimizers, we use a learning rate of 0.0005 with a gradient clip of 2.0. We initialize all weights in our model with Kaiming initialization (He et al., 2015) and adopt zero initialization for the bias. All experiments are conducted on a single NVIDIA GTX 1080Ti GPU. 4.3 Results To measure the actual inference time multiplier of our model, we evaluate the runtime of the best- performing models on the validation sets of both the WoZ2.0 and MultiWoZ datasets. During eval- uation, we set the batch size to 1 to avoid the in\ufb02u- ence of data parallelism and sequence padding. On the validation set of WoZ2.0, we obtain a runtime of 65.6 seconds, while on MultiWoZ, the runtime is 835.2 seconds. Results are averaged across 5 runs. Considering that the validation set of Mul- tiWoZ is 5 times larger than that of WoZ2.0, the actual inference time multiplier is 2.54 for our model.",
      "Results are averaged across 5 runs. Considering that the validation set of Mul- tiWoZ is 5 times larger than that of WoZ2.0, the actual inference time multiplier is 2.54 for our model. Since the actual inference time multiplier roughly of the same magnitude as the theoretical value of 2.15, we can con\ufb01rm empirically that we have the O(1) inference time complexity and thus obtain full scalability to the number of slots and values pre-de\ufb01ned in an ontology.",
      "DST Models Joint Acc. WoZ 2.0 Joint Acc. MultiWoZ ITC Baselines (Mrksic et al., 2017) 70.8% 25.83% O(mn) NBT-CNN (Mrksic et al., 2017) 84.2% - O(mn) StateNet PSI (Ren et al., 2018) 88.9% - O(n) GLAD (Nouri and Hosseini-Asl, 2018) 88.5% 35.58% O(mn) HyST (ensemble) (Goel et al., 2019) - 44.22% O(n) DSTRead (ensemble) (Gao et al., 2019) - 42.12% O(n) TRADE (Wu et al., 2019) - 48.62% O(n) COMER 88.6% 48.79% O(1) Table 3: The joint goal accuracy of the DST models on the WoZ2.0 test set and the MultiWoZ test set.",
      "We also include the Inference Time Complexity (ITC) for each model as a metric for scalability. The baseline accuracy for the WoZ2.0 dataset is the Delexicalisation-Based (DB) Model (Mrksic et al., 2017), while the baseline for the MultiWoZ dataset is taken from the of\ufb01cial website of MultiWoZ (Budzianowski et al., 2018). Model Joint Acc. COMER 88.64% - Hierachical-Attn 86.69% - MLP 83.24% Table 4: The ablation study on the WoZ2.0 dataset with the joint goal accuracy on the test set. For \u201c- Hierachical-Attn\u201d, we remove the residual connections between the attention modules in the CMR decoders and all the attention memory access are based on the output from the LSTM. For \u201c- MLP\u201d, we further re- place the MLP with a single linear layer with the non- linear activation. Table 3 compares our model with the previous state-of-the-art on both the WoZ2.0 test set and the MultiWoZ test set.",
      "For \u201c- MLP\u201d, we further re- place the MLP with a single linear layer with the non- linear activation. Table 3 compares our model with the previous state-of-the-art on both the WoZ2.0 test set and the MultiWoZ test set. For the WoZ2.0 dataset, we maintain performance at the level of the state-of- the-art, with a marginal drop of 0.3% compared with previous work. Considering the fact that WoZ2.0 is a relatively small dataset, this small dif- ference does not represent a signi\ufb01cant big perfor- mance drop. On the muli-domain dataset, Multi- WoZ, our model achieves a joint goal accuracy of 48.79%, which marginally outperforms the previ- ous state-of-the-art. 4.4 Ablation Study To prove the effectiveness of our structure of the Conditional Memory Relation Decoder (CMRD), we conduct ablation experiments on the WoZ2.0 dataset. The effectiveness of our hierarchical at- tention design is proved by an accuracy drop of 1.95% after removing residual connections and the Model JD Acc. JDS Acc.",
      "The effectiveness of our hierarchical at- tention design is proved by an accuracy drop of 1.95% after removing residual connections and the Model JD Acc. JDS Acc. JG Acc. COMER 95.52% 55.81% 48.79% - moveDrop 95.34% 55.08% 47.19% - postprocess 95.53% 54.74% 45.72% - ShareParam 94.96% 54.40% 44.38% - Order 95.55% 55.06% 42.84% - Nested - 49.58% 40.57% - BlockGrad - 49.36% 39.15% Table 5: The ablation study on the MultiWoZ dataset with the joint domain accuracy (JD Acc.), joint domain-slot accuracy (JDS Acc.) and joint goal accu- racy (JG Acc.) on the test set. For \u201c- moveDrop\u201d, we move the dropout layer to be in front of the \ufb01nal lin- ear layer before the Softmax.",
      "), joint domain-slot accuracy (JDS Acc.) and joint goal accu- racy (JG Acc.) on the test set. For \u201c- moveDrop\u201d, we move the dropout layer to be in front of the \ufb01nal lin- ear layer before the Softmax. For \u201c- postprocess\u201d, we further \ufb01x the decoder embedding layer and remove the post-processing during model evaluation. For \u201c- ShareParam\u201d, we further remove the parameter shar- ing mechanism on the encoders and the attention mod- ules. For \u201c- Order\u201d, we further arrange the order of the slots according to its global frequencies in the training set instead of the local frequencies given the domain it belongs to. For \u201c- Nested\u201d, we do not generate do- main sequences but generate combined slot sequences which combines the domain and the slot together. For \u201c- BlockGrad\u201d, we further remove the gradient block- ing mechanism in the CMR decoder. hierarchical stack of our attention modules.",
      "For \u201c- Nested\u201d, we do not generate do- main sequences but generate combined slot sequences which combines the domain and the slot together. For \u201c- BlockGrad\u201d, we further remove the gradient block- ing mechanism in the CMR decoder. hierarchical stack of our attention modules. The accuracy drop of removing MLP is partly due to the reduction of the number of model parameters, but it also proves that stacking more layers in an MLP can improve the relational reasoning perfor- mance given a concatenation of multiple represen- tations from different sources.",
      "We also conduct the ablation study on the Mul- tiWoZ dataset for a more precise analysis on the hierarchical generation process. For joint domain accuracy, we calculate the probability that all do- mains generated in each turn are exactly matched with the labels provided. The joint domain-slot accuracy further calculate the probability that all domains and slots generated are correct, while the joint goal accuracy requires all the domains, slots and values generated are exactly matched with the labels. From Table 5, We can further calcu- late that given the correct slot prediction, COMER has 87.42% chance to make the correct value pre- diction. While COMER has done great job on domain prediction (95.52%) and value prediction (87.42%), the accuracy of the slot prediction given the correct domain is only 58.43%. We suspect that this is because we only use the previous be- lief state to represent the dialogue history, and the inter-turn reasoning ability on the slot prediction suffers from the limited context and the accuracy is harmed due to the multi-turn mapping problem (Wu et al., 2019). We can also see that the JDS Acc.",
      "We can also see that the JDS Acc. has an absolute boost of 5.48% when we switch from the combined slot representation to the nested tuple representation. This is because the subordinate relationship between the domains and the slots can be captured by the hierarchi- cal sequence generation, while this relationship is missed when generating the domain and slot to- gether via the combined slot representation. 4.5 Qualitative Analysis Figure 5 shows an example of the belief state pre- diction result in one turn of a dialogue on the MultiWoZ test set. The visualization includes the CMRD attention scores over the belief states, sys- tem transcript and user utterance during the decod- ing stage of the slot sequence. From the system attention (top right), since it is the \ufb01rst attention module and no previous con- text information is given, it can only \ufb01nd the in- formation indicating the slot \u201cdeparture\u201d from the system utterance under the domain condition, and attend to the evidence \u201cleaving\u201d correctly during the generation step of \u201cdeparture\u201d.",
      "From the user attention, we can see that it captures the most help- ful keywords that are necessary for correct pre- diction, such as \u201cafter\u201d for \u201cday\u201d and \u201cleave at\u201d, \u201cto\u201d for \u201cdestination\u201d. Moreover, during the gen- eration step of \u201cdeparture\u201d, the user attention suc- cessfully discerns that, based on the context, the word \u201cleave\u201d is not the evidence that need to be accumulated and choose to attend nothing in this step. For the belief attention, we can see that the belief attention module correctly attends to a pre- vious slot for each generation step of a slot that has been presented in the previous state. For the gen- eration step of the new slot \u201cdestination\u201d, since the previous state does not have the \u201cdestination\u201d slot, the belief attention module only attends to the \u2018-\u2019 mark after the \u2018train\u2019 domain to indicate that the generated word should belong to this domain. 5 Related Work Semi-scalable Belief Tracker Rastogi et al. (2017) proposed an approach that can generate \ufb01xed-length candidate sets for each of the slots from the dialogue history.",
      "5 Related Work Semi-scalable Belief Tracker Rastogi et al. (2017) proposed an approach that can generate \ufb01xed-length candidate sets for each of the slots from the dialogue history. Although they only need to perform inference for a \ufb01xed number of values, they still need to iterate over all slots de- \ufb01ned in the ontology to make a prediction for a given dialogue turn. In addition, their method needs an external language understanding module to extract the exact entities from a dialogue to form candidates, which will not work if the label value is an abstraction and does not have the exact match with the words in the dialogue. StateNet (Ren et al., 2018) achieves state-of- the-art performance with the property that its pa- rameters are independent of the number of slot val- ues in the candidate set, and it also supports online training or inference with dynamically changing slots and values. Given a slot that needs tracking, it only needs to perform inference once to make the prediction for a turn, but this also means that its inference time complexity is proportional to the number of slots.",
      "Given a slot that needs tracking, it only needs to perform inference once to make the prediction for a turn, but this also means that its inference time complexity is proportional to the number of slots. TRADE (Wu et al., 2019) achieves state-of-the- art performance on the MultiWoZ dataset by ap- plying the copy mechanism for the value sequence generation. Since TRADE takes n combinations of the domains and slots as the input, the infer- ence time complexity of TRADE is O(n). The performance improvement achieved by TRADE is mainly due to the fact that it incorporates the copy mechanism that can boost the accuracy on the name slot, which mainly needs the ability in copying names from the dialogue history. How- ever, TRADE does not report its performance on the WoZ2.0 dataset which does not have the name slot.",
      "Figure 5: An example belief prediction of our model on the MultiWoZ test set. The attention scores for belief states, system transcript and user utterance in CMRD is visualized on the right. Each row corresponds to the attention score of the generation step of a particular slot under the \u2018train\u2019 domain. DSTRead (Gao et al., 2019) formulate the dia- logue state tracking task as a reading comprehen- sion problem by asking slot speci\ufb01ed questions to the BERT model and \ufb01nd the answer span in the dialogue history for each of the pre-de\ufb01ned com- bined slot. Thus its inference time complexity is still O(n). This method suffers from the fact that its generation vocabulary is limited to the words occurred in the dialogue history, and it has to do a manual combination strategy with another joint state tracking model on the development set to achieve better performance. Contextualized Word Embedding (CWE) was \ufb01rst proposed by Peters et al. (2018). Based on the intuition that the meaning of a word is highly cor- related with its context, CWE takes the complete context (sentences, passages, etc.)",
      "Contextualized Word Embedding (CWE) was \ufb01rst proposed by Peters et al. (2018). Based on the intuition that the meaning of a word is highly cor- related with its context, CWE takes the complete context (sentences, passages, etc.) as the input, and outputs the corresponding word vectors that are unique under the given context. Recently, with the success of language models (e.g. Devlin et al. (2018)) that are trained on large scale data, con- textualizeds word embedding have been further improved and can achieve the same performance compared to (less \ufb02exible) \ufb01nely-tuned pipelines. Sequence Generation Models. Recently, se- quence generation models have been successfully applied in the realm of multi-label classi\ufb01ca- tion (MLC) (Yang et al., 2018). Different from traditional binary relevance methods, they pro- posed a sequence generation model for MLC tasks which takes into consideration the correlations be- tween labels.",
      "Different from traditional binary relevance methods, they pro- posed a sequence generation model for MLC tasks which takes into consideration the correlations be- tween labels. Speci\ufb01cally, the model follows the encoder-decoder structure with an attention mech- anism (Cho et al., 2014), where the decoder gen- erates a sequence of labels. Similar to language modeling tasks, the decoder output at each time step will be conditioned on the previous predic- tions during generation. Therefore the correlation between generated labels is captured by the de- coder. 6 Conclusion In this work, we proposed the Conditional Mem- ory Relation Network (COMER), the \ufb01rst dia- logue state tracking model that has a constant in- ference time complexity with respect to the num- ber of domains, slots and values pre-de\ufb01ned in an ontology. Besides its scalability, the joint goal ac- curacy of our model also achieve the similar per- formance compared with the state-of-the-arts on both the MultiWoZ dataset and the WoZ dataset.",
      "Besides its scalability, the joint goal ac- curacy of our model also achieve the similar per- formance compared with the state-of-the-arts on both the MultiWoZ dataset and the WoZ dataset. Due to the \ufb02exibility of our hierarchical encoder- decoder framework and the CMR decoder, abun- dant future research direction remains as apply- ing the transformer structure, incorporating open vocabulary and copy mechanism for explicit un- seen words generation, and inventing better dia- logue history access mechanism to accommodate ef\ufb01cient inter-turn reasoning. Acknowledgements. This work is partly sup- ported by NSF #1750063. We thank all the re- viewers for their constructive suggestions. We also want to thank Zhuowen Tu and Shengnan Zhang for the early discussions of the project.",
      "References Pawel Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I\u02dcnigo Casanueva, Stefan Ultes, Osman Ra- madan, and Milica Gasic. 2018. Multiwoz - a large- scale multi-domain wizard-of-oz dataset for task- oriented dialogue modelling. In EMNLP. Kyunghyun Cho, Bart van Merrienboer, aglar Glehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. In NAACL-HLT. Shuyang Gao, Abhishek Sethi, Sanchit Agarwal, Tagy- oung Chung, and Dilek Zeynep Hakkani. 2019.",
      "In NAACL-HLT. Shuyang Gao, Abhishek Sethi, Sanchit Agarwal, Tagy- oung Chung, and Dilek Zeynep Hakkani. 2019. Di- alog state tracking: A neural reading comprehension approach. ArXiv, abs/1908.01946. Rahul Goel, Shachi Paul, and Dilek Zeynep Hakkani. 2019. Hyst: A hybrid approach for \ufb02exible and accurate dialogue state tracking. ArXiv, abs/1907.00883. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving deep into recti\ufb01ers: Surpass- ing human-level performance on imagenet classi- \ufb01cation. 2015 IEEE International Conference on Computer Vision (ICCV), pages 1026\u20131034. Matthew Henderson, Blaise Thomson, and Jason D. Williams. 2014. The second dialog state tracking challenge. In SIGDIAL Conference.",
      "2015 IEEE International Conference on Computer Vision (ICCV), pages 1026\u20131034. Matthew Henderson, Blaise Thomson, and Jason D. Williams. 2014. The second dialog state tracking challenge. In SIGDIAL Conference. Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9:1735\u2013 1780. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. CoRR. Fei Liu and Julien Perez. 2017. Dialog state tracking, a machine reading approach using memory network. In EACL. Nikola Mrksic, Diarmuid \u00b4O S\u00b4eaghdha, Tsung-Hsien Wen, Blaise Thomson, and Steve J. Young. 2017. Neural belief tracker: Data-driven dialogue state tracking. In ACL. Elnaz Nouri and Ehsan Hosseini-Asl. 2018. Toward scalable neural dialogue state tracking model.",
      "2017. Neural belief tracker: Data-driven dialogue state tracking. In ACL. Elnaz Nouri and Ehsan Hosseini-Asl. 2018. Toward scalable neural dialogue state tracking model. arXiv preprint arXiv:1812.00899. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke S. Zettlemoyer. 2018. Deep contextualized word representations. In NAACL-HLT. Osman Ramadan, Pawel Budzianowski, and Milica Gasic. 2018. Large-scale multi-domain belief track- ing with knowledge sharing. In ACL. Abhinav Rastogi, Dilek Hakkani-Tur, and Larry Heck. 2017. Scalable multi-domain dialogue state track- ing. arXiv preprint arXiv:1712.10224. Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. 2018. On the convergence of adam and beyond. In ICLR.",
      "arXiv preprint arXiv:1712.10224. Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. 2018. On the convergence of adam and beyond. In ICLR. Liliang Ren, Kaige Xie, Lu Chen, and Kai Yu. 2018. Towards universal dialogue state tracking. In EMNLP. Adam Santoro, David Raposo, David G. T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter W. Battaglia, and Timothy P. Lillicrap. 2017. A sim- ple neural network module for relational reasoning. In NIPS. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. CoRR, abs/1508.07909. Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. 2015. End-to-end memory net- works. In NIPS.",
      "CoRR, abs/1508.07909. Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. 2015. End-to-end memory net- works. In NIPS. Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Lina Maria Rojas-Barahona, Pei hao Su, Stefan Ultes, David Vandyke, and Steve J. Young. 2017. A network-based end-to-end trainable task-oriented dialogue system. In EACL. Chien-Sheng Wu, Andrea Madotto, Ehsan Hosseini- Asl, Caiming Xiong, Richard Socher, and Pascale Fung. 2019. Transferable multi-domain state gener- ator for task-oriented dialogue systems. In ACL. Puyang Xu and Qi Hu. 2018. An end-to-end approach for handling unknown slot values in dialogue state tracking. In ACL. Pengcheng Yang, Xu Sun, Wei Li, Shuming Ma, Wei Wu, and Houfeng Wang. 2018.",
      "Puyang Xu and Qi Hu. 2018. An end-to-end approach for handling unknown slot values in dialogue state tracking. In ACL. Pengcheng Yang, Xu Sun, Wei Li, Shuming Ma, Wei Wu, and Houfeng Wang. 2018. Sgm: Sequence gen- eration model for multi-label classi\ufb01cation. In COL- ING. Steve Young, Milica Ga\u02c7si\u00b4c, Blaise Thomson, and Ja- son D Williams. 2013. Pomdp-based statistical spo- ken dialog systems: A review. Proceedings of the IEEE, 101(5):1160\u20131179. Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. 2018. Residual dense network for im- age super-resolution. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2472\u20132481. Victor Zhong, Caiming Xiong, and Richard Socher. 2018. Global-locally self-attentive dialogue state tracker.",
      "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2472\u20132481. Victor Zhong, Caiming Xiong, and Richard Socher. 2018. Global-locally self-attentive dialogue state tracker. CoRR, abs/1805.09655."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1909.00754.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":9395,
  "avg_doclen":177.2641509434,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1909.00754.pdf"
    }
  }
}