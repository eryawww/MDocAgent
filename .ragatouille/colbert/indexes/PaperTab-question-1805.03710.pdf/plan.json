{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Incorporating Subword Information into Matrix Factorization Word Embeddings Alexandre Salle Aline Villavicencio Institute of Informatics Universidade Federal do Rio Grande do Sul Porto Alegre, Brazil alex@alexsalle.com avillavicencio@inf.ufrgs.br Abstract The positive effect of adding subword infor- mation to word embeddings has been demon- strated for predictive models. In this paper we investigate whether similar bene\ufb01ts can also be derived from incorporating subwords into counting models. We evaluate the impact of different types of subwords (n-grams and un- supervised morphemes), with results con\ufb01rm- ing the importance of subword information in learning representations of rare and out-of- vocabulary words. 1 1 Introduction Low dimensional word representations (embed- dings) have become a key component in modern NLP systems for language modeling, parsing, sen- timent classi\ufb01cation, and many others. These em- beddings are usually derived by employing the dis- tributional hypothesis: that similar words appear in similar contexts (Harris, 1954).",
            "These em- beddings are usually derived by employing the dis- tributional hypothesis: that similar words appear in similar contexts (Harris, 1954). The models that perform the word embedding can be divided into two classes: predictive, which learn a target or context word distribution, and counting, which use a raw, weighted, or factored word-context co-occurrence matrix (Baroni et al., 2014). The most well-known predictive model, which has become eponymous with word embed- ding, is word2vec (Mikolov et al., 2013a). Pop- ular counting models include PPMI-SVD (Levy et al., 2014), GloVe (Pennington et al., 2014), and LexVec (Salle et al., 2016b). These models all learn word-level represen- tations, which presents two main problems: 1) Learned information is not explicitly shared among the representations as each word has an in- dependent vector. 2) There is no clear way to rep- resent out-of-vocabulary (OOV) words.",
            "These models all learn word-level represen- tations, which presents two main problems: 1) Learned information is not explicitly shared among the representations as each word has an in- dependent vector. 2) There is no clear way to rep- resent out-of-vocabulary (OOV) words. 1This is a preprint of the paper that will be presented at the Second Workshop on Subword and Character LEvel Models in NLP (SCLeM) to be held at NAACL 2018. fastText (Bojanowski et al., 2017) addresses these issues in the Skip-gram word2vec model by representing a word by the sum of a unique vector and a set of shared character n-grams (from hereon simply referred to as n-grams) vectors. This ad- dresses both issues above as learned information is shared through the n-gram vectors and from these OOV word representations can be constructed. In this paper we propose incorporating subword information into counting models using a strategy similar to fastText.",
            "This ad- dresses both issues above as learned information is shared through the n-gram vectors and from these OOV word representations can be constructed. In this paper we propose incorporating subword information into counting models using a strategy similar to fastText. We use LexVec as the counting model as it generally outperforms PPMI-SVD and GloVe on intrinsic and extrinsic evaluations (Salle et al., 2016a; Cer et al., 2017; Wohlgenannt et al., 2017; Konkol et al., 2017), but the method proposed here should transfer to GloVe unchanged. The LexVec objective is modi\ufb01ed 2 such that a word\u2019s vector is the sum of all its subword vectors. We compare 1) the use of n-gram subwords, like fastText, and 2) unsupervised morphemes iden- ti\ufb01ed using Morfessor (Virpioja, 2013) to learn whether more linguistically motivated subwords offer any advantage over simple n-grams.",
            "To evaluate the impact subword information has on in-vocabulary (IV) word representations, we run intrinsic evaluations consisting of word sim- ilarity and word analogy tasks. The incorporation of subword information results in similar gains (and losses) to that of fastText over Skip-gram. Whereas incorporating n-gram subwords tends to capture more syntactic information, unsupervised morphemes better preserve semantics while also improving syntactic results. Given that intrin- sic performance can correlate poorly with per- formance on downstream tasks (Tsvetkov et al., 2015), we also conduct evaluation using the Ve- cEval suite of tasks (Nayak et al., 2016), in which 2Our implementation of subword LexVec is available at https:\/\/github.com\/alexandres\/lexvec arXiv:1805.03710v1  [cs.CL]  9 May 2018",
            "all subword models, including fastText, show no signi\ufb01cant improvement over word-level mod- els. We verify the model\u2019s ability to represent OOV words by quantitatively evaluating nearest- neighbors. Results show that, like fastText, both LexVec n-gram and (to a lesser degree) unsuper- vised morpheme models give coherent answers. This paper discusses related word (\u00a72), intro- duces the subword LexVec model (\u00a73), describes experiments (\u00a74), analyzes results (\u00a75), and con- cludes with ideas for future works (\u00a76). 2 Related Work Word embeddings that leverage subword informa- tion were \ufb01rst introduced by Sch\u00a8utze (1993) which represented a word of as the sum of four-gram vec- tors obtained running an SVD of a four-gram to four-gram co-occurrence matrix. Our model dif- fers by learning the subword vectors and result- ing representation jointly as weighted factoriza- tion of a word-context co-occurrence matrix is per- formed.",
            "Our model dif- fers by learning the subword vectors and result- ing representation jointly as weighted factoriza- tion of a word-context co-occurrence matrix is per- formed. There are many models that use character-level subword information to form word representations (Ling et al., 2015; Cao and Rei, 2016; Kim et al., 2016; Wieting et al., 2016; Verwimp et al., 2017), as well as fastText (the model on which we base our work). Closely related are models that use morphological segmentation in learning word rep- resentations (Luong et al., 2013; Botha and Blun- som, 2014; Qiu et al., 2014; Mitchell and Steed- man, 2015; Cotterell and Sch\u00a8utze, 2015; Bhatia et al., 2016). Our model also uses n-grams and morphological segmentation, but it performs ex- plicit matrix factorization to learn subword and word representations, unlike these related models which mostly use neural networks. Finally, Cotterell et al.",
            "Our model also uses n-grams and morphological segmentation, but it performs ex- plicit matrix factorization to learn subword and word representations, unlike these related models which mostly use neural networks. Finally, Cotterell et al. (2016) and V\u00b4ulic et al. (2017) retro\ufb01t morphological information onto pre-trained models. These differ from our work in that we incorporate morphological information at training time, and that only Cotterell et al. (2016) is able to generate embeddings for OOV words. 3 Subword LexVec The LexVec (Salle et al., 2016a) model factorizes the PPMI-weighted word-context co-occurrence matrix using stochastic gradient descent.",
            "(2016) is able to generate embeddings for OOV words. 3 Subword LexVec The LexVec (Salle et al., 2016a) model factorizes the PPMI-weighted word-context co-occurrence matrix using stochastic gradient descent. PPMIwc = max(0, log Mwc M\u2217\u2217 Mw\u2217M\u2217c ) (1) where M is the word-context co-occurrence ma- trix constructed by sliding a window of \ufb01xed size centered over every target word w in the subsampled (Mikolov et al., 2013a) training corpus and incrementing cell Mwc for ev- ery context word c appearing within this window (forming a (w, c) pair). LexVec adjusts the PPMI matrix using context distribution smoothing (Levy et al., 2014).",
            "LexVec adjusts the PPMI matrix using context distribution smoothing (Levy et al., 2014). With the PPMI matrix calculated, the sliding window process is repeated and the following loss functions are minimized for every observed (w, c) pair and target word w: Lwc = 1 2(u\u22a4 wvc \u2212PPMIwc)2 (2) Lw = 1 2 k X i=1 Eci\u223cPn(c)(u\u22a4 wvci \u2212PPMIwci)2 (3) where uw and vc are d-dimensional word and context vectors. The second loss function de- scribes how, for each target word, k negative sam- ples (Mikolov et al., 2013a) are drawn from the smoothed context unigram distribution. Given a set of subwords Sw for a word w, we follow fastText and replace uw in eqs.",
            "Given a set of subwords Sw for a word w, we follow fastText and replace uw in eqs. (2) and (3) by u\u2032 w such that: u\u2032 w = 1 |Sw| + 1(uw + X s\u2208Sw qhash(s)) (4) such that a word is the sum of its word vector and its d-dimensional subword vectors qx. The num- ber of possible subwords is very large so the func- tion hash(s)3 hashes a subword to the interval [1, buckets]. For OOV words, u\u2032 w = 1 |Sw| X s\u2208Sw qhash(s) (5) We compare two types of subwords: simple n-grams (like fastText) and unsupervised mor- phemes. For example, given the word \u201ccat\u201d, we mark beginning and end with angled brackets and use all n-grams of length 3 to 6 as subwords, yielding Scat = {\u27e8ca, at\u27e9, cat}.",
            "For example, given the word \u201ccat\u201d, we mark beginning and end with angled brackets and use all n-grams of length 3 to 6 as subwords, yielding Scat = {\u27e8ca, at\u27e9, cat}. Morfessor (Vir- pioja, 2013) is used to probabilistically segment 3http:\/\/www.isthe.com\/chongo\/tech\/comp\/fnv\/",
            "words into morphemes. The Morfessor model is trained using raw text so it is entirely unsu- pervised. For the word \u201csubsequent\u201d, we get Ssubsequent = {\u27e8sub, sequent\u27e9}. 4 Materials Our experiments aim to measure if the incorpora- tion of subword information into LexVec results in similar improvements as observed in moving from Skip-gram to fastText, and whether unsu- pervised morphemes offer any advantage over n- grams. For IV words, we perform intrinsic evalua- tion via word similarity and word analogy tasks, as well as downstream tasks. OOV word representa- tion is tested through qualitative nearest-neighbor analysis. All models are trained using a 2015 dump of Wikipedia, lowercased and using only alphanu- meric characters. Vocabulary is limited to words that appear at least 100 times for a total of 303517 words. Morfessor is trained on this vocabulary list. We train the standard LexVec (LV), LexVec using n-grams (LV-N), and LexVec using unsupervised morphemes (LV-M) using the same hyper-parameters as Salle et al.",
            "Morfessor is trained on this vocabulary list. We train the standard LexVec (LV), LexVec using n-grams (LV-N), and LexVec using unsupervised morphemes (LV-M) using the same hyper-parameters as Salle et al. (2016a) (window = 2, initial learning rate = .025, subsampling = 10\u22125, negative samples = 5, context distribution smoothing = .75, positional contexts = True). Both Skip-gram (SG) and fastText (FT) are trained using the reference implementation4 of fastText with the hyper-parameters given by Bojanowski et al. (2017) (window = 5, initial learning rate = .025, subsampling = 10\u22124, negative samples = 5). All \ufb01ve models are run for 5 iterations over the training corpus and generate 300 dimensional word representations. LV-N, LV-M, and FT use 2000000 buckets when hashing subwords.",
            "All \ufb01ve models are run for 5 iterations over the training corpus and generate 300 dimensional word representations. LV-N, LV-M, and FT use 2000000 buckets when hashing subwords. For word similarity evaluations, we use the WordSim-353 Similarity (WS-Sim) and Relat- edness (WS-Rel) (Finkelstein et al., 2001) and SimLex-999 (SimLex) (Hill et al., 2015) datasets, and the Rare Word (RW) (Luong et al., 2013) dataset to verify if subword information improves rare word representation. Relationships are mea- sured using the Google semantic (GSem) and syn- tactic (GSyn) analogies (Mikolov et al., 2013a) and the Microsoft syntactic analogies (MSR) dataset (Mikolov et al., 2013b).",
            "4https:\/\/github.com\/facebookresearch\/fastText Evaluation LV LV-N LV-M SG FT WS-Sim .749 .748 .746 .783 .778 WS-Rel .627 .627 .625 .683 .672 SimLex .359 .374 .366 .371 .367 RW .461 .522 .479 .481 .500 GSem 80.7 73.8 80.7 78.9 77.0 GSyn 62.8 68.6 63.8 68.2 71.1 MSR 49.6 55.0 53.8 57.8 59.6 Chunk 90.4 90.6 90.5 90.4 90.4 Sentiment 77.0 77.0 77.6 75.3 77.9 Questions 87.4 87.4 87.3 86.6 85.1 NLI 43.3 43.4 43.3 43.4 43.8 Table 1: Word similarity (Spearman\u2019s rho), analogy (% accuracy), and downstream task (% accuracy) re- sults.",
            "In downstream tasks, for the same model accu- racy varies over different runs, so we report the mean over 20 runs, in which the only signi\ufb01cantly (p < .05 under a random permutation test) different result is in chunking. We also evaluate all \ufb01ve models on downstream tasks from the VecEval suite (Nayak et al., 2016)5, using only the tasks for which training and evalu- ation data is freely available: chunking, sentiment and question classi\ufb01cation, and natural language identi\ufb01cation (NLI). The default settings from the suite are used, but we run only the \ufb01xed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embed- dings. Finally, we use LV-N, LV-M, and FT to gen- erate OOV word representations for the follow- ing words: 1) \u201chellooo\u201d: a greeting commonly used in instant messaging which emphasizes a syl- lable.",
            "Finally, we use LV-N, LV-M, and FT to gen- erate OOV word representations for the follow- ing words: 1) \u201chellooo\u201d: a greeting commonly used in instant messaging which emphasizes a syl- lable. 2) \u201cmarvelicious\u201d: a made-up word ob- tained by merging \u201cmarvelous\u201d and \u201cdelicious\u201d. 3) \u201clouisana\u201d: a misspelling of the proper name \u201cLouisiana\u201d. 4) \u201crereread\u201d: recursive use of pre\ufb01x \u201cre\u201d. 5) \u201ctuzread\u201d: made-up pre\ufb01x \u201ctuz\u201d. 5 Results Results for IV evaluation are shown in table 1, and for OOV in table 2. Like in FT, the use of subword information in both LV-N and LV-M results in 1) better represen- tation of rare words, as evidenced by the increase in RW correlation, and 2) signi\ufb01cant improvement on the GSyn and MSR tasks, in evidence of sub- 5https:\/\/github.com\/NehaNayak\/veceval",
            "Word Model 5 Nearest Neighbors \u201chellooo\u201d LV-N hellogoodbye, hello, helloworld, helloween, helluva LV-M kitsos, \ufb01nos, neros, nonono, theodoroi FT hello, helloworld, hellogoodbye, helloween, joegazz \u201cmarvelicious\u201d LV-N delicious, marveled, marveling, licious, marvellous LV-M marveling, marvelously, marveled, marvelled, loquacious FT delicious, deliciously, marveling, licious, marvelman \u201clouisana\u201d LV-N luisana, pisana, belisana, chiisana, rosana LV-M louisy, louises, louison, louiseville, louisiade FT luisana, louisa, belisana, anabella, rosana \u201crereread\u201d LV-N reread, rereading, read, writeread, rerecord LV-M alread, carreer, whiteread, unremarked, oread FT reread, rereading, read, reiterate, writeread \u201ctuzread\u201d LV-N tuzi, tuz, tuzla,",
            "read, writeread, rerecord LV-M alread, carreer, whiteread, unremarked, oread FT reread, rereading, read, reiterate, writeread \u201ctuzread\u201d LV-N tuzi, tuz, tuzla, prizren, momchilgrad, studenica LV-M tuzluca, paczk, goldsztajn, belzberg, yizkor FT pazaryeri, tufanbeyli, yenipazar, leskovac, berovo Table 2: We generate vectors for OOV using subword information and search for the nearest (cosine distance) words in the embedding space. The LV-M segmentation for each word is: {\u27e8hell, o, o, o\u27e9}, {\u27e8marvel, i, cious\u27e9}, {\u27e8louis, ana\u27e9}, {\u27e8re, re, read\u27e9}, {\u27e8tu, z, read\u27e9}. We omit the LV-N and FT n-grams as they are trivial and too numerous to list.",
            "We omit the LV-N and FT n-grams as they are trivial and too numerous to list. words encoding information about a word\u2019s syn- tactic function (the suf\ufb01x \u201cly\u201d, for example, sug- gests an adverb). There seems to a trade-off between capturing semantics and syntax as in both LV-N and FT there is an accompanying decrease on the GSem tasks in exchange for gains on the GSyn and MSR tasks. Morphological segmentation in LV-M appears to favor syntax less strongly than do simple n-grams. On the downstream tasks, we only observe sta- tistically signi\ufb01cant (p < .05 under a random per- mutation test) improvement on the chunking task, and it is a very small gain. We attribute this to both regular and subword models having very similar quality on frequent IV word representation. Statis- tically, these are the words are that are most likely to appear in the downstream task instances, and so the superior representation of rare words has, due to their nature, little impact on over- all accuracy.",
            "Statis- tically, these are the words are that are most likely to appear in the downstream task instances, and so the superior representation of rare words has, due to their nature, little impact on over- all accuracy. Because in all tasks OOV words are mapped to the \u201c\u27e8unk\u27e9\u201d token, the subword models are not being used to the fullest, and in future work we will investigate whether generating representa- tions for all words improves task performance. In OOV representation (table 2), LV-N and FT work almost identically, as is to be expected. Both \ufb01nd highly coherent neighbors for the words \u201chel- looo\u201d, \u201cmarvelicious\u201d, and \u201crereread\u201d. Interest- ingly, the misspelling of \u201clouisana\u201d leads to co- herent name-like neighbors, although none is the expected correct spelling \u201clouisiana\u201d. All models stumble on the made-up pre\ufb01x \u201ctuz\u201d. A possible \ufb01x would be to down-weigh very rare subwords in the vector summation.",
            "All models stumble on the made-up pre\ufb01x \u201ctuz\u201d. A possible \ufb01x would be to down-weigh very rare subwords in the vector summation. LV-M is less robust than LV-N and FT on this task as it is highly sensitive to incorrect segmentation, exempli\ufb01ed in the \u201chel- looo\u201d example. Finally, we see that nearest-neighbors are a mixture of similarly pre\/suf\ufb01xed words. If these pre\/suf\ufb01xes are semantic, the neighbors are se- mantically related, else if syntactic they have sim- ilar syntactic function. This suggests that it should be possible to get tunable representations which are more driven by semantics or syntax by a weighted summation of subword vectors, given we can identify whether a pre\/suf\ufb01x is semantic or syntactic in nature and weigh them accordingly. This might be possible without supervision using corpus statistics as syntactic subwords are likely to be more frequent, and so could be down-weighted for more semantic representations. This is some- thing we will pursue in future work.",
            "This might be possible without supervision using corpus statistics as syntactic subwords are likely to be more frequent, and so could be down-weighted for more semantic representations. This is some- thing we will pursue in future work. 6 Conclusion and Future Work In this paper, we incorporated subword infor- mation (simple n-grams and unsupervised mor- phemes) into the LexVec word embedding model and evaluated its impact on the resulting IV and OOV word vectors. Like fastText, sub- word LexVec learns better representations for rare",
            "words than its word-level counterpart. All mod- els generated coherent representations for OOV words, with simple n-grams demonstrating more robustness than unsupervised morphemes. In fu- ture work, we will verify whether using OOV rep- resentations in downstream tasks improves perfor- mance. We will also explore the trade-off between semantics and syntax when subword information is used. References Marco Baroni, Georgiana Dinu, and Germ\u00b4an Kruszewski. 2014. Dont count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1, pages 238\u2013247. Parminder Bhatia, Robert Guthrie, and Jacob Eisen- stein. 2016. Morphological priors for probabilistic neural word embeddings. In EMNLP. Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. TACL, 5:135\u2013146.",
            "In EMNLP. Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. TACL, 5:135\u2013146. Jan A. Botha and Phil Blunsom. 2014. Compositional morphology for word representations and language modelling. In ICML. Kris Cao and Marek Rei. 2016. A joint model for word embedding and word morphology. In Rep4NLP@ACL. Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez- Gazpio, and Lucia Specia. 2017. Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Eval- uation (SemEval-2017), pages 1\u201314. Ryan Cotterell and Hinrich Sch\u00a8utze. 2015. Morpho- logical word-embeddings. In HLT-NAACL.",
            "Ryan Cotterell and Hinrich Sch\u00a8utze. 2015. Morpho- logical word-embeddings. In HLT-NAACL. Ryan Cotterell, Hinrich Sch\u00a8utze, and Jason Eisner. 2016. Morphological smoothing and extrapolation of word embeddings. In ACL. Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey- tan Ruppin. 2001. Placing search in context: The concept revisited. In Proceedings of the 10th inter- national conference on World Wide Web, pages 406\u2013 414. ACM. Zellig S Harris. 1954. Distributional structure. Word. Felix Hill, Roi Reichart, and Anna Korhonen. 2015. Simlex-999: Evaluating semantic models with (gen- uine) similarity estimation. Computational Linguis- tics, 41(4):665\u2013695.",
            "Word. Felix Hill, Roi Reichart, and Anna Korhonen. 2015. Simlex-999: Evaluating semantic models with (gen- uine) similarity estimation. Computational Linguis- tics, 41(4):665\u2013695. Yoon Kim, Yacine Jernite, David Sontag, and Alexan- der M. Rush. 2016. Character-aware neural lan- guage models. In AAAI. Michal Konkol, Tomas Brychcin, Michal Nykl, and Tom\u00b4as Hercig. 2017. Geographical evaluation of word embeddings. In IJCNLP. Omer Levy, Yoav Goldberg, and Israel Ramat-Gan. 2014. Linguistic regularities in sparse and explicit word representations. CoNLL-2014, page 171. Wang Ling, Chris Dyer, Alan W. Black, Isabel Tran- coso, Ramon Fermandez, Silvio Amir, Lu\u00b4\u0131s Marujo, and Tiago Lu\u00b4\u0131s. 2015.",
            "CoNLL-2014, page 171. Wang Ling, Chris Dyer, Alan W. Black, Isabel Tran- coso, Ramon Fermandez, Silvio Amir, Lu\u00b4\u0131s Marujo, and Tiago Lu\u00b4\u0131s. 2015. Finding function in form: Compositional character models for open vocabu- lary word representation. In EMNLP. Minh-Thang Luong, Richard Socher, and Christo- pher D Manning. 2013. Better word representa- tions with recursive neural networks for morphol- ogy. CoNLL-2013, 104. Tomas Mikolov, Kai Chen, Greg Corrado, and Jef- frey Dean. 2013a. Ef\ufb01cient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013b. Linguistic regularities in continuous space word representations. In HLT-NAACL, pages 746\u2013 751. Jeff Mitchell and Mark Steedman.",
            "Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013b. Linguistic regularities in continuous space word representations. In HLT-NAACL, pages 746\u2013 751. Jeff Mitchell and Mark Steedman. 2015. Orthogonality of syntax and semantics within distributional spaces. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Lan- guage Processing (Volume 1: Long Papers), vol- ume 1, pages 1301\u20131310. Neha Nayak, Gabor Angeli, and Christopher D. Man- ning. 2016. Evaluating word embeddings using a representative suite of practical tasks. In Pro- ceedings of the 1st Workshop on Evaluating Vector- Space Representations for NLP, RepEval@ACL 2016, Berlin, Germany, August 2016, pages 19\u201323. Association for Computational Linguistics. Jeffrey Pennington, Richard Socher, and Christo- pher D Manning. 2014.",
            "Association for Computational Linguistics. Jeffrey Pennington, Richard Socher, and Christo- pher D Manning. 2014. Glove: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12. Siyu Qiu, Qing Cui, Jiang Bian, Bin Gao, and Tie-Yan Liu. 2014. Co-learning of word representations and morpheme representations. In Proceedings of COL- ING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 141\u2013150. Alexandre Salle, Marco Idiart, and Aline Villavicen- cio. 2016a. Enhancing the lexvec distributed word representation model using positional contexts and external memory. CoRR, abs\/1606.01283.",
            "Alexandre Salle, Aline Villavicencio, and Marco Idiart. 2016b. Matrix factorization using window sampling and negative sampling for improved word represen- tations. In Proceedings of the 54th Annual Meet- ing of the Association for Computational Linguis- tics, ACL 2016, August 7-12, 2016, Berlin, Ger- many, Volume 2: Short Papers. The Association for Computer Linguistics. Hinrich Sch\u00a8utze. 1993. Word space. In Advances in neural information processing systems, pages 895\u2013 902. Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Guil- laume Lample, and Chris Dyer. 2015. Evaluation of word vector representations by subspace alignment. In EMNLP. Lyan Verwimp, Joris Pelemans, Hugo Van hamme, and Patrick Wambacq. 2017. Character-word lstm lan- guage models. In EACL. Peter; Grnroos Stig-Arne; Kurimo Mikko Virpioja, Sami; Smit.",
            "2017. Character-word lstm lan- guage models. In EACL. Peter; Grnroos Stig-Arne; Kurimo Mikko Virpioja, Sami; Smit. 2013. Morfessor 2.0: Python im- plementation and extensions for morfessor base- line. D4 julkaistu kehittmis- tai tutkimusraportti tai -selvitys. Ivan V\u00b4ulic, Nikola Mrksic, Roi Reichart, Diarmuid \u00b4O S\u00b4eaghdha, Steve J. Young, and Anna Korhonen. 2017. Morph-\ufb01tting: Fine-tuning word vector spaces with simple language-speci\ufb01c rules. In Pro- ceedings of the 55th Annual Meeting of the Asso- ciation for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pages 56\u201368. Association for Compu- tational Linguistics.",
            "Association for Compu- tational Linguistics. John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2016. Charagram: Embedding words and sentences via character n-grams. In EMNLP. Gerhard Wohlgenannt, Nikolay Klimov, Dmitry Mouromtsev, Daniil Razdyakonov, Dmitry Pavlov, and Yury Emelyanov. 2017. Using word embed- dings for visual data exploration with ontodia and wikidata. In BLINK\/NLIWoD3@ISWC."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1805.03710.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 5904.000183105469,
    "avg_doclen_est": 173.64706420898438
}
