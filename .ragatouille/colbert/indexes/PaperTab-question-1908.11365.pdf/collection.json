[
  "Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention Biao Zhang1 Ivan Titov1,2 Rico Sennrich3,1 1School of Informatics, University of Edinburgh 2ILLC, University of Amsterdam 3Institute of Computational Linguistics, University of Zurich B.Zhang@ed.ac.uk, ititov@inf.ed.ac.uk, sennrich@cl.uzh.ch Abstract The general trend in NLP is towards in- creasing model capacity and performance via deeper neural networks. However, simply stacking more layers of the popular Trans- former architecture for machine translation re- sults in poor convergence and high computa- tional overhead. Our empirical analysis sug- gests that convergence is poor due to gradi- ent vanishing caused by the interaction be- tween residual connections and layer normal- ization. We propose depth-scaled initialization (DS-Init), which decreases parameter variance at the initialization stage, and reduces out- put variance of residual connections so as to ease gradient back-propagation through nor- malization layers.",
  "We propose depth-scaled initialization (DS-Init), which decreases parameter variance at the initialization stage, and reduces out- put variance of residual connections so as to ease gradient back-propagation through nor- malization layers. To address computational cost, we propose a merged attention sublayer (MAtt) which combines a simpli\ufb01ed average- based self-attention sublayer and the encoder- decoder attention sublayer on the decoder side. Results on WMT and IWSLT translation tasks with \ufb01ve translation directions show that deep Transformers with DS-Init and MAtt can sub- stantially outperform their base counterpart in terms of BLEU (+1.1 BLEU on average for 12-layer models), while matching the decod- ing speed of the baseline model thanks to the ef\ufb01ciency improvements of MAtt.1 1 Introduction The capability of deep neural models of handling complex dependencies has bene\ufb01ted various ar- ti\ufb01cial intelligence tasks, such as image recogni- tion where test error was reduced by scaling VGG nets (Simonyan and Zisserman, 2015) up to hun- dreds of convolutional layers (He et al., 2015).",
  "In NLP, deep self-attention networks have enabled large-scale pretrained language models such as BERT (Devlin et al., 2019) and GPT (Radford 1Source code for reproduction is available at https:// github.com/bzhangGo/zero 0 1 2 3 4 5 6 Gradient Norm (a) Encoder Transformer 6L Transformer 12L Transformer 18L Transformer+DS-Init 6L Transformer+DS-Init 12L Transformer+DS-Init 18L 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Layer Depth 0 1 2 3 4 5 6 Gradient Norm (b) Decoder Transformer 6L Transformer 12L Transformer 18L Transformer+DS-Init 6L Transformer+DS-Init 12L Transformer+DS-Init 18L Figure 1: Gradient norm (y-axis) of each encoder layer (top) and decoder layer (bottom) in Transformer with respect to layer depth (x-axis).",
  "Gradients are estimated with \u223c3k target tokens at the beginning of training. \u201cDS-Init\u201d: the proposed depth-scaled initialization. \u201c6L\u201d: 6 layers. Solid lines indi- cate the vanilla Transformer, and dashed lines denote our pro- posed method. During back-propagation, gradients in Trans- former gradually vanish from high layers to low layers. et al., 2018) to boost state-of-the-art (SOTA) per- formance on downstream applications. By con- trast, though neural machine translation (NMT) gained encouraging improvement when shifting from a shallow architecture (Bahdanau et al., 2015) to deeper ones (Zhou et al., 2016; Wu et al., 2016; Zhang et al., 2018; Chen et al., 2018), the Transformer (Vaswani et al., 2017), a cur- rently SOTA architecture, achieves best results with merely 6 encoder and decoder layers, and no gains were reported by Vaswani et al. (2017) from arXiv:1908.11365v1  [cs.CL]  29 Aug 2019",
  "further increasing its depth on standard datasets. We start by analysing why the Transformer does not scale well to larger model depth. We \ufb01nd that the architecture suffers from gradient vanishing as shown in Figure 1, leading to poor convergence. An in-depth analysis reveals that the Transformer is not norm-preserving due to the involvement of and the interaction between residual connection (RC) (He et al., 2015) and layer normalization (LN) (Ba et al., 2016). To address this issue, we propose depth-scaled initialization (DS-Init) to improve norm preserva- tion. We ascribe the gradient vanishing to the large output variance of RC and resort to strategies that could reduce it without model structure adjust- ment. Concretely, DS-Init scales down the vari- ance of parameters in the l-th layer with a discount factor of 1 \u221a l at the initialization stage alone, where l denotes the layer depth starting from 1. The in- tuition is that parameters with small variance in upper layers would narrow the output variance of corresponding RCs, improving norm preservation as shown by the dashed lines in Figure 1.",
  "The in- tuition is that parameters with small variance in upper layers would narrow the output variance of corresponding RCs, improving norm preservation as shown by the dashed lines in Figure 1. In this way, DS-Init enables the convergence of deep Transformer models to satisfactory local optima. Another bottleneck for deep Transformers is the increase in computational cost for both train- ing and decoding. To combat this, we pro- pose a merged attention network (MAtt). MAtt simpli\ufb01es the decoder by replacing the separate self-attention and encoder-decoder attention sub- layers with a new sublayer that combines an ef\ufb01cient variant of average-based self-attention (AAN) (Zhang et al., 2018) and the encoder- decoder attention. We simplify the AAN by re- ducing the number of linear transformations, re- ducing both the number of model parameters and computational cost. The merged sublayer bene\ufb01ts from parallel calculation of (average-based) self- attention and encoder-decoder attention, and re- duces the depth of each decoder block.",
  "The merged sublayer bene\ufb01ts from parallel calculation of (average-based) self- attention and encoder-decoder attention, and re- duces the depth of each decoder block. We conduct extensive experiments on WMT and IWSLT translation tasks, covering \ufb01ve transla- tion tasks with varying data conditions and trans- lation directions. Our results show that deep Transformers with DS-Init and MAtt can substan- tially outperform their base counterpart in terms of BLEU (+1.1 BLEU on average for 12-layer mod- els), while matching the decoding speed of the baseline model thanks to the ef\ufb01ciency improve- ments of MAtt. Our contributions are summarized as follows: \u2022 We analyze the vanishing gradient issue in the Transformer, and identify the interaction of residual connections and layer normaliza- tion as its source. \u2022 To address this problem, we introduce depth- scaled initialization (DS-Init). \u2022 To reduce the computational cost of training deep Transformers, we introduce a merged attention model (MAtt). MAtt combines a simpli\ufb01ed average-attention model and the encoder-decoder attention into a single sub- layer, allowing for parallel computation.",
  "\u2022 To reduce the computational cost of training deep Transformers, we introduce a merged attention model (MAtt). MAtt combines a simpli\ufb01ed average-attention model and the encoder-decoder attention into a single sub- layer, allowing for parallel computation. \u2022 We conduct extensive experiments and ver- ify that deep Transformers with DS-Init and MAtt improve translation quality while pre- serving decoding ef\ufb01ciency. 2 Related Work Our work aims at improving translation quality by increasing model depth. Compared with the single-layer NMT system (Bahdanau et al., 2015), deep NMT models are typically more capable of handling complex language variations and trans- lation relationships via stacking multiple encoder and decoder layers (Zhou et al., 2016; Wu et al., 2016; Britz et al., 2017; Chen et al., 2018), and/or multiple attention layers (Zhang et al., 2018). One common problem for the training of deep neural models are vanishing or exploding gradients.",
  "One common problem for the training of deep neural models are vanishing or exploding gradients. Ex- isting methods mainly focus on developing novel network architectures so as to stabilize gradient back-propagation, such as the fast-forward con- nection (Zhou et al., 2016), the linear associa- tive unit (Wang et al., 2017), or gated recurrent network variants (Hochreiter and Schmidhuber, 1997; Gers and Schmidhuber, 2001; Cho et al., 2014; Di Gangi and Federico, 2018). In contrast to the above recurrent network based NMT models, recent work focuses on feed-forward alternatives with more smooth gradient \ufb02ow, such as convo- lutional networks (Gehring et al., 2017) and self- attention networks (Vaswani et al., 2017). The Transformer represents the current SOTA in NMT. It heavily relies on the combination of residual connections (He et al., 2015) and layer normalization (Ba et al., 2016) for convergence.",
  "The Transformer represents the current SOTA in NMT. It heavily relies on the combination of residual connections (He et al., 2015) and layer normalization (Ba et al., 2016) for convergence. Nevertheless, simply extending this model with more layers results in gradient vanishing due to the interaction of RC and LN (see Section 4). Re- cent work has proposed methods to train deeper",
  "Transformer models, including a rescheduling of RC and LN (Vaswani et al., 2018), the transpar- ent attention model (Bapna et al., 2018) and the stochastic residual connection (Pham et al., 2019). In contrast to these work, we identify the large output variance of RC as the source of gradient vanishing, and employ scaled initialization to mit- igate it without any structure adjustment. The ef- fect of careful initialization on boosting conver- gence was also investigated and veri\ufb01ed in previ- ous work (Zhang et al., 2019; Child et al., 2019; Devlin et al., 2019; Radford et al., 2018). The merged attention network falls into the cat- egory of simplifying the Transformer so as to shorten training and/or decoding time.",
  "The merged attention network falls into the cat- egory of simplifying the Transformer so as to shorten training and/or decoding time. Methods to improve the Transformer\u2019s running ef\ufb01ciency range from algorithmic improvements (Junczys- Dowmunt et al., 2018), non-autoregressive trans- lation (Gu et al., 2018; Ghazvininejad et al., 2019) to decoding dependency reduction such as aver- age attention network (Zhang et al., 2018) and blockwise parallel decoding (Stern et al., 2018). Our MAtt builds upon the AAN model, further simplifying the model by reducing the number of linear transformations, and combining it with the encoder-decoder attention. In work concur- rent to ours, So et al. (2019) propose the evolved Transformer which, based on automatic architec- ture search, also discovered a parallel structure of self-attention and encoder-decoder attention. 3 Background: Transformer Given a source sequence X = {x1, x2, . . .",
  "(2019) propose the evolved Transformer which, based on automatic architec- ture search, also discovered a parallel structure of self-attention and encoder-decoder attention. 3 Background: Transformer Given a source sequence X = {x1, x2, . . . , xn} \u2208 Rn\u00d7d, the Transformer predicts a target sequence Y = {y1, y2, . . . , ym} under the encoder-decoder framework. Both the encoder and the decoder in the Transformer are composed of attention net- works, functioning as follows: ATT(Zx, Zy) = \u0014 softmax(QKT \u221a d )V \u0015 Wo Q, K, V = ZxWq, ZyWk, ZyWv, (1) where Zx \u2208RI\u00d7d and Zy \u2208RJ\u00d7d are input se- quence representations of length I and J respec- tively, W\u2217\u2208Rd\u00d7d denote weight parameters. The attention network can be further enhanced with multi-head attention (Vaswani et al., 2017).",
  "The attention network can be further enhanced with multi-head attention (Vaswani et al., 2017). Formally, the encoder stacks L identical lay- ers, each including a self-attention sublayer (Eq. 2) and a point-wise feed-forward sublayer (Eq. 3): \u00afHl = LN \u0010 RC \u0010 Hl\u22121, ATT(Hl\u22121, Hl\u22121) \u0011\u0011 (2) Hl = LN \u0010 RC \u0010 \u00afHl, FFN( \u00afHl) \u0011\u0011 . (3) Hl \u2208Rn\u00d7d denotes the sequence representation of the l-th encoder layer. Input to the \ufb01rst layer H0 is the element-wise addition of the source word embedding X and the corresponding positional encoding. FFN(\u00b7) is a two-layer feed-forward net- work with a large intermediate representation and ReLU activation function. Each encoder sublayer is wrapped with a residual connection (Eq. 4), fol- lowed by layer normalization (Eq.",
  "FFN(\u00b7) is a two-layer feed-forward net- work with a large intermediate representation and ReLU activation function. Each encoder sublayer is wrapped with a residual connection (Eq. 4), fol- lowed by layer normalization (Eq. 5): RC(z, z\u2032) = z + z\u2032, (4) LN(z) = z \u2212\u00b5 \u03c3 \u2299g + b, (5) where z and z\u2032 are input vectors, and \u2299indicates element-wise multiplication. \u00b5 and \u03c3 denote the mean and standard deviation statistics of vector z. The normalized z is then re-scaled and re-centered by trainable parameters g and b individually. The decoder also consists of L identical layers, each of them extends the encoder sublayers with an encoder-decoder attention sublayer (Eq.",
  "\u00b5 and \u03c3 denote the mean and standard deviation statistics of vector z. The normalized z is then re-scaled and re-centered by trainable parameters g and b individually. The decoder also consists of L identical layers, each of them extends the encoder sublayers with an encoder-decoder attention sublayer (Eq. 7) to capture translation alignment from target words to relevant source words: \u02dcSl = LN \u0010 RC \u0010 Sl\u22121, ATT(Sl\u22121, Sl\u22121) \u0011\u0011 (6) \u00afSl = LN \u0010 RC \u0010 \u02dcSl, ATT(\u02dcSl, HL) \u0011\u0011 (7) Sl = LN \u0010 RC \u0010 \u00afSl, FFN(\u00afSl) \u0011\u0011 . (8) Sl \u2208Rm\u00d7d is the sequence representation of the l- th decoder layer. Input S0 is de\ufb01ned similar to H0. To ensure auto-regressive decoding, the attention weights in Eq. 6 are masked to prevent attention to future target tokens.",
  "(8) Sl \u2208Rm\u00d7d is the sequence representation of the l- th decoder layer. Input S0 is de\ufb01ned similar to H0. To ensure auto-regressive decoding, the attention weights in Eq. 6 are masked to prevent attention to future target tokens. The Transformer\u2019s parameters are typically ini- tialized by sampling from a uniform distribution: W \u2208Rdi\u00d7do \u223cU (\u2212\u03b3, \u03b3) , \u03b3 = r 6 di + do , (9) where di and do indicate input and output dimen- sion separately. This initialization has the advan- tage of maintaining activation variances and back- propagated gradients variance and can help train deep neural networks (Glorot and Bengio, 2010).",
  "4 Vanishing Gradient Analysis One natural way to deepen Transformer is simply enlarging the layer number L. Unfortunately, Fig- ure 1 shows that this would give rise to gradient vanishing on both the encoder and the decoder at the lower layers, and that the case on the decoder side is worse. We identi\ufb01ed a structural problem in the Transformer architecture that gives rise to this issue, namely the interaction of RC and LN, which we will here discuss in more detail. Given an input vector z \u2208Rd, let us consider the general structure of RC followed by LN: r = RC (z, f(z)) , (10) o = LN (r) , (11) where r, o \u2208Rd are intermediate outputs. f(\u00b7) represents any neural network, such as recurrent, convolutional or attention network, etc. Suppose during back-propagation, the error signal at the output of LN is \u03b4o.",
  "f(\u00b7) represents any neural network, such as recurrent, convolutional or attention network, etc. Suppose during back-propagation, the error signal at the output of LN is \u03b4o. Contributions of RC and LN to the error signal are as follows: \u03b4r = \u2202o \u2202r \u03b4o = diag( g \u03c3r )(I \u22121 \u2212\u00afr\u00afrT d )\u03b4o (12) \u03b4z = \u2202r \u2202z\u03b4r = (1 + \u2202f \u2202z )\u03b4r, (13) where \u00afr denotes the normalized input. I is the identity matrix and diag(\u00b7) establishes a diagonal matrix from its input. The resulting \u03b4r and \u03b4z are error signals arrived at output r and z respectively.",
  "I is the identity matrix and diag(\u00b7) establishes a diagonal matrix from its input. The resulting \u03b4r and \u03b4z are error signals arrived at output r and z respectively. We de\ufb01ne the change of error signal as follows: \u03b2 = \u03b2LN \u00b7 \u03b2RC = \u2225\u03b4z\u22252 \u2225\u03b4r\u22252 \u00b7 \u2225\u03b4r\u22252 \u2225\u03b4o\u22252 , (14) where \u03b2 (or model ratio), \u03b2LN (or LN ratio) and \u03b2RC (or RC ratio) measure the gradient norm ratio2 of the whole residual block, the layer normaliza- tion and the residual connection respectively. In- formally, a neural model should preserve the gra- dient norm between layers (\u03b2 \u22481) so as to al- low training of very deep models (see Zaeemzadeh et al., 2018). We resort to empirical evidence to analyze these ratios. Results in Table 1 show that LN weak- ens error signal (\u03b2LN < 1) but RC strengthens it (\u03b2RC > 1).",
  "We resort to empirical evidence to analyze these ratios. Results in Table 1 show that LN weak- ens error signal (\u03b2LN < 1) but RC strengthens it (\u03b2RC > 1). One explanation about LN\u2019s decay ef- fect is the large output variance of RC (Var(r) > 2Model gradients depend on both error signal and layer activation. Reduced/enhanced error signal does not necessar- ily result in gradient vanishing/explosion, but strongly con- tributes to it.",
  "Reduced/enhanced error signal does not necessar- ily result in gradient vanishing/explosion, but strongly con- tributes to it. Method Module Self Cross FFN Base Enc \u03b2LN 0.86 - 0.84 \u03b2RC 1.22 - 1.10 \u03b2 1.05 - 0.93 Var(r) 1.38 - 1.40 Dec \u03b2LN 0.82 0.74 0.84 \u03b2RC 1.21 1.00 1.11 \u03b2 0.98 0.74 0.93 Var(r) 1.48 1.84 1.39 Ours Enc \u03b2LN 0.96 - 0.95 \u03b2RC 1.04 - 1.02 \u03b2 1.02 - 0.98 Var(r) 1.10 - 1.10 Dec \u03b2LN 0.95 0.94 0.94 \u03b2RC 1.05 1.00 1.02 \u03b2 1.10 0.95 0.98 Var(r) 1.13 1.15 1.11 Table 1: Empirical measure of output variance Var(r) of RC and error signal change ratio \u03b2LN, \u03b2RC and \u03b2 (Eq.",
  "14) averaged over 12 layers. These values are estimated with \u223c3k target tokens at the beginning of training using 12-layer Transformer. \u201cBase\u201d: the baseline Transformer. \u201cOurs\u201d: the Transformer with DS-Init. Enc and Dec stand for en- coder and decoder respectively. Self, Cross and FFN in- dicate the self-attention, encoder-decoder attention and the feed-forward sublayer respectively. 1) which negatively affects \u03b4r as shown in Eq. 12. By contrast, the short-cut in RC ensures that the error signal at higher layer \u03b4r can always be safely carried on to lower layer no matter how complex \u2202f \u2202z would be as in Eq. 13, increasing the ratio. 5 Depth-Scaled Initialization Results on the model ratio show that self-attention sublayer has a (near) increasing effect (\u03b2 > 1) that intensi\ufb01es error signal, while feed-forward sublayer manifests a decreasing effect (\u03b2 < 1). In particular, though the encoder-decoder atten- tion sublayer and the self-attention sublayer share the same attention formulation, the model ratio of the former is smaller.",
  "In particular, though the encoder-decoder atten- tion sublayer and the self-attention sublayer share the same attention formulation, the model ratio of the former is smaller. As shown in Eq. 7 and 1, part of the reason is that encoder-decoder attention can only back-propagate gradients to lower lay- ers through the query representation Q, bypassing gradients at the key K and the value V to the en- coder side. This negative effect explains why the decoder suffers from more severe gradient vanish- ing than the encoder in Figure 1. The gradient norm is preserved better through the self-attention layer than the encoder-decoder attention, which offers insights on the successful training of the deep Transformer in BERT (De- vlin et al., 2019) and GPT (Radford et al., 2018), where encoder-decoder attention is not involved. However, results in Table 1 also suggests that the self-attention sublayer in the encoder is not strong",
  "Linear Linear Linear Linear Scale & Mask MatMul SoftMax MatMul target Q K V (a) Self-Attention MatMul Linear target Avg Mask Linear Relu Gate Layer (b) AAN Linear Linear Linear Scale & Mask SoftMax MatMul target Q K V source Avg Mask MatMul MatMul V Linear Linear (c) Merged attention with simpli\ufb01ed AAN Figure 2: An overview of self-attention, AAN and the proposed merged attention with simpli\ufb01ed AAN. enough to counteract the gradient loss in the feed- forward sublayer. That is why BERT and GPT adopt a much smaller standard deviation (0.02) for initialization, in a similar spirit to our solution. We attribute the gradient vanishing issue to the large output variance of RC (Eq. 12). Consid- ering that activation variance is positively corre- lated with parameter variance (Glorot and Bengio, 2010), we propose DS-Init and change the original initialization method in Eq.",
  "12). Consid- ering that activation variance is positively corre- lated with parameter variance (Glorot and Bengio, 2010), we propose DS-Init and change the original initialization method in Eq. 9 as follows: W \u2208Rdi\u00d7do \u223cU \u0012 \u2212\u03b3 \u03b1 \u221a l , \u03b3 \u03b1 \u221a l \u0013 , (15) where \u03b1 is a hyperparameter in the range of [0, 1] and l denotes layer depth. Hyperparameter \u03b1 im- proves the \ufb02exibility of our method. Compared with existing approaches (Vaswani et al., 2018; Bapna et al., 2018), our solution does not require modi\ufb01cations in the model architecture and hence is easy to implement. According to the property of uniform distribu- tion, the variance of model parameters decreases from \u03b32 3 to \u03b32\u03b12 3l after applying DS-Init. By doing so, a higher layer would have smaller output vari- ance of RC so that more gradients can \ufb02ow back.",
  "By doing so, a higher layer would have smaller output vari- ance of RC so that more gradients can \ufb02ow back. Results in Table 1 suggest that DS-Init narrows both the variance and different ratios to be \u223c1, en- suring the stability of gradient back-propagation. Evidence in Figure 1 also shows that DS-Init helps keep the gradient norm and slightly increases it on the encoder side. This is because DS-Init endows lower layers with parameters of larger variance and activations of larger norm. When error signals at different layers are of similar scale, the gradi- ent norm at lower layers would be larger. Never- theless, this increase does not hurt model training based on our empirical observation. DS-Init is partially inspired by the Fixup ini- tialization (Zhang et al., 2019). Both of them try to reduce the output variance of RC. The differ- ence is that Fixup focuses on overcoming gradi- ent explosion cased by consecutive RCs and seeks to enable training without LN but at the cost of carefully handling parameter initialization of each matrix transformation, including manipulating ini- tialization of different bias and scale terms.",
  "The differ- ence is that Fixup focuses on overcoming gradi- ent explosion cased by consecutive RCs and seeks to enable training without LN but at the cost of carefully handling parameter initialization of each matrix transformation, including manipulating ini- tialization of different bias and scale terms. In- stead, DS-Init aims at solving gradient vanishing in deep Transformer caused by the structure of RC followed by LN. We still employ LN to standard- ize layer activation and improve model conver- gence. The inclusion of LN ensures the stability and simplicity of DS-Init. 6 Merged Attention Model With large model depth, deep Transformer un- avoidably introduces high computational over- head. This brings about signi\ufb01cantly longer train- ing and decoding time. To remedy this issue, we propose a merged attention model for decoder that integrates a simpli\ufb01ed average-based self- attention sublayer into the encoder-decoder atten- tion sublayer. Figure 2 highlights the difference.",
  "To remedy this issue, we propose a merged attention model for decoder that integrates a simpli\ufb01ed average-based self- attention sublayer into the encoder-decoder atten- tion sublayer. Figure 2 highlights the difference. The AAN model (Figure 2(b)), as an alternative to the self-attention model (Figure 2(a)), acceler- ates Transformer decoding by allowing decoding in linear time, avoiding the O(n2) complexity of the self-attention mechanism (Zhang et al., 2018). Unfortunately, the gating sublayer and the feed- forward sublayer inside AAN reduce the empiri- cal performance improvement. We propose a sim- pli\ufb01ed AAN by removing all matrix computation except for two linear projections: SAAN(Sl\u22121) = h Ma(Sl\u22121Wv) i Wo, (16)",
  "Dataset #Src #Tgt #Sent #BPE WMT14 En-De 116M 110M 4.5M 32K WMT14 En-Fr 1045M 1189M 36M 32K WMT18 En-Fi 73M 54M 3.3M 32K WMT18 Zh-En 510M 576M 25M 32K IWSLT14 De-En 3.0M 3.2M 159K 30K Table 2: Statistics for different training datasets. #Src and #Tgt denote the number of source and target tokens respec- tively. #Sent: the number of bilingual sentences. #BPE: the number of merge operations in BPE. M: million, K: thousand. where Ma denotes the average mask matrix for parallel computation (Zhang et al., 2018).",
  "#Sent: the number of bilingual sentences. #BPE: the number of merge operations in BPE. M: million, K: thousand. where Ma denotes the average mask matrix for parallel computation (Zhang et al., 2018). This new model is then combined with the encoder- decoder attention as shown in Figure 2(c): MATT(Sl\u22121) = SAAN(Sl\u22121) + ATT(Sl\u22121, HL) \u00afSl = LN \u0010 RC \u0010 Sl\u22121, MATT(Sl\u22121) \u0011\u0011 . (17) The mapping Wo is shared for SAAN and ATT. After combination, MAtt allows for the paral- lelization of AAN and encoder-decoder attention.",
  "(17) The mapping Wo is shared for SAAN and ATT. After combination, MAtt allows for the paral- lelization of AAN and encoder-decoder attention. 7 Experiments 7.1 Datasets and Evaluation We take WMT14 English-German translation (En-De) (Bojar et al., 2014) as our bench- mark for model analysis, and examine the generalization of our approach on four other tasks: WMT14 English-French (En-Fr), IWSLT14 German-English (De-En) (Cettolo et al., 2014), WMT18 English-Finnish (En-Fi) and WMT18 Chinese-English (Zh-En) (Bojar et al., 2018). Byte pair encoding algorithm (BPE) (Sennrich et al., 2016) is used in preprocessing to handle low frequency words. Statistics of different datasets are listed in Table 2. Except for IWSLT14 De-En task, we collect subword units independently on the source and target side of training data.",
  "Statistics of different datasets are listed in Table 2. Except for IWSLT14 De-En task, we collect subword units independently on the source and target side of training data. We directly use the preprocessed training data from the WMT18 web- site3 for En-Fi and Zh-En tasks, and use new- stest2017 as our development set, newstest2018 as our test set. Our training data for WMT14 En- De and WMT14 En-Fr is identical to previous se- tups (Vaswani et al., 2017; Wu et al., 2019). We use newstest2013 as development set for WMT14 En-De and newstest2012+2013 for WMT14 En- Fr. Apart from newstest2014 test set4, we also 3http://www.statmt.org/wmt18/translation-task.html 4We use the \ufb01ltered test set consisting of 2737 sentence pairs. The difference of translation quality on \ufb01ltered and full test sets is marginal.",
  "The difference of translation quality on \ufb01ltered and full test sets is marginal. evaluate our model on all WMT14-18 test sets for WMT14 En-De translation. The settings for IWSLT14 De-En are as in Ranzato et al. (2016), with 7584 sentence pairs for development, and the concatenated dev sets for IWSLT 2014 as test set (tst2010, tst2011, tst2012, dev2010, dev2012). We report tokenized case-sensitive BLEU (Pa- pineni et al., 2002) for WMT14 En-De and WMT14 En-Fr, and provide detokenized case- sensitive BLEU for WMT14 En-De, WMT18 En- Fi and Zh-En with sacreBLEU (Post, 2018)5. We also report chrF score for En-Fi translation which was found correlated better with human evalu- ation (Bojar et al., 2018). Following previous work (Wu et al., 2019), we evaluate IWSLT14 De- En with tokenized case-insensitive BLEU.",
  "We also report chrF score for En-Fi translation which was found correlated better with human evalu- ation (Bojar et al., 2018). Following previous work (Wu et al., 2019), we evaluate IWSLT14 De- En with tokenized case-insensitive BLEU. 7.2 Model Settings We experiment with both base (layer size 512/2048, 8 heads) and big (layer size 1024/4096, 16 heads) settings as in Vaswani et al. (2017). Ex- cept for the vanilla Transformer, we also compare with the structure that is currently default in ten- sor2tensor (T2T), which puts layer normalization before residual blocks (Vaswani et al., 2018). We use an in-house toolkit for all experiments. Dropout is applied to the residual connection (dpr) and attention weights (dpa). We share the target embedding matrix with the softmax projec- tion matrix but not with the source embedding ma- trix.",
  "We use an in-house toolkit for all experiments. Dropout is applied to the residual connection (dpr) and attention weights (dpa). We share the target embedding matrix with the softmax projec- tion matrix but not with the source embedding ma- trix. We train all models using Adam optimizer (0.9/0.98 for base, 0.9/0.998 for big) with adap- tive learning rate schedule (warm-up step 4K for base, 16K for big) as in (Vaswani et al., 2017) and label smoothing of 0.1. We set \u03b1 in DS-Init to 1.0. Sentence pairs containing around 25K\u223c50K (bs) target tokens are grouped into one batch. We use relatively larger batch size and dropout rate for deeper and bigger models for better conver- gence. We perform evaluation by averaging last 5 checkpoints. Besides, we apply mixed-precision training to all big models.",
  "We use relatively larger batch size and dropout rate for deeper and bigger models for better conver- gence. We perform evaluation by averaging last 5 checkpoints. Besides, we apply mixed-precision training to all big models. Unless otherwise stated, we train base and big model with 300K maximum steps, and decode sentences using beam search with a beam size of 4 and length penalty of 0.6. Decoding is implemented with cache to save re- dundant computations. Other settings for speci\ufb01c translation tasks are explained in the individual subsections. 5Signature BLEU+c.mixed+#.1+s.exp+tok.13a+v.1.2.20",
  "ID Model #Param Test14 \u25b3Dec \u25b3Train 1 Base 6 layers dpa = dpr = 0.1, bs = 25K 72.3M 27.59 (26.9) 62.26/1.00\u00d7 0.105/1.00\u00d7 2 1 + T2T 72.3M 27.20 (26.5) 68.04/0.92\u00d7 0.105/1.00\u00d7 3 1 + DS-Init 72.3M 27.50 (26.8) \u22c6/1.00\u00d7 \u22c6/1.00\u00d7 4 1 + MAtt 66.0M 27.49 (26.8) 40.51/1.54\u00d7 0.094/1.12\u00d7 5 1 + MAtt + DS-Init 66.0M 27.35 (26.8) 40.84/1.52\u00d7 0.094/1.12\u00d7 6 1 + MAtt with self-attention 72.3M 27.41 (26.7) 60.",
  "0M 27.35 (26.8) 40.84/1.52\u00d7 0.094/1.12\u00d7 6 1 + MAtt with self-attention 72.3M 27.41 (26.7) 60.25/1.03\u00d7 0.105/1.00\u00d7 7 1 + MAtt with original AAN 72.2M 27.36 (26.7) 46.13/1.35\u00d7 0.098/1.07\u00d7 8 1 + bs = 50K 72.3M 27.84 (27.2) \u22c6/1.00\u00d7 \u22c6/1.00\u00d7 9 1 + > 12 layers + bs = 25K \u223c50K - - - - 10 4 + > 12 layers + bs = 25K \u223c50K - - - 11 3 + 12 layers + bs = 40K, dpr = 0.3, dpa = 0.2 116.4M 28.27 (27.6) 102.9/1.",
  "dpr = 0.3, dpa = 0.2 116.4M 28.27 (27.6) 102.9/1.00\u00d7\u2020 0.188/1.00\u00d7\u2020 12 11 + T2T 116.5M 28.03 (27.4) 107.7/0.96\u00d7\u2020 0.191/0.98\u00d7\u2020 13 11 + MAtt 103.8M 28.55 (27.9) 67.12/1.53\u00d7\u2020 0.164 /1.15\u00d7\u2020 14 3 + 20 layers + bs = 44K, dpr = 0.3, dpa = 0.2 175.3M 28.42 (27.7) 157.8/1.00\u00d7\u2021 0.283/1.00\u00d7\u2021 15 14 + T2T 175.3M 28.27 (27.6) 161.2/0.98\u00d7\u2021 0.289/0.98\u00d7\u2021 16 14 + MAtt 154.",
  "283/1.00\u00d7\u2021 15 14 + T2T 175.3M 28.27 (27.6) 161.2/0.98\u00d7\u2021 0.289/0.98\u00d7\u2021 16 14 + MAtt 154.3M 28.67 (28.0) 108.6/1.45\u00d7\u2021 0.251/1.13\u00d7\u2021 Table 3: Tokenized case-sensitive BLEU (in parentheses: sacreBLEU) on WMT14 En-De translation task. #Param: number of model parameters. \u25b3Dec: decoding time (seconds)/speedup on newstest2014 dataset with a batch size of 32. \u25b3Train: training time (seconds)/speedup per training step evaluated on 0.5K steps with a batch size of 1K target tokens. Time is averaged over 3 runs using Tensor\ufb02ow on a single TITAN X (Pascal). \u201c-\u201d: optimization failed and no result. \u201c\u22c6\u201d: the same as model 1\u20dd.",
  "Time is averaged over 3 runs using Tensor\ufb02ow on a single TITAN X (Pascal). \u201c-\u201d: optimization failed and no result. \u201c\u22c6\u201d: the same as model 1\u20dd. \u2020 and \u2021: comparison against 11 \u20ddand 14 \u20ddrespectively rather than 1\u20dd. Base: the baseline Transformer with base setting. Bold indicates best BLEU score. dpa and dpr: dropout rate on attention weights and residual connection. bs: batch size in tokens. 7.3 WMT14 En-De Translation Task Table 3 summarizes translation results under dif- ferent settings. Applying DS-Init and/or MAtt to Transformer with 6 layers slightly decreases translation quality by \u223c0.2 BLEU (27.59\u219227.35). However, they allow scaling up to deeper architec- tures, achieving a BLEU score of 28.55 (12 layers) and 28.67 (20 layers), outperforming all baselines.",
  "However, they allow scaling up to deeper architec- tures, achieving a BLEU score of 28.55 (12 layers) and 28.67 (20 layers), outperforming all baselines. These improvements can not be obtained via en- larging the training batch size ( 8\u20dd), con\ufb01rming the strength of deep models. We also compare our simpli\ufb01ed AAN in MAtt ( 4\u20dd) with two variants: a self-attention network ( 6\u20dd), and the original AAN ( 7\u20dd). Results show minor differences in translation quality, but im- provements in training and decoding speed, and a reduction in the number of model parameters. Compared to the baseline, MAtt improves decod- ing speed by 50%, and training speed by 10%, while having 9% fewer parameters. Result 9\u20ddindicates that the gradient vanishing issue prevents training of deep vanilla Transform- ers, which cannot be solved by only simplifying the decoder via MAtt (10 \u20dd). By contrast, both T2T and DS-Init can help.",
  "Result 9\u20ddindicates that the gradient vanishing issue prevents training of deep vanilla Transform- ers, which cannot be solved by only simplifying the decoder via MAtt (10 \u20dd). By contrast, both T2T and DS-Init can help. Our DS-Init improves norm preservation through speci\ufb01c parameter initializa- tion, while T2T reschedules the LN position.",
  "By contrast, both T2T and DS-Init can help. Our DS-Init improves norm preservation through speci\ufb01c parameter initializa- tion, while T2T reschedules the LN position. Re- sults in Table 3 show that T2T underperforms DS- Init by 0.2 BLEU on average, and slightly in- creases training and decoding time (by 2%) com- pared to the original Transformer due to additional ID BLEU PPL Train Dev Train Dev 1 28.64 26.16 5.23 4.76 11 29.63 26.44 4.48 4.38 12 29.75 26.16 4.60 4.49 13 29.43 26.51 5.09 4.71 14 30.71 26.52 3.96 4.32 15 30.89 26.53 4.09 4.41 16 30.25 26.56 4.62 4.58 Table 4: Tokenized case-sensitive BLEU (BLEU) and per- plexity (PPL) on training (Train) and development (new- stest2013, Dev) set.",
  "We randomly select 3K sentence pairs as our training data for evaluation. Lower PPL is better. LN layers. This suggests that our solution is more effective and ef\ufb01cient. Surprisingly, training deep Transformers with both DS-Init and MAtt improves not only run- ning ef\ufb01ciency but also translation quality (by 0.2 BLEU), compared with DS-Init alone. To get an improved understanding, we analyze model per- formance on both training and development set. Results in Table 4 show that models with DS-Init yield the best perplexity on both training and de- velopment set, and those with T2T achieve the best BLEU on the training set. However, DS- Init+MAtt performs best in terms of BLEU on the development set. This indicates that the success of DS-Init+MAtt comes from its better generaliza- tion rather than better \ufb01tting training data. We also attempt to apply DS-Init on the encoder alone or the decoder alone for 12-layer models. Unfortunately, both variants lead to unstable op- timization where gradients tend to explode at the",
  "Task Model #Param BLEU \u25b3Dec \u25b3Train WMT14 En-Fr Base + 6 layers 76M 39.09 167.56/1.00\u00d7 0.171/1.00\u00d7 Ours + Base + 12 layers 108M 40.58 173.62/0.97\u00d7 0.265/0.65\u00d7 IWSLT14 De-En Base + 6 layers 61M 34.41 315.59/1.00\u00d7 0.153/1.00\u00d7 Ours + Base + 12 layers 92M 35.63 329.95/0.96\u00d7 0.247/0.62\u00d7 WMT18 En-Fi Base + 6 layers 65M 15.5 (50.82) 156.32/1.00\u00d7 0.165/1.00\u00d7 Ours + Base + 12 layers 96M 15.8 (51.47) 161.74/0.97\u00d7 0.259/0.64\u00d7 WMT18 Zh-En Base + 6 layers 77M 21.1 217.40/1.",
  "8 (51.47) 161.74/0.97\u00d7 0.259/0.64\u00d7 WMT18 Zh-En Base + 6 layers 77M 21.1 217.40/1.00\u00d7 0.173/1.00\u00d7 Ours + Base + 12 layers 108M 22.3 228.57/0.95\u00d7 0.267/0.65\u00d7 Table 5: Translation results on different tasks. Settings for BLEU score is given in Section 7.1. Numbers in bracket denote chrF score. Our model outperforms the vanilla base Transformer on all tasks. \u201cOurs\u201d: DS-Init+MAtt. 10 15 20 25 30 Model Depth 27.50 27.75 28.00 28.25 28.50 28.75 BLEU Score Figure 3: Test BLEU score on newstest2014 with respect to model depth for Transformer+DS-Init+MAtt. Model #Param Test14 Test14-18 Vaswani et al. (2017) 213M 28.4 - Chen et al.",
  "Model #Param Test14 Test14-18 Vaswani et al. (2017) 213M 28.4 - Chen et al. (2018) 379M 28.9 - Ott et al. (2018) 210M 29.3 - Bapna et al. (2018) 137M 28.04 - Wu et al. (2019) 213M 29.76 (29.0) \u2217 33.13 (32.86) \u2217 Big + 6 layers 233M 29.07 (28.3) 33.16 (32.88) Ours + Big + 12 layers 359M 29.47 (28.7) 33.21 (32.90) Ours + Big + 20 layers 560M 29.62 (29.0) 33.26 (32.96) Table 6: Tokenized case-sensitive BLEU (sacreBLEU) on WMT14 En-De translation task. \u201cTest14-18\u201d: BLEU score averaged over newstest2014\u223cnewstest2018.",
  "\u201cTest14-18\u201d: BLEU score averaged over newstest2014\u223cnewstest2018. \u2217: results ob- tained by running code and model released by Wu et al. (2019). \u201cOurs\u201d: DS-Init+MAtt. middle of training. We attempt to solve this issue with gradient clipping of rate 1.0. Results show that this fails for decoder and achieves only 27.89 BLEU for encoder, losing 0.66 BLEU compared with the full variant (28.55). We leave further analysis to future work and recommend using DS- Init on both the encoder and the decoder. Effect of Model Depth We empirically com- pare a wider range of model depths for Transformer+DS-Init+MAtt with up to 30 layers. Hyperparameters are the same as for 14 \u20ddexcept that we use 42K and 48K batch size for 18 and 30 lay- ers respectively. Figure 3 shows that deeper Trans- formers yield better performance. However, im- provements are steepest going from 6 to 12 layers, and further improvements are small.",
  "Figure 3 shows that deeper Trans- formers yield better performance. However, im- provements are steepest going from 6 to 12 layers, and further improvements are small. 7.3.1 Comparison with Existing Work Table 6 lists the results in big setting and compares with current SOTA. Big models are trained with dpa = 0.1 and dpr = 0.3. The 6-layer baseline and the deeper ones are trained with batch size of 48K and 54K respectively. Deep Transformer with our method outperforms its 6-layer counterpart by over 0.4 points on newstest2014 and around 0.1 point on newstest2014\u223cnewstest2018. Our model outperforms the transparent model (Bapna et al., 2018) (+1.58 BLEU), an approach for the deep encoder. Our model performs on par with current SOTA, the dynamic convolution model (DCNN) (Wu et al., 2019). In particular, though DCNN achieves encouraging performance on newstest2014, it falls behind the baseline on other test sets.",
  "Our model performs on par with current SOTA, the dynamic convolution model (DCNN) (Wu et al., 2019). In particular, though DCNN achieves encouraging performance on newstest2014, it falls behind the baseline on other test sets. By contrast, our model obtains more consistent performance improvements. In work concurrent to ours, Wang et al. (2019) discuss how the placement of layer normalization affects deep Transformers, and compare the orig- inal post-norm (which we consider our baseline) and a pre-norm layout (which we call T2T). Their results also show that pre-norm allows training of deeper Transformers. Our results show that deep post-norm Transformers are also trainable with ap- propriate initialization, and tend to give slightly better results. 7.4 Results on Other Translation Tasks We use 12 layers for our model in these tasks. We enlarge the dropout rate to dpa = 0.3, dpr = 0.5 for IWSLT14 De-En task and train models on WMT14 En-Fr and WMT18 Zh-En with 500K steps.",
  "We enlarge the dropout rate to dpa = 0.3, dpr = 0.5 for IWSLT14 De-En task and train models on WMT14 En-Fr and WMT18 Zh-En with 500K steps. Other models are trained with the same set- tings as in WMT14 En-De. We report translation results on other tasks in Table 5. Results show that our model beats the baseline on all tasks with gains of over 1 BLEU,",
  "0.0 0.2 0.4 0.6 0.8 1.0 1.2 Gradient Norm (a) Encoder Transformer L1 Transformer L18 Transformer + DS-Init L1 Transformer + DS-Init L18 0 10 20 30 40 50 60 70 80 90 Training Steps (x50) 0.0 0.2 0.4 0.6 0.8 1.0 1.2 Gradient Norm (b) Decoder Transformer L1 Transformer L18 Transformer + DS-Init L1 Transformer + DS-Init L18 Figure 4: Gradient norm (y-axis) of the \ufb01rst and the last encoder layers (top) and decoder layers (bottom) in 18-layer deep Transformer over the \ufb01st 5k training steps. We use around 25k source/target tokens in each training batch. Each point in this plot is averaged over 50 training steps. \u201cL1/L18\u201d denotes the \ufb01rst/last layer. DS-Init helps stabilize the gradient norm during training.",
  "We use around 25k source/target tokens in each training batch. Each point in this plot is averaged over 50 training steps. \u201cL1/L18\u201d denotes the \ufb01rst/last layer. DS-Init helps stabilize the gradient norm during training. except the WMT18 En-Fi where our model yields marginal BLEU improvements (+0.3 BLEU). We argue that this is due to the rich morphology of Finnish, and BLEU\u2019s inability to measure im- provements below the word level. We also provide the chrF score in which our model gains 0.6 points. In addition, speed measures show that though our model consumes 50+% more training time, there is only a small difference with respect to decoding time thanks to MAtt. 7.5 Analysis of Training Dynamics Our analysis in Figure 1 and Table 1 is based on gradients estimated exactly after parameter ini- tialization without considering training dynam- ics. Optimizers with adaptive step rules, such as Adam, could have an adverse effect that enables gradient scale correction through the accumulated \ufb01rst and second moments.",
  "Optimizers with adaptive step rules, such as Adam, could have an adverse effect that enables gradient scale correction through the accumulated \ufb01rst and second moments. However, results in Fig- ure 4 show that without DS-Init, the encoder gra- dients are less stable and the decoder gradients still suffer from the vanishing issue, particularly at the \ufb01rst layer. DS-Init makes the training more stable and robust.6 8 Conclusion and Future Work This paper discusses training of very deep Trans- formers. We show that the training of deep Trans- formers suffers from gradient vanishing, which we mitigate with depth-scaled initialization. To im- prove training and decoding ef\ufb01ciency, we pro- pose a merged attention sublayer that integrates a simpli\ufb01ed average-based self-attention sublayer into the encoder-decoder attention sublayer. Ex- perimental results show that deep models trained with these techniques clearly outperform a vanilla Transformer with 6 layers in terms of BLEU, and outperforms other solutions to train deep Trans- formers (Bapna et al., 2018; Vaswani et al., 2018).",
  "Thanks to the more ef\ufb01cient merged attention sublayer, we achieve these quality improvements while matching the decoding speed of the baseline model. In the future, we would like to extend our model to other sequence-to-sequence tasks, such as summarization and dialogue generation, as well as adapt the idea to other generative architec- tures (Zhang et al., 2016, 2018). We have trained models with up to 30 layers each for the encoder and decoder, and while training was successful and improved over shallower counterparts, gains are relatively small beyond 12 layers. An open question is whether there are other structural is- sues that limit the bene\ufb01ts of increasing the depth of the Transformer architecture, or whether the bene\ufb01t of very deep models is greater for other tasks and dataset. Acknowledgments We thank the reviewers for their insightful com- ments. This project has received funding from the grant H2020-ICT-2018-2-825460 (ELITR) by the European Union. Biao Zhang also acknowledges the support of the Baidu Scholarship.",
  "Acknowledgments We thank the reviewers for their insightful com- ments. This project has received funding from the grant H2020-ICT-2018-2-825460 (ELITR) by the European Union. Biao Zhang also acknowledges the support of the Baidu Scholarship. This work has been performed using resources provided by the Cambridge Tier-2 system operated by the Uni- versity of Cambridge Research Computing Ser- vice (http://www.hpc.cam.ac.uk) funded by EP- SRC Tier-2 capital grant EP/P020259/1. 6We observe this both in the raw gradients and after taking the Adam step rules into account.",
  "References Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin- ton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In Proceedings of the International Conference on Learning Represen- tations (ICLR). Ankur Bapna, Mia Chen, Orhan Firat, Yuan Cao, and Yonghui Wu. 2018. Training deeper neural ma- chine translation models with transparent attention. In Proceedings of the 2018 Conference on Em- pirical Methods in Natural Language Processing, pages 3028\u20133033, Brussels, Belgium. Association for Computational Linguistics. Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ale\u02c7s Tamchyna. 2014.",
  "2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12\u201358, Baltimore, Maryland, USA. Associa- tion for Computational Linguistics. Ond\u02c7rej Bojar, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Philipp Koehn, and Christof Monz. 2018. Findings of the 2018 con- ference on machine translation (WMT18). In Pro- ceedings of the Third Conference on Machine Trans- lation: Shared Task Papers, pages 272\u2013303, Bel- gium, Brussels. Association for Computational Lin- guistics. Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc Le. 2017. Massive exploration of neural machine translation architectures. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1442\u20131451, Copenhagen, Denmark. Association for Computa- tional Linguistics.",
  "2017. Massive exploration of neural machine translation architectures. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1442\u20131451, Copenhagen, Denmark. Association for Computa- tional Linguistics. Mauro Cettolo, Jan Niehues, Sebastian St\u00a8uker, Luisa Bentivogli, and Marcello Federico. 2014. Report on the 11th IWSLT Evaluation Campaign, IWSLT 2014. In Proceedings of the 11th Workshop on Spo- ken Language Translation, pages 2\u201316, Lake Tahoe, CA, USA. Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones, Mike Schuster, Noam Shazeer, Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Zhifeng Chen, Yonghui Wu, and Macduff Hughes. 2018. The best of both worlds: Combining recent advances in neural machine translation.",
  "2018. The best of both worlds: Combining recent advances in neural machine translation. In Proceed- ings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers), pages 76\u201386. Association for Computational Linguistics. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. CoRR, abs/1904.10509. Kyunghyun Cho, Bart van Merrienboer, Caglar Gul- cehre, Dzmitry Bahdanau, Fethi Bougares, Hol- ger Schwenk, and Yoshua Bengio. 2014. Learn- ing Phrase Representations using RNN Encoder\u2013 Decoder for Statistical Machine Translation. In Pro- ceedings of the 2014 Conference on Empirical Meth- ods in Natural Language Processing (EMNLP), pages 1724\u20131734, Doha, Qatar. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics. Mattia Antonino Di Gangi and Marcello Federico. 2018. Deep neural machine translation with weakly- recurrent units. In Proceedings of EAMT, Alicante, Spain. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. 2017. Convolutional sequence to sequence learning. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1243\u20131252, International Convention Centre, Sydney, Australia. PMLR. Felix A. Gers and J\u00a8urgen Schmidhuber. 2001.",
  "PMLR. Felix A. Gers and J\u00a8urgen Schmidhuber. 2001. Long Short-Term Memory Learns Context Free and Con- text Sensitive Languages. In Proceedings of the ICANNGA 2001 Conference, volume 1, pages 134\u2013 137. Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke S. Zettlemoyer. 2019. Constant-time machine translation with conditional masked language mod- els. CoRR, abs/1904.09324. Xavier Glorot and Y Bengio. 2010. Understanding the dif\ufb01culty of training deep feedforward neural net- works. Journal of Machine Learning Research - Proceedings Track, 9:249\u2013256. Jiatao Gu, James Bradbury, Caiming Xiong, Vic- tor O.K. Li, and Richard Socher. 2018. Non- autoregressive neural machine translation. In Inter- national Conference on Learning Representations. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015.",
  "Li, and Richard Socher. 2018. Non- autoregressive neural machine translation. In Inter- national Conference on Learning Representations. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep residual learning for image recog- nition. CoRR, abs/1512.03385. Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997. Long Short-Term Memory. Neural Comput., 9(8):1735\u2013 1780.",
  "Marcin Junczys-Dowmunt, Kenneth Hea\ufb01eld, Hieu Hoang, Roman Grundkiewicz, and Anthony Aue. 2018. Marian: Cost-effective high-quality neural machine translation in C++. In Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, Melbourne, Australia. Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. 2018. Scaling neural machine trans- lation. arXiv preprint arXiv:1806.00187. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: A method for automatic eval- uation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Com- putational Linguistics, ACL \u201902, pages 311\u2013318, Stroudsburg, PA, USA. Association for Computa- tional Linguistics. Ngoc-Quan Pham, Thai-Son Nguyen, Jan Niehues, Markus M\u00a8uller, and Alexander H. Waibel. 2019.",
  "Association for Computa- tional Linguistics. Ngoc-Quan Pham, Thai-Son Nguyen, Jan Niehues, Markus M\u00a8uller, and Alexander H. Waibel. 2019. Very deep self-attention networks for end-to-end speech recognition. CoRR, abs/1904.13377. Matt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186\u2013 191. Association for Computational Linguistics. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing by generative pre-training. Marc\u2019Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2016. Sequence Level Training with Recurrent Neural Networks. In The International Conference on Learning Representa- tions. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units.",
  "2016. Sequence Level Training with Recurrent Neural Networks. In The International Conference on Learning Representa- tions. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u2013 1725, Berlin, Germany. Association for Computa- tional Linguistics. K. Simonyan and A. Zisserman. 2015. Very deep con- volutional networks for large-scale image recogni- tion. In International Conference on Learning Rep- resentations. David R So, Chen Liang, and Quoc V Le. 2019. The evolved transformer. arXiv preprint arXiv:1901.11117. Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. 2018. Blockwise parallel decoding for deep autore- gressive models. In Advances in Neural Information Processing Systems, pages 10086\u201310095.",
  "Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. 2018. Blockwise parallel decoding for deep autore- gressive models. In Advances in Neural Information Processing Systems, pages 10086\u201310095. Ashish Vaswani, Samy Bengio, Eugene Brevdo, Fran- cois Chollet, Aidan Gomez, Stephan Gouws, Llion Jones, \u0141ukasz Kaiser, Nal Kalchbrenner, Niki Par- mar, Ryan Sepassi, Noam Shazeer, and Jakob Uszkoreit. 2018. Tensor2Tensor for neural machine translation. In Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers), pages 193\u2013 199, Boston, MA. Association for Machine Transla- tion in the Americas. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need.",
  "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar- nett, editors, Advances in Neural Information Pro- cessing Systems 30, pages 5998\u20136008. Curran As- sociates, Inc. Mingxuan Wang, Zhengdong Lu, Jie Zhou, and Qun Liu. 2017. Deep neural machine translation with lin- ear associative unit. In Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 136\u2013 145, Vancouver, Canada. Association for Computa- tional Linguistics. Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. 2019.",
  "Association for Computa- tional Linguistics. Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. 2019. Learning deep transformer models for ma- chine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Lin- guistics, pages 1810\u20131822, Florence, Italy. Associa- tion for Computational Linguistics. Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and Michael Auli. 2019. Pay less attention with lightweight and dynamic convolutions. In Interna- tional Conference on Learning Representations.",
  "Associa- tion for Computational Linguistics. Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and Michael Auli. 2019. Pay less attention with lightweight and dynamic convolutions. In Interna- tional Conference on Learning Representations. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, ukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. CoRR, abs/1609.08144.",
  "2016. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. CoRR, abs/1609.08144. Alireza Zaeemzadeh, Nazanin Rahnavard, and Mubarak Shah. 2018. Norm-preservation: Why residual networks can become extremely deep? CoRR, abs/1805.07477. Biao Zhang, Deyi Xiong, and Jinsong Su. 2018. Accel- erating neural transformer via an average attention network. In Proceedings of the 56th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1789\u20131798, Mel- bourne, Australia. Association for Computational Linguistics. Biao Zhang, Deyi Xiong, and Jinsong Su. 2018. Neu- ral machine translation with deep attention. IEEE",
  "Transactions on Pattern Analysis and Machine In- telligence, pages 1\u20131. Biao Zhang, Deyi Xiong, Jinsong Su, Hong Duan, and Min Zhang. 2016. Variational neural machine trans- lation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Process- ing, pages 521\u2013530, Austin, Texas. Association for Computational Linguistics. Hongyi Zhang, Yann N. Dauphin, and Tengyu Ma. 2019. Fixup initialization: Residual learning with- out normalization via better initialization. In Inter- national Conference on Learning Representations. Xiangwen Zhang, Jinsong Su, Yue Qin, Yang Liu, Ron- grong Ji, and Hongji Wang. 2018. Asynchronous bidirectional decoding for neural machine transla- tion. In Thirty-Second AAAI Conference on Arti\ufb01- cial Intelligence. Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. 2016. Deep recurrent models with fast-forward connections for neural machine translation. Trans- actions of the Association for Computational Lin- guistics, 4:371\u2013383."
]