{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder Li Du, Xiao Ding, Ting Liu\u2217and Zhongyang Li Research Center for Social Computing and Information Retrieval Harbin Institute of Technology, China {ldu, xding, tliu, zyli}@ir.hit.edu.cn Abstract Understanding event and event-centered com- monsense reasoning are crucial for natural lan- guage processing (NLP). Given an observed event, it is trivial for human to infer its in- tents and effects, while this type of If-Then reasoning still remains challenging for NLP systems. To facilitate this, a If-Then common- sense reasoning dataset Atomic is proposed, together with an RNN-based Seq2Seq model to conduct such reasoning. However, two fun- damental problems still need to be addressed: \ufb01rst, the intents of an event may be multiple, while the generations of RNN-based Seq2Seq models are always semantically close; sec- ond, external knowledge of the event back- ground may be necessary for understanding events and conducting the If-Then reasoning.",
      "To address these issues, we propose a novel context-aware variational autoencoder effec- tively learning event background information to guide the If-Then reasoning. Experimental results show that our approach improves the accuracy and diversity of inferences compared with state-of-the-art baseline methods. 1 Introduction Recently, event-centered commonsense knowl- edge has attracted much attention (Chambers and Jurafsky, 2008; Segers et al., 2016; Wang et al., 2017; Li et al., 2018), because of understanding events is an important component of NLP. Given a daily-life event, human can easily understand it and reason about its causes, effects, and so on. However, it still remains a challenging task for NLP systems. This is partly due to most of them are trained for task-speci\ufb01c datasets or objectives, which results in models that are adapt at \ufb01nding task-speci\ufb01c underlying correlation patterns but have limited capability in simple and explainable commonsense reasoning (Sap et al., 2018). \u2217Corresponding author Figure 1: A illustration of two challenging problems in If- Then reasoning.",
      "\u2217Corresponding author Figure 1: A illustration of two challenging problems in If- Then reasoning. (a) Given an observed event, the feelings about this event could be multiple. (b) Background knowl- edge is need for generating reasonable inferences, which is absent in the dataset (marked by dashed lines). To facilitate this, Rashkin et al. (2018) build the Event2Mind dataset and Sap et al. (2018) present the Atomic dataset, mainly focus on nine If-Then reasoning types to describe causes, effects, intents and participant characteristic about events. To- gether with these datasets, a simple RNN-based encoder-decoder framework is proposed to con- duct the If-Then reasoning. However, there still remains two challenging problems. First, as illustrated in Figure 1, given an event \u201cPersonX \ufb01nds a new job\u201d, the plausi- ble feeling of PersonX about that event could be multiple (such as \u201cneedy/stressed out\u201d and \u201cre- lieved/joyful\u201d).",
      "First, as illustrated in Figure 1, given an event \u201cPersonX \ufb01nds a new job\u201d, the plausi- ble feeling of PersonX about that event could be multiple (such as \u201cneedy/stressed out\u201d and \u201cre- lieved/joyful\u201d). Previous work showed that for the one-to-many problem, conventional RNN-based encoder-decoder models tend to generate generic responses, rather than meaningful and speci\ufb01c an- swers (Li et al., 2016; Serban et al., 2016). Second, as a commonsense reasoning problem, rich background knowledge is necessary for gen- erating reasonable inferences. For example, as shown in Figure 1, the feeling of PersonX upon the event \u201cPersonX \ufb01nds a new job\u201d could be multi- ple. However, after given a context \u201cPersonX was \ufb01red\u201d, the plausible inferences would be narrowed down to \u201cneedy\u201d or \u201cstressed out\u201d. To better solve these problems, we propose a context-aware variational autoencoder (CWVAE) together with a two-stage training procedure.",
      "However, after given a context \u201cPersonX was \ufb01red\u201d, the plausible inferences would be narrowed down to \u201cneedy\u201d or \u201cstressed out\u201d. To better solve these problems, we propose a context-aware variational autoencoder (CWVAE) together with a two-stage training procedure. Vari- arXiv:1909.08824v3  [cs.CL]  1 Dec 2019",
      "ational Autoencoder (VAE) based models have shown great potential in modeling the one-to- many problem and generate diversi\ufb01ed inferences (Bowman et al., 2015; Zhao et al., 2017). In addition to the traditional VAE structure, we introduces an extra context-aware latent variable in CWVAE to learn the event background knowl- edge. In the pretrain stage, CWVAE is trained on an auxiliary dataset (consists of three narra- tive story corpora and contains rich event back- ground knowledge), to learn the event background information by using the context-aware latent vari- able. Subsequently, in the \ufb01netune stage, CWVAE is trained on the task-speci\ufb01c dataset to adapt the event background information to each speci\ufb01c as- pect of If-Then inferential target (e.g., intents, re- actions, etc.). Experiments on the Event2Mind and Atomic dataset show that our proposed approach outper- forms baseline methods in both the accuracy and diversity of inferences. The code is released at https://github.com/sjcfr/CWVAE.",
      "Experiments on the Event2Mind and Atomic dataset show that our proposed approach outper- forms baseline methods in both the accuracy and diversity of inferences. The code is released at https://github.com/sjcfr/CWVAE. 2 Background Before speci\ufb01cally describing two dataset \u2014- Event2Mind and Atomic used in this paper as well as the If-Then reasoning task, for clarity, we de\ufb01ne the following terminologies: Base event: the prerequisite event in If-Then reasoning, organized as a verb phrase with a predi- cate and its arguments, such as the event \u201cPersonX \ufb01nds a new job\u201d shown in Figure 1. Inference dimension: a particular If-Then rea- soning type, e.g., intents, effects of the base event. Details are shown in Table 1 and Table 2. Target: the inferential results. For example, as shown in Figure 1, given a base event \u201cPer- sonX \ufb01nds a new job\u201d and one inference dimen- sion \u201cxReact\u201d, the targets could be \u201crelieved\u201d or \u201cneedy\u201d. Notice that each inference dimension can have multiple targets.",
      "For example, as shown in Figure 1, given a base event \u201cPer- sonX \ufb01nds a new job\u201d and one inference dimen- sion \u201cxReact\u201d, the targets could be \u201crelieved\u201d or \u201cneedy\u201d. Notice that each inference dimension can have multiple targets. Event2Mind Dataset contains 25K base events and 300K targets, annotated through crowdsourc- ing. Event2Mind is organized in a hierarchical form: each base event has three types of inference dimensions, and given a base event, under one of inference dimensions, several targets may simul- taneously exist. Table 1 shows the (base event- inference dimension-target) hierarchical structure through an example from Event2Mind. Atomic Dataset Inspired by Event2Mind, the Base event Inference Dim. Target PersonX writes PersonY a letter xIntent to send a message, express themself xReact nervous, thoughtful oReact indifferent, receptive Table 1: Hierarchical structure of Event2Mind dataset. For speci\ufb01c inference dimensions, \u201cx\u201d and \u201co\u201d refers to PersonX and others respectively. Base event Inference Dim.",
      "For speci\ufb01c inference dimensions, \u201cx\u201d and \u201co\u201d refers to PersonX and others respectively. Base event Inference Dim. Target PersonX adopts a child xIntent to help another person, to have a child xNeed to visit adoption agency, to be approved for adoption xAttr compassionate, generous xEffect becomes a parent, gains love and companionship xWant take child home, buy child clothes xReact happy, caring oReact has a parent, receives love and affection oWant try on new clothes, to have a family oEffect has a parent, Receives love and affection Table 2: Hierarchical structure of Atomic dataset. For spe- ci\ufb01c inference dimensions, \u201cx\u201d and \u201co\u201d refers to PersonX and others respectively. Atomic dataset shares the same hierarchical struc- ture as Event2Mind, while scales up the size of dataset and expands the scope to nine types of inference dimensions. Table 2 shows the (base event-inference dimension-target) hierarchi- cal structure through an example from Atomic. Though Atomic covers the inference dimensions of Event2Mind, the base event collection of Event2Mind is nonidentical to that of Atomic.",
      "Table 2 shows the (base event-inference dimension-target) hierarchi- cal structure through an example from Atomic. Though Atomic covers the inference dimensions of Event2Mind, the base event collection of Event2Mind is nonidentical to that of Atomic. Problem De\ufb01nition The If-Then reasoning task could be formally de\ufb01ned as a conditional one-to- many generation problem: given a base event x and one inference dimension d, the model is re- quired to generate targets y = f(x, d) as close to the ground truths as possible. Both x and y consist of sequence of words: x = {x1, . . . , xm}, and y = {y1, . . . , yn}, where m and n denotes the length of x and y, respectively. Conditional Variational Autoencoder The vari- ational autoencoder (VAE) de\ufb01nes a generative framework suited for one-to-many generation problem (Kingma and Welling, 2014). While con-",
      "Figure 2: Illustration of inference and generation process of CVAE in a directed graph. Dashed lines represent the infer- ence of z. Solid lines represent the generation process. ditional variational autoencoder (CVAE) (Sohn et al., 2015) is an extension of VAE on the con- ditional generation problem. As shown in Fig- ure 2 (a), CVAE characterizes the conditional one- to-many generation problem using three random variables: event x, target y and a latent variable z, which is used for modeling the latent distri- bution of semantic over targets given an event. Hence, under a certain inference dimension, with regard to the latent semantic variable z, the condi- tional generation problem could be expressed as p(y|x) = R p(y|x, z)p(z|x)dz. CVAE models p(y|x, z) and p(z|x) using deep neural networks (parameterized by \u03b8) p\u03b8(y|x, z) and p\u03b8(z|x).",
      "CVAE models p(y|x, z) and p(z|x) using deep neural networks (parameterized by \u03b8) p\u03b8(y|x, z) and p\u03b8(z|x). Then as illustrated in Figure 2 (b), y could be generated from x and z. CVAE is trained to maximize the conditional likelihood p(y|x), which involves an intractable marginalization over the latent variable z. In- stead, following Kingma and Welling (2014), a practical way is to introduce another deep network (parameterized by \u03c6) q\u03c6(z|x, y) to approximate the true posterior distribution p(z|x, y) and maxi- mize the evidence lower bound (ELBO) of the log- likelihood function: LELBO(\u03b8, \u03c6) =Eq\u03c6(z|x,y)log(p\u03b8(y|x, z))\u2212 KL(q\u03c6(z|x, y)||p\u03b8(z|x)) \u2264logp(y|x) (1) Therefore, CVAE is composed of three neural networks in general. We refer to p\u03b8(z|x) as a prior network, q\u03c6(z|x, y) as a recognition network, and p\u03b8(y|x, z) as a neural decoder.",
      "We refer to p\u03b8(z|x) as a prior network, q\u03c6(z|x, y) as a recognition network, and p\u03b8(y|x, z) as a neural decoder. 3 Context-aware Variational Autoencoder Traditional CVAE can model the event-target relation. In other words, given an observed event, CVAE can generate its corresponding tar- gets. While in this paper we model the If-Then reasoning as a [(background), event]-target process. It means that in addition to the observed Figure 3: Illustration of pretrain, \ufb01netune and generation process of CWVAE in a directed graph. Dashed lines rep- resent the inference of z, zc and zc\u2032. Solid lines represent the generation process. Red circle denotes the context-aware latent variable. event, we also want to involve the event back- ground knowledge (which can be learned from event contexts) to generate the reasonable targets.",
      "Solid lines represent the generation process. Red circle denotes the context-aware latent variable. event, we also want to involve the event back- ground knowledge (which can be learned from event contexts) to generate the reasonable targets. To this end, we propose a context-aware varia- tional autoencoder (CWVAE), with two additional latent variables: a context-acquiring latent vari- able zc to directly acquire context information, and a context-aware latent variable zc\u2032 to learn background knowledge from zc, as shown in Fig- ure 3 (a). However, the event context information is absent in the Event2Mind and Atomic dataset. To learn from the external event context informa- tion, we design the following two-stage training procedure for CWVAE. Pretrain: Learning Event Background Knowl- edge from Auxiliary Dataset In the pretrain stage, CWVAE is trained on three narrative story corpora with rich event context information. As shown in Figure 3 (a), context-acquiring latent variable zc is directly conditioned on the context c. Hence, zc could be employed for acquiring back- ground knowledge from event contexts.",
      "As shown in Figure 3 (a), context-acquiring latent variable zc is directly conditioned on the context c. Hence, zc could be employed for acquiring back- ground knowledge from event contexts. Then, we minimize the distance between zc and the context- aware latent variable zc\u2032, by which the event back- ground knowledge is transferred from zc to zc\u2032. Finetune: Adapt Event Background Knowl- edge to Each Inference Dimension In the \ufb01ne- tune stage, as shown in Figure 3 (b), CWVAE is trained on the Event2Mind and Atomic dataset without the event context information. Pretrained CWVAE is \ufb01netuned to learn the speci\ufb01c inferen- tial knowledge of each inference dimension. After the training procedure, as shown in Figure 3 (c), samples of z is generated based on x and samples of zc\u2032, where zc\u2032 contains rich event background knowledge helpful for If-Then reasoning.",
      "Figure 4: Architecture of CWVAE. We mark Neural encoder in green, prior network in blue, recognition network in brown and neural decoder in orange, respectively. 3.1 Architecture of CWVAE As shown in Figure 4, CWVAE is mainly com- posed of four parts: a neural encoder that provides distributed representations of base events/targets, a recognition network for inferring q\u03c6(zc\u2032|x, y), q\u03c6(zc|x, c) and q\u03c6(z|zc\u2032, x), a prior network for modeling p\u03b8(zc\u2032|x) and p\u03b8(z|x, zc\u2032), and a neural decoder that integrates the information from z and zc\u2032 to generate targets. Neural Encoder We employ a bidirectional GRU as neural encoder, which encodes context c, event x and target y into distributed representations hc = {hc 1, . . . , hc lc}, hx = {hx 1, . . . , hx lx} and hy = {hy 1, . . . , hy ly}, where lc, lx and ly is the length of c, x and y, respectively.",
      ". . , hc lc}, hx = {hx 1, . . . , hx lx} and hy = {hy 1, . . . , hy ly}, where lc, lx and ly is the length of c, x and y, respectively. Recognition Network The recognition network models q\u03c6(z|x, y), q\u03c6(zc|x, c), q\u03c6(z|zc\u2032, x) based on hx, hy and hc. Following traditional VAE, the above- mentioned three distributions are assumed to be multivariate Gaussian distribution with a diagonal covariance structure: q\u03c6(zc|x, c) \u223cN(\u00b5zc(x, c), \u03c3zc(x, c)I) q\u03c6(zc\u2032|x, y) \u223cN(\u00b5zc\u2032(x, y), \u03c3zc\u2032(x, y)I) q\u03c6(z|x, y) \u223cN(\u00b5z(x, y), \u03c3z(x, y)I) (2) where \u00b5 denotes the mean of the distribution, \u03c3 denotes the standard deviation of the distribution, and I denotes the identity matrix.",
      "Given hx, hy and hc, we propose a novel attention-based inferer (ABI) module to estimate the mean and standard deviation of q\u03c6(zc|x, c), q\u03c6(zc\u2032|x, y) and q\u03c6(z|x, y): \u00b5zc, \u03c3z = ABIc(hc, hx) \u00b5zc\u2032, \u03c3z = ABIc\u2032(hy, hx) \u00b5z, \u03c3z = ABIz(zc\u2032, hx) (3) Brie\ufb02y, through the attention mechanism, ABI can capture the semantic interaction between input sequences, and estimate the parameters of distri- butions based on it. We will introduce the speci\ufb01c structure of ABI in below. Prior Network Prior Network models p\u03b8(zc\u2032|x) and p\u03b8(z|x, zc\u2032) based on hx.",
      "We will introduce the speci\ufb01c structure of ABI in below. Prior Network Prior Network models p\u03b8(zc\u2032|x) and p\u03b8(z|x, zc\u2032) based on hx. The distribution of p\u03b8(zc\u2032|x) and p\u03b8(z|x, zc\u2032) are still assumed to be multivariate Gaussian, whereas the parameters are different: p\u03b8(z|x) \u223cN(\u00b5 \u2032 z(x), \u03c3 \u2032 z(x)I) p\u03b8(zc\u2032|x) \u223cN(\u00b5 \u2032 zc\u2032(x), \u03c3 \u2032 zc\u2032(x)I) (4) where \u00b5 \u2032 denotes the mean of the distribution, \u03c3 \u2032 denotes the standard deviation of the distribution and I denotes the identity matrix.",
      "Then the attention-based inferer module is still employed to estimate parameters of distributions: \u00b5 \u2032 zc\u2032, \u03c3 \u2032 z = ABI \u2032 c\u2032(hx, hx) \u00b5 \u2032 z, \u03c3 \u2032 z = ABI \u2032 z(zc\u2032, hx) (5) Neural Decoder Given the base event x, the se- mantic latent variable z, and the context-aware la- tent variable zc\u2032, the neural decoder de\ufb01nes the generation probability of y as following: p(y|x, z, zc\u2032) = n Y j=1 p(yj|y < j, z, zc\u2032, x) (6) where p(yj|y < j, z, zc\u2032, x) = g(yj\u22121, sj\u22121, ej), g(\u00b7) is an attention-based feed forward model, ej = P i \u03b1jihx i is the context vector and sj\u22121 is the hidden state of the decoder. We obtain g(\u00b7) and ej the same way as Bahdanau et al. (2014). Whereas our decoder differs from Bah- danau et al.",
      "We obtain g(\u00b7) and ej the same way as Bahdanau et al. (2014). Whereas our decoder differs from Bah- danau et al. (2014) in that our model integrates the context-aware latent variable zc\u2032 and seman- tic latent variable z in the computation of sj = GRU([Eyj; sj\u22121, z, zc\u2032]), where Eyj is the word embeddings of target words. Note that through concatenating z and zc\u2032 with Eyj and sj\u22121, sj could be affected by context- aware latent variable zc\u2032 and semantic latent vari- able z. This allows model to directly access to the event background knowledge from zc\u2032. In addi- tion, the randomness of z and zc\u2032 would increase the diversity of model generation. Attention-based Inferer Attention mechanism has shown strong ability in capturing semantic in- teractions (Gong et al., 2017). Inspired by the co- attention mechanism (Parikh et al., 2016), we pro- pose an attention-based inferer (ABI) to estimate",
      "the mean and standard deviation of a distribution belongs to p\u03b8(\u00b7) or q\u03c6(\u00b7) by capturing semantic in- teractions of input sequences. Speci\ufb01cally, given two input sequences (e.g., representations of contexts and events) a = {a1, . . . , ala} and b = {b1, . . . , blb} with length la and lb, we \ufb01rst obtain the attention scores from each side through: \u03b3a t = exp[(Waha t )T (Wbhb i)] P i exp[(Waha t )T (Wbhb i)] \u03b3b t = exp[(Waha i )T (Wbhb t)] P i exp[(Waha i )T (Wbhb t)] (7) where Wa \u2208Rd\u00d7da and Wb \u2208Rd\u00d7db are parame- ter weights. With these attention scores,",
      "With these attention scores, the context vectors of both sequences are given by: ca t = a\u03b3a t cb t = b\u03b3b t (8) Then we perform a mean pooling operation on context vectors of both sequences: \u00afca = 1 la la X t=1 ca t \u00afcb = 1 lb lb X t=1 cb t (9) To obtain the mean and standard deviation, the pooled context vectors \u00afca and \u00afcb which carry se- mantic interaction between two sequences, are concatenated and projected into a latent semantic space through a nonlinear transformation: hz = tanh(W[\u00afca; \u00afcb] + bz) (10) Finally the mean and standard deviation are generated through a nonlinear transformation over hz: \u00b5 = W\u00b5hz + b\u00b5 \u03c3 = softplus(W\u03c3hz + b\u03c3) (11) 3.2 Optimizing With the incorporation of zc\u2032, the original loglike- lihood could be decomposed as: logp(y|x) = ZZ p(y|z, zc\u2032, x)dzdzc\u2032 (12) Then following traditional CVAE,",
      "2 Optimizing With the incorporation of zc\u2032, the original loglike- lihood could be decomposed as: logp(y|x) = ZZ p(y|z, zc\u2032, x)dzdzc\u2032 (12) Then following traditional CVAE, the ELBO of CWVAE is de\ufb01ned as follows: LELBO(\u03b8, \u03c6) = Eq\u03c6(z|x,zc\u2032 )q\u03c6(zc\u2032 |x,y)logp\u03b8(y|x, z, zc\u2032) | {z } Reconstruction Loss \u2212 Z q\u03c6(z|x, y)KL(q\u03c6(zc\u2032|x, y)||p\u03b8(zc\u2032|x))dz | {z } KL term \u2212 Z q\u03c6(zc\u2032|x, y)KL(q\u03c6(z|x, zc\u2032)||p\u03b8(z|x, zc\u2032))dzc\u2032 | {z } KL term (13) which is the objective function at the \ufb01netune stage.",
      "y)KL(q\u03c6(z|x, zc\u2032)||p\u03b8(z|x, zc\u2032))dzc\u2032 | {z } KL term (13) which is the objective function at the \ufb01netune stage. While in the pretrain stage, as we aim to learn background knowledge through minimizing the distance between zc and zc\u2032, in addition to LELBO, a context-aware regulation term is introduced: LP T (\u03b8, \u03c6) =LELBO(\u03b8, \u03c6) \u2212\u03bb Z KL(q\u03c6(zc|x, c)||q\u03c6(zc\u2032|x, y))dzc\u2032 | {z } context-aware regulation (14) where the context aware regularization term is the KL distance between z and zc\u2032. Through mini- mizing the context aware regularization term, we aim to pass event context knowledge from zc to the context aware latent variable zc\u2032. 3.3 Training Details To test the performance of CWVAE, we split the Event2Mind and Atomic dataset into training, de- velopment and test sets (80%, 10%, 10%) in the same way as Rashkin et al. (2018) and Sap et al.",
      "3.3 Training Details To test the performance of CWVAE, we split the Event2Mind and Atomic dataset into training, de- velopment and test sets (80%, 10%, 10%) in the same way as Rashkin et al. (2018) and Sap et al. (2018), respectively. We initialize the embedding layer from 300d GloVe word embeddings. The neural encoder is chosen to be biGRU with 300 hidden units. For the ABI module, size of Wa and Wb is set to be 100 \u00d7 da and 100 \u00d7 db respectively. The dimen- sion of zc, zc\u2032 and z is all set as 40. The neural decoder is set to be GRU with 300d hidden state. Regulation coef\ufb01cient \u03bb of context-aware regula- tion term is set to be 0.1. Models are trained using an Adam optimizer (Kinga and Adam, 2015) with a learning rate of 0.001. 4 Experiments 4.1 Auxiliary Dataset The auxiliary dataset is built upon three human- written story corpora: ROCStories (Mostafazadeh",
      "Context Event Inference Target 1\u20ddjason had been really stressed out at work. 2\u20ddhe decided he needed a different kind of job. 3\u20ddjason applied for a job in a different \ufb01eld. 4\u20ddhe got the job . 5\u20ddjason was much happier at his new job . Table 3: An example for the construction of auxiliary dataset. For a \ufb01ve-sentence-paragraph, the \ufb01rst three sentences are taken as event context, while the fourth and \ufb01fth sentence is taken as base event and target respectively. et al., 2016), VIST (Huang et al., 2016) and Writ- ingPrompts (Fan et al., 2018). ROCStories and VIST are composed of short stories with \ufb01ve sen- tences. We \ufb01lter out stories of more than 1,000 words in WritingPrompts, and cut the remaining stories into \ufb01ve-sentence-paragraphs.",
      "ROCStories and VIST are composed of short stories with \ufb01ve sen- tences. We \ufb01lter out stories of more than 1,000 words in WritingPrompts, and cut the remaining stories into \ufb01ve-sentence-paragraphs. For each \ufb01ve-sentence-paragraph, we de\ufb01ne the \ufb01rst three sentences as contexts of the base event, the fourth sentence as the base event, and the \ufb01fth sentence as the inference target. For example, as shown in Table 3, the \ufb01rst three sentences describe a context that Jason was unsatis\ufb01ed about his job and applied for a new job. Hence, after happening the event \u201che got the job\u201d, a plausible react about the event could be \u201cjason was much happier at his new job\u201d. In total, the auxiliary dataset contains 192,316 (context, event, target) triples. 4.2 Baselines We compared our proposed model with the follow- ing four baseline methods: \u2022 RNN-based Seq2Seq proposed by Sap et al. (2018) for the If-Then reasoning on Atomic.",
      "4.2 Baselines We compared our proposed model with the follow- ing four baseline methods: \u2022 RNN-based Seq2Seq proposed by Sap et al. (2018) for the If-Then reasoning on Atomic. \u2022 Variational Seq2Seq combines a latent vari- able with the encoder-decoder structure through converting the last hidden state of RNN en- coder into a Gaussian distributed latent variable (Bowman et al., 2015). \u2022 VRNMT Propose by Su et al. (2018), VRNMT combines CVAE with attention-based encoder- decoder framework through introduces a latent variable to model the semantic distribution of targets. \u2022 CWVAE-Unpretrained refers to the CWVAE model without the pretrain stage. Note that, for each baseline method, we train dis- tinct models for each distinct inference dimension, respectively. 4.3 Evaluation Metrics Automatic Evaluation We \ufb01rst compare the perplexity of CWVAE with baseline methods.",
      "Note that, for each baseline method, we train dis- tinct models for each distinct inference dimension, respectively. 4.3 Evaluation Metrics Automatic Evaluation We \ufb01rst compare the perplexity of CWVAE with baseline methods. Per- plexity measures the probability of model to re- generate the exact targets, which is particular suit- Metric Methods xIntent xReact oReact PPL RNN-based Seq2Seq 44.12 29.18 14.08 Variational Seq2Seq 42.06 28.22 12.62 VRNMT 33.45 25.54 11.93 CWVAE-Unpretrained 31.32 24.07 11.37 CWVAE 29.23 23.17 11.04 BLEU RNN-based Seq2Seq 2.75 2.11 5.18 Variational Seq2Seq 2.84 2.43 2.08 VRNMT 4.81 3.94 6.61 CWVAE-Unpretrained 7.36 5.52 5.33 CWVAE 12.98 5.65 6.97 Table 4: Average perplexity and BLEU score (reported in percentages) for the top 10 generations under each inference dimension of Event2Mind.",
      "The the best result for each di- mension is emboldened.",
      "The the best result for each di- mension is emboldened. Metric Methods xIntent xReact oReact dist-1 RNN-based Seq2Seq 0.0002 0.0002 0.0001 Variational Seq2Seq 0.0006 0.0003 0.0001 VRNMT 0.0002 0.0002 0.0003 CWVAE-Unpretrained 0.0023 0.0017 0.0004 CWVAE 0.0052 0.0033 0.0025 dist-2 RNN-based Seq2Seq 0.0005 0.0002 0.0002 Variational Seq2Seq 0.0014 0.0002 0.0001 VRNMT 0.0005 0.0003 0.0001 CWVAE-Unpretrained 0.0061 0.0040 0.0013 CWVAE 0.0146 0.0099 0.0063 Table 5: Distinct-1 and distinct-2 scores for the top 10 gen- erations under each inference dimension of Event2Mind.",
      "The the best result for each dimension is emboldened. able for evaluating the model performance on one- to-many problem (Serban et al., 2017). Further, we employ BLEU score to evaluate the accuracy of generations (Papineni et al., 2002), and the number of distinct n-gram to evaluate the diver- sity of generations (Li et al., 2016). The distinct is normalized to [0, 1] by dividing the total number of generated tokens. Human Evaluation Since automatic evaluation of generations is still a challenging task (Liu et al., 2016), we also conduct human evaluations on the model performance. Five human experts are em- ployed to evaluate the coherence, diversity and \ufb02u- ency of generated targets. Experts are asked to vote for if a generation is \ufb02uent or coherent for each generated target, and give a 1-5 score for the diversity of generations. For both Event2Mind and Atomic datasets, 100 events are randomly selected from the test set. For each method, top 10 gener- ated targets of each base event are used for eval-",
      "Metric Methods xIntent xNeed xAttr xEffect xReact xWant oWant oReact oEffect PPL RNN-based Seq2Seq 22.54 24.69 33.54 65.13 29.52 26.63 16.76 14.99 35.17 Variational Seq2Seq 26.48 28.31 33.00 68.62 29.93 29.50 16.98 14.25 34.20 VRNMT 21.04 24.28 24.87 61.05 26.62 28.57 14.45 14.86 30.12 CWVAE-Unpretrained 20.73 23.72 25.80 60.62 25.75 26.71 15.93 12.82 32.00 CWVAE 15.93 20.32 23.85 50.74 21.39 24.02 14.02 11.70 29.13 BLEU RNN-based Seq2Seq 8.17 12.35 2.96 5.26 3.43 13.",
      "93 20.32 23.85 50.74 21.39 24.02 14.02 11.70 29.13 BLEU RNN-based Seq2Seq 8.17 12.35 2.96 5.26 3.43 13.44 7.08 4.09 6.42 Variational Seq2Seq 8.31 12.05 2.13 6.07 2.52 11.71 7.40 4.08 6.38 VRNMT 9.52 13.35 4.87 4.42 7.64 9.80 10.79 5.28 13.71 CWVAE-Unpretrained 11.37 14.64 4.07 14.11 7.86 12.70 12.09 8.16 14.93 CWVAE 12.12 15.67 5.63 14.64 8.13 15.01 13.83 8.58 11.",
      "07 14.11 7.86 12.70 12.09 8.16 14.93 CWVAE 12.12 15.67 5.63 14.64 8.13 15.01 13.83 8.58 11.63 Table 6: Average perplexity and BLEU scores (reported in percentages) for the top 10 generations under each inference dimension of Atomic. The the best result for each dimension is emboldened. Metric Methods xIntent xNeed xAttr xEffect xReact xWant oWant oReact oEffect dist-1 RNN-based Seq2Seq 0.0012 0.0029 0.0004 0.0019 0.0001 0.0022 0.0006 0.0001 0.0006 Variational Seq2Seq 0.0006 0.0018 0.0002 0.0002 0.0001 0.0013 0.0007 0.0001 0.0002 VRNMT 0.0002 0.0001 0.0053 0.",
      "0006 0.0018 0.0002 0.0002 0.0001 0.0013 0.0007 0.0001 0.0002 VRNMT 0.0002 0.0001 0.0053 0.0005 0.0018 0.0022 0.0005 0.0001 0.0004 CWVAE-Unpretrained 0.0019 0.0036 0.0119 0.0046 0.0021 0.0013 0.0018 0.0005 0.0006 CWVAE 0.0055 0.0045 0.0142 0.0028 0.0043 0.0040 0.0021 0.0030 0.0033 dist-2 RNN-based Seq2Seq 0.0036 0.0081 0.0002 0.0018 0.0002 0.0006 0.0013 0.0001 0.0011 Variational Seq2Seq 0.0013 0.",
      "0036 0.0081 0.0002 0.0018 0.0002 0.0006 0.0013 0.0001 0.0011 Variational Seq2Seq 0.0013 0.0042 0.0001 0.0003 0.0002 0.0026 0.0002 0.0003 0.0006 VRNMT 0.0002 0.0011 0.0002 0.0005 0.0001 0.0034 0.0005 0.0001 0.0004 CWVAE-Unpretrained 0.0060 0.0088 0.0136 0.0113 0.0043 0.0029 0.0041 0.0011 0.0009 CWVAE 0.0162 0.0112 0.0146 0.0072 0.0013 0.0107 0.0044 0.0068 0.",
      "0029 0.0041 0.0011 0.0009 CWVAE 0.0162 0.0112 0.0146 0.0072 0.0013 0.0107 0.0044 0.0068 0.0093 Table 7: Distinct-1 and distinct-2 scores for the top 10 generations under each inference dimension of Atomic. The the best result for each dimension is emboldened. uation. Finally we report three overall averaged scores of coherence, diversity and \ufb02uency on both datasets, respectively. 4.4 Overall Results We list the perplexity and BLEU score of CWVAE and baseline methods on Event2Mind and Atomic in Table 4 and Table 6, respectively, and show the distinct-1 and distinct-2 score on Event2Mind and Atomic in Table 5 and Table 7, respectively.",
      "We \ufb01nd that: (1) As shown in Table 5 and Table 7, compari- son between RNN-based Seq2Seq and variational- based methods, including Variational Seq2Seq, VRNMT, CWVAE-unpretrained and CWVAE shows that, variational-based methods could in- crease the diversity of generations. This con- \ufb01rms one of our motivations that variational-based methods could capture the latent semantic distri- bution within targets and increase the diversity of If-Then reasoning. (2) Comparing CWVAE-unpretrained with other baseline methods shows that, in general CW- VAE improves the accuracy and diversity on both dataset. These results indicate the ef\ufb01ciency of CWVAE in capturing the latent semantic distribu- tion of targets, and generate more reasonable in- ferential results.",
      "These results indicate the ef\ufb01ciency of CWVAE in capturing the latent semantic distribu- tion of targets, and generate more reasonable in- ferential results. (3) Comparison between CWVAE and CWVAE-unpretrained shows that the pre- Methods Coherence Diversity Fluency RNN-based Seq2Seq 0.28 2.03 0.73 Variational Seq2Seq 0.33 1.67 0.92 VRNMT 0.32 2.60 0.83 CWVAE-Unpretrained 0.36 2.10 0.92 CWVAE 0.43 2.85 0.96 Table 8: Human evaluation results on Event2Mind.",
      "Methods Coherence Diversity Fluency RNN-based Seq2Seq 0.21 2.66 0.78 Variational Seq2Seq 0.22 2.70 0.90 VRNMT 0.24 2.61 0.78 CWVAE-Unpretrained 0.25 2.72 0.83 CWVAE 0.32 3.03 0.90 Table 9: Human evaluation results on Atomic. train stage could enhance the performance of CWVAE in both the accuracy and diversity. This is mainly because event knowledge could offer the guidance for If-Then reasoning. In the pretrain stage, CWVAE could capture the event background knowledge through context-aware latent variable, and such knowledge could be be adapted to our task through the \ufb01ntune stage. To further evaluate the effectiveness of our pro- posed approach, we also conduct human eval- uations, the results of which are shown in Ta- ble 8 and Table 9. On both datasets, CWVAE- based methods achieve consistent better coher- ence, diversity and \ufb02uency performances.",
      "On both datasets, CWVAE- based methods achieve consistent better coher- ence, diversity and \ufb02uency performances. While comparing with CWVAE-Unpretrained, the pre- train procedure could improves the performance",
      "Base event Inference dim. Generations Ground truth CWVAE RNN-based Seq2Seq PersonX works tirelessly xIntent be productive and hardworking \ufb01nish his work soon earn more money and accomplish goal \ufb01nish his work and make money \ufb01nish his work and accomplish goal be productive and successful get the job done get the job done on time get the job done in the way get the job done for his life make money for his own future make money for his own money be productive \ufb01nish the project as soon as possible reach goal Table 10: An example of inferences made by CWVAE and RNN-based Seq2Seq model under inference dimension \u201cxIntent\u201d. on coherence and \ufb02uency. The main reasons are twofold: \ufb01rst, the CWVAE has advantage in cap- turing the semantic distribution of targets; second, event background learned from the pretrain stage is helpful for the If-Then reasoning. 4.5 Case Study Table 10 provides an example of model genera- tions given the base event \u201cPersonX works tire- lessly\u201d and the inference dimension \u201cxIntent\u201d.",
      "4.5 Case Study Table 10 provides an example of model genera- tions given the base event \u201cPersonX works tire- lessly\u201d and the inference dimension \u201cxIntent\u201d. The generations under CWVAE mainly contain four kinds of semantics: (1) be productive, (2) \ufb01n- ish his work soon, (3) accomplish goal, (4) earn more money. While the semantics of generations using baseline RNN-based Seq2Seq model is rel- atively limited. Furthermore, the \ufb01rst three kinds of semantic overlap the three ground truth targets, and the fourth kind of semantic is in accordance with daily-life commonsense. Compared to RNN- based Seq2Seq model, our approach can increase the diversity and rationality of generations, mean- while keep the accuracy. 5 Related Work 5.1 Event-Centered Commonsense Reasoning Understanding events and constructing event- centered commonsense knowledge are crucial to many NLP applications, such as intention recog- nition (Goldwasser and Zhang, 2016) and dialog generation (Wen et al., 2017).",
      "Recently a growing number of studies focus on event-centered com- monsense reasoning, which mainly concentrates on two areas, script event prediction and story end- ing generation/choosing. Script event prediction concerns with the temporal relationships between script events (Granroth-Wilding and Clark, 2016), which re- quires models to choose a correct subsequent triple-organized event among the candidates (Wang et al., 2017). Prior work mainly focused on modeling event pairs (Granroth-Wilding and Clark, 2016), event chains (Wang et al., 2017) and event graph (Li et al., 2018) to predict the subse- quent event. Story ending generation focuses on generating plausible story endings (Mostafazadeh et al., 2016), which requires models to understand the story context, and keep generated endings log- ically consistent with it (Peng et al., 2017; Guan et al., 2019). The above tasks mainly investi- gate the logical orders of events, whereas the If- Then reasoning task focuses on inferring the men- tal state of event participants.",
      "The above tasks mainly investi- gate the logical orders of events, whereas the If- Then reasoning task focuses on inferring the men- tal state of event participants. 5.2 Variational AutoEncoder-Decoder Based Natural Language Generation VAE (Kingma and Welling, 2014) has been widely applied in various of text generation tasks, such as dialogue and machine translation. In dialogue generation, Zhao et al. (2017) adapts VAE with encoder-decoder framework to model the latent semantic distribution of answers, which can increase the diversity of generations. For the task of machine translation, Su et al. (2018) and Zhang et al. (2016) employ a latent variable to capture the semantic interaction be- tween the source and target sentence, and regard the latent variable as a supplementation of atten- tion mechanism. While Wang et al. (2019) use the latent variable to model topic distributions in text generation. In this paper, we introduce an addi- tional context-aware latent variable to effectively learn background knowledge and conduct If-Then reasoning on the guidance of it.",
      "While Wang et al. (2019) use the latent variable to model topic distributions in text generation. In this paper, we introduce an addi- tional context-aware latent variable to effectively learn background knowledge and conduct If-Then reasoning on the guidance of it. 6 Conclusion In this paper, we propose a novel context-aware VAE (CWVAE) framework with two training stages for If-Then commonsense reasoning. By in- troducing an additional context-aware latent vari- able, CWVAE is able to learn external background knowledge, and conduct If-Then reasoning under its guidance. In the pretrain stage, CWVAE learns event background knowledge, then in the \ufb01netune stage CWVAE adapts such knowledge to each in- ference dimension. Experimental results demon- strate that CWVAE outperforms baseline methods in both the accuracy and diversity of generations.",
      "7 Acknowledgments We thank the anonymous reviewers for their constructive comments, and gratefully acknowl- edge the support of the National Key Re- search and Development Program of China (SQ2018AAA010010), the National Key Re- search and Development Program of China (2018YFB1005103), the National Natural Science Foundation of China (NSFC) via Grant 61702137. References Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473. Samuel R Bowman, Luke Vilnis, Oriol Vinyals, An- drew M Dai, Rafal Jozefowicz, and Samy Ben- gio. 2015. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349. Nathanael Chambers and Dan Jurafsky. 2008. Unsu- pervised learning of narrative event chains. Proc. of ACL-08: HLT, pages 789\u2013797.",
      "arXiv preprint arXiv:1511.06349. Nathanael Chambers and Dan Jurafsky. 2008. Unsu- pervised learning of narrative event chains. Proc. of ACL-08: HLT, pages 789\u2013797. Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceed- ings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers), pages 889\u2013898. Dan Goldwasser and Xiao Zhang. 2016. Understand- ing satirical articles using common-sense. Transac- tions of the Association for Computational Linguis- tics, 4:537\u2013549. Yichen Gong, Heng Luo, and Jian Zhang. 2017. Natu- ral language inference over interaction space. arXiv preprint arXiv:1709.04348. Mark Granroth-Wilding and Stephen Clark. 2016. What happens next? event prediction using a com- positional neural network model.",
      "Natu- ral language inference over interaction space. arXiv preprint arXiv:1709.04348. Mark Granroth-Wilding and Stephen Clark. 2016. What happens next? event prediction using a com- positional neural network model. In AAAI. Jian Guan, Yansen Wang, and Minlie Huang. 2019. Story ending generation with incremental encoding and commonsense knowledge. Ting-Hao Kenneth Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Ja- cob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al. 2016. Visual storytelling. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 1233\u20131239. D Kinga and J Ba Adam. 2015. A method for stochas- tic optimization. In ICLR, volume 5. Diederik P Kingma and Max Welling. 2014.",
      "D Kinga and J Ba Adam. 2015. A method for stochas- tic optimization. In ICLR, volume 5. Diederik P Kingma and Max Welling. 2014. Auto- encoding variational bayes. stat, 1050:10. Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016. A diversity-promoting objec- tive function for neural conversation models. In Pro- ceedings of NAACL-HLT, pages 110\u2013119. Zhongyang Li, Xiao Ding, and Ting Liu. 2018. Con- structing narrative event evolutionary graph for script event prediction. In Proceedings of the 27th International Joint Conference on Arti\ufb01cial Intelli- gence, pages 4201\u20134207. AAAI Press. Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose- worthy, Laurent Charlin, and Joelle Pineau. 2016.",
      "AAAI Press. Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose- worthy, Laurent Charlin, and Joelle Pineau. 2016. How not to evaluate your dialogue system: An em- pirical study of unsupervised evaluation metrics for dialogue response generation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2122\u20132132. Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A cor- pus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the NAACL: Human Language Tech- nologies, pages 839\u2013849. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proc. of the 40th ACL, pages 311\u2013318.",
      "Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proc. of the 40th ACL, pages 311\u2013318. Ankur Parikh, Oscar T\u00a8ackstr\u00a8om, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In Proceed- ings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2249\u20132255. Haoruo Peng, Snigdha Chaturvedi, and Dan Roth. 2017. A joint model for semantic sequences: Frames, entities, sentiments. In Proceedings of the 21st Conference on Computational Natural Lan- guage Learning (CoNLL 2017), pages 173\u2013183. Hannah Rashkin, Maarten Sap, Emily Allaway, Noah A Smith, and Yejin Choi. 2018. Event2mind: Commonsense inference on events, intents, and re- actions. In Proc.",
      "Hannah Rashkin, Maarten Sap, Emily Allaway, Noah A Smith, and Yejin Choi. 2018. Event2mind: Commonsense inference on events, intents, and re- actions. In Proc. of the 56th ACL, volume 1, pages 463\u2013473. Maarten Sap, Ronan LeBras, Emily Allaway, Chan- dra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A Smith, and Yejin Choi. 2018. Atomic: An atlas of machine commonsense for if- then reasoning. arXiv preprint arXiv:1811.00146. Roxane Segers, Marco Rospocher, Piek Vossen, Egoitz Laparra, German Rigau, and Anne-Lyse Minard. 2016. The event and implied situation ontol- ogy (eso): Application and evaluation. In Pro- ceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016), Paris, France. European Language Resources Asso- ciation (ELRA).",
      "Iulian V Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, and Joelle Pineau. 2016. Building end-to-end dialogue systems using generative hier- archical neural network models. In Thirtieth AAAI Conference on Arti\ufb01cial Intelligence. Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron Courville, and Yoshua Bengio. 2017. A hierarchical latent variable encoder-decoder model for generating dialogues. In AAAI. Kihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015. Learning structured output representation using deep conditional generative models. In Advances in neural information processing systems, pages 3483\u2013 3491. Jinsong Su, Shan Wu, Deyi Xiong, Yaojie Lu, Xianpei Han, and Biao Zhang. 2018. Variational recurrent neural machine translation. In AAAI. Wenlin Wang, Zhe Gan, Hongteng Xu, Ruiyi Zhang, and Lawrence Carin. 2019.",
      "2018. Variational recurrent neural machine translation. In AAAI. Wenlin Wang, Zhe Gan, Hongteng Xu, Ruiyi Zhang, and Lawrence Carin. 2019. Topic-guided variational autoencoders for text generation. Zhongqing Wang, Yue Zhang, and Ching-Yun Chang. 2017. Integrating order information and event rela- tion for script event prediction. In Proc. of the 2017 EMNLP, pages 57\u201367. Tsung-Hsien Wen, Yishu Miao, Phil Blunsom, and Steve Young. 2017. Latent intention dialogue mod- els. In Proc. of the 34th ICML-Volume 70, pages 3732\u20133741. JMLR. org. Biao Zhang, Deyi Xiong, Hong Duan, Min Zhang, et al. 2016. Variational neural machine translation. In Proc. of the 2016 EMNLP, pages 521\u2013530. Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi.",
      "2016. Variational neural machine translation. In Proc. of the 2016 EMNLP, pages 521\u2013530. Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi. 2017. Learning discourse-level diversity for neural dialog models using conditional variational autoen- coders. In Proceedings of the 55th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 654\u2013664."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1909.08824.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":10175,
  "avg_doclen":175.4310344828,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1909.08824.pdf"
    }
  }
}