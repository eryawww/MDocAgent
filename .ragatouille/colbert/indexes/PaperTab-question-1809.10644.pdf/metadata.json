{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "arXiv:1809.10644v1  [cs.CL]  27 Sep 2018 Predictive Embeddings for Hate Speech Detection on Twitter Rohan Kshirsagar1 Tyrus Cukuvac1 Kathleen McKeown1 Susan McGregor2 1Department of Computer Science at Columbia University 2School of Journalism at Columbia University rmk2161@columbia.edu thc2125@columbia.edu kathy@cs.columbia.edu sem2196@columbia.edu Abstract We present a neural-network based approach to classifying online hate speech in general, as well as racist and sexist speech in par- ticular. Using pre-trained word embeddings and max/mean pooling from simple, fully- connected transformations of these embed- dings, we are able to predict the occurrence of hate speech on three commonly used publicly available datasets. Our models match or out- perform state of the art F1 performance on all three datasets using signi\ufb01cantly fewer param- eters and minimal feature preprocessing com- pared to previous methods.",
      "Our models match or out- perform state of the art F1 performance on all three datasets using signi\ufb01cantly fewer param- eters and minimal feature preprocessing com- pared to previous methods. 1 Introduction The increasing popularity of social media plat- forms like Twitter for both personal and politi- cal communication (Lapowsky, 2017) has seen a well-acknowledged rise in the presence of toxic and abusive speech on these platforms (Hillard, 2018; Drum, 2017). Although the terms of ser- vices on these platforms typically forbid hate- ful and harassing speech, enforcing these rules has proved challenging, as identifying hate speech speech at scale is still a largely unsolved prob- lem in the NLP community. Waseem and Hovy (2016), for example, identify many ambiguities in classifying abusive communications, and high- light the dif\ufb01culty of clearly de\ufb01ning the parame- ters of such speech. This problem is compounded by the fact that identifying abusive or harassing speech is a challenge for humans as well as au- tomated systems.",
      "This problem is compounded by the fact that identifying abusive or harassing speech is a challenge for humans as well as au- tomated systems. Despite the lack of consensus around what con- stitutes abusive speech, some de\ufb01nition of hate speech must be used to build automated systems to address it. We rely on Davidson et al. (2017)\u2019s de\ufb01nition of hate speech, speci\ufb01cally: \u201clanguage that is used to express hatred towards a targeted group or is intended to be derogatory, to humili- ate, or to insult the members of the group.\u201d In this paper, we present a neural classi\ufb01cation system that uses minimal preprocessing to take ad- vantage of a modi\ufb01ed Simple Word Embeddings- based Model (Shen et al., 2018) to predict the oc- currence of hate speech.",
      "Our classi\ufb01er features: \u2022 A simple deep learning approach with few parameters enabling quick and robust train- ing \u2022 Signi\ufb01cantly better performance than two other state of the art methods on publicly available datasets \u2022 An interpretable approach facilitating analy- sis of results In the following sections, we discuss related work on hate speech classi\ufb01cation, followed by a description of the datasets, methods and results of our study. 2 Related Work Many efforts have been made to classify hate speech using data scraped from online message fo- rums and popular social media sites such as Twit- ter and Facebook. Waseem and Hovy (2016) ap- plied a logistic regression model that used one- to four-character n-grams for classi\ufb01cation of tweets labeled as racist, sexist or neither. Davidson et al. (2017) experimented in classi\ufb01cation of hateful as well as offensive but not hateful tweets. They ap- plied a logistic regression classi\ufb01er with L2 reg- ularization using word level n-grams and various part-of-speech, sentiment, and tweet-level meta- data features.",
      "They ap- plied a logistic regression classi\ufb01er with L2 reg- ularization using word level n-grams and various part-of-speech, sentiment, and tweet-level meta- data features. Additional projects have built upon the data sets created by Waseem and/or Davidson. For exam- ple, Park and Fung (2017) used a neural network",
      "approach with two binary classi\ufb01ers: one to pre- dict the presence abusive speech more generally, and another to discern the form of abusive speech. Zhang et al. (2018), meanwhile, used pre- trained word2vec embeddings, which were then fed into a convolutional neural network (CNN) with max pooling to produce input vectors for a Gated Recurrent Unit (GRU) neural network. Other researchers have experimented with us- ing metadata features from tweets. Founta et al. (2018) built a classi\ufb01er composed of two sepa- rate neural networks, one for the text and the other for metadata of the Twitter user, that were trained jointly in interleaved fashion. Both networks used in combination - and especially when trained us- ing transfer learning - achieved higher F1 scores than either neural network classi\ufb01er alone. In contrast to the methods described above, our approach relies on a simple word embedding (SWEM)-based architecture (Shen et al., 2018), reducing the number of required parameters and length of training required, while still yielding im- proved performance and resilience across related classi\ufb01cation tasks.",
      "In contrast to the methods described above, our approach relies on a simple word embedding (SWEM)-based architecture (Shen et al., 2018), reducing the number of required parameters and length of training required, while still yielding im- proved performance and resilience across related classi\ufb01cation tasks. Moreover, our network is able to learn \ufb02exible vector representations that demon- strate associations among words typically used in hateful communication. Finally, while metadata- based augmentation is intriguing, here we sought to develop an approach that would function well even in cases where such additional data was miss- ing due to the deletion, suspension, or deactivation of accounts. 3 Data In this paper, we use three data sets from the liter- ature to train and evaluate our own classi\ufb01er. Al- though all address the category of hateful speech, they used different strategies of labeling the col- lected data. Table 1 shows the characteristics of the datasets.",
      "Al- though all address the category of hateful speech, they used different strategies of labeling the col- lected data. Table 1 shows the characteristics of the datasets. Data collected by Waseem and Hovy (2016), which we term the Sexist/Racist (SR) data set1, was collected using an initial Twitter search fol- lowed by analysis and \ufb01ltering by the authors and their team who identi\ufb01ed 17 common phrases, hashtags, and users that were indicative of abu- sive speech. Davidson et al. (2017) collected the HATE dataset by searching for tweets using a lexicon provided by Hatebase.org.",
      "Davidson et al. (2017) collected the HATE dataset by searching for tweets using a lexicon provided by Hatebase.org. The \ufb01nal data 1Some Tweet IDs/users have been deleted since the cre- ation, so the total number may differ from the original Dataset Labels and Counts Total SR Sexist Racist Neither 3086 1924 10,898 15,908 HATE Hate Speech Not Hate Speech 1430 23,353 24,783 HAR Harassment Non Harassing 5,285 15,075 20,360 Table 1: Dataset Characteristics set we used, which we call HAR, was collected by Golbeck et al. (2017); we removed all retweets re- ducing the dataset to 20,000 tweets. Tweets were labeled as \u201cHarrassing\u201d or \u201cNon-Harrassing\u201d; hate speech was not explicitly labeled, but treated as an unlabeled subset of the broader \u201cHarrassing\u201d category(Golbeck et al., 2017).",
      "Tweets were labeled as \u201cHarrassing\u201d or \u201cNon-Harrassing\u201d; hate speech was not explicitly labeled, but treated as an unlabeled subset of the broader \u201cHarrassing\u201d category(Golbeck et al., 2017). 4 Transformed Word Embedding Model (TWEM) Our training set consists of N examples {Xi, Y i}N i=1 where the input Xi is a sequence of tokens w1, w2, ..., wT , and the output Y i is the numerical class for the hate speech class. Each input instance represents a Twitter post and thus, is not limited to a single sentence. We modify the SWEM-concat (Shen et al., 2018) architecture to allow better handling of in- frequent and unknown words and to capture non- linear word combinations. 4.1 Word Embeddings Each token in the input is mapped to an embed- ding. We used the 300 dimensional embeddings for all our experiments, so each word wt is mapped to xt \u2208R300. We denote the full embedded se- quence as x1:T .",
      "4.1 Word Embeddings Each token in the input is mapped to an embed- ding. We used the 300 dimensional embeddings for all our experiments, so each word wt is mapped to xt \u2208R300. We denote the full embedded se- quence as x1:T . We then transform each word embedding by applying 300 dimensional 1-layer Multi Layer Perceptron (MLP) Wt with a Recti- \ufb01ed Liner Unit (ReLU) activation to form an up- dated embedding space z1:T . We \ufb01nd this bet- ter handles unseen or rare tokens in our training data by projecting the pretrained embedding into a space that the encoder can understand. 4.2 Pooling We make use of two pooling methods on the up- dated embedding space z1:T . We employ a max pooling operation on z1:T to capture salient word",
      "features from our input; this representation is de- noted as m. This forces words that are highly indicative of hate speech to higher positive val- ues within the updated embedding space. We also average the embeddings z1:T to capture the over- all meaning of the sentence, denoted as a, which provides a strong conditional factor in conjunction with the max pooling output. This also helps reg- ularize gradient updates from the max pooling op- eration. 4.3 Output We concatenate a and m to form a document rep- resentation d and feed the representation into a 50 node 2 layer MLP followed by ReLU Activation to allow for increased nonlinear representation learn- ing. This representation forms the preterminal layer and is passed to a fully connected softmax layer whose output is the probability distribution over labels. 5 Experimental Setup We tokenize the data using Spacy (Honnibal and Johnson, 2015). We use 300 Dimensional Glove Common Crawl Embeddings (840B Token) (Pennington et al., 2014) and \ufb01ne tune them for the task.",
      "5 Experimental Setup We tokenize the data using Spacy (Honnibal and Johnson, 2015). We use 300 Dimensional Glove Common Crawl Embeddings (840B Token) (Pennington et al., 2014) and \ufb01ne tune them for the task. We experimented extensively with pre-processing variants and our results showed better performance without lemmatization and lower-casing (see supplement for details). We pad each input to 50 words. We train using RMSprop with a learning rate of .001 and a batch size of 512. We add dropout with a drop rate of 0.1 in the \ufb01nal layer to reduce over\ufb01tting (Srivastava et al., 2014), batch size, and input length empirically through random hyperparameter search. All of our results are produced from 10-fold cross validation to allow comparison with previ- ous results. We trained a logistic regression base- line model (line 1 in Table 2) using character ngrams and word unigrams using TF*IDF weight- ing (Salton and Buckley, 1987), to provide a base- line since HAR has no reported results.",
      "We trained a logistic regression base- line model (line 1 in Table 2) using character ngrams and word unigrams using TF*IDF weight- ing (Salton and Buckley, 1987), to provide a base- line since HAR has no reported results. For the SR and HATE datasets, the authors reported their trained best logistic regression model\u2019s2 results on their respective datasets.",
      "For the SR and HATE datasets, the authors reported their trained best logistic regression model\u2019s2 results on their respective datasets. 2Features described in Related Works section 3SR: Sexist/Racist (Waseem and Hovy, 2016), HATE: Hate (Davidson et al., 2017) HAR: Harassment (Golbeck et al., 2017) Method SR HATE HAR LR(Char-gram + Unigram) 0.79 0.85 0.68 LR(Waseem and Hovy, 2016) 0.74 - - LR (Davidson et al., 2017) - 0.90 - CNN (Park and Fung, 2017) 0.83 - - GRU Text (Founta et al., 2018) 0.83 0.89 - GRU Text + Metadata 0.87 0.89 - TWEM (Ours) 0.86 0.924 0.71 Table 2: F1 Results3 6 Results and Discussion The approach we have developed establishes a new state of the art for classifying hate speech, outperforming previous results by as much as 12 F1 points.",
      "Table 2 illustrates the robustness of our method, which often outperform previous results, measured by weighted F1. 4 Using the Approximate Randomization (AR) Test (Riezler and Maxwell, 2005), we perform signi\ufb01cance testing using a 75/25 train and test split to compare against (Waseem and Hovy, 2016) and (Davidson et al., 2017), whose models we re- implemented. We found 0.001 signi\ufb01cance com- pared to both methods. We also include in-depth precision and recall results for all three datasets in the supplement. Our results indicate better performance than several more complex approaches, including Davidson et al. (2017)\u2019s best model (which used word and part-of-speech ngrams, sentiment, readability, text, and Twitter speci\ufb01c features), Park and Fung (2017) (which used two fold classi- \ufb01cation and a hybrid of word and character CNNs, using approximately twice the parameters we use excluding the word embeddings) and even recent work by Founta et al.",
      "(2018), (whose best model relies on GRUs, metadata including popularity, network reciprocity, and subscribed lists). On the SR dataset, we outperform Founta et al. (2018)\u2019s text based model by 3 F1 points, while just falling short of the Text + Metadata Inter- leaved Training model. While we appreciate the potential added value of metadata, we believe a tweet-only classi\ufb01er has merits because retriev- 4This was used in previous work, as con\ufb01rmed by check- ing with authors",
      "ing features from the social graph is not always tractable in production settings. Excluding the em- bedding weights, our model requires 100k param- eters , while Founta et al. (2018) requires 250k pa- rameters. 6.1 Error Analysis False negatives5 Many of the false negatives we see are speci\ufb01c ref- erences to characters in the TV show \u201cMy Kitchen Rules\u201d, rather than something about women in general. Such examples may be innocuous in iso- lation but could potentially be sexist or racist in context. While this may be a limitation of consid- ering only the content of the tweet, it could also be a mislabel. Debra are now my most hated team on #mkr after least night\u2019s ep. Snakes in the grass those two. Along these lines, we also see correct predictions of innocuous speech, but \ufb01nd data mislabeled as hate speech: @LoveAndLonging ...how is that exam- ple \u201dsexism\u201d? @amberhasalamb ...in what way?",
      "Snakes in the grass those two. Along these lines, we also see correct predictions of innocuous speech, but \ufb01nd data mislabeled as hate speech: @LoveAndLonging ...how is that exam- ple \u201dsexism\u201d? @amberhasalamb ...in what way? Another case our classi\ufb01er misses is problematic speech within a hashtag: :D @nkrause11 Dudes who go to culi- nary school: #why #\ufb01ndawife #notsex- ist :) This limitation could be potentially improved through the use of character convolutions or sub- word tokenization. False Positives In certain cases, our model seems to be learning user names instead of semantic content: RT @GrantLeeStone: @MT8 9 I don\u2019t even know what that is, or where it\u2019s from. Was that supposed to be funny? It wasn\u2019t. Since the bulk of our model\u2019s weights are in the embedding and embedding-transformation matri- ces, we cluster the SR vocabulary using these transformed embeddings to clarify our intuitions 5Focused on the SR Dataset (Waseem and Hovy, 2016) about the model (8).",
      "Since the bulk of our model\u2019s weights are in the embedding and embedding-transformation matri- ces, we cluster the SR vocabulary using these transformed embeddings to clarify our intuitions 5Focused on the SR Dataset (Waseem and Hovy, 2016) about the model (8). We elaborate on our clus- tering approach in the supplement. We see that the model learned general semantic groupings of words associated with hate speech as well as spe- ci\ufb01c idiosyncrasies related to the dataset itself (e.g. katieandnikki) Cluster Tokens Geopolitical and religious refer- ences around Islam and the Middle East bomb, mobs, jewish, kidnapped, airstrikes, secularization, ghettoes, islamic, burnt, mur- derous, penal, traitor, intelligence, molesting, cannibalism Strong epithets and adjectives associated with harassment and hatespeech liberals, argumentative, dehumanize, gen- dered, stereotype, sociopath,bigot, repressed, judgmental, heinous, misandry, shameless, depravity, scumbag, Miscellaneous turnt, pedophelia, fricken, exfoliated,",
      "argumentative, dehumanize, gen- dered, stereotype, sociopath,bigot, repressed, judgmental, heinous, misandry, shameless, depravity, scumbag, Miscellaneous turnt, pedophelia, fricken, exfoliated, soci- olinguistic, proph, cissexism, guna, lyked, mobbing, capsicums, orajel, bitchslapped, venturebeat, hair\ufb02ip, mongodb, intersec- tional, agender Sexist related epi- thets and hashtags malnourished, katieandnikki, chevapi, dumb- slut, mansplainers, crazybitch, horrendous- ness, justhonest, bile, womenaretoohardtoan- imate, Sexist, sexual, and pornographic terms actress, feminism, skank, breasts, redhead, anime, bra, twat, chick, sluts, trollop, teenage, pantyhose, pussies, dyke, blonds, Table 3: Projected Embedding Cluster Analysis from SR Dataset 7 Conclusion Despite minimal tuning of hyper-parameters, fewer weight parameters, minimal text preprocess- ing,",
      "anime, bra, twat, chick, sluts, trollop, teenage, pantyhose, pussies, dyke, blonds, Table 3: Projected Embedding Cluster Analysis from SR Dataset 7 Conclusion Despite minimal tuning of hyper-parameters, fewer weight parameters, minimal text preprocess- ing, and no additional metadata, the model per- forms remarkably well on standard hate speech datasets. Our clustering analysis adds inter- pretability enabling inspection of results. Our results indicate that the majority of recent deep learning models in hate speech may rely on word embeddings for the bulk of predictive power and the addition of sequence-based param- eters provide minimal utility. Sequence based ap- proaches are typically important when phenom- ena such as negation, co-reference, and context- dependent phrases are salient in the text and thus, we suspect these cases are in the minority for pub- licly available datasets. We think it would be valuable to study the occurrence of such linguistic phenomena in existing datasets and construct new datasets that have a better representation of sub- tle forms of hate speech. In the future, we plan to investigate character based representations, using character CNNs and highway layers (Kim et al.,",
      "2016) along with word embeddings to allow robust representations for sparse words such as hashtags. References Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated hate speech detection and the problem of offensive language. arXiv preprint arXiv:1703.04009. Kevin Drum. 2017. Twitter Is a Cesspool, But It\u2019s Our Cesspool. Mother Jones. Antigoni-Maria Founta, Despoina Chatzakou, Nico- las Kourtellis, Jeremy Blackburn, Athena Vakali, and Ilias Leontiadis. 2018. A uni\ufb01ed deep learn- ing architecture for abuse detection. arXiv preprint arXiv:1802.00385. Jennifer Golbeck, Zahra Ashktorab, Rashad O Banjo, Alexandra Berlinger, Siddharth Bhagwan, Cody Buntain, Paul Cheakalos, Alicia A Geller, Quint Gergory, Rajesh Kumar Gnanasekaran, et al. 2017. A large labeled corpus for online harassment re- search.",
      "2017. A large labeled corpus for online harassment re- search. In Proceedings of the 2017 ACM on Web Science Conference, pages 229\u2013233. ACM. Graham Hillard. 2018. Stop Complaining about Twit- ter \u2014 Just Leave It. National Review. Matthew Honnibal and Mark Johnson. 2015. An im- proved non-monotonic transition system for depen- dency parsing. In Proceedings of the 2015 Con- ference on Empirical Methods in Natural Language Processing, pages 1373\u20131378,Lisbon, Portugal. As- sociation for Computational Linguistics. Ian T Jolliffe. 1986. Principal component analysis and factor analysis. In Principal component analysis, pages 115\u2013128. Springer. Yoon Kim, Yacine Jernite, David Sontag, and Alexan- der M Rush. 2016. Character-aware neural language models. In AAAI, pages 2741\u20132749. Issie Lapowsky. 2017. Trump faces lawsuit over his Twitter blocking habits. Wired.",
      "2016. Character-aware neural language models. In AAAI, pages 2741\u20132749. Issie Lapowsky. 2017. Trump faces lawsuit over his Twitter blocking habits. Wired. Ji Ho Park and Pascale Fung. 2017. One-step and two- step classi\ufb01cation for abusive language detection on twitter. CoRR, abs/1706.01206. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten- hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas- sos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830. Jeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. Glove: Global vectors for word representation.",
      "Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830. Jeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Nat- ural Language Processing (EMNLP), pages 1532\u2013 1543. Stefan Riezler and John T Maxwell. 2005. On some pitfalls in automatic evaluation and signi\ufb01cance test- ing for mt. In Proceedings of the ACL workshop on",
      "intrinsic and extrinsic evaluation measures for ma- chine translation and/or summarization, pages 57\u2013 64. Gerard Salton and Chris Buckley. 1987. Term weight- ing approaches in automatic text retrieval. Technical report, Cornell University. Dinghan Shen, Guoyin Wang, Wenlin Wang, Mar- tin Renqiang Min, Qinliang Su, Yizhe Zhang, Ri- cardo Henao, and Lawrence Carin. 2018. On the use of word embeddings alone to represent natural language sequences. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from over\ufb01tting. The Journal of Machine Learning Research, 15(1):1929\u20131958. Zeerak Waseem and Dirk Hovy. 2016. Hateful sym- bols or hateful people? predictive features for hate speech detection on twitter. In Proceedings of the NAACL student research workshop, pages 88\u201393.",
      "Zeerak Waseem and Dirk Hovy. 2016. Hateful sym- bols or hateful people? predictive features for hate speech detection on twitter. In Proceedings of the NAACL student research workshop, pages 88\u201393. Z. Zhang, D. Robinson, and J. Tepper. 2018. Detecting hate speech on twitter using a convolution-gru based deep neural network. c\u20dd2018 Springer Verlag. A Supplemental Material We experimented with several different prepro- cessing variants and were surprised to \ufb01nd that re- ducing preprocessing improved the performance on the task for all of our tasks. We go through each preprocessing variant with an example and then describe our analysis to compare and evalu- ate each of them. A.1 Preprocessing Original text RT @AGuyNamed Nick Now, I\u2019m not sexist in any way shape or form but I think women are better at gift wrapping. It\u2019s the XX chromosome thing Tokenize (Basic Tokenize: Keeps case and words intact with limited sanitizing) RT @AGuyNamed Nick Now , I \u2019m not sexist in any way shape or form but I think women are better at gift wrapping .",
      "It\u2019s the XX chromosome thing Tokenize (Basic Tokenize: Keeps case and words intact with limited sanitizing) RT @AGuyNamed Nick Now , I \u2019m not sexist in any way shape or form but I think women are better at gift wrapping . It \u2019s the XX chromosome thing Tokenize Lowercase: Lowercase the basic tok- enize scheme rt @aguynamed nick now , i \u2019m not sex- ist in any way shape or form but i think women are better at gift wrapping . it \u2019s the xx chromosome thing Token Replace: Replaces entities and user names with placeholder) ENT USER now , I \u2019m not sexist in any way shape or form but I think women are better at gift wrapping . It \u2019s the xx chromosome thing Token Replace Lowercase: Lowercase the To- ken Replace Scheme ENT USER now , i \u2019m not sexist in any way shape or form but i think women are better at gift wrapping . it \u2019s the xx chromosome thing We did analysis on a validation set across mul- tiple datasets to \ufb01nd that the \u201dTokenize\u201d scheme was by far the best. We believe that keeping the case in tact provides useful information about the user.",
      "it \u2019s the xx chromosome thing We did analysis on a validation set across mul- tiple datasets to \ufb01nd that the \u201dTokenize\u201d scheme was by far the best. We believe that keeping the case in tact provides useful information about the user. For example, saying something in all CAPS is a useful signal that the model can take advantage of.",
      "Preprocessing Scheme Avg. Validation Loss Token Replace Lowercase 0.47 Token Replace 0.46 Tokenize 0.32 Tokenize Lowercase 0.40 Table 4: Average Validation Loss for each Preprocess- ing Scheme A.2 In-Depth Results Waseem 2016 Ours P R F1 P R F1 none 0.76 0.98 0.86 0.88 0.93 0.90 sexism 0.95 0.38 0.54 0.79 0.74 0.76 racism 0.85 0.30 0.44 0.86 0.72 0.78 0.74 0.86 Table 5: SR Results Davidson 2017 Ours P R F1 P R F1 none 0.82 0.95 0.88 0.89 0.94 0.91 offensive 0.96 0.91 0.93 0.95 0.96 0.96 hate 0.44 0.61 0.51 0.61 0.41 0.49 0.",
      "89 0.94 0.91 offensive 0.96 0.91 0.93 0.95 0.96 0.96 hate 0.44 0.61 0.51 0.61 0.41 0.49 0.90 0.924 Table 6: HATE Results Method Prec Rec F1 Avg F1 Ours 0.713 0.206 0.319 0.711 LR Baseline 0.820 0.095 0.170 0.669 Table 7: HAR Results A.3 Embedding Analysis Since our method was a simple word embedding based model, we explored the learned embedding space to analyze results. For this analysis, we only use the max pooling part of our architecture to help analyze the learned embedding space because it encourages salient words to increase their val- ues to be selected. We projected the original pre- trained embeddings to the learned space using the time distributed MLP. We summed the embedding dimensions for each word and sorted by the sum in descending order to \ufb01nd the 1000 most salient word embeddings from our vocabulary.",
      "We projected the original pre- trained embeddings to the learned space using the time distributed MLP. We summed the embedding dimensions for each word and sorted by the sum in descending order to \ufb01nd the 1000 most salient word embeddings from our vocabulary. We then ran PCA (Jolliffe, 1986) to reduce the dimension- ality of the projected embeddings from 300 dimen- sions to 75 dimensions. This captured about 60% of the variance. Finally, we ran K means cluster- ing for k = 5 clusters to organize the most salient embeddings in the projected space. The learned clusters from the SR vocabulary were very illuminating (see Table 8); they gave in- sights to how hate speech surfaced in the datasets. One clear grouping we found is the misogynistic and pornographic group, which contained words like breasts, blonds, and skank. Two other clusters had references to geopolitical and religious issues in the Middle East and disparaging and resentful epithets that could be seen as having an intellec- tual tone. This hints towards the subtle pedagogic forms of hate speech that surface.",
      "Two other clusters had references to geopolitical and religious issues in the Middle East and disparaging and resentful epithets that could be seen as having an intellec- tual tone. This hints towards the subtle pedagogic forms of hate speech that surface. We ran silhouette analysis (Pedregosa et al., 2011) on the learned clusters to \ufb01nd that the clus- ters from the learned representations had a 35% higher silhouette coef\ufb01cient using the projected embeddings compared to the clusters created from the original pre-trained embeddings. This rein- forces the claim that our training process pushed hate-speech related words together, and words from other clusters further away, thus, structuring the embedding space effectively for detecting hate speech. Cluster Tokens Geopolitical and religious refer- ences around Islam and the Middle East bomb, mobs, jewish, kidnapped, airstrikes, secularization, ghettoes, islamic, burnt, mur- derous, penal, traitor, intelligence, molesting, cannibalism Strong epithets and adjectives associated with harassment and hatespeech liberals, argumentative, dehumanize, gen- dered, stereotype, sociopath,bigot, repressed, judgmental, heinous,",
      "burnt, mur- derous, penal, traitor, intelligence, molesting, cannibalism Strong epithets and adjectives associated with harassment and hatespeech liberals, argumentative, dehumanize, gen- dered, stereotype, sociopath,bigot, repressed, judgmental, heinous, misandry, shameless, depravity, scumbag, Miscellaneous turnt, pedophelia, fricken, exfoliated, soci- olinguistic, proph, cissexism, guna, lyked, mobbing, capsicums, orajel, bitchslapped, venturebeat, hair\ufb02ip, mongodb, intersec- tional, agender Sexist related epi- thets and hashtags malnourished, katieandnikki, chevapi, dumb- slut, mansplainers, crazybitch, horrendous- ness, justhonest, bile, womenaretoohardtoan- imate, Sexist, sexual, and pornographic terms actress, feminism, skank, breasts, redhead, anime, bra, twat, chick, sluts, trollop, teenage, pantyhose, pussies, dyke, blonds,",
      "bile, womenaretoohardtoan- imate, Sexist, sexual, and pornographic terms actress, feminism, skank, breasts, redhead, anime, bra, twat, chick, sluts, trollop, teenage, pantyhose, pussies, dyke, blonds, Table 8: Projected Embedding Cluster Analysis from SR Dataset",
      "arXiv:1809.10644v1  [cs.CL]  27 Sep 2018 1 000 001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053 054 055 056 057 058 059 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 Con\ufb01dential Review Copy. DO NOT DISTRIBUTE.",
      "DO NOT DISTRIBUTE. 1 Supplemental Material We experimented with several different prepro- cessing variants and were surprised to \ufb01nd that re- ducing preprocessing improved the performance on the task for all of our tasks. We go through each preprocessing variant with an example and then describe our analysis to compare and evalu- ate each of them. 1.1 Preprocessing Original text RT @AGuyNamed Nick Now, I\u2019m not sexist in any way shape or form but I think women are better at gift wrapping. It\u2019s the XX chromosome thing Tokenize (Basic Tokenize: Keeps case and words intact with limited sanitizing) RT @AGuyNamed Nick Now , I \u2019m not sexist in any way shape or form but I think women are better at gift wrapping . It \u2019s the XX chromosome thing Tokenize Lowercase: Lowercase the basic tok- enize scheme rt @aguynamed nick now , i \u2019m not sex- ist in any way shape or form but i think women are better at gift wrapping .",
      "It \u2019s the XX chromosome thing Tokenize Lowercase: Lowercase the basic tok- enize scheme rt @aguynamed nick now , i \u2019m not sex- ist in any way shape or form but i think women are better at gift wrapping . it \u2019s the xx chromosome thing Token Replace: Replaces entities and user names with placeholder) ENT USER now , I \u2019m not sexist in any way shape or form but I think women are better at gift wrapping . It \u2019s the xx chromosome thing Token Replace Lowercase: Lowercase the To- ken Replace Scheme ENT USER now , i \u2019m not sexist in any way shape or form but i think women are better at gift wrapping . it \u2019s the xx chromosome thing We did analysis on a validation set across mul- tiple datasets to \ufb01nd that the \u201dTokenize\u201d scheme was by far the best. We believe that keeping the case in tact provides useful information about the user. For example, saying something in all CAPS is a useful signal that the model can take advantage of. Preprocessing Scheme Avg.",
      "We believe that keeping the case in tact provides useful information about the user. For example, saying something in all CAPS is a useful signal that the model can take advantage of. Preprocessing Scheme Avg. Validation Loss Token Replace Lowercase 0.47 Token Replace 0.46 Tokenize 0.32 Tokenize Lowercase 0.40 Table 1: Average Validation Loss for each Preprocess- ing Scheme 1.2 In-Depth Results Waseem ? Ours P R F1 P R F1 none 0.76 0.98 0.86 0.88 0.93 0.90 sexism 0.95 0.38 0.54 0.79 0.74 0.76 racism 0.85 0.30 0.44 0.86 0.72 0.78 0.74 0.86 Table 2: SR Results Davidson ?",
      "Ours P R F1 P R F1 none 0.82 0.95 0.88 0.89 0.94 0.91 offensive 0.96 0.91 0.93 0.95 0.96 0.96 hate 0.44 0.61 0.51 0.61 0.41 0.49 0.90 0.924 Table 3: HATE Results Method Prec Rec F1 Avg F1 Ours 0.713 0.206 0.319 0.711 LR Baseline 0.820 0.095 0.170 0.669 Table 4: HAR Results 1.3 Embedding Analysis Since our method was a simple word embedding based model, we explored the learned embedding space to analyze results. For this analysis, we only use the max pooling part of our architecture to help analyze the learned embedding space because it encourages salient words to increase their val- ues to be selected. We projected the original pre- trained embeddings to the learned space using the time distributed MLP. We summed the embedding",
      "2 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 Con\ufb01dential Review Copy. DO NOT DISTRIBUTE. dimensions for each word and sorted by the sum in descending order to \ufb01nd the 1000 most salient word embeddings from our vocabulary. We then ran PCA (?)",
      "DO NOT DISTRIBUTE. dimensions for each word and sorted by the sum in descending order to \ufb01nd the 1000 most salient word embeddings from our vocabulary. We then ran PCA (?) to reduce the dimensionality of the projected embeddings from 300 dimensions to 75 dimensions. This captured about 60% of the vari- ance. Finally, we ran K means clustering for k = 5 clusters to organize the most salient embeddings in the projected space. The learned clusters from the SR vocabulary were very illuminating (see Table 5); they gave in- sights to how hate speech surfaced in the datasets. One clear grouping we found is the misogynistic and pornographic group, which contained words like breasts, blonds, and skank. Two other clusters had references to geopolitical and religious issues in the Middle East and disparaging and resentful epithets that could be seen as having an intellec- tual tone. This hints towards the subtle pedagogic forms of hate speech that surface. We ran silhouette analysis (?)",
      "Two other clusters had references to geopolitical and religious issues in the Middle East and disparaging and resentful epithets that could be seen as having an intellec- tual tone. This hints towards the subtle pedagogic forms of hate speech that surface. We ran silhouette analysis (?) on the learned clusters to \ufb01nd that the clusters from the learned representations had a 35% higher silhouette coef- \ufb01cient using the projected embeddings compared to the clusters created from the original pre-trained embeddings. This reinforces the claim that our training process pushed hate-speech related words together, and words from other clusters further away, thus, structuring the embedding space effec- tively for detecting hate speech. Cluster Tokens Geopolitical and religious refer- ences around Islam and the Middle East bomb, mobs, jewish, kidnapped, airstrikes, secularization, ghettoes, islamic, burnt, mur- derous, penal, traitor, intelligence, molesting, cannibalism Strong epithets and adjectives associated with harassment and hatespeech liberals, argumentative, dehumanize, gen- dered, stereotype, sociopath,bigot, repressed, judgmental, heinous, misandry, shameless, depravity,",
      "traitor, intelligence, molesting, cannibalism Strong epithets and adjectives associated with harassment and hatespeech liberals, argumentative, dehumanize, gen- dered, stereotype, sociopath,bigot, repressed, judgmental, heinous, misandry, shameless, depravity, scumbag, Miscellaneous turnt, pedophelia, fricken, exfoliated, soci- olinguistic, proph, cissexism, guna, lyked, mobbing, capsicums, orajel, bitchslapped, venturebeat, hair\ufb02ip, mongodb, intersec- tional, agender Sexist related epi- thets and hashtags malnourished, katieandnikki, chevapi, dumb- slut, mansplainers, crazybitch, horrendous- ness, justhonest, bile, womenaretoohardtoan- imate, Sexist, sexual, and pornographic terms actress, feminism, skank, breasts, redhead, anime, bra, twat, chick, sluts, trollop, teenage, pantyhose, pussies, dyke, blonds,",
      "bile, womenaretoohardtoan- imate, Sexist, sexual, and pornographic terms actress, feminism, skank, breasts, redhead, anime, bra, twat, chick, sluts, trollop, teenage, pantyhose, pussies, dyke, blonds, Table 5: Projected Embedding Cluster Analysis from SR Dataset"
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1809.10644.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":8148,
  "avg_doclen":177.1304347826,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1809.10644.pdf"
    }
  }
}