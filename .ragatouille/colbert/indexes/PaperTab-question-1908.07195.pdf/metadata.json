{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "ARAML: A Stable Adversarial Training Framework for Text Generation Pei Ke\u2217, Fei Huang\u2217, Minlie Huang\u2020, Xiaoyan Zhu Institute for Arti\ufb01cial Intelligence, State Key Lab of Intelligent Technology and Systems Beijing National Research Center for Information Science and Technology Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China kepei1106@outlook.com, f-huang18@mails.tsinghua.edu.cn aihuang@tsinghua.edu.cn, zxy-dcs@tsinghua.edu.cn Abstract Most of the existing generative adversarial net- works (GAN) for text generation suffer from the instability of reinforcement learning train- ing algorithms such as policy gradient, leading to unstable performance. To tackle this prob- lem, we propose a novel framework called Ad- versarial Reward Augmented Maximum Like- lihood (ARAML). During adversarial train- ing, the discriminator assigns rewards to sam- ples which are acquired from a stationary dis- tribution near the data rather than the gen- erator\u2019s distribution. The generator is opti- mized with maximum likelihood estimation augmented by the discriminator\u2019s rewards in- stead of policy gradient.",
      "The generator is opti- mized with maximum likelihood estimation augmented by the discriminator\u2019s rewards in- stead of policy gradient. Experiments show that our model can outperform state-of-the-art text GANs with a more stable training process. 1 Introduction Natural text generation, as a key task in NLP, has been advanced substantially thanks to the \ufb02ourish of neural models (Bengio et al., 2003; Mikolov et al., 2010). Typical frameworks such as sequence-to-sequence (seq2seq) have been ap- plied to various generation tasks, including ma- chine translation (Sutskever et al., 2014) and di- alogue generation (Vinyals and Le, 2015). The standard paradigm to train such neural models is maximum likelihood estimation (MLE), which maximizes the log-likelihood of observing each word in the text given the ground-truth proceed- ing context (Graves, 2013).",
      "The standard paradigm to train such neural models is maximum likelihood estimation (MLE), which maximizes the log-likelihood of observing each word in the text given the ground-truth proceed- ing context (Graves, 2013). Although widely used, MLE suffers from the exposure bias problem (Bengio et al., 2015; Ran- zato et al., 2016): during test, the model sequen- tially predicts the next word conditioned on its pre- vious generated words while during training con- ditioned on ground-truth words.",
      "To tackle this \u2217Equal contribution \u2020 Corresponding author: Minlie Huang problem, generative adversarial networks (GAN) with reinforcement learning (RL) training ap- proaches have been introduced to text generation tasks (Yu et al., 2017; Che et al., 2017; Lin et al., 2017; Fedus et al., 2018; Guo et al., 2018; Shi et al., 2018; Xu et al., 2018), where the discrim- inator is trained to distinguish real and generated text samples to provide reward signals for the gen- erator, and the generator is optimized via policy gradient (Yu et al., 2017). However, recent studies have shown that poten- tial issues of training GANs on discrete data are more severe than exposure bias (Semeniuta1 et al., 2018; Caccia et al., 2018). One of the fundamental issues when generating discrete text samples with GANs is training instability.",
      "One of the fundamental issues when generating discrete text samples with GANs is training instability. Updating the gener- ator with policy gradient always leads to an un- stable training process because it\u2019s dif\ufb01cult for the generator to derive positive and stable reward sig- nals from the discriminator even with careful pre- training (Che et al., 2017). As a result, the genera- tor gets lost due to the high variance of reward sig- nals and the training process may \ufb01nally collapse (Li et al., 2017). In this paper, we propose a novel adversar- ial training framework called Adversarial Reward Augmented Maximum Likelihood (ARAML) to deal with the instability issue of training GANs for text generation. At each iteration of adversarial training, we \ufb01rst train the discriminator to assign higher rewards to real data than to generated sam- ples. Then, inspired by reward augmented max- imum likelihood (RAML) (Norouzi et al., 2016), the generator is updated on the samples acquired from a stationary distribution with maximum like- lihood estimation (MLE), weighted by the dis- criminator\u2019s rewards.",
      "Then, inspired by reward augmented max- imum likelihood (RAML) (Norouzi et al., 2016), the generator is updated on the samples acquired from a stationary distribution with maximum like- lihood estimation (MLE), weighted by the dis- criminator\u2019s rewards. This stationary distribution is designed to guarantee that training samples are surrounding the real data, thus the exploration space of our generator is indeed restricted by the arXiv:1908.07195v1  [cs.CL]  20 Aug 2019",
      "MLE training objective, resulting in more stable training. Compared to other text GANs with RL training techniques, our framework acquires sam- ples from the stationary distribution rather than the generator\u2019s distribution, and uses RAML training paradigm to optimize the generator instead of pol- icy gradient. Our contributions are mainly as fol- lows: \u2022 We analyze the fundamental issue of current GANs for text generation from the perspec- tives of training instability. \u2022 We propose a novel framework called Adver- sarial Reward Augmented Maximum Like- lihood (ARAML), which incorporates sta- ble RAML training into adversarial training paradigm. Experimental results on three text generation tasks show the effectiveness of our method. 2 Related Work Recently, text generation has been widely studied with neural models trained with maximum likeli- hood estimation (Graves, 2013). However, MLE tends to generate universal text (Li et al., 2016).",
      "2 Related Work Recently, text generation has been widely studied with neural models trained with maximum likeli- hood estimation (Graves, 2013). However, MLE tends to generate universal text (Li et al., 2016). Various methods have been proposed to enhance the generation quality by re\ufb01ning the objective function (Li et al., 2016; Mou et al., 2016) or mod- ifying the generation distribution with external in- formation like topic (Xing et al., 2017), sentence type (Ke et al., 2018), emotion (Zhou et al., 2018a) and knowledge (Zhou et al., 2018b). As mentioned above, MLE suffers from the ex- posure bias problem (Bengio et al., 2015; Ranzato et al., 2016). Thus, reinforcement learning has been introduced to text generation tasks such as policy gradient (Ranzato et al., 2016) and actor- critic (Bahdanau et al., 2017).",
      "Thus, reinforcement learning has been introduced to text generation tasks such as policy gradient (Ranzato et al., 2016) and actor- critic (Bahdanau et al., 2017). (Norouzi et al., 2016) proposed an ef\ufb01cient and stable approach called Reward Augmented Maximum Likelihood (RAML), which connects the log-likelihood and expected rewards to incorporate MLE training ob- jective into RL framework. Since some text generation tasks have no ex- plicit metrics to be directly optimized, adversar- ial training has been applied to generating discrete text samples with a discriminator to learn a proper reward. For instance, SeqGAN (Yu et al., 2017) devised a discriminator to distinguish the real data and generated samples, and a generator to max- imize the reward from the discriminator via pol- icy gradient. Other variants of GANs have been proposed to improve the generator or the discrimi- nator. To improve the generator, MaliGAN (Che et al., 2017) developed a normalized maximum likelihood optimization target for the generator to stably model the discrete sequences.",
      "Other variants of GANs have been proposed to improve the generator or the discrimi- nator. To improve the generator, MaliGAN (Che et al., 2017) developed a normalized maximum likelihood optimization target for the generator to stably model the discrete sequences. LeakGAN (Guo et al., 2018) guided the generator with re- ward signals leaked from the discriminator at all generation steps to deal with long text generation task. MaskGAN (Fedus et al., 2018) employed an actor-critic architecture to make the generator \ufb01ll in missing text conditioned on the surrounding context, which is expected to mitigate the prob- lem of mode collapse. As for the discriminator, RankGAN (Lin et al., 2017) replaced traditional discriminator with a ranker to learn the relative ranking information between the real texts and generated ones. Inverse reinforcement learning (Shi et al., 2018) used a trainable reward approxi- mator as the discriminator to provide dense reward signals at each generation step. DPGAN (Xu et al., 2018) introduced a language model based discrim- inator and regarded cross-entropy as rewards to promote the diversity of generation results.",
      "DPGAN (Xu et al., 2018) introduced a language model based discrim- inator and regarded cross-entropy as rewards to promote the diversity of generation results. The most similar works to our model are RAML (Norouzi et al., 2016) and MaliGAN (Che et al., 2017): 1) Compared with RAML, our model adds a discriminator to learn the reward signals instead of choosing existing metrics as rewards. We be- lieve that our model can adapt to various text gen- eration tasks, particularly those without explicit evaluation metrics. 2) Unlike MaliGAN, we ac- quire samples from a \ufb01xed distribution near the real data rather than the generator\u2019s distribution, which is expected to make the training process more stable. 3 Model 3.1 Task De\ufb01nition and Model Overview Text generation can be formulated as follows: given the real data distribution Pdata(X), the task is to train a generative model G\u03b8 where PG\u03b8(X) can \ufb01t Pdata(X) well.",
      "3 Model 3.1 Task De\ufb01nition and Model Overview Text generation can be formulated as follows: given the real data distribution Pdata(X), the task is to train a generative model G\u03b8 where PG\u03b8(X) can \ufb01t Pdata(X) well. In this formulation, X = x1x2 \u00b7 \u00b7 \u00b7 xm and xt(1 \u2264t \u2264m) denotes a word in the vocabulary V. Figure 1 shows the overview of our model ARAML. This adversarial training framework consists of two phases: 1) The discriminator is trained to assign higher rewards to real data than to generated data. 2) The generator is trained on the samples acquired from a stationary distribu-",
      "Real Data  Sample from \ud835\udc43\ud835\udc60  reward1  reward2  reward3  Discriminator  Generator  Generated Data  Figure 1: Overview of ARAML. The training samples are acquired from a stationary distribution Ps based on the real data. The generator is then trained on the samples augmented by the discriminator\u2019s rewards. The discriminator is trained to distinguish real data and generated data. tion with reward augmented MLE training objec- tive. This training paradigm of the generator in- deed constrains the search space with the MLE training objective, which alleviates the issue of un- stable training. 3.2 Discriminator The discriminator D\u03c6 aims to distinguish real data and generated data like other GANs.",
      "This training paradigm of the generator in- deed constrains the search space with the MLE training objective, which alleviates the issue of un- stable training. 3.2 Discriminator The discriminator D\u03c6 aims to distinguish real data and generated data like other GANs. Inspired by Least-Square GAN (Mao et al., 2017), we devise the loss function as follows: LD\u03c6 = 1 2EX\u223cPdata(X) \u0002 (D\u03c6(X) \u22121)2\u0003 + 1 2EX\u223cPG\u03b8(X) \u0002 (D\u03c6(X))2\u0003 (1) This loss function forces the discriminator to as- sign higher rewards to real data than to gener- ated data, so the discriminator can learn to provide more proper rewards as the training proceeds.",
      "3.3 Generator The training objective of our generator G\u03b8 is de- rived from the objective of other discrete GANs with RL training method: LRL,\u03b8 = \u2212EX\u223cPG\u03b8(X)[r\u03c6(X)] \u2212\u03c4H(PG\u03b8(X)) (2) where r\u03c6(X) denotes the rewards from the dis- criminator D\u03c6 and the entropy regularized term H(PG\u03b8(X)) encourages G\u03b8 to generate diverse text samples. \u03c4 is a temperature hyper-parameter to balance these two terms. As mentioned above, discrete GANs suffer from the instability issue due to policy gradient, thus they are consequently dif\ufb01cult to train. In- spired by RAML (Norouzi et al., 2016), we intro- duce an exponential payoff distribution Q\u03c6(X) to connect RL loss with RAML loss: Q\u03c6(X) = 1 Z exp(r\u03c6(X)/\u03c4) (3) where Z = P X exp(r\u03c6(X)/\u03c4).",
      "Thus, we can rewrite LRL,\u03b8 with PG\u03b8(X) and Q\u03c6(X) as fol- lows: LRL,\u03b8 = \u03c4KL(PG\u03b8(X)||Q\u03c6(X)) + constant (4) Following RAML, we remove the constant term and optimize the KL divergence in the opposite direction: LRAML,\u03b8 = KL(Q\u03c6(X)||PG\u03b8(X)) = \u2212EX\u223cQ\u03c6(X)[log PG\u03b8(X)] \u2212H(Q\u03c6(X)) = \u2212EX\u223cQ\u03c6(X)[log PG\u03b8(X)] + constant (5) where H(Q\u03c6(X)) is a constant in the training phase of the generator. It has been proved that LRL,\u03b8 and LRAML,\u03b8 are equivalent up to their \ufb01rst order Taylor approximations, and they have the same global optimum (Norouzi et al., 2016). LRAML,\u03b8 can be trained in a MLE-like fash- ion but sampling from the distribution Q\u03c6(X) is intractable in the adversarial setting, because Q\u03c6(X) varies with the discriminator D\u03c6.",
      "LRAML,\u03b8 can be trained in a MLE-like fash- ion but sampling from the distribution Q\u03c6(X) is intractable in the adversarial setting, because Q\u03c6(X) varies with the discriminator D\u03c6. Thus, we introduce importance sampling to separate sampling process from D\u03c6 and obtain the \ufb01nal loss function: LG\u03b8 = \u2212EX\u223cPs(X)[W\u03c6(X) log PG\u03b8(X)] (6)",
      "where Ps(X) denotes a stationary distribution and W\u03c6(X) \u221dQ\u03c6(X)/Ps(X). To optimize this loss function, we \ufb01rst construct the \ufb01xed distribution Ps(X) to get samples, and devise the proper re- ward function r\u03c6(X) to train the generator in a stable and effective way. 3.3.1 Sampling We construct the distribution Ps based on Pdata: Ps(X) = EX\u223cPdata(X)[Ps(Xs|X)] (7) In this way, Ps(Xs|X) can be designed to guaran- tee that Ps(X) is near Pdata(X), leading to a more stable training process. To obtain a new sample Xs from a real data sample X, we can design three steps which contain sampling an edit distance d, the positions {p1, p2, \u00b7 \u00b7 \u00b7 , pd} for substitution and the new words {w1, w2, \u00b7 \u00b7 \u00b7 , wd} \ufb01lled into the corresponding positions.",
      "Thus, Ps(Xs|X) can be decomposed into three terms: Ps(Xs|X) = P(d, p, w|X) = P(d|X)P(p|X, d)P(w|X, d, p) (8) The \ufb01rst step is to sample an edit distance based on a real data sample X, where X = x1x2 \u00b7 \u00b7 \u00b7 xm is a sequence of length m. The number of sen- tences which have the edit distance e to some in- put sentence can be computed approximately as below: c(e, m) = \u0012m e \u0013 \u00b7 (|V| \u22121)e (9) where c(e, m) denotes the number of sentences which have an edit distance e(e \u2208{0, 1, ..., m}) to a sentence of length m, and |V| indicates the size of vocabulary.",
      "We then follow (Norouzi et al., 2016) to re-scale the counts by exp{\u2212e/\u03c4} and do normalization, so that we can sample an edit dis- tance d\u2217from: P(d = d\u2217|X) = exp{\u2212d\u2217/\u03c4}c(d\u2217, m) Pm e=0 exp{\u2212e/\u03c4}c(e, m) (10) where \u03c4, as a temperature hyper-parameter, re- stricts the search space surrounding the original sentence. Larger \u03c4 brings more samples with long edit distances. The next step is to select positions for substitu- tion based on the sampled edit distance d\u2217. Intu- itively, we can randomly choose d\u2217distinct posi- tions in X to be replaced by new words. The prob- ability of choosing the position p\u2217is calculated as follows: P(p = p\u2217|X, d = d\u2217) = d\u2217 m (11) Following this sampling strategy, we can obtain the position set {p1, p2, \u00b7 \u00b7 \u00b7 , pd\u2217}.",
      "This strategy approximately guarantees that the edit distance be- tween a new sentence and the original sentence is d\u2217. At the \ufb01nal step, our model determines new words for substitution at each sampled position pj(j = 1, 2, ..., d\u2217). We can formulate this sam- pling process from the original sequence X to a new sample Xs as a sequential transition X = X0 \u2192X1 \u2192\u00b7 \u00b7 \u00b7 \u2192Xd\u2217= Xs. At each step from Xj\u22121 to Xj (j = 1, \u00b7 \u00b7 \u00b7 , d\u2217), we \ufb01rst sample a new word wj from the distribution P(w|Xj\u22121, p = pj), then replace the old word at position pj of Xj\u22121 to obtain Xj.",
      "The whole sam- pling process can be decomposed as follows: P(w|X, d = d\u2217,p = {p1, p2, \u00b7 \u00b7 \u00b7 , pd\u2217}) = d\u2217 Y j=1 P(wj|Xj\u22121, p = pj) (12) There are two common sampling strategies to model P(w|Xj\u22121, p = pj), i.e. random sam- pling and constrained sampling. Random sam- pling strategy samples a new word wj according to the uniform distribution over the vocabulary V (Norouzi et al., 2016), while constrained sam- pling strategy samples wj to maximize the lan- guage model score of the target sentence Xj (Su et al., 2018; Miao et al., 2019). Here, we adopt constrained sampling in our model and compare the performances of two strategies in the experi- ment.",
      "Here, we adopt constrained sampling in our model and compare the performances of two strategies in the experi- ment. 3.3.2 Training We devise the reward function r\u03c6(X) according to the discriminator\u2019s output D\u03c6(X) and the station- ary distribution Ps(X): r\u03c6(X) = \u03c4 \u00b7 [log Ps(X) + D\u03c6(X)] (13) Intuitively, this reward function encourages the generator to generate sentences with large sam- pling probability and high rewards from the dis- criminator. Thus, the weight of samples W\u03c6(X) can be calculated as follows: W\u03c6(X) \u221dQ\u03c6(X) Ps(X) \u221dexp {D\u03c6(X)} (14)",
      "So far, we can successfully optimize the gener- ator\u2019s loss LG\u03b8 via Equation 6. This training paradigm makes our generator avoid possible vari- ances caused by policy gradient and get more sta- ble reward signals from the discriminator, because our generator is restricted to explore the training samples near the real data. Algorithm 1 Adversarial Reward Augmented Maximum Likelihood Require: Total adversarial training iterations: N iters Steps of training generator: G steps Steps of training discriminator: D steps 1: Pre-train the generator G\u03b8 with MLE loss 2: Generate samples from PG\u03b8 3: Pre-train the discriminator D\u03c6 via Eq.(1) 4: Construct Ps(X) via Eq.(7) - Eq.(12) 5: for each s = 1, 2, ..., N iters do 6: for each j = 1, 2, ..., G steps do 7: Update G\u03b8 via Eq.(6) 8: end for 9: for each k = 1, 2, ..., D steps do 10: Update D\u03c6 via Eq.",
      "(6) 8: end for 9: for each k = 1, 2, ..., D steps do 10: Update D\u03c6 via Eq.(1) 11: end for 12: end for 3.4 Extension to Conditional Text Generation We have shown our adversarial training frame- work for text generation tasks without an in- put. Actually, it can also be extended to con- ditional text generation tasks like dialogue gen- eration. Given the data distribution Pdata(C, X) where C, X denote contexts and responses respec- tively, the objective function of ARAML\u2019s genera- tor can be modi\ufb01ed as below: LG\u03b8 = \u2212E(C,X)\u223cPdata(C,X) \u0002 EXs\u223cPs(Xs|C,X) [W\u03c6(C, Xs) log PG\u03b8(Xs|C)] \u0003 (15) where W\u03c6(C, Xs) \u221d exp{D\u03c6(C, Xs)} and D\u03c6(C, Xs) is trained to distinguish whether Xs is the true response to C.",
      "3.5 Comparison with RAML and MaliGAN The most similar works to our framework are RAML (Norouzi et al., 2016) and MaliGAN (Che et al., 2017). The main difference among them is the training objective of their generators. We have shown different objective functions in Table 1. For comparison, we use the form with no input for all the three models. Our model is greatly inspired by RAML, which gets samples from a non-parametric distribution Q(X) constructed based on a speci\ufb01c reward. Compared to RAML, our reward comes from a learnable discriminator which varies as the adver- sarial training proceeds rather than a speci\ufb01c re- ward function. This difference equips our frame- work with the ability to adapt to the text gener- ation tasks with no explicit evaluation metrics as rewards. Our model is also similar to MaliGAN, which gets samples from the generator\u2019s distribution. In MaliGAN\u2019s training objective, G\u03b8\u2032 also indi- cates the generator\u2019s distribution but it\u2019s used in the sampling phase and \ufb01xed at each optimiza- tion step.",
      "Our model is also similar to MaliGAN, which gets samples from the generator\u2019s distribution. In MaliGAN\u2019s training objective, G\u03b8\u2032 also indi- cates the generator\u2019s distribution but it\u2019s used in the sampling phase and \ufb01xed at each optimiza- tion step. The weight of samples W \u2032 \u03c6(X) \u221d D\u03c6(X) 1\u2212D\u03c6(X). Different from our model, MaliGAN acquires samples from the generator\u2019s distribution PG\u03b8\u2032, which usually brings samples with low re- wards even with careful pre-training for the gen- erator, leading to training instability. Instead, our framework gets samples from a stationary distri- bution Ps around real data, thus our training pro- cess is more stable. Model Training Objective of Generator RAML LG\u03b8 = \u2212EX\u223cQ(X)[log PG\u03b8(X)] MaliGAN LG\u03b8 = \u2212EX\u223cPG\u03b8\u2032 (X)[W \u2032 \u03c6(X) log PG\u03b8(X)] ARAML LG\u03b8 = \u2212EX\u223cPs(X)[W\u03c6(X) log PG\u03b8(X)] Table 1: Training objectives of generators for RAML, MaliGAN and ARAML.",
      "4 Experiment 4.1 Datasets Dataset Amount(Train/Test) Vocabulary Length COCO 80,000/5,000 4,839 12.8 EMNLP2017 49,996/10,000 5,721 27.8 WeiboDial 100,000/5,000 7,998 7.3/10.8 Table 2: Statistics of COCO, EMNLP2017 WMT and WeiboDial. The average lengths 7.3/10.8 of Weibo- Dial indicate the lengths of posts and responses, respec- tively. We evaluated ARAML on three datasets: COCO image caption dataset (Chen et al., 2015), EMNLP2017 WMT dataset1 and Weibo- Dial single-turn dialogue dataset (Qian et al., 1http://statmt.org/wmt17/translation-task.html",
      "2018). COCO and EMNLP2017 WMT are the common benchmarks with no input to evaluate the performance of discrete GANs, and we followed the existing works to preprocess these datasets (Shi et al., 2018; Guo et al., 2018). WeiboDial, as a dialogue dataset, was applied to test the perfor- mance of our model with input trigger. We sim- ply removed post-response pairs containing low- frequency words and randomly selected a subset for our training/test set. The statistics of three datasets are presented in Table 2. 4.2 Baselines We compared our model with MLE, RL and GAN baselines. Since COCO and EMNLP2017 WMT don\u2019t have input while WeiboDial regards posts as input, we chose the following baselines respec- tively: MLE: a RNN model trained with MLE objective (Graves, 2013). Its extension, Seq2Seq, can work on the dialogue dataset (Sutskever et al., 2014).",
      "Its extension, Seq2Seq, can work on the dialogue dataset (Sutskever et al., 2014). SeqGAN: The \ufb01rst text GAN model that updates the generator with policy gradient based on the re- wards from the discriminator (Yu et al., 2017). LeakGAN: A variant of SeqGAN that provides rewards based on the leaked information of the discriminator for the generator (Guo et al., 2018). MaliGAN: A variant of SeqGAN that optimizes the generator with a normalized maximum likeli- hood objective (Che et al., 2017). IRL: This inverse reinforcement learning method replaces the discriminator with a reward approxi- mator to provide dense rewards (Shi et al., 2018). RAML: A RL approach to incorporate MLE ob- jective into RL training framework, which regards BLEU as rewards (Norouzi et al., 2016). DialogGAN: An extension of SeqGAN tuned to dialogue generation task with MLE objective added to the adversarial objective (Li et al., 2017).",
      "DialogGAN: An extension of SeqGAN tuned to dialogue generation task with MLE objective added to the adversarial objective (Li et al., 2017). DPGAN: A variant of DialogGAN which uses a language model based discriminator and regards cross-entropy as rewards (Xu et al., 2018). Note that MLE, SeqGAN, LeakGAN, Mali- GAN and IRL are the baselines on COCO and EMNLP2017 WMT, while MLE, RAML, Dialog- GAN, and DPGAN on WeiboDial. The original codes are used to test the baselines. 4.3 Implementation Details The implementation details of our model are shown in Table 3.",
      "The original codes are used to test the baselines. 4.3 Implementation Details The implementation details of our model are shown in Table 3. For COCO / EMNLP2017, the Dataset COCO / EMNLP2017 WeiboDial Generator LSTM GRU Discriminator GRU & CNN GRU & MLP Temperature 0.85 (COCO) 0.95 0.9 (EMNLP2017) Sampling size 5 5 Dimension of 128 100 word embedding Batch size 100 100 Pretraining epochs 50 / 15 / 50 50 / 10 / 30 (G/D/LM) Optimizer Adam Adam Learning rate(G/D) 0.001 / 0.0001 0.001 / 0.0001 Table 3: Implementation details of ARAML. G/D/LM indicates the generator / discriminator / language model used in constrained sampling, respectively. generator is a LSTM unit (Hochreiter and Schmid- huber, 1997) with 128 cells, and the discriminator is implemented based on (Yu et al., 2017).",
      "G/D/LM indicates the generator / discriminator / language model used in constrained sampling, respectively. generator is a LSTM unit (Hochreiter and Schmid- huber, 1997) with 128 cells, and the discriminator is implemented based on (Yu et al., 2017). For WeiboDial, the generator is an encoder-decoder structure with attention mechanism, where both the encoder and the decoder consist of a two-layer GRU (Cho et al., 2014) with 128 cells. The dis- criminator is implemented based on (Tao et al., 2018). The language model used in the con- strained sampling of ARAML is implemented in the same setting as the generators, and is pre- trained on the training set of each dataset. The codes and the datasets are available at https: //github.com/kepei1106/ARAML. As for the details of the baselines, the genera- tors of all the baselines except LeakGAN are the same as ours. Note that the generator of Leak- GAN consists of a hierarchical LSTM unit, thus we followed the implementation in the original paper.",
      "As for the details of the baselines, the genera- tors of all the baselines except LeakGAN are the same as ours. Note that the generator of Leak- GAN consists of a hierarchical LSTM unit, thus we followed the implementation in the original paper. In terms of the differences, the discrimi- nators of GAN baselines are implemented based on the original papers. Other hyper-parameters of baselines including batch size, learning rate, and pre-training epochs, were set based on the origi- nal codes, because the convergence of baselines is sensitive to these hyper-parameters. 4.4 Language Generation on COCO and EMNLP2017 WMT We adopted forward/reverse perplexity (Zhao et al., 2018) and Self-BLEU (Zhu et al., 2018) to evaluate the quality of generated texts. For- ward perplexity (PPL-F) indicates the perplexity on the generated data provided by a language model trained on real data to measure the \ufb02uency of generated samples. Reverse perplexity (PPL-R) switches the roles of generated data and real data",
      "Model COCO EMNLP2017 WMT PPL-F PPL-R S-BLEU-2/3/4 PPL-F PPL-R S-BLEU-2/3/4 MLE 18.83 \u00b1 0.51 38.81 \u00b1 0.97 0.790 \u00b1 0.006 / 0.598 \u00b1 0.009 / 0.419 \u00b1 0.010 55.64 \u00b1 1.03 192.33 \u00b1 9.51 0.771 \u00b1 0.005 / 0.505 \u00b1 0.009 / 0.304 \u00b1 0.008 SeqGAN 33.07 \u00b1 1.98 49.24 \u00b1 2.36 0.814 \u00b1 0.005 / 0.619 \u00b1 0.008 / 0.430 \u00b1 0.007 67.60 \u00b1 3.48 276.82 \u00b1 5.12 0.786 \u00b1 0.019 / 0.500 \u00b1 0.029 / 0.276 \u00b1 0.023 LeakGAN 11.43 \u00b1 2.74 87.54 \u00b1 6.",
      "60 \u00b1 3.48 276.82 \u00b1 5.12 0.786 \u00b1 0.019 / 0.500 \u00b1 0.029 / 0.276 \u00b1 0.023 LeakGAN 11.43 \u00b1 2.74 87.54 \u00b1 6.42 0.877 \u00b1 0.032 / 0.762 \u00b1 0.045 / 0.645 \u00b1 0.049 17.92 \u00b1 1.77 491.70 \u00b1 18.29 0.890 \u00b1 0.013 / 0.751 \u00b1 0.011 / 0.604 \u00b1 0.009 MaliGAN 47.16 \u00b1 2.94 54.40 \u00b1 1.29 0.786 \u00b1 0.005 / 0.572 \u00b1 0.008 / 0.370 \u00b1 0.007 126.84 \u00b1 11.18 288.20 \u00b1 16.48 0.780 \u00b1 0.019 / 0.494 \u00b1 0.032 / 0.265 \u00b1 0.028 IRL 41.86 \u00b1 11.82 84.23 \u00b1 7.",
      "84 \u00b1 11.18 288.20 \u00b1 16.48 0.780 \u00b1 0.019 / 0.494 \u00b1 0.032 / 0.265 \u00b1 0.028 IRL 41.86 \u00b1 11.82 84.23 \u00b1 7.02 0.857 \u00b1 0.014 / 0.687 \u00b1 0.031 / 0.499 \u00b1 0.062 285.20 \u00b1 36.47 1124.57 \u00b1 109.80 0.890 \u00b1 0.008 / 0.656 \u00b1 0.052 / 0.406 \u00b1 0.077 ARAML 26.97 \u00b1 0.55 35.79 \u00b1 0.49 0.777 \u00b1 0.005 / 0.560 \u00b1 0.006/ 0.366 \u00b1 0.008 77.90 \u00b1 0.70 188.41 \u00b1 3.18 0.745 \u00b1 0.002 / 0.455 \u00b1 0.006 / 0.257 \u00b1 0.006 Table 4: Automatic evaluation on COCO and EMNLP2017 WMT.",
      "90 \u00b1 0.70 188.41 \u00b1 3.18 0.745 \u00b1 0.002 / 0.455 \u00b1 0.006 / 0.257 \u00b1 0.006 Table 4: Automatic evaluation on COCO and EMNLP2017 WMT. Each metric is presented with mean and standard deviation. to re\ufb02ect the discrepancy between the generated distribution and the data distribution. Self-BLEU (S-BLEU) regards each sentence in the generated collection as hypothesis and the others as refer- ence to obtain BLEU scores, which evaluates the diversity of generated results. Results are shown in Table 4. LeakGAN per- forms best on forward perplexity because it can generate more \ufb02uent samples. As for reverse per- plexity, our model ARAML beats other baselines, showing that our model can \ufb01t the data distribution better.",
      "Results are shown in Table 4. LeakGAN per- forms best on forward perplexity because it can generate more \ufb02uent samples. As for reverse per- plexity, our model ARAML beats other baselines, showing that our model can \ufb01t the data distribution better. Other GANs, particularly LeakGAN, ob- tain high reverse perplexity due to mode collapse (Shi et al., 2018), thus they only capture limited \ufb02uent expressions, resulting in large discrepancy between the generated distribution and data distri- bution. ARAML also outperforms the baselines in terms of Self-BLEU, indicating that our model doesn\u2019t fall into mode collapse with the help of the MLE training objective and has the ability to gen- erate more diverse sentences. We also provide standard deviation of each met- ric in Table 4, re\ufb02ecting the stability of each model\u2019s performance. Our model ARAML nearly achieves the smallest standard deviation in all the metrics, indicating that our framework outper- forms policy gradient in the stability of adversarial training.",
      "We also provide standard deviation of each met- ric in Table 4, re\ufb02ecting the stability of each model\u2019s performance. Our model ARAML nearly achieves the smallest standard deviation in all the metrics, indicating that our framework outper- forms policy gradient in the stability of adversarial training. 4.5 Dialogue Generation on WeiboDial Dialogue evaluation is an open problem and ex- isting works have found that automatic metrics have low correlation to human evaluation (Liu et al., 2016; Novikova et al., 2017; Chaganty et al., 2018). Thus, we resorted to manual evaluation to assess the generation quality on WeiboDial. We randomly sampled 200 posts from the test set and collected the generated results from all the mod- els. For each pair of responses (one from ARAML and the other from a baseline, given the same in- put post), \ufb01ve annotators were hired to label which response is better (i.e.",
      "For each pair of responses (one from ARAML and the other from a baseline, given the same in- put post), \ufb01ve annotators were hired to label which response is better (i.e. win, lose or tie) in terms of grammaticality (whether a response itself is gram- matical and logical) and relevance (whether a re- sponse is appropriate and relevant to the post). The two metrics were evaluated independently. The evaluation results are shown in Table 5. To measure the inter-annotator agreement, we calcu- lated Fleiss\u2019 kappa (Fleiss, 1971) for each pair- wise comparison where results show moderate agreement (0.4 \u2264\u03ba \u22640.6). We also conducted sign test to check the signi\ufb01cance of the differ- ences. As shown in Table 5, ARAML performs signif- icantly better than other baselines in all the cases. This result indicates that the samples surrounding true responses provide stable rewards for the gen- erator, and stable RAML training paradigm signif- icantly enhances the performance in both metrics.",
      "As shown in Table 5, ARAML performs signif- icantly better than other baselines in all the cases. This result indicates that the samples surrounding true responses provide stable rewards for the gen- erator, and stable RAML training paradigm signif- icantly enhances the performance in both metrics. 4.6 Further Analysis on Stability 0 50 100 150 200 250 Epoch 0 10 20 30 40 50 60 PPL-F ARAML SeqGAN LeakGAN MaliGAN IRL 0 50 100 150 200 250 Epoch 20 40 60 80 100 120 PPL-R ARAML SeqGAN LeakGAN MaliGAN IRL Figure 2: PPL-F/PPL-R curves of ARAML, SeqGAN, LeakGAN, MaliGAN and IRL in the training pro- cess. The shade area indicates the standard devia- tion at each data point. The dotted vertical lines sep- arate pre-training and adversarial training phases (50 for ARAML, IRL and MaliGAN, 80 for SeqGAN and LeakGAN).",
      "The shade area indicates the standard devia- tion at each data point. The dotted vertical lines sep- arate pre-training and adversarial training phases (50 for ARAML, IRL and MaliGAN, 80 for SeqGAN and LeakGAN). To verify the training stability, we conducted experiments on COCO many times and chose the best 5 trials for SeqGAN, LeakGAN, IRL, Mali- GAN and ARAML, respectively. Then, we pre- sented the forward/reverse perplexity in the train-",
      "Model Grammaticality \u03ba Relevance \u03ba Win (%) Lose (%) Tie (%) Win (%) Lose (%) Tie (%) ARAML vs. MLE 56.5** 36.5 7.0 0.456 50.5** 37.5 12.0 0.465 ARAML vs. RAML 54.5** 37.5 8.0 0.416 47.0* 40.5 12.5 0.480 ARAML vs. DialogGAN 75.5** 13.5 11.0 0.445 73.0** 11.0 16.0 0.460 ARAML vs. DPGAN 57.5** 36.0 6.5 0.435 56.5** 30.5 13.0 0.529 Table 5: Human evaluation on WeiboDial. The scores represent the percentages of Win, Lose or Tie when our model is compared with a baseline. \u03ba denotes Fleiss\u2019 kappa (all are moderate agreement).",
      "The scores represent the percentages of Win, Lose or Tie when our model is compared with a baseline. \u03ba denotes Fleiss\u2019 kappa (all are moderate agreement). The scores marked with * mean p-value< 0.05 and ** indicates p-value< 0.01 in sign test. ing process in Figure 2. We can see that our model with smaller standard deviation is more stable than other GAN baselines in both metrics. Although LeakGAN reaches the best forward perplexity, its standard deviation is extremely large and it per- forms badly in reverse perplexity, indicating that it generates limited expressions that are grammatical yet divergent from the data distribution. 4.7 Ablation Study 4.7.1 Impact of Temperature The temperature \u03c4 controls the search space sur- rounding the real data as we analyze in Section 3.3.1. To investigate its impact on the perfor- mance of our model, we \ufb01xed all the other hyper- parameters and test ARAML with different tem- peratures on COCO. 0.75 0.8 0.85 0.9 0.",
      "To investigate its impact on the perfor- mance of our model, we \ufb01xed all the other hyper- parameters and test ARAML with different tem- peratures on COCO. 0.75 0.8 0.85 0.9 0.95 1 Temparature 20 22 24 26 28 30 32 PPL-F 0.75 0.8 0.85 0.9 0.95 1 Temperature 34 34.5 35 35.5 36 36.5 37 PPL-R 0.8 0.9 1 Temperature 0.75 0.755 0.76 0.765 0.77 0.775 0.78 0.785 0.79 S-BLEU-2 0.8 0.9 1 Temperature 0.5 0.52 0.54 0.56 0.58 0.6 S-BLEU-3 0.8 0.9 1 Temperature 0.3 0.32 0.34 0.36 0.38 0.",
      "5 0.52 0.54 0.56 0.58 0.6 S-BLEU-3 0.8 0.9 1 Temperature 0.3 0.32 0.34 0.36 0.38 0.4 S-BLEU-4 Figure 3: PPL-F, PPL-R and S-BLEU of ARAML with different temperatures \u03c4 \u2208{0.8, 0.85, 0.9, 0.95} on COCO. The experimental results are shown in Figure 3. We can see that as the temperature becomes larger, forward perplexity increases gradually while Self- BLEU decreases. As mentioned in Section 3.3.1, large temperatures encourage our generator to ex- plore the samples that are distant from real data distribution, thus the diversity of generated results will be improved. However, these samples dis- tant from the data distribution are more likely to be poor in \ufb02uency, leading to worse forward per- plexity.",
      "However, these samples dis- tant from the data distribution are more likely to be poor in \ufb02uency, leading to worse forward per- plexity. Reverse perplexity is in\ufb02uenced by both generation quality and diversity, so the correla- tion between temperature and reverse perplexity is not intuitive. We can observe that the model with \u03c4 = 0.95 reaches the best reverse perplexity. 4.7.2 Impact of Sampling Strategy We have mentioned two common sampling strate- gies in Section 3.3.1, i.e. random sampling and constrained sampling. To analyze their im- pact, we keep all the model structures and hyper- parameters \ufb01xed and test ARAML with these two strategies on COCO.",
      "random sampling and constrained sampling. To analyze their im- pact, we keep all the model structures and hyper- parameters \ufb01xed and test ARAML with these two strategies on COCO. Model PPL-F PPL-R S-BLEU-2/3/4 ARAML-R 37.48\u00b10.53 37.44\u00b10.56 0.752/0.571/0.384 ARAML-C 26.97\u00b10.55 35.79\u00b10.49 0.777/0.560/0.366 Table 6: PPL-F, PPL-R and S-BLEU of ARAML with random sampling (ARAML-R) and constrained sam- pling (ARAML-C) on COCO. Table 6 shows the results. It\u2019s obvious that ran- dom sampling hurts the model performance ex- cept Self-BLEU-1, because it indeed allows low- quality samples available to the generator. Explor- ing these samples degrades the quality and diver- sity of generated results. Despite the worse per- formance on automatic metrics, random sampling doesn\u2019t affect the training stability of our frame- work.",
      "Explor- ing these samples degrades the quality and diver- sity of generated results. Despite the worse per- formance on automatic metrics, random sampling doesn\u2019t affect the training stability of our frame- work. The standard deviation of ARAML-R is still smaller than other GAN baselines. 4.8 Case Study Table 7 presents the examples generated by the models on COCO. We can \ufb01nd that other baselines suffer from grammatical errors (e.g. \u201cin front of \ufb02ying her kite\u201d from MLE), repetitive expressions",
      "Model Generated Samples MLE A little girl sitting on a beach in front of \ufb02ying her kite at the beach. A little boy standing in a room next to a desk. SeqGAN A man sitting on a bench with snow board in the background. A brown gray cat is in the corner of a street. LeakGAN A person that is holding something while another kid is standing in the water. A room with a television, mantle, and a chair. MaliGAN A man with a shirt on holding one large pink giant and white kite. A couple and vases are outside on the bed. IRL A group of people wearing helmet sitting down on a cell phone. A group of people sitting in the middle of tracks. ARAML A man is wearing a hat and holding a toothbrush as he stands on the grass of a \ufb01eld. A boy reading a book on a sofa in a room. Table 7: Examples of generated sentences on COCO. Grammatical errors are in red, while blue text repre- sents repetitive expressions and green part indicates in- coherent statements. (e.g.",
      "A boy reading a book on a sofa in a room. Table 7: Examples of generated sentences on COCO. Grammatical errors are in red, while blue text repre- sents repetitive expressions and green part indicates in- coherent statements. (e.g. \u201cA group of people\u201d from IRL) and incoher- ent statements (e.g. \u201cA group of people sitting on a cell phone\u201d from IRL). By contrast, our model performs well in these sentences and has the abil- ity to generate grammatical and coherent results. Post: \u4f60\u8fd8\u8fdf\u5230\u54ea\uff1f\u4f60\u6ca1\u8fd9\u4e60\u60ef\u554a Are you late? You don\u2019t have this habit. MLE: \u4f60\u662f\u4e0d\u662f\u8981\u597d\u597d\u7167\u987e\u81ea\u5df1\u7684\uff01\uff01\uff01 Would you take care of yourself !!! RAML: \u8fd9\u662f\u6211\u7684\u751f\u6d3b\uff0c\u6211\u4e5f\u4e0d\u60f3\u5e72 This is my life. I don\u2019t want to do this. DialogGAN: \u8fd9\u793e\u4f1a\u7684\u6e29\u5ea6\u5feb\u5230\u4e86\u3002\u5468\u672b\uff0c\u8981\u8eba\u5e8a\u4e0a\uff01 The temperature of the society has nearly reached.",
      "I don\u2019t want to do this. DialogGAN: \u8fd9\u793e\u4f1a\u7684\u6e29\u5ea6\u5feb\u5230\u4e86\u3002\u5468\u672b\uff0c\u8981\u8eba\u5e8a\u4e0a\uff01 The temperature of the society has nearly reached. I\u2019ll lie on my bed on the weekend! DPGAN: \u6211\u5728\u4e0a\u73ed\uff0c\u6211\u5c31\u4e0d\u4e60\u60ef\u4e86 I\u2019m working and I\u2019m used to doing this. ARAML: \u55ef\uff0c\u5468\u672b\u5c31\u4e0d\u8fdf\u5230\u4e86\uff01 Hum. I won\u2019t be late on the weekend! Table 8: Examples of generated responses on Weibo- Dial. Table 8 shows the generated examples on Wei- boDial. It\u2019s obvious that other baselines don\u2019t cap- ture the topic word \u201clate\u201d in the post, thus gener- ate irrelevant responses. ARAML can provide a response that is grammatical and closely relevant to the post. 5 Conclusion We propose a novel adversarial training frame- work to deal with the instability problem of current GANs for text generation.",
      "ARAML can provide a response that is grammatical and closely relevant to the post. 5 Conclusion We propose a novel adversarial training frame- work to deal with the instability problem of current GANs for text generation. To address the instabil- ity issue caused by policy gradient, we incorpo- rate RAML into the advesarial training paradigm to make our generator acquire stable rewards. Ex- periments show that our model performs better than several state-of-the-art GAN baselines with lower training variance, yet producing better per- formance on three text generation tasks. Acknowledgments This work was supported by the National Science Foundation of China (Grant No. 61936010/61876096) and the National Key R&D Program of China (Grant No. 2018YFC0830200). We would like to thank THUNUS NExT Joint-Lab for the support. References Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. 2017. An actor-critic algorithm for sequence prediction.",
      "References Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. 2017. An actor-critic algorithm for sequence prediction. In Proceedings of International Conference on Learning Represen- tations. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled sampling for se- quence prediction with recurrent neural networks. In Advances in Neural Information Processing Sys- tems, pages 1171\u20131179. Yoshua Bengio, R\u00b4ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic lan- guage model. Journal of Machine Learning Re- search, 3:1137\u20131155. Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Charlin. 2018. Language gans falling short. arXiv preprint arXiv: 1811.02549.",
      "Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Charlin. 2018. Language gans falling short. arXiv preprint arXiv: 1811.02549. Arun Tejasvi Chaganty, Stephen Mussmann, and Percy Liang. 2018. The price of debiasing automatic met- rics in natural language evaluation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 643\u2013653. Tong Che, Yanran Li, Ruixiang Zhang, R Devon Hjelm, Wenjie Li, Yangqiu Song, and Yoshua Ben- gio. 2017. Maximum-likelihood augmented discrete generative adversarial networks. arXiv preprint arXiv: 1702.07983. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr- ishna Vedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence Zitnick. 2015. Microsoft coco cap- tions: Data collection and evaluation server.",
      "Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr- ishna Vedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence Zitnick. 2015. Microsoft coco cap- tions: Data collection and evaluation server. arXiv preprint arXiv: 1504.00325. Kyunghyun Cho, Bart Van Merrienboer, Dzmitry Bah- danau, and Yoshua Bengio. 2014. On the proper- ties of neural machine translation: Encoder-decoder",
      "approaches. In Proceedings of Eighth Workshop on Syntax, Semantics and Structure in Statistical Trans- lation, pages 103\u2013111. William Fedus, Ian J. Goodfellow, and Andrew M. Dai. 2018. Maskgan: Better text generation via \ufb01lling in the . In Proceedings of International Confer- ence on Learning Representations. Joseph L Fleiss. 1971. Measuring nominal scale agree- ment among many raters. Psychological Bulletin, 76(5):378\u2013382. Alex Graves. 2013. Generating sequences with re- current neural networks. arXiv preprint arXiv: 1308.0850. Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and Jun Wang. 2018. Long text generation via adversarial training with leaked information. In Pro- ceedings of AAAI conference on Arti\ufb01cial Intelli- gence, pages 5141\u20135148. Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997. Long short-term memory.",
      "In Pro- ceedings of AAAI conference on Arti\ufb01cial Intelli- gence, pages 5141\u20135148. Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735\u20131780. Pei Ke, Jian Guan, Minlie Huang, and Xiaoyan Zhu. 2018. Generating informative responses with con- trolled sentence function. In Proceedings of the 56th Annual Meeting of the Association for Compu- tational Linguistics, pages 1499\u20131508. Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016. A diversity-promoting ob- jective function for neural conversation models. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 110\u2013119. Jiwei Li, Will Monroe, Tianlin Shi, Sebastien Jean, Alan Ritter, and Dan Jurafsky. 2017. Adversarial learning for neural dialogue generation.",
      "Jiwei Li, Will Monroe, Tianlin Shi, Sebastien Jean, Alan Ritter, and Dan Jurafsky. 2017. Adversarial learning for neural dialogue generation. In Proceed- ings of the Conference on Empirical Methods in Nat- ural Language Processing, pages 2157\u20132169. Kevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang, and Ming-Ting Sun. 2017. Adversarial ranking for language generation. In Advances in Neural Infor- mation Processing Systems, pages 3155\u20133165. Chia Wei Liu, Ryan Lowe, Iulian V. Serban, Michael Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How not to evaluate your dialogue system: An empirical study of unsupervised evaluation met- rics for dialogue response generation. In Proceed- ings of the Conference on Empirical Methods in Nat- ural Language Processing, pages 2122\u20132132. Xudong Mao, Qing Li, Haoran Xie, Raymond Y. K. Lau, Zhen Wang, and Stephen Paul Smolley. 2017.",
      "In Proceed- ings of the Conference on Empirical Methods in Nat- ural Language Processing, pages 2122\u20132132. Xudong Mao, Qing Li, Haoran Xie, Raymond Y. K. Lau, Zhen Wang, and Stephen Paul Smolley. 2017. Least squares generative adversarial net- works. In International Conference on Computer Vision, pages 2813\u20132821. Ning Miao, Hao Zhou, Lili Mou, Rui Yan, and Lei Li. 2019. Cgmh: Constrained sentence generation by metropolis-hastings sampling. In Proceedings of AAAI conference on Arti\ufb01cial Intelligence. Tomas Mikolov, Martin Kara\ufb01at, Lukas Burget, Jan Honza Cernock, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Proceedings of the 11st Annual Conference of the International Speech Communication Association, pages 1045\u20131048. Lili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang, and Zhi Jin. 2016.",
      "Recurrent neural network based language model. In Proceedings of the 11st Annual Conference of the International Speech Communication Association, pages 1045\u20131048. Lili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang, and Zhi Jin. 2016. Sequence to backward and for- ward sequences: A content-introducing approach to generative short-text conversation. In Proceedings of 26th International Conference on Computational Linguistics, pages 3349\u20133358. Mohammad Norouzi, Samy Bengio, Zhifeng Chen, Navdeep Jaitly, Mike Schuster, Yonghui Wu, and Dale Schuurmans. 2016. Reward augmented max- imum likelihood for neural structured prediction. In Advances in Neural Information Processing Sys- tems, pages 1723\u20131731. Jekaterina Novikova, Ondrej Dusek, Amanda Cercas Curry, and Verena Rieser. 2017. Why we need new evaluation metrics for NLG. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2241\u20132252.",
      "2017. Why we need new evaluation metrics for NLG. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2241\u20132252. Qiao Qian, Minlie Huang, Haizhou Zhao, Jingfang Xu, and Xiaoyan Zhu. 2018. Assigning personal- ity/pro\ufb01le to a chatting machine for coherent con- versation generation. In Proceedings of the Twenty- Seventh International Joint Conference on Arti\ufb01cial Intelligence, pages 4279\u20134285. Marc\u2019Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2016. Sequence level train- ing with recurrent neural networks. In Proceedings of International Conference on Learning Represen- tations. Stanislau Semeniuta1, Aliaksei Severyn, and Syl- vain Gelly. 2018. On accurate evaluation of gans for language generation. arXiv preprint arXiv: 1806.04936. Zhan Shi, Xinchi Chen, Xipeng Qiu, and Xuanjing Huang.",
      "2018. On accurate evaluation of gans for language generation. arXiv preprint arXiv: 1806.04936. Zhan Shi, Xinchi Chen, Xipeng Qiu, and Xuanjing Huang. 2018. Toward diverse text generation with inverse reinforcement learning. In Proceedings of the Twenty-Seventh International Joint Conference on Arti\ufb01cial Intelligence, pages 4361\u20134367. Jinyue Su, Jiacheng Xu, Xipeng Qiu, and Xuanjing Huang. 2018. Incorporating discriminator in sen- tence generation: A gibbs sampling method. In Proceedings of AAAI conference on Arti\ufb01cial Intel- ligence, pages 5496\u20135503. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural net- works. In Advances in Neural Information Process- ing Systems, pages 3104\u20133112.",
      "Chongyang Tao, Lili Mou, Dongyan Zhao, and Rui Yan. 2018. Ruber: An unsupervised method for au- tomatic evaluation of open-domain dialog systems. In Proceedings of the Thirty-Second AAAI Confer- ence on Arti\ufb01cial Intelligence, pages 722\u2013729. Oriol Vinyals and Quoc Le. 2015. A neural conversa- tional model. In International Conference on Ma- chine Learning Deep Learning Workshop. Chen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang, Ming Zhou, and Wei-Ying Ma. 2017. Topic aware neural response generation. In Proceedings of the Thirty-First AAAI Conference on Arti\ufb01cial Intelli- gence, pages 3351\u20133357. Jingjing Xu, Xuancheng Ren, Junyang Lin, and Xu Sun. 2018. Diversity-promoting gan: A cross- entropy based generative adversarial network for di- versi\ufb01ed text generation. In Conference on Empiri- cal Methods in Natural Language Processing, page 3940\u20133949.",
      "2018. Diversity-promoting gan: A cross- entropy based generative adversarial network for di- versi\ufb01ed text generation. In Conference on Empiri- cal Methods in Natural Language Processing, page 3940\u20133949. Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. Seqgan: Sequence generative adversarial nets with policy gradient. In Proceedings of AAAI con- ference on Arti\ufb01cial Intelligence, pages 2852\u20132858. Junbo Jake Zhao, Yoon Kim, Kelly Zhang, Alexan- der M. Rush, and Yann LeCun. 2018. Adversari- ally regularized autoencoders. In Proceedings of the 35th International Conference on Machine Learn- ing, pages 5897\u20135906. Hao Zhou, Minlie Huang, Tianyang Zhang, Xiaoyan Zhu, and Bing Liu. 2018a. Emotional chatting ma- chine: Emotional conversation generation with in- ternal and external memory. In Proceedings of AAAI conference on Arti\ufb01cial Intelligence.",
      "Hao Zhou, Minlie Huang, Tianyang Zhang, Xiaoyan Zhu, and Bing Liu. 2018a. Emotional chatting ma- chine: Emotional conversation generation with in- ternal and external memory. In Proceedings of AAAI conference on Arti\ufb01cial Intelligence. Hao Zhou, Tom Young, Minlie Huang, Haizhou Zhao, Jingfang Xu, and Xiaoyan Zhu. 2018b. Com- monsense knowledge aware conversation generation with graph attention. In Proceedings of the Twenty- Seventh International Joint Conference on Arti\ufb01cial Intelligence, pages 4623\u20134629. Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. 2018. Texygen: A benchmarking platform for text gener- ation models. In Proceedings of the 41st Interna- tional ACM SIGIR Conference on Research Devel- opment in Information Retrieval, pages 1097\u20131100."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1908.07195.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":11434,
  "avg_doclen":170.6567164179,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1908.07195.pdf"
    }
  }
}