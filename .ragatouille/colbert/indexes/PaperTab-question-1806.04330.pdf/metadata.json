{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Neural Network Models for Paraphrase Identi\ufb01cation, Semantic Textual Similarity, Natural Language Inference, and Question Answering Wuwei Lan and Wei Xu Department of Computer Science and Engineering Ohio State University {lan.105, xu.1265}@osu.edu Abstract In this paper, we analyze several neural network designs (and their variations) for sentence pair modeling and compare their performance extensively across eight datasets, including paraphrase identi\ufb01cation, semantic textual similarity, natural language inference, and question answering tasks. Although most of these models have claimed state-of-the-art performance, the original papers often reported on only one or two selected datasets. We provide a systematic study and show that (i) en- coding contextual information by LSTM and inter-sentence interactions are critical, (ii) Tree-LSTM does not help as much as previously claimed but surprisingly improves performance on Twitter datasets, (iii) the Enhanced Sequential Inference Model (Chen et al., 2017) is the best so far for larger datasets, while the Pairwise Word Interaction Model (He and Lin, 2016) achieves the best performance when less data is available. We release our implementations as an open-source toolkit.",
      "We release our implementations as an open-source toolkit. 1 Introduction Sentence pair modeling is a fundamental technique underlying many NLP tasks, including the following: \u2022 Semantic Textual Similarity (STS), which measures the degree of equivalence in the underlying se- mantics of paired snippets of text (Agirre et al., 2016). \u2022 Paraphrase Identi\ufb01cation (PI), which identi\ufb01es whether two sentences express the same meaning (Dolan and Brockett, 2005; Xu et al., 2014; Xu et al., 2015). \u2022 Natural Language Inference (NLI), also known as recognizing textual entailment (RTE), which con- cerns whether a hypothesis can be inferred from a premise, requiring understanding of the semantic similarity between the hypothesis and the premise (Dagan et al., 2006; Bowman et al., 2015). \u2022 Question Answering (QA), which can be approximated as ranking candidate answer sentences or phrases based on their similarity to the original question (Yang et al., 2015). \u2022 Machine Comprehension (MC), which requires sentence matching between a passage and a question, pointing out the text region that contains the answer.",
      "\u2022 Machine Comprehension (MC), which requires sentence matching between a passage and a question, pointing out the text region that contains the answer. (Rajpurkar et al., 2016). Traditionally, researchers had to develop different methods speci\ufb01c for each task. Now neural networks can perform all the above tasks with the same architecture by training end to end. Various neural models (He and Lin, 2016; Chen et al., 2017; Parikh et al., 2016; Wieting et al., 2016; Tomar et al., 2017; Wang et al., 2017; Shen et al., 2017a; Yin et al., 2016) have declared state-of-the-art results for sentence pair modeling tasks; however, they were carefully designed and evaluated on selected (often one or two) datasets that can demonstrate the superiority of the model. The research questions are as follows: Do they perform well on other tasks and datasets? How much performance gain is due to certain system design choices and hyperparameter optimizations? This work is licensed under a Creative Commons Attribution 4.0 International License.",
      "The research questions are as follows: Do they perform well on other tasks and datasets? How much performance gain is due to certain system design choices and hyperparameter optimizations? This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. arXiv:1806.04330v2  [cs.CL]  23 Aug 2018",
      "To answer these questions and better understand different network designs, we systematically analyze and compare the state-of-the-art neural models across multiple tasks and multiple domains. Namely, we imple- ment \ufb01ve models and their variations on the same PyTorch platform: InferSent model (Conneau et al., 2017), Shortcut-stacked Sentence Encoder Model (Nie and Bansal, 2017), Pairwise Word Interaction Model (He and Lin, 2016), Decomposable Attention Model (Parikh et al., 2016), and Enhanced Sequential Inference Model (Chen et al., 2017). They are representative of the two most common approaches: sentence encoding models that learn vector representations of individual sentences and then calculate the semantic relationship between sentences based on vector distance and sentence pair interaction models that use some sorts of word alignment mechanisms (e.g., attention) then aggregate inter-sentence interactions.",
      "They are representative of the two most common approaches: sentence encoding models that learn vector representations of individual sentences and then calculate the semantic relationship between sentences based on vector distance and sentence pair interaction models that use some sorts of word alignment mechanisms (e.g., attention) then aggregate inter-sentence interactions. We focus on identifying important network designs and present a series of \ufb01ndings with quantitative measurements and in-depth analyses, including (i) incorporating inter-sentence interactions is critical; (ii) Tree-LSTM does not help as much as previously claimed but surprisingly improves performance on Twitter data; (iii) Enhanced Sequential Inference Model has the most consistent high performance for larger datasets, while Pairwise Word Interaction Model performs better on smaller datasets and Shortcut-Stacked Sentence Encoder Model is the best performaning model on the Quora corpus. We release our implementations as a toolkit to the research community.1 2 General Framework for Sentence Pair Modeling Various neural networks have been proposed for sentence pair modeling, all of which fall into two types of approaches. The sentence encoding approach encodes each sentence into a \ufb01xed-length vector and then computes sentence similarity directly.",
      "The sentence encoding approach encodes each sentence into a \ufb01xed-length vector and then computes sentence similarity directly. The model of this type has advantages in the simplicity of the network design and generalization to other NLP tasks. The sentence pair interaction approach takes word alignment and interactions between the sentence pair into account and often show better performance when trained on in-domain data. Here we outline the two types of neural networks under the same general framework: \u2022 The Input Embedding Layer takes vector representations of words as input, where pretrained word embeddings are most commonly used, e.g. GloVe (Pennington et al., 2014) or Word2vec (Mikolov et al., 2013). Some work used embeddings specially trained on phrase or sentence pairs that are para- phrases (Wieting and Gimpel, 2017; Tomar et al., 2017); some used subword embeddings, which showed improvement on social media data (Lan and Xu, 2018). \u2022 The Context Encoding Layer incorporates word context and sequence order into modeling for better vector representation.",
      "\u2022 The Context Encoding Layer incorporates word context and sequence order into modeling for better vector representation. This layer often uses CNN (He et al., 2015), LSTM (Chen et al., 2017), recursive neural network (Socher et al., 2011), or highway network (Gong et al., 2017). The sentence encoding type of model will stop at this step, and directly use the encoded vectors to compute the semantic similarity through vector distances and/or the output classi\ufb01cation layer. \u2022 The Interaction and Attention Layer calculates word pair (or n-gram pair) interactions using the outputs of the encoding layer. This is the key component for the interaction-aggregation type of model. In the PWIM model (He and Lin, 2016), the interactions are calculated by cosine similarity, Euclidean distance, and the dot product of the vectors. Various models put different weights on different interac- tions, primarily simulating the word alignment between two sentences.",
      "In the PWIM model (He and Lin, 2016), the interactions are calculated by cosine similarity, Euclidean distance, and the dot product of the vectors. Various models put different weights on different interac- tions, primarily simulating the word alignment between two sentences. The alignment information is useful for sentence pair modeling because the semantic relation between two sentences depends largely on the relations of aligned chunks as shown in the SemEval-2016 task of interpretable semantic textual similarity (Agirre et al., 2016). \u2022 The Output Classi\ufb01cation Layer adapts CNN or MLP to extract semantic-level features on the atten- tive alignment and applies softmax function to predict probability for each class. 1The code is available on the authors\u2019 homepages and GitHub: https://github.com/lanwuwei/SPM_toolkit",
      "3 Representative Models for Sentence Pair Modeling Table 1 gives a summary of typical models for sentence pair modeling in recent years. In particular, we investigate \ufb01ve models in depth: two are representative of the sentence encoding type of model, and three are representative of the interaction-aggregation type of model. These models have reported state-or-the-art results with varied architecture design (this section) and implementation details (Section 4.2). Models Sentence Interaction and Aggregation and Encoder Attention Classi\ufb01cation (Shen et al., 2017b) Directional self-attention network - MLP (Choi et al., 2017) Gumbel Tree-LSTM - MLP (Wieting and Gimpel, 2017) Gated recurrent average network - MLP SSE (Nie and Bansal, 2017) Shortcut-stacked BiLSTM - MLP (He et al., 2015) CNN multi-perspective matching pooling + MLP (Rockt\u00a8aschel et al., 2016) LSTM word-by-word neural attention MLP (Liu et al., 2016) LSTM coupled LSTMs dynamic pooling + MLP (Yin et al.,",
      ", 2015) CNN multi-perspective matching pooling + MLP (Rockt\u00a8aschel et al., 2016) LSTM word-by-word neural attention MLP (Liu et al., 2016) LSTM coupled LSTMs dynamic pooling + MLP (Yin et al., 2016) CNN attention matrix logistic regression DecAtt (Parikh et al., 2016) - dot product + soft alignment summation + MLP PWIM (He and Lin, 2016) BiLSTM cosine, Euclidean, dot product + hard alignment CNN + MLP (Wang and Jiang, 2017) LSTM encodes both context and attention word-by-word neural attention MLP ESIM (Chen et al., 2017) BiLSTM (Tree-LSTM) before and after attention dot product + soft alignment average and max pooling + MLP (Wang et al., 2017) BiLSTM multi-perspective matching BiLSTM + MLP (Shen et al., 2017a) BiLSTM + intra-attention soft alignment + orthogonal decomposition MLP (Ghaeini et al.,",
      ", 2017) BiLSTM multi-perspective matching BiLSTM + MLP (Shen et al., 2017a) BiLSTM + intra-attention soft alignment + orthogonal decomposition MLP (Ghaeini et al., 2018) dependent reading BiLSTM dot product + soft alignment average and max pooling+MLP Table 1: Summary of representative neural models for sentence pair modeling. The upper half contains sentence encoding models, and the lower half contains sentence pair interaction models. (a) InferSent (b) SSE (c) Classi\ufb01cation Layer Figure 1: Sentence encoding models focus on learning vector representations of individual sentences and then calculate the semantic relationship between sentences based on vector distance.",
      "3.1 The Bi-LSTM Max-pooling Network (InferSent) We choose the simple Bi-LSTM max-pooling network from InferSent (Conneau et al., 2017): \u2190\u2192 h i = BiLSTM(xi, \u2190\u2192 h i\u22121) (1) v = max(\u2190\u2192 h 1, \u2190\u2192 h 2, ..., \u2190\u2192 h n) (2) where \u2190\u2192 h i represents the concatenation of hidden states in both directons. It has shown better transfer learning capabilities than several other sentence embedding models, including SkipThought (Kiros et al., 2015) and FastSent (Hill et al., 2016), when trained on the natural language inference datasets.",
      "It has shown better transfer learning capabilities than several other sentence embedding models, including SkipThought (Kiros et al., 2015) and FastSent (Hill et al., 2016), when trained on the natural language inference datasets. 3.2 The Shortcut-Stacked Sentence Encoder Model (SSE) The Shortcut-Stacked Sentence Encoder model (Nie and Bansal, 2017) is a sentence-based embedding model, which enhances multi-layer Bi-LSTM with skip connection to avoid training error accumulation, and calculates each layer as follows: \u2190\u2192 h k i = BiLSTM(xk i , \u2190\u2192 h k i\u22121) (3) x1 i = wi (k = 1), xk i = [wi, \u2190\u2192 h k\u22121 i , \u2190\u2192 h k\u22122 i , ..., \u2190\u2192 h 1 i ] (k > 1) (4) v = max(\u2190\u2192 h m 1 , \u2190\u2192 h m 2 , ..., \u2190\u2192 h m n ) (5) where xk i is the input of the kth Bi-LSTM layer at time step i, which is the combination of outputs from all previous layers, \u2190\u2192 h k i represents the hidden state of the kth Bi-LSTM layer in both directions.",
      "The \ufb01nal sentence embedding v is the row-based max pooling over the output of the last Bi-LSTM layer, where n denotes the number of words within a sentence and m is the number of Bi-LSTM layers (m = 3 in SSE). 3.3 The Pairwise Word Interaction Model (PWIM) In the Pairwise Word Interaction model (He and Lin, 2016), each word vector wi is encoded with context through forward and backward LSTMs: \u2212\u2192 h i = LSTM f(wi, \u2212\u2192 h i\u22121) and \u2190\u2212 h i = LSTM b(wi, \u2190\u2212 h i+1).",
      "For every word pair (wa i , wb j) across sentences, the model directly calculates word pair interactions using cosine similarity, Euclidean distance, and dot product over the outputs of the encoding layer: D(\u2212\u2192 h i, \u2212\u2192 h j) = [cos(\u2212\u2192 h i, \u2212\u2192 h j), \u2225\u2212\u2192 h i \u2212\u2212\u2192 h j\u2225, \u2212\u2192 h i \u00b7 \u2212\u2192 h j] (6) The above equation not only applies to forward hidden state \u2212\u2192 h i and backward hidden state \u2190\u2212 h i, but also to the concatenation \u2190\u2192 h i = [\u2212\u2192 h i, \u2190\u2212 h i] and summation h+ i = \u2212\u2192 h i + \u2190\u2212 h i, resulting in a tensor D13\u00d7|sent1|\u00d7|sent2| after padding one extra bias term. A \u201chard\u201d attention is applied to the interaction tensor to build word alignment: selecting the most related word pairs and increasing the corresponding weights by 10 times. Then a 19-layer deep CNN is applied to aggregate the word interaction features for \ufb01nal classi\ufb01cation.",
      "A \u201chard\u201d attention is applied to the interaction tensor to build word alignment: selecting the most related word pairs and increasing the corresponding weights by 10 times. Then a 19-layer deep CNN is applied to aggregate the word interaction features for \ufb01nal classi\ufb01cation. 3.4 The Decomposable Attention Model (DecAtt) The Decomposable Attention model (Parikh et al., 2016) is one of the earliest models to introduce attention- based alignment for sentence pair modeling, and it achieved state-of-the-art results on the SNLI dataset with about an order of magnitude fewer parameters than other models (see more in Table 5) without relying on word order information. It computes the word pair interaction between wa i and wb j (from input sentences sa and sb, each with m and n words, respectively) as eij = F(wa i )T F(wb j), where F is a feedforward network; then alignment is determined as follows: \u03b2i = n X j=1 exp(eij) Pn k=1 exp(eik)wb j \u03b1j = m X i=1 exp(eij) Pm k=1 exp(ekj)wa i (7)",
      "where \u03b2i is the soft alignment between wa i and subphrases wb j in sentence sb, and vice versa for \u03b1j. The aligned phrases are fed into another feedforward network G: va i = G([wa i ; \u03b2i]) and vb j = G([wb j; \u03b1j]) to generate sets {va i } and {vb j}, which are aggregated by summation and then concatenated together for classi\ufb01cation. 3.5 The Enhanced Sequential Inference Model (ESIM) The Enhanced Sequential Inference Model (Chen et al., 2017) is closely related to the DecAtt model, but it differs in a few aspects. First, Chen et al. (2017) demonstrated that using Bi-LSTM to encode sequential contexts is important for performance improvement. They used the concatenation wi = \u2190\u2192 h i = [\u2212\u2192 h i, \u2190\u2212 h i] of both directions as in the PWIM model. The word alignment \u03b2i and \u03b1j between wa and wb are calculated the same way as in DecAtt. Second, they showed the competitive performance of recursive architecture with constituency parsing, which complements with sequential LSTM.",
      "The word alignment \u03b2i and \u03b1j between wa and wb are calculated the same way as in DecAtt. Second, they showed the competitive performance of recursive architecture with constituency parsing, which complements with sequential LSTM. The feedforward function G in DecAtt is replaced with Tree-LSTM: va i = TreeLSTM([wa i ; \u03b2i; wa i \u2212\u03b2i; wa i \u2299\u03b2i]) (8) vb j = TreeLSTM([wb j; \u03b1j; wb j \u2212\u03b1j; wb j \u2299\u03b1j]) (9) Third, instead of using summation in aggregation, ESIM adapts the average and max pooling and concate- nation v = [va ave; va max; vb ave; vb max] before passing through multi-layer perceptron (MLP) for classi\ufb01cation: va ave = m X i=1 va i m , va max = m max i=1 va i , vb ave = n X j=1 vb j n , vb max = n max j=1 vb j (10) (a) PWIM (b) ESIMseq (DecAtt is similar and simpler.)",
      "Figure 2: Sentence pair interaction models use different word alignment mechanisms before aggregation. 4 Experiments and Analysis 4.1 Datasets We conducted sentence pair modeling experiments on eight popular datasets: two NLI datasets, three PI datasets, one STS dataset and two QA datasets. Table 2 gives a comparison of these datasets: \u2022 SNLI (Bowman et al., 2015) contains 570k hypotheses written by crowdsourcing workers given the premises. It focuses on three semantic relations: the premise entails the hypothesis (entailment), they contradict each other (contradiction), or they are unrelated (neutral). \u2022 Multi-NLI (Williams et al., 2017) extends the SNLI corpus to multiple genres of written and spoken texts with 433k sentence pairs.",
      "Dataset Size Example and Label SNLI train 550,152 sa: Two men on bicycles competing in a race. entailment dev 10,000 sb: Men are riding bicycles on the street. neutral test 10,000 contradict Multi-NLI train 392,703 sa: The Old One always comforted Ca\u2019daan, except today. entailment dev 20,000 sb: Ca\u2019daan knew the Old One very well. neutral test 20,000 contradict Quora train 384,348 sa: What should I do to avoid sleeping in class? paraphrase dev 10,000 sb: How do I not sleep in a boring class? non-paraphrase test 10,000 Twitter-URL train 42,200 sa: Letter warned Wells Fargo of \u201cwidespread\u201d fraud in 2007. paraphrase dev - sb: Letters suggest Wells Fargo scandal started earlier.",
      "non-paraphrase test 10,000 Twitter-URL train 42,200 sa: Letter warned Wells Fargo of \u201cwidespread\u201d fraud in 2007. paraphrase dev - sb: Letters suggest Wells Fargo scandal started earlier. non-paraphrase test 9,324 PIT-2015 train 11,530 sa: Ezekiel Ansah w the 3D shades Popped out lens paraphrase dev 4,142 sb: Ezekiel Ansah was wearing lens less 3D glasses non-paraphrase test 838 STS-2014 train 7,592 sa: Then perhaps we could have avoided a catastrophe. score [0, 5] dev - sb: Then we might have been able to avoid a disaster. 4.6 test 3,750 WikiQA train 8,672 sa: How much is 1 tablespoon of water? true dev 1,130 sb: In Australia one tablespoon (measurement unit) is 20 mL. false test 2,351 TrecQA train 53,417 sa: Who was Lincoln\u2019s Secretary of State?",
      "true dev 1,130 sb: In Australia one tablespoon (measurement unit) is 20 mL. false test 2,351 TrecQA train 53,417 sa: Who was Lincoln\u2019s Secretary of State? true dev 1,148 sb: William Seward false test 1,517 Table 2: Basic statistics and examples of different datasets for sentence pair modeling tasks. \u2022 Quora (Iyer et al., 2017) contains 400k question pairs collected from the Quora website. This dataset has balanced positive and negative labels indicating whether the questions are duplicated or not. \u2022 Twitter-URL (Lan et al., 2017) includes 50k sentence pairs collected from tweets that share the same URL of news articles. This dataset contains both formal and informal language. \u2022 PIT-2015 (Xu et al., 2015) comes from SemEval-2015 and was collected from tweets under the same trending topic. It contains naturally occurred (i.e. written by independent Twitter users spontaneously) paraphrases and non-paraphrases with varied topics and language styles.",
      "It contains naturally occurred (i.e. written by independent Twitter users spontaneously) paraphrases and non-paraphrases with varied topics and language styles. \u2022 STS-2014 (Agirre et al., 2014) is from SemEval-2014, constructed from image descriptions, news headlines, tweet news, discussion forums, and OntoNotes (Hovy et al., 2006). \u2022 WikiQA (Yang et al., 2015) is an open-domain question-answering dataset. Following He and Lin (2016), questions without correct candidate answer sentences are excluded, and answer sentences are truncated to 40 tokens, resulting in 12k question-answer pairs for our experiments. \u2022 TrecQA (Wang et al., 2007) is an answer selection task of 56k question-answer pairs and created in Text Retrieval Conferences (TREC). For both WikiQA and TrecQA datasets, the best answer is selected according to the semantic relatedness with the question.",
      "For both WikiQA and TrecQA datasets, the best answer is selected according to the semantic relatedness with the question. 4.2 Implementation Details We implement all the models with the same PyTorch framework.23 Below, we summarize the implementa- tion details that are key for reproducing results for each model: \u2022 SSE: This model can converge very fast, for example, 2 or 3 epochs for the SNLI dataset. We control the convergence speed by updating the learning rate for each epoch: speci\ufb01cally, lr = 1 2 epoch i 2 \u2217init lr, where init lr is the initial learning rate and epoch i is the index of current epoch. 2InferSent and SSE have open-source PyTorch implementations by the original authors, for which we reused part of the code. 3Our code is available at: https://github.com/lanwuwei/SPM_toolkit",
      "\u2022 DecAtt: It is important to use gradient clipping for this model: for each gradient update, we check the L2 norm of all the gradient values, if it is greater than a threshold b, we scale the gradient by a factor \u03b1 = b/L2 norm. Another useful procedure is to assemble batches of sentences with similar length. \u2022 ESIM: Similar but different from DecAtt, ESIM batches sentences with varied length and uses masks to \ufb01lter out padding information. In order to batch the parse trees within Tree-LSTM recursion, we follow Bowman et al.\u2019s (2016) procedure that converts tree structures into the linear sequential structure of a shift reduce parser. Two additional masks are used for producing left and right children of a tree node. \u2022 PWIM: The cosine and Euclidean distances used in the word interaction layer have smaller values for similar vectors while dot products have larger values. The performance increases if we add a negative sign to make all the vector similarity measurements behave consistently. 4.3 Analysis 4.3.1 Re-implementation Results vs.",
      "The performance increases if we add a negative sign to make all the vector similarity measurements behave consistently. 4.3 Analysis 4.3.1 Re-implementation Results vs. Previously Reported Results Table 3 and 4 show the results reported in the original papers and the replicated results with our implemen- tation. We use accuracy, F1 score, Pearson\u2019s r, Mean Average Precision (MAP), and Mean Reciprocal Rank (MRR) for evaluation on different datasets following the literature. Our reproduced results are slightly lower than the original results by 0.5 \u223c1.5 points on accuracy. We suspect the following potential reasons: (i) less extensive hyperparameter tuning for each individual dataset; (ii) only one run with random seeding to report results; and (iii) use of different neural network toolkits: for example, the original ESIM model was implemented with Theano, and PWIM model was in Torch. 4.3.2 Effects of Model Components Herein, we examine the main components that account for performance in sentence pair modeling. How important is LSTM encoded context information for sentence pair modeling? Regarding DecAtt, Parikh et al.",
      "4.3.2 Effects of Model Components Herein, we examine the main components that account for performance in sentence pair modeling. How important is LSTM encoded context information for sentence pair modeling? Regarding DecAtt, Parikh et al. (2016) mentioned that \u201cintra-sentence attention is optional\u201d; they can achieve competitive results without considering context information. However, not surprisingly, our experi- ments consistently show that encoding sequential context information with LSTM is critical. Compared to DecAtt, ESIM shows better performance on every dataset (see Table 4 and Figure 3). The main difference between ESIM and DecAtt that contributes to performance improvement, we found, is the use of Bi-LSTM and Tree-LSTM for sentence encoding, rather than the different choices of aggregation functions. Why does Tree-LSTM help with Twitter data? Chen et al. (2017) offered a simple combination (ESIMseq+tree) by averaging the prediction probabilities of two ESIM variants that use sequential Bi-LSTM and Tree-LSTM respectively, and suggested \u201cparsing information complements very well with ESIM and further improves the performance\u201d.",
      "Chen et al. (2017) offered a simple combination (ESIMseq+tree) by averaging the prediction probabilities of two ESIM variants that use sequential Bi-LSTM and Tree-LSTM respectively, and suggested \u201cparsing information complements very well with ESIM and further improves the performance\u201d. However, we found that adding Tree-LSTM only helps slightly or not at all for most datasets, but it helps noticably with the two Twitter paraphrase datasets. We hypothesize the reason is that these two datasets come from real-world tweets which often contain extraneous text fragments, in contrast to SNLI and other datasets that have sentences written by crowdsourcing workers. For example, the segment \u201cever wondered ,\u201d in the sentence pair ever wondered , why your recorded #voice sounds weird to you? and why do our recorded voices sound so weird to us? introduces a disruptive context into the Bi-LSTM encoder, while Tree-LSTM can put it in a less important position after constituency parsing. How important is attentive interaction for sentence pair modeling? Why does SSE excel on Quora? Both ESIM and DecAtt (Eq.",
      "introduces a disruptive context into the Bi-LSTM encoder, while Tree-LSTM can put it in a less important position after constituency parsing. How important is attentive interaction for sentence pair modeling? Why does SSE excel on Quora? Both ESIM and DecAtt (Eq. 7) calculate an attention-based soft alignment between a sentence pair, which was also proposed in (Rockt\u00a8aschel et al., 2016) and (Wang and Jiang, 2017) for sentence pair modeling, whereas PWIM utilizes a hard attention mechanism. Both attention strategies are critical for model performance. In PWIM model (He and Lin, 2016), we observed a 1\u223c2 point performance drop after",
      "Model SNLI Multi-NLI Quora Twitter-URL PIT-2015 STS-2014 WikiQA TrecQA Acc Acc m/Acc um Acc F1 F1 r MAP/MRR MAP/MRR InferSent 0.845 -/- - - - 0.7005 - - SSE 0.860 0.746/0.736 - - - - - - DecAtt 0.863 - 0.8653 - - - - - ESIMtree 0.878 - - - - - - - ESIMseq 0.880 0.723/0.7214 - - - - - - ESIMseq+tree 0.886 - - - - - - - PWIM - - - 0.749 0.667 0.767 0.709/0.723 0.759/0.822 Table 3: Reported results from original papers, which are mostly limited to a few datasets. For the Multi-NLI dataset, Acc m represents testing accuracy for the matched genre and Acc um for the unmatched genre.",
      "For the Multi-NLI dataset, Acc m represents testing accuracy for the matched genre and Acc um for the unmatched genre. Model SNLI Multi-NLI Quora Twitter-URL PIT-2015 STS-2014 WikiQA TrecQA Acc Acc m/Acc um Acc F1 F1 r MAP/MRR MAP/MRR InferSent 0.846 0.705/0.703 0.866 0.746 0.451 0.715 0.287/0.287 0.521/0.559 SSE 0.855 0.740/0.734 0.878 0.650 0.422 0.378 0.624/0.638 0.628/0.670 DecAtt 0.856 0.719/0.713 0.845 0.652 0.430 0.317 0.603/0.619 0.660/0.712 ESIMtree 0.864 0.736/0.727 0.755 0.740 0.447 0.493 0.618/0.633 0.698/0.",
      "317 0.603/0.619 0.660/0.712 ESIMtree 0.864 0.736/0.727 0.755 0.740 0.447 0.493 0.618/0.633 0.698/0.734 ESIMseq 0.870 0.752/0.738 0.850 0.748 0.520 0.602 0.652/0.664 0.771/0.795 ESIMseq+tree 0.871 0.753/0.748 0.854 0.759 0.538 0.589 0.647/0.658 0.749/0.768 PWIM 0.822 0.722/0.716 0.834 0.761 0.656 0.743 0.706/0.723 0.739/0.795 Table 4: Replicated results with our reimplementation in PyTorch across multiple tasks and datasets. The best result in each dataset is denoted by a bold typeface, and the second best is denoted by an underline.",
      "706/0.723 0.739/0.795 Table 4: Replicated results with our reimplementation in PyTorch across multiple tasks and datasets. The best result in each dataset is denoted by a bold typeface, and the second best is denoted by an underline. removing the hard attention, 0\u223c3 point performance drop and \u223c25% training time reduction after removing the 19-layer CNN aggregation. Likely without even the authors of SSE knowing, the SSE model performs extraordinarily well on the Quora corpus, perhaps because Quora contains many sentence pairs with less complicated inter-sentence interactions (e.g., many identical words in the two sentences) and incorrect ground truth labels (e.g., What is your biggest regret in life? and What\u2019s the biggest regret you\u2019ve had in life? are labeled as non-duplicate questions by mistake). 4.3.3 Learning Curves and Training Time Figure 3 shows the learning curves. The DecAtt model converges quickly and performs well on large NLI datasets due to its design simplicity.",
      "are labeled as non-duplicate questions by mistake). 4.3.3 Learning Curves and Training Time Figure 3 shows the learning curves. The DecAtt model converges quickly and performs well on large NLI datasets due to its design simplicity. PWIM is the slowest model (see time comparison in Table 5) but shows very strong performance on semantic similarity and paraphrase identi\ufb01cation datasets. ESIM and SSE keep a good balance between training time and performance. 3This number was reported in (Tomar et al., 2017) by co-authors of DecAtt (Parikh et al., 2016). 4This number was reproduced by Williams et al. (2017). 5This number was generated by InferSent traind on SNLI and Multi-NLI datasets.",
      "Figure 3: Training curves of ESIM, DecAtt, PWIM, SSE and InferSent models on eight datasets. InferSent SSE DecAtt ESIMseq ESIMtree PWIM Number of parameters 47M 140M 380K 4.3M 7.7M 2.2M Avg epoch time (seconds) / sentence pair 0.005 0.032 0.0006 0.013 0.016 0.60 Ratio compared to DecAtt model \u00d78 \u00d753 1 \u00d722 \u00d726 \u00d71000 Table 5: Average training time per sentence pair in the Twitter-URL dataset (similar time for other datasets). 4.3.4 Effects of Training Data Size As shown in Figure 4, we experimented with different training sizes of the largest SNLI dataset. All the models show improved performance as we increase the training size. ESIM and SSE have very similar trends and clearly outperform PWIM on the SNLI dataset. DecAtt shows a performance jump when the training size exceeds a threshold.",
      "All the models show improved performance as we increase the training size. ESIM and SSE have very similar trends and clearly outperform PWIM on the SNLI dataset. DecAtt shows a performance jump when the training size exceeds a threshold. 4.3.5 Categorical Performance Comparison We conducted an in-depth analysis of model performance on the Multi-domain NLI dataset based on dif- ferent categories: text genre, sentence pair overlap, and sentence length. As shown in Table 7, all models have comparable performance between matched genre and unmatched genre. Sentence length and overlap turn out to be two important factors \u2013 the longer the sentences and the fewer tokens in common, the more challenging it is to determine their semantic relationship. These phenomena shared by the state-of-the-art systems re\ufb02ect their similar design framework which is symmetric at processing both sentences in the pair, while question answering and natural language inference tasks are directional (Ghaeini et al., 2018). How to incorporate asymmetry into model design will be worth more exploration in future research.",
      "How to incorporate asymmetry into model design will be worth more exploration in future research. 4.3.6 Transfer Learning Experiments In addition to the cross-domain study (Table 7), we conducted transfer learning experiments on three para- phrase identi\ufb01cation datasets (Table 6). The most noteworthy phenomenon is that the SSE model performs better on Twitter-URL and PIT-2015 when trained on the large out-of-domain Quora data than the small in-domain training data. Two likely reasons are: (i) the SSE model with over 29 million parameters is data hungry and (ii) SSE model is a sentence encoding model, which generalizes better across domains/tasks than sentence pair interaction models. Sentence pair interaction models may encounter dif\ufb01culties on Quora, which contains sentence pairs with the highest word overlap (51.5%) among all datasets and often causes",
      "Figure 4: Performance vs. training size (log scale in x-axis) on SNLI dataset. Models Quora URL PIT train/test trained on Quora on PIT InferSent 0.866 0.528 0.394 0.451 SSE 0.878 0.681 0.594 0.422 DecAtt 0.845 0.649 0.497 0.430 ESIMseq 0.850 0.643 0.501 0.520 PWIM 0.835 0.601 0.518 0.656 trained on URL InferSent 0.703 0.746 0.535 0.451 SSE 0.630 0.650 0.477 0.422 DecAtt 0.632 0.652 0.450 0.430 ESIMseq 0.641 0.748 0.511 0.520 PWIM 0.678 0.761 0.634 0.656 Table 6: Transfer learning experiments for para- phrase identi\ufb01cation task.",
      "Category #Examples InferSent SSE DecAtt ESIMseq PWIM Matched Fiction 1973 0.703 0.727 0.706 0.742 0.707 Genre Government 1945 0.753 0.746 0.743 0.790 0.751 Slate 1955 0.653 0.670 0.671 0.697 0.670 Telephone 1966 0.718 0.728 0.717 0.753 0.709 Travel 1976 0.705 0.701 0.733 0.752 0.714 Mismatched 9/11 1974 0.685 0.710 0.699 0.737 0.711 Genre Face-to-face 1974 0.713 0.729 0.720 0.761 0.710 Letters 1977 0.734 0.757 0.754 0.775 0.757 OUP 1961 0.698 0.715 0.719 0.759 0.710 Verbatim 1946 0.691 0.",
      "710 Letters 1977 0.734 0.757 0.754 0.775 0.757 OUP 1961 0.698 0.715 0.719 0.759 0.710 Verbatim 1946 0.691 0.701 0.709 0.725 0.713 Overlap >60% 488 0.756 0.795 0.805 0.842 0.811 30% \u223c60% 3225 0.740 0.751 0.745 0.769 0.743 <30% 6102 0.685 0.689 0.691 0.727 0.682 Length >20 tokens 3730 0.692 0.676 0.685 0.731 0.694 10\u223c20 tokens 3673 0.712 0.725 0.721 0.753 0.720 <10 tokens 2412 0.721 0.758 0.748 0.762 0.724 Table 7: Categorical performance (accuracy) on Multi-NLI dataset.",
      "712 0.725 0.721 0.753 0.720 <10 tokens 2412 0.721 0.758 0.748 0.762 0.724 Table 7: Categorical performance (accuracy) on Multi-NLI dataset. Overlap is the percentage of shared tokens between two sentences. Length is calculated based on the number of tokens of the longer sentence. the interaction patterns to focus on a few key words that differ. In contrast, the Twitter-URL dataset has the lowest overlap (23.0%) with a semantic relationship that is mainly based on the intention of the tweets. 5 Conclusion We analyzed \ufb01ve different neural models (and their variations) for sentence pair modeling and conducted a series of experiments with eight representative datasets for different NLP tasks. We quanti\ufb01ed the impor- tance of the LSTM encoder and attentive alignment for inter-sentence interaction, as well as the transfer learning ability of sentence encoding based models. We showed that the SNLI corpus of over 550k sen- tence pairs cannot saturate the learning curve. We systematically compared the strengths and weaknesses of different network designs and provided insights for future work.",
      "We showed that the SNLI corpus of over 550k sen- tence pairs cannot saturate the learning curve. We systematically compared the strengths and weaknesses of different network designs and provided insights for future work. Acknowledgements We thank Ohio Supercomputer Center (Center, 2012) for computing resources. This work was supported in part by NSF CRII award (RI-1755898) and DARPA through the ARO (W911NF-17-C-0095). The con- tent of the information in this document does not necessarily re\ufb02ect the position or the policy of the U.S. Government, and no of\ufb01cial endorsement should be inferred.",
      "References [Agirre et al.2014] Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Wei- wei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2014. Semeval-2014 task 10: Multilingual semantic textual similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014). [Agirre et al.2016] Eneko Agirre, Aitor Gonzalez-Agirre, Inigo Lopez-Gazpio, Montse Maritxalar, German Rigau, and Larraitz Uria. 2016. Semeval-2016 task 2: Interpretable semantic textual similarity. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval). [Bowman et al.2015] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference.",
      "In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval). [Bowman et al.2015] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). [Bowman et al.2016] Samuel R. Bowman, Jon Gauthier, Abhinav Rastogi, Raghav Gupta, Christopher D. Manning, and Christopher Potts. 2016. A fast uni\ufb01ed model for parsing and sentence understanding. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL). [Center2012] Ohio Supercomputer Center. 2012. Oakley supercomputer. http://osc.edu/ark:/19495/ hpc0cvqn. [Chen et al.2017] Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2017. Enhanced LSTM for natural language inference.",
      "[Chen et al.2017] Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2017. Enhanced LSTM for natural language inference. In Proceedings of the 55th Annual Meeting of the Association for Computa- tional Linguistics (ACL). [Choi et al.2017] Jihun Choi, Kang Min Yoo, and Sang-goo Lee. 2017. Unsupervised learning of task-speci\ufb01c tree structures with tree-LSTMs. arXiv preprint arXiv:1707.02786. [Conneau et al.2017] Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP). [Dagan et al.2006] Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006.",
      "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP). [Dagan et al.2006] Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL recognising textual entail- ment challenge. In Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classi\ufb01cation, and Recognizing Textual Entailment. [Dolan and Brockett2005] William B Dolan and Chris Brockett. 2005. Automatically constructing a corpus of senten- tial paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP). [Ghaeini et al.2018] Reza Ghaeini, Sadid A Hasan, Vivek Datla, Joey Liu, Kathy Lee, Ashequl Qadir, Yuan Ling, Aaditya Prakash, Xiaoli Z Fern, and Oladimeji Farri. 2018. DR-BiLSTM: Dependent reading bidirectional LSTM for natural language inference.",
      "2018. DR-BiLSTM: Dependent reading bidirectional LSTM for natural language inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). [Gong et al.2017] Yichen Gong, Heng Luo, and Jian Zhang. 2017. Natural language inference over interaction space. arXiv preprint arXiv:1709.04348. [He and Lin2016] Hua He and Jimmy Lin. 2016. Pairwise word interaction modeling with deep neural networks for semantic similarity measurement. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). [He et al.2015] Hua He, Kevin Gimpel, and Jimmy Lin. 2015. Multi-perspective sentence similarity modeling with convolutional neural networks. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). [Hill et al.2016] Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.",
      "Multi-perspective sentence similarity modeling with convolutional neural networks. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). [Hill et al.2016] Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sentences from unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). [Hovy et al.2006] Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The 90% solution. In Proceedings of the Human Language Technology Conference of the North Amer- ican Chapter of the ACL (NAACL).",
      "[Iyer et al.2017] Shankar Iyer, Nikhil Dandekar, and Kornl Csernai. 2017. First Quora Dataset Release: Question Pairs. In https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs. [Kiros et al.2015] Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Tor- ralba, and Sanja Fidler. 2015. Skip-thought vectors. In Advances in Neural Information Processing Systems (NIPS). [Lan and Xu2018] Wuwei Lan and Wei Xu. 2018. The importance of subword embeddings in sentence pair model- ing. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). [Lan et al.2017] Wuwei Lan, Siyu Qiu, Hua He, and Wei Xu. 2017. A continuously growing dataset of sentential paraphrases.",
      "[Lan et al.2017] Wuwei Lan, Siyu Qiu, Hua He, and Wei Xu. 2017. A continuously growing dataset of sentential paraphrases. In Proceedings of The 2017 Conference on Empirical Methods on Natural Language Processing (EMNLP). [Liu et al.2016] Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016. Modelling interaction of sentence pair with coupled-LSTMs. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP). [Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems (NIPS). [Nie and Bansal2017] Yixin Nie and Mohit Bansal. 2017. Shortcut-stacked sentence encoders for multi-domain infer- ence. In Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for NLP.",
      "[Nie and Bansal2017] Yixin Nie and Mohit Bansal. 2017. Shortcut-stacked sentence encoders for multi-domain infer- ence. In Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for NLP. [Parikh et al.2016] Ankur Parikh, Oscar T\u00a8ackstr\u00a8om, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP). [Pennington et al.2014] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). [Rajpurkar et al.2016] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text.",
      "[Rajpurkar et al.2016] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP). [Rockt\u00a8aschel et al.2016] Tim Rockt\u00a8aschel, Edward Grefenstette, Karl Moritz Hermann, Tom\u00b4a\u02c7s Ko\u02c7cisk`y, and Phil Blun- som. 2016. Reasoning about entailment with neural attention. In Proceedings of the International Conference on Learning Representations (ICLR). [Shen et al.2017a] Gehui Shen, Yunlun Yang, and Zhi-Hong Deng. 2017a. Inter-weighted alignment network for sen- tence pair modeling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "2017a. Inter-weighted alignment network for sen- tence pair modeling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP). [Shen et al.2017b] Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Shirui Pan, and Chengqi Zhang. 2017b. Disan: Directional self-attention network for RNN/CNN-free language understanding. In Proceedings of the Association for the Advancement of Arti\ufb01cial Intelligence (AAAI). [Socher et al.2011] Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. 2011. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML). [Tomar et al.2017] Gaurav Singh Tomar, Thyago Duque, Oscar T\u00a8ackstr\u00a8om, Jakob Uszkoreit, and Dipanjan Das. 2017. Neural paraphrase identi\ufb01cation of questions with noisy pretraining.",
      "[Tomar et al.2017] Gaurav Singh Tomar, Thyago Duque, Oscar T\u00a8ackstr\u00a8om, Jakob Uszkoreit, and Dipanjan Das. 2017. Neural paraphrase identi\ufb01cation of questions with noisy pretraining. In Proceedings of the First Workshop on Subword and Character Level Models in NLP. [Wang and Jiang2017] Shuohang Wang and Jing Jiang. 2017. A compare-aggregate model for matching text se- quences. In Proceedings of the International Conference on Learning Representations (ICLR).",
      "[Wang et al.2007] Mengqiu Wang, Noah A Smith, and Teruko Mitamura. 2007. What is the Jeopardy model? A quasi-synchronous grammar for qa. In Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL). [Wang et al.2017] Zhiguo Wang, Wael Hamza, and Radu Florian. 2017. Bilateral multi-perspective matching for natu- ral language sentences. In Proceedings of the Twenty-Sixth International Joint Conference on Arti\ufb01cial Intelligence (IJCAI). [Wieting and Gimpel2017] John Wieting and Kevin Gimpel. 2017. Revisiting recurrent networks for paraphrastic sentence embeddings. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL). [Wieting et al.2016] John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2016. Towards universal para- phrastic sentence embeddings.",
      "[Wieting et al.2016] John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2016. Towards universal para- phrastic sentence embeddings. In Proceedings of the 4th International Conference on Learning Representations (ICLR). [Williams et al.2017] Adina Williams, Nikita Nangia, and Samuel R Bowman. 2017. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426. [Xu et al.2014] Wei Xu, Alan Ritter, Chris Callison-Burch, William B. Dolan, and Yangfeng Ji. 2014. Extracting lex- ically divergent paraphrases from Twitter. Transactions of the Association for Computational Linguistics (TACL). [Xu et al.2015] Wei Xu, Chris Callison-Burch, and William B. Dolan. 2015. SemEval-2015 Task 1: Paraphrase and semantic similarity in Twitter (PIT). In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval).",
      "2015. SemEval-2015 Task 1: Paraphrase and semantic similarity in Twitter (PIT). In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval). [Yang et al.2015] Yi Yang, Wen-tau Yih, and Christopher Meek. 2015. WikiQA: A challenge dataset for open-domain question answering. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). [Yin et al.2016] Wenpeng Yin, Hinrich Schtze, Bing Xiang, and Bowen Zhou. 2016. ABCNN: Attention-based convo- lutional neural network for modeling sentence pairs. Transactions of the Association for Computational Linguistics (TACL).",
      "[Yin et al.2016] Wenpeng Yin, Hinrich Schtze, Bing Xiang, and Bowen Zhou. 2016. ABCNN: Attention-based convo- lutional neural network for modeling sentence pairs. Transactions of the Association for Computational Linguistics (TACL). Appendices A Pretrained Word Embeddings We used the 200-dimensional GloVe word vectors (Pennington et al., 2014), trained on 27 billion words from Twitter (vocabulary size of 1.2 milion words) for Twitter URL (Lan et al., 2017) and PIT-2015 (Xu et al., 2015) datasets, and the 300-dimensional GloVe vectors, trained on 840 billion words (vocabulary size of 2.2 milion words) from Common Crawl for all other datasets. For out-of-vocabulary words, we initialized the word vectors using normal distribution with mean 0 and deviation 1. B Hyper-parameter Settings We followed original papers or code implementations to set hyper-parameters for these models.",
      "For out-of-vocabulary words, we initialized the word vectors using normal distribution with mean 0 and deviation 1. B Hyper-parameter Settings We followed original papers or code implementations to set hyper-parameters for these models. In Infersent model (Conneau et al., 2017), the hidden dimension size for Bi-LSTM is 2048, and the fully connected layers have 512 hidden units. In SSE model (Nie and Bansal, 2017), the hidden size for three Bi-LSTMs is 512, 2014 and 2048, respectively. The fully connected layers have 1600 units. PWIM (He and Lin, 2016) and ESIM (Chen et al., 2017) both use Bi-LSTM for context encoding, having 200 hidden units and 300 hidden units respectively. The DecAtt model (Parikh et al., 2016) uses three kinds of feed forward networks, all of which have 300 hidden units. Other parameters like learning rate, batch size, dropout rate, and all of them use the same settings as in original papers.",
      "C Fine-tuning the Models It is not practical to \ufb01ne tune every hyper-parameter in every model and every dataset, since we want to show how these models can generalize well on other datasets, we need try to avoid \ufb01ne-tuning these parameters on some speci\ufb01c datasets, otherwise we can easily get over-\ufb01tted models. Therefore, we keep the hyper- parameters unchanged across different datasets, to demonstrate the generalization capability of each model. The default number of epochs for training these models is set to 20, if some models could converge earlier (no more performance gain on development set), we would stop running them before they approached epoch 20. The 20 epochs can guarantee every model get converged on every dataset."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1806.04330.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":10687,
  "avg_doclen":175.1967213115,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1806.04330.pdf"
    }
  }
}