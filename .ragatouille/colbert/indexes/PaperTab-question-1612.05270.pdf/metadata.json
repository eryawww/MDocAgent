{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "A Simple Approach to Multilingual Polarity Classi\ufb01cation in Twitter Eric S. Tellez\u2217\u2020 eric.tellez@infotec.mx Sabino Miranda-Jim\u00b4enez\u2217\u2020 sabino.miranda@infotec.mx Mario Gra\ufb00\u2217\u2020 mario.graff@infotec.mx Daniela Moctezuma\u2217\u2021 dmoctezuma@centrogeo.edu.mx Ranyart R. Su\u00b4arez\u00a7 ranyart@dep.fie.umich.mx Oscar S. Siordia\u2021 osanchez@centrogeo.edu.mx Sept. 2016 Abstract Recently, sentiment analysis has received a lot of attention due to the interest in mining opinions of social media users. Sentiment analysis consists in determining the polarity of a given text, i.e., its degree of positiveness or negativeness. Traditionally, Sentiment Analysis algorithms have been tailored to a speci\ufb01c language given the complexity of having a number of lexical variations and errors introduced by the people generating content.",
      "Traditionally, Sentiment Analysis algorithms have been tailored to a speci\ufb01c language given the complexity of having a number of lexical variations and errors introduced by the people generating content. In this contribution, our aim is to provide a simple to implement and easy to use multilingual framework, that can serve as a baseline for sentiment analysis contests, and as starting point to build new sentiment analysis systems. We compare our approach in eight di\ufb00erent languages, three of them have important international contests, namely, SemEval (English), TASS (Spanish), and SENTIPOLC (Italian). Within the competitions our approach reaches from medium to high positions in the rankings; whereas in the remaining languages our approach outperforms the reported results. 1 Introduction Sentiment analysis is a crucial task in opinion mining \ufb01eld where the goal is to extract opinions, emotions, or attitudes to di\ufb00erent entities (person, objects, news, among others). Clearly, this task is of interest for all languages; however, there exists a signi\ufb01cant gap between English state-of-the-art methods and other languages.",
      "Clearly, this task is of interest for all languages; however, there exists a signi\ufb01cant gap between English state-of-the-art methods and other languages. It is expected that some researchers decided to test the straightforward approach which consists in, \ufb01rst, translating the messages to English, and, then, use a high performing English sentiment classi\ufb01er (for instance, see [3] and [4]) instead of creating a sentiment classi\ufb01er optimized for a given language. However, the advantages of a properly tuned sentiment classi\ufb01er have been studied for di\ufb00erent languages (for instance, see [18, 25, 1, 2]). This manuscript focuses on the particular case of multilingual sentiment analysis of short informal texts such as Twitter messages. Our aim is to provide an easy-to-use tool to create sentiment classi\ufb01ers based \u2217CONACyT Consejo Nacional de Ciencia y Tecnolog\u00b4\u0131a, Direcci\u00b4on de C\u00b4atedras, Insurgentes Sur 1582, Cr\u00b4edito Constructor 03940, Ciudad de M\u00b4exico, M\u00b4exico.",
      "\u2020INFOTEC Centro de Investigaci\u00b4on e Innovaci\u00b4on en Tecnolog\u00b4\u0131as de la Informaci\u00b4on y Comunicaci\u00b4on, Circuito Tecnopolo Sur No 112, Fracc. Tecnopolo Pocitos II, Aguascalientes 20313, M\u00b4exico. \u2021Centro de Investigaci\u00b4on en Geograf\u00b4\u0131a y Geom\u00b4atica \u201cIng. Jorge L. Tamayo\u201d, A.C. Circuito Tecnopolo Norte No. 117, Col. Tecnopolo Pocitos II, C.P. 20313,. Aguascalientes, Ags, M\u00b4exico. \u00a7Departamento de Estudios de Posgrado, Facultad de Ingenier\u00b4\u0131a El\u00b4ectrica, Universidad Michoacana de San Nicol\u00b4as de Hidalgo, Santiago Tapia 403, Morelia 58000, M\u00b4exico 1 arXiv:1612.05270v1  [cs.CL]  15 Dec 2016",
      "on supervised learning (i.e., labeled dataset) where the classi\ufb01er should be competitive to those sentiment classi\ufb01ers carefully tuned by some given languages. Furthermore, our second contribution is to create a well-performing baseline to compare new sentiment classi\ufb01ers in a broad range of languages or to bootstrap new sentiment analysis systems. Our approach is based on selecting the best text-transforming techniques that optimize some performance measures where the chosen techniques are robust to typical writing errors. In this context, we propose a robust multilingual sentiment analysis method, tested in eight di\ufb00erent languages: Spanish, English, Italian, Arabic, German, Portuguese, Russian and Swedish. We compare our approach ranking in three international contests: TASS\u201915, SemEval\u201915-16 and SENTIPOLC\u201914, for Spanish, English and Italian respectively; the remaining languages are compared directly with the results reported in the literature. The experimental results locate our approach in good positions for all considered competitions; and excellent results in the other \ufb01ve languages tested.",
      "The experimental results locate our approach in good positions for all considered competitions; and excellent results in the other \ufb01ve languages tested. Finally, even when our method is almost cross-language, it can be extended to take advantage of language dependencies; we also provide experimental evidence of the advantages of using these language-dependent techniques. The rest of the manuscript is organized as follows. Section 2 describes our proposed Sentiment Analysis method. Section 3 describes the datasets and contests used to test our approach; whereas, the experimental results, and, the discussion are presented on Section 4. Finally, Section 5 concludes. 2 Our Approach: Multilingual Polarity Classi\ufb01cation We propose a method for multilingual polarity classi\ufb01cation that can serve as a baseline as well as a framework to build more complex sentiment analysis systems due to its simplicity and availability as an open source software1. As we mentioned, this baseline algorithm for multilingual Sentiment Analysis (B4MSA) was designed with the purpose of being multilingual and easy to implement. B4MSA is not a na\u00a8\u0131ve baseline which is experimentally proved by evaluating it on several international competitions.",
      "As we mentioned, this baseline algorithm for multilingual Sentiment Analysis (B4MSA) was designed with the purpose of being multilingual and easy to implement. B4MSA is not a na\u00a8\u0131ve baseline which is experimentally proved by evaluating it on several international competitions. In a nutshell, B4MSA starts by applying text-transformations to the messages, then transformed text is represented in a vector space model (see Subsection 2.3), and \ufb01nally, a Support Vector Machine (with linear kernel) is used as the classi\ufb01er. B4MSA uses a number of text transformations that are categorized in cross-language features (see Subsection 2.1) and language dependent features (see Subsection 2.2). It is important to note that, all the text-transformations considered are either simple to implement or there is a well-known library (e.g. [9, 23]) to use them. It is important to note that to maintain the cross-language property, we limit ourselves to not use additional knowledge, this include knowledge from a\ufb00ective lexicons or models based on distributional semantics.",
      "[9, 23]) to use them. It is important to note that to maintain the cross-language property, we limit ourselves to not use additional knowledge, this include knowledge from a\ufb00ective lexicons or models based on distributional semantics. To obtain the best performance, one needs to select those text-transformations that work best for a particular dataset, therefore, B4MSA uses a simple random search and hill-climbing (see Subsection 2.4) in space of text-transformations to free the user from this delicate and time-consuming task. Before going into the details of each text-transformation, Table 1 gives a summary of the text-transformations used as well as their parameters associated. 2.1 Cross-language Features We de\ufb01ned cross-language features as a set of features that could be applied in most similar languages, not only related language families such as Germanic languages (English, German, etc.), Romance languages (Spanish, Italian, etc.), among others; but also similar surface features such as punctuation, diacritics, symbol duplication, case sensitivity, etc.",
      "), Romance languages (Spanish, Italian, etc.), among others; but also similar surface features such as punctuation, diacritics, symbol duplication, case sensitivity, etc. Later, the combination of these features will be explored to \ufb01nd the best con\ufb01guration for a given classi\ufb01er. 2.1.1 Spelling Features Generally, Twitter messages are full of slang, misspelling, typographical and grammatical errors among others; in order to tackle these aspects we consider di\ufb00erent parameters to study this e\ufb00ect. The following 1https://github.com/INGEOTEC/b4msa 2",
      "Table 1: Parameter list and a brief description of the functionality cross-language features name values description del-d1 yes, no If it is enabled then the sequences of repeated symbols are replaced by a single occurrence of the symbol. del-diac yes, no Determines if diacritic symbols, e.g., accent symbols, should be removed from the text. emo remove, group, none Controls how emoticons are handled, i.e. removed, grouped by expressed emotion, or nothing. num remove, group, none Controls how numbers are handled, i.e., removed, grouped into a special tag, or nothing. url remove, group, none Controls how URLs are handled, i.e., removed, grouped into a special tag, or nothing. usr remove, group, none Controls how users are handled, i.e., removed, grouped into a special tag, or nothing. lc yes, no Letters are normalized to be lowercase if it is enabled tokenizer P(n-words \u222aq-grams) One item among the power set (discarding the empty- set) of the union of *n-words and *q-grams. *n-words {1, 2} The number of words used to describe a token.",
      "*n-words {1, 2} The number of words used to describe a token. *q-grams {1, 2, 3, 4, 5, 6, 7} The length in characters of a token. language dependent features name values description stem yes, no Determines if words are stemmed. neg yes, no Determines if negation operators in the text are nor- malized and directly connected with the next content word. sw remove, group, none Controls how stopwords are handled, i.e., removed, grouped, or left untouched. :) :D :P \u2192 pos :( :-( :\u2019( \u2192 neg :-| U U -.- \u2192 neu Table 2: An excerpt of the mapping table from Emoticons to its polarity words. points are the parameters to be considered as spelling features. Punctuation (del-punc) considers the use of symbols such as question mark, period, exclamation point, commas, among other spelling marks.",
      "points are the parameters to be considered as spelling features. Punctuation (del-punc) considers the use of symbols such as question mark, period, exclamation point, commas, among other spelling marks. Diacritic symbols (del-diac) are commonly used in languages such as Spanish, Italian, Russian, etc., and its wrong usage is one of the main sources of orthographic errors in informal texts; this parameter considers the use or absence of diacritical marks. Symbol reduction (del-d1), usually, twitter messages use repeated characters to emphasize parts of the word to attract user\u2019s attention. This aspect makes the vocabulary explodes. We applied the strategy of replacing the repeated symbols by one occurrence of the symbol. Case sensitivity (lc) considers letters to be normalized in lowercase or to keep the original source; the aim is to cut the words that are the same in uppercase and lowercase.",
      "This aspect makes the vocabulary explodes. We applied the strategy of replacing the repeated symbols by one occurrence of the symbol. Case sensitivity (lc) considers letters to be normalized in lowercase or to keep the original source; the aim is to cut the words that are the same in uppercase and lowercase. 2.1.2 Emoticon (emo) Feature We classi\ufb01ed around 500 most popular emoticons, included text emoticons, and the whole set of unicode emoticons (around 1, 600) de\ufb01ned by [27] into three classes: positive, negative and neutral, which are grouped under its corresponding polarity word de\ufb01ned by the class name. Table 2 shows an excerpt of the dictionary that maps emoticons to their corresponding polarity class. 3",
      "2.1.3 Word-based n-grams (n-words) Feature N-words (word sequences) are widely used in many NLP tasks, and they have also been used in Sentiment Analysis [26] and [11]. To compute the N-words, the text is tokenized and N-words are calculated from tokens. For example, let T = \"the lights and shadows of your future\" be the text, so its 1-words (unigrams) are each word alone, and its 2-words (bigrams) set are the sequences of two words, the set (W T 2 ), and so on. W T 2 = {the lights, lights and, and shadows, shadows of, of your, your future}, so, given text of size m words, we obtain a set containing at most m \u2212n + 1 elements. Generally, N-words are used up to 2 or 3-words because it is uncommon to \ufb01nd, between texts, good matches of word sequences greater than three or four words [14].",
      "Generally, N-words are used up to 2 or 3-words because it is uncommon to \ufb01nd, between texts, good matches of word sequences greater than three or four words [14]. 2.1.4 Character-based q-grams (q-grams) In addition to the traditional N-words representation, we represent the resulting text as q-grams. A q-grams is an agnostic language transformation that consists in representing a document by all its substring of length q. For example, let T = \u201cabra cadabra\u201d be the text, its 3-grams set are QT 3 = {abr, bra, ra , a c, ca, aca, cad, ada, dab}, so, given text of size m characters, we obtain a set with at most m \u2212q + 1 elements. Notice that this transformation handles white-spaces as part of the text. Since there will be q-grams connecting words, in some sense, applying q-grams to the entire text can capture part of the syntactic and contextual information in the sentence. The rationale of q-grams is also to tackle misspelled sentences from the approximate pattern matching perspective [21].",
      "Since there will be q-grams connecting words, in some sense, applying q-grams to the entire text can capture part of the syntactic and contextual information in the sentence. The rationale of q-grams is also to tackle misspelled sentences from the approximate pattern matching perspective [21]. 2.2 Language Dependent Features The following features are language dependent because they use speci\ufb01c information from the language concerned. Usually, the use of stopwords, stemming and negations are traditionally used in Sentiment Analysis. The users of this approach could add other features such as part of speech, a\ufb00ective lexicons, etc. to improve the performance [16]. 2.2.1 Stopwords (sw) Feature In many languages, there is a set of extremely common words such as determiners or conjunctions (the or and) which help to build sentences but do not carry any meaning for themselves. These words are known as Stopwords, and they are removed from text before any attempt to classify them. Generally, a stopword list is built using the most frequent terms from a huge document collection.",
      "These words are known as Stopwords, and they are removed from text before any attempt to classify them. Generally, a stopword list is built using the most frequent terms from a huge document collection. We used the Spanish, English and Italian stopword lists included in the NLTK Python package [9] in order to identify them. 2.2.2 Stemming (stem) Feature Stemming is a well-known heuristic process in Information Retrieval \ufb01eld that chops o\ufb00the end of words and often includes the removal of derivational a\ufb03xes. This technique uses the morphology of the language coded in a set of rules that are applied to \ufb01nd out word stems and reduce the vocabulary collapsing derivationally related words. In our study, we use the Snowball Stemmer for Spanish and Italian, and the Porter Stemmer for English that are implemented in NLTK package [9]. 2.2.3 Negation (neg) Feature Negation markers might change the polarity of the message. Thus, we attached the negation clue to the nearest word, similar to the approaches used in [26].",
      "2.2.3 Negation (neg) Feature Negation markers might change the polarity of the message. Thus, we attached the negation clue to the nearest word, similar to the approaches used in [26]. A set of rules was designed for common negation structures that involve negation markers for Spanish, English and Italian. For instance, negation markers used for Spanish are no (not), nunca, jam\u00b4as (never), and sin (without). The rules (regular expressions) are 4",
      "processed in order, and their purpose is to negate the nearest word to the negation marker using only the information on the text, e.g., avoiding mainly pronouns and articles. For example, in the sentence El coche no es bonito (The car is not nice), the negation marker no and not (for English) is attached to its adjective no bonito (not nice). 2.3 Text Representation After text-transformations, it is needed to represent the text in suitable form in order to use a traditional classi\ufb01er such as SVM. It was decided to select the well known vector representation of a text given its sim- plicity and powerful representation. Particularly, it is used the Term Frequency-Inverse Document Frequency which is a well-known weighting scheme in NLP. TF-IDF computes a weight that represents the importance of words or terms inside a document in a collection of documents, i.e., how frequently they appear across multiple documents. Therefore, common words such as the and in, which appear in many documents, will have a low score, and words that appear frequently in a single document will have high score. This weighting scheme selects the terms that represent a document.",
      "Therefore, common words such as the and in, which appear in many documents, will have a low score, and words that appear frequently in a single document will have high score. This weighting scheme selects the terms that represent a document. 2.4 Parameter Optimization The model selection, sometimes called hyper-parameter optimization, is essential to ensure the performance of a sentiment classi\ufb01er. In particular, our approach is highly parametric; in fact, we use such property to adapt to several languages. Table 1 summarizes the parameters and their valid values. The search space contains more than 331 thousand con\ufb01gurations when limited to multilingual and language independent parameters; while the search space reaches close to 4 million con\ufb01gurations when we add our three language- dependent parameters. Depending on the size of the training set, each con\ufb01guration needs several minutes on a commodity server to be evaluated; thus, an exhaustive exploration of the parameter space can be quite expensive making the approach useless in practice. To tackle the e\ufb03ciency problems, we perform the model selection using two hyper-parameter optimization algorithms. The \ufb01rst corresponds to Random Search, described in depth in [8].",
      "To tackle the e\ufb03ciency problems, we perform the model selection using two hyper-parameter optimization algorithms. The \ufb01rst corresponds to Random Search, described in depth in [8]. Random search consists on randomly sampling the parameter space and select the best con\ufb01guration among the sample. The second algorithm consists on a Hill Climbing [10, 7] implemented with a memory to avoid testing a con\ufb01guration twice. The main idea behind hill climbing H+M is to take a pivoting con\ufb01guration, explore the con\ufb01guration\u2019s neighborhood, and greedily moves to the best neighbor. The process is repeated until no improvement is possible. The con\ufb01guration neighborhood is de\ufb01ned as the set of con\ufb01gurations such that these di\ufb00er in just one parameter\u2019s value. This rule is strengthened for tokenizer (see Table 1) to di\ufb00er in a single internal value not in the whole parameter value.",
      "This rule is strengthened for tokenizer (see Table 1) to di\ufb00er in a single internal value not in the whole parameter value. More precisely, let t be a valid value for tokenizer and N(t) the set of valid values for neighborhoods of t, then |t \u222as| \u2208{|t|, |t| + 1} and |t \u2229s| \u2208{|t|, |t| \u22121} for any s \u2208N(t). To guarantee a better or equal performance than random search, the H+M process starts with the best con\ufb01guration found in the random search. By using H+M, sample size can be set to 32 or 64, as rule of thumb, and even reach improvements in most cases (see \u00a74). Nonetheless, this simpli\ufb01cation and performance boosting comes along with possible higher optimization times. Finally, the performance of each con\ufb01guration is obtained using a cross-validation technique on the training data, and the metrics are usually used in classi\ufb01cation such as: accuracy, score F1, and recall, among others.",
      "Finally, the performance of each con\ufb01guration is obtained using a cross-validation technique on the training data, and the metrics are usually used in classi\ufb01cation such as: accuracy, score F1, and recall, among others. 3 Datasets and contests Nowadays, there are several international competitions related to text mining, which include diverse tasks such as: polarity classi\ufb01cation (at di\ufb00erent levels), subjectivity classi\ufb01cation, entity detection, and iron de- tection, among others. These competitions are relevant to measure the potential of di\ufb00erent proposed tech- niques. In this case, we focused on polarity classi\ufb01cation task, hence, we developed a baseline method with an acceptable performance achieved in three di\ufb00erent contests, namely, TASS\u201915 (Spanish) [28], SemEval\u201915- 16 (English) [24, 20], and SENTIPOLC\u201914 (Italian) [6]. In addition, our approach was tested with other 5",
      "Table 3: Datasets details from each competition tested in this work Dataset Positive Neutral Negative None Total SemEval\u201915 Training 2,800 3,661 1,060 - 7,521 (English) Development 446 580 262 - 1,288 Gold 841 824 298 - 1,963 SemEval\u201916 Training 3,094 2,043 863 - 6,000 (English) Development 844 765 391 - 2,000 Gold 7,059 10,342 3,231 - 20,632 TASS\u201915 Training 2,884 670 2,182 1,482 7,218 (Spanish) Development - - - - - Gold 1K 363 22 268 347 1,000 Gold 60K 22,233 1,305 15,844 21,416 60,798 SENTIPOL\u201914 Training 969 320 1,671 1,541 4,501 (Spanish) Development - - - - - Gold 453 113 754 607 1,927 Arabic [18, 25] Unique 448 202 1,",
      "416 60,798 SENTIPOL\u201914 Training 969 320 1,671 1,541 4,501 (Spanish) Development - - - - - Gold 453 113 754 607 1,927 Arabic [18, 25] Unique 448 202 1,350 - 2,000 German [19] Unique 23,860 50,368 17,274 - 91,502 Portuguese [19] Unique 24,595 29,357 32,110 - 86,062 Russian [19] Unique 19,238 28,665 21,197 - 69,100 Swedish [19] Unique 13,265 15,410 20,580 - 49,255 languages (Arabic, German, Portuguese, Russian, and Swedish) to show that is feasible to use our framework as basis for building more complex sentiment analysis systems. From these languages, datasets and results can be seen in [19], [25] and [18]. Table 3 presents the details of each of the competitions considered as well as the other languages tested.",
      "From these languages, datasets and results can be seen in [19], [25] and [18]. Table 3 presents the details of each of the competitions considered as well as the other languages tested. It can be observed, from the table, the number of examples as well as the number of instances for each polarity level, namely, positive, neutral, negative and none. The training and development (only in SemEval) sets are used to train the sentiment classi\ufb01er, and the gold set is used to test the classi\ufb01er. In the case there dataset was not split in training and gold (Arabic, German, Portuguese, Russian, and Swedish) then a cross- validation (10 folds) technique is used to test the classi\ufb01er. The performance of the classi\ufb01er is presented using di\ufb00erent metrics depending the competition. SemEval uses the average of score F1 of positive and negative labels, TASS uses the accuracy and SENTIPOLC uses a custom metric (see [28, 24, 20, 6]). 4 Experimental Results We tested our framework on two kinds of datasets.",
      "SemEval uses the average of score F1 of positive and negative labels, TASS uses the accuracy and SENTIPOLC uses a custom metric (see [28, 24, 20, 6]). 4 Experimental Results We tested our framework on two kinds of datasets. On one hand, we compare our performance on three languages having well known sentiment analysis contests; here, we compare our work against competitors of those challenges. On the other hand, we selected \ufb01ve languages without popular opinion mining contests; for these languages, we compare our approach against research works reporting the used corpus. 4.1 Performance on sentiment analysis contests Figure 1 shows the performance on four contests, corresponding to three di\ufb00erent languages. The perfor- mance corresponds to the multilingual set of features, i.e., we do not used language-dependent techniques. Figures 1(a)-1(d) illustrates the results on each challenge, all competitors are ordered in score\u2019s descending order (higher is better). The achieved performance of our approach is marked with a horizontal line on each \ufb01gure.",
      "Figures 1(a)-1(d) illustrates the results on each challenge, all competitors are ordered in score\u2019s descending order (higher is better). The achieved performance of our approach is marked with a horizontal line on each \ufb01gure. Figure 1(e) brie\ufb02y describes each challenge and summarizes our performance on each contest; also, we added three standard measures to simplify the insight\u2019s creation of the reader. 6",
      "The winner method in SENTIPOLC\u201914 (Italian) is reported in [5]. This method uses three groups of features: keyword and micro-blogging characteristics, Sentiment Lexicons, SentiWordNet and MultiWordNet, and Distributional Semantic Model (DSM) with a SVM classi\ufb01er. In contrast with our method, in [5] three external sentiment lexicons dictionaries were employed; that is, external information. In TASS\u201915 (Spanish) competition, the winner reported method was [17], which proposed an adaptation based on a tokenizer of tweets Tweetmotif [15], Freeling [22] as lemmatizer, entity detector, morphosyntactic labeler and a translation of the A\ufb01nn dictionary. In contrast with our method, [17] employs several complex and expensive tools. In this task we reached the fourteenth position with an accuracy of 0.637. Figure 1(b) shows the B4MSA performance to be over two thirds of the competitors. The remaining two contests correspond to the SemEval\u201915-16.",
      "In this task we reached the fourteenth position with an accuracy of 0.637. Figure 1(b) shows the B4MSA performance to be over two thirds of the competitors. The remaining two contests correspond to the SemEval\u201915-16. The B4MSA performance in SemEval is depicted in Figures 1(c) and 1(d); here, B4MSA does not perform as well as in other challenges, mainly because, contrary to other challenges, SemEval rules promotes the enrichment of the o\ufb03cial training set. To be consistent with the rest of the experiments, B4MSA uses only the o\ufb03cial training set. The results can be signi\ufb01cantly improved using larger training datasets; for example, joining SemEval\u201913 and SemEval\u201916 training sets, we can reach 0.54 for SemEval\u201916, which improves the B4MSA\u2019s performance (see Table 1).",
      "The results can be signi\ufb01cantly improved using larger training datasets; for example, joining SemEval\u201913 and SemEval\u201916 training sets, we can reach 0.54 for SemEval\u201916, which improves the B4MSA\u2019s performance (see Table 1). In SemEval\u201915, the winner method is [12], which combines three approaches among the participants of SemEval\u201913, teams: NRC-Canada, GU-MLT-LT and KLUE, and from SemEval\u201914 the participant TeamX all of them employing external information. In SemEval\u201916, the winner method was [13] is composed with an ensemble of two subsystems based on convolutional neural networks, the \ufb01rst subsystem is created using 290 million tweets, and the second one is feeded with 150 million tweets. All these tweets were selected from a very large unlabeled dataset through distant supervision techniques.",
      "All these tweets were selected from a very large unlabeled dataset through distant supervision techniques. rank 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 score (a) SENTIPOLC\u201914 rank 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 score (b) TASS\u201915 rank 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 score (c) SemEval\u201915 rank 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 score (d) SemEval\u201916 name language classes challenge\u2019s score rank score acc.",
      "macro F1 (F pos 1 + F neg 1 ) /2 SENTIPOLC\u201914 italian {pos, neg, none, mix} custom (see [6]) 2 / 14 0.677 0.604 0.599 0.599 TASS\u201915 spanish {pos, neg, neu, none} accuracy 14 / 40 0.637 0.637 0.498 0.697 SemEval\u201915 english {pos, neg, neu} (F pos 1 + F neg 1 ) /2 34 / 42 0.534 0.629 0.584 0.534 SemEval\u201916 english {pos, neg, neu} (F pos 1 + F neg 1 ) /2 28 / 36 0.454 0.534 0.477 0.454 (e) B4MSA\u2019s performance summary; the last columns correspond to three standard scores. Figure 1: The performance listing in four di\ufb00erence challenges. The horizontal lines appearing in a) to d) correspond to B4MSA\u2019s performance.",
      "Figure 1: The performance listing in four di\ufb00erence challenges. The horizontal lines appearing in a) to d) correspond to B4MSA\u2019s performance. All scores were computed using the o\ufb03cial gold-standard and the proper score for each challenge. Table 4 shows the multilingual set of techniques and the set with language-dependent techniques; for each, we optimized the set of parameters through Random Search and H + M (see Subsection 2.4). The reached performance is reported using both cross-validation and the o\ufb03cial gold-standard. Please notice how H + M consistently reaches better performances, even on small sampling sizes. The sampling size is indicated with subscripts in Table 4. Note that, in SemEval challenges, the cross-validation performances are higher than those reached by evaluating the gold-standard, mainly because the gold-standard does not 7",
      "follow the distribution of training set. This can be understood because the rules of SemEval promote the use of external knowledge. Table 4: B4MSA\u2019s performance on cross-validation and gold standard. The subscript at right of each score means for the random-search\u2019s parameter (sampling size) needed to \ufb01nd that value. Dataset Multilingual Parameters Language-Dependent Parameters Random Search H+M Search Random Search H+M Search cross-val. gold-std. cross-val. gold-std. cross-val. gold-std. cross-val. gold-std.",
      "Dataset Multilingual Parameters Language-Dependent Parameters Random Search H+M Search Random Search H+M Search cross-val. gold-std. cross-val. gold-std. cross-val. gold-std. cross-val. gold-std. SENTIPOLC\u201914 - 0.678256 - 0.67716 - 0.6758 - 0.674256 TASS\u201915 0.643128 0.636128 0.6488 0.6378 0.644256 0.635256 0.64932 0.63732 SemEval\u201915 0.585256 0.530256 0.5908 0.5348 0.590256 0.520256 0.596128 0.528128 SemEval\u201916 0.57564 0.45664 0.57864 0.45464 0.580256 0.462256 0.583256 0.462256 Table 5 compares our performance on \ufb01ve di\ufb00erent languages; we do not apply language-dependent techniques.",
      "For each comparison, we took a labeled corpus from [25] (Arabic) and [19] (the remaining languages). According to author\u2019s reports, all tweets were manually labeled by native speakers as pos, neg, or neu. The Arabic dataset contains 2, 000 items; the other datasets contain from 58 thousand tweets to more than 157 thousand tweets. We were able to fetch a fraction of the original datasets; so, we drop the necessary items to hold the original class-population ratio. The ratio of tweets in our training dataset, respect to the original dataset, is indicated beside the name. As before, we evaluate our algorithms through a 10-fold cross validation. In [25, 18], the authors study the e\ufb00ect of translation in sentiment classi\ufb01ers; they found better to use native Arabic speakers as annotators than \ufb01ne-tuned translators plus \ufb01ne-tuned English sentiment classi\ufb01ers. In [19], the idea is to measure the e\ufb00ect of the agreement among annotators on the production of a sentiment- analysis corpus.",
      "In [19], the idea is to measure the e\ufb00ect of the agreement among annotators on the production of a sentiment- analysis corpus. Both, on the technical side, both papers use \ufb01ne tuned classi\ufb01ers plus a variety of pre- processing techniques to prove their claims. Table 5 supports the idea of choosing B4MSA as a bootstrapping sentiment classi\ufb01er because, in the overall, B4MSA reaches superior performances regardless of the language. Our approach achieves those performance\u2019s levels since it optimizes a set of parameters carefully selected to work on a variety of languages and being robust to informal writing. The latter problem is not properly tackled in many cases. Table 5: Performance on multilingual sentiment analysis (not challenges). B4MSA was restricted to use only the multilingual set of parameters. language F1 (F pos 1 + F neg 1 ) /2 acc Arabic Salameh et al. [25] - - 0.787 Saif et al.",
      "B4MSA was restricted to use only the multilingual set of parameters. language F1 (F pos 1 + F neg 1 ) /2 acc Arabic Salameh et al. [25] - - 0.787 Saif et al. [18] - - 0.794 B4MSA (100%) 0.642 0.781 0.799 German Mozeti\u02c7c et al. [19] - 0.536 0.610 B4MSA (89%) 0.621 0.559 0.668 Portuguese Mozeti\u02c7c et al. [19] - 0.553 0.507 B4MSA (58%) 0.550 0.591 0.555 Russian Mozeti\u02c7c et al. [19] - 0.615 0.603 B4MSA (69%) 0.754 0.768 0.750 Swedish Mozeti\u02c7c et al. [19] - 0.657 0.616 B4MSA (93%) 0.680 0.717 0.691 8",
      "5 Conclusions We presented a simple to implement multilingual framework for polarity classi\ufb01cation whose main contribu- tions are in two aspects. On one hand, our approach can serve as a baseline to compare other classi\ufb01cation systems. It considers techniques for text representation such as spelling features, emoticons, word-based n-grams, character-based q-grams and language dependent features. On the other hand, our approach is a framework for practitioners or researchers looking for a bootstrapping sentiment classi\ufb01er method in order to build more elaborated systems. Besides the text-transformations, the proposed framework uses a SVM classi\ufb01er (with linear kernel), and, hyper-parameter optimization using random search and H+M over the space of text-transformations. The experimental results show good overall performance in all international contests considered, and the best results in the other \ufb01ve languages tested. It is important to note that all the methods that outperformed B4MSA in the sentiment analysis contests use extra knowledge (lexicons included) meanwhile B4MSA uses only the information provided by each contests.",
      "It is important to note that all the methods that outperformed B4MSA in the sentiment analysis contests use extra knowledge (lexicons included) meanwhile B4MSA uses only the information provided by each contests. In future work, we will extend our methodology to include extra-knowledge in order to improve the performance. Acknowledgements We would like to thank Valerio Basile, Julio Villena-Roman, and Preslav Nakov for kindly give us access to the gold-standards of SENTIPOLC\u201914, TASS\u201915 and SemEval 2015 & 2016, respectively. The authors also thank Elio Villase\u02dcnor for the helpful discussions in early stages of this research. References [1] Matheus Araujo, Julio Reis, Adriano Pereira, and Fabricio Benevenuto. An evaluation of machine translation for multilingual sentence-level sentiment analysis. In Proceedings of the 31st Annual ACM Symposium on Applied Computing, SAC \u201916, pages 1140\u20131145, New York, NY, USA, 2016. ACM.",
      "An evaluation of machine translation for multilingual sentence-level sentiment analysis. In Proceedings of the 31st Annual ACM Symposium on Applied Computing, SAC \u201916, pages 1140\u20131145, New York, NY, USA, 2016. ACM. [2] Daniella Bal, Malissa Bal, Arthur van Bunningen, Alexander Hogenboom, Frederik Hogenboom, and Flavius Frasincar. Sentiment Analysis with a Multilingual Pipeline, pages 129\u2013142. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011. [3] Alexandra Balahur and Marco Turchi. Multilingual sentiment analysis using machine translation? In Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis, WASSA \u201912, pages 52\u201360, Stroudsburg, PA, USA, 2012. Association for Computational Linguistics. [4] Alexandra Balahur and Marco Turchi. Comparative experiments using supervised learning and machine translation for multilingual sentiment analysis. Computer Speech & Language, 28(1):56 \u2013 75, 2014.",
      "Association for Computational Linguistics. [4] Alexandra Balahur and Marco Turchi. Comparative experiments using supervised learning and machine translation for multilingual sentiment analysis. Computer Speech & Language, 28(1):56 \u2013 75, 2014. [5] Pierpaolo Basile and Nicole Novielli. Uniba at evalita 2014-sentipolc task: Predicting tweet sentiment polarity combining micro-blogging, lexicon and semantic features. In Proceedings of the 4th evaluation campaign of Natural Language Processing and Speech tools for Italian (EVALITA\u201914), Pisa, Italy, 2014. [6] Valerio Basile, Andrea Bolioli, Malvina Nissim, Viviana Patti, and Paolo Rosso. Overview of the Evalita 2014 SENTIment POLarity Classi\ufb01cation Task. In Proceedings of the 4th evaluation campaign of Natural Language Processing and Speech tools for Italian (EVALITA\u201914), Pisa, Italy, 2014. [7] Roberto Battiti, Mauro Brunato, and Franco Mascia.",
      "In Proceedings of the 4th evaluation campaign of Natural Language Processing and Speech tools for Italian (EVALITA\u201914), Pisa, Italy, 2014. [7] Roberto Battiti, Mauro Brunato, and Franco Mascia. Reactive search and intelligent optimization, volume 45. Springer Science & Business Media, 2008. [8] James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13(Feb):281\u2013305, 2012. 9",
      "[9] Steven Bird, Ewan Klein, and Edward Loper. Natural Language Processing with Python. O\u2019Reilly Media, 2009. [10] Edmund K Burke, Graham Kendall, et al. Search methodologies. Springer, 2005. [11] Zhijian Cui, Xiaodong Shi, and Yidong Chen. Sentiment analysis via integrating distributed represen- tations of variable-length word sequence. Neurocomputing, 2015. [12] Matthias Hagen, Martin Potthast, Michel Bchner, and Benno Stein. Webis: an ensemble for twitter sentiment detection. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015, pages 582\u2013589. Association for Computational Linguistics, June 2015. [13] Jan Deriu; Maurice Gonzenbach; Fatih Uzdilli; Aurelien Lucchi; Valeria De Luca; Martin Jaggi. Swiss- cheese at semeval-2016 task 4: Sentiment classi\ufb01cation using an ensemble of convolutional neural net- works with distant supervision.",
      "Swiss- cheese at semeval-2016 task 4: Sentiment classi\ufb01cation using an ensemble of convolutional neural net- works with distant supervision. In Proceedings of the 10th International Workshop on Semantic Evalu- ation, SemEval \u201916, San Diego, California, June 2016. Association for Computational Linguistics. [14] Daniel Jurafsky and James H. Martin. Speech and Language Processing (2Nd Edition). Prentice-Hall, Inc., Upper Saddle River, NJ, USA, 2009. [15] Michel Krieger and David Ahn. Tweetmotif: exploratory search and topic summarization for twitter. In In Proc. of AAAI Conference on Weblogs and Social, 2010. [16] Bing Liu. Sentiment Analysis: Mining Opinions, Sentiments, and Emotions. Cambridge University Press, 2015. ISBN: 1-107-01789-0. 381 pages. [17] Ferran Pla Llu\u00b4\u0131s-F Hurtado and Davide Buscaldi.",
      "Cambridge University Press, 2015. ISBN: 1-107-01789-0. 381 pages. [17] Ferran Pla Llu\u00b4\u0131s-F Hurtado and Davide Buscaldi. Elirf-upv en tass 2015: Anlisis de sentimientos en twitter. In TASS 2015: Workshop on Sentiment Analysis at SEPLN co-located with 31st SEPLN Con- ference (SEPLN 2015), pages 75\u201379. Villena Rom\u00b4an, Julio; Garc\u00b4\u0131a Morera, Janine; Garc\u00b4\u0131a Cumbreras, Miguel \u00b4Angel; Mart\u00b4\u0131nez C\u00b4amara, Eugenio; Mart\u00b4\u0131n Valdivia, M. Teresa; Ure\u02dcna L\u00b4opez, L. Alfonso, 2015. [18] Saif M. Mohammad, Mohammad Salameh, and Svetlana Kiritchenko. How translation alters sentiment. Journal of Arti\ufb01cial Intelligence Research, 55:95\u2013130, 2016.",
      "[18] Saif M. Mohammad, Mohammad Salameh, and Svetlana Kiritchenko. How translation alters sentiment. Journal of Arti\ufb01cial Intelligence Research, 55:95\u2013130, 2016. [19] Igor Mozeti\u02c7c, Miha Gr\u02c7car, and Jasmina Smailovi\u00b4c. Multilingual twitter sentiment classi\ufb01cation: The role of human annotators. PloS one, 11(5):e0155036, 2016. [20] Preslav Nakov, Alan Ritter, Sara Rosenthal, Veselin Stoyanov, and Fabrizio Sebastiani. SemEval-2016 task 4: Sentiment analysis in Twitter. In Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval \u201916, San Diego, California, June 2016. Association for Computational Linguistics. [21] G. Navarro and M. Ra\ufb03not. Flexible Pattern Matching in Strings \u2013 Practical on-line search algorithms for texts and biological sequences. Cambridge University Press, 2002.",
      "Association for Computational Linguistics. [21] G. Navarro and M. Ra\ufb03not. Flexible Pattern Matching in Strings \u2013 Practical on-line search algorithms for texts and biological sequences. Cambridge University Press, 2002. ISBN 0-521-81307-7. 280 pages. [22] Llus Padr and Evgeny Stanilovsky. Freeling 3.0: Towards wider multilinguality. In Proceedings of the Language Resources and Evaluation Conference (LREC 2012), Istanbul, Turkey, May 2012. ELRA. [23] Radim \u02c7Reh\u02dau\u02c7rek and Petr Sojka. Software Framework for Topic Modelling with Large Corpora. In Pro- ceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u201350, Valletta, Malta, May 2010. ELRA. http://is.muni.cz/publication/884893/en.",
      "In Pro- ceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u201350, Valletta, Malta, May 2010. ELRA. http://is.muni.cz/publication/884893/en. [24] Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko, Saif Mohammad, Alan Ritter, and Veselin Stoy- anov. Semeval-2015 task 10: Sentiment analysis in twitter. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 451\u2013463, Denver, Colorado, June 2015. As- sociation for Computational Linguistics. 10",
      "[25] Mohammad Salameh, Saif Mohammad, and Svetlana Kiritchenko. Sentiment after translation: A case- study on arabic social media posts. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 767\u2013 777, Denver, Colorado, May\u2013June 2015. Association for Computational Linguistics. [26] Grigori Sidorov, Sabino Miranda-Jim\u00b4enez, Francisco Viveros-Jim\u00b4enez, Alexander Gelbukh, No\u00b4e Castro- S\u00b4anchez, Francisco Vel\u00b4asquez, Ismael D\u00b4\u0131az-Rangel, Sergio Su\u00b4arez-Guerra, Alejandro Trevi\u02dcno, and Juan Gordon. Empirical study of machine learning based approach for opinion mining in tweets. In Proceedings of the 11th Mexican International Conference on Advances in Arti\ufb01cial Intelligence - Volume Part I, MICAI\u201912, pages 1\u201314, Berlin, Heidelberg, 2013. Springer-Verlag. [27] Unicode. Unicode emoji chart.",
      "In Proceedings of the 11th Mexican International Conference on Advances in Arti\ufb01cial Intelligence - Volume Part I, MICAI\u201912, pages 1\u201314, Berlin, Heidelberg, 2013. Springer-Verlag. [27] Unicode. Unicode emoji chart. http://unicode.org/emoji/charts/full-emoji-list.html, 2016. Accessed 20-May-2016. [28] Julio Villena Rom\u00b4an, Janine Garc\u00b4\u0131a Morera, Miguel \u00b4Angel Garc\u00b4\u0131a Cumbreras, Eugenio Mart\u00b4\u0131nez C\u00b4amara, M. Teresa Mart\u00b4\u0131n Valdivia, and L. Alfonso Ure\u02dcna L\u00b4opez. Overview of tass 2015. In TASS 2015: Workshop on Sentiment Analysis at SEPLN co-located with 31st SEPLN Conference (SEPLN 2015), pages 13\u201321.",
      "Overview of tass 2015. In TASS 2015: Workshop on Sentiment Analysis at SEPLN co-located with 31st SEPLN Conference (SEPLN 2015), pages 13\u201321. Villena Rom\u00b4an, Julio; Garc\u00b4\u0131a Morera, Janine; Garc\u00b4\u0131a Cumbreras, Miguel \u00b4Angel; Mart\u00b4\u0131nez C\u00b4amara, Eugenio; Mart\u00b4\u0131n Valdivia, M. Teresa; Ure\u02dcna L\u00b4opez, L. Alfonso, 2015. 11"
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1612.05270.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":9004,
  "avg_doclen":169.8867924528,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1612.05270.pdf"
    }
  }
}