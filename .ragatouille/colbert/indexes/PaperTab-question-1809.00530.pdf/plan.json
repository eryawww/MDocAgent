{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Adaptive Semi-supervised Learning for Cross-domain Sentiment Classi\ufb01cation Ruidan He\u2020\u2021, Wee Sun Lee\u2020, Hwee Tou Ng\u2020, and Daniel Dahlmeier\u2021 \u2020Department of Computer Science, National University of Singapore \u2021SAP Innovation Center Singapore \u2020{ruidanhe,leews,nght}@comp.nus.edu.sg \u2021d.dahlmeier@sap.com Abstract We consider the cross-domain sentiment clas- si\ufb01cation problem, where a sentiment classi- \ufb01er is to be learned from a source domain and to be generalized to a target domain. Our ap- proach explicitly minimizes the distance be- tween the source and the target instances in an embedded feature space. With the differ- ence between source and target minimized, we then exploit additional information from the target domain by consolidating the idea of semi-supervised learning, for which, we jointly employ two regularizations \u2013 entropy minimization and self-ensemble bootstrapping \u2013 to incorporate the unlabeled target data for classi\ufb01er re\ufb01nement.",
            "Our experimental results demonstrate that the proposed approach can better leverage unlabeled data from the target domain and achieve substantial improvements over baseline methods in various experimental settings. 1 Introduction In practice, it is often dif\ufb01cult and costly to anno- tate suf\ufb01cient training data for diverse application domains on-the-\ufb02y. We may have suf\ufb01cient la- beled data in an existing domain (called the source domain), but very few or no labeled data in a new domain (called the target domain). This issue has motivated research on cross-domain sentiment classi\ufb01cation, where knowledge in the source do- main is transferred to the target domain in order to alleviate the required labeling effort. One key challenge of domain adaptation is that data in the source and target domains are drawn from different distributions. Thus, adaptation per- formance will decline with an increase in distribu- tion difference. Speci\ufb01cally, in sentiment analy- sis, reviews of different products have different vo- cabulary.",
            "Thus, adaptation per- formance will decline with an increase in distribu- tion difference. Speci\ufb01cally, in sentiment analy- sis, reviews of different products have different vo- cabulary. For instance, restaurants reviews would contain opinion words such as \u201ctender\u201d, \u201ctasty\u201d, or \u201cundercooked\u201d and movie reviews would contain \u201cthrilling\u201d, \u201chorri\ufb01c\u201d, or \u201chilarious\u201d. The intersec- tion between these two sets of opinion words could be small which makes domain adaptation dif\ufb01cult. Several techniques have been proposed for ad- dressing the problem of domain shifting. The aim is to bridge the source and target domains by learning domain-invariant feature representa- tions so that a classi\ufb01er trained on a source do- main can be adapted to another target domain.",
            "Several techniques have been proposed for ad- dressing the problem of domain shifting. The aim is to bridge the source and target domains by learning domain-invariant feature representa- tions so that a classi\ufb01er trained on a source do- main can be adapted to another target domain. In cross-domain sentiment classi\ufb01cation, many works (Blitzer et al., 2007; Pan et al., 2010; Zhou et al., 2015; Wu and Huang, 2016; Yu and Jiang, 2016) utilize a key intuition that domain-speci\ufb01c features could be aligned with the help of domain- invariant features (pivot features). For instance, \u201chilarious\u201d and \u201ctasty\u201d could be aligned as both of them are relevant to \u201cgood\u201d. Despite their promising results, these works share two major limitations. First, they highly de- pend on the heuristic selection of pivot features, which may be sensitive to different applications. Thus the learned new representations may not ef- fectively reduce the domain difference.",
            "Despite their promising results, these works share two major limitations. First, they highly de- pend on the heuristic selection of pivot features, which may be sensitive to different applications. Thus the learned new representations may not ef- fectively reduce the domain difference. Further- more, these works only utilize the unlabeled tar- get data for representation learning while the sen- timent classi\ufb01er was solely trained on the source domain. There have not been many studies on ex- ploiting unlabeled target data for re\ufb01ning the clas- si\ufb01er, even though it may contain bene\ufb01cial infor- mation. How to effectively leverage unlabeled tar- get data still remains an important challenge for domain adaptation. In this work, we argue that the information from unlabeled target data is bene\ufb01cial for do- main adaptation and we propose a novel Domain Adaptive Semi-supervised learning framework (DAS) to better exploit it. Our main intuition is to treat the problem as a semi-supervised learn- ing task by considering target instances as unla- arXiv:1809.00530v1  [cs.CL]  3 Sep 2018",
            "beled data, assuming the domain distance can be effectively reduced through domain-invariant rep- resentation learning. Speci\ufb01cally, the proposed approach jointly performs feature adaptation and semi-supervised learning in a multi-task learning setting. For feature adaptation, it explicitly mini- mizes the distance between the encoded represen- tations of the two domains. On this basis, two semi-supervised regularizations \u2013 entropy mini- mization and self-ensemble bootstrapping \u2013 are jointly employed to exploit unlabeled target data for classi\ufb01er re\ufb01nement. We evaluate our method rigorously under multi- ple experimental settings by taking label distribu- tion and corpus size into consideration. The re- sults show that our model is able to obtain sig- ni\ufb01cant improvements over strong baselines. We also demonstrate through a series of analysis that the proposed method bene\ufb01ts greatly from incor- porating unlabeled target data via semi-supervised learning, which is consistent with our motivation. Our datasets and source code can be obtained from https:\/\/github.com\/ruidan\/DAS.",
            "We also demonstrate through a series of analysis that the proposed method bene\ufb01ts greatly from incor- porating unlabeled target data via semi-supervised learning, which is consistent with our motivation. Our datasets and source code can be obtained from https:\/\/github.com\/ruidan\/DAS. 2 Related Work Domain Adaptation: The majority of feature adaptation methods for sentiment analysis rely on a key intuition that even though certain opinion words are completely distinct for each domain, they can be aligned if they have high correlation with some domain-invariant opinion words (pivot words) such as \u201cexcellent\u201d or \u201cterrible\u201d. Blitzer et al. (2007) proposed a method based on struc- tural correspondence learning (SCL), which uses pivot feature prediction to induce a projected fea- ture space that works well for both the source and the target domains. The pivot words are selected in a way to cover common domain-invariant opinion words. Subsequent research aims to better align the domain-speci\ufb01c words (Pan et al., 2010; He et al., 2011; Wu and Huang, 2016) such that the domain discrepancy could be reduced.",
            "Subsequent research aims to better align the domain-speci\ufb01c words (Pan et al., 2010; He et al., 2011; Wu and Huang, 2016) such that the domain discrepancy could be reduced. More re- cently, Yu and Jiang (2016) borrow the idea of pivot feature prediction from SCL and extend it to a neural network-based solution with auxiliary tasks. In their experiment, substantial improve- ment over SCL has been observed due to the use of real-valued word embeddings. Unsupervised representation learning with deep neural networks (DNN) such as denoising autoencoders has also been explored for feature adaptation (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014). It has been shown that DNNs could learn transferable representations that disentangle the underlying factors of variation behind data sam- ples. Although the aforementioned methods aim to reduce the domain discrepancy, they do not explic- itly minimize the distance between distributions, and some of them highly rely on the selection of pivot features. In our method, we formally con- struct an objective for this purpose.",
            "Although the aforementioned methods aim to reduce the domain discrepancy, they do not explic- itly minimize the distance between distributions, and some of them highly rely on the selection of pivot features. In our method, we formally con- struct an objective for this purpose. Similar ideas have been explored in many computer vision prob- lems, where the representations of the underlying domains are encouraged to be similar through ex- plicit objectives (Tzeng et al., 2014; Ganin and Lempitsky, 2015; Long et al., 2015; Zhuang et al., 2015; Long et al., 2017) such as maximum mean discrepancy (MMD) (Gretton et al., 2012). In NLP tasks, Li et al. (2017) and Chen et al. (2017) both proposed using adversarial training framework for reducing domain difference. In their model, a sub- network is added as a domain discriminator while deep features are learned to confuse the discrim- inator. The feature adaptation component in our model shares similar intuition with MMD and ad- versary training.",
            "In their model, a sub- network is added as a domain discriminator while deep features are learned to confuse the discrim- inator. The feature adaptation component in our model shares similar intuition with MMD and ad- versary training. We will show a detailed compar- ison with them in our experiments. Semi-supervised Learning: We attempt to treat domain adaptation as a semi-supervised learning task by considering the target instances as unla- beled data. Some efforts have been initiated on transfer learning from unlabeled data (Dai et al., 2007; Jiang and Zhai, 2007; Wu et al., 2009). In our model, we reduce the domain discrep- ancy by feature adaptation, and thereafter adopt semi-supervised learning techniques to learn from unlabeled data. Primarily motivated by (Grand- valet and Bengio, 2004) and (Laine and Aila, 2017), we employed entropy minimization and self-ensemble bootstrapping as regularizations to incorporate unlabeled data.",
            "Primarily motivated by (Grand- valet and Bengio, 2004) and (Laine and Aila, 2017), we employed entropy minimization and self-ensemble bootstrapping as regularizations to incorporate unlabeled data. Our experimental re- sults show that both methods are effective when jointly trained with the feature adaptation objec- tive, which con\ufb01rms to our motivation. 3 Model Description 3.1 Notations and Model Overview We conduct most of our experiments under an un- supervised domain adaptation setting, where we have no labeled data from the target domain. Con- sider two sets Ds and Dt. Ds = {x(s) i , y(s) i }|ns i=1 is",
            "from the source domain with ns labeled examples, where yi \u2208RC is a one-hot vector representation of sentiment label and C denotes the number of classes. Dt = {x(t) i }|nt i=1 is from the target domain with nt unlabeled examples. N = ns +nt denotes the total number of training documents including both labeled and unlabeled1. We aim to learn a sentiment classi\ufb01er from Ds and Dt such that the classi\ufb01er would work well on the target domain. We also present some results under a setting where we assume that a small number of labeled target examples are available (see Figure 3). For the proposed model, we denote G parame- terized by \u03b8g as a neural-based feature encoder that maps documents from both domains to a shared feature space, and F parameterized by \u03b8f as a fully connected layer with softmax activation serv- ing as the sentiment classi\ufb01er.",
            "For the proposed model, we denote G parame- terized by \u03b8g as a neural-based feature encoder that maps documents from both domains to a shared feature space, and F parameterized by \u03b8f as a fully connected layer with softmax activation serv- ing as the sentiment classi\ufb01er. We aim to learn fea- ture representations that are domain-invariant and at the same time discriminative on both domains, thus we simultaneously consider three factors in our objective: (1) minimize the classi\ufb01cation error on the labeled source examples; (2) minimize the domain discrepancy; and (3) leverage unlabeled data via semi-supervised learning.",
            "Suppose we already have the encoded features of documents {\u03be(s,t) i = G(x(s,t) i ; \u03b8g)}|N i=1 (see Section 4.1), the objective function for purpose (1) is thus the cross entropy loss on the labeled source examples L = \u22121 ns ns X i=1 C X j=1 y(s) i (j) log \u02dcy(s) i (j) (1) where \u02dcy(s) i = F(\u03be(s) i ; \u03b8f) denotes the predicted la- bel distribution. In the following subsections, we will explain how to perform feature adaptation and domain adaptive semi-supervised learning in de- tails for purpose (2) and (3) respectively. 3.2 Feature Adaptation Unlike prior works (Blitzer et al., 2007; Yu and Jiang, 2016), our method does not attempt to align domain-speci\ufb01c words through pivot words. In our preliminary experiments, we found that word embeddings pre-trained on a large corpus are able to adequately capture this information.",
            "In our preliminary experiments, we found that word embeddings pre-trained on a large corpus are able to adequately capture this information. As we will 1Note that unlabeled source examples can also be in- cluded for training. In that case, N = ns + nt + ns\u2032 where ns\u2032 denotes the number of unlabeled source examples. This corresponds to our experimental setting 2. For simplicity, we only consider ns and nt in our description. later show in our experiments, even without adap- tation, a naive neural network classi\ufb01er with pre- trained word embeddings can already achieve rea- sonably good results. We attempt to explicitly minimize the distance between the source and target feature represen- tations ({\u03be(s) i }|ns i=1 and {\u03be(t) i }nt i=1). A few meth- ods from literature can be applied such as Maxi- mum Mean Discrepancy (MMD) (Gretton et al., 2012) or adversary training (Li et al., 2017; Chen et al., 2017).",
            "A few meth- ods from literature can be applied such as Maxi- mum Mean Discrepancy (MMD) (Gretton et al., 2012) or adversary training (Li et al., 2017; Chen et al., 2017). The main idea of MMD is to esti- mate the distance between two distributions as the distance between sample means of the projected embeddings in Hilbert space. MMD is implicitly computed through a characteristic kernel, which is used to ensure that the sample mean is injective, leading to the MMD being zero if and only if the distributions are identical. In our implementation, we skip the mapping procedure induced by a char- acteristic kernel for simplifying the computation and learning. We simply estimate the distribution distance as the distance between the sample means in the current embedding space. Although this ap- proximation cannot preserve all statistical features of the underlying distributions, we \ufb01nd it performs comparably to MMD on our problem.",
            "We simply estimate the distribution distance as the distance between the sample means in the current embedding space. Although this ap- proximation cannot preserve all statistical features of the underlying distributions, we \ufb01nd it performs comparably to MMD on our problem. The follow- ing equations formally describe the feature adap- tation loss J : J = KL(gs||gt) + KL(gt||gs) (2) g\u2032 s = 1 ns ns X i=1 \u03be(s) i , gs = g\u2032 s \u2225g\u2032s\u22251 (3) g\u2032 t = 1 nt nt X i=1 \u03be(t) i , gt = g\u2032 t \u2225g\u2032 t\u22251 (4) L1 normalization is applied on the mean represen- tations g\u2032 s and g\u2032 t, rescaling the vectors such that all entries sum to 1. We adopt a symmetric ver- sion of KL divergence (Zhuang et al., 2015) as the distance function. Given two distribution vectors P, Q \u2208Rk, KL(P||Q) = Pk i=1 P(i) log( P(i) Q(i)).",
            "We adopt a symmetric ver- sion of KL divergence (Zhuang et al., 2015) as the distance function. Given two distribution vectors P, Q \u2208Rk, KL(P||Q) = Pk i=1 P(i) log( P(i) Q(i)). 3.3 Domain Adaptive Semi-supervised Learning (DAS) We attempt to exploit the information in target data through semi-supervised learning objectives, which are jointly trained with L and J . Normally, to incorporate target data, we can minimize the cross entropy loss between the true label distri- butions y(t) i and the predicted label distributions",
            "\u02dcy(t) i = F(\u03be(t) i ; \u03b8f) over target samples. The chal- lenge here is that y(t) i is unknown, and thus we attempt to estimate it via semi-supervised learn- ing. We use entropy minimization and bootstrap- ping for this purpose. We will later show in our experiments that both methods are effective, and jointly employing them overall yields the best re- sults. Entropy Minimization: In this method, y(t) i is estimated as the predicted label distribution \u02dcy(t) i , which is a function of \u03b8g and \u03b8f.",
            "Entropy Minimization: In this method, y(t) i is estimated as the predicted label distribution \u02dcy(t) i , which is a function of \u03b8g and \u03b8f. The loss can thus be written as \u0393 = \u22121 nt nt X i=1 C X j=1 \u02dcy(t) i (j) log \u02dcy(t) i (j) (5) Assume the domain discrepancy can be effectively reduced through feature adaptation, by minimiz- ing the entropy penalty, training of the classi\ufb01er is in\ufb02uenced by the unlabeled target data and will generally maximize the margins between the tar- get examples and the decision boundaries, increas- ing the prediction con\ufb01dence on the target domain. Self-ensemble Bootstrapping: Another way to estimate y(t) i corresponds to bootstrapping. The idea is to estimate the unknown labels as the predictions of the model learned from the pre- vious round of training. Bootstrapping has been explored for domain adaptation in previous works (Jiang and Zhai, 2007; Wu et al., 2009).",
            "The idea is to estimate the unknown labels as the predictions of the model learned from the pre- vious round of training. Bootstrapping has been explored for domain adaptation in previous works (Jiang and Zhai, 2007; Wu et al., 2009). However, in their methods, domain discrepancy was not explicitly minimized via feature adap- tation. Applying bootstrapping or other semi- supervised learning techniques in this case may worsen the results as the classi\ufb01er can perform quite bad on the target data. Inspired by the ensembling method proposed in (Laine and Aila, 2017), we estimate y(t) i by forming ensemble predictions of labels during training, using the outputs on different training epochs. The loss is formulated as follows: \u2126= \u22121 N N X i=1 C X j=1 \u02dcz(s,t) i (j) log \u02dcy(s,t) i (j) (6) where \u02dcz denotes the estimated labels computed on the ensemble predictions from different epochs. The loss is applied on all documents. It serves for bootstrapping on the unlabeled target data,",
            "The loss is applied on all documents. It serves for bootstrapping on the unlabeled target data, and it also serves as a regularization that encourages Algorithm 1 Pseudocode for training DAS Require: Ds, Dt, G, F Require: \u03b1 = ensembling momentum, 0 \u2264\u03b1 < 1 Require: w(t) = weight ramp-up function Z \u21900[N\u00d7C] \u02dcz \u21900[N\u00d7C] for t \u2208[1, max-epochs] do for each minibatch B(s), B(t), B(u) in Ds, Dt, {x(s,t) i }|N i=1 do compute loss L on [xi\u2208B(s), yi\u2208B(s)] compute loss J on [xi\u2208B(s), xj\u2208B(t)] compute loss \u0393 on xi\u2208B(t) compute loss \u2126on [xi\u2208B(u), \u02dczi\u2208B(u)] overall-loss \u2190L + \u03bb1J + \u03bb2\u0393 + w(t)\u2126 update network parameters end for Z\u2032 i \u2190F(G(xi)),",
            "\u02dczi\u2208B(u)] overall-loss \u2190L + \u03bb1J + \u03bb2\u0393 + w(t)\u2126 update network parameters end for Z\u2032 i \u2190F(G(xi)), for i \u2208N Z \u2190\u03b1Z + (1 \u2212\u03b1)Z\u2032 \u02dcz \u2190one-hot-vectors(Z) end for the network predictions to be consistent in differ- ent training epochs. \u2126is jointly trained with L, J , and \u0393. Algorithm 1 illustrates the overall train- ing process of the proposed domain adaptive semi- supervised learning (DAS) framework. In Algorithm 1, \u03bb1, \u03bb2, and w(t) are weights to balance the effects of J , \u0393, and \u2126respectively. \u03bb1 and \u03bb2 are constant hyper-parameters. We set w(t) = exp[\u22125(1 \u2212 t max-epochs)2]\u03bb3 as a Gaus- sian curve to ramp up the weight from 0 to \u03bb3. This is to ensure the ramp-up of the bootstrapping loss component is slow enough in the beginning of the training.",
            "This is to ensure the ramp-up of the bootstrapping loss component is slow enough in the beginning of the training. After each training epoch, we com- pute Z\u2032 i which denotes the predictions made by the network in current epoch, and then the ensemble prediction Zi is updated as a weighted average of the outputs from previous epochs and the current epoch, with recent epochs having larger weight. For generating estimated labels \u02dczi, Zi is converted to a one-hot vector where the entry with the maxi- mum value is set to one and other entries are set to zeros. The self-ensemble bootstrapping is a gener- alized version of bootstrappings that only use the outputs from the previous round of training (Jiang and Zhai, 2007; Wu et al., 2009). The ensemble prediction is likely to be closer to the correct, un- known labels of the target data.",
            "Domain #Pos #Neg #Neu Total Book Set 1 2000 2000 2000 6000 Set 2 4824 513 663 6000 Electronics Set 1 2000 2000 2000 6000 Set 2 4817 694 489 6000 Beauty Set 1 2000 2000 2000 6000 Set 2 4709 616 675 6000 Music Set 1 2000 2000 2000 6000 Set 2 4441 785 774 6000 (a) Small-scale datasets Domain #Pos #Neg #Neu Total IMDB 55,242 11,735 17,942 84,919 Yelp 155,625 29,597 45,941 231,163 Cell Phone 148,657 24,343 21,439 194,439 Baby 126,525 17,012 17,255 160,792 (b) Large-scale datasets Table 1: Summary of datasets.",
            "4 Experiments 4.1 CNN Encoder Implementation We have left the feature encoder G unspeci\ufb01ed, for which, a few options can be considered. In our implementation, we adopt a one-layer CNN structure from previous works (Kim, 2014; Yu and Jiang, 2016), as it has been demonstrated to work well for sentiment classi\ufb01cation tasks. Given a re- view document x = (x1, x2, ..., xn) consisting of n words, we begin by associating each word with a continuous word embedding (Mikolov et al., 2013) ex from an embedding matrix E \u2208RV \u00d7d, where V is the vocabulary size and d is the embed- ding dimension. E is jointly updated with other network parameters during training.",
            "E is jointly updated with other network parameters during training. Given a win- dow of dense word embeddings ex1, ex2, ..., exl, the convolution layer \ufb01rst concatenates these vec- tors to form a vector \u02c6x of length ld and then the output vector is computed by Equation (7): Conv(\u02c6x) = f(W \u00b7 \u02c6x + b) (7) \u03b8g = {W, b} is the parameter set of the en- coder G and is shared across all windows of the sequence. f is an element-wise non-linear activa- tion function. The convolution operation can cap- ture local contextual dependencies of the input se- quence and the extracted feature vectors are sim- ilar to n-grams. After the convolution operation is applied to the whole sequence, we obtain a list of hidden vectors H = (h1, h2, ..., hn). A max- over-time pooling layer is applied to obtain the \ufb01- nal vector representation \u03be of the input document.",
            "After the convolution operation is applied to the whole sequence, we obtain a list of hidden vectors H = (h1, h2, ..., hn). A max- over-time pooling layer is applied to obtain the \ufb01- nal vector representation \u03be of the input document. 4.2 Datasets and Experimental Settings Existing benchmark datasets such as the Amazon benchmark (Blitzer et al., 2007) typically remove reviews with neutral labels in both domains. This is problematic as the label information of the tar- get domain is not accessible in an unsupervised domain adaptation setting. Furthermore, remov- ing neutral instances may bias the dataset favor- ably for max-margin-based algorithms like ours, since the resulting dataset has all uncertain labels removed, leaving only high con\ufb01dence examples. Therefore, we construct new datasets by ourselves. The results on the original Amazon benchmark is qualitatively similar, and we present them in Ap- pendix A for completeness since most of previous works reported results on it. Small-scale datasets: Our new dataset was de- rived from the large-scale Amazon datasets2 re- leased by McAuley et al. (2015).",
            "Small-scale datasets: Our new dataset was de- rived from the large-scale Amazon datasets2 re- leased by McAuley et al. (2015). It contains four domains3: Book (BK), Electronics (E), Beauty (BT), and Music (M). Each domain contains two datasets. Set 1 contains 6000 instances with ex- actly balanced class labels, and set 2 contains 6000 instances that are randomly sampled from the large dataset, preserving the original label dis- tribution, which we believe better re\ufb02ects the label distribution in real life. The examples in these two sets do not overlap. Detailed statistics of the gen- erated datasets are given in Table 1a. In all our experiments on the small-scale datasets, we use set 1 of the source domain as the only source with sentiment label information dur- ing training, and we evaluate the trained model on set 1 of the target domain. Since we cannot con- trol the label distribution of unlabeled data during training, we consider two different settings: Setting (1): Only set 1 of the target domain is used as the unlabeled set.",
            "Since we cannot con- trol the label distribution of unlabeled data during training, we consider two different settings: Setting (1): Only set 1 of the target domain is used as the unlabeled set. This tells us how the method performs in a condition when the target domain has a close-to-balanced label distribution. As we also evaluate on set 1 of the target domain, this is also considered as a transductive setting. Setting (2): Set 2 from both the source and target domains are used as unlabeled sets. Since set 2 is directly sampled from millions of reviews, it better re\ufb02ects real-life sentiment distribution. Large-scale datasets: We further conduct ex- periments on four much larger datasets: IMDB4 2http:\/\/jmcauley.ucsd.edu\/data\/amazon\/ 3The original reviews were rated on a 5-point scale. We label them with rating < 3, > 3, and = 3 as negative, posi- tive, and neutral respectively.",
            "We label them with rating < 3, > 3, and = 3 as negative, posi- tive, and neutral respectively. 4IMDB is rated on a 10-point scale, and we label reviews with rating < 5, > 6, and = 5\/6 as negative, positive, and neutral respectively.",
            "(I), Yelp2014 (Y), Cell Phone (C), and Baby (B). IMDB and Yelp2014 were previously used in (Tang et al., 2015; Yang et al., 2017). Cell phone and Baby are from the large-scale Amazon dataset (McAuley et al., 2015; He and McAuley, 2016). Detailed statistics are summarized in Ta- ble 1b. We keep all reviews in the original datasets and consider a transductive setting where all target examples are used for both training (without la- bel information) and evaluation. We perform sam- pling to balance the classes of labeled source data in each minibatch B(s) during training. 4.3 Selection of Development Set Ideally, the development set should be drawn from the same distribution as the test set. However, un- der the unsupervised domain adaptation setting, we do not have any labeled target data at training phase which could be used as development set. In all of our experiments, for each pair of domains, we instead sample 1000 examples from the train- ing set of the source domain as development set.",
            "In all of our experiments, for each pair of domains, we instead sample 1000 examples from the train- ing set of the source domain as development set. We train the network for a \ufb01xed number of epochs, and the model with the minimum classi\ufb01cation er- ror on this development set is saved for evaluation. This approach works well on most of the problems since the target domain is supposed to behave like the source domain if the domain difference is ef- fectively reduced. Another problem is how to select the values for hyper-parameters. If we tune \u03bb1 and \u03bb2 directly on the development set from the source domain, most likely both of them will be set to 0, as un- labeled target data is not helpful for improving in- domain accuracy of the source domain. Other neu- ral network models also have the same problem for hyper-parameter tuning. Therefore, our strategy is to use the development set from the target domain to optimize \u03bb1 and \u03bb2 for one problem (e.g., we only do this on E\u2192BK), and \ufb01x their values on the other problems.",
            "Therefore, our strategy is to use the development set from the target domain to optimize \u03bb1 and \u03bb2 for one problem (e.g., we only do this on E\u2192BK), and \ufb01x their values on the other problems. This setting assumes that we have at least two labeled domains such that we can op- timize the hyper-parameters, and then we \ufb01x them for other new unlabeled domains to transfer to. 4.4 Training Details and Hyper-parameters We initialize word embeddings using the 300- dimension GloVe vectors supplied by Pennington et al., (2014), which were trained on 840 billion tokens from the Common Crawl. For each pair of domains, the vocabulary consists of the top 10000 most frequent words. For words in the vocabulary but not present in the pre-trained embeddings, we randomly initialize them. We set hyper-parameters of the CNN en- coder following previous works (Kim, 2014; Yu and Jiang, 2016) without speci\ufb01c tuning on our datasets. The window size is set to 3 and the size of the hidden layer is set to 300. The nonlinear activation function is Relu.",
            "The window size is set to 3 and the size of the hidden layer is set to 300. The nonlinear activation function is Relu. For regularization, we also follow their settings and employ dropout with probability set to 0.5 on \u03bei before feeding it to the output layer F, and constrain the l2-norm of the weight vector \u03b8f, setting its max norm to 3. On the small-scale datasets and the Aamzon benchmark, \u03bb1 and \u03bb2 are set to 200 and 1, respectively, tuned on the development set of task E\u2192BK under setting 1. On the large-scale datasets, \u03bb1 and \u03bb2 are set to 500 and 0.2, re- spectively, tuned on I\u2192Y. We use a Gaussian curve w(t) = exp[\u22125(1 \u2212 t tmax )2]\u03bb3 to ramp up the weight of the bootstrapping loss \u2126from 0 to \u03bb3, where tmax denotes the maximum number of training epochs. We train 30 epochs for all exper- iments. We set \u03bb3 to 3 and \u03b1 to 0.5 for all experi- ments.",
            "We train 30 epochs for all exper- iments. We set \u03bb3 to 3 and \u03b1 to 0.5 for all experi- ments. The batch size is set to 50 on the small-scale datasets and the Amazon benchmark. We increase the batch size to 250 on the large-scale datasets to reduce the number of iterations. RMSProp opti- mizer with learning rate set to 0.0005 is used for all experiments. 4.5 Models for Comparison We compare with the following baselines: (1) Naive: A non-domain-adaptive baseline with bag-of-words representations and SVM clas- si\ufb01er trained on the source domain. (2) mSDA (Chen et al., 2012): This is the state- of-the-art method based on discrete input features. Top 1000 bag-of-words features are kept as pivot features. We set the number of stacked layers to 3 and the corruption probability to 0.5.",
            "Top 1000 bag-of-words features are kept as pivot features. We set the number of stacked layers to 3 and the corruption probability to 0.5. (3) NaiveNN: This is a non-domain-adaptive CNN trained on source domain, which is a variant of our model by setting \u03bb1, \u03bb2, and \u03bb3 to zeros. (4) AuxNN (Yu and Jiang, 2016): This is a neu- ral model that exploits auxiliary tasks, which has achieved state-of-the-art results on cross-domain sentiment classi\ufb01cation. The sentence encoder used in this model is the same as ours. (5) ADAN (Chen et al., 2017): This method exploits adversarial training to reduce representa-",
            "40 45 50 55 60 65 70 Naive mSDA NaiveNN AuxNN ADAN MMD Ours: FANN Ours: DAS-SE Ours: DAS-EM Ours: DAS Accuracy (%) (a) Accuracy on the small-scale dataset under setting 1. 40 45 50 55 60 65 Naive mSDA NaiveNN AuxNN ADAN MMD Ours: FANN Ours: DAS-SE Ours: DAS-EM Ours: DAS Accuracy (%) (b) Accuracy on the small-scale dataset under setting 2. 45 50 55 60 NaiveNN ADAN MMD Ours: FANN Ours: DAS-SE Ours: DAS-EM Ours: DAS Macro-F1 (%) (c) Macro-F1 on the large-scale dataset. Figure 1: Performance comparison. Average results over 5 runs with random initializations are reported for each neural method.",
            "Figure 1: Performance comparison. Average results over 5 runs with random initializations are reported for each neural method. \u2217indicates that the proposed method (either of DAS, DAS-EM, DAS-SE) is signi\ufb01cantly better than other baselines (baseline 1-6) with p < 0.05 based on one-tailed unpaired t-test. tion difference between domains. The original pa- per uses a simple feedforward network as encoder. For fair comparison, we replace it with our CNN- based encoder. We train 5 iterations on the dis- criminator per iteration on the encoder and senti- ment classi\ufb01er as suggested in their paper. (6) MMD: MMD has been widely used for min- imizing domain discrepancy on images. In those works (Tzeng et al., 2014; Long et al., 2017), vari- ants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly mini- mized.",
            "In those works (Tzeng et al., 2014; Long et al., 2017), vari- ants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly mini- mized. In NLP, adding more layers of CNNs may not be very helpful and thus those models from image-related tasks can not be directly applied to our problem. To compare with MMD-based method, we train a model that jointly minimize the classi\ufb01cation loss L on the source domain and MMD between {\u03be(s) i |ns i=1} and {\u03be(t) i |nt i=1}. For computing MMD, we use a Gaussian RBF which is a common choice for characteristic kernel. In addition to the above baselines, we also show results of different variants of our model. DAS as shown in Algorithm 1 denotes our full model. DAS-EM denotes the model with only entropy minimization for semi-supervised learning (set \u03bb3 = 0). DAS-SE denotes the model with only self-ensemble bootstrapping for semi-supervised learning (set \u03bb2 = 0).",
            "DAS-EM denotes the model with only entropy minimization for semi-supervised learning (set \u03bb3 = 0). DAS-SE denotes the model with only self-ensemble bootstrapping for semi-supervised learning (set \u03bb2 = 0). FANN (feature-adaptation neural network) denotes the model without semi- supervised learning performed (set both \u03bb2 and \u03bb3 to zeros). 4.6 Main Results Figure 15 shows the comparison of adaptation re- sults (see Appendix B for the exact numerical numbers). We report classi\ufb01cation accuracy on the small-scale dataset. For the large-scale dataset, macro-F1 is instead used since the label distribu- tion in the test set is extremely unbalanced. Key observations are summarized as follows. (1) Both DAS-EM and DAS-SE perform better in most cases compared with ADAN, MDD, and FANN, in which only feature adaptation is performed. This demonstrates the effectiveness of the pro- 5We exclude results of Naive, mSDA and AuxNN on the large-scale dataset.",
            "This demonstrates the effectiveness of the pro- 5We exclude results of Naive, mSDA and AuxNN on the large-scale dataset. Both Naive and mSDA have dif\ufb01culties to scale up to the large dataset. AuxNN relies on manually selecting positive and negative pivots before training.",
            "0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 55 60 65 DAS MMD ADAN (a)  Percentage of unlabeled target examples (BT->BK) Accuracy (%) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 45 50 55 DAS MMD ADAN (b)  Percentage of unlabeled target examples (M->E) Accuracy (%) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 50 55 60 DAS MMD ADAN (c)  Percentage of unlabeled target examples (E->BT) Accuracy (%) Figure 2: Accuracy vs. percentage of unlabeled target training examples.",
            "percentage of unlabeled target training examples. 0 50 100 150 200 250 300 350 400 450 500 60 65 70 DAS MMD ADAN NaiveNN (a)  # of labeled target examples (BT->BK) Accuracy (%) 0 50 100 150 200 250 300 350 400 450 500 50 55 60 DAS MMD ADAN NaiveNN (b)  # of labeled target examples (M->E) Accuracy (%) 0 50 100 150 200 250 300 350 400 450 500 55 60 65 DAS MMD ADAN NaiveNN (c)  # of labeled target examples (E->BT) Accuracy (%) Figure 3: Accuracy vs. number of labeled target training examples. posed domain adaptive semi-supervised learning framework. DAS-EM is more effective than DAS- SE in most cases, and the full model DAS with both techniques jointly employed overall has the best performance. (2) When comparing the two settings on the small-scale dataset, all domain- adaptive methods6 generally perform better under setting 1.",
            "DAS-EM is more effective than DAS- SE in most cases, and the full model DAS with both techniques jointly employed overall has the best performance. (2) When comparing the two settings on the small-scale dataset, all domain- adaptive methods6 generally perform better under setting 1. In setting 1, the target examples are bal- anced in classes, which can provide more diverse opinion-related features. However, when consid- ering unsupervised domain adaptation, we should not presume the label distribution of the unlabeled data. Thus, it is necessary to conduct experiments using datasets that re\ufb02ect real-life sentiment dis- tribution as what we did on setting2 and the large- scale dataset. Unfortunately, this is ignored by most of previous works. (3) Word-embeddings are very helpful, as we can see even NaiveNN can sub- stantially outperform mSDA on most tasks. To see the effect of semi-supervised learning alone, we also conduct experiments by setting \u03bb1 = 0 to eliminate the effect of feature adapta- tion. Both entropy minimization and bootstrap- ping perform very badly in this setting.",
            "To see the effect of semi-supervised learning alone, we also conduct experiments by setting \u03bb1 = 0 to eliminate the effect of feature adapta- tion. Both entropy minimization and bootstrap- ping perform very badly in this setting. En- tropy minimization gives almost random predic- tions with accuracy below 0.4, and the results of bootstrapping are also much lower compared to NaiveNN. This suggests that the feature adap- tation component is essential. Without it, the learned target representations are less meaning- ful and discriminative. Applying semi-supervised 6Results of Naive and NaiveNN do not change under both settings as they are only trained on the source domain. learning in this case is likely to worsen the results. 4.7 Further Analysis In Figure 2, we show the change of accuracy with respect to the percentage of unlabeled data used for training on three particular problems under set- ting 1. The value at x = 0 denotes the accuracies of NaiveNN which does not utilize any target data.",
            "4.7 Further Analysis In Figure 2, we show the change of accuracy with respect to the percentage of unlabeled data used for training on three particular problems under set- ting 1. The value at x = 0 denotes the accuracies of NaiveNN which does not utilize any target data. For DAS, we observe a nonlinear increasing trend where the accuracy quickly improves at the be- ginning, and then gradually stabilizes. For other methods, this trend is less obvious, and adding more unlabeled data sometimes even worsen the results. This \ufb01nding again suggests that the pro- posed approach can better exploit the information from unlabeled data. We also conduct experiments under a setting with a small number of labeled target examples available. Figure 3 shows the change of accuracy with respect to the number of labeled target exam- ples added for training. We can observe that DAS is still more effective under this setting, while the performance differences to other methods gradu- ally decrease with the increasing number of la- beled target examples. 4.8 CNN Filter Analysis In this subsection, we aim to better understand DAS by analyzing sentiment-related CNN \ufb01lters.",
            "4.8 CNN Filter Analysis In this subsection, we aim to better understand DAS by analyzing sentiment-related CNN \ufb01lters. To do that, 1) we \ufb01rst select a list of the most re- lated CNN \ufb01lters for predicting each sentiment la- bel (positive, negative neutral). Those \ufb01lters can be identi\ufb01ed according to the learned weights \u03b8f",
            "best-value-at highly-recommend-! nars-are-amazing beauty-store-suggested since-i-love good-value-at highly-advise-! ulta-are-fantastic durable-machine-and years-i-love perfect-product-for gogeous-absolutely-perfect length-are-so perfect-length-and bonus-i-love great-product-at love-love-love expected-in-perfect great-store-on appearance-i-love amazing-product-\u2217 highly-recommend-for setting-works-perfect beauty-store-for relaxing-i-love (a) NaiveNN prices-my-favorite so-nicely-! purchase-thanks-! feel-wonderfully-clean are-really-cleaning brands-my-favorite more-affordable-price buy-again-! on-nicely-builds washing-and-cleaning very-great-stores shampoo-a-perfect without-hesitation-! polish-easy-and really-good-shampoo great-bottle-also an-excellent-value buy-this-! felt-cleanser-than deeply-cleans-my scent-pleasantly-\ufb02oral really-enjoy-it discount-too-!",
            "polish-easy-and really-good-shampoo great-bottle-also an-excellent-value buy-this-! felt-cleanser-than deeply-cleans-my scent-pleasantly-\ufb02oral really-enjoy-it discount-too-! honestly-perfect-it totally-moisturize-our (b) FANN bath-\u2019s-wonderful love-fruity-sweet feeling-smooth-radiant cleans-thoroughly-* excellent-everyday-lotion all-pretty-affordable absorb-really-nicely love-lavender-scented loving-this-soap affordable-cleans-nicely it-delivers-fabulous shower-lather-wonderfully am-very-grateful bed-of-love fantastic-base-coat and-blends-nicely *-smells-fantastic love-fruity-fragrances shower-!-* nice-gentle-scrub heats-quickly-love and-clean-excellent perfect-beautiful-shimmer radiant-daily-moisturizer surprisingly-safe-on (c) DAS Table 2: Comparison of the top trigrams (each column) from the target domain (beauty) captured by the 5 most positive-sentiment-related CNN \ufb01lters learned on E\u2192BT.",
            "\u2217denotes a padding. of the output layer F. Higher weight indicates stronger relatedness. 2) Recall that in our im- plementation, each CNN \ufb01lter has a window size of 3 with Relu activation. We can thus represent each selected \ufb01lter as a ranked list of trigrams with highest activation values. We analyze the CNN \ufb01lters learned by NaiveNN, FANN and DAS respectively on task E\u2192BT under setting 1. We focus on E\u2192BT for study because electronics and beauty are very dif- ferent domains and each of them has a diverse set of domain-speci\ufb01c sentiment expressions. For each method, we identify the top 10 most related \ufb01lters for each sentiment label, and extract the top trigrams of each selected \ufb01lter on both source and target domains. Since labeled source examples are used for training, we \ufb01nd the \ufb01lters learned by the three methods capture similar expressions on the source domain, containing both domain-invariant and domain-speci\ufb01c trigrams.",
            "Since labeled source examples are used for training, we \ufb01nd the \ufb01lters learned by the three methods capture similar expressions on the source domain, containing both domain-invariant and domain-speci\ufb01c trigrams. On the target do- main, DAS captures more target-speci\ufb01c expres- sions compared to the other two methods. Due to space limitation, we only present a small sub- set of positive-sentiment-related \ufb01lters in Table 2. The complete results are provided in Appendix C. From Table 2, we can observe that the \ufb01lters learned by NaiveNN are almost unable to cap- ture target-speci\ufb01c sentiment expressions, while FANN is able to capture limited target-speci\ufb01c words such as \u201cclean\u201d and \u201cscent\u201d. The \ufb01lters learned by DAS are more domain-adaptive, cap- turing diverse sentiment expressions in the target domain. 5 Conclusion In this work, we propose DAS, a novel frame- work that jointly performs feature adaptation and semi-supervised learning.",
            "The \ufb01lters learned by DAS are more domain-adaptive, cap- turing diverse sentiment expressions in the target domain. 5 Conclusion In this work, we propose DAS, a novel frame- work that jointly performs feature adaptation and semi-supervised learning. We have demonstrated through multiple experiments that DAS can better leverage unlabeled data, and achieve substantial improvements over baseline methods. We have also shown that feature adaptation is an essen- tial component, without which, semi-supervised learning is not able to function properly. The pro- posed framework could be potentially adapted to other domain adaptation tasks, which is the focus of our future studies. References John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, Bollywood, boom-boxes and blenders: domain adaptation for sentiment classi\ufb01- cation. In Annual Meeting of the Association for Computational Linguistics. Minmin Chen, Zhixiang Xu, Kilian Q. Weinberger, and Fei Sha. 2012. Marginalized denoising autoen- coders for domain adaptation. In The 29th Interna- tional Conference on Machine Learning.",
            "Minmin Chen, Zhixiang Xu, Kilian Q. Weinberger, and Fei Sha. 2012. Marginalized denoising autoen- coders for domain adaptation. In The 29th Interna- tional Conference on Machine Learning. Xilun Chen, Yu Sun, Ben Athiwarakun, Claire Cardie, and Kilian Weinberger. 2017. Adversarial deep av- eraging networks for cross-lingual sentiment classi- \ufb01er. In Arxiv e-prints arXiv:1606.01614. Wenyuan Dai, Gui rong Xue, Qiang Yang, and Yong Yu. 2007. Transferring naive Bayes classi\ufb01ers for text classi\ufb01cation. In AAAI Conference on Arti\ufb01cial Intelligence.",
            "Yaroslav Ganin and Victor Lempitsky. 2015. Unsuper- vised domain adaptation by backpropagation. In In- ternational Conference on Machine Learning. Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Domain adaptation for large-scale sentiment classi\ufb01cation: a deep learning approach. In The 28th International Conference on Machine Learning. Yves Grandvalet and Yoshua Bengio. 2004. Semi- supervised learning by entropy minimization. In Neural Information Processing Systems. Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Sch\u00a8olkopf, and Alexander Smola. 2012. A kernel two-sample test. Journal of Machine Learn- ing Research, 13:723\u2013773. Ruining He and Julian McAuley. 2016. Ups and downs: modeling the visual evolution of fashion trends with one-class collaborative \ufb01ltering. In WWW. Yulan He, Chenghua Lin, and Harith Alani. 2011.",
            "Ruining He and Julian McAuley. 2016. Ups and downs: modeling the visual evolution of fashion trends with one-class collaborative \ufb01ltering. In WWW. Yulan He, Chenghua Lin, and Harith Alani. 2011. Automatically extracting polarity-bearing topics for cross-domain sentiment classi\ufb01cation. In ACL. Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in NLP. In Annual Meeting of the Association for Computational Lin- guistics. Yoon Kim. 2014. Convolutional neural networks for sentence classi\ufb01cation. In Conference on Empirical Methods in Natural Language Processing. Samuli Laine and Timo Aila. 2017. Temporal ensem- bling for semi-supervised learning. In International Conference on Learning Representation. Zheng Li, Yun Zhang, Ying Wei, Yuxiang Wu, and Qiang Yang. 2017. End-to-end adversarial mem- ory network for cross-domain sentiment classi\ufb01ca- tion. In The 26th International Joint Conference on Arti\ufb01cial Intelligence.",
            "2017. End-to-end adversarial mem- ory network for cross-domain sentiment classi\ufb01ca- tion. In The 26th International Joint Conference on Arti\ufb01cial Intelligence. Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. 2015. Learning transferable fea- tures with deep adaptation networks. In Interna- tional Conference on Machine Learning. Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I. Jordan. 2017. Deep transfer learning with joint adaptation networks. In International Confer- ence on Machine Learning. Julian J. McAuley, Christopher Targett, Qinfeng Shi, and Anton van den Hengel. 2015. Image-based rec- ommendations on styles and substitutes. In The 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor- rado, and Jeffrey Dean. 2013. Distributed represen- tations of words and phrases and their composition- ality.",
            "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor- rado, and Jeffrey Dean. 2013. Distributed represen- tations of words and phrases and their composition- ality. In Neural Information Processing Systems. Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang Yang, and Zheng Chen. 2010. Cross-domain senti- ment classi\ufb01cation via spectral feature alignment. In The 19th International World Wide Web Conference. Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. GloVe: Global vectors for word representation. In Conference on Empirical Meth- ods in Natural Language Processing. Duyu Tang, Bing Qin, and Ting Liu. 2015. Learn- ing semantic representation of users and products for document level sentiment classi\ufb01cation. In Annual Meeting of the Association for Computational Lin- guistics. Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. 2014. Deep domain confusion: maximizing for domain invariance.",
            "In Annual Meeting of the Association for Computational Lin- guistics. Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. 2014. Deep domain confusion: maximizing for domain invariance. In Arxiv e-prints arXiv:1412.3474. Dan Wu, Wee Sun Lee, Nan Ye, and Hai Leong Chieu. 2009. Domain adaptive bootstrapping for named en- tity recognition. In Conference on Empirical Meth- ods in Natural Language Processing. Fangzhao Wu and Yongfeng Huang. 2016. Sentiment domain adaptation with multiple sources. In Annual Meeting of the Association for Computational Lin- guistics. Wei Yang, Wei Lu, and Vincent W. Zheng. 2017. A simple regularization-based algorithm for learning cross-domain word embeddings. In Conference on Empirical Methods in Natural Language Process- ing. Yi Yang and Jacob Eisenstein. 2014. Fast easy unsu- pervised domain adaptation with marginalized struc- tured dropout. In Annual Meeting of the Association for Computational Linguistics.",
            "In Conference on Empirical Methods in Natural Language Process- ing. Yi Yang and Jacob Eisenstein. 2014. Fast easy unsu- pervised domain adaptation with marginalized struc- tured dropout. In Annual Meeting of the Association for Computational Linguistics. Jianfei Yu and Jing Jiang. 2016. Learning sentence em- beddings with auxiliary tasks for cross-domain sen- timent classi\ufb01cation. In Conference on Empirical Methods in Natural Language Processing. Guangyou Zhou, Tingting He, Wensheng Wu, and Xi- aohua Tony Hu. 2015. Linking heterogeneous input features with pivots for domain adaptation. In The 24th International Joint Conference on Arti\ufb01cial In- telligence. Guangyou Zhou, Zhiwen Xie, Jimmy Xiangji Huang, and Tingting He. 2016. Bi-transferring deep neural networks for domain adaptation. In Annual Meeting of the Association for Computational Linguistics. Fuzhen Zhuang, Xiaohu Cheng, Ping Luo, Sinno Jialin Pan, and Qing He. 2015.",
            "2016. Bi-transferring deep neural networks for domain adaptation. In Annual Meeting of the Association for Computational Linguistics. Fuzhen Zhuang, Xiaohu Cheng, Ping Luo, Sinno Jialin Pan, and Qing He. 2015. Supervised representation learning: transfer learning with deep autoencoders. In The 24th International Joint Conference on Arti- \ufb01cial Intelligence.",
            "S T Naive mSDA NaiveNN AuxNN ADAN MMD FANN DAS-EM DAS-SE DAS D B 75.20 78.50 81.10 80.80 81.70 81.05 80.30 82.00\u2217 82.10\u2217 82.05\u2217 E B 68.85 76.15 77.95 78.00 78.55 78.65 77.25 80.25\u2217 77.75 80.00\u2217 K B 70.00 75.65 77.75 77.85 79.25 79.70 79.20 79.95 79.60 80.05\u2217 B D 77.15 80.60 80.80 81.75 82.30 82.00 81.65 82.65 82.35 82.75\u2217 E D 69.50 76.30 77.00 80.65 79.70 80.10 79.55 81.40\u2217 79.75 80.",
            "00 81.65 82.65 82.35 82.75\u2217 E D 69.50 76.30 77.00 80.65 79.70 80.10 79.55 81.40\u2217 79.75 80.15 K D 71.40 76.05 79.35 78.90 80.45 79.35 76.90 81.65\u2217 82.15\u2217 81.40\u2217 B E 72.15 75.55 76.20 76.40 77.60 76.45 76.75 80.25\u2217 75.80 81.15\u2217 D E 71.65 76.00 76.60 77.55 79.70 80.20 79.25 81.40\u2217 80.05 81.55\u2217 K E 79.75 84.20 84.85 84.05 86.85 85.75 85.60 85.70 85.95 85.80 B K 73.50 75.",
            "40\u2217 80.05 81.55\u2217 K E 79.75 84.20 84.85 84.05 86.85 85.75 85.60 85.70 85.95 85.80 B K 73.50 75.95 77.40 78.10 76.10 75.20 77.55 81.55\u2217 79.45\u2217 82.25\u2217 D K 72.00 76.30 78.55 80.05 77.35 79.70 78.00 80.80 79.50 81.50\u2217 E K 82.80 84.45 84.95 84.15 83.95 81.75 83.85 84.50 83.80 84.85 Average 73.66 77.98 79.38 79.85 80.29 80.00 79.65 81.84 80.68 81.96 Table 3: Accuracies on the Amazon benchmark.",
            "50 83.80 84.85 Average 73.66 77.98 79.38 79.85 80.29 80.00 79.65 81.84 80.68 81.96 Table 3: Accuracies on the Amazon benchmark. Average results over 5 runs with random initializations are reported for each neural method. \u2217indicates that the proposed method (DAS-EM, DAS-SE, DAS) is signi\ufb01cantly better than other baselines with p < 0.05 based on one-tailed unpaired t-test. A Results on Amazon Benchmark Most previous works (Blitzer et al., 2007; Pan et al., 2010; Glorot et al., 2011; Chen et al., 2012; Zhou et al., 2016) carried out experiments on the Amazon benchmark released by Blitzer et al. (2007). The dataset contains 4 different domains: Book (B), DVDs (D), Electronics (E), and Kitchen (K).",
            "(2007). The dataset contains 4 different domains: Book (B), DVDs (D), Electronics (E), and Kitchen (K). Following their experimental set- tings, we consider the binary classi\ufb01cation task to predict whether a review is positive or negative on the target domain. Each domain consists of 1000 positive and 1000 negative reviews respec- tively. We also allow 4000 unlabeled reviews to be used for both the source and the target domains, of which the positive and negative reviews are bal- anced as well, following the settings in previous works. We construct 12 cross-domain sentiment classi\ufb01cation tasks and split the labeled data in each domain into a training set of 1600 reviews and a test set of 400 reviews. The classi\ufb01er is trained on the training set of the source domain and is evaluated on the test set of the target do- main. The comparison results are shown in Ta- ble 3. B Numerical Results of Figure 1 Due to space limitation, we only show results in \ufb01gures in the paper.",
            "The comparison results are shown in Ta- ble 3. B Numerical Results of Figure 1 Due to space limitation, we only show results in \ufb01gures in the paper. All numerical numbers used for plotting Figure 1 are presented in Table 4. We can observe that DAS-EM, DAS-SE, and DAS all achieve substantial improvements over baseline methods under different settings. C CNN Filter Analysis Full Results As mentioned in Section 4.8, we conduct CNN \ufb01l- ter analysis on NaiveNN, FANN, and DAS. For each method, we identify the top 10 most related \ufb01lters for positive, negative, neutral sentiment la- bels respectively, and then represent each selected \ufb01lter as a ranked list of trigrams with the highest activation values on it. Table 5, 6, 7 in the fol- lowing pages illustrate the trigrams from the tar- get domain (beauty) captured by the selected \ufb01l- ters learned on E\u2192BT for each method.",
            "Table 5, 6, 7 in the fol- lowing pages illustrate the trigrams from the tar- get domain (beauty) captured by the selected \ufb01l- ters learned on E\u2192BT for each method. We can observe that compared to NaiveNN and FANN, DAS is able to capture a more diverse set of relevant sentiment expressions on the target do- main for each sentiment label. This observation is consistent with our motivation. Since NaiveNN, FANN and other baseline methods solely train the sentiment classi\ufb01er on the source domain, the learned encoder is not able to produce discrimina- tive features on the target domain. DAS addresses this problem by re\ufb01ning the classi\ufb01er on the tar- get domain with semi-supervised learning, and the overall objective forces the encoder to learn feature representations that are not only domain- invariant but also discriminative on both domains.",
            "S T Naive mSDA NaiveNN AuxNN ADAN MMD FANN DAS-EM DAS-SE DAS E BK 49.07 55.13 58.26 60.62 63.32 60.38 59.59 66.48\u2217 62.37 67.12\u2217 BT BK 48.17 53.53 58.48 59.86 65.62 59.66 59.28 66.78\u2217 61.17 66.53 M BK 45.20 49.22 57.10 60.43 62.87 60.20 57.65 69.63\u2217 65.24\u2217 70.31\u2217 BK E 46.43 48.22 47.15 48.45 47.42 53.32 51.27 58.59\u2217 55.15\u2217 58.73\u2217 BT E 53.63 57.32 58.77 60.98 63.13 60.53 60.62 65.71\u2217 61.",
            "32 51.27 58.59\u2217 55.15\u2217 58.73\u2217 BT E 53.63 57.32 58.77 60.98 63.13 60.53 60.62 65.71\u2217 61.78 66.14\u2217 M E 37.93 38.13 47.28 49.60 46.57 51.55 47.23 55.88\u2217 53.22\u2217 55.78\u2217 BK BT 45.57 50.77 48.35 48.67 46.14 49.48 50.24 49.49 54.23\u2217 51.30\u2217 E BT 48.43 53.13 54.07 55.58 50.98 54.83 56.78 61.53\u2217 59.52\u2217 60.76\u2217 M BT 39.42 39.37 47.23 48.65 44.26 48.35 48.89 47.65 50.67\u2217 50.",
            "78 61.53\u2217 59.52\u2217 60.76\u2217 M BT 39.42 39.37 47.23 48.65 44.26 48.35 48.89 47.65 50.67\u2217 50.66\u2217 BK M 43.32 47.88 47.67 48.87 51.10 53.04 52.35 55.47\u2217 55.13\u2217 55.98\u2217 E M 41.83 47.88 50.21 51.19 50.23 51.81 52.14 58.28\u2217 55.60\u2217 59.06\u2217 BT M 43.55 49.62 50.27 53.11 55.35 54.43 53.84 60.95\u2217 56.90\u2217 60.5\u2217 Average 45.21 49.18 52.07 53.84 53.92 54.80 54.15 59.74 57.58 60.",
            "43 53.84 60.95\u2217 56.90\u2217 60.5\u2217 Average 45.21 49.18 52.07 53.84 53.92 54.80 54.15 59.74 57.58 60.24 (a) Accuracy on the small-scale dataset under setting 1 S T Naive mSDA NaiveNN AuxNN ADAN MMD FANN DAS-EM DAS-SE DAS E BK 49.07 52.88 58.26 57.72 57.07 57.43 56.43 57.78 58.93 55.20 BT BK 48.17 47.65 58.48 58.46 59.78 56.17 57.98 61.17\u2217 60.17\u2217 63.32\u2217 M BK 45.20 48.33 57.10 58.15 58.67 57.08 57.75 58.62 58.25 60.77\u2217 BK E 46.43 47.07 47.15 48.",
            "32\u2217 M BK 45.20 48.33 57.10 58.15 58.67 57.08 57.75 58.62 58.25 60.77\u2217 BK E 46.43 47.07 47.15 48.22 49.48 45.42 51.95 54.51\u2217 52.47\u2217 53.92\u2217 BT E 53.63 55.12 58.77 59.08 59.45 60.24 58.67 61.27 61.42 59.83 M E 37.93 37.40 47.28 49.43 47.00 48.72 48.92 51.28\u2217 51.18\u2217 52.88\u2217 BK BT 45.57 49.63 48.35 47.80 47.52 45.43 49.83 53.72\u2217 51.23\u2217 54.67\u2217 E BT 48.43 51.98 54.07 54.37 51.",
            "57 49.63 48.35 47.80 47.52 45.43 49.83 53.72\u2217 51.23\u2217 54.67\u2217 E BT 48.43 51.98 54.07 54.37 51.28 54.92 55.42 53.10 56.43\u2217 56.05\u2217 M BT 39.43 37.73 47.23 46.92 45.73 46.68 48.48 47.18 51.57\u2217 49.73\u2217 BK M 43.32 45.97 47.67 48.79 50.20 48.76 49.47 52.37\u2217 52.68\u2217 53.52\u2217 E M 41.83 45.12 50.21 52.31 52.57 51.50 48.18 53.63\u2217 52.25 55.38\u2217 BT M 43.55 45.78 50.27 53.55 54.68 54.",
            "12 50.21 52.31 52.57 51.50 48.18 53.63\u2217 52.25 55.38\u2217 BT M 43.55 45.78 50.27 53.55 54.68 54.55 53.41 56.24\u2217 56.23\u2217 56.02\u2217 Average 45.21 47.06 52.07 52.98 52.79 52.23 53.04 55.07 55.23 55.94 (b) Accuracy on the small-scale dataset under setting 2 S T NaiveNN ADAN MMD FANN DAS-EM DAS-SE DAS Y I 53.01 55.52 54.16 54.46 55.04 56.66\u2217 58.54\u2217 C I 51.84 55.07 53.35 53.07 57.27\u2217 55.18 57.28\u2217 B I 45.85 54.64 51.40 52.39 57.31\u2217 54.",
            "54\u2217 C I 51.84 55.07 53.35 53.07 57.27\u2217 55.18 57.28\u2217 B I 45.85 54.64 51.40 52.39 57.31\u2217 54.30 58.02\u2217 I Y 55.46 52.57 56.52 56.30 57.92\u2217 58.72\u2217 58.92\u2217 C Y 61.22 60.70 60.81 56.02 61.17 59.14 61.39 B Y 56.86 58.42 58.77 55.72 59.94\u2217 58.43 61.87\u2217 I C 50.38 47.27 50.49 51.04 53.46\u2217 51.97\u2217 53.38\u2217 Y C 53.87 52.53 53.12 51.86 53.48 54.67\u2217 55.44\u2217 B C 59.48 59.91 61.",
            "46\u2217 51.97\u2217 53.38\u2217 Y C 53.87 52.53 53.12 51.86 53.48 54.67\u2217 55.44\u2217 B C 59.48 59.91 61.23 60.19 59.84 59.98 59.76 I B 50.05 46.34 47.35 48.17 48.84 50.81 48.84 Y B 54.73 50.82 54.43 53.54 52.87 52.95 52.91 C B 60.47 59.99 60.52 55.56 57.74 58.12 59.75 Average 54.43 54.48 55.18 54.02 56.24 55.91 57.18 (c) Macro-F1 scores on the large-scale dataset Table 4: Performance comparison. Average results over 5 runs with random initializations are reported for each neural method.",
            "43 54.48 55.18 54.02 56.24 55.91 57.18 (c) Macro-F1 scores on the large-scale dataset Table 4: Performance comparison. Average results over 5 runs with random initializations are reported for each neural method. \u2217indicates that the proposed method (DAS, DAS-EM, DAS-SE) is signi\ufb01cantly better than other baselines with p < 0.05 based on one-tailed unpaired t-test.",
            "1 2 3 4 5 best-value-at highly-recommend-! nars-are-amazing beauty-store-suggested since-i-love good-value-at highly-advise-! ulta-are-fantastic durable-machine-and years-i-love perfect-product-for gogeous-absolutely-perfect length-are-so perfect-length-and bonus-i-love great-product-at love-love-love expected-in-perfect great-store-on appearance-i-love amazing-product-\u2217 highly-recommend-for setting-works-perfect beauty-store-for relaxing-i-love 6 7 8 9 10 store-and-am of\ufb01ce-setting-thanks car-washes-!",
            "speed-is-perfect !-i-recommend cleanser-and-am locks-shimmering-color price-in-stores buttons-are-perfect !-i-highly olay-and-am dirty-blonde-color products-are-priced unit-is-superb shower-i-slather daily-and-need victoria-secrets-gorgeous car-and-burning spray-is-perfect spots-i-needed shower-and-noticed dirty-pinkish-color from-our-store coverage-is-excellent best-i-use (a) NaiveNN 1 2 3 4 5 prices-my-favorite so-nicely-! purchase-thanks-! feel-wonderfully-clean are-really-cleaning brands-my-favorite more-affordable-price buy-again-! on-nicely-builds washing-and-cleaning very-great-stores shampoo-a-perfect without-hesitation-! polish-easy-and really-good-shampoo great-bottle-also an-excellent-value buy-this-! felt-cleanser-than deeply-cleans-my scent-pleasantly-\ufb02oral really-enjoy-it discount-too-!",
            "polish-easy-and really-good-shampoo great-bottle-also an-excellent-value buy-this-! felt-cleanser-than deeply-cleans-my scent-pleasantly-\ufb02oral really-enjoy-it discount-too-! honestly-perfect-it totally-moisturize-our 6 7 8 9 10 shower-or-cleaning de\ufb01nitely-purchase-again more-affordable-price absolutely-wonderful-! felt-cleaner-than water-onto-my de\ufb01nitely-buy-again a-perfect-length perfect-for-running \ufb02at-iron-through bleach-your-towels perfect-for-my an-exceptional-value concealer-for-my rubbed-grease-on pump-onto-my de\ufb01nitely-order-again \u2019ve-enjoyed-it moisturizing-for-my deeply-cleans-my water-great-for super-happy-to pretty-decent-layer super-glue-even being-cleaner-after (b) FANN 1 2 3 4 5 bath-\u2019s-wonderful love-fruity-sweet feeling-smooth-radiant cleans-thoroughly-* excellent-everyday-lotion all-pretty-affordable absorb-really-nicely",
            "super-glue-even being-cleaner-after (b) FANN 1 2 3 4 5 bath-\u2019s-wonderful love-fruity-sweet feeling-smooth-radiant cleans-thoroughly-* excellent-everyday-lotion all-pretty-affordable absorb-really-nicely love-lavender-scented loving-this-soap affordable-cleans-nicely it-delivers-fabulous shower-lather-wonderfully am-very-grateful bed-of-love fantastic-base-coat and-blends-nicely *-smells-fantastic love-fruity-fragrances shower-!-* nice-gentle-scrub heats-quickly-love and-clean-excellent perfect-beautiful-shimmer radiant-daily-moisturizer surprisingly-safe-on 6 7 8 9 10 shower-lather-wonderfully highly-recommend-! excellent-fragrance-and its-unique-smoothing forgeous-gragrance-mist affordable-cleans-nicely de\ufb01nitely-recommend-! fantastic-for-daytime smooth-luxurious-texture wonderful-bedtime-scent peels-great-price love-love-!",
            "excellent-fragrance-and its-unique-smoothing forgeous-gragrance-mist affordable-cleans-nicely de\ufb01nitely-recommend-! fantastic-for-daytime smooth-luxurious-texture wonderful-bedtime-scent peels-great-price love-love-! wonderfully-moisturizing-and \u2019s-extremely-gentle love-essie-polish daughter-loves-this highly-advise-! lathers-great-cleans \u2019s-affordable-combination perfect-beautiful-shimmer cleans-great-smells time-advise-! delightful-shampoo-works absorbs-quite-well fantastic-coverage-hydrates (c) DAS Table 5: Top 5 trigrams from the target domain (beauty) captured by the top 10 most positive-sentiment- related CNN \ufb01lters learned on E\u2192BT. \u2217denotes a padding.",
            "1 2 3 4 5 pads-ruined-my simply-threw-out hours-after-trying junk-drawer-\u2217 contacted-manufacturer-about highly-disappointed-and reviewer-pointed-out minutes-after-rinsing refund-time-! minutes-not-worth dryers-blew-my extracts-broke-into disappointed-after-trying total-fake-wen \u2019ve-owned-this completely-worthless-didn\u2019t actually-threw-out lips-after-trying waste-your-time hour-unless-it am-disappointed-and clips-barely-keep dry-after-shampooing total-fail-! results-they-claim 6 7 8 9 10 were-awful-garbage two-failed-attempts auto-ship-sent refund-and-dispose broke-don\u2019t-\ufb01x what-awful-garbage a-mistake-save am-returning-to refund-spend-your sent-me-expired and-utter-waste a-de\ufb01nite-return am-unable-to wouldn\u2019t-recommend-! wearing-false-eyelashes are-absolute-garbage a-pathetic-limp am-pale-ghost not-buy-dunhill a-temporary-\ufb01x piece-of-junk a-total-disappointment",
            "wearing-false-eyelashes are-absolute-garbage a-pathetic-limp am-pale-ghost not-buy-dunhill a-temporary-\ufb01x piece-of-junk a-total-disappointment got-returned-and not-worth-returning a-disappointment-cheap (a) NaiveNN 1 2 3 4 5 nasty-sunburn-lol the-worse-mascaras stale-very-unhappy actually-hurts-your a-return-label bother-returning-them it-caused-patchy were-horrible-failures didn\u2019t-bother-returning stay-away-completely fails-miserably-at lifeless-disaster-enter send-this-crap it-hurts-your like-bug-quit minutes-auric-needs it-fails-miserably were-awful-garbage didn\u2019t-exist-in a-defective-brown severely-burned-me feel-worse-leaving were-horribly-red skin-horribly-after \u2019d-refund-the 6 7 8 9 10 worse-with-exercise not-stink-your it-fails-miserably got-progressively-worse stopped-working-for worse-and-after mistake-save-your is-ineffective-apart gave-opposite-result",
            "were-horribly-red skin-horribly-after \u2019d-refund-the 6 7 8 9 10 worse-with-exercise not-stink-your it-fails-miserably got-progressively-worse stopped-working-for worse-and-after mistake-save-your is-ineffective-apart gave-opposite-result uncomfortable-i-returned unable-to-return nothing-!-by but-horribly-unhealthy another-epic-fail i-am-returning worse-my-face nothing-happened-! a-pathetic-limp got-horribly-painful stopped-working-shortly poorly-in-step nothing-save-your a-worse-job was-downright-painful not-waterproof-makeup (b) FANN 1 2 3 4 5 poorly-designed-product a-refund-spend completely-waste-of smells-disgusting-!",
            "burning-rubber-stench defective-dryer-promising a-refund-save of-junk-* smells-horribly-like began-smelling-vomit disgusting-smelling-thing i-regret-spending were-awful-garbage does-not-straighten reaction-and-wasted hurts-your-scalp just-wouldn\u2019t-spend worthless-waste-of \u2019s-false-advertising control-and-smelled hurts-your-hair looked-washed-out throwing-money-away a-disgusting-cheap using-this-disgusting 6 7 8 9 10 super-irritating-!",
            "got-promptly-broke sore-and-painful it-caused-patchy painful-it-hurt strong-reaction-and after-ive-washed is-simply-irritating layer-hydrogenated-alcohols unnecessary-health-risks really-burned-and after-several-attempts tight-and-uncomfortable the-harmful-uva uncomfortable-to-wear very-pasty-and this-stuff-stinks drying-and-irritating my-severe-dark stinging-your-eyes super-streaky-and again-i-threw goopy-and-unpleasant a-allergic-reaction unbearable-to-wear (c) DAS Table 6: Top 5 trigrams from the target domain (beauty) captured by the top 10 most negative-sentiment- related CNN \ufb01lters learned on E\u2192BT. \u2217denotes a padding.",
            "1 2 3 4 5 purpose-cologne-splash okay-cord-was hands-feet-elbows aggressive-in-general but-its-okay other-hanae-mori cocamide-dea-is been-sealed-tight pimples-in-general it-moisturizes-okay the-mavala-peeled coily-conditioner-is stainless-steel-blackhead biotin-in-general but-moisturizes-keeps avoid-hair-pulling \ufb02aky-dandruff-is severely-tight-chest dimethicone-is-terrible but-don\u2019t-expect cause-rashes-stinging quickly-cord-is thick-nasty-callouses but-in-general it-lathers-ok 6 7 8 9 10 pretty-damaged-from darker-olive-complexion doesn\u2019t-mind-pushing producto-por-los feeling-didn\u2019t-last daughter-suffers-from stronger-healthier-or kinda-doesn\u2019t-its unstuck-frownies-\u2217 curls-didn\u2019t-last teenager-suffers-from natural-ingredient-however kinda-kinky-coily they-push-\u2217 extra-uv-protection tissue-damage-during vitamin-enriched-color okay-job-of uva-rays-uva",
            "kinda-doesn\u2019t-its unstuck-frownies-\u2217 curls-didn\u2019t-last teenager-suffers-from natural-ingredient-however kinda-kinky-coily they-push-\u2217 extra-uv-protection tissue-damage-during vitamin-enriched-color okay-job-of uva-rays-uva garnier-fructis-curl the-damage-on natural-ingredients-\u2217 intended-purpose-that tend-to-slip the-mavala-after (a) NaiveNN 1 2 3 4 5 worse-and-after maybe-a-refund very-disappointing-waste my-ears-are pretty-neutral-neither worse-before-improving ok-mask-i ok-but-clean my-neck-line ok-so-if unable-to-return ok-pining-it ok-but-will cause-unsightly-beads ok-during-pregnancy unless-your-entire ok-try-i ok-but-didn\u2019t my-sporadic-line kinda-annoying-if horrible-in-execution ok-tho-i ok-nothing-special your-ear-is ok-this-seems 6 7 8 9 10 uncomfortable-i-returned sticky-lathers-and some-fading-when are-very-painful its-also-convenient weak-they-bend",
            "horrible-in-execution ok-tho-i ok-nothing-special your-ear-is ok-this-seems 6 7 8 9 10 uncomfortable-i-returned sticky-lathers-and some-fading-when are-very-painful its-also-convenient weak-they-bend quickly-deep-cleans real-disappointment-the are-less-painful that-also-my claimed-faulty-\u2217 but-elegant-bottle especially-noticeable-after are-a-pain that-may-make suffers-from-wind beat-the-price progressively-worse-during about-sum-damage that-allows-your as-defective-\u2217 and-reasonably-priced style-unfortunately-the offered-no-pain its-helpful-to (b) FANN 1 2 3 4 5 \u2019m-kinda-pale darker-but-nope ok-but-horrible noticeable-i-avoid same-result-mediocre a-terrible-headache gray-didn\u2019t-cover ok-but-didn\u2019t however-i-lean it-caused-patchy but-kinda-annoying makeup-doesn\u2019t-sweat okay-but-doesn\u2019t but-otherwise-ok doesn\u2019t-cause-\ufb02are \u2019m-kinda-mad dark-spots-around",
            "ok-but-didn\u2019t however-i-lean it-caused-patchy but-kinda-annoying makeup-doesn\u2019t-sweat okay-but-doesn\u2019t but-otherwise-ok doesn\u2019t-cause-\ufb02are \u2019m-kinda-mad dark-spots-around okay-however-it but-im-deciding the-harmful-uva i-kinda-stopped moist-but-thats unfortunately-straight however-i-prefer rather-unpleasant-smell 6 7 8 9 10 kinda-annoying-if brutal-winter-however higher-rating-because nothing-for-odor but-darker-* pretty-bad-breakage summer-color-however slight-burnt-rubber kinda-recommend-this slightly-darker-shade my-slight-discoloration beige-shade-however noticeable-tan-since not-recommend-if somewhat-pale-affect smells-kinda-bad is-okay-however somewhat-pale-affect noticeable-but-non but-somewhat-heavy look-kinda-crappy bit-greasy-however kinda-pale-so nothing-special-moderate bit-dull-heavy (c) DAS Table 7: Top 5 trigrams from the target domain (beauty) captured by the top 10 most",
            "noticeable-but-non but-somewhat-heavy look-kinda-crappy bit-greasy-however kinda-pale-so nothing-special-moderate bit-dull-heavy (c) DAS Table 7: Top 5 trigrams from the target domain (beauty) captured by the top 10 most neutral-sentiment- related CNN \ufb01lters learned on E\u2192BT. \u2217denotes a padding."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1809.00530.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 15092.00015258789,
    "avg_doclen_est": 162.27957153320312
}
