{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Cross-Lingual Machine Reading Comprehension Yiming Cui\u2020\u2021, Wanxiang Che\u2020, Ting Liu\u2020, Bing Qin\u2020, Shijin Wang\u2021\u00a7, Guoping Hu\u2021 \u2020Research Center for Social Computing and Information Retrieval (SCIR), Harbin Institute of Technology, Harbin, China \u2021State Key Laboratory of Cognitive Intelligence, iFLYTEK Research, China \u00a7iFLYTEK AI Research (Hebei), Langfang, China \u2020{ymcui,car,tliu,qinb}@ir.hit.edu.cn \u2021\u00a7{ymcui,sjwang3,gphu}@iflytek.com Abstract Though the community has made great progress on Machine Reading Comprehension (MRC) task, most of the previous works are solving English-based MRC problems, and there are few efforts on other languages mainly due to the lack of large-scale training data. In this paper, we propose Cross-Lingual Machine Reading Comprehension (CLMRC) task for the languages other than English. Firstly, we present several back-translation approaches for CLMRC task, which is straightforward to adopt.",
      "In this paper, we propose Cross-Lingual Machine Reading Comprehension (CLMRC) task for the languages other than English. Firstly, we present several back-translation approaches for CLMRC task, which is straightforward to adopt. However, to accurately align the answer into another language is dif\ufb01cult and could introduce additional noise. In this context, we propose a novel model called Dual BERT, which takes advantage of the large-scale train- ing data provided by rich-resource language (such as English) and learn the semantic re- lations between the passage and question in a bilingual context, and then utilize the learned knowledge to improve reading comprehension performance of low-resource language. We conduct experiments on two Chinese machine reading comprehension datasets CMRC 2018 and DRCD. The results show consistent and signi\ufb01cant improvements over various state- of-the-art systems by a large margin, which demonstrate the potentials in CLMRC task. 1 1 Introduction Machine Reading Comprehension (MRC) has been a popular task to test the reading ability of the machine, which requires to read text material and answer the questions based on it.",
      "1 1 Introduction Machine Reading Comprehension (MRC) has been a popular task to test the reading ability of the machine, which requires to read text material and answer the questions based on it. Starting from cloze-style reading comprehension, various neural network approaches have been proposed and mas- sive progresses have been made in creating large- scale datasets and neural models (Hermann et al., 2015; Hill et al., 2015; Kadlec et al., 2016; Cui 1Resources available: https://github.com/ ymcui/Cross-Lingual-MRC. et al., 2017; Rajpurkar et al., 2016; Dhingra et al., 2017). Though various types of contributions had been made, most works are dealing with English reading comprehension. Reading comprehension in other than English has not been well-addressed mainly due to the lack of large-scale training data. To enrich the training data, there are two tradi- tional approaches. Firstly, we can annotate data by human experts, which is ideal and high-quality, while it is time-consuming and rather expensive.",
      "To enrich the training data, there are two tradi- tional approaches. Firstly, we can annotate data by human experts, which is ideal and high-quality, while it is time-consuming and rather expensive. One can also obtain large-scale automatically gen- erated data (Hermann et al., 2015; Hill et al., 2015; Liu et al., 2017), but the quality is far beyond the usable threshold. Another way is to exploit cross-lingual approaches to utilize the data in rich- resource language to implicitly learn the relations between <passage, question, answer>. In this paper, we propose the Cross-Lingual Machine Reading Comprehension (CLMRC) task that aims to help reading comprehension in low- resource languages. First, we present several back-translation approaches when there is no or partially available resources in the target language. Then we propose a novel model called Dual BERT to further improve the system performance when there is training data available in the target lan- guage. We \ufb01rst translate target language train- ing data into English to form pseudo bilingual parallel data.",
      "Then we propose a novel model called Dual BERT to further improve the system performance when there is training data available in the target lan- guage. We \ufb01rst translate target language train- ing data into English to form pseudo bilingual parallel data. Then we use multilingual BERT (Devlin et al., 2019) to simultaneously model the <passage, question, answer> in both languages, and fuse the representations of both to generate \ufb01- nal predictions. Experimental results on two Chi- nese reading comprehension dataset CMRC 2018 (Cui et al., 2019) and DRCD (Shao et al., 2018) show that by utilizing English resources could substantially improve system performance and the proposed Dual BERT achieves state-of-the-art per- formances on both datasets, and even surpass hu- man performance on some metrics. Also, we arXiv:1909.00361v1  [cs.CL]  1 Sep 2019",
      "conduct experiments on the Japanese and French SQuAD (Asai et al., 2018) and achieves substan- tial improvements. Moreover, detailed ablations and analysis are carried out to demonstrate the effectiveness of exploiting knowledge from rich- resource language. To best of our knowledge, this is the \ufb01rst time that the cross-lingual approaches applied and evaluated on realistic reading compre- hension data. The main contributions of our paper can be concluded as follows. \u2022 We present several back-translation based read- ing comprehension approaches and yield state- of-the-art performances on several reading comprehension datasets, including Chinese, Japanese, and French. \u2022 We propose a model called Dual BERT to si- multaneously model the <passage, question> in both source and target language to enrich the text representations. \u2022 Experimental results on two public Chinese reading comprehension datasets show that the proposed cross-lingual approaches yield signif- icant improvements over various baseline sys- tems and set new state-of-the-art performances. 2 Related Works Machine Reading Comprehension (MRC) has been a trending research topic in recent years.",
      "2 Related Works Machine Reading Comprehension (MRC) has been a trending research topic in recent years. Among various types of MRC tasks, span- extraction reading comprehension has been enor- mously popular (such as SQuAD (Rajpurkar et al., 2016)), and we have seen a great progress on re- lated neural network approaches (Wang and Jiang, 2016; Seo et al., 2016; Xiong et al., 2016; Cui et al., 2017; Hu et al., 2019), especially those were built on pre-trained language models, such as BERT (Devlin et al., 2019). While massive achievements have been made by the community, reading comprehension in other than English has not been well-studied mainly due to the lack of large-scale training data. Asai et al. (2018) proposed to use runtime ma- chine translation for multilingual extractive read- ing comprehension. They \ufb01rst translate the data from the target language to English and then ob- tain an answer using an English reading com- prehension model.",
      "Asai et al. (2018) proposed to use runtime ma- chine translation for multilingual extractive read- ing comprehension. They \ufb01rst translate the data from the target language to English and then ob- tain an answer using an English reading com- prehension model. Finally, they recover the cor- responding answer in the original language us- ing soft-alignment attention scores from the NMT model. However, though an interesting attempt has been made, the zero-shot results are quite low, and alignments between different languages, es- pecially for those have different word orders, are signi\ufb01cantly different. Also, they only evaluate on a rather small dataset (hundreds of samples) that was translated from SQuAD (Rajpurkar et al., 2016), which is not that realistic. To solve the issues above and better exploit large-scale rich-resourced reading comprehension data, in this paper, we propose several zero- shot approaches which yield state-of-the-art per- formances on Japanese and French SQuAD data. Moreover, we also propose a supervised approach for the condition that there are training samples available for the target language.",
      "Moreover, we also propose a supervised approach for the condition that there are training samples available for the target language. To evaluate the effectiveness of our approach, we carried out ex- periments on two realistic public Chinese read- ing comprehension data: CMRC 2018 (simpli\ufb01ed Chinese) (Cui et al., 2019) and DRCD (traditional Chinese) (Shao et al., 2018). Experimental results demonstrate the effectiveness by modeling train- ing samples in a bilingual environment. 3 Back-Translation Approaches In this section, we illustrate back-translation ap- proaches for cross-lingual machine reading com- prehension, which is natural and easy to imple- ment. Before introducing these approaches in de- tail, we will clarify crucial terminologies in this paper for better understanding. \u2022 Source Language: Rich-resourced and has suf- \ufb01cient large-scale training data that we aim to extract knowledge from. We use subscript S for variables in the source language. \u2022 Target Language: Low-resourced and has only a few training data that we wish to optimize on. We use subscript T for variables in the target language.",
      "We use subscript S for variables in the source language. \u2022 Target Language: Low-resourced and has only a few training data that we wish to optimize on. We use subscript T for variables in the target language. In this paper, we aim to improve the machine reading comprehension performance in Chinese (target language) by introducing English (source language) resources. The general idea of back- translation approaches is to translate <passage, question> pair into the source language and gen- erate an answer using a reading comprehension system in the source language. Finally, the generated answer is back-translated into the tar- get language. In the following subsections, we will introduce several back-translation approaches",
      "Target Input [CLS] Qt1 Qtn[SEP] Pt1 Ptm[SEP] Q P Source Input Source BERT [CLS] Qs1 Qsn[SEP] Ps1 Psm[SEP] Q P [CLS] Q1 \u2026 Qn [SEP] Ps1 \u2026\u2026 Psi \u2026\u2026 Psj \u2026.. Psm [SEP] Extracted Source Span SRC\u2192TRG GNMT Translated Target Span TRG\u2192SRC GNMT Target Input Target BERT Translated Target Span [CLS] Ct1 \u2026 Ctn [SEP] Pt1 \u2026 Pti \u2026 Ptj ... Ptm [SEP] Extracted Target Span [CLS] Ct1 Ctk [SEP] Pt1 Ptm [SEP] C P Target Input Target BERT Translated Target Span [CLS] C \u266fQ [SEP] Pt1 \u2026 Pti \u2026 Ptj ... Ptm [SEP] Extracted Target Span [CLS] Ct1 Ctk \u266fQt1 Qtn [SEP] Pt1 Ptm [SEP] C Q P Figure 1: Back-translation approaches for cross-lingual machine reading comprehension (Left: GNMT, Middle: Answer Aligner,",
      ". Ptm [SEP] Extracted Target Span [CLS] Ct1 Ctk \u266fQt1 Qtn [SEP] Pt1 Ptm [SEP] C Q P Figure 1: Back-translation approaches for cross-lingual machine reading comprehension (Left: GNMT, Middle: Answer Aligner, Right: Answer Veri\ufb01er) for cross-lingual machine reading comprehension task. The architectures of the proposed back- translation approaches are depicted in Figure 1. 3.1 GNMT To build a simple cross-lingual machine reading comprehension system, it is straightforward to uti- lize translation system to bridge source and target language (Asai et al., 2018). Brie\ufb02y, we \ufb01rst trans- late the target sample to the source language. Then we use a source reading comprehension system, such as BERT (Devlin et al., 2019), to generate an answer in the source language. Finally, we use back-translation to get the answer in the target lan- guage. As we do not exploit any training data in the target language, we could regard this approach as a zero-shot cross-lingual baseline system.",
      "Finally, we use back-translation to get the answer in the target lan- guage. As we do not exploit any training data in the target language, we could regard this approach as a zero-shot cross-lingual baseline system. Speci\ufb01cally, we use Google Neural Machine Translation (GNMT) system for source-to-target and target-to-source translations. One may also use advanced and domain-speci\ufb01c neural machine translation system to achieve better translation performance, while we leave it for individuals, and this is beyond the scope of this paper. However, for span-extraction reading compre- hension task, a major drawback of this approach is that the translated answer may not be the exact span in the target passage. To remedy this, we pro- pose three simple approaches to improve the qual- ity of the translated answer in the target language. 3.2 Simple Match We propose a simple approach to align the trans- lated answer into extract span in the target pas- sage. We calculate character-level text overlap (for Chinese) between translated answer Atrans and arbitrary sliding window in target passage PT[i:j].",
      "3.2 Simple Match We propose a simple approach to align the trans- lated answer into extract span in the target pas- sage. We calculate character-level text overlap (for Chinese) between translated answer Atrans and arbitrary sliding window in target passage PT[i:j]. The length of sliding window ranges len(Atrans) \u00b1 \u03b4, with a relax parameter \u03b4. Typ- ically, the relax parameter \u03b4 \u2208[0, 5] as the length between ground truth and translated answer does not differ much in length. In this way, we would calculate character-level F1-score of each candi- date span PT[i:j] and translated answer Atrans, and we could choose the best matching one accord- ingly. Using the proposed SimpleMatch could en- sure the predicted answer is an exact span in tar- get passage. As SimpleMatch does not use tar- get training data either, it could also be a pipeline component in zero-shot settings. 3.3 Answer Aligner Though we could use unsupervised approaches for aligning answer, such as the proposed Sim- pleMatch, it stops at token-level and lacks seman- tic awareness between the translated answer and ground truth answer.",
      "3.3 Answer Aligner Though we could use unsupervised approaches for aligning answer, such as the proposed Sim- pleMatch, it stops at token-level and lacks seman- tic awareness between the translated answer and ground truth answer. In this paper, we also pro- pose two supervised approaches for further im- proving the answer span when there is training data available in the target language. The \ufb01rst one is Answer Aligner, where we feed",
      "translated answer Atrans and target passage PT into the BERT and outputs the ground truth an- swer span AT . The model will learn the seman- tic relations between them and generate improved span for the target language. 3.4 Answer Veri\ufb01er In Answer Aligner, we did not exploit question information in target training data. One can also utilize question information to transform Answer Aligner into Answer Veri\ufb01er, as we use complete \u27e8PT , QT , AT \u27e9in the target language and addi- tional translated answer Atrans to verify its cor- rectness and generate improved span. 4 Dual BERT One disadvantage of the back-translation ap- proaches is that we have to recover the source an- swer into the target language. To remedy the is- sue, in this paper, we propose a novel model called Dual BERT to simultaneously model the training data in both source and target language to better exploit the relations among <passage, question, answer>. The model could be used when there is training data available for the target language, and we could better utilize source language data to enhance the target reading comprehension system.",
      "The model could be used when there is training data available for the target language, and we could better utilize source language data to enhance the target reading comprehension system. The overall neural architecture for Dual BERT is shown in Figure 2. 4.1 Dual Encoder Bidirectional Encoder Representation from Trans- formers (BERT) has shown marvelous perfor- mance in various NLP tasks, which substantially outperforms non-pretrained models by a large margin (Devlin et al., 2019). In this paper, we use multi-lingual BERT for better encoding the text in both source and target language. Formally, given target passage PT and question QT , we organize the input XT for BERT as follows. [CLS] QT [SEP] PT [SEP] Similarly, we can also obtain source training sam- ple by translating target sample with GNMT, forming input XS for BERT.",
      "Formally, given target passage PT and question QT , we organize the input XT for BERT as follows. [CLS] QT [SEP] PT [SEP] Similarly, we can also obtain source training sam- ple by translating target sample with GNMT, forming input XS for BERT. Then we use XT and XS to obtain deep contextualized representa- tions through a shared multi-lingual BERT, form- ing BT \u2208RLT \u2217h, BS \u2208RLS\u2217h, where L repre- sents the length of input and h is the hidden size (768 for multi-lingual BERT). 4.2 Bilingual Decoder Typically, in the reading comprehension task, at- tention mechanism is used to measure the relations between the passage and question. Moreover, as Transformers are fundamental components of BERT, multi-head self-attention layer (Vaswani et al., 2017) is used to extract useful information within the input sequence. Speci\ufb01cally, in our model, to enhance the target representation, we use a multi-head self-attention layer to extract useful information in source BERT representation BS.",
      "Speci\ufb01cally, in our model, to enhance the target representation, we use a multi-head self-attention layer to extract useful information in source BERT representation BS. We aim to generate target span by not only relying on target representation but also on source representation to simultane- ously consider the <passage, question> relations in both languages, which can be seen as a bilingual decoding process. Brie\ufb02y, we regard target BERT representation BT as query and source BERT representation BS as key and value in multi-head attention mecha- nism. In original multi-head attention, we calcu- late a raw dot attention as follows. 2 This will result in an attention matrix ATS that indicate raw relations between each source and target token. ATS = BT \u00b7 B\u22a4 S , ATS \u2208RLT \u2217LS (1) To combine the bene\ufb01t of both inter-attention and self-attention, instead of using Equation 1, we propose a simple modi\ufb01cation on multi- head attention mechanism, which is called Self- Adaptive Attention (SAA).",
      "First, we calculate self-attention of BT and BS and apply the softmax function, as shown in Equation 2 and 3. This is de- signed to use self-attention to \ufb01lter the irrelevant part within each representation \ufb01rstly, and inform the raw dot attention on paying more attention to the self-attended part, making the attention more precise and accurate. AT = softmax(BT \u00b7 B\u22a4 T ) (2) AS = softmax(BS \u00b7 B\u22a4 S ) (3) Then we use self-attention AT and AS, inter- attention ATS to get self-attentive attention \u02dcATS. We calculate dot product between AST and BS to obtain attended representation R\u2032 \u2208RLT \u2217h. \u02dcATS = AT \u00b7 ATS \u00b7 AS\u22a4, \u02dcATS \u2208RLT \u2217LS (4) R\u2032 = softmax( \u02dcATS) \u00b7 BS (5) 2We omit rather extensive formulations of representation transformations and kindly advise the readers refer to the at- tention implementation in BERT: https://github.com/google- research/bert/blob/master/modeling.py#L558",
      "Source Input Target Input Source BERT Target BERT TRG\u2192SRC GNMT Bilingual Decoder [CLS] Qt1 \u2026 Qtn [SEP] Pt1 \u2026\u2026 Pti \u2026\u2026 Ptj \u2026.. Ptm [SEP] [CLS] Q t1 Q tn [SEP] Pt1 Ptm [SEP] Qt Pt [CLS] Q s1 Q su [SEP] Ps1 Psv [SEP] Qs Ps [CLS] Qs1 \u2026 Qsu [SEP] Ps1 \u2026\u2026 Psh \u2026\u2026 Psk \u2026.. Psv [SEP] Extracted Source Span Extracted Target Span Figure 2: System overview of the Dual BERT model for cross-lingual machine reading comprehension task. After obtaining attended representation R\u2032, we use an additional fully connected layer with resid- ual layer normalization which is similar to BERT implementation. R = WrR\u2032 + br, Wr \u2208Rh\u2217h (6) HT = concat[BT , LayerNorm(BT + R)] (7) Finally, we calculate weighted sum of HT to get \ufb01nal span prediction P s T , P e T (superscript s for start, e for end).",
      "For example, the start position P s T is calculated by the following equation. P s T = softmax(W \u22a4 T HT + b), WT \u2208R2h (8) We calculate standard cross entropy loss for the start and end predictions in the target language. LT = \u22121 N N X i=1 (ys T log(P s T ) + ye T log(P e T )) (9) 4.3 Auxiliary Output In order to evaluate how translated sample behaves in the source language system, we also generate span prediction for source language using BERT representation BS directly without further calcu- lation, resulting in the start and target prediction P s S, P e S (similar to Equation 8). Moreover, we also calculate cross-entropy loss Laux for trans- lated sample (similar to Equation 9), where a \u03bb parameter is applied to this loss. Instead of setting \u03bb with heuristic value, in this paper, we propose a novel approach to better ad- just \u03bb automatically. As the sample was generated by the machine translation system, there would be information loss during the translation process. Wrong or partially translated samples may harm the performance of reading comprehension sys- tem.",
      "As the sample was generated by the machine translation system, there would be information loss during the translation process. Wrong or partially translated samples may harm the performance of reading comprehension sys- tem. To measure how the translated samples as- semble the real target samples, we calculate co- sine similarity between the ground truth span rep- resentation in source and target language (denoted as \u02dcHS and \u02dcHT ). When the ground truth span rep- resentation in the translated sample is similar to the real target samples, the \u03bb increase; otherwise, we only use target span loss as \u03bb may decrease to zero. The span representation is the concatenation of three parts: BERT representation of ground truth start Bs \u2208Rh, ground truth end Be \u2208Rh, and self-attended span Batt \u2208Rh, which considers both boundary information (start/end) and mixed representation of the whole ground truth span. We use BERT representation B3 to get a self-attended span representation Batt using a simple dot prod- uct with average pooling, to get a 2D-tensor.",
      "We use BERT representation B3 to get a self-attended span representation Batt using a simple dot prod- uct with average pooling, to get a 2D-tensor. \u02dcHS = concat[Bs S, Be S, Batt S ] (10) \u02dcHT = concat[Bs T , Be T , Batt T ] (11) \u03bb = max{0, cos < \u02dcHS, \u02dcHT >} (12) The overall loss for Dual BERT is composed by two parts: target span loss LT and auxiliary span loss in source language Laux. L = LT + \u03bbLaux (13) 3We mask out the values that out of span.",
      "5 Experiments 5.1 Experimental Setups We evaluate our approaches on two public Chi- nese span-extraction machine reading comprehen- sion datasets: CMRC 2018 (simpli\ufb01ed Chinese) (Cui et al., 2019)4 and DRCD (traditional Chi- nese) (Shao et al., 2018)5. The statistics of the two datasets are listed in Table 1. Train Dev Test Challenge CMRC 2018 Question # 10,321 3,219 4,895 504 Answer # 1 3 3 3 DRCD Question # 26,936 3,524 3,493 - Answer # 1 2 2 - Table 1: Statistics of CMRC 2018 and DRCD. Note that, since the test and challenge sets are preserved by CMRC 2018 of\ufb01cial to ensure the in- tegrity of the evaluation process, we submitted our best-performing systems to the organizers to get these scores. The resource in source language was chosen as SQuAD (Rajpurkar et al., 2016) training data.",
      "The resource in source language was chosen as SQuAD (Rajpurkar et al., 2016) training data. The settings of the proposed approaches are listed below in detail. \u2022 Tokenization: Following the of\ufb01cial BERT im- plementation, we use WordPiece tokenizer (Wu et al., 2016) for English and character-level tok- enizer for Chinese. \u2022 BERT: We use pre-trained English BERT on SQuAD 1.1 (Rajpurkar et al., 2016) for ini- tialization, denoted as SQ-Ben (base) and SQ- Len (large) for back-translation approaches.",
      "\u2022 BERT: We use pre-trained English BERT on SQuAD 1.1 (Rajpurkar et al., 2016) for ini- tialization, denoted as SQ-Ben (base) and SQ- Len (large) for back-translation approaches. For other conditions, we use multi-lingual BERT as default, denoted as Bmul (and SQ-Bmul for those were pre-trained on SQuAD).6 \u2022 Translation: We use Google Neural Ma- chine Translation (GNMT) system for transla- tion.7 We evaluated GNMT system on NIST MT02/03/04/05/06/08 Chinese-English set and achieved an average BLEU score of 43.24, com- pared to previous best work (43.20) (Cheng et al., 2018), yielding state-of-the-art perfor- mance.",
      "4https://github.com/ymcui/cmrc2018/ 5https://github.com/DRCSolutionService/DRCD/ 6https://github.com/google-research/bert 7https://cloud.google.com/translate/ \u2022 Optimization: Following original BERT im- plementation, we use ADAM with weight decay optimizer (Kingma and Ba, 2014) using an ini- tial learning rate of 4e-5 and use cosine learning rate decay scheme instead of the original linear decay, which we found it bene\ufb01cial for stabiliz- ing results. The training batch size is set to 64, and each model is trained for 2 epochs, which roughly takes 1 hour. \u2022 Implementation: We modi\ufb01ed the TensorFlow (Abadi et al., 2016) version run squad.py provided by BERT. All models are trained on Cloud TPU v2 that has 64GB HBM. 5.2 Overall Results The overall results are shown in Table 2. As we can see that, without using any alignment approach, the zero-shot results are quite lower regardless of using English BERT-base (#1) or BERT-large (#2).",
      "5.2 Overall Results The overall results are shown in Table 2. As we can see that, without using any alignment approach, the zero-shot results are quite lower regardless of using English BERT-base (#1) or BERT-large (#2). When we apply Sim- pleMatch (#3), we observe signi\ufb01cant improve- ments demonstrating its effectiveness. The An- swer Aligner (#4) could further improve the per- formance beyond SimpleMatch approach, demon- strating that the machine learning approach could dynamically adjust the span output by learning the semantic relationship between translated answer and target passage. Also, the Answer Veri\ufb01er (#5) could further boost performance and surpass the multi-lingual BERT baseline (#7) that only use tar- get training data, demonstrating that it is bene\ufb01cial to adopt rich-resourced language to improve ma- chine reading comprehension in other languages.",
      "When we do not use SQuAD pre-trained weights, the proposed Dual BERT (#8) yields sig- ni\ufb01cant improvements (all results are veri\ufb01ed by p-test with p < 0.05) over both Chinese BERT (#6) and multi-lingual BERT (#7) by a large mar- gin. If we only train the BERT with SQuAD (#9), which is a zero-shot system, we can see that it achieves decent performance on two Chi- nese reading comprehension data. Moreover, we can also pursue further improvements by con- tinue training (#10) with Chinese data starting from the system #9, or mixing Chinese data with SQuAD and training from initial multi-lingual BERT (#11). Under powerful SQuAD pre-trained baselines, Dual BERT (#12) still gives moder- ate and consistent improvements over Cascade Training (#10) and Mixed Training (#11) baselines",
      "# System CMRC 2018 DRCD Dev Test Challenge Dev Test EM F1 EM F1 EM F1 EM F1 EM F1 Human Performance 91.1 97.3 92.4 97.9 90.4 95.2 - - 80.4 93.3 P-Reader (single model)\u2020 59.9 81.5 65.2 84.4 15.1 39.6 - - - - Z-Reader (single model)\u2020 79.8 92.7 74.2 88.1 13.9 37.4 - - - - MCA-Reader (ensemble)\u2020 66.7 85.5 71.2 88.1 15.5 37.1 - - - - RCEN (ensemble)\u2020 76.3 91.4 68.7 85.8 15.3 34.5 - - - - r-net (single model)\u2020 - - - - - - - - 29.1 44.4 DA (Yang et al., 2019) 49.2 65.4 - - - - 55.",
      "8 15.3 34.5 - - - - r-net (single model)\u2020 - - - - - - - - 29.1 44.4 DA (Yang et al., 2019) 49.2 65.4 - - - - 55.4 67.7 - - 1 GNMT+BERTSQ\u2212Ben \u2660 15.9 40.3 20.8 45.4 4.2 20.2 28.1 50.0 26.6 48.9 2 GNMT+BERTSQ\u2212Len \u2660 16.8 42.1 21.7 47.3 5.2 22.0 28.9 52.0 28.7 52.1 3 GNMT+BERTSQ\u2212Len+SimpleMatch\u266026.7 56.9 31.3 61.6 9.1 35.5 36.9 60.6 37.0 61.2 4 GNMT+BERTSQ\u2212Len+Aligner 46.1 66.4 49.8 69.3 16.",
      "9 31.3 61.6 9.1 35.5 36.9 60.6 37.0 61.2 4 GNMT+BERTSQ\u2212Len+Aligner 46.1 66.4 49.8 69.3 16.5 40.9 60.1 70.5 59.5 70.7 5 GNMT+BERTSQ\u2212Len+Veri\ufb01er 64.7 84.7 68.9 86.8 20.0 45.6 83.5 90.1 82.6 89.6 6 BERTBcn 63.6 83.9 67.8 86.0 18.4 42.1 83.4 90.1 81.9 89.0 7 BERTBmul 64.1 84.4 68.6 86.8 18.6 43.8 83.2 89.9 82.4 89.5 8 Dual BERT 65.8 86.3 70.4 88.1 23.",
      "1 84.4 68.6 86.8 18.6 43.8 83.2 89.9 82.4 89.5 8 Dual BERT 65.8 86.3 70.4 88.1 23.8 47.9 84.5 90.8 83.7 90.3 9 BERTSQ\u2212Bmul \u2660 56.5 77.5 59.7 79.9 18.6 41.4 66.7 81.0 65.4 80.1 10 BERTSQ\u2212Bmul + Cascade Training 66.6 87.3 71.8 89.4 25.6 52.3 85.2 91.4 84.4 90.8 11 BERTBmul + Mixed Training 66.8 87.5 72.6 89.8 26.7 53.4 85.3 91.6 84.7 91.2 12 Dual BERT (w/ SQuAD) 68.0 88.1 73.",
      "8 87.5 72.6 89.8 26.7 53.4 85.3 91.6 84.7 91.2 12 Dual BERT (w/ SQuAD) 68.0 88.1 73.6 90.2 27.8 55.2 86.0 92.1 85.4 91.6 Table 2: Experimental results on CMRC 2018 and DRCD. \u2020 indicates unpublished works (some of the systems are using development set for training, which makes the results not directly comparable.). \u2660indicates zero-shot approach. We mark our system with an ID in the \ufb01rst column for reference simplicity. and set new state-of-the-art performances on both datasets, demonstrating the effectiveness of using machine-translated sample to enhance the Chinese reading comprehension performance. 5.3 Results on Japanese and French SQuAD In this paper, we propose a simple but effective approach called SimpleMatch to align translated answer to original passage span.",
      "5.3 Results on Japanese and French SQuAD In this paper, we propose a simple but effective approach called SimpleMatch to align translated answer to original passage span. While one may argue that using neural machine translation atten- tion to project source answer to original target pas- sage span is ideal as used in Asai et al. (2018). However, to extract attention value in neural ma- chine translation system and apply it to extract the original passage span is bothersome and compu- tationally ineffective. To demonstrate the effec- tiveness of using SimpleMatch instead of using NMT attention to extract original passage span in zero-shot condition, we applied SimpleMatch to Japanese and French SQuAD (304 samples for each) which is what exactly used in Asai et al. (2018). The results are listed in Table 3.",
      "(2018). The results are listed in Table 3. From the results, we can see that, though our baseline (GNMT+BERTLen) is higher than previ- ous work (Back-Translation (Asai et al., 2018)), when using SimpleMatch to extract original pas- sage span could obtain competitive of even larger Japanese French EM F1 EM F1 Back-Translation\u2020 24.8 42.6 23.5 44.0 +Runtime MT\u2020 37.0 52.2 40.7 61.9 GNMT+BERTLen 26.9 46.2 39.1 67.0 +SimpleMatch 37.3 58.0 47.4 71.5 BERTSQ\u2212Bmul 61.3 73.4 57.6 77.1 Table 3: Zero-shot cross-lingual machine reading com- prehension results on Japanese and French SQuAD data. \u2020 are extracted in Asai et al. (2018). improvements. In Japanese SQuAD, the F1 score improved by 9.6 in Asai et al.",
      "\u2020 are extracted in Asai et al. (2018). improvements. In Japanese SQuAD, the F1 score improved by 9.6 in Asai et al. (2018) using NMT attention, while we obtain larger improve- ment with 11.8 points demonstrating the effective- ness of the proposed method. BERT with pre- trained SQuAD weights yields the best perfor- mance among these systems, as it does not require the machine translation process and has uni\ufb01ed text representations for different languages. 5.4 Ablation Studies In this section, we ablate important components in our model to explicitly demonstrate its effective- ness. The ablation results are depicted in Table 4. As we can see that, removing SQuAD pre-",
      "EM F1 Dual BERT (w/ SQuAD) 68.0 88.1 w/o Auxiliary Loss 67.5 (-0.5) 87.7 (-0.4) w/o Dynamic Lambda 67.3 (-0.7) 87.5 (-0.6) w/o Self-Adaptive Att. 67.2 (-0.8) 87.5 (-0.6) w/o Source BERT 66.6 (-1.4) 87.3 (-0.8) w/o SQuAD Pre-Train 65.8 (-2.2) 86.3 (-1.8) Table 4: Ablations of Dual BERT on the CMRC 2018 development set. trained weights (i.e., using randomly initialized BERT) hurts the performance most, suggesting that it is bene\ufb01cial to use pre-trained weights though the source and the target language is dif- ferent.",
      "trained weights (i.e., using randomly initialized BERT) hurts the performance most, suggesting that it is bene\ufb01cial to use pre-trained weights though the source and the target language is dif- ferent. Removing source BERT will degenerate to cascade training, and the results show that it also harms overall performance, demonstrating that it is bene\ufb01cial to utilize translated sample for bet- ter characterizing the relations between <passage, question, answer>. The other modi\ufb01cations seem to also consistently decrease the performance to some extent, but not as salient as the data-related components (last two lines), indicating that data- related approaches are important in cross-lingual machine reading comprehension task. 6 Discussion In our preliminary cross-lingual experiments, we adopt English as our source language data. How- ever, one question remains unclear. Is it better to pre-train with larger data in a distant language (such as English, as oppose to Simpli\ufb01ed Chinese), or with smaller data in closer language (such as Traditional Chinese)?",
      "How- ever, one question remains unclear. Is it better to pre-train with larger data in a distant language (such as English, as oppose to Simpli\ufb01ed Chinese), or with smaller data in closer language (such as Traditional Chinese)? To investigate the problem, we plot the multi- lingual BERT performance on the CMRC 2018 development data using different language and data size in the pre-training stage. The results are depicted in Figure 3, and we come to several ob- servations. Firstly, when the size of pre-training data is under 25k (training data size of DRCD), we can see that there is no much difference whether we use Chinese or English data for pre-training, and even the English pre-trained models are better than Chinese pre-trained models in most of the times, which is not expected.",
      "We suspect that, by us- ing multi-lingual BERT, the model tend to provide universal representations for the text and learn the language-independent semantic relations among the inputs which is ideal for cross-lingual tasks, 100 200 500 1k 2k 5k 10k 15k 20k 25k 50k 100k 74.0 74.5 75.0 75.5 76.0 76.5 77.0 Average Score DRCD SQuAD Figure 3: BERT performance (average of EM and F1) with different amount of pre-training SQuAD (English) or DRCD (Traditional Chinese). thus the model is not that sensitive to the lan- guage in the pre-training stage. Also, as train- ing data size of SQuAD is larger than DRCD, we could use more data for pre-training. When we add more SQuAD data (>25k) in the pre-training stage, the performance on the downstream task (CMRC 2018) continues to improve signi\ufb01cantly.",
      "When we add more SQuAD data (>25k) in the pre-training stage, the performance on the downstream task (CMRC 2018) continues to improve signi\ufb01cantly. In this context, we conclude that, \u2022 When the pre-training data is not abundant, there is no special preference on the selection of source (pre-training) language. \u2022 If there are large-scale training data available for several languages, we should select the source language as the one that has the largest training data rather than its linguistic similar- ity to the target language. Furthermore, one could also take advantages of data in various languages, but not only in a bilin- gual environment, to further exploit knowledge from various sources, which is beyond the scope of this paper and we leave this for future work. 7 Conclusion In this paper, we propose Cross-Lingual Machine Reading Comprehension (CLMRC) task.",
      "7 Conclusion In this paper, we propose Cross-Lingual Machine Reading Comprehension (CLMRC) task. When there is no training data available for the tar- get language, \ufb01rstly, we provide several zero- shot approaches that were initially trained on En- glish and transfer to other languages, along with three methods to improve the translated answer span by using unsupervised and supervised ap- proaches. When there is training data available for the target language, we propose a novel model",
      "called Dual BERT to simultaneously model the <passage, question, answer> in source and tar- get languages using multi-lingual BERT. The pro- posed method takes advantage of the large-scale training data by rich-resource language (such as SQuAD) and learns the semantic relations be- tween the passage and question in both source and target language. Experiments on two Chi- nese machine reading comprehension datasets in- dicate that the proposed model could give con- sistent and signi\ufb01cant improvements over various state-of-the-art systems by a large margin and set baselines for future research on CLMRC task. Future studies on cross-lingual machine reading comprehension will focus on 1) how to utilize var- ious types of English reading comprehension data; 2) cross-lingual machine reading comprehension without the translation process, etc. Acknowledgments We would like to thank all anonymous reviewers for their thorough reviewing and providing con- structive comments to improve our paper. The \ufb01rst author was partially supported by the Google TensorFlow Research Cloud (TFRC) program for Cloud TPU access.",
      "Acknowledgments We would like to thank all anonymous reviewers for their thorough reviewing and providing con- structive comments to improve our paper. The \ufb01rst author was partially supported by the Google TensorFlow Research Cloud (TFRC) program for Cloud TPU access. This work was supported by the National Natural Science Foundation of China (NSFC) via grant 61976072, 61632011, and 61772153. References Mart\u00b4\u0131n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. 2016. Tensor\ufb02ow: a system for large-scale machine learning. In OSDI, volume 16, pages 265\u2013 283. Akari Asai, Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa Tsuruoka. 2018. Multilingual extractive reading comprehension by runtime machine transla- tion. arXiv preprint arXiv:1809.03275.",
      "Akari Asai, Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa Tsuruoka. 2018. Multilingual extractive reading comprehension by runtime machine transla- tion. arXiv preprint arXiv:1809.03275. Yong Cheng, Zhaopeng Tu, Fandong Meng, Junjie Zhai, and Yang Liu. 2018. Towards robust neural machine translation. In Proceedings of the 56th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1756\u2013 1766. Association for Computational Linguistics. Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang, Ting Liu, and Guoping Hu. 2017. Attention-over- attention neural networks for reading comprehen- sion. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 593\u2013602. Association for Computational Linguistics.",
      "2017. Attention-over- attention neural networks for reading comprehen- sion. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 593\u2013602. Association for Computational Linguistics. Yiming Cui, Ting Liu, Wanxiang Che, Li Xiao, Zhipeng Chen, Wentao Ma, Shijin Wang, and Guop- ing Hu. 2019. A span-extraction dataset for chinese machine reading comprehension. In Proceedings of the 2019 Conference on Empirical Methods in Natu- ral Language Processing and 9th International Joint Conference on Natural Language Processing. Asso- ciation for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics.",
      "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics. Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William Cohen, and Ruslan Salakhutdinov. 2017. Gated- attention readers for text comprehension. In Pro- ceedings of the 55th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers), pages 1832\u20131846. Association for Compu- tational Linguistics. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su- leyman, and Phil Blunsom. 2015. Teaching ma- chines to read and comprehend. In Advances in Neu- ral Information Processing Systems, pages 1684\u2013 1692. Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston. 2015.",
      "2015. Teaching ma- chines to read and comprehend. In Advances in Neu- ral Information Processing Systems, pages 1684\u2013 1692. Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston. 2015. The goldilocks principle: Reading children\u2019s books with explicit memory representa- tions. arXiv preprint arXiv:1511.02301. Minghao Hu, Furu Wei, Yuxing Peng, Zhen Huang, Nan Yang, and Dongsheng Li. 2019. Read + verify: Machine reading comprehension with unanswerable questions. Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence, 33(01):6529\u20136537. Rudolf Kadlec, Martin Schmid, Ond\u02c7rej Bajgar, and Jan Kleindienst. 2016. Text understanding with the at- tention sum reader network. In Proceedings of the 54th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 908\u2013918.",
      "2016. Text understanding with the at- tention sum reader network. In Proceedings of the 54th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 908\u2013918. Association for Computational Linguis- tics. Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Ting Liu, Yiming Cui, Qingyu Yin, Wei-Nan Zhang, Shijin Wang, and Guoping Hu. 2017. Generating and exploiting large-scale pseudo training data for zero pronoun resolution. In Proceedings of the 55th",
      "Annual Meeting of the Association for Computa- tional Linguistics (Volume 1: Long Papers), pages 102\u2013111. Association for Computational Linguis- tics. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Nat- ural Language Processing, pages 2383\u20132392. Asso- ciation for Computational Linguistics. Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hananneh Hajishirzi. 2016. Bi-directional attention \ufb02ow for machine comprehension. arXiv preprint arXiv:1611.01603. Chih Chieh Shao, Trois Liu, Yuting Lai, Yiying Tseng, and Sam Tsai. 2018. Drcd: a chinese machine reading comprehension dataset. arXiv preprint arXiv:1806.00920.",
      "Chih Chieh Shao, Trois Liu, Yuting Lai, Yiying Tseng, and Sam Tsai. 2018. Drcd: a chinese machine reading comprehension dataset. arXiv preprint arXiv:1806.00920. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, pages 5998\u20136008. Shuohang Wang and Jing Jiang. 2016. Machine com- prehension using match-lstm and answer pointer. arXiv preprint arXiv:1608.07905. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016.",
      "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google\u2019s neural ma- chine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144. Caiming Xiong, Victor Zhong, and Richard Socher. 2016. Dynamic coattention networks for question answering. arXiv preprint arXiv:1611.01604. Wei Yang, Yuqing Xie, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019. Data augmentation for bert \ufb01ne-tuning in open-domain question answering. arXiv preprint arXiv:1904.06652."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1909.00361.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":9126,
  "avg_doclen":169.0,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1909.00361.pdf"
    }
  }
}