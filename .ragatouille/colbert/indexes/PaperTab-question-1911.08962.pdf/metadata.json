{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "arXiv:1911.08962v3  [cs.CL]  25 Nov 2019 CAIL2019-SCM: A Dataset of Similar Case Matching in Legal Domain Chaojun Xiao1\u2217Haoxi Zhong1\u2217Zhipeng Guo1 Cunchao Tu1 Zhiyuan Liu1 Maosong Sun1 Tianyang Zhang2 Xianpei Han3 Zhen Hu4 Heng Wang4 Jianfeng Xu5 1Department of Computer Science and Technology, Tsinghua University, China 2Beijing Powerlaw Intelligent Technology Co., Ltd., China 3Institute of Software, Chinese Academy of Sciences, China 4China Justice Big Data Institute 5Supreme People Court, China Abstract In this paper, we introduce CAIL2019-SCM, Chinese AI and Law 2019 Similar Case Matching dataset. CAIL2019-SCM con- tains 8,964 triplets of cases published by the Supreme People\u2019s Court of China. CAIL2019- SCM focuses on detecting similar cases, and the participants are required to check which two cases are more similar in the triplets.",
      "CAIL2019-SCM con- tains 8,964 triplets of cases published by the Supreme People\u2019s Court of China. CAIL2019- SCM focuses on detecting similar cases, and the participants are required to check which two cases are more similar in the triplets. There are 711 teams who partic- ipated in this year\u2019s competition, and the best team has reached a score of 71.88. We have also implemented several baselines to help researchers better understand this task. The dataset and more details can be found from https://github.com/china-ai-law- challenge/CAIL2019/tree/master/scm. 1 Introduction Similar Case Matching (SCM) plays a major role in legal system, especially in common law legal system. The most similar cases in the past de- termine the judgment results of cases in common law systems. As a result, legal professionals of- ten spend much time \ufb01nding and judging similar cases to prove fairness in judgment. As automat- ically \ufb01nding similar cases can bene\ufb01t to the le- gal system, we select SCM as one of the tasks of CAIL2019.",
      "As automat- ically \ufb01nding similar cases can bene\ufb01t to the le- gal system, we select SCM as one of the tasks of CAIL2019. Chinese AI and Law Challenge (CAIL) is a competition of applying arti\ufb01cial intelligence tech- nology to legal tasks. The goal of the competi- tion is to use AI to help the legal system. CAIL was \ufb01rst held in 2018, and the main task of CAIL2018 (Xiao et al., 2018; Zhong et al., 2018b) is predicting the judgment results from the fact de- scription. The judgment results include the accu- sation, applicable articles, and the term of penalty. CAIL2019 contains three different tasks, includ- ing Legal Question-Answering, Legal Case Ele- \u2217indicates equal contribution. ment Prediction, and Similar Case Matching. Fur- thermore, we will focus on SCM in this paper. More speci\ufb01cally, CAIL2019-SCM contains 8,964 triplets of legal documents.",
      "ment Prediction, and Similar Case Matching. Fur- thermore, we will focus on SCM in this paper. More speci\ufb01cally, CAIL2019-SCM contains 8,964 triplets of legal documents. Every legal documents is collected from China Judgments On- line1. In order to ensure the similarity of the cases in one triplet, all selected documents are related to Private Lending. Every document in the triplet contains the fact description. CAIL2019-SCM re- quires researchers to decide which two cases are more similar in a triplet. By detecting similar cases in triplets, we can apply this algorithm for ranking all documents to \ufb01nd the most similar doc- ument in the database. There are 247 teams who have participated CAIL2019-SCM, and the best team has reached a score of 71.88, which is about 20 points higher than the baseline. The results show that the existing methods have made great progress on this task, but there is still much room for improvement. In other words, CAIL2019-SCM can bene\ufb01t the research of legal case matching.",
      "The results show that the existing methods have made great progress on this task, but there is still much room for improvement. In other words, CAIL2019-SCM can bene\ufb01t the research of legal case matching. Furthermore, there are several main challenges of CAIL2019- SCM: (1) The difference between documents may be small, and then it is hard to decide which two documents are more similar. Moreover, the simi- larity is de\ufb01ned by legal workers. We must utilize legal knowledge into this task rather than calculate similarity on the lexical level. (2) The length of the documents is quite long. Most documents contain more than 512 characters, and then it is hard for existing methods to capture document level infor- mation. In the following parts, we will give more details about CAIL2019-SCM, including related works about SCM, the task de\ufb01nition, the construction of the dataset, and several experiments on the dataset. 1http://wenshu.court.gov.cn/",
      "2 Related Work 2.1 Semantic Text Matching SCM aims to measure the similarity between legal case documents. Essentially, it is an application of semantic text matching, which is central for many tasks in natural language processing, such as ques- tion answering, information retrieval, and natural language inference. Take information retrieval as an example, given a query and a database, a seman- tic matching model is required to judge the seman- tic similarity between the query and documents in the database. Moreover, the tasks related to seman- tic matching have attracted the attention of many researchers in recent decades. Intuitively traditional approaches calculate word-to-word similarity with vector space model, e.g. term frequency-inverse docu- ment frequency (Wu et al., 2008), bag-of- words (Bilotti et al., 2007). However, due to the variety of words in different texts, these approaches achieve limited success in the task. Recently, with the development of deep learning in natural language processing, re- searchers attempt to apply neural models to en- code text into distributed representation.",
      "However, due to the variety of words in different texts, these approaches achieve limited success in the task. Recently, with the development of deep learning in natural language processing, re- searchers attempt to apply neural models to en- code text into distributed representation. The Siamese structure (Bromley et al., 1994) for met- ric learning achieve great success and is widely applied (Amiri et al., 2016; Liu et al., 2018; Mueller and Thyagarajan, 2016; Neculoiu et al., 2016; Wan et al., 2016; He et al., 2015). Be- sides, there are many researchers put emphasis on integrating syntactic structure into semantic matching (Liu et al., 2018; Chen et al., 2017) and multi-level text matching with attention-aware rep- resentation (Duan et al., 2018; Tan et al., 2018; Yin et al., 2016). Nevertheless, most previous studies are de- signed for identifying the relationship between two sentences with limited length. 2.2 Legal Intelligence Researchers widely concern tasks for legal intelli- gence.",
      "Nevertheless, most previous studies are de- signed for identifying the relationship between two sentences with limited length. 2.2 Legal Intelligence Researchers widely concern tasks for legal intelli- gence. Applying NLP techniques to solve a legal problem becomes more and more popular in recent years. Previous works (Kort, 1957; Keown, 1980; Lauderdale and Clark, 2012) focus on analyzing existing cases with mathematical tools. With the development of deep learning, more researchers pay much efforts on predicting the judgment result of legal cases (Luo et al., 2017; Hu et al., 2018; Zhong et al., 2018a; Chalkidis et al., 2019; Jiang et al., 2018; Yang et al., 2019).",
      "Further- more, there are many works on generating court views to interpret charge results (Ye et al., 2018), information extraction from legal text (Vacek and Schilder, 2017; Vacek et al., 2019), legal event detection (Yan et al., 2017), identifying applicable law articles (Liu et al., 2015; Liu and Hsieh, 2006) and legal question answering (Kim et al., 2015; Fawei et al., 2018). Meanwhile, retrieving related legal documents with a query has been studied for decades and is a critical issue in applications of legal in- telligence. Raghav et al. (2016) emphasize ex- ploiting paragraph-level and citation information. Kano et al. (2017) and Zhong et al. (2018b) held a legal information extraction and entailment com- petition to promote progress in legal case retrieval. 3 Overview of Dataset 3.1 Task De\ufb01nition We \ufb01rst de\ufb01ne the task of CAIL2019-SCM here.",
      "(2018b) held a legal information extraction and entailment com- petition to promote progress in legal case retrieval. 3 Overview of Dataset 3.1 Task De\ufb01nition We \ufb01rst de\ufb01ne the task of CAIL2019-SCM here. The input of CAIL2019-SCM is a triplet (A, B, C), where A, B, C are fact descriptions of three cases. Here we de\ufb01ne a function sim which is used for measuring the similarity between two cases. Then the task of CAIL2019-SCM is to predict whether sim(A, B) > sim(A, C) or sim(A, C) > sim(A, B). 3.2 Dataset Construction and Details To ensure the quality of the dataset, we have sev- eral steps of constructing the dataset. First, we se- lect many documents within the range of Private Lending. However, although all cases are related to Private Lending, they are still various so that many cases are not similar at all. If the cases in the triplets are not similar, it does not make sense to compare their similarities.",
      "However, although all cases are related to Private Lending, they are still various so that many cases are not similar at all. If the cases in the triplets are not similar, it does not make sense to compare their similarities. To produce quali\ufb01ed triplets, we \ufb01rst annotated some crucial elements in Private Lending for each document. The ele- ments include: \u2022 The properties of lender and borrower, whether they are a natural person, a legal per- son, or some other organization. \u2022 The type of guarantee, including no guaran- tee, guarantee, mortgage, pledge, and others. \u2022 The usage of the loan, including personal life, family life, enterprise production and opera- tion, crime, and others.",
      "\u2022 The lending intention, including regular lend- ing, transfer loan, and others. \u2022 Conventional interest rate method, including no interest, simple interest, compound inter- est, unclear agreement, and others. \u2022 Interest during the agreed period, including [0%, 24%], (24%, 36%], (36%, \u221e), and oth- ers. \u2022 Borrowing delivery form, including no lend- ing, cash, bank transfer, online electronic re- mittance, bill, online loan platform, autho- rization to control a speci\ufb01c fund account, un- known or fuzzy, and others. \u2022 Repayment form, including unpaid, partial repayment, cash, bank transfer, online elec- tronic remittance, bill, unknown or fuzzy, and others. \u2022 Loan agreement, including loan contract, or borrowing, \u201cWeChat, SMS, phone or other chat records\u201d, receipt, irrigation, repayment commitment, guarantee, unknown or fuzzy and others. After annotating these elements, we can assume that cases with similar elements are quite similar.",
      "\u2022 Loan agreement, including loan contract, or borrowing, \u201cWeChat, SMS, phone or other chat records\u201d, receipt, irrigation, repayment commitment, guarantee, unknown or fuzzy and others. After annotating these elements, we can assume that cases with similar elements are quite similar. So when we construct the triplets, we calculate the tf-idf similarity and elemental similarity between cases and select those similar cases to construct our dataset. We have constructed 8,964 triples in total by these methods, and the statistics can be found from Table 1. Then, legal professionals will annotate every triplet to see whether sim(A, B) > sim(A, C) or sim(A, B) < sim(A, C). Further- more, to ensure the quality of annotation, every document and triplet is annotated by at least three legal professionals to reach an agreement. Type Count Small Train 500 Small Test 326 Large Train 5,102 Large Valid 1,500 Large Test 1,536 Total 8,964 Table 1: The number of triplets in different stages of CAIL2019-SCM.",
      "Type Count Small Train 500 Small Test 326 Large Train 5,102 Large Valid 1,500 Large Test 1,536 Total 8,964 Table 1: The number of triplets in different stages of CAIL2019-SCM. 4 Experiments To access the challenge of the similar cases match- ing task, we evaluate several baselines on our dataset. The experiment results show that even the state-of-the-art systems perform poorly in evaluat- ing the similarity between different cases. Baselines. All the baseline models are trained on Large Train and tested on Large Valid and Large Test. We adapt the Siamese framework (Bromley et al., 1994) to our scenario with different encoder, e.g. CNN (Kim, 2014), LSTM (Hochreiter and Schmidhuber, 1997), Bert (Devlin et al., 2019), used for encoding the legal documents. We will elaborate on the details of the framework in the following part.",
      "CNN (Kim, 2014), LSTM (Hochreiter and Schmidhuber, 1997), Bert (Devlin et al., 2019), used for encoding the legal documents. We will elaborate on the details of the framework in the following part. Given the triplet of fact description, (DA, DB, DC), we \ufb01rst encode them into distributed vectors with the same encoder and then compute the sim- ilarity scores between the query case DA and the candidate cases DB, DC with a linear layer. As- sume that each document D consisting of n words, i.e. D = {w1, w2, ..., wn}. For CNN/LSTM encoder, we \ufb01rst employ THU- LAC (Sun et al., 2016) for word segmentation and then transform each word into distributed representation X = {x1, x2, ..., xn} with Glove (Pennington et al., 2014), where xi \u2208Rd, i = 1, 2, ..., n and d is the dimension of word embed- dings.",
      "Next, the encoder layer and max pool- ing layer transform the embedding sequence X into features h \u2208Rdh, where dh is the dimen- sion of hidden vector. While for Bert encoder, we feed the document in character-level into the bert base chinese model to get the features h. hA = Encoder(DA) hB = Encoder(DB) hC = Encoder(DC) (1) Afterward, we calculate the similarity with a lin- ear layer with softmax activation. W \u2208Rdh\u00d7dh is a weight matrix to be learned. sAj = softmax(exp(hAWhj)) j = B, C (2) For the learning objective, we apply the binary cross-entropy loss function with ground-truth la- bel p: L(\u03b8) = E[pln(sAB) + (1 \u2212p)ln(sAC)] (3)",
      "Method Valid Test baselines CNN 62.27 69.53 LSTM 62.00 68.00 BERT 61.93 67.32 Teams AlphaCourt 70.07 72.66 backward 67.73 71.81 11.2 yuan 66.73 72.07 Table 2: Results of baselines and scores of top 3 partic- ipants on valid and test datasets. Model Performance. We use the accuracy met- ric in our experiments. Table 2 shows the re- sults of baselines and top 3 participant teams on Large Valid and Large Test dataset, from which we get the following conclusion: 1) The participants achieve promising progress compared to baseline models. 2) Both the baselines systems and par- ticipant teams perform poorly on the dataset, due to the lack of utilization of prior legal knowledge. It\u2019s still challenging to utilize legal knowledge and simulate legal reasoning for the dataset. 5 Conclusion In this paper, we propose a new dataset, CAIL2019-SCM, which focuses on the task of similar case matching in the legal domain.",
      "It\u2019s still challenging to utilize legal knowledge and simulate legal reasoning for the dataset. 5 Conclusion In this paper, we propose a new dataset, CAIL2019-SCM, which focuses on the task of similar case matching in the legal domain. Com- pared with existing datasets, CAIL2019-SCM can bene\ufb01t the case matching in the legal domain to help the legal partitioners work better. Experimen- tal results also show that there is still plenty of room for improvement. References Hadi Amiri, Philip Resnik, Jordan Boyd-Graber, and Hal Daum\u00b4e III. 2016. Learning text pair similarity with context-sensitive autoencoders. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1882\u20131892. Matthew W Bilotti, Paul Ogilvie, Jamie Callan, and Eric Nyberg. 2007. Structured retrieval for question answering. In Proceedings of the 30th annual inter- national ACM SIGIR conference on Research and development in information retrieval, pages 351\u2013 358. ACM.",
      "2007. Structured retrieval for question answering. In Proceedings of the 30th annual inter- national ACM SIGIR conference on Research and development in information retrieval, pages 351\u2013 358. ACM. Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard S\u00a8ackinger, and Roopak Shah. 1994. Signature veri\ufb01- cation using a\u201d siamese\u201d time delay neural network. In Advances in neural information processing sys- tems, pages 737\u2013744. Ilias Chalkidis, Ion Androutsopoulos, and Nikolaos Aletras. 2019. Neural legal judgment prediction in english. In Proceddings of ACL. Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2017. Enhanced lstm for natural language inference. In Proceedings of the 55th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 1657\u20131668. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.",
      "In Proceedings of the 55th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 1657\u20131668. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4171\u20134186. Chaoqun Duan, Lei Cui, Xinchi Chen, Furu Wei, Con- ghui Zhu, and Tiejun Zhao. 2018. Attention-fused deep matching network for natural language infer- ence. In IJCAI, pages 4033\u20134040. Biralatei Fawei, Jeff Z Pan, Martin Kollingbaum, and Adam Z Wyner. 2018. A methodology for a crim- inal law and procedure ontology for legal question answering. In In Proceddings of JIST. Springer Ver- lag.",
      "Biralatei Fawei, Jeff Z Pan, Martin Kollingbaum, and Adam Z Wyner. 2018. A methodology for a crim- inal law and procedure ontology for legal question answering. In In Proceddings of JIST. Springer Ver- lag. Hua He, Kevin Gimpel, and Jimmy Lin. 2015. Multi- perspective sentence similarity modeling with con- volutional neural networks. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1576\u20131586. Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735\u20131780. Zikun Hu, Xiang Li, Cunchao Tu Zhiyuan Liu, and Maosong Sun. 2018. Few-shot charge prediction with discriminative legal attributes. Xin Jiang, Hai Ye, Zhunchen Luo, WenHan Chao, and Wenjia Ma. 2018. Interpretable rationale augmented charge prediction system. In In Proceedings of COL- ING.",
      "2018. Few-shot charge prediction with discriminative legal attributes. Xin Jiang, Hai Ye, Zhunchen Luo, WenHan Chao, and Wenjia Ma. 2018. Interpretable rationale augmented charge prediction system. In In Proceedings of COL- ING. Yoshinobu Kano, Mi-Young Kim, Randy Goebel, and Ken Satoh. 2017. Overview of coliee 2017. In COL- IEE@ ICAIL, pages 1\u20138. R Keown. 1980. Mathematical models for legal predic- tion. Computer/lj, 2:829. Mi-Young Kim, Randy Goebel, and S Ken. 2015. Coliee-2015: evaluation of legal question answering. In In Proceddings of JURISIN. Yoon Kim. 2014. Convolutional neural networks for sentence classi\ufb01cation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Lan- guage Processing (EMNLP), pages 1746\u20131751.",
      "Fred Kort. 1957. Predicting supreme court decisions mathematically: A quantitative analysis of the\u201d right to counsel\u201d cases. The American Political Science Review, 51(1):1\u201312. Benjamin E Lauderdale and Tom S Clark. 2012. The supreme court\u2019s many median justices. American Political Science Review, 106(4):847\u2013866. Bang Liu, Ting Zhang, Fred X Han, Di Niu, Kun- feng Lai, and Yu Xu. 2018. Matching natural lan- guage sentences with hierarchical sentence factoriza- tion. In Proceedings of the 2018 World Wide Web Conference, pages 1237\u20131246. International World Wide Web Conferences Steering Committee. Chao-Lin Liu and Chwen-Dar Hsieh. 2006. Exploring phrase-based classi\ufb01cation of judicial documents for criminal charges in chinese. In Proceedings of the 16th international conference on Foundations of In- telligent Systems. Springer-Verlag. Yi-Hung Liu, Yen-Liang Chen, and Wu-Liang Ho. 2015.",
      "In Proceedings of the 16th international conference on Foundations of In- telligent Systems. Springer-Verlag. Yi-Hung Liu, Yen-Liang Chen, and Wu-Liang Ho. 2015. Predicting associated statutes for legal prob- lems. Information Processing & Management, 51(1):194\u2013211. Bingfeng Luo, Yansong Feng, Jianbo Xu, Xiang Zhang, and Dongyan Zhao. 2017. Learning to predict charges for criminal cases with legal basis. In In Proceedings of EMNLP. Jonas Mueller and Aditya Thyagarajan. 2016. Siamese recurrent architectures for learning sentence similar- ity. In Thirtieth AAAI Conference on Arti\ufb01cial Intel- ligence. Paul Neculoiu, Maarten Versteegh, and Mihai Rotaru. 2016. Learning text similarity with siamese recur- rent networks. In Proceedings of the 1st Workshop on Representation Learning for NLP, pages 148\u2013 157. Jeffrey Pennington, Richard Socher, and Christopher Manning.",
      "2016. Learning text similarity with siamese recur- rent networks. In Proceedings of the 1st Workshop on Representation Learning for NLP, pages 148\u2013 157. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word rep- resentation. In Proceedings of the 2014 conference on empirical methods in natural language process- ing (EMNLP), pages 1532\u20131543. K Raghav, P Krishna Reddy, and V Balakista Reddy. 2016. Analyzing the extraction of relevant legal judgments using paragraph-level and citation infor- mation. AI4JCArti\ufb01cial Intelligence for Justice, page 30. Maosong Sun, Xinxiong Chen, Kaixu Zhang, Zhipeng Guo, and Zhiyuan Liu. 2016. Thulac: An ef\ufb01cient lexical analyzer for chinese. Technical report, Tech- nical Report. Technical Report. Chuanqi Tan, Furu Wei, Wenhui Wang, Weifeng Lv, and Ming Zhou. 2018.",
      "2016. Thulac: An ef\ufb01cient lexical analyzer for chinese. Technical report, Tech- nical Report. Technical Report. Chuanqi Tan, Furu Wei, Wenhui Wang, Weifeng Lv, and Ming Zhou. 2018. Multiway attention networks for modeling sentence pairs. In IJCAI, pages 4411\u2013 4417. Thomas Vacek, Ronald Teo, Dezhao Song, Conner Cowling, Frank Schilder, Timothy Nugent, and Ca- nary Wharf. 2019. Litigation analytics: Case out- comes extracted from us federal court dockets. In Proceddings of NAACL-HLT. Tom Vacek and Frank Schilder. 2017. A sequence ap- proach to case outcome detection. In In Proceedings of ICAIL. ACM. Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu, Liang Pang, and Xueqi Cheng. 2016. A deep ar- chitecture for semantic matching with multiple po- sitional sentence representations.",
      "ACM. Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu, Liang Pang, and Xueqi Cheng. 2016. A deep ar- chitecture for semantic matching with multiple po- sitional sentence representations. In Thirtieth AAAI Conference on Arti\ufb01cial Intelligence. Ho Chung Wu, Robert Wing Pong Luk, Kam Fai Wong, and Kui Lam Kwok. 2008. Interpreting tf-idf term weights as making relevance decisions. ACM Trans- actions on Information Systems (TOIS), 26(3):13. Chaojun Xiao, Haoxi Zhong, Zhipeng Guo, Cunchao Tu, Zhiyuan Liu, Maosong Sun, Yansong Feng, Xianpei Han, Zhen Hu, Heng Wang, et al. 2018. Cail2018: A large-scale legal dataset for judgment prediction. arXiv preprint arXiv:1807.02478. Yukun Yan, Daqi Zheng, Zhengdong Lu, and Sen Song. 2017.",
      "2018. Cail2018: A large-scale legal dataset for judgment prediction. arXiv preprint arXiv:1807.02478. Yukun Yan, Daqi Zheng, Zhengdong Lu, and Sen Song. 2017. Event identi\ufb01cation as a decision process with non-linear representation of text. arXiv preprint arXiv:1710.00969. Wenmian Yang, Weijia Jia, Xiaojie Zhou, and Yutao Luo. 2019. Legal judgment prediction via multi- perspective bi-feedback network. Hai Ye, Xin Jiang, Zhunchen Luo, and Wenhan Chao. 2018. Interpretable charge predictions for criminal cases: Learning to generate court views from fact descriptions. In In Proceedings ofNAACL. Wenpeng Yin, Hinrich Sch\u00a8utze, Bing Xiang, and Bowen Zhou. 2016. Abcnn: Attention-based convo- lutional neural network for modeling sentence pairs. Transactions of the Association for Computational Linguistics, 4:259\u2013272.",
      "Wenpeng Yin, Hinrich Sch\u00a8utze, Bing Xiang, and Bowen Zhou. 2016. Abcnn: Attention-based convo- lutional neural network for modeling sentence pairs. Transactions of the Association for Computational Linguistics, 4:259\u2013272. Haoxi Zhong, Zhipeng Guo, Cunchao Tu, Chaojun Xiao, Zhiyuan Liu, and Maosong Sun. 2018a. Le- gal judgment prediction via topological learning. In In Proceedings of the EMNLP. Haoxi Zhong, Chaojun Xiao, Zhipeng Guo, Cunchao Tu, Zhiyuan Liu, Maosong Sun, Yansong Feng, Xi- anpei Han, Zhen Hu, Heng Wang, et al. 2018b. Overview of cail2018: Legal judgment prediction competition. arXiv preprint arXiv:1810.05851."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1911.08962.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":5204,
  "avg_doclen":179.4482758621,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1911.08962.pdf"
    }
  }
}