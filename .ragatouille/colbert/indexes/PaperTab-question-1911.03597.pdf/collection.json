[
  "Zero-Shot Paraphrase Generation with Multilingual Language Models Yinpeng Guo1, Yi Liao1, Xin Jiang1, Qing Zhang2, Yibo Zhang2, Qun Liu1 1Huawei Noah\u2019s Ark Lab 2Intelligence Engineering Department, Huawei Consumer Business Group {guo.yinpeng, liao.yi, jiang.xin, zhangqing49, yibo.cheung, qun.liu}@huawei.com Abstract Leveraging multilingual parallel texts to au- tomatically generate paraphrases has drawn much attention as size of high-quality para- phrase corpus is limited. Round-trip transla- tion, also known as the pivoting method, is a typical approach to this end. However, we notice that the pivoting process involves mul- tiple machine translation models and is likely to incur semantic drift during the two-step translations. In this paper, inspired by the Transformer-based language models, we pro- pose a simple and uni\ufb01ed paraphrasing model, which is purely trained on multilingual paral- lel data and can conduct zero-shot paraphrase generation in one step.",
  "In this paper, inspired by the Transformer-based language models, we pro- pose a simple and uni\ufb01ed paraphrasing model, which is purely trained on multilingual paral- lel data and can conduct zero-shot paraphrase generation in one step. Compared with the piv- oting approach, paraphrases generated by our model is more semantically similar to the input sentence. Moreover, since our model shares the same architecture as GPT (Radford and Sutskever, 2018), we are able to pre-train the model on large-scale unparallel corpus, which further improves the \ufb02uency of the output sen- tences. In addition, we introduce the mecha- nism of denoising auto-encoder (DAE) to im- prove diversity and robustness of the model. Experimental results show that our model sur- passes the pivoting method in terms of rele- vance, diversity, \ufb02uency and ef\ufb01ciency. 1 Introduction Paraphrasing is to express the same meaning us- ing different expressions.",
  "Experimental results show that our model sur- passes the pivoting method in terms of rele- vance, diversity, \ufb02uency and ef\ufb01ciency. 1 Introduction Paraphrasing is to express the same meaning us- ing different expressions. Paraphrase generation plays an important role in various natural language processing (NLP) tasks such as response diversi- \ufb01cation in dialogue system, query reformulation in information retrieval, and data augmentation in machine translation. Recently, models based on Seq2Seq learning (Ilya Sutskever, 2014) have achieved the state-of-the-art results on paraphrase generation. Most of these models (Prakash et al., 2016; Ziqiang Cao, 2017; Ankush Gupta, 2018; Figure 1: Paraphrase generation via round-trip transla- tion. Zichao Li, 2018, 2019) focus on training the para- phrasing models based on a paraphrase corpus, which contains a number of pairs of paraphrases. However, high-quality paraphrases are usually dif- \ufb01cult to acquire in practice, which becomes the major limitation of these methods.",
  "However, high-quality paraphrases are usually dif- \ufb01cult to acquire in practice, which becomes the major limitation of these methods. Therefore, we focus on zero-shot paraphrase generation approach in this paper, which aims to generate paraphrases without requiring a paraphrase corpus. A natural choice is to leverage the bilingual or multilingual parallel data used in machine transla- tion, which are of great quantity and quality. The basic assumption is that if two sentences in one language (e.g., English) have the same translation in another language (e.g., French), they are as- sumed to have the same meaning, i.e., they are paraphrases of each other. Therefore, one typical solution for paraphrasing in one language is to pivot over a translation in another language. Speci\ufb01cally, it is implemented as the round-trip translation, where the input sentence is translated into a for- eign sentence, then back-translated into a sentence in the same language as input (Jonathan Mallinson and Lapata, 2017). The process is shown in Fig- ure 1.",
  "The process is shown in Fig- ure 1. Apparently, two machine translation sys- tems (English\u2192French and French\u2190English) are needed to conduct the generation of a paraphrase. Although the pivoting approach works in gen- eral, there are several intrinsic defects. First, arXiv:1911.03597v1  [cs.CL]  9 Nov 2019",
  "the round-trip system can hardly explore all the paths of paraphrasing, since it is pivoted through the \ufb01nite intermedia outputs of a translation sys- tem. More formally, let Z denote the meaning representation of a sentence X, and \ufb01nding para- phrases of X can be treated as sampling another sentence Y conditioning on the representation Z. Ideally, paraphrases should be generated by follow- ing P(Y |X) = R Z P(Y |Z)P(Z|X)dZ, which is marginalized over all possible values of Z. How- ever, in the round-trip translation, only one or sev- eral Zs are sampled from the machine translation system P(Z|X), which can lead to an inaccurate approximation of the whole distribution and is prone to the problem of semantic drift due to the sampling variances. Second, the results are deter- mined by the pre-existing translation systems, and it is dif\ufb01cult to optimize the pipeline end-to-end. Last, the system is not ef\ufb01cient especially at the inference stage, because it needs two rounds of translation decoding.",
  "Second, the results are deter- mined by the pre-existing translation systems, and it is dif\ufb01cult to optimize the pipeline end-to-end. Last, the system is not ef\ufb01cient especially at the inference stage, because it needs two rounds of translation decoding. To address these issues, we propose a single-step zero-shot paraphrase generation model, which can be trained on machine translation corpora in an end-to-end fashion. Unlike the pivoting approach, our proposed model does not involve explicit trans- lation between multiple languages. Instead, it di- rectly learns the paraphrasing distribution P(Y |X) from the parallel data sampled from P(Z|X) and P(Y |Z). Speci\ufb01cally, we build a Transformer- based (Ashish Vaswani, 2017) language model, which is trained on the concatenated bilingual par- allel sentences with language indicators. At infer- ence stage, given a input sentence in a particular language, the model is guided to generate sentences in the same language, which are deemed as para- phrases of the input. Our model is simple and compact, and can empirically reduce the risk of semantic drift to a large extent.",
  "At infer- ence stage, given a input sentence in a particular language, the model is guided to generate sentences in the same language, which are deemed as para- phrases of the input. Our model is simple and compact, and can empirically reduce the risk of semantic drift to a large extent. Moreover, we can initialize our model with generative pre-training (GPT) (Radford and Sutskever, 2018) on mono- lingual data, which can bene\ufb01t the generation in low-resource languages. Finally, we borrow the idea of denoising auto-encoder (DAE) to further enhance robustness in paraphrase generation. We conduct experiments on zero-shot paraphrase generation task, and \ufb01nd that the proposed model signi\ufb01cantly outperforms the pivoting approach in terms of both automatic and human evaluations. Meanwhile, the training and inference cost are largely reduced compared to the pivot-based meth- ods which involves multiple systems. 2 Methodology 2.1 Transformer-based Language Model Transformer-based language model (TLM) is a neural language model constructed with a stack of Transformer decoder layers (Ashish Vaswani, 2017).",
  "2 Methodology 2.1 Transformer-based Language Model Transformer-based language model (TLM) is a neural language model constructed with a stack of Transformer decoder layers (Ashish Vaswani, 2017). Given a sequence of tokens, TLM is trained with maximizing the likelihood: L(X) = n X i=1 log P(xi|x1,...,i\u22121; \u03b8) (1) where X = [x1, x2, . . . , xn] is a sentence in a lan- guage (e.g., English), and \u03b8 denotes the parameters of the model. Each Transformer layer is composed of multi-head self-attention, layer normalization and a feed-forward network. We refer reader to the original paper for details of each component. Formally, the decoding probability is given by [e1, . . . , ei\u22121] = [Wex1 + p1, . . . , Wexi\u22121 + pi\u22121], [h1, . . . , hi\u22121] = Transformer([e1, . . .",
  ". . , ei\u22121] = [Wex1 + p1, . . . , Wexi\u22121 + pi\u22121], [h1, . . . , hi\u22121] = Transformer([e1, . . . , ei\u22121]), P(xi|x1,...,i\u22121; \u03b8) = Softmax(Wohi\u22121), (2) where xi denotes the token embedding, pi denote the positional embedding and hi denotes the output states of the i-th token, and We and Wo are the input and output embedding matrices. Although TLM is normally employed to model monolingual sequences, there is no barrier to utilize TLM to model sequences in multiple languages. In this paper, inspired by Lample and Conneau (2019), we concatenate pairs of sentences from bilingual parallel corpora (e.g., English\u2192French) as training instances to the model.",
  "In this paper, inspired by Lample and Conneau (2019), we concatenate pairs of sentences from bilingual parallel corpora (e.g., English\u2192French) as training instances to the model. Let X and Y denote the parallel sentences in two different languages, the training objective becomes L(X,Y ) = n X i=1 log P(xi|x1,...,i\u22121; \u03b8) + m X j=1 log P(yj|x1,...,n, y1,...,j\u22121; \u03b8). (3) This bilingual language model can be regarded as the decoder-only model compared to the traditional encoder-decoder model. It has been proved to work effectively on monolingual text-to-text generation tasks such as summarization (Peter J. Liu, 2018). The advantages of such architecture include less",
  "(a) Multilingual Language Model Training. (b) Zero-Shot Paraphrase Generation. Figure 2: Paraphrase generation via multilingual language model training. model parameters, easier optimization and poten- tial better performance for longer sequences. Fur- thermore, it naturally integrates with language mod- els pre-training on monolingual corpus. For each input sequence of concatenated sen- tences, we add special tokens \u27e8bos\u27e9and \u27e8eos\u27e9at the beginning and the end, and \u27e8delim\u27e9in between the sentences. Moreover, at the beginning of each sentence, we add a special token as its language identi\ufb01er, for instance, \u27e8en\u27e9for English, \u27e8fr\u27e9for French. One example of English\u2192French training sequence is \u201c\u27e8bos\u27e9\u27e8en\u27e9cat sat on the mat \u27e8delim\u27e9 \u27e8fr\u27e9chat assis sur le tapis \u27e8eos\u27e9\u201d.",
  "One example of English\u2192French training sequence is \u201c\u27e8bos\u27e9\u27e8en\u27e9cat sat on the mat \u27e8delim\u27e9 \u27e8fr\u27e9chat assis sur le tapis \u27e8eos\u27e9\u201d. At inference stage, the model predicts the next word as the conventional auto-regressive model: \u02c6yj \u223cP(yj|x1,...,n, y1,...,j\u22121; \u03b8) (4) 2.2 Zero-shot Paraphrase Generation We train the bilingual language model on multiple bilingual corpora, for example, English\u2194French and German\u2194Chinese. Once the language model has been trained, we can conduct zero-shot para- phrase generation based on the model. Speci\ufb01cally, given an input sentence that is fed into the language model, we set the output language identi\ufb01er the same as input, and then simply conduct decoding to generate paraphrases of the input sentence. Figure 2 illustrates the training and decoding process of our model. In the training stage, the model is trained to sequentially generate the input sentence and its translation in a speci\ufb01c language. Training is conducted in the way of teacher-forcing.",
  "Figure 2 illustrates the training and decoding process of our model. In the training stage, the model is trained to sequentially generate the input sentence and its translation in a speci\ufb01c language. Training is conducted in the way of teacher-forcing. In the decoding stage, after an English sentence \u201c\u27e8bos\u27e9\u27e8en\u27e9cat sat on the mat \u27e8delim\u27e9\u201d is fed to the model, we intentionally set the output language identi\ufb01er as \u201c\u27e8en\u27e9\u201d, in order to guide the model to continue to generate English words. At the same time, since the model has been trained on transla- tion corpus, it implicitly learns to keep the semantic meaning of the output sentence the same as the in- put. Accordingly, the model will probably generate the paraphrases of the input sentence, such as \u201cthe cat sitting on the carpet \u27e8eos\u27e9\u201d. It should be noted our model can obviously be trained on parallel paraphrase data without any modi\ufb01cation. But in this paper, we will mainly focus on the research and evaluation in the zero- shot learning setting.",
  "It should be noted our model can obviously be trained on parallel paraphrase data without any modi\ufb01cation. But in this paper, we will mainly focus on the research and evaluation in the zero- shot learning setting. In the preliminary experiments of zero-shot para- phrasing, we \ufb01nd the model does not perform con- sistently well and sometimes fails to generate the words in the correct language as indicated by the language identi\ufb01er. Similar phenomenon has been observed in the research of zero-shot neural ma- chine translation (Sestorain et al., 2018; Arivazha- gan et al., 2019; Jiatao Gu, 2019), which is referred as the degeneracy problem by Jiatao Gu (2019). To address these problems in zero-shot paraphrase gen- eration, we propose several techniques to improve the quality and diversity of the model as follows. 2.2.1 Language Embeddings The language identi\ufb01er prior to the sentence does not always guarantee the language of the sequences generated by the model.",
  "2.2.1 Language Embeddings The language identi\ufb01er prior to the sentence does not always guarantee the language of the sequences generated by the model. In order to keep the lan- guage consistency, we introduce language embed- dings, where each language is assigned a speci\ufb01c vector representation. Supposing that the language embedding for the i-th token in a sentence is ai, we concatenate the language embedding with the Transformer output states and feed it to the softmax layer for predicting each token: P(yj|x1,...,n, y1,...,j\u22121; \u03b8) = Softmax(Wo[hj, aj]) (5)",
  "We empirically demonstrate that the language em- bedding added to each tokens can effectively guide the model to generate sentences in the required lan- guage. Note that we still let the model to learn the output distribution for each language rather than simply restricting the vocabularies of output space. This offers \ufb02exibility to handle coding switching cases commonly seen in real-world data, e.g., En- glish words could also appear in French sentences. 2.2.2 Pre-Training on Monolingual Corpora Language model pre-training has shown its effec- tiveness in language generation tasks such as ma- chine translation, text summarization and genera- tive question answering (Radford et al., 2019; Dong et al., 2019; Song et al., 2019). It is particularly helpful to the low/zero-resource tasks since the knowledge learned from large-scale monolingual corpus can be transferred to downstream tasks via the pre-training-then-\ufb01ne-tuning approach. Since our model for paraphrase generation shares the same architecture as the language model, we are able to pre-train the model on massive monolingual data.",
  "Since our model for paraphrase generation shares the same architecture as the language model, we are able to pre-train the model on massive monolingual data. Pre-training on monolingual data is conducted in the same way as training on parallel data, except that each training example contains only one sen- tence with the beginning/end of sequence tokens and the language identi\ufb01er. The language embed- dings are also employed. The pre-training objective is the same as Equation (1). In our experiments, we \ufb01rst pre-train the model on monolingual corpora of multiple languages re- spectively, and then \ufb01ne-tune the model on parallel corpora. 2.2.3 Denoising Auto-Encoder We adopt the idea of denoising auto-encoder (DAE) to further improve the robustness of our para- phrasing model. DAE is originally proposed to learn intermediate representations that are robust to partial corruption of the inputs in training auto- encoders (Pascal Vincent, 2008).",
  "DAE is originally proposed to learn intermediate representations that are robust to partial corruption of the inputs in training auto- encoders (Pascal Vincent, 2008). Speci\ufb01cally, the initial input X is \ufb01rst partially corrupted as \u02dcX, which can be treated as sampling from a noise dis- tribution \u02dcX \u223cq( \u02dcX|X). Then, an auto-encoder is trained to recover the original X from the noisy input \u02dcX by minimizing the reconstruction error. In the applications of text generation (Freitag and Roy, 2018) and machine translation (Yunsu Kim, 2018), DAE has shown to be able to learn represen- tations that are more robust to input noises and also generalize to unseen examples. Inspired by (Yunsu Kim, 2018), we directly in- ject three different types of noises into input sen- tence that are commonly encountered in real appli- cations.",
  "Inspired by (Yunsu Kim, 2018), we directly in- ject three different types of noises into input sen- tence that are commonly encountered in real appli- cations. 1) Deletion: We randomly delete 1% tokens from source sentences, for example, \u201ccat sat on the mat 7\u2192cat on the mat.\u201d 2) Insertion: We insert a random token into source sentences in 1% random positions, for example, \u201ccat sat on the mat 7\u2192cat sat on red the mat.\u201d 3) Reordering: We randomly swap 1% tokens in source sentences, and keep the distance between tokens being swapped within 5. \u201ccat sat on the mat 7\u2192mat sat on the cat.\u201d By introducing such noises into the input sen- tences while keeping the target sentences clean in training, our model can be more stable in generat- ing paraphrases and generalisable to unseen sen- tences in the training corpus.",
  "\u201ccat sat on the mat 7\u2192mat sat on the cat.\u201d By introducing such noises into the input sen- tences while keeping the target sentences clean in training, our model can be more stable in generat- ing paraphrases and generalisable to unseen sen- tences in the training corpus. The training objective with DAE becomes L(X,Y ) = L(X) + L(Y | \u02dcX)q( \u02dcX|X) = n X i=1 log P(xi|x1,...,i\u22121; \u03b8) + m X j=1 log P(yj|\u02dcx1,...,n, y1,...,j\u22121; \u03b8). (6) Once the model is trained, we generate para- phrases of a given sentence based on P(Y |X; \u03b8). 3 Experiments 3.1 Datasets We adopt the mixture of two multilingual trans- lation corpus as our training data: MultiUN (An- dreas Eisele, 2010) and OpenSubtitles (Pierre Li- son, 2016).",
  "3 Experiments 3.1 Datasets We adopt the mixture of two multilingual trans- lation corpus as our training data: MultiUN (An- dreas Eisele, 2010) and OpenSubtitles (Pierre Li- son, 2016). MultiUN consists of 463,406 of\ufb01cial documents in six languages, containing around 300M words for each language. OpenSubtitles is a corpus consisting of movie and TV subtitles, which contains 2.6B sentences over 60 languages. We select four shared languages of the two corpora: English, Spanish, Russian and Chinese. Statistics of the training corpus are shown in Table 1. Sen- tences are tokenized by Wordpiece as in BERT. A multilingual vocabulary of 50K tokens is used. For validation and testing, we randomly sample 10000 sentences respectively from each language pair. The rest data are used for training. For mono-",
  "Table 1: Statistics of training data (#sentences). En\u2194Es En\u2194Ru En\u2194Zh Es\u2194Ru Es\u2194Zh Ru\u2194Zh OpenSubtitles 11.7M 11.7M 11.2M 10.5M 8.5M 9.6M MultiUN 11.4M 11.7M 9.6M 10.6M 9.8M 9.6M Total 23.1M 23.4M 20.8M 21.1M 18.3M 19.2M (a) (b) (c) (d) Figure 3: Automatic evaluation: (a)(c) Distinct-2 versus Relevance; (b)(d) Inverse Self-BLEU versus Relevance. lingual pre-training, we use English Wikipedia1 corpus, which contains 2,500M words. 3.2 Experimental Settings We implement our model in Tensor\ufb02ow (Abadi et al., 2016). The size of our Transformer model is identical to BERT-base (Jacob Devlin, 2019).",
  "3.2 Experimental Settings We implement our model in Tensor\ufb02ow (Abadi et al., 2016). The size of our Transformer model is identical to BERT-base (Jacob Devlin, 2019). The model is constituted by 12 layers of Transformer blocks. Number of dimension of token embed- ding, position embedding and transformer hidden 1https://dumps.wikimedia.org/enwiki/ latest/ state are 768, while that of states in position-wise feed-forward networks are 3072. The number of attention heads is 12. Models are train using Adam optimization (Diederik P. Kingma) with a learn- ing rate up to 1e \u22124, \u03b21 = 0.9, \u03b22 = 0.999 and L2 weight decay of 0.01. We use top-k truncated random sampling strategy for inference that only sample from k candidate words with highest proba- bilities. Throughout our experiments, we train and evaluate two models for paraphrase generation: the bilingual model and the multilingual model.",
  "The bilingual models are trained only with English\u2194Chinese, while the multilingual models are trained with all the data between the four lan- guages. The round-trip translation baseline is based on the Transformer-based neural translation model. 3.3 Automatic Evaluation We evaluate the relevance between input and gen- erated paraphrase as well as the diversity among multiple generated paraphrases from the same in- put. For relevance, we use the cosine similarity be- tween the sentential representations (Chia-Wei Liu, 2016). Speci\ufb01cally, we use the Glove-840B embed- dings (Jeffrey Pennington, 2014) for word repre- sentation and Vector Extrema (Chia-Wei Liu, 2016) for sentential representation. For generation diver- sity, we employ two evaluation metrics: Distinct- 22 and inverse Self-BLEU (de\ufb01ned as: 1\u2212Self- BLEU) (Yaoming Zhu, 2018). Larger values of Distinct-2 and inverse Self-BLEU indicate higher diversity of the generation.",
  "Larger values of Distinct-2 and inverse Self-BLEU indicate higher diversity of the generation. For each model, we draw curves in Figure 3 with the aforementioned metrics as coordinates, and each data-point is obtained at a speci\ufb01c sampling temperature. Since a good paraphrasing model should generate both relevant and diverse para- phrases, the model with curve lying towards the up-right corner is regarded as with good perfor- mance. 3.3.1 Comparison with Baseline First we compare our models with the conventional pivoting method, i.e., round-trip translation. As shown in Figure 3 (a)(b), either the bilingual or the multilingual model is better than the baseline in terms of relevance and diversity in most cases. In other words, with the same generation diversity (measured by both Distinct-2 and Self-BLEU), our models can generate paraphrase with more seman- tically similarity to the input sentence. Note that in Figure 3 (a), there is a cross point between the curve of the bilingual model and the baseline curve when relevance is around 0.71.",
  "Note that in Figure 3 (a), there is a cross point between the curve of the bilingual model and the baseline curve when relevance is around 0.71. We particularly investigate generated paraphrases around this point and \ufb01nd that the baseline actually achieves better relevance when Distinct-2 is at a high level (>0.3). It means our bilingual model is semantically drifting faster than the baseline model as the Distinct-2 diversity increases. The round-trip 2https://github.com/ neural-dialogue-metrics/Distinct-N translation performs two-round of supervised trans- lations, while the zero-shot paraphrasing performs single-round unsupervised \u2018translation\u2019 (paraphras- ing). We suspect that the unsupervised paraphras- ing can be more sensitive to the decoding strategy. It also implies the latent, language-agnostic repre- sentation may be not well learned in our bilingual model. While on the other hand, our multilingual model alleviate this insuf\ufb01ciency. We further verify and analyze it as follows.",
  "It also implies the latent, language-agnostic repre- sentation may be not well learned in our bilingual model. While on the other hand, our multilingual model alleviate this insuf\ufb01ciency. We further verify and analyze it as follows. 3.3.2 Multilingual Models As mentioned above, our bilingual model can be unstable in some cases due to the lack of a well- learned language-agnostic semantic representation. A natural method is to introduce multilingual cor- pus, which consists of various translation direc- tions. Training over multilingual corpus forces the model to decouple the language type and semantic representation. Empirical results shows that our multilingual model performs signi\ufb01cantly better than the bilin- gual model. The red and blue curves in Figure 3 (a)(b) demonstrates a great improvement of our multilingual model over the bilingual model. In addition, the multilingual model also signi\ufb01cantly outperforms the baseline in the setting with the reasonable relevance scores. 3.3.3 Denoising Auto-Encoder To verify the effectiveness of DAE in our model, various experiments with different hyper- parameters were conducted.",
  "In addition, the multilingual model also signi\ufb01cantly outperforms the baseline in the setting with the reasonable relevance scores. 3.3.3 Denoising Auto-Encoder To verify the effectiveness of DAE in our model, various experiments with different hyper- parameters were conducted. We \ufb01nd that DAE works the best when uniformly perturbing input sentences with probability 0.01, using only Dele- tion and Reordering operations. We investigate DAE over both bilingual and multilingual models as plotted in Figure 3 (c)(d). Curves with the yellow circles represent models with DAE training. Results in the Figure 3 (c)(d) demonstrate posi- tive effects of DAE in either bilingual or multilin- gual models. It is worth to note that, while DAE have marginal impact on multilingual model, it im- proves bilingual model signi\ufb01cantly. This is an evi- dence indicating that DAE can improve the model in learning a more robust representation.",
  "It is worth to note that, while DAE have marginal impact on multilingual model, it im- proves bilingual model signi\ufb01cantly. This is an evi- dence indicating that DAE can improve the model in learning a more robust representation. More speci\ufb01cally, since Deletion forces model to focus on sentence-level semantics rather than word-level meaning while Reordering forces model to focus more on meaning rather than their posi- tions, it would be more dif\ufb01cult for a model to learn shortcuts (e.g. copy words). In other words, DAE",
  "improves models\u2019 capability in extracting deep se- mantic representation, which has a similar effect to introducing multilingual data. 3.3.4 Monolingual Pre-Training Table 2: Log-probabilities of the generated sentences. \u221aand \u00d7 symbols denote learning with or without pre- training respectively, bold font denotes greater values. Model Sampling Pre-Training Log-Prob Multilingual greedy, temp=1 \u221a -0.1427 \u00d7 -0.1428 top-3, temp=1 \u221a -0.1425 \u00d7 -0.1448 top-3, temp=1.5 \u221a -0.1420 \u00d7 -0.1425 Bilingual greedy, temp=1 \u221a -0.1472 \u00d7 -0.1484 top-3, temp=1 \u221a -0.1487 \u00d7 -0.1502 top-3, temp=1.5 \u221a -0.1461 \u00d7 -0.1506 As shown in Figure 3 (a)(b), the model with lan- guage model pre-training almost performs equally to its contemporary without pre-training. However, evaluations on \ufb02uency uncover the value of pre- training.",
  "However, evaluations on \ufb02uency uncover the value of pre- training. We evaluate a group of models over our test set in terms of \ufb02uency, using a n-grams lan- guage model3 trained on 14k public domain books. As depicted in Table 2, models with language model pre-training stably achieves greater log- probabilities than the model without pre-training. Namely, language model pre-training brings better \ufb02uency. 3.4 Human Evaluation 200 sentences are sampled from our test set for human evaluation. The human evaluation guid- ance generally follows that of (Zichao Li, 2018) but with a compressed scoring range from [1, 5] to [1, 4]. We recruit \ufb01ve human annotators to evalu- ate models in semantic relevance and \ufb02uency. A test example consists of one input sentence, one generated sentence from baseline model and one generated sentence from our model. We randomly permute a pair of generated sentences to reduce annotators\u2019 bias on a certain model. Each example is evaluated by two annotators.",
  "A test example consists of one input sentence, one generated sentence from baseline model and one generated sentence from our model. We randomly permute a pair of generated sentences to reduce annotators\u2019 bias on a certain model. Each example is evaluated by two annotators. As shown in Table 3, our method outperforms the baseline in both relevance and \ufb02uency signif- icantly. We further calculate agreement (Cohen\u2019s kappa) between two annotators. 3http://www.openslr.org/11/ Table 3: Human evaluation results. Model Relevance Fluency Agreement Round-trip 2.72 3.61 0.36 Multilingual (ours) 3.43 3.75 0.35 Both round-trip translation and our method per- forms well as to \ufb02uency. But the huge gap of rele- vance between the two systems draw much atten- tion of us. We investigate the test set in details and \ufb01nd that round-trip approach indeed generate more noise as shown in case studies. 3.5 Case Studies We further study some generated cases from dif- ferent models.",
  "We investigate the test set in details and \ufb01nd that round-trip approach indeed generate more noise as shown in case studies. 3.5 Case Studies We further study some generated cases from dif- ferent models. All results in Table 4 are generated over our test set using randomly sampling. For both baseline and multilingual model, we tune their sam- pling temperatures to control the Distinct-2 and the inverse Self-BLEU at 0.31 and 0.47 respectively. In the case studies, we \ufb01nd that our method usually generates sentences with better relevance to source inputs, while the round-trip translation method can sometimes run into serious semantic drift. In the second case, our model demonstrates a good feature that it maintains the meaning and even a proper noun guide unchanged while modi\ufb01es the source sentence by both changing and reordering words. This feature may be introduced by DAE perturbation strategies which improves model\u2019s ro- bustness and diversity simultaneously. These re- sults evidence that our methods outperforms the baseline in both relevance and diversity.",
  "This feature may be introduced by DAE perturbation strategies which improves model\u2019s ro- bustness and diversity simultaneously. These re- sults evidence that our methods outperforms the baseline in both relevance and diversity. 4 Related Work Generating paraphrases based on deep neural net- works, especially Seq2Seq models, has become the mainstream approach. A majority of neu- ral paraphrasing models tried to improve gener- ation quality and diversity with high-quality para- phrase corpora. Prakash et al. (2016) starts a deep learning line of paraphrase generation through in- troducing stacked residual LSTM network. A word constraint model proposed by Ziqiang Cao (2017) improves both generation quality and diver- sity. Ankush Gupta (2018) adopts variational auto- encoder to further improve generation diversity. Zichao Li (2018) utilize neural reinforcement learn- ing and adversarial training to promote generation quality. Zichao Li (2019) decompose paraphrase generation into phrase-level and sentence-level.",
  "Table 4: Case studies. For each input source, we randomly sample three paraphrases for comparison. Source I guess I kinda felt insigni\ufb01cant. Round-trip I think I just don\u2019t feel right about that. I guess I\u2019m a little uncomfortable. I think I\u2019m a little bit of a problem right now. Multilingual (ours) I guess I was feeling a bit unsigni\ufb01cant. I guess I felt some sorts of insigni\ufb01cant. I guess I kind of felt insigni\ufb01cant. Source This site will make better use of the guide and will increase its distribution. Round-trip The site would make better use of the guidelines and would be expanded. The site will make the best use of guides and expand them. This site would have made use more of the guidelines and could be expanded to its distribution. Multilingual (ours) This web site will make better use of the guide and will increase its dissemination. This site will better utilize the guide, and will improve its distribution. The web site is going to make the guide\u2019s use more ef\ufb01cient and its distribution will grow.",
  "Multilingual (ours) This web site will make better use of the guide and will increase its dissemination. This site will better utilize the guide, and will improve its distribution. The web site is going to make the guide\u2019s use more ef\ufb01cient and its distribution will grow. Source That\u2019s how eric got the passcodes. Round-trip Then eric has a code. Then eric has the codes. Then erik\u2019ll have the codes. Multilingual (ours) That\u2019s the way eric got the password codes. That\u2019s how eric got passwords. That\u2019s where eric gets the passcodes. Several works tried to generate paraphrases from monolingual non-parallel or translation cor- pora. Lilin Zhang (2016) exploits Markov Net- work model to extract paraphrase tables from monolingual corpus.",
  "That\u2019s where eric gets the passcodes. Several works tried to generate paraphrases from monolingual non-parallel or translation cor- pora. Lilin Zhang (2016) exploits Markov Net- work model to extract paraphrase tables from monolingual corpus. Quirk, Brockett, and Dolan (2004), Wubben, van den Bosch, and Krahmer (2010) and Wubben, van den Bosch, and Krah- mer (2014) create paraphrase corpus through clus- tering and aligning paraphrases from crawled ar- ticles or headlines. With parallel translation cor- pora, pivoting approaches such round-trip trans- lation (Jonathan Mallinson and Lapata, 2017) and back-translation (John Wieting, 2018) are explored. However, to the best knowledge of us, none of these paraphrase generation models has been trained directly from parallel translation corpora as a single-round end-to-end model. 5 Conclusions In this work, we have proposed a Transformer- based model for zero-shot paraphrase generation, which can leverage huge amount of off-the-shelf translation corpora.",
  "5 Conclusions In this work, we have proposed a Transformer- based model for zero-shot paraphrase generation, which can leverage huge amount of off-the-shelf translation corpora. Moreover, we improve gener- ation \ufb02uency of our model with language model pre-training. Empirical results from both automatic and human evaluation demonstrate that our model surpasses the conventional pivoting approaches in terms of relevance, diversity, \ufb02uency and ef\ufb01ciency. Nevertheless, there are some interesting directions to be explored. For instance, how to obtain a bet- ter latent semantic representation with multi-modal data and how to further improve the generation di- versity without sacri\ufb01cing relevance. We plan to strike these challenging yet valuable problems in the future.",
  "Nevertheless, there are some interesting directions to be explored. For instance, how to obtain a bet- ter latent semantic representation with multi-modal data and how to further improve the generation di- versity without sacri\ufb01cing relevance. We plan to strike these challenging yet valuable problems in the future. References Abadi, M.; Barham, P.; Chen, J.; Chen, Z.; Davis, A.; Dean, J.; Devin, M.; Ghemawat, S.; Irving, G.; Isard, M.; Kudlur, M.; Levenberg, J.; Monga, R.; Moore, S.; Murray, D. G.; Steiner, B.; Tucker, P.; Vasude- van, V.; Warden, P.; Wicke, M.; Yu, Y.; and Zheng, X. 2016. Tensor\ufb02ow: A system for large-scale ma- chine learning. In 12th USENIX Symposium on Op- erating Systems Design and Implementation (OSDI 16), 265\u2013283. Andreas Eisele, Y. C. 2010. Multiun: A multilingual corpus from united nation documents.",
  "In 12th USENIX Symposium on Op- erating Systems Design and Implementation (OSDI 16), 265\u2013283. Andreas Eisele, Y. C. 2010. Multiun: A multilingual corpus from united nation documents. In LREC. Ankush Gupta, Arvind Agarwal, P. S. P. R. 2018. A deep generative framework for paraphrase genera- tion. In AAAI. Arivazhagan, N.; Bapna, A.; Firat, O.; Aharoni, R.; Johnson, M.; and Macherey, W. 2019. The miss- ing ingredient in zero-shot neural machine transla- tion. arXiv preprint arXiv:1903.07091. Ashish Vaswani, Noam Shazeer, N. P. J. U. L. J. A. N. G. L. K. I. P. 2017. Attention is all you need. In NIPS.",
  "Ashish Vaswani, Noam Shazeer, N. P. J. U. L. J. A. N. G. L. K. I. P. 2017. Attention is all you need. In NIPS. Chia-Wei Liu, Ryan Lowe, I. V. S. M. N. L. C. J. P. 2016. How not to evaluate your dialogue system: An empirical study of unsupervised evaluation met- rics for dialogue response generation. In EMNLP. Diederik P. Kingma, J. B. Adam: A method for stochastic optimization. In ICLR.",
  "Dong, L.; Yang, N.; Wang, W.; Wei, F.; Liu, X.; Wang, Y.; Gao, J.; Zhou, M.; and Hon, H.-W. 2019. Uni\ufb01ed language model pre-training for natural lan- guage understanding and generation. arXiv preprint arXiv:1905.03197. Freitag, M., and Roy, S. 2018. Unsupervised natural language generation with denoising autoencoders. In EMNLP. Ilya Sutskever, Oriol Vinyals, Q. V. L. 2014. Sequence to sequence learning with neural networks. In NIPS. Jacob Devlin, Ming-Wei Chang, K. L. K. T. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL. Jeffrey Pennington, Richard Socher, C. D. M. 2014. Glove: Global vectors for word representation. In EMNLP. Jiatao Gu, Yong Wang, K. C. V. O. L.",
  "In NAACL. Jeffrey Pennington, Richard Socher, C. D. M. 2014. Glove: Global vectors for word representation. In EMNLP. Jiatao Gu, Yong Wang, K. C. V. O. L. 2019. Im- proved zero-shot neural machine translation via ig- noring spurious correlations. In ACL. John Wieting, K. G. 2018. Paranmt-50m - pushing the limits of paraphrastic sentence embeddings with millions of machine translations. In ACL. Jonathan Mallinson, R. S., and Lapata, M. 2017. Para- phrasing revisited with neural machine translation. In EACL. Lample, G., and Conneau, A. 2019. Cross- lingual language model pretraining. arXiv preprint arXiv:1901.07291. Lilin Zhang, Zhen Weng, W. X. J. W. Z. C. Y. T. M. L. M. W. 2016.",
  "arXiv preprint arXiv:1901.07291. Lilin Zhang, Zhen Weng, W. X. J. W. Z. C. Y. T. M. L. M. W. 2016. Extract domain-speci\ufb01c paraphrase from monolingual corpus for automatic evaluation of machine translation. In MT. Pascal Vincent, Hugo Larochelle, Y. B. P.-A. M. 2008. Extracting and composing robust features with de- noising autoencoders. In ICML. Peter J. Liu, Mohammad Saleh, E. P.-B. G. R. S. L. K. N. S. 2018. Generating wikipedia by summarizing long sequences. In ICLR. Pierre Lison, J. T. 2016. Opensubtitles2016: Extract- ing large parallel corpora from movie and tv subti- tles. In LREC. Prakash, A.; Hasan, S. A.; Lee, K.; Datla, V.; Qadir, A.; Liu, J.; and Farri, O.",
  "In LREC. Prakash, A.; Hasan, S. A.; Lee, K.; Datla, V.; Qadir, A.; Liu, J.; and Farri, O. 2016. Neural paraphrase gener- ation with stacked residual LSTM networks. In Pro- ceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Techni- cal Papers, 2923\u20132934. Osaka, Japan: The COL- ING 2016 Organizing Committee. Quirk, C.; Brockett, C.; and Dolan, W. 2004. Monolin- gual machine translation for paraphrase generation. In EMNLP, 142\u2013149. Barcelona, Spain: Associa- tion for Computational Linguistics. Radford, A., and Sutskever, I. 2018. Improving lan- guage understanding by generative pre-training. In arxiv. Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and Sutskever, I. 2019.",
  "2018. Improving lan- guage understanding by generative pre-training. In arxiv. Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and Sutskever, I. 2019. Language models are unsu- pervised multitask learners. OpenAI Blog 1(8). Sestorain, L.; Ciaramita, M.; Buck, C.; and Hofmann, T. 2018. Zero-shot dual machine translation. arXiv preprint arXiv:1805.10338. Song, K.; Tan, X.; Qin, T.; Lu, J.; and Liu, T.-Y. 2019. Mass: Masked sequence to sequence pre- training for language generation. arXiv preprint arXiv:1905.02450. Wubben, S.; van den Bosch, A.; and Krahmer, E. 2010. Paraphrase generation as monolingual trans- lation: Data and evaluation. In INLG, INLG \u201910, 203\u2013207.",
  "Wubben, S.; van den Bosch, A.; and Krahmer, E. 2010. Paraphrase generation as monolingual trans- lation: Data and evaluation. In INLG, INLG \u201910, 203\u2013207. Stroudsburg, PA, USA: Association for Computational Linguistics. Wubben, S.; van den Bosch, A.; and Krahmer, E. 2014. Creating and using large monolingual parallel cor- pora for sentential paraphrase generation. In LREC, 4292\u20134299. Reykjavik, Iceland: European Lan- guages Resources Association (ELRA). Yaoming Zhu, Sidi Lu, L. Z. J. G. W. Z. J. W. Y. Y. 2018. Texygen: A benchmarking platform for text generation models. In SIGIR. Yunsu Kim, Jiahui Geng, H. N. 2018. Improving un- supervised word-by-word translation with language model and denoising autoencoder. In EMNLP.",
  "In SIGIR. Yunsu Kim, Jiahui Geng, H. N. 2018. Improving un- supervised word-by-word translation with language model and denoising autoencoder. In EMNLP. Zichao Li, Xin Jiang, L. S. H. L. 2018. Paraphrase generation with deep reinforcement learning. In EMNLP. Zichao Li, Xin Jiang, L. S. Q. L. 2019. Decomposable neural paraphrase generation. In ACL. Ziqiang Cao, Chuwei Luo, W. L. S. L. 2017. Joint copying and restricted generation for paraphrase. In AAAI."
]