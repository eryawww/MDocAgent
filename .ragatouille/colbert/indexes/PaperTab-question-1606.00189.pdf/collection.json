[
  "Neural Network Translation Models for Grammatical Error Correction Shamil Chollampatt1 and Kaveh Taghipour2 and Hwee Tou Ng1,2 1NUS Graduate School for Integrative Sciences and Engineering 2Department of Computer Science National University of Singapore shamil@u.nus.edu, {kaveh, nght}@comp.nus.edu.sg Abstract Phrase-based statistical machine translation (SMT) systems have previously been used for the task of grammatical error correction (GEC) to achieve state-of-the-art accuracy. The superiority of SMT systems comes from their ability to learn text trans- formations from erroneous to corrected text, with- out explicitly modeling error types. However, phrase-based SMT systems suffer from limitations of discrete word representation, linear mapping, and lack of global context. In this paper, we address these limitations by using two different yet comple- mentary neural network models, namely a neural network global lexicon model and a neural network joint model. These neural networks can generalize better by using continuous space representation of words and learn non-linear mappings. Moreover, they can leverage contextual information from the source sentence more effectively.",
  "These neural networks can generalize better by using continuous space representation of words and learn non-linear mappings. Moreover, they can leverage contextual information from the source sentence more effectively. By adding these two components, we achieve statistically signi\ufb01- cant improvement in accuracy for grammatical er- ror correction over a state-of-the-art GEC system. 1 Introduction Grammatical error correction (GEC) is a challenging task due to the variability of the type of errors and the syntactic and se- mantic dependencies of the errors on the surrounding context. Most of the grammatical error correction systems use clas- si\ufb01cation and rule-based approaches for correcting speci\ufb01c error types. However, these systems use several linguistic cues as features. The standard linguistic analysis tools like part-of-speech (POS) taggers and parsers are often trained on well-formed text and perform poorly on ungrammatical text. This introduces further errors and limits the perfor- mance of rule-based and classi\ufb01cation approaches to GEC.",
  "The standard linguistic analysis tools like part-of-speech (POS) taggers and parsers are often trained on well-formed text and perform poorly on ungrammatical text. This introduces further errors and limits the perfor- mance of rule-based and classi\ufb01cation approaches to GEC. As a consequence, the phrase-based statistical machine trans- lation (SMT) approach to GEC has gained popularity because of its ability to learn text transformations from erroneous text to correct text from error-corrected parallel corpora without any additional linguistic information. They are also not lim- ited to speci\ufb01c error types. Currently, many state-of-the-art GEC systems are based on SMT or use SMT components for error correction [Susanto et al., 2014; Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014]. In this paper, grammatical error correction includes correcting errors of all types, including word choice errors and collocation errors which constitute a large class of learners\u2019 errors. We model our GEC system based on the phrase-based SMT approach.",
  "In this paper, grammatical error correction includes correcting errors of all types, including word choice errors and collocation errors which constitute a large class of learners\u2019 errors. We model our GEC system based on the phrase-based SMT approach. However, traditional phrase-based SMT sys- tems treat words and phrases as discrete entities. We take advantage of continuous space representation by adding two neural network components that have been shown to improve SMT systems [Ha et al., 2014; Devlin et al., 2014]. These neural networks are able to capture non-linear relationships between source and target sentences and can encode contex- tual information more effectively. Our experiments show that the addition of these two neural networks leads to signi\ufb01cant improvements over a strong baseline and outperforms the cur- rent state of the art. 2 Related Work In the past decade, there has been increasing attention on grammatical error correction in English, mainly due to the growing number of English as Second Language (ESL) learn- ers around the world.",
  "2 Related Work In the past decade, there has been increasing attention on grammatical error correction in English, mainly due to the growing number of English as Second Language (ESL) learn- ers around the world. The popularity of this problem in natu- ral language processing research grew further through Help- ing Our Own (HOO) and the CoNLL shared tasks [Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; 2014]. Most published work in GEC aimed at building spe- ci\ufb01c classi\ufb01ers for different error types and then use them to build hybrid systems [Dahlmeier et al., 2012; Rozovskaya et al., 2014]. One of the \ufb01rst approaches of using SMT for GEC focused on correction of countability errors of mass nouns (e.g., many informations\u2192much information) [Brock- ett et al., 2006]. They had to use an arti\ufb01cially constructed parallel corpus for training their SMT system.",
  "They had to use an arti\ufb01cially constructed parallel corpus for training their SMT system. Later, the availability of large-scale error corrected data [Mizumoto et al., 2011] further improved SMT-based GEC systems. Recently, continuous space representations of words and phrases have been incorporated into SMT systems via neural networks. Speci\ufb01cally, addition of monolingual neural net- work language models [Bengio et al., 2003; Vaswani et al., 2013], neural network joint models (NNJM) [Devlin et al., 2014], and neural network global lexicon models (NNGLM) [Ha et al., 2014] have been shown to be useful for SMT. Neu- ral networks have been previously used for GEC as a lan- guage model feature in the classi\ufb01cation approach [Wu et al., arXiv:1606.00189v1  [cs.CL]  1 Jun 2016",
  "2014] and as a classi\ufb01er for article error correction [Sun et al., 2015]. Recently, a neural machine translation approach has been proposed for GEC [Yuan and Briscoe, 2016]. This method uses a recurrent neural network to perform sequence- to-sequence mapping from erroneous to well-formed sen- tences. Additionally, it relies on a post-processing step based on statistical word-based translation models to replace out- of-vocabulary words. In this paper, we investigate the effec- tiveness of two neural network models, NNGLM and NNJM, in SMT-based GEC. To the best of our knowledge, there is no prior work that uses these two neural network models for SMT-based GEC. 3 A Machine Translation Framework for Grammatical Error Correction In this paper, the task of grammatical error correction is for- mulated as a translation task from the language of \u2018bad\u2019 En- glish to the language of \u2018good\u2019 English. That is, the source sentence is written by a second language learner and poten- tially contains grammatical errors, whereas the target sen- tence is the corrected \ufb02uent sentence.",
  "That is, the source sentence is written by a second language learner and poten- tially contains grammatical errors, whereas the target sen- tence is the corrected \ufb02uent sentence. We use a phrase-based machine translation framework [Koehn et al., 2003] for trans- lation, which employs a log-linear model to \ufb01nd the best translation T \u2217given a source sentence S. The best transla- tion is selected according to the following equation: T \u2217= argmax T P(T|S) = argmax T N X i=1 \u03bbihi(T, S) where N is the number of features, hi and \u03bbi are the ith fea- ture function and feature weight, respectively. We make use of the standard features used in phrase-based translation with- out any reordering, leading to monotone translations. The features can be broadly categorized as translation model and language model features. The translation model in the phrase- based machine translation framework is trained using par- allel data, i.e., sentence-aligned erroneous source text and corrected target text.",
  "The features can be broadly categorized as translation model and language model features. The translation model in the phrase- based machine translation framework is trained using par- allel data, i.e., sentence-aligned erroneous source text and corrected target text. The translation model is responsible for \ufb01nding the best transformation of the source sentence to produce the corrected sentence. On the other hand, the lan- guage model is trained on well-formed English text and this ensures the \ufb02uency of the corrected text. To \ufb01nd the opti- mal feature weights (\u03bb), we use minimum error rate training (MERT), maximizing the F0.5 measure on the development set [Junczys-Dowmunt and Grundkiewicz, 2014]. The F0.5 measure [Dahlmeier and Ng, 2012], which weights precision twice as much as recall, is the evaluation metric widely used for GEC and was the of\ufb01cial evaluation metric adopted in the CoNLL 2014 shared task [Ng et al., 2014].",
  "Additionally, we augment the feature set by adding two neural network translation models, namely a neural network global lexicon model [Ha et al., 2014] and a neural network joint model [Devlin et al., 2014]. These models are described in detail in Sections 4 and 5. 4 Neural Network Global Lexicon Model A global lexicon model is used to predict the presence of words in the corrected output. The model estimates the over- [P(t|S)] O1 \u015c Figure 1: A single hidden layer neural network global lexicon model all probability of a target hypothesis (i.e., a candidate cor- rected sentence) given the source sentence, by making use of the probability computed for each word in the hypothe- sis. The individual word probabilities can be computed by training density estimation models such as maximum entropy [Mauser et al., 2009] or probabilistic neural networks [Ha et al., 2014]. Following [Ha et al., 2014], we formulate our global lexicon model using a feed-forward neural network. The model and the training algorithm are described below.",
  "Following [Ha et al., 2014], we formulate our global lexicon model using a feed-forward neural network. The model and the training algorithm are described below. 4.1 Model The probability of a target hypothesis is computed using the following equation: P(T|S) \u2248 |T | Y i=1 P(ti|S) (1) where S and T are the source sentence and the target hy- pothesis respectively, and |T| denotes the number of words in the target hypothesis. P(ti|S) is the probability of the target word ti given the source sentence S. P(ti|S) is the output of the neural network. The architecture of the neural network is shown in Figure 1. P(ti|S) is calculated by: P(ti|S) = \u03c3i(W2 \u00b7 O1 + b2) where O1 is the hidden layer output, and W2 and b2 are the output layer weights and biases respectively. \u03c3i is the element-wise sigmoid function which scales the output to (0, 1).",
  "\u03c3i is the element-wise sigmoid function which scales the output to (0, 1). O1 is computed by the following equation: O1 = \u03c6(W1 \u00b7 \u02c6S + b1) where \u03c6 is the activation function, and W1 and b1 are the hidden layer weights and biases applied on a binary bag-of- words representation of the input sentence denoted by \u02c6S. The size of \u02c6S is equal to the size of the source vocabulary |Vs| and each element indicates the presence or absence (denoted by 1 or 0 respectively) of a given source word. 4.2 Training The model is trained using mini-batch gradient descent with back-propagation. We use binary cross entropy (Equation 2)",
  "as the cost function: E = \u22121 |Vt| |Vt| X i=1 h \u02c6Ti log p(ti|S) + (1 \u2212\u02c6Ti) log(1 \u2212p(ti|S)) i (2) where \u02c6T refers to the binary bag-of-words representation of the reference target sentence, and Vt is the target vocabulary. Each mini-batch is composed of a \ufb01xed number of sentence pairs (S, T). The training algorithm repeatedly minimizes the cost function calculated for a given mini-batch by updating the parameters according to the gradients. 4.3 Rescaling Since the prior probability of observing a particular word in a sentence is usually a small number, the probabilistic out- put of NNGLM can be biased towards zero. This bias can hurt the performance of our system and therefore, we try to alleviate this problem by rescaling the output after training NNGLM. Our solution is to map the output probabilities to a new probability space by \ufb01tting a logistic function on the out- put.",
  "This bias can hurt the performance of our system and therefore, we try to alleviate this problem by rescaling the output after training NNGLM. Our solution is to map the output probabilities to a new probability space by \ufb01tting a logistic function on the out- put. Formally, we use Equation 3 as the mapping function: Q(ti|S) = 1 1 + exp(\u2212w \u00b7 P(ti|S) \u2212b) (3) where Q(ti|S) is the rescaled probability and w and b are the parameters. For each sentence pair (S, T) in the development set, we collect training instances of the form (x, y) for every word t in the target vocabulary, where x = P(t|S) and y \u2208 {0, 1}.",
  "For each sentence pair (S, T) in the development set, we collect training instances of the form (x, y) for every word t in the target vocabulary, where x = P(t|S) and y \u2208 {0, 1}. The value of y is set according to the presence (y = 1) or absence (y = 0) of the word t in the target sentence T. We use weighted cross entropy loss function with L2- regularization to train w and b on the development set: E = \u2212 M X j=1 [c1yj log p(xj) + c0(1 \u2212yj) log(1 \u2212p(xj))] Here, M is the number of training samples, p(x) is the prob- ability of x computed by p(x) = 1/(1+exp(\u2212wx\u2212b)), and c0 and c1 are the weights assigned to the two classes y = 0 and y = 1, respectively.",
  "In order to balance the two classes, we weight each class inversely proportional to class frequen- cies in the training data (Equation 4) to put more weight on the less frequent class: c0 = M/(2f0), c1 = M/(2f1) (4) In Equation 4, f0 and f1 are the number of samples in each class. After training the rescaling model, we use w and b to calculate Q(ti|S) according to Equation 3. Finally, we use Q(ti|S) instead of P(ti|S) in Equation 1. 5 Neural Network Joint Model Joint models in translation augment the context information in language models with words from the source sentence. A neural network joint model (NNJM) [Devlin et al., 2014] uses a neural network to model the word probabilities given a con- text composed of source and target words.",
  "A neural network joint model (NNJM) [Devlin et al., 2014] uses a neural network to model the word probabilities given a con- text composed of source and target words. NNJM can scale [P(t|h)]  O1 O0 h Figure 2: A single hidden layer neural network joint model up to large order of n-grams and still perform well because of its ability to capture semantic information through continuous space representations of words and to learn non-linear rela- tionship between source and target words. Unlike the global lexicon model, NNJM uses a \ufb01xed window from the source side and take sequence information of words into consider- ation in order to estimate the probability of the target word. The model and the training method are described below. 5.1 Model The probability of the target hypothesis T given the source sentence S is estimated by the following equation: P(T|S) \u2248 |T | Y i=1 P(ti|hi) (5) where |T| is the number of words in the target sentence, ti is the ith target word, and hi is the context (history) for the target word ti.",
  "The context hi consists of a set of m source words represented by (sai\u2212m\u22121 2 , \u00b7 \u00b7 \u00b7 , sai+ m\u22121 2 ) and n \u22121 words preceding ti from the target sentence represented by (ti\u2212n+1, \u00b7 \u00b7 \u00b7 , ti\u22121). The context words from the source side are the words in the window of size m surrounding the source word sai that is aligned to the target word ti. The output of the neural network P(ti|hi) is the output of the \ufb01nal softmax layer which is given by the following equation: P(ti|hi) = 1 Z(hi) exp Ui(hi) (6) where Ui(hi) is the output of the neural network before ap- plying softmax and Z(hi) is given by following equation: Z(hi) = |Vo| X i\u2032=1 exp Ui\u2032(hi) The output of the neural network before softmax is computed by applying output layer weights W2 and biases b2 to the hid- den layer output O1. U(hi) = W2 \u00b7 O1 + b2",
  "O1 is computed by applying weights W1 and biases b1 on the hidden layer input O0 and using a non-linear activation function \u03c6: O1 = \u03c6(W1 \u00b7 O0 + b1) The input to the hidden layer (O0) is a concatenated vector of context word embeddings: O0 = (Es \u00b7 \u02c6sai\u2212m\u22121 2 , \u00b7 \u00b7 \u00b7 , Es \u00b7 \u02c6sai+ m\u22121 2 , Et \u00b7 \u02c6ti\u2212n+1, \u00b7 \u00b7 \u00b7 , Et \u00b7 \u02c6ti\u22121) where \u02c6s and \u02c6t are the one-hot representations of the source word s and the target word t, respectively. Similarly, Es and Et are the word embeddings matrices for the source words and the target words. As we use log probabilities instead of raw probabilities in our GEC system, Equation 5 can be rewritten as the follow- ing: log P(ti|hi) = Ui(hi) \u2212log Z(hi) (7) Finally, since the network is trained by Noise Contrastive Es- timation (NCE) (described in Section 5.2), it becomes self- normalized.",
  "This means that Z(hi) will be approximately 1 and hence the raw output of the neural network Ui can be directly used as the log probabilities during decoding. 5.2 Training To avoid the costly softmax layer and thereby speed up both training and decoding, we use Noise Contrastive Estimation (NCE) following [Vaswani et al., 2013]. During training, the negative log likelihood cost function is modi\ufb01ed to a proba- bilistic binary classi\ufb01er, which learns to discriminate between the actual target word and k random words (noisy samples) per training instance selected from a noise distribution q. The two classes are C = 1 indicating that the word is the target word and C = 0 indicating that the word is a noisy sample.",
  "The conditional probabilities for C = 0 and C = 1 given a target word and context is given by: P(C = 1|ti, hi) = 1 k+1P(ti|hi) 1 k+1P(ti|hi) + k k+1q(ti) P(C = 0|ti, hi) = k k+1q(ti) 1 k+1P(ti|hi) + k k+1q(ti) where, P(ti|hi) is the model probability given in Equation 6. The negative log likelihood cost function is replaced by the following function. L = \u2212 X i \uf8ee \uf8f0log P(C = 1|ti, hi) + k X j=1 log P(C = 0|\u00aftij, hi) \uf8f9 \uf8fb where \u00aftij refers to the jth noise sample for the target word ti. Z(hi) is required for the computation of the neural network output, P(ti|hi). However, setting the term Z(hi) to 1 during training forces the output of the neural network to be self- normalized.",
  "Z(hi) is required for the computation of the neural network output, P(ti|hi). However, setting the term Z(hi) to 1 during training forces the output of the neural network to be self- normalized. Hence, Equation 7 reduces to: log P(ti|hi) \u2248Ui(hi) (8) Using Equation 8 avoids the expensive softmax computa- tion in the \ufb01nal layer and consequently speeds up decoding. 6 Experiments We describe our experimental setup including the description of the data we used, the con\ufb01guration of our baseline sys- tem and the neural network components, and the evaluation method in Section 6.1, followed by the results and discussion in Section 6.2 6.1 Setup We use the popular phrase-based machine translation toolkit Moses1 as our baseline SMT system. NUCLE [Dahlmeier et al., 2013], which is the of\ufb01cial training data for the CoNLL 2013 and 2014 shared tasks, is used as the parallel text for training.",
  "NUCLE [Dahlmeier et al., 2013], which is the of\ufb01cial training data for the CoNLL 2013 and 2014 shared tasks, is used as the parallel text for training. Additionally, we obtain parallel corpora from Lang- 8 Corpus of Learner English v1.0 [Mizumoto et al., 2011], which consists of texts written by ESL (English as Second Language) learners on the language learning platform Lang- 82. We use the test data for the CoNLL 2013 shared task as our development data. The statistics of the training and development data are given in Table 1. Source side refers to the original text written by the ESL learners and target side refers to the corresponding corrected text hand-corrected by humans. The source side and the target side are sentence- aligned and tokenized. Dataset No. of sentences No. of source side tokens No.",
  "Source side refers to the original text written by the ESL learners and target side refers to the corresponding corrected text hand-corrected by humans. The source side and the target side are sentence- aligned and tokenized. Dataset No. of sentences No. of source side tokens No. of target side tokens NUCLE 57,151 1,161,567 1,155,559 Lang-8 v1.0 1,114,139 12,945,666 13,232,058 CoNLL 2013 1,381 29,207 28,743 Table 1: Statistics of training and development data We train the translation model for our SMT system using a concatenation of NUCLE and Lang-8 v1.0 parallel data. The training data is cleaned up by removing sentence pairs in which either the source or the target sentence is empty, or is too long (greater than 80 tokens), or violate a 9:1 sen- tence ratio limit. The translation model uses the default fea- tures in Moses which include the forward and inverse phrase translation probabilities, forward and inverse lexical weights, word penalty, and phrase penalty.",
  "The translation model uses the default fea- tures in Moses which include the forward and inverse phrase translation probabilities, forward and inverse lexical weights, word penalty, and phrase penalty. We compute the phrase alignments using standard tools in Moses. We use two lan- guage model features: a 5-gram language model trained us- ing the target side of NUCLE used for training the translation model and a 5-gram language model trained using English Wikipedia (\u223c1.78 billion tokens). Both language models are estimated with KenLM3 using modi\ufb01ed Kneser-Ney smooth- ing. We use MERT for tuning the feature weights by opti- mizing the F0.5 measure (which weights precision twice as much as recall). This system constitutes our baseline sys- tem in Table 2. Our baseline system uses exactly the same training data as [Susanto et al., 2014] for training the transla- tion model and the language model. The difference between 1https://github.com/moses-smt/mosesdecoder/tree/fscorer 2http://lang-8.com/ 3https://khea\ufb01eld.com/code/kenlm/",
  "our baseline system and the SMT components of [Susanto et al., 2014] is that we tune with F0.5 instead of BLEU and we use the standard Moses con\ufb01guration without the Levenshtein distance feature. On top of our baseline system described above, we incor- porate the two neural network components, neural network global lexicon model (NNGLM) and neural network joint model (NNJM) as features. Both NNGLM and NNJM are trained using the parallel data used to train the translation model of our baseline system. We implement NNGLM using the Theano library4 in Python in order to make use of parallelization with GPUs, thus speeding up training signi\ufb01cantly. We use a source and target vocabulary of 10,000 most frequent words on both sides. We use a single hidden layer neural network with 2,000 hidden nodes. We use tanh as the activation function for the hidden layer. We optimize the model weights by stochastic gradient descent using a mini-batch size of 100 and a learn- ing rate5 of 10. We train the model for 45 epochs.",
  "We use tanh as the activation function for the hidden layer. We optimize the model weights by stochastic gradient descent using a mini-batch size of 100 and a learn- ing rate5 of 10. We train the model for 45 epochs. The lo- gistic regression function for rescaling is trained using the probabilities obtained from this model on the development set. To speed up tuning and decoding, we pre-compute the probabilities of target words using the source side sentences of the development and the test sets, respectively. We imple- ment a feature function in Moses to compute the probability of a target hypothesis given the source sentence using the pre- computed probabilities. To train NNJM, we use the publicly available imple- mentation, Neural Probabilistic Language Model (NPLM) [Vaswani et al., 2013]. The latest version of Moses can in- corporate NNJM trained using NPLM as a feature while de- coding. Similar to NNGLM, we use the parallel text used for training the translation model in order to train NNJM. We use a source context window size of 5 and a target context window size of 4.",
  "Similar to NNGLM, we use the parallel text used for training the translation model in order to train NNJM. We use a source context window size of 5 and a target context window size of 4. We select a source context vocabulary of 16,000 most frequent words from the source side. The target context vocabulary and output vocabulary is set to the 32,000 most frequent words. We use a single hidden layer to speed up training and decoding with an input embedding dimension of 192 and 512 hidden layer nodes. We use recti\ufb01ed linear units (ReLU) as the activation function. We train NNJM with noise contrastive estimation with 100 noise samples per train- ing instance, which are obtained from a unigram distribution. The neural network is trained for 30 epochs using stochastic gradient descent optimization with a mini-batch size of 128 and learning rate of 0.1. We conduct experiments by incorporating NNGLM and NNJM both independently and jointly into our baseline sys- tem. The results of our experiments are described in Sec- tion 6.2.",
  "We conduct experiments by incorporating NNGLM and NNJM both independently and jointly into our baseline sys- tem. The results of our experiments are described in Sec- tion 6.2. The evaluation is performed similar to the CoNLL 2014 shared task setting using the the of\ufb01cial test data of the CoNLL 2014 shared task with annotations from two anno- tators (without considering alternative annotations suggested by the participating teams). The test dataset consists of 1,312 error-annotated sentences with 30,144 tokens on the source side. We make use of the of\ufb01cial scorer for the shared task, 4http://deeplearning.net/software/theano 5We divide the gradient by the mini-batch size. M2Scorer v3.2 [Dahlmeier and Ng, 2012], for evaluation. We perform statistical signi\ufb01cance test using one-tailed sign test with bootstrap resampling on 100 samples. 6.2 Results and Discussion Table 2 presents the results of our experiments with neural network global lexicon model (NNGLM) and neural network joint model (NNJM).",
  "We perform statistical signi\ufb01cance test using one-tailed sign test with bootstrap resampling on 100 samples. 6.2 Results and Discussion Table 2 presents the results of our experiments with neural network global lexicon model (NNGLM) and neural network joint model (NNJM). System P R F0.5 Baseline 50.56 22.68 40.58 Baseline + NNGLM 50.73 23.21 41.01* Baseline + NNJM 51.39 23.26 41.38* Baseline + NNGLM + NNJM 52.34 23.07 41.75* Table 2: Results of our experiments with NNGLM and NNJM on the CoNLL 2014 test set (* indicates statistical signi\ufb01cance with p < 0.01) We see that the addition of both NNGLM and NNJM to our baseline individually improves F0.5 measure on the CoNLL 2014 test set by 0.43 and 0.80, respectively.",
  "Although both improvements over the baseline are statistically signi\ufb01- cant (with p < 0.01), we observe that the improvement of NNGLM is slightly lower than that of NNJM. NNGLM en- codes the entire lexical information from the source sentence without word ordering information. Hence, it focuses mostly on the choice of words appearing in the output. Many of the words in the source context may not be necessary for ensur- ing the quality of corrected output. On the other hand, NNJM looks at a smaller window of words in the source side. NNJM can act as a language model and can ensure a \ufb02uent transla- tion output compared to NNGLM. We also found rescaling to be important for NNGLM be- cause of imbalanced training data. While the most frequent words in the data, \u2018I\u2019 and to\u2019, appear in 43% and 27% of the training sentences, respectively, most words occur in very few sentences only. For example, the word \u2018set\u2019 appears in 0.15% of the sentences and the word \u2018enterprise\u2019 appears in 0.003% of the sentences.",
  "For example, the word \u2018set\u2019 appears in 0.15% of the sentences and the word \u2018enterprise\u2019 appears in 0.003% of the sentences. By incorporating both components together, we obtain an improvement of 1.17 in terms of F0.5 measure. This indi- cates that both components are bene\ufb01cial and complement each other to improve the performance of the baseline sys- tem. While NNGLM looks at the entire source sentence and ensures the appropriate choice of words to appear in the out- put sentence, NNJM encourages the system to choose appro- priate corrections that give a \ufb02uent output. We compare our system to the top 3 systems in the CoNLL 2014 shared task and to the best published results [Yuan and Briscoe, 2016; Susanto et al., 2014] on the test data of the CoNLL 2014 shared task. The results are summarized in Ta- ble 4.",
  "The results are summarized in Ta- ble 4. Our \ufb01nal system including both neural network mod- els outperforms the best system [Yuan and Briscoe, 2016] by 1.85 in F0.5 measure. It should be noted that this is despite the fact that the system proposed in [Yuan and Briscoe, 2016] uses much larger training data than our system.",
  "System: Baseline + NNGLM Source However , there are also a great amount of people who against this technology . Baseline However , there are also a great amount of people who are against this technology . System However , there are also a great number of people who are against this technology . Reference However , there are also a great number of people who are against this technology . System: Baseline + NNJM Source The parents give knowledge and love to the children , meanwhile they feel happy with the accompany of the children . Baseline The parents give knowledge and love to the children , while they feel happy with the accompaniment of the children . System The parents give knowledge and love to the children , meanwhile they feel happy with the company of the children . Reference The parents give knowledge and love to the children , meanwhile they feel happy with the company of the children . System: Baseline + NNGLM + NNJM Source ... by equipping them with a powerful tool to disseminate information almost immediate to the people around them . Baseline ... by equipping them with a powerful tool to disseminate information almost immediate to the people around them .",
  "System: Baseline + NNGLM + NNJM Source ... by equipping them with a powerful tool to disseminate information almost immediate to the people around them . Baseline ... by equipping them with a powerful tool to disseminate information almost immediate to the people around them . System ... by equipping them with a powerful tool to disseminate information almost immediately to the people around them . Reference ... by equipping them with a powerful tool to disseminate information almost immediately to the people around them . Table 3: Examples from the outputs of the systems compared against our baseline system. \u2018Source\u2019 is the erroneous input sentence, \u2018Baseline\u2019 and \u2018System\u2019 are the outputs of our baseline and our neural networks-enhanced system, respectively. \u2018Reference\u2019 is the corrected sentence in which the corrections are made by a human annotator.",
  "\u2018Source\u2019 is the erroneous input sentence, \u2018Baseline\u2019 and \u2018System\u2019 are the outputs of our baseline and our neural networks-enhanced system, respectively. \u2018Reference\u2019 is the corrected sentence in which the corrections are made by a human annotator. System P R F0.5 Baseline + NNGLM + NNJM 52.34 23.07 41.75 Baseline 50.56 22.68 40.58 [Yuan and Briscoe, 2016] - - 39.90 [Susanto et al., 2014] 53.55 19.14 39.39 Top 3 systems in the CoNLL 2014 shared task CAMB 39.71 30.10 37.33 CUUI 41.78 24.88 36.79 AMU 41.62 21.40 35.01 Table 4: Our system compared against competitive grammat- ical error correction systems. CAMB, CUUI, and AMU are Team IDs in the CoNLL 2014 shared task.",
  "CAMB, CUUI, and AMU are Team IDs in the CoNLL 2014 shared task. We qualitatively analyze the output of our neural network- enhanced systems against the outputs produced by our base- line system. We have included some examples in Table 3 and the corresponding outputs of the baseline system and the ref- erence sentences. The selected examples show that NNGLM and NNJM choose appropriate words by making use of the surrounding context effectively. Note that our neural networks, which rely on \ufb01xed source and target vocabulary, map the rare words and misspelled words to the UNK token. Therefore, phrases with the UNK token may get a higher probability than they actually should due to the large number of UNK tokens seen during training. This leads to fewer spelling error corrections compared to the baseline system which does not employ these neural net- works. Consider the following example from the test data: ... numerous pro\ufb01t-driven companies realize the hugh (huge) human traf\ufb01c on such social media sites .... The spelling error hugh \u2192huge is corrected by the baseline system, but not by our \ufb01nal system with the neural networks.",
  "Consider the following example from the test data: ... numerous pro\ufb01t-driven companies realize the hugh (huge) human traf\ufb01c on such social media sites .... The spelling error hugh \u2192huge is corrected by the baseline system, but not by our \ufb01nal system with the neural networks. This is because the misspelled word hugh is not in the neu- ral network vocabulary and so it is mapped to the UNK to- ken. The sentence with the UNK token gets a higher score and hence the system chooses this output over the correct one. From our experiments and analysis, we see that NNGLM and NNJM capture contextual information better than regular translation models and language models. This is because they make use of larger source sentence contexts and continuous space representation of words. This enables them to make better predictions compared to traditional translation models and language models. We also observed that our system has an edge over the baseline for correction of word choice and collocation errors. 7 Conclusion Our experiments show that using the two neural network translation models improves the performance of a phrase- based SMT approach to GEC.",
  "We also observed that our system has an edge over the baseline for correction of word choice and collocation errors. 7 Conclusion Our experiments show that using the two neural network translation models improves the performance of a phrase- based SMT approach to GEC. To the best of our knowledge, this is the \ufb01rst work that uses these two neural network mod- els for SMT-based GEC. The ability of neural networks to model words and phrases in continuous space and capture non-linear relationships enables them to generalize better and make more accurate grammatical error correction. We have achieved state-of-the-art results on the CoNLL 2014 shared task test dataset. This has been done without using any addi- tional training data compared to the best performing systems evaluated on the same dataset. Acknowledgments This research is supported by Singapore Ministry of Educa- tion Academic Research Fund Tier 2 grant MOE2013-T2-1- 150. References [Bengio et al., 2003] Yoshua Bengio, R\u00b4ejean Ducharme, Pascal Vincent, and Christian Janvin. A neural proba- bilistic language model.",
  "References [Bengio et al., 2003] Yoshua Bengio, R\u00b4ejean Ducharme, Pascal Vincent, and Christian Janvin. A neural proba- bilistic language model. Journal of Machine Learning Re- search, 3:1137\u20131155, 2003.",
  "[Brockett et al., 2006] Chris Brockett, William B. Dolan, and Michael Gamon. Correcting ESL errors using phrasal SMT techniques. In Proc. of ACL, 2006. [Dahlmeier and Ng, 2012] Daniel Dahlmeier and Hwee Tou Ng. Better evaluation for grammatical error correction. In Proc. of NAACL, 2012. [Dahlmeier et al., 2012] Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun Feng Ng. NUS at the HOO 2012 shared task. In Proceedings of the Seventh Workshop on Building Edu- cational Applications Using NLP, 2012. [Dahlmeier et al., 2013] Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu. Building a large annotated corpus of learner English: The NUS corpus of learner English. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, 2013. [Dale and Kilgarriff, 2011] Robert Dale and Adam Kilgar- riff.",
  "Building a large annotated corpus of learner English: The NUS corpus of learner English. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, 2013. [Dale and Kilgarriff, 2011] Robert Dale and Adam Kilgar- riff. Helping Our Own: The HOO 2011 pilot shared task. In Proceedings of the 13th European Workshop on Natural Language Generation, 2011. [Dale et al., 2012] Robert Dale, Ilya Anisimoff, and George Narroway. HOO 2012: A report on the preposition and determiner error correction shared task. In Proceedings of the Seventh Workshop on Building Educational Applica- tions Using NLP, 2012. [Devlin et al., 2014] Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. Fast and robust neural network joint models for statistical machine translation. In Proc. of ACL, 2014.",
  "[Devlin et al., 2014] Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. Fast and robust neural network joint models for statistical machine translation. In Proc. of ACL, 2014. [Felice et al., 2014] Mariano Felice, Zheng Yuan, \u00d8istein E. Andersen, Helen Yannakoudakis, and Ekaterina Kochmar. Grammatical error correction using hybrid systems and type \ufb01ltering. In Proc. of CoNLL Shared Task, 2014. [Ha et al., 2014] Thanh-Le Ha, Jan Niehues, and Alex Waibel. Lexical translation model using a deep neural network architecture. In Proceedings of the 11th Interna- tional Workshop on Spoken Language Translation, 2014. [Junczys-Dowmunt and Grundkiewicz, 2014] Marcin Junczys-Dowmunt and Roman Grundkiewicz.",
  "In Proceedings of the 11th Interna- tional Workshop on Spoken Language Translation, 2014. [Junczys-Dowmunt and Grundkiewicz, 2014] Marcin Junczys-Dowmunt and Roman Grundkiewicz. The AMU system in the CoNLL-2014 shared task: Grammatical error correction by data-intensive and feature-rich statisti- cal machine translation. In Proc. of CoNLL Shared Task, 2014. [Koehn et al., 2003] Philipp Koehn, Franz Josef Och, and Daniel Marcu. Statistical phrase-based translation. In Proc. of NAACL, 2003. [Mauser et al., 2009] Arne Mauser, Sa\u02c7sa Hasan, and Her- mann Ney. Extending statistical machine translation with discriminative and trigger-based lexicon models. In Proc. of EMNLP, 2009. [Mizumoto et al., 2011] Tomoya Mizumoto, Mamoru Ko- machi, Masaaki Nagata, and Yuji Matsumoto.",
  "In Proc. of EMNLP, 2009. [Mizumoto et al., 2011] Tomoya Mizumoto, Mamoru Ko- machi, Masaaki Nagata, and Yuji Matsumoto. Min- ing revision log of language learning SNS for automated Japanese error correction of second language learners. In Proc. of IJCNLP, 2011. [Ng et al., 2013] Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian Hadiwinoto, and Joel Tetreault. The CoNLL- 2013 shared task on grammatical error correction. In Proc. of CoNLL Shared Task, 2013. [Ng et al., 2014] Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian Hadiwinoto, Raymond Hendy Susanto, and Christopher Bryant. The CoNLL-2014 shared task on grammatical error correction. In Proc. of CoNLL Shared Task, 2014.",
  "The CoNLL-2014 shared task on grammatical error correction. In Proc. of CoNLL Shared Task, 2014. [Rozovskaya et al., 2014] Alla Rozovskaya, Kai-Wei Chang, Mark Sammons, Dan Roth, and Nizar Habash. The Illinois-Columbia system in the CoNLL-2014 shared task. In Proc. of CoNLL Shared Task, 2014. [Sun et al., 2015] Chengjie Sun, Xiaoqiang Jin, Lei Lin, Yuming Zhao, and Xiaolong Wang. Convolutional neu- ral networks for correcting English article errors. In Pro- ceedings of the 4th CCF Conference on Natural Language Processing and Chinese Computing, 2015. [Susanto et al., 2014] Raymond Hendy Susanto, Peter Phandi, and Hwee Tou Ng. System combination for grammatical error correction. In Proc. of EMNLP, 2014.",
  "[Susanto et al., 2014] Raymond Hendy Susanto, Peter Phandi, and Hwee Tou Ng. System combination for grammatical error correction. In Proc. of EMNLP, 2014. [Vaswani et al., 2013] Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. Decoding with large- scale neural language models improves translation. In Proc. of EMNLP, 2013. [Wu et al., 2014] Jian-Cheng Wu, Tzu-Hsi Yen, Jim Chang, Guan-Cheng Huang, Jimmy Chang, Hsiang-Ling Hsu, Yu- Wei Chang, and Jason S. Chang. NTHU at the CoNLL- 2014 shared task. In Proc. of CoNLL Shared Task, 2014. [Yuan and Briscoe, 2016] Zheng Yuan and Ted Briscoe. Grammatical error correction using neural machine trans- lation. In Proc. of NAACL, 2016."
]