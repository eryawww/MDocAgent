{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Cost-Sensitive BERT for Generalisable Sentence Classi\ufb01cation with Imbalanced Data Harish Tayyar Madabushi1 and Elena Kochkina2,3 and Michael Castelle2,3 1 University of Birmingham, UK H.TayyarMadabushi.1@bham.ac.uk 2University of Warwick, UK (E.Kochkina,M.Castelle.1)@warwick.ac.uk 3Alan Turing Institute, UK Abstract The automatic identi\ufb01cation of propaganda has gained signi\ufb01cance in recent years due to technological and social changes in the way news is generated and consumed. That this task can be addressed effectively using BERT, a powerful new architecture which can be \ufb01ne- tuned for text classi\ufb01cation tasks, is not sur- prising. However, propaganda detection, like other tasks that deal with news documents and other forms of decontextualized social com- munication (e.g. sentiment analysis), inher- ently deals with data whose categories are si- multaneously imbalanced and dissimilar.",
            "However, propaganda detection, like other tasks that deal with news documents and other forms of decontextualized social com- munication (e.g. sentiment analysis), inher- ently deals with data whose categories are si- multaneously imbalanced and dissimilar. We show that BERT, while capable of handling imbalanced classes with no additional data augmentation, does not generalise well when the training and test data are suf\ufb01ciently dis- similar (as is often the case with news sources, whose topics evolve over time). We show how to address this problem by providing a statisti- cal measure of similarity between datasets and a method of incorporating cost-weighting into BERT when the training and test sets are dis- similar. We test these methods on the Propa- ganda Techniques Corpus (PTC) and achieve the second highest score on sentence-level pro- paganda classi\ufb01cation.",
            "We test these methods on the Propa- ganda Techniques Corpus (PTC) and achieve the second highest score on sentence-level pro- paganda classi\ufb01cation. 1 Introduction The challenges of imbalanced classi\ufb01cation\u2014in which the proportion of elements in each class for a classi\ufb01cation task signi\ufb01cantly differ\u2014and of the ability to generalise on dissimilar data have re- mained important problems in Natural Language Processing (NLP) and Machine Learning in gen- eral. Popular NLP tasks including sentiment anal- ysis, propaganda detection, and event extraction from social media are all examples of imbalanced classi\ufb01cation problems. In each case the num- ber of elements in one of the classes (e.g. nega- tive sentiment, propagandistic content, or speci\ufb01c events discussed on social media, respectively) is signi\ufb01cantly lower than the number of elements in the other classes. The recently introduced BERT language model for transfer learning (Devlin et al., 2018) uses a deep bidirectional transformer architecture to pro- duce pre-trained context-dependent embeddings.",
            "The recently introduced BERT language model for transfer learning (Devlin et al., 2018) uses a deep bidirectional transformer architecture to pro- duce pre-trained context-dependent embeddings. It has proven to be powerful in solving many NLP tasks and, as we \ufb01nd, also appears to handle imbal- anced classi\ufb01cation well, thus removing the need to use standard methods of data augmentation to mitigate this problem (see Section 2.2.2 for related work and Section 4.1 for analysis). BERT is credited with the ability to adapt to many tasks and data with very little training (De- vlin et al., 2018). However, we show that BERT fails to perform well when the training and test data are signi\ufb01cantly dissimilar, as is the case with several tasks that deal with social and news data. In these cases, the training data is necessarily a subset of past data, while the model is likely to be used on future data which deals with different topics. This work addresses this problem by incor- porating cost-sensitivity (Section 4.2) into BERT.",
            "In these cases, the training data is necessarily a subset of past data, while the model is likely to be used on future data which deals with different topics. This work addresses this problem by incor- porating cost-sensitivity (Section 4.2) into BERT. We test these methods by participating in the Shared Task on Fine-Grained Propaganda Detec- tion for the 2nd Workshop on NLP for Internet Freedom, for which we achieve the second rank on sentence-level classi\ufb01cation of propaganda, con- \ufb01rming the importance of cost-sensitivity when the training and test sets are dissimilar. 1.1 Detecting Propaganda The term \u2018propaganda\u2019 derives from propagare in post-classical Latin, as in \u201cpropagation of the faith\u201d (Auerbach and Castronovo, 2014), and thus has from the beginning been associated with an intentional and potentially multicast communica- tion; only later did it become a pejorative term. It was pragmatically de\ufb01ned in the World War II arXiv:2003.11563v1  [cs.CL]  16 Mar 2020",
            "era as \u201cthe expression of an opinion or an action by individuals or groups deliberately designed to in\ufb02uence the opinions or the actions of other indi- viduals or groups with reference to predetermined ends\u201d (Institute for Propaganda Analysis, 1937). For the philosopher and sociologist Jacques El- lul, however, in a society with mass communica- tion, propaganda is inevitable and thus it is nec- essary to become more aware of it (Ellul, 1973); but whether or not to classify a given strip of text as propaganda depends not just on its content but on its use on the part of both addressers and ad- dressees (Auerbach and Castronovo, 2014, 6), and this fact makes the automated detection of propa- ganda intrinsically challenging. Despite this dif\ufb01culty, interest in automatically detecting misinformation and\/or propaganda has gained signi\ufb01cance due to the exponential growth in online sources of information combined with the speed with which information is shared today. The sheer volume of social interactions makes it impossible to manually check the veracity of all information being shared.",
            "The sheer volume of social interactions makes it impossible to manually check the veracity of all information being shared. Automation thus remains a potentially viable method of ensuring that we continue to enjoy the bene\ufb01ts of a con- nected world without the spread of misinformation through either ignorance or malicious intent. In the task introduced by Da San Martino et al. (2019), we are provided with articles tagged as propaganda at the sentence and fragment (or span) level and are tasked with making predictions on a development set followed by a \ufb01nal held-out test set. We note this gives us access to the articles in the development and test sets but not their labels. We participated in this task under the team name ProperGander and were placed 2nd on the sen- tence level classi\ufb01cation task where we make use of our methods of incorporating cost-sensitivity into BERT. We also participated in the fragment level task and were placed 7th. The signi\ufb01cant con- tributions of this work are: \u2022 We show that common (\u2018easy\u2019) methods of data augmentation for dealing with class im- balance do not improve base BERT perfor- mance.",
            "We also participated in the fragment level task and were placed 7th. The signi\ufb01cant con- tributions of this work are: \u2022 We show that common (\u2018easy\u2019) methods of data augmentation for dealing with class im- balance do not improve base BERT perfor- mance. \u2022 We provide a statistical method of establish- ing the similarity of datasets. \u2022 We incorporate cost-sensitivity into BERT to enable models to adapt to dissimilar datasets. \u2022 We release all our program code on GitHub and Google Colaboratory1, so that other re- searchers can bene\ufb01t from this work. 2 Related work 2.1 Propaganda detection Most of the existing works on propaganda detec- tion focus on identifying propaganda at the news article level, or even at the news outlet level with the assumption that each of the articles of the suspected propagandistic outlet are propaganda (Rashkin et al., 2017; Barr\u00b4on-Cede\u02dcno et al., 2019).",
            "Here we study two tasks that are more \ufb01ne-grained, speci\ufb01cally propaganda detection at the sentence and phrase (fragment) levels (Da San Martino et al., 2019). This \ufb01ne-grained setup aims to train models that identify linguistic propaganda techniques rather than distinguishing between the article source styles. Da San Martino et al. (2019) were the \ufb01rst to propose this problem setup and release it as a shared task.2 Along with the released dataset, Da San Martino et al. (2019) proposed a multi-granularity neural network, which uses the deep bidirectional transformer architecture known as BERT, which features pre-trained context- dependent embeddings (Devlin et al., 2018). Their system takes a joint learning approach to the sentence- and phrase-level tasks, concatenating the output representation of the less granular (sentence-level) task with the more \ufb01ne-grained task using learned weights. In this work we also take the BERT model as the basis of our approach and focus on the class imbalance as well as the lack of similarity between training and test data inherent to the task.",
            "In this work we also take the BERT model as the basis of our approach and focus on the class imbalance as well as the lack of similarity between training and test data inherent to the task. 2.2 Class imbalance A common issue for many Natural Language Pro- cessing (NLP) classi\ufb01cation tasks is class imbal- ance, the situation where one of the class cate- gories comprises a signi\ufb01cantly larger proportion of the dataset than the other classes. It is especially prominent in real-world datasets and complicates classi\ufb01cation when the identi\ufb01cation of the minor- ity class is of speci\ufb01c importance. Models trained on the basis of minimising er- rors for imbalanced datasets tend to more fre- 1http:\/\/www.harishmadabushi.com\/ research\/propaganda-detection\/ 2https:\/\/propaganda.qcri.org\/ nlp4if-shared-task\/",
            "quently predict the majority class; achieving high accuracy in such cases can be misleading. Be- cause of this, the macro-averaged F-score, chosen for this competition, is a more suitable metric as it weights the performance on each class equally. As class imbalance is a widespread issue, multi- ple techniques have been developed that help alle- viate it (Buda et al., 2018; Haixiang et al., 2017), by either adjusting the model (e.g. changing the performance metric) or changing the data (e.g. oversampling the minority class or undersampling the majority class). 2.2.1 Cost-sensitive learning Cost-sensitive classi\ufb01cation can be used when the \u201ccost\u201d of mislabelling one class is higher than that of mislabelling other classes (Elkan, 2001; Kukar et al., 1998). For example, the real cost to a bank of miscategorising a large fraudulent transaction as authentic is potentially higher than miscate- gorising (perhaps only temporarily) a valid trans- action as fraudulent.",
            "For example, the real cost to a bank of miscategorising a large fraudulent transaction as authentic is potentially higher than miscate- gorising (perhaps only temporarily) a valid trans- action as fraudulent. Cost-sensitive learning tack- les the issue of class imbalance by changing the cost function of the model such that misclassi- \ufb01cation of training examples from the minority class carries more weight and is thus more \u2018ex- pensive\u2019. This is achieved by simply multiplying the loss of each example by a certain factor. This cost-sensitive learning technique takes misclassi- \ufb01cation costs into account during model training, and does not modify the imbalanced data distribu- tion directly. 2.2.2 Data augmentation Common methods that tackle the problem of class imbalance by modifying the data to cre- ate balanced datasets are undersampling and over- sampling. Undersampling randomly removes in- stances from the majority class and is only suitable for problems with an abundance of data. Over- sampling means creating more minority class in- stances to match the size of the majority class. Oversampling methods range from simple random oversampling, i.e.",
            "Undersampling randomly removes in- stances from the majority class and is only suitable for problems with an abundance of data. Over- sampling means creating more minority class in- stances to match the size of the majority class. Oversampling methods range from simple random oversampling, i.e. repeating the training proce- dure on instances from the minority class, cho- sen at random, to the more complex, which in- volves constructing synthetic minority-class sam- ples. Random oversampling is similar to cost- sensitive learning as repeating the sample several times makes the cost of its mis-classi\ufb01cation grow proportionally. Kolomiyets et al. (2011), Zhang et al. (2015), and Wang and Yang (2015) per- form data augmentation using synonym replace- ment, i.e. replacing random words in sentences with their synonyms or nearest-neighbor embed- dings, and show its effectiveness on multiple tasks and datasets. Wei et al. (2019) provide a great overview of \u2018easy\u2019 data augmentation (EDA) tech- niques for NLP, including synonym replacement as described above, and random deletion, i.e.",
            "Wei et al. (2019) provide a great overview of \u2018easy\u2019 data augmentation (EDA) tech- niques for NLP, including synonym replacement as described above, and random deletion, i.e. re- moving words in the sentence at random with pre-de\ufb01ned probability. They show the effective- ness of EDA across \ufb01ve text classi\ufb01cation tasks. However, they mention that EDA may not lead to substantial improvements when using pre-trained models. In this work we test this claim by com- paring performance gains of using cost-sensitive learning versus two data augmentation methods, synonym replacement and random deletion, with a pre-trained BERT model. More complex augmentation methods include back-translation (Sennrich et al., 2015), transla- tional data augmentation (Fadaee et al., 2017), and noising (Xie et al., 2017), but these are out of the scope of this study.",
            "More complex augmentation methods include back-translation (Sennrich et al., 2015), transla- tional data augmentation (Fadaee et al., 2017), and noising (Xie et al., 2017), but these are out of the scope of this study. 3 Dataset The Propaganda Techniques Corpus (PTC) dataset for the 2019 Shared Task on Fine-Grained Pro- paganda consists of a training set of 350 news articles, consisting of just over 16,965 total sen- tences, in which speci\ufb01cally propagandistic frag- ments have been manually spotted and labelled by experts. This is accompanied by a development set (or dev set) of 61 articles with 2,235 total sen- tences, whose labels are maintained by the shared task organisers; and two months after the release of this data, the organisers released a test set of 86 articles and 3,526 total sentences.",
            "In the training set, 4,720 (\u223c28%) of the sentences have been as- sessed as containing propaganda, with 12,245 sen- tences (\u223c72%) as non-propaganda, demonstrat- ing a clear class imbalance. In the binary sentence-level classi\ufb01cation (SLC) task, a model is trained to detect whether each and every sentence is either \u2019propaganda\u2019 or \u2019non- propaganda\u2019; in the more challenging \ufb01eld-level classi\ufb01cation (FLC) task, a model is trained to detect one of 18 possible propaganda technique types in spans of characters within sentences. These propaganda types are listed in Da San Mar- tino et al. (2019) and range from those which might be recognisable at the lexical level (e.g.",
            "NAME CALLING, REPETITION), and those which would likely need to incorporate semantic under- standing (RED HERRING, STRAW MAN).3 For several example sentences from a sample document annotated with fragment-level classi- \ufb01cations (FLC) (Figure 1). The corresponding sentence-level classi\ufb01cation (SLC) labels would indicate that sentences 3, 4, and 7 are \u2019propa- ganda\u2019 while the the other sentences are \u2018non- propaganda\u2019. 3.1 Data Distribution One of the most interesting aspects of the data pro- vided for this task is the notable difference be- tween the training and the development\/test sets. We emphasise that this difference is realistic and re\ufb02ective of real world news data, in which major stories are often accompanied by the introduction of new terms, names, and even phrases. This is because the training data is a subset of past data while the model is to be used on future data which deals with different newsworthy topics. We demonstrate this difference statistically by using a method for \ufb01nding the similarity of cor- pora suggested by Kilgarriff (2001).",
            "This is because the training data is a subset of past data while the model is to be used on future data which deals with different newsworthy topics. We demonstrate this difference statistically by using a method for \ufb01nding the similarity of cor- pora suggested by Kilgarriff (2001). We use the Wilcoxon signed-rank test (Wilcoxon, 1945) which compares the frequency counts of randomly sampled elements from different datasets to deter- mine if those datasets have a statistically similar distribution of elements. We implement this as follows. For each of the training, development and test sets, we extract all words (retaining the repeats) while ignoring a set of stopwords (identi\ufb01ed through the Python Nat- ural Language Toolkit). We then extract 10,000 samples (with replacements) for various pairs of these datasets (training, development, and test sets along with splits of each of these datasets). Fi- nally, we use comparative word frequencies from the two sets to calculate the p-value using the Wilcoxon signed-rank test. Table 1 provides the minimum and maximum p-values and their inter- pretations for ten such runs of each pair reported.",
            "Fi- nally, we use comparative word frequencies from the two sets to calculate the p-value using the Wilcoxon signed-rank test. Table 1 provides the minimum and maximum p-values and their inter- pretations for ten such runs of each pair reported. With p-value less than 0.05, we show that the train, development and test sets are self-similar and also signi\ufb01cantly different from each other. In mea- suring self-similarity, we split each dataset after shuf\ufb02ing all sentences. While this comparison is made at the sentence level (as opposed to the arti- 3https:\/\/propaganda.qcri.org\/ annotations\/ includes a \ufb02owchart instructing annotators to discover and isolate these 18 propaganda categories.",
            "While this comparison is made at the sentence level (as opposed to the arti- 3https:\/\/propaganda.qcri.org\/ annotations\/ includes a \ufb02owchart instructing annotators to discover and isolate these 18 propaganda categories. Set 1 Set 2 p-value (min) p-value (max) % Similar Tests 50% Train 50% Train 2.38E-01 9.11E-01 100 50% Dev 50% Dev 5.55E-01 9.96E-01 100 50% Test 50% Test 6.21E-01 8.88E-01 100 25% Dev 75% Dev 1.46E-01 5.72E-01 100 25% Test 75% Test 3.70E-02 7.55E-01 90 25% Train 75% Train 9.08E-02 9.66E-01 100 Train Dev 2.05E-09 4.33E-05 0 Train Test 8.37E-23 1.18E-14 0 Dev Test 2.72E-04 2.11E-02 0 Table 1: p-values representing the similarity between (parts of) the train, test and development sets.",
            "cle level), it is consistent with the granularity used for propaganda detection, which is also at the sen- tence level. We also perform measurements of self similarity after splitting the data at the article level and \ufb01nd that the conclusions of similarity between the sets hold with a p-value threshold of 0.001, where p-values for similarity between the train- ing and dev\/test sets are orders of magnitude lower compared to self-similarity. Since we use random sampling we run this test 10 times and present the both the maximum and minimum p-values. We in- clude the similarity between 25% of a dataset and the remaining 75% of that set because that is the train\/test ratio we use in our experiments, further described in our methodology (Section 4). This analysis shows that while all splits of each of the datasets are statistically similar, the train- ing set (and the split of the training set that we use for experimentation) are signi\ufb01cantly differ- ent from the development and test sets.",
            "This analysis shows that while all splits of each of the datasets are statistically similar, the train- ing set (and the split of the training set that we use for experimentation) are signi\ufb01cantly differ- ent from the development and test sets. While our analysis does show that the development and the test sets are dissimilar, we note (based on the p- values) that they are signi\ufb01cantly more similar to each other than they are to the training set. 4 Methodology We were provided with two tasks: (1) propaganda fragment-level identi\ufb01cation (FLC) and (2) pro- pagandistic sentence-level identi\ufb01cation (SLC). While we develop systems for both tasks, our main focus is toward the latter. Given the differences between the training, development, and test sets, we focus on methods for generalising our models. We note that propaganda identi\ufb01cation is, in gen- eral, an imbalanced binary classi\ufb01cation problem as most sentences are not propagandistic. Due to the non-deterministic nature of fast GPU computations, we run each of our models three times and report the average of these three runs",
            "Sentence 1: The Senate Judiciary Committee voted 11-10 along party lines to advance the nomination of Judge Brett Kavanaugh out of committee to the Senate \ufb02oor for a vote. Sentence 2: Of course, RINO Senator Jeff Flake (R-AZ) wanted to side with Senate Democrats in pushing for a FBI investigation into unsubstantiated allegations against Kavanaugh. Sentence 3: Outgoing Flake, and <LOADED LANGUAGE> good riddance <\/LOADED LANGUAGE>, said that he sided with his colleagues in having a \u201dlimited time and scope\u201d investigation by the FBI into the allegations against Kavanaugh. Sentence 4: \u201c<FLAG-WAVING> This country is being ripped apart here, and we\u2019ve got to make sure we do due diligence<\/FLAG-WAVING>,\u201d Flake said. Sentence 5: He added that he would be more \u201dcomfortable\u201d with an FBI investigation. Sentence 6: Comfort? Sentence 7: <WHATABOUTISM>What about Judge Kavanaugh\u2019s comfort in being put through the ringer without a shred of evidence, Senator Flake<\/WHATABOUTISM>?",
            "Sentence 6: Comfort? Sentence 7: <WHATABOUTISM>What about Judge Kavanaugh\u2019s comfort in being put through the ringer without a shred of evidence, Senator Flake<\/WHATABOUTISM>? Figure 1: Excerpt of an example (truncated) news document with three separate \ufb01eld-level classi\ufb01cation (FLC) tags, for LOADED LANGUAGE, FLAG-WAVING, AND WHATABOUTISM. through the rest of this section. When picking the model to use for our \ufb01nal submission, we pick the model that performs best on the development set. When testing our models, we split the labelled training data into two non-overlapping parts: the \ufb01rst one, consisting of 75% of the training data is used to train models, whereas the other is used to test the effectiveness of the models. All models are trained and tested on the same split to ensure comparability. Similarly, to ensure that our mod- els remain comparable, we continue to train on the same 75% of the training set even when testing on the development set.",
            "All models are trained and tested on the same split to ensure comparability. Similarly, to ensure that our mod- els remain comparable, we continue to train on the same 75% of the training set even when testing on the development set. Once the best model is found using these meth- ods, we train that model on all of the training data available before then submitting the results on the development set to the leaderboard. These results are detailed in the section describing our results (Section 5). 4.1 Class Imbalance in Sentence Level Classi\ufb01cation The sentence level classi\ufb01cation task is an imbal- anced binary classi\ufb01cation problem that we ad- dress using BERT (Devlin et al., 2018). We use BERTBASE, uncased, which consists of 12 self- attention layers, and returns a 768-dimension vec- tor that representation a sentence.",
            "We use BERTBASE, uncased, which consists of 12 self- attention layers, and returns a 768-dimension vec- tor that representation a sentence. So as to make use of BERT for sentence classi\ufb01cation, we in- clude a fully connected layer on top of the BERT self-attention layers, which classi\ufb01es the sentence embedding provided by BERT into the two classes of interest (propaganda or non-propaganda). We attempt to exploit various data augmenta- tion techniques to address the problem of class im- balance. Table 2 shows the results of our experi- ments for different data augmentation techniques when, after shuf\ufb02ing the training data, we train the model on 75% of the training data and test it on the remaining 25% of the training data and the devel- opment data.",
            "Table 2 shows the results of our experi- ments for different data augmentation techniques when, after shuf\ufb02ing the training data, we train the model on 75% of the training data and test it on the remaining 25% of the training data and the devel- opment data. Augmentation Technique f1-score on 25% of Train f1-score on Dev None 0.7954 0.5803 Synonym Insertion 0.7889 0.5833 Dropping Words 0.7791 0.5445 Over Sampling 0.7843 0.6276 Table 2: F1 scores on an unseen (not used for train- ing) part of the training set and the development set on BERT using different augmentation techniques. We observe that BERT without augmentation consistently outperforms BERT with augmenta- tion in the experiments when the model is trained on 75% of the training data and evaluated on the rest, i.e trained and evaluated on similar data, coming from the same distribution. This is con- sistent with observations by Wei et al. (2019) that contextual word embeddings do not gain from data augmentation.",
            "This is con- sistent with observations by Wei et al. (2019) that contextual word embeddings do not gain from data augmentation. The fact that we shuf\ufb02e the training data prior to splitting it into training and testing subsets could imply that the model is learning to associate topic words, such as \u2018Mueller\u2019, as pro- paganda. However, when we perform model eval- uation using the development set, which is dissim- ilar to the training, we observe that synonym in- sertion and word dropping techniques also do not bring performance gains, while random oversam- pling increases performance over base BERT by 4%. Synonym insertion provides results very sim- ilar to base BERT, while random deletion harms model performance producing lower scores. We believe that this could be attributed to the fact that",
            "synonym insertion and random word dropping in- volve the introduction of noise to the data, while oversampling does not. As we are working with natural language data, this type of noise can in fact change the meaning of the sentence. Oversam- pling on the other hand purely increases the impor- tance of the minority class by repeating training on the unchanged instances. So as to better understand the aspects of over- sampling that contribute to these gains, we per- form a class-wise performance analysis of BERT with\/without oversampling. The results of these experiments (Table 3) show that oversampling in- creases the overall recall while maintaining preci- sion. This is achieved by signi\ufb01cantly improving the recall of the minority class (propaganda) at the cost of the recall of the majority class.",
            "The results of these experiments (Table 3) show that oversampling in- creases the overall recall while maintaining preci- sion. This is achieved by signi\ufb01cantly improving the recall of the minority class (propaganda) at the cost of the recall of the majority class. OS No OS precision 0.7967 0.7933 recall 0.7767 0.8000 f1-score 0.7843 0.7954 Non-Propaganda precision 0.8733 0.8467 Non-Propaganda recall 0.8100 0.8900 Non-Propaganda F1 0.8433 0.8667 Propaganda precision 0.5800 0.6600 Propaganda recall 0.6933 0.5533 Propaganda F1 0.6300 0.6033 Table 3: Class-wise precision and recall with and with- out oversampling (OS) achieved on unseen part of the training set.",
            "So far we have been able to establish that a) the training and test sets are dissimilar, thus requir- ing us to generalise our model, b) oversampling provides a method of generalisation, and c) over- sampling does this while maintaining recall on the minority (and thus more interesting) class. Given this we explore alternative methods of increasing minority class recall without a signif- icant drop in precision. One such method is cost-sensitive classi\ufb01cation, which differs from random oversampling in that it provides a more continuous-valued and consistent method of weighting samples of imbalanced training data; for example, random oversampling will inevitably emphasise some training instances at the expense of others. We detail our methods of using cost- sensitive classi\ufb01cation in the next section. Further experiments with oversampling might have pro- vided insights into the relationships between these methods, which we leave for future exploration. 4.2 Cost-sensitive Classi\ufb01cation As discussed in Section 2.2.1, cost-sensitive clas- si\ufb01cation can be performed by weighting the cost function.",
            "4.2 Cost-sensitive Classi\ufb01cation As discussed in Section 2.2.1, cost-sensitive clas- si\ufb01cation can be performed by weighting the cost function. We increase the weight of incorrectly la- belling a propagandistic sentence by altering the cost function of the training of the \ufb01nal fully con- nected layer of our model previously described in Section 4.1. We make these changes through the use of PyTorch (Paszke et al., 2017) which cal- culates the cross-entropy loss for a single predic- tion x, an array where the jth element represents the models prediction for class j, labelled with the class class as given by Equation 1. loss(x, class) = \u2212log   exp(x[class]) P j exp(x[j]) !",
            "= \u2212x[class] + log \uf8eb \uf8edX j exp(x[j]) \uf8f6 \uf8f8 (1) The cross-entropy loss given in Equation 1 is modi\ufb01ed to accommodate an array weight, the ith element of which represents the weight of the ith class, as described in Equation 2. loss(x, class) = weight[class]\u0398 where, \u0398 = \u2212x[class] + log \uf8eb \uf8edX j exp(x[j]) \uf8f6 \uf8f8 (2) Intuitively, we increase the cost of getting the classi\ufb01cation of an \u201cimportant\u201d class wrong and corresponding decrees the cost of getting a less important class wrong. In our case, we increase the cost of mislabelling the minority class which is \u201cpropaganda\u201d (as opposed to \u201cnon-propaganda\u201d). We expect the effect of this to be similar to that of oversampling, in that it is likely to enable us to increase the recall of the minority class thus resulting in the decrease in recall of the overall model while maintaining high precision.",
            "We expect the effect of this to be similar to that of oversampling, in that it is likely to enable us to increase the recall of the minority class thus resulting in the decrease in recall of the overall model while maintaining high precision. We re- iterate that this speci\ufb01c change to a model results in increasing the model\u2019s ability to better identify elements belonging to the minority class in dissim- ilar datasets when using BERT. We explore the validity of this by perform- ing several experiments with different weights as- signed to the minority class. We note that in our experiments use signi\ufb01cantly higher weights than the weights proportional to class frequencies in the",
            "Figure 2: The impact of modifying the minority class weights on the performance on similar (subset of train- ing set) and dissimilar (development) datasets. The method of increasing minority class weights is able to push the model towards generalisation while maintain- ing precision. training data, that are common in literature (Ling and Sheng, 2011). Rather than directly using the class proportions of the training set, we show that tuning weights based on performance on the de- velopment set is more bene\ufb01cial. Figure 2 shows the results of these experiments wherein we are able to maintain the precision on the subset of the training set used for testing while reducing its re- call and thus generalising the model. The fact that the model is generalising on a dissimilar dataset is con\ufb01rmed by the increase in the development set F1 score. We note that the gains are not in\ufb01- nite and that a balance must be struck based on the amount of generalisation and the corresponding loss in accuracy.",
            "We note that the gains are not in\ufb01- nite and that a balance must be struck based on the amount of generalisation and the corresponding loss in accuracy. The exact weight to use for the best transfer of classi\ufb01cation accuracy is related to the dissimilarity of that other dataset and hence is to be obtained experimentally through hyperpa- rameter search. Our experiments showed that a value of 4 is best suited for this task. We do not include the complete results of our experiments here due to space constraints but in- clude them along with charts and program code on our project website. Based on this exploration we \ufb01nd that the best weights for this particular dataset are 1 for non-propaganda and 4 for propaganda and we use this to train the \ufb01nal model used to submit results to the leaderboard. We also found that adding Part of Speech tags and Named En- tity information to BERT embeddings by concate- nating these one-hot vectors to the BERT embed- dings does not improve model performance. We describe these results in Section 5.",
            "We also found that adding Part of Speech tags and Named En- tity information to BERT embeddings by concate- nating these one-hot vectors to the BERT embed- dings does not improve model performance. We describe these results in Section 5. 4.3 Fragment-level classi\ufb01cation (FLC) In addition to participating in the Sentence Level Classi\ufb01cation task we also participate in the Frag- ment Level Classi\ufb01cation task. We note that ex- tracting fragments that are propagandistic is sim- ilar to the task of Named Entity Recognition, in that they are both span extraction tasks, and so use a BERT based model designed for this task - We build on the work by Emelyanov and Artemova (2019) which makes use of Continuous Random Field stacked on top of an LSTM to predict spans. This architecture is standard amongst state of the art models that perform span identi\ufb01cation. While the same span of text cannot have multi- ple named entity labels, it can have different pro- paganda labels. We get around this problem by picking one of the labels at random.",
            "This architecture is standard amongst state of the art models that perform span identi\ufb01cation. While the same span of text cannot have multi- ple named entity labels, it can have different pro- paganda labels. We get around this problem by picking one of the labels at random. Addition- ally, so as to speed up training, we only train our model on those sentences that contain some propa- gandistic fragment. In hindsight, we note that both these decisions were not ideal and discuss what we might have otherwise done in Section 7. 5 Results In this section, we show our rankings on the leaderboard on the test set. Unlike the previous ex- ploratory sections, in which we trained our model on part of the training set, we train models de- scribed in this section on the complete training set. 5.1 Results on the SLC task Our best performing model, selected on the ba- sis of a systematic analysis of the relationship between cost weights and recall, places us sec- ond amongst the 25 teams that submitted their re- sults on this task. We present our score on the test set alongside those of comparable teams in Table 4.",
            "We present our score on the test set alongside those of comparable teams in Table 4. We note that the task description pa- per (Da San Martino et al., 2019) describes a method of achieving an F1 score of 60.98% on a similar task although this reported score is not directly comparable to the results on this task be- cause of the differences in testing sets. 5.2 Results on the FLC task We train the model described in Section 4.3 on the complete training set before submitting to the leaderboard. Our best performing model was",
            "Rank Team F1 Precision Recall 1 ltuorp 0.632375 0.602885 0.664899 2 Proper- Gander 0.625651 0.564957 0.700954 3 YMJA 0.624934 0.625265 0.624602 . . . 20 Baseline 0.434701 0.388010 0.494168 Table 4: Our results on the SLC task (2nd, in bold) alongside comparable results from the competition leaderboard. placed 7th amongst the 13 teams that submitted results for this task. We present our score on the test set alongside those of comparable teams in Table 5. We note that the task description paper (Da San Martino et al., 2019) describes a method of achieving an F1 score of 22.58% on a similar task although, this reported score is not di- rectly comparable to the results on this task.",
            "We note that the task description paper (Da San Martino et al., 2019) describes a method of achieving an F1 score of 22.58% on a similar task although, this reported score is not di- rectly comparable to the results on this task. Rank Team F1 Precision Recall 1 newspeak 0.248849 0.286299 0.220063 2 Anti- ganda 0.226745 0.288213 0.186887 . . . 6 aschern 0.109060 0.071528 0.229464 7 Proper- Gander 0.098969 0.065167 0.205634 . . . 11 Baseline 0.000015 0.011628 0.000008 Table 5: Our results on the FLC task (7th, in bold) alongside those of better performing teams from the competition leaderboard. One of the major setbacks to our method for identifying sentence fragments was the loss of training data as a result of randomly picking one label when the same fragment had multiple labels.",
            "One of the major setbacks to our method for identifying sentence fragments was the loss of training data as a result of randomly picking one label when the same fragment had multiple labels. This could have been avoided by training differ- ent models for each label and simply concatenat- ing the results. Additionally, training on all sen- tences, including those that did not contain any fragments labelled as propagandistic would have likely improved our model performance. We in- tend to perform these experiments as part of our ongoing research. 6 Issues of Decontextualization in Automated Propaganda Detection It is worth re\ufb02ecting on the nature of the shared task dataset (PTC corpus) and its structural cor- respondence (or lack thereof) to some of the de\ufb01nitions of propaganda mentioned in the in- troduction.",
            "First, propaganda is a social phe- nomenon and takes place as an act of communi- cation (O\u2019Shaughnessy, 2005, 13-14), and so it is more than a simple information-theoretic mes- sage of zeros and ones\u2014it also incorporates an addresser and addressee(s), each in phatic con- tact (typically via broadcast media), ideally with a shared denotational code and contextual sur- round(s) (Jakobson, 1960). As such, a dataset of decontextualised docu- ments with labelled sentences, devoid of autho- rial or publisher metadata, has taken us at some remove from even a simple everyday de\ufb01nition of propaganda. Our models for this shared task can- not easily incorporate information about the ad- dresser or addressee; are left to assume a shared denotational code between author and reader (one perhaps simulated with the use of pre-trained word embeddings); and they are unaware of when or where the act(s) of propagandistic communication took place. This slipperiness is illustrated in our example document (Fig.",
            "This slipperiness is illustrated in our example document (Fig. 1): note that while Sen- tences 3 and 7, labelled as propaganda, re\ufb02ect a propagandistic attitude on the part of the journal- ist and\/or publisher, Sentence 4\u2014also labelled as propaganda in the training data\u2014instead re\ufb02ects a \u201c\ufb02ag-waving\u201d propagandistic attitude on the part of U.S. congressman Jeff Flake, via the conven- tions of reported speech (Volo\u02c7sinov, 1973, 115- 130). While reported speech often is signaled by speci\ufb01c morphosyntactic patterns (e.g. the use of double-quotes and \u201cFlake said\u201d) (Spronck and Nikitina, 2019), we argue that human readers rou- tinely distinguish propagandistic reportage from the propagandastic speech acts of its subjects, and to con\ufb02ate these categories in a propaganda detec- tion corpus may contribute to the occurrence of false positives\/negatives.",
            "7 Conclusions and Future Work In this work we have presented a method of in- corporating cost-sensitivity into BERT to allow for better generalisation and additionally, we provide a simple measure of corpus similarity to determine when this method is likely to be useful. We intend to extend our analysis of the ability to generalise models to less similar data by experimenting on other datasets and models. We hope that the re- lease of program code and documentation will al- low the research community to help in this exper- imentation while exploiting these methods.",
            "Acknowledgements We would like to thank Dr Leandro Minku from the University of Birmingham for his insights into and help with the statistical analysis presented in this paper. This work was also partially supported by The Alan Turing Institute under the EPSRC grant EP\/N510129\/1. Work by Elena Kochkina was par- tially supported by the Leverhulme Trust through the Bridges Programme and Warwick CDT for Ur- ban Science & Progress under the EPSRC Grant Number EP\/L016400\/1. References Jonathan Auerbach and Russ Castronovo, editors. 2014. The Oxford Handbook of Propaganda Stud- ies. Oxford Handbooks. Oxford University Press, Oxford, New York. Alberto Barr\u00b4on-Cede\u02dcno, Giovanni Da San Martino, Is- raa Jaradat, and Preslav Nakov. 2019. Proppy: A system to unmask propaganda in online news. In Proceedings of the AAAI Conference on Arti\ufb01cial In- telligence, volume 33, pages 9847\u20139848.",
            "2019. Proppy: A system to unmask propaganda in online news. In Proceedings of the AAAI Conference on Arti\ufb01cial In- telligence, volume 33, pages 9847\u20139848. Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski. 2018. A systematic study of the class imbalance problem in convolutional neural networks. Neural Networks, 106:249\u2013259. Giovanni Da San Martino, Seunghak Yu, Alberto Barr\u00b4on-Cede\u02dcno, Rostislav Petrov, and Preslav Nakov. 2019. Fine-grained analysis of propaganda in news articles. In Proceedings of the 2019 Con- ference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, EMNLP-IJCNLP 2019, Hong Kong, China. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.",
            "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of deep bidirectional transformers for language under- standing. arXiv preprint arXiv:1810.04805. Charles Elkan. 2001. The foundations of cost-sensitive learning. In International joint conference on ar- ti\ufb01cial intelligence, volume 17, pages 973\u2013978. Lawrence Erlbaum Associates Ltd. Jacques Ellul. 1973. Propaganda. Random House USA Inc, New York. Anton A. Emelyanov and Ekaterina Artemova. 2019. Multilingual named entity recognition using pre- trained embeddings, attention mechanism and NCRF. CoRR, abs\/1906.09978. Marzieh Fadaee, Arianna Bisazza, and Christof Monz. 2017. Data augmentation for low- resource neural machine translation. arXiv preprint arXiv:1705.00440.",
            "CoRR, abs\/1906.09978. Marzieh Fadaee, Arianna Bisazza, and Christof Monz. 2017. Data augmentation for low- resource neural machine translation. arXiv preprint arXiv:1705.00440. Guo Haixiang, Li Yijing, Jennifer Shang, Gu Mingyun, Huang Yuanyue, and Gong Bing. 2017. Learn- ing from class-imbalanced data: Review of methods and applications. Expert Systems with Applications, 73:220\u2013239. Institute for Propaganda Analysis. 1937. How to detect propaganda. Propaganda Analysis, 1(2):5\u20138. Roman Jakobson. 1960. Closing Statement: Linguis- tics and Poetics. In Thomas A. Sebeok, editor, Style in Language, pages 350\u2013377. MIT Press, Cam- bridge, MA. Adam Kilgarriff. 2001. Comparing corpora. Interna- tional Journal of Corpus Linguistics, 6(1):97\u2013133.",
            "MIT Press, Cam- bridge, MA. Adam Kilgarriff. 2001. Comparing corpora. Interna- tional Journal of Corpus Linguistics, 6(1):97\u2013133. Oleksandr Kolomiyets, Steven Bethard, and Marie- Francine Moens. 2011. Model-portability experi- ments for textual temporal analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Tech- nologies: short papers-Volume 2, pages 271\u2013276. Association for Computational Linguistics. Matjaz Kukar, Igor Kononenko, et al. 1998. Cost- sensitive learning with neural networks. In ECAI, pages 445\u2013449. CX Ling and VS Sheng. 2011. Cost-sensitive learning and the class imbalance problem. Encyclopedia of Machine Learning: Springer, 24. Nicholas Jackson O\u2019Shaughnessy. 2005. Politics and Propaganda: Weapons of Mass Seduction. Univer- sity of Michigan Press, Ann Arbor.",
            "Cost-sensitive learning and the class imbalance problem. Encyclopedia of Machine Learning: Springer, 24. Nicholas Jackson O\u2019Shaughnessy. 2005. Politics and Propaganda: Weapons of Mass Seduction. Univer- sity of Michigan Press, Ann Arbor. Adam Paszke, Sam Gross, Soumith Chintala, Gre- gory Chanan, Edward Yang, Zachary DeVito, Zem- ing Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in PyTorch. In NIPS Autodiff Workshop. Hannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana Volkova, and Yejin Choi. 2017. Truth of varying shades: Analyzing language in fake news and polit- ical fact-checking. In Proceedings of the 2017 Con- ference on Empirical Methods in Natural Language Processing, pages 2931\u20132937. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015. Improving neural machine translation models with monolingual data.",
            "In Proceedings of the 2017 Con- ference on Empirical Methods in Natural Language Processing, pages 2931\u20132937. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015. Improving neural machine translation models with monolingual data. arXiv preprint arXiv:1511.06709. Stef Spronck and Tatiana Nikitina. 2019. Reported speech forms a dedicated syntactic domain. Linguis- tic Typology, 23(1):119\u2013159. V. N. Volo\u02c7sinov. 1973. Marxism and the Philosophy of Language: Studies in Language. Academic Press Inc, New York.",
            "William Yang Wang and Diyi Yang. 2015. Thats so an- noying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic cat- egorization of annoying behaviors using# petpeeve tweets. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process- ing, pages 2557\u20132563. Jason W Wei and Kai Zou. 2019. Eda: Easy data augmentation techniques for boosting perfor- mance on text classi\ufb01cation tasks. arXiv preprint arXiv:1901.11196. Frank Wilcoxon. 1945. Individual comparisons by ranking methods. Biometrics, 1(6):80\u201383. Ziang Xie, Sida I Wang, Jiwei Li, Daniel L\u00b4evy, Aiming Nie, Dan Jurafsky, and Andrew Y Ng. 2017. Data noising as smoothing in neural network language models. arXiv preprint arXiv:1703.02573. Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.",
            "2017. Data noising as smoothing in neural network language models. arXiv preprint arXiv:1703.02573. Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text clas- si\ufb01cation. In Advances in neural information pro- cessing systems, pages 649\u2013657."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-2003.11563.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 9747.0,
    "avg_doclen_est": 180.5
}
