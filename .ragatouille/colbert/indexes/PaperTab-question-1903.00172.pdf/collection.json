[
  "Open Information Extraction from Question-Answer Pairs Nikita Bhutani \u2217 University of Michigan nbhutani@umich.edu Yoshihiko Suhara Megagon Labs yoshi@megagon.ai Wang-Chiew Tan Megagon Labs wangchiew@megagon.ai Alon Halevy Megagon Labs alon@megagon.ai H. V. Jagadish University of Michigan jag@eecs.umich.edu Abstract Open Information Extraction (OPENIE) ex- tracts meaningful structured tuples from free- form text. Most previous work on OPENIE considers extracting data from one sentence at a time. We describe NEURON, a system for extracting tuples from question-answer pairs. Since real questions and answers often con- tain precisely the information that users care about, such information is particularly desir- able to extend a knowledge base with. NEURON addresses several challenges. First, an answer text is often hard to understand without knowing the question, and second, rel- evant information can span multiple sentences. To address these, NEURON formulates extrac- tion as a multi-source sequence-to-sequence learning task, wherein it combines distributed representations of a question and an answer to generate knowledge facts.",
  "To address these, NEURON formulates extrac- tion as a multi-source sequence-to-sequence learning task, wherein it combines distributed representations of a question and an answer to generate knowledge facts. We describe exper- iments on two real-world datasets that demon- strate that NEURON can \ufb01nd a signi\ufb01cant num- ber of new and interesting facts to extend a knowledge base compared to state-of-the-art OPENIE methods. 1 Introduction Open Information Extraction (OPENIE) (Banko et al., 2007) is the problem of extracting structured data from a text corpus, without knowing a priori which relations will be extracted. It is one of the primary technologies used in building knowledge bases (KBs) that, in turn, power question answer- ing (Berant et al., 2013). The vast majority of pre- vious work on OPENIE extracts structured informa- tion (e.g., triples) from individual sentences. This paper addresses the problem of extract- ing structured data from conversational question- answer (CQA) data. Often, CQA data contains pre- cisely the knowledge that users care about.",
  "This paper addresses the problem of extract- ing structured data from conversational question- answer (CQA) data. Often, CQA data contains pre- cisely the knowledge that users care about. As \u2217Part of the work was done while the author was at Megagon Labs. such, this data offers a goal-directed method for extending existing knowledge bases. Consider, for example, a KB about a hotel that is used to power its website and/or a conversational interface for hotel guests. The KB provides information about the hotel\u2019s services: complimentary breakfast, free wi\ufb01, spa. However, it may not include information about the menu/times for the breakfast, credentials for the wi\ufb01, or the cancellation policy for a spa appointment at the hotel. Given the wide range of information that may be of interest to guests, it is not clear how to extend the KB in the most effective way. However, the conversational logs, which many hotels keep, contain the actual ques- tions from guests, and can therefore be used as a resource for extending the KB. Following exam- ples illustrate the kind of data we aim to extract: Example 1.",
  "However, the conversational logs, which many hotels keep, contain the actual ques- tions from guests, and can therefore be used as a resource for extending the KB. Following exam- ples illustrate the kind of data we aim to extract: Example 1. Q: Does the hotel have a gym? A: It is located on the third \ufb02oor and is 24/7. Tuple: \u27e8gym, is located on, third \ufb02oor\u27e9 Example 2. Q: What time does the pool open? A: 6:00am daily. Tuple: \u27e8pool, open, 6:00am daily\u27e9 As can be seen from these examples, harvest- ing facts from CQA data presents signi\ufb01cant chal- lenges. In particular, the system must interpret in- formation collectively between the questions and answers. In this case, it must realize that \u2018third \ufb02oor\u2019 refers to the location of the \u2018gym\u2019 and that 6:00am refers to the opening time of the pool. OPENIE systems that operate over individual sen- tences ignore the discourse and context in a QA pair.",
  "In this case, it must realize that \u2018third \ufb02oor\u2019 refers to the location of the \u2018gym\u2019 and that 6:00am refers to the opening time of the pool. OPENIE systems that operate over individual sen- tences ignore the discourse and context in a QA pair. Without knowing the question, they either fail to or incorrectly interpret the answer. This paper describes NEURON, an end-to-end system for extracting information from CQA data. We cast OPENIE from CQA as a multi-source sequence-to-sequence generation problem to ex- plicitly model both the question and answer arXiv:1903.00172v2  [cs.CL]  6 Apr 2019",
  "in a QA pair. We propose a multi-encoder, constrained-decoder framework that uses two en- coders to encode each of the question and answer to an internal representation. The two representa- tions are then used by a decoder to generate an out- put sequence corresponding to an extracted tuple. For example, the output sequence of Example 2 is: \u27e8arg1\u27e9pool \u27e8/arg1\u27e9\u27e8rel\u27e9open \u27e8/rel\u27e9\u27e8arg2\u27e96:00am daily \u27e8/arg2\u27e9 While encoder-decoder frameworks have been used extensively for machine translation and sum- marization, there are two key technical challenges in extending them for information extraction from CQA data. First, it is vital for the translation model to learn constraints such as, arguments and rela- tions are sub-spans from the input sequence, out- put sequence must have a valid syntax (e.g., \u27e8arg1\u27e9 must precede \u27e8rel\u27e9). These and other constraints can be integrated as hard constraints in the de- coder. Second, the model must recognize auxiliary information that is irrelevant to the KB.",
  "These and other constraints can be integrated as hard constraints in the de- coder. Second, the model must recognize auxiliary information that is irrelevant to the KB. For exam- ple, in the hotel application, NEURON must learn to discard greetings in the data. Since existing facts in the KB are representative of the domain of the KB, this prior knowledge can be incorporated as soft constraints in the decoder to rank various out- put sequences based on their relevance. Our con- tributions are summarized below: \u2022 We develop NEURON, a system for extracting in- formation from CQA data. NEURON is a novel multi-encoder constrained-decoder method that explicitly models both the question and the an- swer of a QA pair. It incorporates vocabulary and syntax as hard constraints and prior knowl- edge as soft constraints in the decoder. \u2022 We conduct comprehensive experiments on two real-world CQA datasets. Our experimental re- sults show that the use of hard and soft con- straints improves the extraction accuracy and NEURON achieves the highest accuracy in ex- tracting tuples from QA pairs compared with state-of-the-art sentence-based models, with a relative improvement as high as 13.3%.",
  "Our experimental re- sults show that the use of hard and soft con- straints improves the extraction accuracy and NEURON achieves the highest accuracy in ex- tracting tuples from QA pairs compared with state-of-the-art sentence-based models, with a relative improvement as high as 13.3%. NEU- RON\u2019s higher accuracy and ability to discover 15-25% tuples that are not extracted by state- of-the-art models make it suitable as a tuple ex- traction tool for KB extension. \u2022 We present a case study to demonstrate how a KB can be extended iteratively using tuples ex- tracted using NEURON. In each iteration, only relevant tuples are included in the KB. In turn, the extended KB is used to improve relevance scoring for subsequent iterations. 2 Task Formulation In this work, we choose to model an OPENIE ex- traction from a question-answer (QA) pair as a tuple consisting of a single relation with two ar- guments, where the relation and arguments are contiguous spans from the QA pair.",
  "2 Task Formulation In this work, we choose to model an OPENIE ex- traction from a question-answer (QA) pair as a tuple consisting of a single relation with two ar- guments, where the relation and arguments are contiguous spans from the QA pair. Formally, let (q, a) be a QA pair, where question q = (q1, q2, ..., qm) and answer a = (a1, a2, ..., an) are word sequences. The output is a triple (arg1,rel,arg2) extracted from (q, a). The output triple can be naturally interpreted as a sequence y = (y1, y2, ..., yo) where yi is either a word or a placeholder tag (\u27e8arg1\u27e9, \u27e8rel\u27e9, \u27e8arg2\u27e9) that marks relevant portions of the triple. In OPENIE, the ex- tracted tuple should be asserted by the input QA pair. Formulating this, therefore, requires the vo- cabulary of y to be restricted to the vocabulary of (q, a) and placeholder tags.",
  "In OPENIE, the ex- tracted tuple should be asserted by the input QA pair. Formulating this, therefore, requires the vo- cabulary of y to be restricted to the vocabulary of (q, a) and placeholder tags. Following this de\ufb01nition, our aim is to di- rectly model the conditional probability p(y|q, a) of mapping input sequences q and a into an output sequence: P(y|q, a) = o Y i=1 p(yi|y1, . . . , yi\u22121, q, a). (1) In our formulation, a triple is generated as a se- quence: a head argument phrase arg1, followed by a relation phrase rel and a tail argument phrase arg2. It is possible to consider different orderings in the output sequence (such as (rel,arg1,arg2)). However, the goal of OPENIE is to identify the re- lation phrase that holds between a pair of argu- ments. Our representation is, thus, consistent with this de\ufb01nition as it models the relation phrase to depend on the head argument.",
  "However, the goal of OPENIE is to identify the re- lation phrase that holds between a pair of argu- ments. Our representation is, thus, consistent with this de\ufb01nition as it models the relation phrase to depend on the head argument. 3 NeurON: A Multi-Encoder Constrained-Decoder Model Overview of NEURON We propose to extract tuples using a variation of an encoder-decoder RNN architecture (Cho et al., 2014) operating on variable-length sequences of tokens. Fig. 1 shows the architecture of NEURON. It uses two encoders to encode question and answer sequences in a QA pair separately into \ufb01xed-length vector represen- tations. A decoder then decodes the vector rep- resentations into a variable-length sequence corre- sponding to the tuple. The decoder is integrated",
  "Is the indoor open Question Encoder Constrained-Decoder vocab-mask  combiner tag-mask x x \u27e8arg1\u27e9 indoor pool .. It opens at 10am Answer Encoder attention relevance scoring \u27e8s\u27e9 \u27e8arg1\u27e9 indoor .. Soft Constraints Hard Constraints ] ] Figure 1: Multi-Encoder, Constrained-Decoder model for tuple extraction from (q, a). with a set of hard constraints (e.g., output vocabu- lary) and soft constraints (e.g., relevance scoring) suited for the extraction task. 3.1 Multiple Encoders Given an input QA pair, two RNN encoders sepa- rately encode the question and answer. The ques- tion encoder converts q into hidden representa- tion hq = (hq 1, ..., hq m) and the answer encoder converts a into ha = (ha 1, ..., hq n), where hq t = lstm(qt, hq t\u22121) is a non-linear function represented by the long short-term memory (LSTM) cell.",
  "The combiner combines the encoders\u2019 states and ini- tializes the hidden states h for the decoder: h = tanh(Wc[hq \u25e6ha]), where \u25e6denotes concatenation. The decoder stage uses the hidden states to generate the output y with another LSTM-based RNN. The probability of each token is de\ufb01ned as: p(yt) = softmax((st \u25e6cq t \u25e6ca t )Wy), (2) where st denotes the decoder state, s0 = h and st = lstm((yt\u22121 \u25e6cq t \u25e6ca t )Ws, st\u22121). The de- coder is initialized by the last hidden state from the combiner. It uses the previous output token at each step. Both Wy and Ws are learned matri- ces. Each decoder state is concatenated with con- text vectors derived from the hidden states of the encoders. Context vector ct is the weighted sum of the encoder hidden states, i.e. cq t = Pm i=1 \u03b1tihq i , where \u03b1ti corresponds to an attention weight.",
  "Each decoder state is concatenated with con- text vectors derived from the hidden states of the encoders. Context vector ct is the weighted sum of the encoder hidden states, i.e. cq t = Pm i=1 \u03b1tihq i , where \u03b1ti corresponds to an attention weight. The attention model (Bahdanau et al., 2015) helps the model learn to focus on speci\ufb01c parts of the in- put sequences, instead of solely relying on hidden vectors of the decoders\u2019 LSTM. This is crucial for extraction from (q, a) pairs where input sequences tend to be long. 3.2 Constrained Decoder The decoder \ufb01nds the best hypothesis (i.e., the best output sequence) for the given input representa- tions. Typically, the output sequence is generated, one unit at a time, using beam search. At each time step, the decoder stores the top-k scoring par- tial sequences, considers all possible single token extensions of them, and keeps k most-likely se- quences based on model\u2019s probabilities (Eq. 1).",
  "At each time step, the decoder stores the top-k scoring par- tial sequences, considers all possible single token extensions of them, and keeps k most-likely se- quences based on model\u2019s probabilities (Eq. 1). As soon as the \u27e8/S\u27e9symbol is appended, the sequence is removed from the beam and added to the set of complete sequences. The most-likely complete se- quence is \ufb01nally generated. Hard Constraints While such encoder-decoder models typically outperform conventional ap- proaches (Cho et al., 2014; Zoph and Knight, 2016; Xiong et al., 2017) on a wide variety of tasks including machine translation and question answering, the accuracy and training ef\ufb01ciency has been shown to improve when the model is in- tegrated with the constraints of the output domain (Xiao et al., 2016; Yin and Neubig, 2017). Moti- vated by these, NEURON allows constraints relevant to information extraction to be incorporated in the model. Speci\ufb01cally, we describe how the decoder can enforce vocabulary and structural constraints on the output. \u2022 Vocabulary constraints.",
  "Moti- vated by these, NEURON allows constraints relevant to information extraction to be incorporated in the model. Speci\ufb01cally, we describe how the decoder can enforce vocabulary and structural constraints on the output. \u2022 Vocabulary constraints. Since the arguments and relations in the extracted tuples typically cor- respond to the input QA pair, the decoder must constraint the space of next valid tokens when gen- erating the output sequence. NEURON uses a mask- ing technique in the decoder to mask the probabil- ity of tokens (as in Eq. 2) that do not appear in the input (q, a) pair. Speci\ufb01cally, it computes a binary mask vector v, where |v| is vocabulary size and vi = 1 if and only if i-th token appears in q or a. The probability of each token is modi\ufb01ed as: p(yt) = softmax((st \u25e6cq t \u25e6ca t )Wy \u2297v), (3)",
  "\u27e8S \u27e9 \u27e8S \u27e9: (V \u222aT )\\\u27e8arg1\u27e9 B-arg1 I-arg1 I-arg\u2032!1 B-rel I-rel I-rel\u2032 B-arg1 I-arg1 I-arg\u2032!1 \u27e8/S \u27e9 \u27e8rel\u27e9: T \u27e8arg2\u27e9: T \u27e8arg1\u27e9: T w \u2208V : T \\\u27e8/arg1\u27e9 w \u2208V : T \\\u27e8/arg1\u27e9 w \u2208V : T \\\u27e8/rel\u27e9 w \u2208V : T \\\u27e8/rel\u27e9 w \u2208V : T \\\u27e8/arg2\u27e9 w \u2208V : T \\\u27e8/arg2\u27e9 (inpu t) : (tag mask) \u27e8/arg1\u27e9: (V \u222aT )\\\u27e8rel\u27e9 \u27e8/rel\u27e9: (V \u222aT )\\\u27e8arg2\u27e9 \u27e8/arg2\u27e9: (V \u222aT )\\\u27e8/S \u27e9 Figure 2: State diagram for tag masking rules. V is the vocabulary including placeholder tags, T is the set of placeholder tags. where \u2297indicates element-wise multiplication. \u2022 Structural constraints.",
  "V is the vocabulary including placeholder tags, T is the set of placeholder tags. where \u2297indicates element-wise multiplication. \u2022 Structural constraints. For the output se- quence to correspond to a valid tuple with non- empty arguments, the decoding process must con- form to the underlying grammar of a tuple. For instance, decoding should always begin in the \u27e8S\u27e9 state, where only \u27e8arg1\u27e9can be generated. In subsequent time steps, all other placeholders ex- cept \u27e8/arg1\u27e9should be restricted to ensure a non- empty argument. Once \u27e8/arg1\u27e9is generated, \u27e8rel\u27e9 must be generated in the next time step and so on. The various states and grammar rules can be de- scribed as a \ufb01nite state transducer as shown in Fig- ure 2. Depending upon the state, NEURON generates a mask r based on this grammar and uses r to further modify the probabilities of the tokens as follows: p(yt) = softmax((st \u25e6cq t \u25e6ca t )Wy \u2297v \u2297r).",
  "Depending upon the state, NEURON generates a mask r based on this grammar and uses r to further modify the probabilities of the tokens as follows: p(yt) = softmax((st \u25e6cq t \u25e6ca t )Wy \u2297v \u2297r). (4) Soft Constraints OPENIE systems are typically used to extract broad-coverage facts to extend ex- isting KBs. Facts already existing in the KB are representative of the domain of the KB. It is, there- fore, useful to incorporate this prior knowledge in the extraction itself. NEURON is able to use prior knowledge (incorporated as soft constraints) in the decoder to understand the relevance of candidate extractions and adjust the ranking of various out- put sequences accordingly. To see why such soft constraints can be useful, consider the example: Example 3. Q: \u201cIs the pool open?\u201d A: \u201cI am sorry but our pool reopens at 7:00am.\u201d Tuple: \u27e8I, am, sorry\u27e9; \u27e8pool, reopens at, 7:00am\u27e9 Both the tuple facts are correct given the input QA pair but only the second tuple contains useful in- formation.",
  "Filtering such irrelevant facts is dif\ufb01- cult without additional evidence. The multi-encoder and constrained-decoder in NEURON are jointly optimized to maximize the log probability of output sequence conditioned on the input sequences. At inference, the decoder estimates the likelihood of various candidate se- quences and generates the sequence with the high- est likelihood. As shown in Eq. 1, this likelihood is conditioned solely on the input (q, a) pair, thus increasing the possibility of obtaining facts that may be correct but irrelevant. Instead, if a rele- vance scoring function were integrated at extrac- tion time, the candidate output sequences could be re-ranked so that the predicted output sequence is likely to be both correct and relevant. Learning a relevance scoring function can be modeled as a KB completion task, where miss- ing facts have to be inferred from existing ones. A promising approach is to learn vector represen- tations of entities and relations in a KB by max- imizing the total plausibility of existing facts in the KB (Wang et al., 2017).",
  "A promising approach is to learn vector represen- tations of entities and relations in a KB by max- imizing the total plausibility of existing facts in the KB (Wang et al., 2017). For a new candidate output sequence, its plausibility can be predicted using the learned embeddings for the entities and relation in the sequence. In NEURON, we learn the entity and relation embeddings using Knowledge Embedding (KE) methods such as TransE (Bordes et al., 2013) and HolE (Nickel et al., 2016). Note that NEURON is \ufb02exible with how the relevance scoring func- tion is learned or which KE method is chosen. In this paper, we use TransE for evaluation. TransE computes the plausibility score S of a tuple y = \u27e8arg1, rel, arg2\u27e9as: S(y) = ||varg1 + vrel \u2212varg2||, where varg1, vrel, and varg2 are embedding vec- tors for arg1, rel, and arg2 respectively.",
  "Follow- ing (Jain et al., 2018), we compute the embed- ding vectors of out-of-vocabulary arguments (and relations) as the average of embedding vectors of known arguments (and relations). We generate the most-likely output based on its conditional proba- bility and plausibility score: \u02c6y = argmax y (log P(y|q, a) + \u03b3 log S(y)). (5) To implement the tuple relevance scoring func- tion, we employ the re-ranking approach, which is a common technique for sequence genera- tion methods (Luong et al., 2015b). Our re- ranking method \ufb01rst obtains candidates from a beam search decoder and then re-ranks the can-",
  "didates based on the objective function (Eq. 5). 4 Experiments We evaluated the performance of NEURON on two CQA datasets. In our analysis, we \ufb01nd that inte- grating hard and soft constraints in the decoder improved the extraction performance irrespective of the number of encoders used. Also, 15-25% of the tuples extracted by NEURON were not extracted by state-of-the-art sentence-based methods. 4.1 Datasets and Data Preparation ConciergeQA is a real-world internal corpus of 33,158 QA pairs collected via a multi-channel communication platform for guests and hotel staff. Questions (answers) are always made by guests (staff). An utterance has 36 tokens on average, and there are 25k unique tokens in the dataset. A QA utterance has 2.8 sentences on average, with the question utterance having 1.02 sentences on aver- age and answer utterance having 1.78 sentences on average. AmazonQA (Wan and McAuley, 2016; McAuley and Yang, 2016) is a public dataset with 314,264 QA pairs about electronic products on ama- zon.com.",
  "AmazonQA (Wan and McAuley, 2016; McAuley and Yang, 2016) is a public dataset with 314,264 QA pairs about electronic products on ama- zon.com. The dataset contains longer and more diverse utterances than the ConciergeQA dataset: utterances have an average of 45 tokens and the vocabulary has more than 50k unique tokens. A QA utterance has 3.5 sentences on average. The question utterances had 1.5 sentences on average and the answer having 2 sentences. For training NEURON, we bootstrapped a large number of high-quality training examples using a state-of-the-art OPENIE system. Such bootstrap- ping has been shown to be effective in informa- tion extraction tasks (Mausam et al., 2012; Saha et al., 2017). The StanfordIE (Angeli et al., 2015) system is used to extract tuples from QA pairs for training examples. To further obtain high-quality tuples, we \ufb01ltered out tuples that occur too infre- quently (< 5) or too frequently (> 100).",
  "The StanfordIE (Angeli et al., 2015) system is used to extract tuples from QA pairs for training examples. To further obtain high-quality tuples, we \ufb01ltered out tuples that occur too infre- quently (< 5) or too frequently (> 100). For each tuple in the set, we retrieved all QA pairs that con- tain all the content words of the tuple and included them in the training set. This helps create a train- ing set encapsulating the multiplicity of ways in which tuples are expressed across QA pairs. We randomly sampled 100 QA pairs from our boot- strapping set and found 74 of them supported the corresponding tuples. We \ufb01nd this quality of boot- strapped dataset satisfactory, since the seed tuples Instance type ConciergeQA AmazonQA Exclusively from question 13.9% 13.8% Exclusively from answer 25.8% 17.6% Ambiguous 36.9% 29.8% Jointly from Q-A 23.4% 38.8% Table 1: Various types of training instances.",
  "Dataset Q-A |V| Train Dev Test (Q-A) ConciergeQA 33k 25k 1.25M 128k 2,905 AmazonQA 314k 50k 1.43M 159k 39,663 Table 2: Training, Dev and Test splits. for bootstrapping could be noisy as they were gen- erated by another OPENIE system. Our bootstrapped dataset included training in- stances where a tuple matched (a) tokens in the questions exclusively, (b) tokens in the answers exclusively, (c) tokens from both questions and an- swers. Table 1 shows the distribution of the var- ious types of training instances. Less than 40% (30%) of ground truth tuples for ConciergeQA (AmazonQA) exclusively appear in the questions or answers. Also, 22.1% (37.2%) of ground truth tuples for ConciergeQA (AmazonQA) are ex- tracted from the combination of questions and an- swers. These numbers support our motivation of extracting tuples from QA pairs.",
  "Also, 22.1% (37.2%) of ground truth tuples for ConciergeQA (AmazonQA) are ex- tracted from the combination of questions and an- swers. These numbers support our motivation of extracting tuples from QA pairs. We used standard techniques to construct training/dev/test splits so that QA pairs in the three sets are disjoint. Table 2 shows the details of the various subsets. 4.2 Baseline Approaches We compared NEURON with two methods that can be trained for tuple extraction from QA pairs: BILSTM-CRF (Huang et al., 2015) and NEURALOPE- NIE (Cui et al., 2018). BILSTM-CRF is a sequence tagging model that has achieved state-of-the-art accuracy on POS, chunking, NER and OPENIE (Stanovsky et al., 2018) tasks. For OPENIE, the model predicts boundary labels (e.g., B-ARG1, I- ARG1, B-ARG2, O) for the various tokens in a QA pair. NEURALOPENIE is an encoder-decoder model that generates a tuple sequence given an in- put sequence.",
  "For OPENIE, the model predicts boundary labels (e.g., B-ARG1, I- ARG1, B-ARG2, O) for the various tokens in a QA pair. NEURALOPENIE is an encoder-decoder model that generates a tuple sequence given an in- put sequence. Since it uses a single encoder, we generate the input sequence by concatenating the question and answer in a QA pair. We trained all the models using the same training data. 4.3 Performance Metrics We examine the performance of different methods using three metrics: precision, recall, and relative coverage (RC). Given a QA pair, each system re-",
  "turns a sequence. We label the sequence correct if it matches one of the ground-truth tuples for the QA pair, incorrect otherwise. We then measure precision of a method (i.e., # of correct predictions of the method / # of question-answer pairs) and recall (i.e., # of correct predictions of the method / # of correct predictions of any method) follow- ing (Stanovsky and Dagan, 2016). To compare the coverage of sequences extracted by NEURON against the baseline method, we compute relative coverage of NEURON as the fraction of all cor- rect predictions that were generated exclusively by NEURON. Speci\ufb01cally, RC = |TPNEURON\\TPbaseline| |TPNEURON S TPbaseline|, where TP denotes the correct predictions. 4.4 Model Training and Optimization We implemented NEURON using OpenNMT- tf (Klein et al., 2017) , an open-source neural machine translation system that supports multi- source encoder-decoder models. We implemented NEURALOPENIE using the same system.",
  "4.4 Model Training and Optimization We implemented NEURON using OpenNMT- tf (Klein et al., 2017) , an open-source neural machine translation system that supports multi- source encoder-decoder models. We implemented NEURALOPENIE using the same system. We used the open-source implementation of BILSTM- CRF (Reimers and Gurevych, 2017). For fair com- parison, we used identical con\ufb01gurations for NEU- RON and NEURALOPENIE. Each encoder used a 3- layer bidirectional LSTM and the decoder used a 3-layer bidirectional LSTM. The models used 256-dimensional hidden states, 300-dimensional word embeddings, and a vocabulary size of 50k. The word embeddings were initialized with pre- trained GloVe embeddings (glove.6B) (Penning- ton et al., 2014). We used an initial learning rate of 1 and optimized the model with stochastic gradi- ent descent. We used a decay rate of 0.7, a dropout rate of 0.3 and a batch size of 64.",
  "We used an initial learning rate of 1 and optimized the model with stochastic gradi- ent descent. We used a decay rate of 0.7, a dropout rate of 0.3 and a batch size of 64. The models were trained for 1M steps for the ConciergeQA dataset and 100k steps for the AmazonQA dataset. We used TESLA K80 16GB GPU for training the models. We trained the KE models for relevance scoring using our bootstrapped training dataset. For integrating the relevance scoring function, we experimented with different values for \u03b3 and found it not have a major impact within a range of 0.02 to 0.2. We used a value of 0.05 in all the experi- ments. 4.5 Experimental Results The BILSTM-CRF model showed extremely low (2-15%) precision values.",
  "We used a value of 0.05 in all the experi- ments. 4.5 Experimental Results The BILSTM-CRF model showed extremely low (2-15%) precision values. Very few of the tagged Method P R RC NEURALOPENIE (baseline) 0.769 0.580 - + hard constraints 0.776 0.585 - + hard and soft constraints 0.796 0.600 - NEURON (our method) 0.791 0.597 0.224 + hard constraints 0.792 0.597 0.204 + hard and soft constraints 0.807 0.608 0.245 Table 3: Precision (P), Recall (R), and Relative Cover- age (RC) results on ConciergeQA. sequences (32-39%) could be converted to a tuple. Most tagged sequences had multiple relations and arguments, indicating that it is dif\ufb01cult to learn how to tag a sequence corresponding to a tuple. The model only learns how to best predict tags for each token in the sequence, and does not take into account the long-range dependencies to previously predicted tags.",
  "Most tagged sequences had multiple relations and arguments, indicating that it is dif\ufb01cult to learn how to tag a sequence corresponding to a tuple. The model only learns how to best predict tags for each token in the sequence, and does not take into account the long-range dependencies to previously predicted tags. This is still an open problem and is outside the scope of this paper. Tables 3 and 4 show the performance of NEU- RALOPENIE and NEURON on the two CQA datasets. NEURON achieves higher precision on both the datasets. This is because NEURALOPENIE uses a sin- gle encoder to interpret the question and answer in the same vector space, which leads to lower per- formance. Furthermore, concatenating the ques- tion and answer makes the input sequence too long for the decoder to capture long-distance depen- dencies in history (Zhang et al., 2016; Toral and S\u00b4anchez-Cartagena, 2017). Despite the attention mechanism, the model ignores past alignment in- formation. This makes it less effective than the dual-encoder model used in NEURON.",
  "Despite the attention mechanism, the model ignores past alignment in- formation. This makes it less effective than the dual-encoder model used in NEURON. The tables also show that incorporating task- speci\ufb01c hard constraints helps further improve the overall precision and recall, regardless of the methods and the datasets. Re-ranking the tuples based on the soft constraints derived from the ex- isting KB further improves the performance of both methods in ConciergeQA and NEURALOPE- NIE in AmazonQA. The existing KB also helps boost the likelihood of a correct candidate tuple se- quence that was otherwise scored to be less likely. Lastly, we found that NEURON has signi\ufb01cant rela- tive coverage; it discovered signi\ufb01cant additional, unique tuples missed by NEURALOPENIE. Table 4 shows a slight decrease in performance for NEURON after soft constraints are added. This is likely caused by the lower quality KE model due to the larger vocabulary in AmazonQA. In con- trast, even with the lower quality KE model, NEU-",
  "Method P R RC NEURALOPENIE (baseline) 0.557 0.594 - + hard constraints 0.563 0.601 - + hard and soft constraints 0.571 0.610 - NEURON (our method) 0.610 0.652 0.139 + hard constraints 0.631 0.674 0.164 + hard and soft constraints 0.624 0.666 0.149 Table 4: Precision (P), Recall (R), and Relative Cover- age (RC) results on AmazonQA dataset. RALOPENIE improved slightly. This is likely be- cause the NEURALOPENIE model, at this stage, still had a larger margin for improvement. We note however that learning the best KE model is not the focus of this work. AmazonQA is a more challenging dataset than ConciergeQA: longer utterances (avg. 45 tokens vs. 36 tokens) and richer vocabulary (> 50k unique tokens vs. < 25k unique tokens). This is re\ufb02ected in lower precision and recall values of both the systems on the AmazonQA dataset.",
  "45 tokens vs. 36 tokens) and richer vocabulary (> 50k unique tokens vs. < 25k unique tokens). This is re\ufb02ected in lower precision and recall values of both the systems on the AmazonQA dataset. While the performance of end-to-end extraction systems depends on the complexity and diversity of the dataset, incorporating hard and soft constraints al- leviates the problem to some extent. End-to-end extraction systems tend to out- perform rule-based systems on extraction from CQA datasets. We observed that training data for ConciergeQA had a large number (> 750k) dependency-based pattern rules, of which < 5% matched more than 5 QA pairs. The set of rules is too large, diverse and sparse to train an accurate rule-based extractor. Even though our training data was generated by bootstrap- ping from a rule-based extractor StanfordIE, we found only 51.5% (30.7%) of correct tuples from NEURON exactly matched the tuples from Stan- fordIE in ConciergeQA (AmazonQA). This indi- cates that NEURON combined information from question and answer, otherwise not accessible to sentence-wise extractors.",
  "This indi- cates that NEURON combined information from question and answer, otherwise not accessible to sentence-wise extractors. As an evidence, we found 11.4% (6.1%) of tuples were extracted from answers, 16.8% (5.0%) from questions, while 79.6% (82.5%) combined information from ques- tions and answers in ConciergeQA (AmazonQA). Multiple Encoders: Our motivation to use dif- ferent encoders for questions and answers is based on the assumption that they use different vocab- ulary and semantics. We found that there were 8k (72k) unique words in questions, 18k (114k) unique words in answers, and the Jaccard co- Figure 3: Example embedding vectors from question and answer encoders. Underlines denote similar em- bedding vectors in both the encoders. ef\ufb01cient between two vocabulary sets was 0.25 (0.25) in ConciergeQA (AmazonQA), indicating that two sources use signi\ufb01cantly different vocab- ulary.",
  "Underlines denote similar em- bedding vectors in both the encoders. ef\ufb01cient between two vocabulary sets was 0.25 (0.25) in ConciergeQA (AmazonQA), indicating that two sources use signi\ufb01cantly different vocab- ulary. Also, the same word can have different meanings depending on a speaker, and thus such words in the two sources should be embedded dif- ferently. To visualize the embedding vectors of common words in ConciergeQA, we mapped them into 2D space using t-SNE (Maaten and Hinton, 2008). Fig. 3 shows that subjective words that rep- resents speakers attitude (e.g., \u201cready\u201d, \u201cguests\u201d, \u201ctime\u201d) had signi\ufb01cantly different embeddings in the question and answer encoders. In contrast, ob- jective words such as menu, or activity (e.g., \u201cba- con\u201d, \u201ccruise\u201d, \u201cweekday\u201d) had similar embed- dings although the two encoders do not directly share the embedding parameters.",
  "In contrast, ob- jective words such as menu, or activity (e.g., \u201cba- con\u201d, \u201ccruise\u201d, \u201cweekday\u201d) had similar embed- dings although the two encoders do not directly share the embedding parameters. This indicates that multiple encoders not only capture the differ- ent meanings in questions and answers but also re- tain consistent meanings for words that keep the same meanings in the two sources. Relevance Scoring: We compared with another NEURON model that uses HolE (Nickel et al., 2016) for relevance scoring. Both the HolE and TransE models achieved the same precision of 80.7%, with HolE achieving slightly higher re- call (+1.4%). This suggests that incorporating rel- evance scoring in NEURON can robustly improve the extraction accuracy, regardless of the choice of the knowledge embedding method. We also esti- mated the upper-bound precision by evaluating if the correct tuple was included in the top-500 can- didates. The upper-bound precision was 85.0% on ConciergeQA, indicating that there is still room for improvement on incorporating relevance scoring.",
  "1 Q: Tell me what the username and password is for WiFi A: Absolutely! Both the username and passcode is C800. StanfordIE: \u27e8passcode, is, C800 \u27e9 NEURON: \u27e8password, is, C800 \u27e9 2 Q: Do hotel guys have ice? A: There is an ice machine on \ufb01rst \ufb02oor lobby. StanfordIE: \u27e8hotel, do, ice \u27e9 NEURON: \u27e8hotel, have, ice machine \u27e9 3 Q: Is there a charge for parking a rental car on the property? A: Self-parking will be $15 per night. StanfordIE: None NEURON: \u27e8parking, will, charge \u27e9 4 Q: arrange late check out for tomorrow? A: I have notated a 12 pm check out. Normal check out time is at 11 am. StanfordIE: \u27e8normal check, is at, 11 am \u27e9 NEURON: \u27e8check, is at, 11 \u27e9 Table 5: Examples of successful cases (1 and 2) and failed cases (3 and 4) from test data.",
  "StanfordIE: \u27e8normal check, is at, 11 am \u27e9 NEURON: \u27e8check, is at, 11 \u27e9 Table 5: Examples of successful cases (1 and 2) and failed cases (3 and 4) from test data. 4.6 Error Analysis We examined a random sample of 100 errors shared by all the systems across the tested datasets. Arguably, encoder-decoder models suffer when extracting tuples from long utterances (avg. of 54 tokens), contributing to 43% of the errors. 34% of the incorrectly extracted tuples used words that were shared across the two sources. This indicates that the extractor makes errors when resolving am- biguity in tokens. 28% of the error cases used in- formal language that is generally dif\ufb01cult for any extractor to understand. We show some examples (1 and 2 in Table 5) where NEURON successfully combined information across two sources and ex- amples (3 and 4 in Table 5) where it failed.",
  "We show some examples (1 and 2 in Table 5) where NEURON successfully combined information across two sources and ex- amples (3 and 4 in Table 5) where it failed. We further examined three different scenar- ios: a) errors are shared by both NEURON and NEURALOPENIE, b) errors are made exclusively by NEURON, c) errors are made exclusively by NEU- RALOPENIE. For each scenario, we examined a ran- dom sample of 100 errors. We categorize the dif- ferent sources of errors and report the results in Table 6. As shown, NEURON is superior on longer utterances compared to NEURALOPENIE (54 tokens vs. 49 tokens). However, ambiguity in tokens in the two sources is a concern for NEURON because it has the \ufb02exibility to interpret the question and answer differently. Not surprisingly, informal ut- terances are hard to translate for both the systems. 5 Case Study - KB Extension The extracted tuples from NEURON can be used to extend a KB for a speci\ufb01c domain.",
  "Not surprisingly, informal ut- terances are hard to translate for both the systems. 5 Case Study - KB Extension The extracted tuples from NEURON can be used to extend a KB for a speci\ufb01c domain. However, au- tomatically fusing the tuples with existing facts in Error Category N, B N, B N, B long utterances 43% 45% 40% avg. length of utterance 54 tokens 49 tokens 54 tokens ambiguity 34% 36% 48% informal language 28% 36% 34% Table 6: Different errors N and B made by NEURON (N) and NEURALOPENIE (B) respectively. Knowledge Base Integrator Facts Candidate Facts Multi-Encoder  Constrained Decoder Knowledge  Embedding Model QA Corpus Figure 4: Human-in-the-loop system for extending a domain-speci\ufb01c KB. the KB can have limited accuracy. This can be due to noise in the source conversation, no prior knowledge of join rules and more.",
  "the KB can have limited accuracy. This can be due to noise in the source conversation, no prior knowledge of join rules and more. One possi- ble solution is to design a human-in-the-loop sys- tem that iteratively extracts tuples and \ufb01lters them based on human feedback (Fig. 4). In each itera- tion, a set of tuples is annotated by human annota- tors based on their relevance to the domain of the KB. The tuples marked relevant are added to the KB and the relevance scoring function is updated for extracting more relevant tuples from the corpus in the next iteration. We conducted a crowdsourced experiment1, simulating the \ufb01rst iteration of the procedure i.e., when no KE model is available. We collected an- notations on top-5 tuples extracted by NEURON for 200 QA pairs in the ConciergeQA dataset. For re- liability, we hired \ufb01ve workers for each extraction. The workers were asked to judge if a tuple is rel- evant to the hotel domain and represents concrete information to be added to a KB.",
  "For re- liability, we hired \ufb01ve workers for each extraction. The workers were asked to judge if a tuple is rel- evant to the hotel domain and represents concrete information to be added to a KB. We found preci- sion@5 was 41.4%, and NEURON extracted at least one useful tuple for 83.0% of the 200 QA pairs. Overall, the system added 243 unique tuples (out of 414 tuples extracted by NEURON) to the KB. We also collected annotations for the tuples extracted by NEURALOPENIE. The precision@5 and recall@5 values were 41.3% and 79.0% respectively. Al- though the precision values are quite similar, NEU- RON can extract correct tuples from more QA pairs than NEURALOPENIE. While the precision can fur- ther be improved, the preliminary results support that NEURON is a good candidate for extraction in 1 https://www.figure-eight.com/",
  "a human-in-the-loop system for KB extension. We did not use any sophisticated methods for ranking tuples in our experiment. Thus, a better ranking algorithm might lead to improved precision. 6 Related Work There is a long history of OPENIE systems for extracting tuples from plain text. They are built on hand-crafted patterns over an intermedi- ate representation of a sentence (e.g., POS tags (Yates et al., 2007; Fader et al., 2011), depen- dency trees (Bhutani et al., 2016; Mausam et al., 2012)). Such rule-based systems require exten- sive engineering when the patterns become di- verse and sparse. Recently, OPENIE systems based on end-to-end frameworks, such as sequence tag- ging (Stanovsky et al., 2018) or sequence-to- sequence generation (Cui et al., 2018), have been shown to alleviate such engineering efforts. How- ever, all these systems focus on sentence-level ex- traction. We are the \ufb01rst to address the problem of extracting tuples from question-answer pairs.",
  "How- ever, all these systems focus on sentence-level ex- traction. We are the \ufb01rst to address the problem of extracting tuples from question-answer pairs. Our proposed system is based on an encoder- decoder architecture, which was \ufb01rst introduced by Cho et al. for machine translation. Atten- tion mechanisms (Bahdanau et al., 2015; Luong et al., 2015b) have been shown to be effective for mitigating the problem of poor translation per- formance on long sequences. Their model can learn how much information to retrieve from spe- ci\ufb01c parts of the input sequence at decoding time. There is abundant research on generalizing such frameworks for multiple tasks, specially by em- ploying multiple encoders. Using multiple en- coders has been shown to be useful in mutli-task learning (Luong et al., 2015a), multi-source trans- lation (Zoph and Knight, 2016) and reading com- prehension (Xiong et al., 2017). We are the \ufb01rst to explore a multi-source encoder-decoder architec- ture for extracting tuples from CQA datasets.",
  "We are the \ufb01rst to explore a multi-source encoder-decoder architec- ture for extracting tuples from CQA datasets. Traditional encoder-decoder architectures are not tailored for information extraction and knowl- edge harvesting. To make them suitable for infor- mation extraction, the sequence generation must be subjected to several constraints on the vocabu- lary, grammar etc. Recently, grammar structures have been integrated into encoder-decoder mod- els (Iyer et al., 2017; Zhang et al., 2017). There are variations such as Pointer Networks (Vinyals et al., 2015) that yield a succession of pointers to tokens in the input sequence. All these studies share a common idea with our paper, which is to enforce constraints at sequence generation time. Since we focus on extraction from CQA datasets, our work is broadly related to the literature on relation extrac- tion (Savenkov et al., 2015; Hixon et al., 2015; Wu et al., 2018) and ontology extraction (S and Ku- mar, 2018) from community generated question- answer datasets.",
  "However, we differ in our under- lying assumption that the relations and entities of interest are not known in advance. Alternatively, a CQA dataset could be transformed into declara- tive sentences (Demszky et al., 2018) for a conven- tional OPENIE system. However, such a two-stage approach is susceptible to error propagation. We adopt an end-to-end solution that is applicable to generic CQA datasets. 7 Conclusions and Future Work We have presented NEURON, a system for extract- ing structured data from QA pairs for the purpose of enriching knowledge bases. NEURON uses a multi-encoder, constrained-decoder framework to generate quality tuples from QA pairs. NEURON achieves the highest precision and re- call in extracting tuples from QA pairs compared with state-of-the-art sentence-based models, with a relative improvement as high as 13.3%. It can discover 15-25% more tuples which makes it suit- able as a tuple extraction tool for KB extension. There are several directions for future research.",
  "It can discover 15-25% more tuples which makes it suit- able as a tuple extraction tool for KB extension. There are several directions for future research. One interesting direction is to investigate whether NEURON can be extended to work on open-domain QA corpus, which may not be restricted to any speci\ufb01c domain. Acknowledgement We thank Tom Mitchell and the anonymous re- viewers for their constructive feedback. This work was supported in part by the UM Of\ufb01ce of Re- search. References Gabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D Manning. 2015. Leveraging linguis- tic structure for open domain information extraction. In Proc. ACL \u201915/IJCNLP \u201915, pages 344\u2013354. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate. In Proc. ICLR \u201915. Michele Banko, Michael J. Cafarella, Stephen Soder- land, Matt Broadhead, and Oren Etzioni. 2007.",
  "Open information extraction from the web. In Proc. IJCAI \u201907, pages 2670\u20132676. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proc. EMNLP \u201913, pages 1533\u20131544. Nikita Bhutani, HV Jagadish, and Dragomir Radev. 2016. Nested propositions in open information ex- traction. In Proc. EMNLP \u201916, pages 55\u201364. Antoine Bordes, Nicolas Usunier, Alberto Garcia- Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi- relational data. In Proc. NIPS \u201913, pages 2787\u2013 2795. Kyunghyun Cho, Bart van Merrienboer, Caglar Gul- cehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014.",
  "Kyunghyun Cho, Bart van Merrienboer, Caglar Gul- cehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation. In Proc. EMNLP \u201914, pages 1724\u20131734. Lei Cui, Furu Wei, and Ming Zhou. 2018. Neural open information extraction. In Proc. ACL \u201918, pages 407\u2013413. Dorottya Demszky, Kelvin Guu, and Percy Liang. 2018. Transforming question answering datasets into natural language inference datasets. arXiv preprint arXiv:1809.02922. Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information ex- traction. In Proc. EMNLP \u201911, pages 1535\u20131545. Ben Hixon, Peter Clark, and Hannaneh Hajishirzi. 2015.",
  "2011. Identifying relations for open information ex- traction. In Proc. EMNLP \u201911, pages 1535\u20131545. Ben Hixon, Peter Clark, and Hannaneh Hajishirzi. 2015. Learning knowledge graphs for question an- swering through conversational dialog. In Proc. NAACL-HLT \u201915, pages 851\u2013861. Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidi- rectional LSTM-CRF models for sequence tagging. arXiv preprint arXiv:1508.01991. Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant Krishnamurthy, and Luke Zettlemoyer. 2017. Learning a neural semantic parser from user feed- back. In Proc. ACL \u201917, pages 963\u2013973. Prachi Jain, Shikhar Murty, Mausam, and Soumen Chakrabarti. 2018. Mitigating the effect of out-of- vocabulary entity pairs in matrix factorization for KB inference. In Proc.",
  "ACL \u201917, pages 963\u2013973. Prachi Jain, Shikhar Murty, Mausam, and Soumen Chakrabarti. 2018. Mitigating the effect of out-of- vocabulary entity pairs in matrix factorization for KB inference. In Proc. IJCAI \u201918, pages 4122\u2013 4129. Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush. 2017. OpenNMT: Open-source toolkit for neural machine translation. In Proc. ACL \u201917 (System Demonstrations), pages 67\u201372. Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. 2015a. Multi-task se- quence to sequence learning. In Proc. ICLR \u201916. Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. 2015b. Effective approaches to attention- based neural machine translation. In Proc. EMNLP \u201915, pages 1412\u20131421.",
  "In Proc. ICLR \u201916. Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. 2015b. Effective approaches to attention- based neural machine translation. In Proc. EMNLP \u201915, pages 1412\u20131421. Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of Machine Learning Research, 9(Nov):2579\u20132605. Mausam, Michael Schmitz, Robert Bart, Stephen Soderland, Oren Etzioni, et al. 2012. Open lan- guage learning for information extraction. In Proc. EMNLP \u201912, pages 523\u2013534. Julian McAuley and Alex Yang. 2016. Addressing complex and subjective product-related queries with customer reviews. In Proc. WWW \u201916, pages 625\u2013 635. Maximilian Nickel, Lorenzo Rosasco, and Tomaso Poggio. 2016. Holographic embeddings of knowl- edge graphs. In Proc.",
  "In Proc. WWW \u201916, pages 625\u2013 635. Maximilian Nickel, Lorenzo Rosasco, and Tomaso Poggio. 2016. Holographic embeddings of knowl- edge graphs. In Proc. AAAI \u201916, pages 1955\u20131961. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for word representation. In Proc. EMNLP \u201914, pages 1532\u2013 1543. Nils Reimers and Iryna Gurevych. 2017. Reporting score distributions makes a difference: Performance study of LSTM-networks for sequence tagging. In Proc. EMNLP \u201917, pages 338\u2013348. Subhashree S and P Sreenivasa Kumar. 2018. En- riching domain ontologies using question-answer datasets. In Proc. CoDS-COMAD \u201918, pages 329\u2013 332. Swarnadeep Saha, Harinder Pal, et al. 2017. Bootstrap- ping for numerical open ie. In Proc.",
  "In Proc. CoDS-COMAD \u201918, pages 329\u2013 332. Swarnadeep Saha, Harinder Pal, et al. 2017. Bootstrap- ping for numerical open ie. In Proc. ACL \u201917, pages 317\u2013323. Denis Savenkov, Wei-Lwun Lu, Jeff Dalton, and Eu- gene Agichtein. 2015. Relation extraction from community generated question-answer pairs. In Proc. NAACL-HLT \u201915, pages 96\u2013102. Gabriel Stanovsky and Ido Dagan. 2016. Creating a large benchmark for open information extraction. In Proc. EMNLP \u201916. Gabriel Stanovsky, Julian Michael, Luke Zettlemoyer, and Ido Dagan. 2018. Supervised open information extraction. In Proc. ACL \u201918, pages 885\u2013895. Antonio Toral and V\u00b4\u0131ctor M. S\u00b4anchez-Cartagena. 2017. A multifaceted evaluation of neural versus phrase- based machine translation for 9 language directions. In Proc.",
  "In Proc. ACL \u201918, pages 885\u2013895. Antonio Toral and V\u00b4\u0131ctor M. S\u00b4anchez-Cartagena. 2017. A multifaceted evaluation of neural versus phrase- based machine translation for 9 language directions. In Proc. EACL \u201917, pages 1063\u20131073. Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In Proc. NIPS \u201915, pages 2692\u20132700. Mengting Wan and Julian McAuley. 2016. Modeling ambiguity, subjectivity, and diverging viewpoints in opinion question answering systems. In Proc. ICDM \u201916, pages 489\u2013498.",
  "Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. 2017. Knowledge graph embedding: A survey of approaches and applications. IEEE Transactions on Knowledge and Data Engineering, 29(12):2724\u2013 2743. Zeqiu Wu, Xiang Ren, Frank F. Xu, Ji Li, and Jiawei Han. 2018. Indirect supervision for relation extrac- tion using question-answer pairs. In Proc. WSDM \u201918, pages 646\u2013654. Chunyang Xiao, Marc Dymetman, and Claire Gardent. 2016. Sequence-based structured prediction for se- mantic parsing. In Proc. ACL \u201916, pages 1341\u2013 1350. Caiming Xiong, Victor Zhong, and Richard Socher. 2017. Dynamic coattention networks for question answering. In Proc. ICLR \u201917. Alexander Yates, Michael Cafarella, Michele Banko, Oren Etzioni, Matthew Broadhead, and Stephen Soderland. 2007. TextRunner: Open information extraction on the web. In Proc.",
  "In Proc. ICLR \u201917. Alexander Yates, Michael Cafarella, Michele Banko, Oren Etzioni, Matthew Broadhead, and Stephen Soderland. 2007. TextRunner: Open information extraction on the web. In Proc. NAACL-HLT \u201907 (Demonstrations), pages 25\u201326. Pengcheng Yin and Graham Neubig. 2017. A syntactic neural model for general-purpose code generation. In Proc. ACL \u201917, pages 440\u2013450. Biao Zhang, Deyi Xiong, and Jinsong Su. 2016. Cseq2seq: Cyclic sequence-to-sequence learning. arXiv preprint arXiv:1607.08725. Yaoyuan Zhang, Zhenxu Ye, Yansong Feng, Dongyan Zhao, and Rui Yan. 2017. A constrained sequence- to-sequence neural model for sentence simpli\ufb01ca- tion. arXiv preprint arXiv:1704.02312. Barret Zoph and Kevin Knight. 2016. Multi-source neural translation.",
  "2017. A constrained sequence- to-sequence neural model for sentence simpli\ufb01ca- tion. arXiv preprint arXiv:1704.02312. Barret Zoph and Kevin Knight. 2016. Multi-source neural translation. In Proc. NAACL-HLT \u201916, pages 30\u201334.",
  "A Supplementary Material This supplementary material contains details of the analysis settings described in Section 4 and ad- ditional results not reported in the main paper. A.1 Word Embedding Analysis We investigated the embedding layers of the ques- tion encoder and the answer encoder of the NEU- RON model trained on the ConciergeQA dataset. For robust analysis, we ignored non-English words and any words that contained numerical digits (e.g., #18D, $10). We used PYENCHANT2 for \ufb01ltering English words. For the remaining words, we \ufb01nd their embedding vectors from the two en- coders, concatenate them to create a single ma- trix. This ensures that same embedding vectors are mapped to the same point in the visualization space. We used t-SNE3 to map embedding vectors into 2D space for visualization. A.2 Crowdsourced Evaluation Figure 5 shows the instructions and examples of the crowdsouced task. In the crowdsourcing task, crowdsourced workers were asked to judge after reading an extracted tuple with the original QA pair.",
  "A.2 Crowdsourced Evaluation Figure 5 shows the instructions and examples of the crowdsouced task. In the crowdsourcing task, crowdsourced workers were asked to judge after reading an extracted tuple with the original QA pair. Since it is dif\ufb01cult to de\ufb01ne the usefulness of the tuples without assuming a KB, we used rel- evance and concreteness as criteria to grade ex- tracted tuples. Speci\ufb01cally, each worker was asked to choose one option from the three options: Not relevant or unclear (0), Relevant (1), Relevant and concrete (2). We set $0.05 as payment for each annotation. We carefully created 51 test questions which were used to \ufb01lter out untrusted judgments and work- ers. The platform increases the number of anno- tators so each tuple should always have 5 trusted annotators. The 5 annotations for each tuple were aggregated into a single label with a con\ufb01dence value that takes into account the accuracy rates of the annotators based on the test questions.",
  "The platform increases the number of anno- tators so each tuple should always have 5 trusted annotators. The 5 annotations for each tuple were aggregated into a single label with a con\ufb01dence value that takes into account the accuracy rates of the annotators based on the test questions. 2 v2.0.0 https://github.com/rfk/pyenchant 3 TSNE v0.20.0 https://scikit-learn.org/stable/ with default con\ufb01guration",
  "Figure 5: Screenshot of the instructions and examples of the crowdsourced task."
]