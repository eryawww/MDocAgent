[
  "Hierarchical Multi-Task Natural Language Understanding for Cross-domain Conversational AI: HERMIT NLU Andrea Vanzo Interaction Lab Heriot-Watt University a.vanzo@hw.ac.uk Emanuele Bastianelli Interaction Lab Heriot-Watt University e.bastianelli@hw.ac.uk Oliver Lemon Interaction Lab Heriot-Watt University o.lemon@hw.ac.uk Abstract We present a new neural architecture for wide- coverage Natural Language Understanding in Spoken Dialogue Systems. We develop a hi- erarchical multi-task architecture, which de- livers a multi-layer representation of sentence meaning (i.e., Dialogue Acts and Frame-like structures). The architecture is a hierarchy of self-attention mechanisms and BiLSTM en- coders followed by CRF tagging layers. We describe a variety of experiments, showing that our approach obtains promising results on a dataset annotated with Dialogue Acts and Frame Semantics. Moreover, we demon- strate its applicability to a different, publicly available NLU dataset annotated with domain- speci\ufb01c intents and corresponding semantic roles, providing overall performance higher than state-of-the-art tools such as RASA, Di- alog\ufb02ow, LUIS, and Watson.",
  "For example, we show an average 4.45% improvement in en- tity tagging F-score over Rasa, Dialog\ufb02ow and LUIS. 1 Introduction Research in Conversational AI (also known as Spoken Dialogue Systems) has applications rang- ing from home devices to robotics, and has a growing presence in industry. A key problem in real-world Dialogue Systems is Natural Language Understanding (NLU) \u2013 the process of extracting structured representations of meaning from user utterances. In fact, the effective extraction of se- mantics is an essential feature, being the entry point of any Natural Language interaction system. Apart from challenges given by the inherent com- plexity and ambiguity of human language, other challenges arise whenever the NLU has to oper- ate over multiple domains. In fact, interaction pat- terns, domain, and language vary depending on the device the user is interacting with. For example, chit-chatting and instruction-giving for executing an action are different processes in terms of lan- guage, domain, syntax and interaction schemes in- volved. And what if the user combines two inter- action domains: \u201cplay some music, but \ufb01rst what\u2019s the weather tomorrow\u201d?",
  "And what if the user combines two inter- action domains: \u201cplay some music, but \ufb01rst what\u2019s the weather tomorrow\u201d? In this work, we present HERMIT, a HiERar- chical MultI-Task Natural Language Understand- ing architecture1, designed for effective seman- tic parsing of domain-independent user utterances, extracting meaning representations in terms of high-level intents and frame-like semantic struc- tures. With respect to previous approaches to NLU for SDS, HERMIT stands out for being a cross-domain, multi-task architecture, capable of recognising multiple intents/frames in an utter- ance. HERMIT also shows better performance with respect to current state-of-the-art commercial systems. Such a novel combination of require- ments is discussed below. Cross-domain NLU A cross-domain dialogue agent must be able to handle heterogeneous types of conversation, such as chit-chatting, giving di- rections, entertaining, and triggering domain/task actions. A domain-independent and rich meaning representation is thus required to properly capture the intent of the user. Meaning is modelled here through three layers of knowledge: dialogue acts, frames, and frame arguments.",
  "A domain-independent and rich meaning representation is thus required to properly capture the intent of the user. Meaning is modelled here through three layers of knowledge: dialogue acts, frames, and frame arguments. Frames and argu- ments can be in turn mapped to domain-dependent intents and slots, or to Frame Semantics\u2019 (Fill- more, 1976) structures (i.e. semantic frames and frame elements, respectively), which allow han- dling of heterogeneous domains and language. Multi-task NLU Deriving such a multi-layered meaning representation can be approached through a multi-task learning approach. Multi- task learning has found success in several NLP 1https://gitlab.com/hwu-ilab/ hermit-nlu arXiv:1910.00912v1  [cs.CL]  2 Oct 2019",
  "problems (Hashimoto et al., 2017; Strubell et al., 2018), especially with the recent rise of Deep Learning. Thanks to the possibility of building complex networks, handling more tasks at once has been proven to be a successful solution, provided that some degree of dependence holds between the tasks. Moreover, multi-task learning allows the use of different datasets to train sub- parts of the network (Sanh et al., 2018). Following the same trend, HERMIT is a hierarchical multi- task neural architecture which is able to deal with the three tasks of tagging dialogue acts, frame-like structures, and their arguments in parallel. The network, based on self-attention mechanisms, seq2seq bi-directional Long-Short Term Memory (BiLSTM) encoders, and CRF tagging layers, is hierarchical in the sense that information output from earlier layers \ufb02ows through the network, feeding following layers to solve downstream dependent tasks. Multi-dialogue act and -intent NLU Another degree of complexity in NLU is represented by the granularity of knowledge that can be extracted from an utterance.",
  "Multi-dialogue act and -intent NLU Another degree of complexity in NLU is represented by the granularity of knowledge that can be extracted from an utterance. Utterance semantics is often rich and expressive: approximating meaning to a single user intent is often not enough to convey the required information. As opposed to the traditional single-dialogue act and single-intent view in previous work (Guo et al., 2014; Liu and Lane, 2016; Hakkani-Tur et al., 2016), HERMIT operates on a meaning representation that is multi-dialogue act and multi-intent. In fact, it is possible to model an utterance\u2019s meaning through multiple dialogue acts and intents at the same time. For example, the user would be able both to request tomorrow\u2019s weather and listen to his/her favourite music with just a single utterance. A further requirement is that for practical applica- tion the system should be competitive with state- of-the-art: we evaluate HERMIT\u2019s effectiveness by running several empirical investigations.",
  "A further requirement is that for practical applica- tion the system should be competitive with state- of-the-art: we evaluate HERMIT\u2019s effectiveness by running several empirical investigations. We perform a robust test on a publicly available NLU- Benchmark (NLU-BM) (Liu et al., 2019) con- taining 25K cross-domain utterances with a con- versational agent. The results obtained show a performance higher than well-known off-the-shelf tools (i.e., Rasa, DialogueFlow, LUIS, and Wat- son). The contribution of the different network components is then highlighted through an abla- tion study. We also test HERMIT on the smaller Robotics-Oriented MUltitask Language Under- Standing (ROMULUS) corpus, annotated with Di- alogue Acts and Frame Semantics. HERMIT pro- duces promising results for the application in a real scenario.",
  "We also test HERMIT on the smaller Robotics-Oriented MUltitask Language Under- Standing (ROMULUS) corpus, annotated with Di- alogue Acts and Frame Semantics. HERMIT pro- duces promising results for the application in a real scenario. 2 Related Work Much research on Natural (or Spoken, depend- ing on the input) Language Understanding has been carried out in the area of Spoken Dia- logue Systems (Chen et al., 2017), where the ad- vent of statistical learning has led to the applica- tion of many data-driven approaches (Lemon and Pietquin, 2012). In recent years, the rise of deep learning models has further improved the state- of-the-art. Recurrent Neural Networks (RNNs) have proven to be particularly successful, espe- cially uni- and bi-directional LSTMs and Gated Recurrent Units (GRUs). The use of such deep architectures has also fostered the development of joint classi\ufb01cation models of intents and slots.",
  "The use of such deep architectures has also fostered the development of joint classi\ufb01cation models of intents and slots. Bi- directional GRUs are applied in (Zhang and Wang, 2016), where the hidden state of each time step is used for slot tagging in a seq2seq fashion, while the \ufb01nal state of the GRU is used for intent classi\ufb01- cation. The application of attention mechanisms in a BiLSTM architecture is investigated in (Liu and Lane, 2016), while the work of (Chen et al., 2016) explores the use of memory networks (Sukhbaatar et al., 2015) to exploit encoding of historical user utterances to improve the slot-\ufb01lling task. Seq2seq with self-attention is applied in (Li et al., 2018), where the classi\ufb01ed intent is also used to guide a special gated unit that contributes to the slot clas- si\ufb01cation of each token. One of the \ufb01rst attempts to jointly detect do- mains in addition to intent-slot tagging is the work of (Guo et al., 2014).",
  "One of the \ufb01rst attempts to jointly detect do- mains in addition to intent-slot tagging is the work of (Guo et al., 2014). An utterance syntax is en- coded through a Recursive NN, and it is used to predict the joined domain-intent classes. Syntac- tic features extracted from the same network are used in the per-word slot classi\ufb01er. The work of (Hakkani-Tur et al., 2016) applies the same idea of (Zhang and Wang, 2016), this time us- ing a context-augmented BiLSTM, and perform- ing domain-intent classi\ufb01cation as a single joint task. As in (Chen et al., 2016), the history of user utterances is also considered in (Bapna et al., 2017), in combination with a dialogue context en- coder. A two-layer hierarchical structure made of a combination of BiLSTM and BiGRU is used",
  "for joint classi\ufb01cation of domains and intents, to- gether with slot tagging. (Rastogi et al., 2018) apply multi-task learning to the dialogue domain. Dialogue state tracking, dialogue act and intent classi\ufb01cation, and slot tagging are jointly learned. Dialogue states and user utterances are encoded to provide hidden representations, which jointly af- fect all the other tasks. Many previous systems are trained and com- pared over the ATIS (Airline Travel Information Systems) dataset (Price, 1990), which covers only the \ufb02ight-booking domain. Some of them also use bigger, not publicly available datasets, which appear to be similar to the NLU-BM in terms of number of intents and slots, but they cover no more than three or four domains. Our work stands out for its more challenging NLU setting, since we are dealing with a higher number of do- mains/scenarios (18), intents (64) and slots (54) in the NLU-BM dataset, and dialogue acts (11), frames (58) and frame elements (84) in the RO- MULUS dataset.",
  "Moreover, we propose a multi- task hierarchical architecture, where each layer is trained to solve one of the three tasks. Each of these is tackled with a seq2seq classi\ufb01cation using a CRF output layer, as in (Sanh et al., 2018). The NLU problem has been studied also on the Interactive Robotics front, mostly to support basic dialogue systems, with few dialogue states and tailored for speci\ufb01c tasks, such as semantic mapping (Kruijff et al., 2007), navigation (Kollar et al., 2010; Bothe et al., 2018), or grounded lan- guage learning (Chai et al., 2016). However, the designed approaches, either based on formal lan- guages or data-driven, have never been shown to scale to real world scenarios. The work of (Hatori et al., 2018) makes a step forward in this direction. Their model still deals with the single \u2018pick and place\u2019 domain, covering no more than two intents, but it is trained on several thousands of examples, making it able to manage more unstructured lan- guage.",
  "Their model still deals with the single \u2018pick and place\u2019 domain, covering no more than two intents, but it is trained on several thousands of examples, making it able to manage more unstructured lan- guage. An attempt to manage a higher number of intents, as well as more variable language, is rep- resented by the work of (Bastianelli et al., 2016) where the sole Frame Semantics is applied to rep- resent user intents, with no Dialogue Acts. 3 Jointly parsing dialogue acts and frame-like structures The identi\ufb01cation of Dialogue Acts (henceforth DAs) is required to drive the dialogue manager to the next dialogue state. General frame struc- tures (FRs) provide a reference framework to cap- ture user intents, in terms of required or de- sired actions that a conversational agent has to perform. Depending on the level of abstraction required by an application, these can be inter- preted as more domain-dependent paradigms like intent, or to shallower representations, such as se- mantic frames, as conceived in FrameNet (Baker et al., 1998).",
  "Depending on the level of abstraction required by an application, these can be inter- preted as more domain-dependent paradigms like intent, or to shallower representations, such as se- mantic frames, as conceived in FrameNet (Baker et al., 1998). From this perspective, semantic frames represent a versatile abstraction that can be mapped over an agent\u2019s capabilities, allowing also the system to be easily extended with new functionalities without requiring the de\ufb01nition of new ad-hoc structures. Similarly, frame argu- ments (ARs) act as slots in a traditional intent-slots scheme, or to frame elements for semantic frames. In our work, the whole process of extracting a complete semantic interpretation as required by the system is tackled with a multi-task learning ap- proach across DAs, FRs, and ARs.",
  "In our work, the whole process of extracting a complete semantic interpretation as required by the system is tackled with a multi-task learning ap- proach across DAs, FRs, and ARs. Each of these tasks is modelled as a seq2seq problem, where a task-speci\ufb01c label is assigned to each token of the sentence according to the IOB2 notation (Sang and Veenstra, 1999), with \u201cB-\u201d marking the Begin- ning of the chunk, \u201cI-\u201d the tokens Inside the chunk while \u201cO-\u201d is assigned to any token that does not belong to any chunk. Task labels are drawn from the set of classes de\ufb01ned for DAs, FRs, and ARs. Figure 1 shows an example of the tagging layers over the sentence Where can I \ufb01nd Starbucks?, where Frame Semantics has been selected as un- derlying reference theory. 3.1 Architecture description The central motivation behind the proposed archi- tecture is that there is a dependence among the three tasks of identifying DAs, FRs, and ARs.",
  "3.1 Architecture description The central motivation behind the proposed archi- tecture is that there is a dependence among the three tasks of identifying DAs, FRs, and ARs. The relationship between tagging frame and arguments appears more evident, as also developed in theo- ries like Frame Semantics \u2013 although it is de\ufb01ned independently by each theory. However, some de- gree of dependence also holds between the DAs and FRs. For example, the FrameNet semantic frame Desiring, expressing a desire of the user for an event to occur, is more likely to be used in the context of an INFORM DA, which indicates the state of notifying the agent with an information, other than in an INSTRUCTION. This is clearly visible in interactions like \u201cI\u2019d like a cup of hot chocolate\u201d or \u201cI\u2019d like to \ufb01nd a shoe shop\u201d, where",
  "Where can I \ufb01nd Starbucks ? DAs B-REQ INFO I-REQ INFO I-REQ INFO I-REQ INFO I-REQ INFO O FRs B-Locating I-Locating I-Locating I-Locating I-Locating O ARs O O B-COGNIZER B-LEXICAL UNIT B-ENTITY O Figure 1: Dialogue Acts (DAs), Frames (FRs \u2013 here semantic frames) and Arguments (ARs \u2013 here frame elements) IOB2 tagging for the sentence Where can I \ufb01nd Starbucks? the user is actually notifying the agent about a de- sire of hers/his. In order to re\ufb02ect such inter-task dependence, the classi\ufb01cation process is tackled here through a hierarchical multi-task learning approach. We de- signed a multi-layer neural network, whose archi- tecture is shown in Figure 2, where each layer is trained to solve one of the three tasks, namely la- belling dialogue acts (DA layer), semantic frames (FR layer), and frame elements (AR layer). The layers are arranged in a hierarchical structure that allows the information produced by earlier layers to be fed to downstream tasks.",
  "The layers are arranged in a hierarchical structure that allows the information produced by earlier layers to be fed to downstream tasks. The network is mainly composed of three BiL- STM (Schuster and Paliwal, 1997) encoding lay- ers. A sequence of input words is initially con- verted into an embedded representation through an ELMo embeddings layer (Peters et al., 2018), and is fed to the DA layer. The embedded representa- tion is also passed over through shortcut connec- tions (Hashimoto et al., 2017), and concatenated with both the outputs of the DA and FR lay- ers. Self-attention layers (Zheng et al., 2018) are placed after the DA and FR BiLSTM encoders.",
  "Self-attention layers (Zheng et al., 2018) are placed after the DA and FR BiLSTM encoders. Where wt is the input word at time step t of the sentence w = (w1, ..., wT ), the architecture can be formalised by: et = ELMo(wt), sDA t = BiLSTM(et) aDA t = SelfAtt(sDA t , sDA), sFR t = BiLSTM(et \u2295aDA t ), aFR t = SelfAtt(sFR t , sFR), sAR t = BiLSTM(et \u2295aFR t ) where \u2295represents the vector concatenation oper- ator, et is the embedding of the word at time t, and sL = (sL 1 , ..., sL T ) is the embedded sequence output of each L layer, with L = {DA, FR, AR}.",
  "Given an input sentence, the \ufb01nal sequence of labels yL for each task is computed through a CRF tagging layer, which operates on the output of the DA and FR self-attention, and of the AR BiLSTM em- O O B-COGNIZER Where can I + + B-REQ_INFO I-REQ_INFO I-REQ_INFO \u2026 B-Locating I-Locating I-Locating \u2026 + + + + + + \u2026 \u2026 Embeddings AR BiLSTM Encoder AR CRF DA CRF FR CRF FR BiLSTM Encoder DA BiLSTM Encoder Self-attention Self-attention Figure 2: HERMIT Network topology bedding, so that: yDA = CRF DA(aDA), yFR = CRF FR(aFR) yAR = CRF AR(sAR), where aDA, aFR are attended embedded se- quences. Due to shortcut connections, layers in the upper levels of the architecture can rely both on direct word embeddings as well as the hidden rep- resentation aL t computed by a previous layer.",
  "Due to shortcut connections, layers in the upper levels of the architecture can rely both on direct word embeddings as well as the hidden rep- resentation aL t computed by a previous layer. Op- erationally, the latter carries task speci\ufb01c informa- tion which, combined with the input embeddings, helps in stabilising the classi\ufb01cation of each CRF layer, as shown by our experiments. The network is trained by minimising the sum of the individual negative log-likelihoods of the three CRF layers, while at test time the most likely sequence is ob- tained through the Viterbi decoding over the out- put scores of the CRF layer. 4 Experimental Evaluation In order to assess the effectiveness of the proposed architecture and compare against existing off-the- shelf tools, we run several empirical evaluations. 4.1 Datasets We tested the system on two datasets, different in size and complexity of the addressed language.",
  "NLU-Benchmark dataset The \ufb01rst (publicly available) dataset, NLU-Benchmark (NLU-BM), contains 25, 716 utterances annotated with tar- geted Scenario, Action, and involved Entities. For example, \u201cschedule a call with Lisa on Monday morning\u201d is labelled to contain a calendar sce- nario, where the set event action is instantiated through the entities [event name: a call with Lisa] and [date: Monday morning]. The Intent is then obtained by concatenating scenario and ac- tion labels (e.g., calendar set event).",
  "The Intent is then obtained by concatenating scenario and ac- tion labels (e.g., calendar set event). This dataset consists of multiple home assistant task do- mains (e.g., scheduling, playing music), chit-chat, and commands to a robot (Liu et al., 2019).2 NLU-BM NLU-BM (reduced) Sentences 25715 11020 Sentences length 7.06 6.84 Scenario labels set 18 18 Action labels set 54 51 Intent labels set 68 64 Entity labels set 56 54 Number of intent 25715 11020 Number of entities 20597 9130 Intents/sentence 1 1 Entities/sentence 0.8 0.83 Table 1: Statistics of the NLU-Benchmark dataset (Liu et al., 2019). ROMULUS dataset The second dataset, RO- MULUS, is composed of 1, 431 sentences, for each of which dialogue acts, semantic frames, and corresponding frame elements are provided.",
  "ROMULUS dataset The second dataset, RO- MULUS, is composed of 1, 431 sentences, for each of which dialogue acts, semantic frames, and corresponding frame elements are provided. This dataset is being developed for modelling user ut- terances to open-domain conversational systems for robotic platforms that are expected to han- dle different interaction situations/patterns \u2013 e.g., chit-chat, command interpretation. The corpus is composed of different subsections, addressing het- erogeneous linguistic phenomena, ranging from imperative instructions (e.g., \u201center the bedroom slowly, turn left and turn the lights off \u201d) to com- plex requests for information (e.g., \u201cgood morn- ing I want to buy a new mobile phone is there any shop nearby?\u201d) or open-domain chit-chat (e.g., \u201cnope thanks let\u2019s talk about cinema\u201d). A consid- erable number of utterances in the dataset is col- lected through Human-Human Interaction studies in robotic domain (\u224870%), though a small portion has been synthetically generated for balancing the frame distribution. 2Available at https://github.com/xliuhw/ NLU-Evaluation-Data.",
  "2Available at https://github.com/xliuhw/ NLU-Evaluation-Data. ROMULUS dataset Sentences 1431 Sentences length 7.24 Dialogue act labels set 11 Frame labels set 58 Frame element labels set 84 Number of dialogue acts 1906 Number of frames 2013 Number of frame elements 5059 Dialogue act/sentence 1.33 Frames/sentence 1.41 Frame elements/sentence 3.54 Table 2: Statistics of the ROMULUS dataset. Note that while the NLU-BM is designed to have at most one intent per utterance, sentences are here tagged following the IOB2 sequence la- belling scheme (see example of Figure 1), so that multiple dialogue acts, frames, and frame elements can be de\ufb01ned at the same time for the same utterance. For example, three dia- logue acts are identi\ufb01ed within the sentence [good morning]OPENING [I want to buy a new mobile phone]INFORM [is there any shop nearby?]REQ INFO.",
  "For example, three dia- logue acts are identi\ufb01ed within the sentence [good morning]OPENING [I want to buy a new mobile phone]INFORM [is there any shop nearby?]REQ INFO. As a result, though smaller, the ROMULUS dataset provides a richer representation of the sen- tence\u2019s semantics, making the tasks more complex and challenging. These observations are high- lighted by the statistics in Table 2, that show an av- erage number of dialogue acts, frames and frame elements always greater than 1 (i.e., 1.33, 1.41 and 3.54, respectively). 4.2 Experimental setup All the models are implemented with Keras (Chol- let et al., 2015) and Tensor\ufb02ow (Abadi et al., 2015) as backend, and run on a Titan Xp. Experiments are performed in a 10-fold setting, using one fold for tuning and one for testing.",
  "Experiments are performed in a 10-fold setting, using one fold for tuning and one for testing. However, since HERMIT is designed to operate on dialogue acts, semantic frames and frame elements, the best hy- perparameters are obtained over the ROMULUS dataset via a grid search using early stopping, and are applied also to the NLU-BM models.3 This guarantees fairness towards other systems, that do not perform any \ufb01ne-tuning on the training data. We make use of pre-trained 1024-dim ELMo em- beddings (Peters et al., 2018) as word vector rep- resentations without re-training the weights. 3Notice that in the NLU-BM experiments only the num- ber of epochs is tuned, using 10% of the training data.",
  "4.3 Experiments on the NLU-Benchmark This section shows the results obtained on the NLU-Benchmark (NLU-BM) dataset provided by (Liu et al., 2019), by comparing HERMIT to off-the-shelf NLU services, namely: Rasa4, Di- alog\ufb02ow5, LUIS6 and Watson7. In order to ap- ply HERMIT to NLU-BM annotations, these have been aligned so that Scenarios are treated as DAs, Actions as FRs and Entities as ARs. To make our model comparable against other approaches, we reproduced the same folds as in (Liu et al., 2019), where a resized version of the original dataset is used. Table 1 shows some statistics of the NLU-BM and its reduced version. Moreover, micro-averaged Precision, Recall and F1 are computed following the original paper to assure consistency. TP, FP and FN of intent labels are obtained as in any other multi-class task. An entity is instead counted as TP if there is an over- lap between the predicted and the gold span, and their labels match. Experimental results are reported in Table 3.",
  "TP, FP and FN of intent labels are obtained as in any other multi-class task. An entity is instead counted as TP if there is an over- lap between the predicted and the gold span, and their labels match. Experimental results are reported in Table 3. The statistical signi\ufb01cance is evaluated through the Wilcoxon signed-rank test. When looking at the intent F1, HERMIT performs signi\ufb01cantly bet- ter than Rasa [Z = \u22122.701, p = .007] and LUIS [Z = \u22122.807, p = .005]. On the contrary, the im- provements w.r.t. Dialog\ufb02ow [Z = \u22121.173, p = .241] do not seem to be signi\ufb01cant. This is prob- ably due to the high variance obtained by Di- alog\ufb02ow across the 10 folds. Watson is by a sig- ni\ufb01cant margin the most accurate system in recog- nising intents [Z = \u22122.191, p = .028], especially due to its Precision score.",
  "Watson is by a sig- ni\ufb01cant margin the most accurate system in recog- nising intents [Z = \u22122.191, p = .028], especially due to its Precision score. The hierarchical multi-task architecture of HERMIT seems to contribute strongly to entity tagging accuracy. In fact, in this task it performs signi\ufb01cantly better than Rasa [Z = \u22122.803, p = .005], Dialog\ufb02ow [Z = \u22122.803, p = .005], LUIS [Z = \u22122.803, p = .005] and Watson [Z = \u22122.805, p = .005], with improvements from 7.08 to 35.92 of F1.9 4https://rasa.com/ 5https://dialogflow.com/ 6https://www.luis.ai/ 7https://www.ibm.com/watson 9Results for Watson are shown for the non-contextual training. Due to Watson limitations, i.e. 2000 training ex- amples for contextual training, we could not run the whole test in such con\ufb01guration.",
  "Due to Watson limitations, i.e. 2000 training ex- amples for contextual training, we could not run the whole test in such con\ufb01guration. For fairness, we report results made on 8 random samplings of 2000/1000 train/test exam- ples a each (F1): Intent=72.64 \u00b1 7.46, Slots=77.01 \u00b1 10.65, Combined=74.85 \u00b1 7.54 Following (Liu et al., 2019), we then evaluated a metric that combines intent and entities, computed by simply summing up the two confusion matrices (Table 4). Results highlight the contribution of the entity tagging task, where HERMIT outperforms the other approaches. Paired-samples t-tests were conducted to compare the HERMIT combined F1 against the other systems.",
  "Results highlight the contribution of the entity tagging task, where HERMIT outperforms the other approaches. Paired-samples t-tests were conducted to compare the HERMIT combined F1 against the other systems. The statistical analysis shows a signi\ufb01cant improvement over Rasa [Z = \u22122.803, p = .005], Dialog\ufb02ow [Z = \u22122.803, p = .005], LUIS [Z = \u22122.803, p = .005] and Watson [Z = \u22122.803, p = .005]. 4.3.1 Ablation study In order to assess the contributions of the HER- MIT\u2019s components, we performed an ablation study. The results are obtained on the NLU-BM, following the same setup as in Section 4.3. Results are shown in Table 5. The \ufb01rst row refers to the complete architecture, while \u2013SA shows the results of HERMIT without the self- attention mechanism. Then, from this latter we further remove shortcut connections (\u2013 SA/CN) and CRF taggers (\u2013 SA/CRF).",
  "The \ufb01rst row refers to the complete architecture, while \u2013SA shows the results of HERMIT without the self- attention mechanism. Then, from this latter we further remove shortcut connections (\u2013 SA/CN) and CRF taggers (\u2013 SA/CRF). The last row (\u2013 SA/CN/CRF) shows the results of a simple archi- tecture, without self-attention, shortcuts, and CRF. Though not signi\ufb01cant, the contribution of the sev- eral architectural components can be observed. The contribution of self-attention is distributed across all the tasks, with a small inclination to- wards the upstream ones. This means that while the entity tagging task is mostly lexicon indepen- dent, it is easier to identify pivoting keywords for predicting the intent, e.g. the verb \u201cschedule\u201d trig- gering the calendar set event intent. The impact of shortcut connections is more evident on entity tagging. In fact, the effect provided by shortcut connections is that the information \ufb02ow- ing throughout the hierarchical architecture allows higher layers to encode richer representations (i.e., original word embeddings + latent semantics from the previous task).",
  "The impact of shortcut connections is more evident on entity tagging. In fact, the effect provided by shortcut connections is that the information \ufb02ow- ing throughout the hierarchical architecture allows higher layers to encode richer representations (i.e., original word embeddings + latent semantics from the previous task). Conversely, the presence of the CRF tagger affects mainly the lower levels of the hierarchical architecture. This is not probably due to their position in the hierarchy, but to the way the tasks have been designed. In fact, while the span of an entity is expected to cover few tokens, in in- tent recognition (i.e., a combination of Scenario and Action recognition) the span always covers all the tokens of an utterance. CRF therefore pre- serves consistency of IOB2 sequences structure. However, HERMIT seems to be the most stable ar-",
  "Intent Entity P R F1 P R F1 Rasa 86.31\u00b11.07 86.31\u00b11.07 86.31\u00b11.07 85.93\u00b11.05 69.40\u00b11.66 76.78\u00b11.27 Dialog\ufb02ow 86.97\u00b12.02 85.87\u00b12.33 86.42\u00b12.18 78.21\u00b13.35 70.85\u00b14.70 74.30\u00b13.74 LUIS 85.53\u00b11.14 85.51\u00b11.15 85.52\u00b11.15 83.69\u00b11.31 72.46\u00b12.05 77.66\u00b11.45 Watson8 88.41\u00b10.68 88.08\u00b10.74 88.24\u00b10.70 35.39\u00b10.93 78.70\u00b12.01 48.82\u00b11.14 HERMIT 87.41\u00b10.63 87.70\u00b10.64 87.55\u00b10.63 87.65\u00b10.98 82.04\u00b12.",
  "39\u00b10.93 78.70\u00b12.01 48.82\u00b11.14 HERMIT 87.41\u00b10.63 87.70\u00b10.64 87.55\u00b10.63 87.65\u00b10.98 82.04\u00b12.12 84.74\u00b11.18 Table 3: Comparison of HERMIT with the results obtained in (Liu et al., 2019) for Intents and Entity Types.",
  "55\u00b10.63 87.65\u00b10.98 82.04\u00b12.12 84.74\u00b11.18 Table 3: Comparison of HERMIT with the results obtained in (Liu et al., 2019) for Intents and Entity Types. Combined P R F1 Rasa 86.16\u00b10.90 78.66\u00b11.28 82.24\u00b11.08 Dialog\ufb02ow 83.19\u00b12.43 79.07\u00b13.10 81.07\u00b12.64 LUIS 84.76\u00b10.67 79.61\u00b11.25 82.1\u00b10.90 Watson 54.02\u00b10.75 83.83\u00b11.02 65.7\u00b10.75 HERMIT 87.52\u00b10.61 85.03\u00b11.11 86.25\u00b10.66 Table 4: Comparison of HERMIT with the results in (Liu et al., 2019) by combining Intent and Entity.",
  "Intent Entity Combined HERMIT 87.55\u00b10.63 84.74\u00b11.18 86.25\u00b10.66 \u2013 SA 87.03\u00b10.74 84.35\u00b11.15 85.81\u00b10.81 \u2013 SA/CN 87.09\u00b10.78 82.43\u00b11.42 84.97\u00b10.72 \u2013 SA/CRF 83.57\u00b10.75 84.77\u00b11.06 84.09\u00b10.79 \u2013 SA/CN/CRF 83.78\u00b11.10 82.22\u00b11.41 83.10\u00b11.06 Table 5: Ablation study of HERMIT on the NLU-BM. chitecture, both in terms of standard deviation and task performance, with a good balance between in- tent and entity recognition. 4.4 Experiments on the ROMULUS dataset In this section we report the experiments per- formed on the ROMULUS dataset (Table 6).",
  "chitecture, both in terms of standard deviation and task performance, with a good balance between in- tent and entity recognition. 4.4 Experiments on the ROMULUS dataset In this section we report the experiments per- formed on the ROMULUS dataset (Table 6). To- gether with the evaluation metrics used in (Liu et al., 2019), we report the span F1, computed us- ing the CoNLL-2000 shared task evaluation script, and the Exact Match (EM) accuracy of the entire sequence of labels. It is worth noticing that the EM Combined score is computed as the conjunction of the three individual predictions \u2013 e.g., a match is when all the three sequences are correct. Results in terms of EM re\ufb02ect the complexity of the different tasks, motivating their position within the hierarchy. Speci\ufb01cally, dialogue act identi\ufb01- cation is the easiest task (89.31%) with respect to frame (82.60%) and frame element (79.73%), due to the shallow semantics it aims to catch.",
  "Speci\ufb01cally, dialogue act identi\ufb01- cation is the easiest task (89.31%) with respect to frame (82.60%) and frame element (79.73%), due to the shallow semantics it aims to catch. However, when looking at the span F1, its score (89.42%) is lower than the frame element iden- ti\ufb01cation task (92.26%). What happens is that even though the label set is smaller, dialogue act spans are supposed to be longer than frame el- ement ones, sometimes covering the whole sen- tence. Frame elements, instead, are often one or two tokens long, that contribute in increasing span based metrics. Frame identi\ufb01cation is the most complex task for several reasons. First, lots of frame spans are interlaced or even nested; this con- tributes to increasing the network entropy. Sec- ond, while the dialogue act label is highly related to syntactic structures, frame identi\ufb01cation is of- ten subject to the inherent ambiguity of language (e.g., get can evoke both Commerce buy and Ar- riving).",
  "Sec- ond, while the dialogue act label is highly related to syntactic structures, frame identi\ufb01cation is of- ten subject to the inherent ambiguity of language (e.g., get can evoke both Commerce buy and Ar- riving). We also report the metrics in (Liu et al., 2019) for consistency. For dialogue act and frame tasks, scores provide just the extent to which the network is able to detect those labels. In fact, the metrics do not consider any span information, es- sential to solve and evaluate our tasks. However, the frame element scores are comparable to the benchmark, since the task is very similar. Overall, getting back to the combined EM ac- curacy, HERMIT seems to be promising, with the network being able to reproduce all the three gold sequences for almost 70% of the cases. The im- portance of this result provides an idea of the ar- chitecture behaviour over the entire pipeline. 4.5 Discussion The experimental evaluation reported in this sec- tion provides different insights.",
  "The im- portance of this result provides an idea of the ar- chitecture behaviour over the entire pipeline. 4.5 Discussion The experimental evaluation reported in this sec- tion provides different insights. The proposed architecture addresses the problem of NLU in wide-coverage conversational systems, modelling semantics through multiple Dialogue Acts and Frame-like structures in an end-to-end fashion. In addition, its hierarchical structure, which re\ufb02ects the complexity of the single tasks, allows provid- ing rich representations across the whole network. In this respect, we can af\ufb01rm that the architecture successfully tackles the multi-task problem, with results that are promising in terms of usability and applicability of the system in real scenarios.",
  "P R F1 span F1 EM Dialogue act 96.49\u00b10.98 95.95\u00b11.41 96.21\u00b11.13 89.42\u00b13.74 89.31\u00b13.28 Frame 95.26\u00b10.95 94.02\u00b11.20 94.64\u00b11.09 84.40\u00b12.99 82.60\u00b12.68 Frame element 95.62\u00b10.61 93.98\u00b10.76 94.79\u00b10.56 92.26\u00b11.22 79.73\u00b12.03 Combined 93.90\u00b10.89 92.95\u00b10.86 93.42\u00b10.83 \u2013 69.53\u00b12.50 Table 6: HERMIT performance over the ROMULUS dataset. P,R and F1 are evaluated following (Liu et al., 2019) metrics However, a thorough evaluation in the wild must be carried out, to assess to what extent the system is able to handle complex spoken language phenomena, such as repetitions, dis\ufb02uencies, etc.",
  "P,R and F1 are evaluated following (Liu et al., 2019) metrics However, a thorough evaluation in the wild must be carried out, to assess to what extent the system is able to handle complex spoken language phenomena, such as repetitions, dis\ufb02uencies, etc. To this end, a real scenario evaluation may open new research directions, by addressing new tasks to be included in the multi-task architecture. This is supported by the scalable nature of the pro- posed approach. Moreover, following (Sanh et al., 2018), corpora providing different annotations can be exploited within the same multi-task network. We also empirically showed how the same ar- chitectural design could be applied to a dataset addressing similar problems. In fact, a compar- ison with off-the-shelf tools shows the bene\ufb01ts provided by the hierarchical structure, with better overall performance better than any current solu- tion. An ablation study has been performed, as- sessing the contribution provided by the different components of the network. The results show how the shortcut connections help in the more \ufb01ne- grained tasks, successfully encoding richer repre- sentations.",
  "An ablation study has been performed, as- sessing the contribution provided by the different components of the network. The results show how the shortcut connections help in the more \ufb01ne- grained tasks, successfully encoding richer repre- sentations. CRFs help when longer spans are be- ing predicted, more present in the upstream tasks. Finally, the seq2seq design allowed obtaining a multi-label approach, enabling the identi\ufb01cation of multiple spans in the same utterance that might evoke different dialogue acts/frames. This rep- resents a novelty for NLU in conversational sys- tems, as such a problem has always been tackled as a single-intent detection. However, the seq2seq approach carries also some limitations, especially on the Frame Semantics side. In fact, label se- quences are linear structures, not suitable for rep- resenting nested predicates, a tough and common problem in Natural Language. For example, in the sentence \u201cI want to buy a new mobile phone\u201d, the [to buy a new mobile phone] span represents both the DESIRED EVENT frame element of the Desir- ing frame and a Commerce buy frame at the same time.",
  "For example, in the sentence \u201cI want to buy a new mobile phone\u201d, the [to buy a new mobile phone] span represents both the DESIRED EVENT frame element of the Desir- ing frame and a Commerce buy frame at the same time. At the moment of writing, we are working on modeling nested predicates through the appli- cation of bilinear models. 5 Future Work We have started integrating a corpus of 5M sen- tences of real users chit-chatting with our conver- sational agent, though at the time of writing they represent only 16% of the current dataset. As already pointed out in Section 4.5, there are some limitations in the current approach that need to be addressed. First, we have to assess the network\u2019s capability in handling typical phenom- ena of spontaneous spoken language input, such as repetitions and dis\ufb02uencies (Shalyminov et al., 2018). This may open new research directions, by including new tasks to identify/remove any kind of noise from the spoken input. Second, the seq2seq scheme does not deal with nested predicates, a common aspect of Natural Language.",
  "This may open new research directions, by including new tasks to identify/remove any kind of noise from the spoken input. Second, the seq2seq scheme does not deal with nested predicates, a common aspect of Natural Language. To the best of our knowledge, there is no architecture that implements an end-to-end network for FrameNet based semantic parsing. Following previous work (Strubell et al., 2018), one of our future goals is to tackle such problems through hierarchical multi- task architectures that rely on bilinear models. 6 Conclusion In this paper we presented HERMIT NLU, a hierarchical multi-task architecture for semantic parsing sentences for cross-domain spoken dia- logue systems. The problem is addressed using a seq2seq model employing BiLSTM encoders and self-attention mechanisms and followed by CRF tagging layers. We evaluated HERMIT on a 25K sentences NLU-Benchmark and out- perform state-of-the-art NLU tools such as Rasa, Dialog\ufb02ow, LUIS and Watson, even without spe- ci\ufb01c \ufb01ne-tuning of the model.",
  "We evaluated HERMIT on a 25K sentences NLU-Benchmark and out- perform state-of-the-art NLU tools such as Rasa, Dialog\ufb02ow, LUIS and Watson, even without spe- ci\ufb01c \ufb01ne-tuning of the model. Acknowledgement This research was partially supported by the Eu- ropean Union\u2019s Horizon 2020 research and in- novation programme under grant agreement No. 688147 (MuMMER project10). 10http://mummer-project.eu/",
  "References Mart\u00b4\u0131n Abadi et al. 2015. TensorFlow: Large- Scale Machine Learning on Heterogeneous Systems. Software available from tensor\ufb02ow.org. Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet Project. In Proceed- ings of ACL and COLING, Association for Compu- tational Linguistics, pages 86\u201390. Ankur Bapna, Gokhan Tur, Dilek Hakkani-Tur, and Larry Heck. 2017. Sequential dialogue context modeling for spoken language understanding. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 103\u2013114. Associa- tion for Computational Linguistics. Emanuele Bastianelli, Danilo Croce, Andrea Vanzo, Roberto Basili, and Daniele Nardi. 2016. A dis- criminative approach to grounded spoken language understanding in interactive robotics. In Proceed- ings of the 2016 International Joint Conference on Arti\ufb01cial Intelligence (IJCAI), New York, USA.",
  "2016. A dis- criminative approach to grounded spoken language understanding in interactive robotics. In Proceed- ings of the 2016 International Joint Conference on Arti\ufb01cial Intelligence (IJCAI), New York, USA. Chandrakant Bothe, Fernando Garc\u00b4\u0131a, Arturo Cruz- Maya, Amit Kumar Pandey, and Stefan Wermter. 2018. Towards dialogue-based navigation with mul- tivariate adaptation driven by intention and polite- ness for social robots. In ICSR, volume 11357 of Lecture Notes in Computer Science, pages 230\u2013240. Springer. Joyce Y Chai, Rui Fang, Changsong Liu, and Lanbo She. 2016. Collaborative language grounding to- ward situated human-robot dialogue. AI Magazine, 37(4):32\u201345. Hongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang Tang. 2017. A survey on dialogue systems: Re- cent advances and new frontiers. SIGKDD Explor. Newsl., 19(2):25\u201335.",
  "Hongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang Tang. 2017. A survey on dialogue systems: Re- cent advances and new frontiers. SIGKDD Explor. Newsl., 19(2):25\u201335. Yun-Nung Chen, Dilek Hakkani-T\u00a8ur, G\u00a8okhan T\u00a8ur, Jianfeng Gao, and Li Deng. 2016. End-to-end mem- ory networks with knowledge carryover for multi- turn spoken language understanding. In INTER- SPEECH, pages 3245\u20133249. ISCA. Franc\u00b8ois Chollet et al. 2015. Keras. https:// keras.io. Charles J Fillmore. 1976. Frame semantics and the na- ture of language. Annals of the New York Academy of Sciences, 280(1):20\u201332. Daniel Guo, G\u00a8okhan T\u00a8ur, Wen-tau Yih, and Geoffrey Zweig. 2014. Joint semantic utterance classi\ufb01cation and slot \ufb01lling with recursive neural networks.",
  "Daniel Guo, G\u00a8okhan T\u00a8ur, Wen-tau Yih, and Geoffrey Zweig. 2014. Joint semantic utterance classi\ufb01cation and slot \ufb01lling with recursive neural networks. In 2014 IEEE Spoken Language Technology Workshop, SLT 2014, South Lake Tahoe, NV, USA, December 7- 10, 2014, pages 554\u2013559. Dilek Hakkani-Tur, Gokhan Tur, Asli Celikyilmaz, Yun-Nung Chen, Jianfeng Gao, Li Deng, and Ye-Yi Wang. 2016. Multi-Domain Joint Semantic Frame Parsing using Bi-directional RNN-LSTM. In Pro- ceedings of Interspeech. Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsu- ruoka, and Richard Socher. 2017. A Joint Many- Task Model: Growing a Neural Network for Mul- tiple NLP Tasks. In Proceedings of the 2017 Con- ference on Empirical Methods in Natural Language Processing, pages 1923\u20131933.",
  "2017. A Joint Many- Task Model: Growing a Neural Network for Mul- tiple NLP Tasks. In Proceedings of the 2017 Con- ference on Empirical Methods in Natural Language Processing, pages 1923\u20131933. Association for Com- putational Linguistics. Jun Hatori, Yuta Kikuchi, Sosuke Kobayashi, Kuniyuki Takahashi, Yuta Tsuboi, Yuya Unno, Wilson Ko, and Jethro Tan. 2018. Interactively picking real-world objects with unconstrained spoken language instruc- tions. In 2018 IEEE International Conference on Robotics and Automation, ICRA 2018, Brisbane, Australia, May 21-25, 2018, pages 3774\u20133781. Thomas Kollar, Stefanie Tellex, Deb Roy, and Nicholas Roy. 2010. Toward understanding natural language directions. In Proceedings of the 5th ACM/IEEE International Conference on Human-robot Interac- tion, HRI \u201910, pages 259\u2013266, Piscataway, NJ, USA. IEEE Press.",
  "2010. Toward understanding natural language directions. In Proceedings of the 5th ACM/IEEE International Conference on Human-robot Interac- tion, HRI \u201910, pages 259\u2013266, Piscataway, NJ, USA. IEEE Press. Geert-Jan M. Kruijff, H. Zender, P. Jensfelt, and Hen- rik I. Christensen. 2007. Situated dialogue and spa- tial organization: What, where...and why? Interna- tional Journal of Advanced Robotic Systems, 4(2). Oliver Lemon and Olivier Pietquin. 2012. Data-Driven Methods for Adaptive Spoken Dialogue Systems: Computational Learning for Conversational Inter- faces. Springer Publishing Company, Incorporated. Changliang Li, Liang Li, and Ji Qi. 2018. A self- attentive model with gate mechanism for spoken lan- guage understanding. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing, pages 3824\u20133833. Association for Computational Linguistics. Bing Liu and Ian Lane. 2016.",
  "In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing, pages 3824\u20133833. Association for Computational Linguistics. Bing Liu and Ian Lane. 2016. Attention-Based Recur- rent Neural Network Models for Joint Intent Detec- tion and Slot Filling. In Interspeech 2016, 17th An- nual Conference of the International Speech Com- munication Association, San Francisco, CA, USA, September 8-12, 2016, pages 685\u2013689. Xingkun Liu, Arash Eshghi, Pawel Swietojanski, and Verena Rieser. 2019. Benchmarking Natural Lan- guage Understanding Services for building Conver- sational Agents. In Proceedings of the International Workshop on Spoken Dialogue System, page to ap- pear, Siracusa, Sicily, Italy. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized Word Rep- resentations.",
  "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized Word Rep- resentations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227\u2013 2237. Association for Computational Linguistics.",
  "P. J. Price. 1990. Evaluation of spoken language sys- tems: The atis domain. In Proceedings of the Work- shop on Speech and Natural Language, HLT \u201990, pages 91\u201395, Stroudsburg, PA, USA. Association for Computational Linguistics. Abhinav Rastogi, Raghav Gupta, and Dilek Hakkani- Tur. 2018. Multi-task Learning for Joint Language Understanding and Dialogue State Tracking. In Pro- ceedings of the 19th Annual SIGdial Meeting on Dis- course and Dialogue, pages 376\u2013384. Association for Computational Linguistics. Erik F. Tjong Kim Sang and Jorn Veenstra. 1999. Rep- resenting text chunks. In Proceedings of the Ninth Conference on European Chapter of the Association for Computational Linguistics, EACL \u201999, pages 173\u2013179, Stroudsburg, PA, USA. Association for Computational Linguistics. Victor Sanh, Thomas Wolf, and Sebastian Ruder. 2018. A hierarchical multi-task approach for learn- ing embeddings from semantic tasks.",
  "Association for Computational Linguistics. Victor Sanh, Thomas Wolf, and Sebastian Ruder. 2018. A hierarchical multi-task approach for learn- ing embeddings from semantic tasks. arXiv preprint arXiv:1811.06031. M. Schuster and K.K. Paliwal. 1997. Bidirectional Recurrent Neural Networks. IEEE Transactions on Signal Processing, 45(11):2673\u20132681. Igor Shalyminov, Arash Eshghi, and Oliver Lemon. 2018. Multi-task learning for domain-general spo- ken dis\ufb02uency detection in dialogue systems. In Proceedings of SemDIAL 2018 (AixDial). Emma Strubell, Patrick Verga, Daniel Andor, David Weiss, and Andrew McCallum. 2018. Linguistically-Informed Self-Attention for Seman- tic Role Labeling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing, pages 5027\u20135038. Association for Computational Linguistics.",
  "2018. Linguistically-Informed Self-Attention for Seman- tic Role Labeling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing, pages 5027\u20135038. Association for Computational Linguistics. Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. 2015. End-to-end memory net- works. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440\u20132448. Curran Associates, Inc. Xiaodong Zhang and Houfeng Wang. 2016. A Joint Model of Intent Determination and Slot Filling for Spoken Language Understanding. In Proceedings of the Twenty-Fifth International Joint Conference on Arti\ufb01cial Intelligence, IJCAI\u201916. AAAI Press. Guineng Zheng, Subhabrata Mukherjee, Xin Luna Dong, and Feifei Li. 2018.",
  "In Proceedings of the Twenty-Fifth International Joint Conference on Arti\ufb01cial Intelligence, IJCAI\u201916. AAAI Press. Guineng Zheng, Subhabrata Mukherjee, Xin Luna Dong, and Feifei Li. 2018. Opentag: Open attribute value extraction from product pro\ufb01les. In Proceed- ings of the 24th ACM SIGKDD International Con- ference on Knowledge Discovery &#38; Data Min- ing, KDD \u201918, pages 1049\u20131058, New York, NY, USA. ACM."
]