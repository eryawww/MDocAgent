{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Spoken Language Identi\ufb01cation using ConvNets Sarthak1, Shikhar Shukla2, and Govind Mittal3 1 Analytics Quotient, Bangalore, India sarthak.sfc@gmail.com, sarthak.j@aqinsights.com 2 Samsung R&D Institute India-Bangalore, Bangalore, India shikhar.00778@gmail.com, shikhar.0077@samsung.com 3 Birla Institute of Technology & Science, Pilani, Rajasthan, India f2014530@pilani.bits-pilani.ac.in Abstract. Language Identi\ufb01cation (LI) is an important \ufb01rst step in sev- eral speech processing systems. With a growing number of voice-based as- sistants, speech LI has emerged as a widely researched \ufb01eld. To approach the problem of identifying languages, we can either adopt an implicit ap- proach where only the speech for a language is present or an explicit one where text is available with its corresponding transcript. This paper focuses on an implicit approach due to the absence of transcriptive data.",
            "To approach the problem of identifying languages, we can either adopt an implicit ap- proach where only the speech for a language is present or an explicit one where text is available with its corresponding transcript. This paper focuses on an implicit approach due to the absence of transcriptive data. This paper benchmarks existing models and proposes a new attention based model for language identi\ufb01cation which uses log-Mel spectrogram images as input. We also present the e\ufb00ectiveness of raw waveforms as features to neural network models for LI tasks. For training and evalu- ation of models, we classi\ufb01ed six languages (English, French, German, Spanish, Russian and Italian) with an accuracy of 95.4% and four lan- guages (English, French, German, Spanish) with an accuracy of 96.3% obtained from the VoxForge dataset. This approach can further be scaled to incorporate more languages. Keywords: Language Identi\ufb01cation \u00b7 Raw Waveform \u00b7 Convolutional Neural Networks \u00b7 Machine Learning. 1 Introduction Language Identi\ufb01cation (LI) is a problem which involves classifying the language being spoken by a speaker.",
            "This approach can further be scaled to incorporate more languages. Keywords: Language Identi\ufb01cation \u00b7 Raw Waveform \u00b7 Convolutional Neural Networks \u00b7 Machine Learning. 1 Introduction Language Identi\ufb01cation (LI) is a problem which involves classifying the language being spoken by a speaker. LI systems can be used in call centers to route international calls to an operator who is \ufb02uent in that identi\ufb01ed language [12]. In speech-based assistants, LI acts as the \ufb01rst step which chooses the corresponding grammar from a list of available languages for its further semantic analysis [1]. It can also be used in multi-lingual voice-controlled information retrieval systems, for example, Apple Siri and Amazon Alexa. Over the years, studies have utilized many prosodic and acoustic features to construct machine learning models for LI systems [18]. Every language is composed of phonemes, which are distinct unit of sounds in that language, such as b of black and g of green. Several prosodic and acoustic features are based on phonemes, which become the underlying features on whom the performance arXiv:1910.04269v1  [cs.CL]  9 Oct 2019",
            "2 Sarthak et al. of the statistical model depends [20,5]. If two languages have many overlapping phonemes, then identifying them becomes a challenging task for a classi\ufb01er. For example, the word cat in English, kat in Dutch, katze in German have di\ufb00erent consonants but when used in a speech they all would sound quite similar. Due to such drawbacks several studies have switched over to using Deep Neu- ral Networks (DNNs) to harness their novel auto-extraction techniques [1,19]. This work follows an implicit approach for identifying six languages with overlap- ping phonemes on the VoxForge [23] dataset and achieves 95.4% overall accuracy. In previous studies [1,17,19], authors use log-Mel spectrum of a raw audio as inputs to their models. One of our contributions is to enhance the perfor- mance of this approach by utilising recent techniques like Mixup augmentation of inputs and exploring the e\ufb00ectiveness of Attention mechanism in enhancing performance of neural network.",
            "One of our contributions is to enhance the perfor- mance of this approach by utilising recent techniques like Mixup augmentation of inputs and exploring the e\ufb00ectiveness of Attention mechanism in enhancing performance of neural network. As log-Mel spectrum needs to be computed for each raw audio input and processing time for generating log-Mel spectrum in- creases linearly with length of audio, this acts as a bottleneck for these models. Hence, we propose the use of raw audio waveforms as inputs to deep neural net- work which boosts performance by avoiding additional overhead of computing log-Mel spectrum for each audio. Our 1D-ConvNet architecture auto-extracts and classi\ufb01es features from this raw audio input. The structure of the work is as follows. In Section 2 we discuss about the previous related studies in this \ufb01eld. The model architecture for both the raw waveforms and log-Mel spectrogram images is discussed in Section 3 along with the a discussion on hyperparameter space exploration. In Section 4 we present the experimental results. Finally, in Section 5 we discuss the conclusions drawn from the experiment and future work.",
            "The model architecture for both the raw waveforms and log-Mel spectrogram images is discussed in Section 3 along with the a discussion on hyperparameter space exploration. In Section 4 we present the experimental results. Finally, in Section 5 we discuss the conclusions drawn from the experiment and future work. 2 Related Work Extraction of language dependent features like prosody and phonemes was a pop- ular approach to classify spoken languages [29,16,6]. Following their success in speaker veri\ufb01cation systems, i-vectors have also been used as features in various classi\ufb01cation networks. These approaches required signi\ufb01cant domain knowledge [4,16]. Nowadays most of the attempts on spoken language identi\ufb01cation rely on neural networks for meaningful feature extraction and classi\ufb01cation [15,7]. Revay et al. [19] used the ResNet50 [9] architecture for classifying languages by generating the log-Mel spectra of each raw audio. The model uses a cyclic learning rate where learning rate increases and then decreases linearly.",
            "Revay et al. [19] used the ResNet50 [9] architecture for classifying languages by generating the log-Mel spectra of each raw audio. The model uses a cyclic learning rate where learning rate increases and then decreases linearly. Maximum learning rate for a cycle is set by \ufb01nding the optimal learning rate using fastai [11] library. The model classi\ufb01ed six languages \u2013 English, French, Spanish, Russian, Italian and German \u2013 and achieving an accuracy of 89.0%. Gazeau et al. [8] in his research showed how Neural Networks, Support Vector Machine and Hidden Markov Model (HMM) can be used to identify French, English, Spanish and German. Dataset was prepared using voice samples from Youtube News [27]and VoxForge [23] datasets. Hidden Markov models convert speech into a sequence of vectors, was used to capture temporal features in",
            "Spoken Language Identi\ufb01cation using ConvNets 3 speech. HMMs trained on VoxForge [23] dataset performed best in comparison to other models proposed by him on same VoxForge dataset. They reported an accuracy of 70.0%. Bartz et al. [1] proposed two di\ufb00erent hybrid Convolutional Recurrent Neu- ral Networks for language identi\ufb01cation. They proposed a new architecture for extracting spatial features from log-Mel spectra of raw audio using CNNs and then using RNNs for capturing temporal features to identify the language. This model achieved an accuracy of 91.0% on Youtube News Dataset [27]. In their sec- ond architecture they used the Inception-v3 [22] architecture to extract spatial features which were then used as input for bi-directional LSTMs to predict the language accurately. This model achieved an accuracy of 96.0% on four languages which were English, German, French and Spanish. They also trained their CNN model (obtained after removing RNN from CRNN model) and the Inception-v3 on their dataset.",
            "This model achieved an accuracy of 96.0% on four languages which were English, German, French and Spanish. They also trained their CNN model (obtained after removing RNN from CRNN model) and the Inception-v3 on their dataset. However they were not able to achieve better results achieving and reported 90% and 95% accuracies, respectively. Kumar et al. [12] used Mel-frequency cepstral coe\ufb03cients (MFCC), Percep- tual linear prediction coe\ufb03cients (PLP), Bark Frequency Cepstral Coe\ufb03cients (BFCC) and Revised Perceptual Linear Prediction Coe\ufb03cients (RPLP) as fea- tures for language identi\ufb01cation. BFCC and RPLP are hybrid features derived using MFCC and PLP. They used two di\ufb00erent models based on Vector Quanti- zation (VQ) with Dynamic Time Warping (DTW) and Gaussian Mixture Model (GMM) for classi\ufb01cation.",
            "They used two di\ufb00erent models based on Vector Quanti- zation (VQ) with Dynamic Time Warping (DTW) and Gaussian Mixture Model (GMM) for classi\ufb01cation. These classi\ufb01cation models were trained with di\ufb00er- ent features. The authors were able to show that these models worked better with hybrid features (BFCC and RPLP) as compared to conventional features (MFCC and PLP). GMM combined with RPLP features gave the most promis- ing results and achieved an accuracy of 88.8% on ten languages. They designed their own dataset comprising of ten languages being Dutch, English, French, German, Italian, Russian, Spanish, Hindi, Telegu, and Bengali. Montavon [17] generated Mel spectrogram as features for a time-delay neu- ral network (TDNN). This network had two-dimensional convolutional layers for feature extraction. An elaborate analysis of how deep architectures outperform their shallow counterparts is presented in this reseacrch.",
            "Montavon [17] generated Mel spectrogram as features for a time-delay neu- ral network (TDNN). This network had two-dimensional convolutional layers for feature extraction. An elaborate analysis of how deep architectures outperform their shallow counterparts is presented in this reseacrch. The di\ufb03culties in clas- sifying perceptually similar languages like German and English were also put forward in this work. It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table 1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classi\ufb01ed and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",
            "4 Sarthak et al. Table 1: Quantitative Review of Previous Studies along with our Results. Year Model basis Features Languages Acc. Remarks Ref. 2019 1D ConvNet Raw Audio En, Fr, De, Es, Ru, It 93.71 Evaulation of our 1D ConvNet model with mixup for six languages. self 2019 2D ConvNet log-Mel En, Fr, De, Es, Ru, It 95.41 Evaulation of our 2D ConvNet model with mixup for six languages. self 2019 2D ConvNet-Bi- directional GRU-Attention log-Mel En, Fr, De, Es, Ru, It 95.01 Result after tuning the hyperparameters of our cnn-bi-directional GRU-attention model and applying mixup self 2019 2D ConvNet log-Mel En, Fr, De, Es 96.31 Our evaluation of 2D ConvNet model for four languages.",
            "self 2019 ResNet50 log-Mel En, Fr, De, Es, Ru, It 89.01 Uses a pretrained ResNet50 architecture and cyclic learner to identify the language. [19] 2018 SVM-HMM model not de\ufb01ned En, Fr, Es, De 70.01 HMMs were used to encode speech into sequences of vectors which were then fed into a neural network. [8] 2017 Inceptionv3 CRNN log-Mel En, Fr, De, Es 96.02 Used Inception-v3 model followed by bi-directional LSTMs to extract convolutional and temporal features. [1] 2017 CRNN log-Mel En, Fr, De, Es 91.02 A new architecture is used to extract spatial features by using CNNs and temporal features using RNNs. [1] 2010 Gaussian Mixture Models Percep- tual Linear Prediction Dut, En, Fr, De, It, Ru, Es, Ben, Hi and Tel 88.83 Used Gaussian mixture models coupled with RPLP features, which were prepared using MFCC and PLP.",
            "[1] 2010 Gaussian Mixture Models Percep- tual Linear Prediction Dut, En, Fr, De, It, Ru, Es, Ben, Hi and Tel 88.83 Used Gaussian mixture models coupled with RPLP features, which were prepared using MFCC and PLP. [12] 2009 CNN-TDNN log-Mel En, Fr , De 91.21 Used a time delay neural network with SGD was used to identify language using log-Mel images as input. [17] Dataset: 1 - VoxForge [23]; 2 - Youtube News [27], 3 - Private",
            "Spoken Language Identi\ufb01cation using ConvNets 5 3 Proposed Method 3.1 Motivations Several state-of-the-art results on various audio classi\ufb01cation tasks have been obtained by using log-Mel spectrograms of raw audio, as features [25]. Convo- lutional Neural Networks have demonstrated an excellent performance gain in classi\ufb01cation of these features [26,10] against other machine learning techniques. It has been shown that using attention layers with ConvNets further enhanced their performance [13]. This motivated us to develop a CNN-based architecture with attention since this approach hasn\u2019t been applied to the task of language identi\ufb01cation before. Recently, using raw audio waveform as features to neural networks has be- come a popular approach in audio classi\ufb01cation [24,13]. Raw waveforms have several artifacts which are not e\ufb00ectively captured by various conventional fea- ture extraction techniques like Mel Frequency Cepstral Coe\ufb03cients (MFCC), Constant Q Transform (CQT), Fast Fourier Transform (FFT), etc.",
            "Raw waveforms have several artifacts which are not e\ufb00ectively captured by various conventional fea- ture extraction techniques like Mel Frequency Cepstral Coe\ufb03cients (MFCC), Constant Q Transform (CQT), Fast Fourier Transform (FFT), etc. Audio \ufb01les are a sequence of spoken words, hence they have temporal features too.A CNN is better at capturing spatial features only and RNNs are better at capturing temporal features as demonstrated by Bartz et al. [1] using audio \ufb01les. Therefore, we combined both of these to make a CRNN model. We propose three types of models to tackle the problem with di\ufb00erent ap- proaches, discussed as follows. 3.2 Description of Features As an average human\u2019s voice is around 300 Hz and according to Nyquist-Shannon sampling theorem all the useful frequencies (0-300 Hz) are preserved with sam- pling at 8 kHz, therefore, we sampled raw audio \ufb01les from all six languages at 8 kHz The average length of audio \ufb01les in this dataset was about 10.4 seconds and standard deviation was 2.3 seconds.",
            "For our experiments, the audio length was set to 10 seconds. If the audio \ufb01les were shorter than 10 second, then the data was repeated and concatenated. If audio \ufb01les were longer, then the data was truncated. 3.3 Model Description We applied the following design principles to all our models: \u2013 Every convolutional layer is always followed by an appropriate max pooling layer. This helps in containing the explosion of parameters and keeps the model small and nimble. \u2013 Convolutional blocks are de\ufb01ned as an individual block with multiple pairs of one convolutional layer and one max pooling layer. Each convolutional block is preceded or succeded by a convolutional layer.",
            "6 Sarthak et al. \u2013 Batch Normalization and Recti\ufb01ed linear unit activations were applied after each convolutional layer. Batch Normalization helps speed up convergence during training of a neural network. \u2013 Model ends with a dense layer which acts the \ufb01nal output layer. 3.4 Model Details: 1D ConvNet As the sampling rate is 8 kHz and audio length is 10 s, hence the input is raw au- dio to the models with input size of (batch size, 1, 80000). In Table 2, we present a detailed layer-by-layer illustration of the model along with its hyperparameter. Table 2: Architecture of the 1D-ConvNet model Layer Name # \ufb01lters \/ kernel \/ stride output # of parameters Conv1 (128, 3, 3) (128, 26664) 384 (Convolutional Block 1) Conv1D MaxPool1D Conv1D MaxPool1D (128, 3, 1) (3, 3) (128, 3, 1) (3, 3) (128, 26658) (128,",
            "3, 1) (3, 3) (128, 3, 1) (3, 3) (128, 26658) (128, 8880) (128, 8880) (128, 2960) 49152 49,152 Conv1D MaxPool1D (256, 3, 1) (3, 3) (256, 2954) (256, 984) 98,304 (Convolutional Block 2) Conv1D MaxPool1D (256, 3, 1) (3, 3) (256, 978) (256, 326) 196,608 Conv1D MaxPool1D (512, 3, 1) (106, 3) (512, 320) (512, 1) 393,216 Dense Layer (512, 6) (6) 3,072 Hyperparameter Optimization: Tuning hyperparameters is a cumbersome process as the hyperparamter space expands exponentially with the number of parameters, therefore e\ufb03cient exploration is needed for any feasible study.",
            "1) 393,216 Dense Layer (512, 6) (6) 3,072 Hyperparameter Optimization: Tuning hyperparameters is a cumbersome process as the hyperparamter space expands exponentially with the number of parameters, therefore e\ufb03cient exploration is needed for any feasible study. We used the random search algorithm supported by Hyperopt [2] library to randomly search for an optimal set of hyperparameters from a given parameter space. In Fig. 1, various hyperparameters we considered are plotted against the validation accuracy as violin plots. Our observations for each hyperparameter are summarized below: Number of \ufb01lters in \ufb01rst layer: We observe that having 128 \ufb01lters gives better results as compared to other \ufb01lter values of 32 and 64 in the \ufb01rst layer. A higher number of \ufb01lters in the \ufb01rst layer of network is able to preserve most of the characteristics of input. Kernel Size: We varied the receptive \ufb01elds of convolutional layers by choosing the kernel size from among the set of {3, 5, 7, 9}.",
            "Kernel Size: We varied the receptive \ufb01elds of convolutional layers by choosing the kernel size from among the set of {3, 5, 7, 9}. We observe that a kernel size of 9 gives better accuracy at the cost of increased computation time and larger number of parameters. A large kernel size is able to capture longer patterns in its input due to bigger receptive power which results in an improved accuracy.",
            "Spoken Language Identi\ufb01cation using ConvNets 7 32 64 128 # filters in first layer 50 60 70 80 90 Validation Accuracy 3 5 7 9 Kernel 50 60 70 80 90 Validation Accuracy 0.0 0.1 0.2 0.3 0.5 0.6 Dropout 50 60 70 80 90 100 Validation Accuracy 32 64 128 Batch Size 50 60 70 80 90 Validation Accuracy 0 1 2 3 # Hidden layers in Conv Block 1 50 60 70 80 90 Validation Accuracy 0 1 2 # Hidden layers in Conv Block 2 50 60 70 80 90 Validation Accuracy Fig. 1: E\ufb00ect of hyperparameter variation of the hyperparameter on the classi- \ufb01cation accuracy for the case of 1D-ConvNet. Orange colored violin plots show the most favored choice of the hyperparameter and blue shows otherwise. One dot represents one sample.",
            "1: E\ufb00ect of hyperparameter variation of the hyperparameter on the classi- \ufb01cation accuracy for the case of 1D-ConvNet. Orange colored violin plots show the most favored choice of the hyperparameter and blue shows otherwise. One dot represents one sample. Dropout: Dropout randomly turns-o\ufb00(sets to 0) various individual nodes during training of the network. In a deep CNN it is important that nodes do not develop a co-dependency amongst each other during training in order to prevent over\ufb01tting on training data [21]. Dropout rate of 0.1 works well for our model. When using a higher dropout rate the network is not able to capture the patterns in training dataset. Batch Size: We chose batch sizes from amongst the set {32, 64, 128}. There is more noise while calculating error in a smaller batch size as compared to a larger one. This tends to have a regularizing e\ufb00ect during training of the network and hence gives better results. Thus, batch size of 32 works best for the model.",
            "There is more noise while calculating error in a smaller batch size as compared to a larger one. This tends to have a regularizing e\ufb00ect during training of the network and hence gives better results. Thus, batch size of 32 works best for the model. Layers in Convolutional block 1 and 2: We varied the number of layers in both the convolutional blocks. If the number of layers is low, then the network does not have enough depth to capture patterns in the data whereas having large number of layers leads to over\ufb01tting on the data. In our network, two layers in the \ufb01rst block and one layer in the second block give optimal results. 3.5 Model Details: 2D ConvNet with Attention and bi-directional GRU Log-Mel spectrogram is the most commonly used method for converting audio into the image domain. The audio data was again sampled at 8 kHz. The input to",
            "8 Sarthak et al. this model was the log-Mel spectra. We generated log-Mel spectrogram using the LibROSA [14] library. In Table 3, we present a detailed layer-by-layer illustration of the model along with its hyperparameter. We took some speci\ufb01c design choices for this model, which are as follows: \u2013 We added residual connections with each convolutional layer. Residual con- nections in a way makes the model selective of the contributing layers, de- termines the optimal number of layers required for training and solves the problem of vanishing gradients. Residual connections or skip connections skip training of those layers that do not contribute much in the overall outcome of model. Table 3: Architecture of the 2D-ConvNet model Layer Name Output features Number of \ufb01lters \/ stride \/ padding No. of parameters (ConvBlock 1) Conv2D Conv2D AvgPool2D (64, 128, 128) (64, 128, 128) (64, 64, 64) (3, 3) \/ (1, 1) \/ (1, 1) (3,",
            "128, 128) (64, 128, 128) (64, 64, 64) (3, 3) \/ (1, 1) \/ (1, 1) (3, 3) \/ (1, 1) \/ (1, 1) 1,728 36,864 (ConvBlock 2) Conv2D Conv2D AvgPool2D (128, 64, 64) (128, 64, 64) (128, 32, 32) (3, 3) \/ (1, 1) \/ (1, 1) (3, 3) \/ (1, 1) \/ (1, 1) 73,728 147,456 (ConvBlock 3) Conv2D Conv2D AvgPool2D (256, 32, 32) (256, 32, 32) (256, 16, 16) (3, 3) \/ (1, 1) \/ (1, 1) (3, 3) \/ (1, 1) \/ (1, 1) 294,912 589,",
            "32, 32) (256, 16, 16) (3, 3) \/ (1, 1) \/ (1, 1) (3, 3) \/ (1, 1) \/ (1, 1) 294,912 589,824 (ConvBlock 4) Conv2D Conv2D AvgPool2D (512, 16, 16) (512, 16, 16) (512, 8, 8) (3, 3) \/ (1, 1) \/ (1, 1) (3, 3) \/ (1, 1) \/ (1, 1) 1,179,648 235,929 Bi-directional GRU Embedding Layer (8, 1536) (8, 768) 1,769,472 1,179,648 (Sequential Block) Dropout (0.2) Linear Dropout (0.1) Linear (256) (6) 131,072 1,536 \u2013 We added spatial attention [3] networks to help the model in focusing on speci\ufb01c regions or areas in an image.",
            "179,648 (Sequential Block) Dropout (0.2) Linear Dropout (0.1) Linear (256) (6) 131,072 1,536 \u2013 We added spatial attention [3] networks to help the model in focusing on speci\ufb01c regions or areas in an image. Spatial attention aids learning irre- spective of transformations, scaling and rotation done on the input images making the model more robust and helping it to achieve better results. \u2013 We added Channel Attention networks so as to help the model to \ufb01nd inter- dependencies among color channels of log-Mel spectra. It adaptively assigns importance to each color channel in a deep convolutional multi-channel net- work. In our model we apply channel and spatial attention just before feeding",
            "Spoken Language Identi\ufb01cation using ConvNets 9 the input into bi-directional GRU. This helps the model to focus on selected regions and at the same time \ufb01nd patterns among channels to better deter- mine the language. 32 64 128 Filter Size 60 70 80 90 Validation Accuracy 3 7 Kernel size 60 70 80 90 Validation Accuracy 0.0 0.1 0.2 0.3 0.5 0.6 Dropout 60 70 80 90 Validation Accuracy 32 64 128 Batch size 60 70 80 90 Validation Accuracy 128 256 384 512 768 1024 1536 # hidden units in bidirectional GRU 60 70 80 90 Validation Accuracy 64 128 Image Size 60 70 80 90 Validation Accuracy Fig. 2: E\ufb00ect of hyperparameter variation of the six selected hyperparameter on the classi\ufb01cation accuracy for the case of 2D-ConvNet. Orange colored vio- lin plots show the most favored choice of the hyperparameter and blue shows otherwise.",
            "2: E\ufb00ect of hyperparameter variation of the six selected hyperparameter on the classi\ufb01cation accuracy for the case of 2D-ConvNet. Orange colored vio- lin plots show the most favored choice of the hyperparameter and blue shows otherwise. One dot represents one sample. Hyperparameter Optimization: We used the random search algorithm sup- ported by Hyperopt [2] library to randomly search for an optimal set of hyper- parameters from a given parameter space. In Fig. 2 ,various hyperparameters we tuned are plotted against the validation accuracy. Our observations for each hyperparameter are summarized below: Filter Size: 64 \ufb01lters in the \ufb01rst layer of network can preserve most of the characteristics of input, but increasing it to 128 is ine\ufb03cient as over\ufb01tting occurs. Kernel Size: There is a trade-o\ufb00between kernel size and capturing complex non-linear features. Using a small kernel size will require more layers to capture features whereas using a large kernel size will require less layers. Large kernels capture simple non-linear features whereas using a smaller kernel will help us capture more complex non-linear features.",
            "Using a small kernel size will require more layers to capture features whereas using a large kernel size will require less layers. Large kernels capture simple non-linear features whereas using a smaller kernel will help us capture more complex non-linear features. However, with more layers, backprop- agation necessitates the need for a large memory. We experimented with large kernel size and gradually increased the layers in order to capture more complex features. The results are not conclusive and thus we chose kernel size of 7 against 3.",
            "10 Sarthak et al. Dropout: Dropout rate of 0.1 works well for our data. When using a higher dropout rate the network is not able to capture the patterns in training dataset. Batch Size: There is always a trade-o\ufb00between batch size and getting ac- curate gradients. Using a large batch size helps the model to get more accurate gradients since the model tries to optimize gradients over a large set of images. We found that using a batch size of 128 helped the model to train faster and get better results than using a batch size less than 128. Number of hidden units in bi-directional GRU : Varying the number of hidden units and layers in GRU helps the model to capture temporal features which can play a signi\ufb01cant role in identifying the language correctly. The optimal number of hidden units and layers depends on the complexity of the dataset. Using less number of hidden units may capture less features whereas using large number of hidden units may be computationally expensive. In our case we found that using 1536 hidden units in a single bi-directional GRU layer leads to the best result.",
            "Using less number of hidden units may capture less features whereas using large number of hidden units may be computationally expensive. In our case we found that using 1536 hidden units in a single bi-directional GRU layer leads to the best result. Image Size: We experimented with log-Mel spectra images of sizes 64 \u00d7 64 and 128 \u00d7 128 pixels and found that our model worked best with images of size of 128 \u00d7 128 pixels. We also evaluated our model on data with mixup augmentation [28]. It is a data augmentation technique that also acts as a regularization technique and prevents over\ufb01tting. Instead of directly taking images from the training dataset as input, mixup takes a linear combination of any two random images and feeds it as input. The following equations were used to prepared a mixed-up dataset: Input Image = \u03b1 \u2217I1 + (1 \u2212\u03b1) \u2217I2, (1) and Input Label = \u03b1 \u2217L1 + (1 \u2212\u03b1) \u2217L2, (2) where \u03b1 \u2208[0, 1] is a random variable from a \u03b2-distribution, I1.",
            "3.6 Model details: 2D-ConvNet This model is a similar model to 2D-ConvNet with Attention and bi-directional GRU described in section 3.5 except that it lacks skip connections, attention layers, bi-directional GRU and the embedding layer incorporated in the previous model. 3.7 Dataset We classi\ufb01ed six languages (English, French, German, Spanish, Russian and Ital- ian) from the VoxForge [23] dataset. VoxForge is an open-source speech corpus which primarily consists of samples recorded and submitted by users using their own microphone. This results in signi\ufb01cant variation of speech quality between samples making it more representative of real world scenarios.",
            "Spoken Language Identi\ufb01cation using ConvNets 11 Our dataset consists of 1,500 samples for each of six languages. Out of 1,500 samples for each language, 1,200 were randomly selected as training dataset for that language and rest 300 as validation dataset using k-fold cross-validation. To sum up, we trained our model on 7,200 samples and validated it on 1800 samples comprising six languages. The results are discussed in next section. 4 Results and Discussion This paper discusses two end-to-end approaches which achieve state-of-the-art results in both the image as well as audio domain on the VoxForge dataset [23]. In Table 4, we present all the classi\ufb01cation accuracies of the two models of the cases with and without mixup for six and four languages. In the audio domain (using raw audio waveform as input), 1D-ConvNet achieved a mean accuracy of 93.7% with a standard deviation of 0.3% on run- ning k-fold cross validation. In Fig 3 (a) we present the confusion matrix for the 1D-ConvNet model.",
            "In Fig 3 (a) we present the confusion matrix for the 1D-ConvNet model. In the image domain (obtained by taking log-Mel spectra of raw audio), 2D- ConvNet with 2D attention (channel and spatial attention) and bi-directional GRU achieved a mean accuracy of 95.0% with a standard deviation of 1.2% for six languages. This model performed better when mixup regularization was ap- plied. 2D-ConvNet achieved a mean accuracy of 95.4% with standard deviation of 0.6% on running k-fold cross validation for six languages when mixup was ap- plied. In Fig 3 (b) we present the confusion matrix for the 2D-ConvNet model. 2D attention models focused on the important features extracted by convolutional layers and bi-directional GRU captured the temporal features. Table 4: Results of the two models and all its variations Languages Feature Desc.",
            "2D attention models focused on the important features extracted by convolutional layers and bi-directional GRU captured the temporal features. Table 4: Results of the two models and all its variations Languages Feature Desc. Network Mixup Accuracy En, Es, Fr, De, Ru, It Raw Waveform 1D ConvNet No 93.7 log-Mel Spectra 2D ConvNet No 94.3 Yes 95.4 2D ConvNet with Attention and GRU No 94.3 Yes 95.0 En, Es, Fr, De Raw Waveform 1D ConvNet No 94.4 log-Mel Spectra 2D ConvNet No 96.0 Yes 96.3 2D ConvNet with Attention and GRU No 94.7 Yes 93.7 Misclassi\ufb01cation Several of the spoken languages in Europe belong to the Indo-European family. Within this family, the languages are divided into three phyla which are Romance, Germanic and Slavic.",
            "Within this family, the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts",
            "12 Sarthak et al. as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Katze in German and Ciao in Italian becomes Chao in Spanish. Our model confuses between French (Fr) and Russian (Ru) while these lan- guages belong to di\ufb00erent phyla, many words from French were adopted into Russian such as automate (oot-oo-mate) in French becomes ABTOMaT (a\ufb00- taa-maat) in Russian which have similar pronunciation. en en es es it it ru ru de de fr fr Predicted label en en es es it it ru ru de de fr fr True label 88.9 2.0 1.7 * 4.5 2.7 * 92.7 3.1 1.6 1.0 * * 2.3 95.8 * * * 1.6 1.7 1.4 93.2 * 1.7 1.9 * * 96.2 1.0 1.6 * 1.4 * * 95.",
            "0 * * 2.3 95.8 * * * 1.6 1.7 1.4 93.2 * 1.7 1.9 * * 96.2 1.0 1.6 * 1.4 * * 95.6 Classification accuracy = 93.7 (a) en en es es it it ru ru de de fr fr Predicted label en en es es it it ru ru de de fr fr True label 95.7 * 1.3 1.0 * 1.0 1.7 94.0 3.7 * * 1.0 1.3 97.0 * * * 1.0 * 97.0 1.0 3.0 * * 1.0 93.7 1.3 1.3 * * 2.3 * 95.0 Classification accuracy = 95.4 (b) Fig. 3: Confusion matrix for classi\ufb01cation of six languages with our (a) 1D- ConvNet and (b) 2D-ConvNet model.",
            "3 * * 2.3 * 95.0 Classification accuracy = 95.4 (b) Fig. 3: Confusion matrix for classi\ufb01cation of six languages with our (a) 1D- ConvNet and (b) 2D-ConvNet model. Asterisk (*) marks a value less than 0.1%. Future Scope The performance of raw audio waveforms as input features to ConvNet can be further improved by applying silence removal in the audio. Also, there is scope for improvement by augmenting available data through various conventional techniques like pitch shifting, adding random noise and changing speed of audio. These help in making neural networks more robust to variations which might be present in real world scenarios. There can be further exploration of various feature extraction techniques like Constant-Q transform and Fast Fourier Transform and assessment of their impact on Language Identi\ufb01cation. There can be further improvements in neural network architectures like con- catenating the high level features obtained from 1D-ConvNet and 2D-ConvNet, before performing classi\ufb01cation. There can be experiments using deeper net- works with skip connections and Inception modules.",
            "There can be further improvements in neural network architectures like con- catenating the high level features obtained from 1D-ConvNet and 2D-ConvNet, before performing classi\ufb01cation. There can be experiments using deeper net- works with skip connections and Inception modules. These are known to have positively impacted the performance of Convolutional Neural Networks. 5 Conclusion There are two main contributions of this paper in the domain of spoken language identi\ufb01cation. Firstly, we presented an extensive analysis of raw audio waveforms as input features to 1D-ConvNet. We experimented with various hyperparame- ters in our 1D-ConvNet and evaluated their e\ufb00ect on validation accuracy. This",
            "Spoken Language Identi\ufb01cation using ConvNets 13 method is able to bypass the computational overhead of conventional approaches which depend on generation of spectrograms as a necessary pre-procesing step. We were able to achieve an accauracy of 93.7% using this technique. Next, we discussed the enhancement in performance of 2D-ConvNet using mixup augmentation, which is a recently developed technique to prevent over- \ufb01tting on test data.This approach achieved an accuracy of 95.4%. We also analysed how attention mechanism and recurrent layers impact the performance of networks. This approach achieved an accuracy of 95.0%. References 1. Bartz, C., Herold, T., Yang, H., Meinel, C.: Language identi\ufb01cation using deep convolutional recurrent neural networks. In: International Conference on Neural Information Processing. pp. 880\u2013889. Springer (2017) 2. Bergstra, J., Yamins, D., Cox, D.D.: Making a science of model search: Hyperpa- rameter optimization in hundreds of dimensions for vision architectures (2013) 3.",
            "pp. 880\u2013889. Springer (2017) 2. Bergstra, J., Yamins, D., Cox, D.D.: Making a science of model search: Hyperpa- rameter optimization in hundreds of dimensions for vision architectures (2013) 3. Chen, L., Zhang, H., Xiao, J., Nie, L., Shao, J., Liu, W., Chua, T.S.: Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5659\u20135667 (2017) 4. Dehak, N., Torres-Carrasquillo, P.A., Reynolds, D., Dehak, R.: Language recogni- tion via i-vectors and dimensionality reduction. In: Twelfth annual conference of the international speech communication association (2011) 5.",
            "Dehak, N., Torres-Carrasquillo, P.A., Reynolds, D., Dehak, R.: Language recogni- tion via i-vectors and dimensionality reduction. In: Twelfth annual conference of the international speech communication association (2011) 5. Endah Sa\ufb01tri, N., Zahra, A., Adriani, M.: Spoken language identi\ufb01- cation with phonotactics methods on minangkabau, sundanese, and ja- vanese languages. Procedia Computer Science 81, 182\u2013187 (12 2016). https:\/\/doi.org\/10.1016\/j.procs.2016.04.047 6. Ferrer, L., Sche\ufb00er, N., Shriberg, E.: A comparison of approaches for modeling prosodic features in speaker recognition. In: 2010 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 4414\u20134417. IEEE (2010) 7.",
            "In: 2010 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 4414\u20134417. IEEE (2010) 7. Ganapathy, S., Han, K., Thomas, S., Omar, M., Segbroeck, M.V., Narayanan, S.S.: Robust language identi\ufb01cation using convolutional neural network features. In: Fifteenth annual conference of the international speech communication association (2014) 8. Gazeau, V., Varol, C.: Automatic spoken language recognition with neural net- works. Int. J. Inf. Technol. Comput. Sci.(IJITCS) 10(8), 11\u201317 (2018) 9. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recog- nition. In: 2016 IEEE Conference on Computer Vision and Pattern Recogni- tion, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016. pp. 770\u2013778 (2016).",
            "In: 2016 IEEE Conference on Computer Vision and Pattern Recogni- tion, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016. pp. 770\u2013778 (2016). https:\/\/doi.org\/10.1109\/CVPR.2016.90, https:\/\/doi.org\/10.1109\/CVPR.2016. 90 10. Hershey, S., Chaudhuri, S., Ellis, D.P., Gemmeke, J.F., Jansen, A., Moore, R.C., Plakal, M., Platt, D., Saurous, R.A., Seybold, B., et al.: Cnn architectures for large-scale audio classi\ufb01cation. In: 2017 ieee international conference on acoustics, speech and signal processing (icassp). pp. 131\u2013135. IEEE (2017) 11. Howard, J., et al.: fastai. https:\/\/github.com\/fastai\/fastai (2018) 12.",
            "pp. 131\u2013135. IEEE (2017) 11. Howard, J., et al.: fastai. https:\/\/github.com\/fastai\/fastai (2018) 12. Kumar, P., Biswas, A., Mishra, A.N., Chandra, M.: Spoken language identi\ufb01cation using hybrid feature extraction methods. arXiv preprint arXiv:1003.5623 (2010)",
            "14 Sarthak et al. 13. Lee, J., Kim, T., Park, J., Nam, J.: Raw waveform-based audio classi\ufb01cation using sample-level cnn architectures. arXiv preprint arXiv:1712.00866 (2017) 14. LibROSA: https:\/\/librosa.github.io\/librosa\/. https:\/\/librosa.github.io\/ librosa\/, accessed on 16 Jul 2019 15. Lopez-Moreno, I., Gonzalez-Dominguez, J., Plchot, O., Martinez, D., Gonzalez- Rodriguez, J., Moreno, P.: Automatic language identi\ufb01cation using deep neural networks. In: 2014 IEEE international conference on acoustics, speech and signal processing (ICASSP). pp. 5337\u20135341. IEEE (2014) 16. Martinez, D., Plchot, O., Burget, L., Glembek, O., Mat\u02c7ejka, P.: Language recogni- tion in ivectors space. In: Twelfth Annual Conference of the International Speech Communication Association (2011) 17.",
            "Martinez, D., Plchot, O., Burget, L., Glembek, O., Mat\u02c7ejka, P.: Language recogni- tion in ivectors space. In: Twelfth Annual Conference of the International Speech Communication Association (2011) 17. Montavon, G.: Deep learning for spoken language identi\ufb01cation. In: NIPS Work- shop on deep learning for speech recognition and related applications. pp. 1\u20134 (2009) 18. Obuchi, Y., Sato, N.: Language identi\ufb01cation using phonetic and prosodic hmms with feature normalization. In: Proceedings.(ICASSP\u201905). IEEE International Con- ference on Acoustics, Speech, and Signal Processing, 2005. vol. 1, pp. I\u2013569. IEEE (2005) 19. Revay, S., Teschke, M.: Multiclass language identi\ufb01cation using deep learning on spectral images of audio signals. arXiv preprint arXiv:1905.04348 (2019) 20.",
            "IEEE (2005) 19. Revay, S., Teschke, M.: Multiclass language identi\ufb01cation using deep learning on spectral images of audio signals. arXiv preprint arXiv:1905.04348 (2019) 20. Rong Tong, Bin Ma, Donglai Zhu, Haizhou Li, Eng Siong Chng: In- tegrating acoustic, prosodic and phonotactic features for spoken lan- guage identi\ufb01cation. In: 2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings. vol. 1, pp. I\u2013I (May 2006). https:\/\/doi.org\/10.1109\/ICASSP.2006.1659993 21. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.: Dropout: a simple way to prevent neural networks from over\ufb01tting. The journal of machine learning research 15(1), 1929\u20131958 (2014) 22.",
            "The journal of machine learning research 15(1), 1929\u20131958 (2014) 22. Szegedy, C., Vanhoucke, V., Io\ufb00e, S., Shlens, J., Wojna, Z.: Rethinking the incep- tion architecture for computer vision. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2818\u20132826 (2016) 23. voxforge.org: Free speech recognition (linux, windows and mac) - voxforge.org. http:\/\/www.voxforge.org\/, accessed on 16 Jul 2019 24. WEI, Q., LIU, Y., RUAN, X.: A report on audio tagging with deeper cnn, 1d- convnet and 2d-convnet 25. Xu, K., Zhu, B., Kong, Q., Mi, H., Ding, B., Wang, D., Wang, H.: General audio tagging with ensembling convolutional neural networks and statistical features. The Journal of the Acoustical Society of America 145(6), EL521\u2013EL527 (2019) 26.",
            "The Journal of the Acoustical Society of America 145(6), EL521\u2013EL527 (2019) 26. Xu, Y., Huang, Q., Wang, W., Foster, P., Sigtia, S., Jackson, P.J., Plumbley, M.D.: Unsupervised feature learning based on deep models for environmental audio tag- ging. IEEE\/ACM Transactions on Audio, Speech, and Language Processing 25(6), 1230\u20131241 (2017) 27. Youtube: Retrieved from www.youtube.com. http:\/\/www.youtube.com, accessed on 16 Jul 2019 28. Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412 (2017) 29. Zissman, M.A.: Comparison of four approaches to automatic language identi\ufb01ca- tion of telephone speech. IEEE Transactions on speech and audio processing 4(1), 31 (1996)"
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1910.04269.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 8858.999816894531,
    "avg_doclen_est": 167.15093994140625
}
