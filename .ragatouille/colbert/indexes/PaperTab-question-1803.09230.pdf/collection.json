[
  "Pay More Attention: Neural Architectures for Question-Answering Zia Hasan Samsung Electronics Suwon, South Korea ziahasan@stanford.edu Sebastian Fischer Deutsche Telekom AG Berlin, Germany seb1988@stanford.edu Abstract Machine comprehension is a representative task of natural language understand- ing. Typically, we are given context paragraph and the objective is to answer a question that depends on the context. Such a problem requires to model the com- plex interactions between the context paragraph and the question. Lately, attention mechanisms have been found to be quite successful at these tasks and in par- ticular, attention mechanisms with attention \ufb02ow from both context-to-question and question-to-context have been proven to be quite useful. In this paper, we study two state-of-the-art attention mechanisms called Bi-Directional Attention Flow (BiDAF) and Dynamic Co-Attention Network (DCN) and propose a hybrid scheme combining these two architectures that gives better overall performance. Moreover, we also suggest a new simpler attention mechanism that we call Dou- ble Cross Attention (DCA) that provides better results compared to both BiDAF and Co-Attention mechanisms while providing similar performance as the hybrid scheme.",
  "Moreover, we also suggest a new simpler attention mechanism that we call Dou- ble Cross Attention (DCA) that provides better results compared to both BiDAF and Co-Attention mechanisms while providing similar performance as the hybrid scheme. The objective of our paper is to focus particularly on the attention layer and to suggest improvements on that. Our experimental evaluations show that both our proposed models achieve superior results on the Stanford Question Answering Dataset (SQuAD) compared to BiDAF and DCN attention mechanisms. 1 Introduction Enabling machines to understand natural language is one of the key challenges to achieve arti\ufb01cially intelligent systems. Asking machines questions and getting a meaningful answer adds value to us since it automatizes knowledge acquisition efforts drastically. Apple\u2019s Siri and Amazon\u2019s Echo are two such examples of mass market products capable of machine comprehension that has led to a paradigm shift on how consumers\u2019 interact with machines. Over the last decade, research in the \ufb01eld of Natural Language Processing (NLP) has massively bene\ufb01ted from neural architectures. Those approaches have outperformed former state-of-the-art non-neural machine learning model families while needing far less human intervention since they don\u2019t require any manual feature engineering.",
  "Those approaches have outperformed former state-of-the-art non-neural machine learning model families while needing far less human intervention since they don\u2019t require any manual feature engineering. A subset of NLP research focuses on building systems that are able to answer questions about a given document. To jointly expand the current best prac- tice, the Stanford Question Answering Dataset (SQuAD) was setup as a basis for a global competi- tion between different research groups [1]. SQuAD was published in 2016 and includes 100,000+ context-question-triplets on 500+ articles, signi\ufb01cantly larger than previous reading comprehension datasets [2]. The context paragraphs were obtained from more then 500 Wikipedia articles and the answers were sourced with Amazon Mechanical Turk. Recently, researchers were able to make machines outperform humans (as of Jan 2018) [2]. Answers in this dataset are taken from the docu- ment itself and are not dynamically generated from scratch.",
  "Recently, researchers were able to make machines outperform humans (as of Jan 2018) [2]. Answers in this dataset are taken from the docu- ment itself and are not dynamically generated from scratch. Instead of generating text that provides a suitable answer, the objective is to \ufb01nd the boundaries in which the answer is contained in the 1 arXiv:1803.09230v1  [cs.CL]  25 Mar 2018",
  "document. The aim is to achieve close to human performance in generating correct answers from a context paragraph given any new unseen questions. To solve this problem of question answering, neural attention mechanisms have recently gained signi\ufb01cant popularity by focusing on the most relevant area within a context paragraph, useful to an- swer the question [3, 4]. Attention mechanisms have proven to be an important extension to achieve better results in NLP problems [5]. While earlier attention mechanisms for this task were usually uni-directional, obtaining a \ufb01xed size vector for each context word summarizing the question words, bi-directional attention \ufb02ow applies an attention scheme in both directions (context-to-question as well as question-to-context). In this paper, we study two state-of-the-art neural architectures with an attention \ufb02ow going in both directions called Bi-Directional Attention Flow (BiDAF) [6] and Dynamic Co-Attention network (DCN) [7] that were once themselves leading architectures in the SQuAD challenge. We would also like to propose yet another hybrid neural architecture that shows competitive results by bringing together these two models.",
  "We would also like to propose yet another hybrid neural architecture that shows competitive results by bringing together these two models. More speci\ufb01cally, we combined the attention layer of both BiDAF and Co-Attention models. In addition to this, we propose another simpler model family called Double Cross Attention (DCA) which in itself performs better than both BiDAF and Co-Attention while giving similar performance as hybrid model. The objective of this paper is to do a comparative study of the performance of attention layer and not to optimize the performance of the overall system. 2 Model We started our development by re-implementing the BiDAF and DCN models. We \ufb01gured that these models individually enhanced the baseline performance signi\ufb01cantly, so the hope was that a combination would eventually lead to superior results. Thereby we created our \u201chybrid\u201d model, which we will subsequently explain shortly. In the following subsections, we describe each layer of our model in more detail. 2.1 Word and Character Embedding Layer The word embedding layer maps each word in a context and question to a vector of \ufb01xed size using pre-trained GloVe embeddings [8].",
  "In the following subsections, we describe each layer of our model in more detail. 2.1 Word and Character Embedding Layer The word embedding layer maps each word in a context and question to a vector of \ufb01xed size using pre-trained GloVe embeddings [8]. First, we encode each word in the question and context with the pre-trained Glove embedding as given in the baseline code. Then we concatenate to the word encodings an optional Character-level Embedding with CNNs since it helps to deal with out-of- vocabulary words [6, 9]. The joint concatenated encoding of words and characters is subsequently fed into the context and question encoding layer. 2.2 Context and Question Encoding Layer Once we have a context and question embeddings, we use a Bidirectional GRU to translate these context and question embeddings into encodings. Whereas a simple LSTM/GRU cell encodes se- quence data such as a sentences only from left-to-right, a bi-directional approach also parses a sentence from the end to the start. Both representations of a sequence are then usually concatenated and are assumed to encode the sequence structure more expressively ultimately leading to higher model performance.",
  "Both representations of a sequence are then usually concatenated and are assumed to encode the sequence structure more expressively ultimately leading to higher model performance. 2.3 Attention Layer The attention layer is the modeling layer that eventually involves modeling the complex interactions between the context and question words. Next, we describe several different attention mechanisms that we implemented in our system. 2.3.1 Bidirectional attention \ufb02ow We implemented a complete BiDAF layer as suggested in the project handout and in the original pa- per [6]. Bi-directional attention \ufb02ow approaches the machine comprehension challenge slightly dif- ferently. Instead of using an attention layer for transforming context inputs to \ufb01xed-size vectors, the BiDAF model computes the attention from both question-to-context as well as context-to-question 2",
  "and combines them effectively. The basic idea is essentially to obtain a similarity matrix to capture relations between context and question words and use this matrix to obtain context-to-question as well as question-to-context attention vectors. Finally, these attention vectors are concatenated to the context encodings in a speci\ufb01c way to obtain the output of the Bi-directional attention \ufb02ow layer. In the original BiDAF paper, an additional Bidirectional-RNN is used to again encode these concate- nated vectors. However, it didn\u2019t give any improvement in our setup, hence we chose to omit it in our \ufb01nal implementation. 2.3.2 Dynamic Co-Attention Dynamic Co-Attention Network layer (DCN), similar to BiDAF involves a two-way attention be- tween the context and the question but unlike BiDAF, DCN involves a second-level attention com- putation over the previously computed attentions [7]. The dynamic co-attention network (DCN) is an end-to-end neural network architecture. The authors claim that the ability of attending to con- text inputs strongly depends on the query (question).",
  "The dynamic co-attention network (DCN) is an end-to-end neural network architecture. The authors claim that the ability of attending to con- text inputs strongly depends on the query (question). The intuition behind that is also re\ufb02ected by a human\u2019s ability to better answer a question on an input paragraph, when the question is known before reading the context itself, because then one can attend speci\ufb01cally to relevant information in the context. For details, please check the project handout, the original paper and our implementation code. In the original paper and the project handout, there was also a concept of sentinel vectors that was introduced but in our tests, it again didn\u2019t seem to provide any signi\ufb01cant advantage, so we again chose to omit this as well in our \ufb01nal implementation. 2.3.3 Hybrid BiDAF-Co-Attention (New Model) This is model that we propose and it builds heavily on aspects of the BiDAF[6] as well as the DCN models[7].",
  "2.3.3 Hybrid BiDAF-Co-Attention (New Model) This is model that we propose and it builds heavily on aspects of the BiDAF[6] as well as the DCN models[7]. Since the attention outputs from both the BiDAF and DCN seem to have their merits, our idea was to combine them by concatenating both attentions to the original context states. The intuition was that the neural network should be able to train in order to use and pick them both effectively. Experimental results that we describe later, also verify our claim. Please check the code for exact implementation details 2.3.4 Double Cross Attention (New Model) In this section, we propose another simple idea called Double Cross Attention (DCA) which seem to provide better results compared to both BiDAF and Co-Attention while providing similar per- formance as concatenated hybrid model discussed in previous section. The motivation behind this approach is that \ufb01rst we pay attention to each context and question and then we attend those atten- tions with respect to each other in a slightly similar way as DCN.",
  "The motivation behind this approach is that \ufb01rst we pay attention to each context and question and then we attend those atten- tions with respect to each other in a slightly similar way as DCN. The intuition is that if iteratively read/attend both context and question, it should help us to search for answers easily. The DCA mechanism is explained graphically in Figure. 1 and the formal description of the layer is as fol- lows. Assume we have context hidden states c1, c2..., cN \u2208 R2h and question hidden states q1, q2..., qM \u2208R2h obtained after passing context and question embeddings through a bi- directional GRU. First, we compute a cross-attention matrix S \u2208RN\u00d7M, which contains a sim- ilarity score Sij for each pair of context and question hidden states (ci, qj). We chose Sij = cT i qj, since it is a parameter free approach to calculate attention but one can also construct this function with a trainable weight parameter (which can be shared in the subsequent step).",
  "We chose Sij = cT i qj, since it is a parameter free approach to calculate attention but one can also construct this function with a trainable weight parameter (which can be shared in the subsequent step). First we obtain Context-to-Question (C2Q) attention vectors ai as follows: \u03b1i = softmaxS(i:) \u2208RM, ai = M X j=1 \u03b1j iqj \u2208R2h (1) Next, we also obtain Question-to-Context (Q2C) attention vectors bj as follows: \u03b2j = softmaxS(:j) \u2208RN, bj = N X i=1 \u03b2i jci \u2208R2h (2) 3",
  "Figure 1: Double Cross Attention Model Then we compute a second-level cross attention matrix R \u2208RN\u00d7M, which contains a similarity score Rij for each pair of context and question attention states (ai, bj). We again choose a simple dot product attention Rij = aT i bj. Additionally, we obtain Context Attention-to-Question Atten- tion(CA2QA) cross attention vectors di as follows: \u03b3i = softmaxR(i:) \u2208RM, di = M X 1 \u03b3j i bj \u2208R2h (3) Finally, we concatenate ci, ai and di as a new state [ci; ai; di] and pass it through a biLSTM layer to obtain double query attended encoded context states as follows. {u1, ....uN} = biLSTM({[c1; a1; d1], ....[cN; aN; dN]}) (4) Finally all attention layer outputs are concatenated and fed into a Softmax layer that computes the probability distributions for the start and end token independently, as it is done in the baseline implementation. 3 Experiments Before we started the enhancements of the baseline model, we studied the SQuAD data set. Figure.",
  "3 Experiments Before we started the enhancements of the baseline model, we studied the SQuAD data set. Figure. 2 shows the distribution of the answer, question and context lengths as well as the relative position of the answer span inside a context. Furthermore, we counted the different question types. We found that most answers have a length less than 5 words. Additionally, a question usually consists of 5-20 words. Moreover, we noticed that on average a context is of length 120 (visualization excluded due to lack of space). Furthermore, answers for a question tend to be represented by spans of context words that are at the beginning of a context. Finally, we can see that \u201cwhat\u201d questions build the majority of questions, almost the same amount as all other question types combined. 3.1 Results In this section, we report the results of our experiments. To ensure the generality of our model, we used Dropout technique for regularizing neural networks. We start our experiments with default hyperparameters: embedding size of 100, batch size 100, hidden size 200, learning rate of 0.001 and a dropout rate of 0.15.",
  "To ensure the generality of our model, we used Dropout technique for regularizing neural networks. We start our experiments with default hyperparameters: embedding size of 100, batch size 100, hidden size 200, learning rate of 0.001 and a dropout rate of 0.15. For character level encoding, default character embedding size is 20, kernel 4",
  "Figure 2: Exploratory Data Analysis MODEL (BiDAF) F1 EM Char Embedding Disabled 45.94% 36.91% Char Embedding Enable 47.48% 38.62% Table 1: Effect of Character Embedding MODEL F1 EM Baseline* 43.44% 34.37% BiDAF 47.48% 38.62% Co-Attention* 69.56% 58.52% Hybrid 70.95% 60.54% DCA 70.68% 60.37% *no char embedding Table 2: Performance Results size is 5 and number of \ufb01lters are 100. For each architecture, we report the evaluation metrics F1 and EM (Exact match) computed on the dev set. The effect of character embedding on the BiDAF model is reported in Table 1. We can notice that character embedding boosts up the performance by roughly 2% for both EM and F1 score. This is expected since character embedding can help deal with non-dictionary words by giving them a unique embedding.",
  "We can notice that character embedding boosts up the performance by roughly 2% for both EM and F1 score. This is expected since character embedding can help deal with non-dictionary words by giving them a unique embedding. Next, we report the results of the model performances for baseline, BiDAF, Co-Attention, Hybrid and DCA attention mechanisms in Table 2. Notice that none of these architectures were optimized for EM/F1 scores but we are more interested in difference between these mechanisms for a \ufb01xed set of hyperparameters. Hybrid and DCA have a slight edge over plain BiDAF and Co-Attention module as per the results. Co-Attention with char embedding was giving us worse results so we put the best numbers we got for Co-Attention there. We would like to point out that the BiDAF model here doesn\u2019t include BiLSTM layer as present in original paper because the BiLSTM didn\u2019t give any advantage except for slowing down the training. Selected tensorboard visualizations are also shown in Figure 3.",
  "We would like to point out that the BiDAF model here doesn\u2019t include BiLSTM layer as present in original paper because the BiLSTM didn\u2019t give any advantage except for slowing down the training. Selected tensorboard visualizations are also shown in Figure 3. Visualizations demonstrate that both hybrid and DCA models perform better than vanilla Co-Attention and BiDAF attention mechanisms and reduce the losses faster and increase the dev F1/EM scores faster as well. 5",
  "Table 3: F1/EM% on different question types MODEL what how who when which where why Baseline 44.23/34.81 54.14/42.38 34.94/31.51 39.28/29.63 45.76/38.57 34.50/26.47 48.65/14.29 BiDAF 50.12/41.51 57.84/46.36 42.64/36.99 44.64/36.42 51.37/44.71 39.91/30.88 72.06/57.14 DCN 71.44/61.53 73.83/60.93 66.24/60.73 66.08/53.08 76.35/67.57 68.64/58.82 82.12/57.14 Hybrid 74.83/64.92 77.71/66.89 66.29/58.45 65.30/53.09 76.52/67.58 73.22/63.24 74.05/57.",
  "12/57.14 Hybrid 74.83/64.92 77.71/66.89 66.29/58.45 65.30/53.09 76.52/67.58 73.22/63.24 74.05/57.14 DCA 73.16/63.62 77.59/65.56 68.77/61.19 65.39/51.85 74.35/64.51 71.33/61.76 70.24/57.14 Figure 3: Tensorboard Visualizations 3.2 Hyperparameter Tuning We made a brief attempt to do a bit of hyperparameter tuning on our proposed DCA model and we report the results in Table 4. Ideally, hyperparameter tuning for neural network architectures should be done using bayesian hyperparameter optimization but due to the lack of time we tried to do a random search on a small set of hyperparameters that we guessed could be more suitable. While, we didn\u2019t \ufb01nd any signi\ufb01cantly good set of parameters, we noticed that reducing the hidden size has a minor effect on improving the performance.",
  "While, we didn\u2019t \ufb01nd any signi\ufb01cantly good set of parameters, we noticed that reducing the hidden size has a minor effect on improving the performance. This is probably because it reduces the system complexity which makes the model easier to train. Table 4: Hyperparameter Tuning for DCA Model dropout learning rate hidden size glove batch-size F1 EM 0.15 0.001 200 100 100 70.68% 60.37% 0.15 0.01 100 100 100 71.01% 60.66% 0.25 0.01 100 100 100 70.88% 60.67% 0.15 0.001 100 100 100 71.30% 60.78% 0.10 0.0001 300 300 50 66.26% 54.83% 6",
  "3.3 Error Analysis In Table 5, we brie\ufb02y provide error analysis on a small sample of results for hybrid and DCA models and try to explain the model behavior. Table 5: Error Analysis on Hybrid and DCA models on some sample examples Context southern california is home to many major business districts. central business districts (cbd) include downtown los angeles, downtown san diego, downtown san bernardino, downtown bakers\ufb01eld, south coast metro and downtown river- side. Question what is the only district in the cbd to not have \u201ddowntown\u201d in it\u2019s name? True Answer south coast metro Predicted Answer \u201d\u201d(Hybrid model) / central business districts (DCA model) Explanation of Model Behav- ior Hybrid model didn\u2019t output any results probably because end position might have been smaller than the start position and this error results in making a mistake on the whole answer, DCA model gets confused as \u201ccentral business districts\u201d term also doesn\u2019t contain downtown. Context ................ , the most important of which were use of a nitrogen/oxygen mixture instead of pure oxygen before and during launch, and removal of \ufb02ammable cabin and space suit materials.",
  "Context ................ , the most important of which were use of a nitrogen/oxygen mixture instead of pure oxygen before and during launch, and removal of \ufb02ammable cabin and space suit materials. the block ii ......... Question what type of materials inside the cabin were removed to help prevent more \ufb01re hazards in the future? True Answer \ufb02ammable cabin and space suit materials Predicted Answer space suit materials (Hybrid model) / \ufb02ammable cabin and space suit materi- als(DCA model) Explanation of Model Behav- ior Hybrid model was not able to capture the \u201c\ufb02ammable cabin\u201d part probably be- cause it was followed by cabin and it looked for materials following the word \u201ccabin\u201d. DCA captures this correctly. Technically both answers are correct Context .............., the statistical behaviour of primes in the large, can be modelled. the \ufb01rst result in that direction is the prime number theorem, proven at the end of the 19th century, which says that......... Question what theorem states that the probability that a number n is prime is inversely proportional to its logarithm ?",
  "the \ufb01rst result in that direction is the prime number theorem, proven at the end of the 19th century, which says that......... Question what theorem states that the probability that a number n is prime is inversely proportional to its logarithm ? True Answer the prime number theorem Predicted Answer the prime number theorem (Hybrid model) / prime number theorem , proven at the end of the 19th century(DCA model) Explanation of Model Behav- ior While Hybrid model gets it right, DCA answer is not wrong but a model trained on tighter start and end dependencies might have been able to capture this more accurately. 4 Conclusions and Future work In this paper, we studied and implemented two well known attention mechanisms namely, BiDAF and Co-Attention. We also introduced a simple combination of these two schemes called Hybrid attention mechanism that outperforms both BiDAF and Co-Attention. In addition to this, we propose our own attention mechanism called Double Cross Attention that gives similar results as the Hybrid model. The objective of the paper was primarily to study and compare the two aforementioned popular attention schemes on their own and not to chase the leaderboard scores.",
  "In addition to this, we propose our own attention mechanism called Double Cross Attention that gives similar results as the Hybrid model. The objective of the paper was primarily to study and compare the two aforementioned popular attention schemes on their own and not to chase the leaderboard scores. In particular, we isolated the attention layer and suggested our own improvements to it. The comparative results between different schemes are obtained for same set of hyperparameters. To improve the F1/EM scores of the overall system, a number of enhancement techniques could be used. For e.g. while we simply concatenated character and word embeddings, more advanced techniques to effectively combine them have been suggested in the literature [10]. Also. a number of other attention mechanisms have been suggested which need to be investigated as well [11, 12]. Another possible improvement is to properly condition the end position on the start position of the answer span. An LSTM based solution was used in the original BiDAF paper. Exponential moving average of weights and ensembling are additional common methods to further \ufb01ne-tune and 7",
  "improve the results. Hierarchical Maxout Network as mentioned in the co-attention paper could be a replacement to our simple Softmax output layer to improve the performance even further. There are also a few possible directions where DCA model can further be improved/extended. We can continue recursively calculating the cross attention weights and combine them in some more intuitive or non-linear way. While, we didn\u2019t optimize for the number of parameters, it is possible to reduce the overall number of trainable parameters by appropriately sharing weights between layers when possible. All of the above mentioned suggestions, we see as enhancement opportunities (some we partially already tried to implement but could not \ufb01nally manage to include in \ufb01nal running model). As a \ufb01nal project for the cs224n course, we found the task challenging but we were extremely satis\ufb01ed with our own personal learning curve. We are sure that with even more time, we could signi\ufb01cantly improve our model from the baseline enhancement we achieved so far. All in all, we believe that the experience of this project, will be of utmost value for our future professional work.",
  "We are sure that with even more time, we could signi\ufb01cantly improve our model from the baseline enhancement we achieved so far. All in all, we believe that the experience of this project, will be of utmost value for our future professional work. Acknowledgements First of all, we would like to thank the course instructor Richard Socher for making the class highly informative and a great learning experience. We would also like to thank the TAs for prompt feed- back and insightful discussions. Lastly, we would like to thank the fellow students who regularly helped each other on course forum regarding any questions.. References [1] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for ma- chine comprehension of text. arXiv preprint arXiv:1606.05250, 2016. [2] SQuAD, The Stanford Question Answering Dataset, https://stanford-qa.com [3] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. ICLR, 2015.",
  "[2] SQuAD, The Stanford Question Answering Dataset, https://stanford-qa.com [3] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. ICLR, 2015. [4] Caiming Xiong, Stephen Merity, and Richard Socher. Dynamic memory networks for visual and textual question answering. ICML, 2016. [5] Dzmitry Bahdanau, KyungHyun Cho, Yoshua Bengio, Neural Machine Translation by jointly learning to align and translate. https://arxiv.org/abs/1409.0473, 2016. [6] Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. Bidirectional attention \ufb02ow for machine comprehension. arXiv preprint arXiv:1611.01603, 2016. [7] Caiming Xiong, Victor Zhong, and Richard Socher. Dynamic coattention networks for question answering. arXiv preprint arXiv:1611.01604, 2017.",
  "[7] Caiming Xiong, Victor Zhong, and Richard Socher. Dynamic coattention networks for question answering. arXiv preprint arXiv:1611.01604, 2017. [8] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word repre- sentation. Empirical Methods in Natural Language Processing (EMNLP), pages 1532?1543, 2014. [9] Yoon Kim. Convolutional Neural Networks for Sentence Classi\ufb01cation. arXiv:1408.5882, 2014. [10] Zhilin Yang, Bhuwan Dhingra, Ye Yuan, Junjie Hu, William W Cohen, and Ruslan Salakhutdinov. \u201cWords or characters? \ufb01ne-grained gating for reading comprehension\u201d. arXiv preprint arXiv:1611.01724, 2016. [11] Shuohang Wang, and Jing Jiang. Machine comprehension using match-lstm and answer pointer.",
  "\u201cWords or characters? \ufb01ne-grained gating for reading comprehension\u201d. arXiv preprint arXiv:1611.01724, 2016. [11] Shuohang Wang, and Jing Jiang. Machine comprehension using match-lstm and answer pointer. arXiv preprint arXiv:1608.07905, 2016. [12] Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang, Ting Liu, Guoping Hu. Attention-over-Attention Neural Networks for Reading Comprehension. https://arxiv.org/abs/1607.04423, 2017. 8"
]