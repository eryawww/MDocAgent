{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Procedural Reasoning Networks for Understanding Multimodal Procedures Mustafa Sercan Amac Semih Yagcioglu Aykut Erdem Erkut Erdem Hacettepe University Computer Vision Lab Dept. of Computer Engineering, Hacettepe University, Ankara, TURKEY {b21626915,n13242994,aykut,erkut}@cs.hacettepe.edu.tr Abstract This paper addresses the problem of com- prehending procedural commonsense knowl- edge. This is a challenging task as it re- quires identifying key entities, keeping track of their state changes, and understanding tem- poral and causal relations. Contrary to most of the previous work, in this study, we do not rely on strong inductive bias and explore the question of how multimodality can be ex- ploited to provide a complementary semantic signal. Towards this end, we introduce a new entity-aware neural comprehension model aug- mented with external relational memory units. Our model learns to dynamically update en- tity states in relation to each other while read- ing the text instructions.",
            "Towards this end, we introduce a new entity-aware neural comprehension model aug- mented with external relational memory units. Our model learns to dynamically update en- tity states in relation to each other while read- ing the text instructions. Our experimental analysis on the visual reasoning tasks in the recently proposed RecipeQA dataset reveals that our approach improves the accuracy of the previously reported models by a large margin. Moreover, we \ufb01nd that our model learns effec- tive dynamic representations of entities even though we do not use any supervision at the level of entity states.1 1 Introduction A great deal of commonsense knowledge about the world we live is procedural in nature and involves steps that show ways to achieve speci\ufb01c goals. Un- derstanding and reasoning about procedural texts (e.g. cooking recipes, how-to guides, scienti\ufb01c pro- cesses) are very hard for machines as it demands modeling the intrinsic dynamics of the procedures (Bosselut et al., 2018; Dalvi et al., 2018; Yagcioglu et al., 2018).",
            "That is, one must be aware of the entities present in the text, infer relations among them and even anticipate changes in the states of the entities after each action. For example, consider the cheeseburger recipe presented in Fig. 1. The 1The project website with code and demo is available at https:\/\/hucvl.github.io\/prn\/ instruction \u201csalt and pepper each patty and cook for 2 to 3 minutes on the \ufb01rst side\u201d in Step 5 entails mixing three basic ingredients, the ground beef, salt and pepper, together and then applying heat to the mix, which in turn causes chemical changes that alter both the appearance and the taste. From a natural language understanding perspective, the main dif\ufb01culty arises when a model sees the word patty again at a later stage of the recipe. It still cor- responds to the same entity, but its form is totally different. Over the past few years, many new datasets and approaches have been proposed that address this in- herently hard problem (Bosselut et al., 2018; Dalvi et al., 2018; Tandon et al., 2018; Du et al., 2019).",
            "Over the past few years, many new datasets and approaches have been proposed that address this in- herently hard problem (Bosselut et al., 2018; Dalvi et al., 2018; Tandon et al., 2018; Du et al., 2019). To mitigate the aforementioned challenges, the ex- isting works rely mostly on heavy supervision and focus on predicting the individual state changes of entities at each step. Although these models can accurately learn to make local predictions, they may lack global consistency (Tandon et al., 2018; Du et al., 2019), not to mention that building such annotated corpora is very labor-intensive. In this work, we take a different direction and explore the problem from a multimodal standpoint. Our basic motivation, as illustrated in Fig. 1, is that accompa- nying images provide complementary cues about causal effects and state changes. For instance, it is quite easy to distinguish raw meat from cooked one in visual domain.",
            "Our basic motivation, as illustrated in Fig. 1, is that accompa- nying images provide complementary cues about causal effects and state changes. For instance, it is quite easy to distinguish raw meat from cooked one in visual domain. In particular, we take advantage of recently pro- posed RecipeQA dataset (Yagcioglu et al., 2018), a dataset for multimodal comprehension of cooking recipes, and ask whether it is possible to have a model which employs dynamic representations of entities in answering questions that require multi- modal understanding of procedures. To this end, in- spired from (Santoro et al., 2018), we propose Pro- cedural Reasoning Networks (PRN) that incorpo- rates entities into the comprehension process and al- arXiv:1909.08859v1  [cs.CL]  19 Sep 2019",
            "dressing Step 1: Ingredients and Tools Step 2:\u00a0Form Patties Step 3:\u00a0Season Step 4:\u00a0Toast Buns Lightly toast the both halves of the hamburger bun, face down in the pan. Set aside. Step 5:\u00a0Cook Step 6:\u00a0Chop Onions & Tomatoes For the \"authentic\" feel you want to get a large onion and a large tomato, then slice a large slice from the middle to use on the hamburger. Step 7:\u00a0Chop Onions & Tomatoes Step 8:\u00a0Enjoy All that's left to do is enjoy this copycat double double! To be honest, this was impressively close to the real taste. I would de\ufb01nitely make this one again.",
            "Step 7:\u00a0Chop Onions & Tomatoes Step 8:\u00a0Enjoy All that's left to do is enjoy this copycat double double! To be honest, this was impressively close to the real taste. I would de\ufb01nitely make this one again. ground beef lettuce leaf onion salt pepper dressing hamburger bun American cheese tomato ground beef ground\u00a0beef ground beef ground\u00a0beef ground\u00a0beef ground\u00a0beef ground\u00a0beef ground\u00a0beef ground\u00a0beef ground beef ground\u00a0beef ground\u00a0beef ground\u00a0beef ground beef hamburger bun hamburger bun hamburger bun hamburger bun hamburger bun hamburger bun hamburger bun hamburger bun salt salt salt pepper pepper tomato lettuce leaf lettuce leaf onion onion onion onion tomato tomato dressing dressing Salt and pepper one side of the patty now, the other half will be done when grilling. Set the patty seasoned side down on the skillet, salt and pepper each patty and cook for 2 to 3 minutes on the \ufb01rst side. Flip the patties over and season with salt and pepper and immediately place one slice of cheese on each one. Cook for 2-3 minutes on the other side.",
            "Flip the patties over and season with salt and pepper and immediately place one slice of cheese on each one. Cook for 2-3 minutes on the other side. 1 hamburger bun,\u00a04 oz. ground beef (25-30% fat if available) (2 ounce per patty),\u00a0salt and pepper,\u00a0Thousand Island dressing (or In-N-Out of\ufb01cial spread),\u00a01 large tomato,\u00a01 large lettuce leaf,\u00a01 whole onion,\u00a02 slices real American cheese Assemble the burger in the following stacking order from the bottom up: \u00a0bottom bun, thousand island dressing, tomato, lettuce, beef patty with cheese, onion slice, beef patty with cheese, top bun Begin by preheating a cast iron skillet over medium heat. Make four patties by rolling 2-ounce portions of beef into balls and weigh it out on the kitchen scale. In-N-Out uses a 25-30% fat beef patty which is not easily available at a local grocery store, in many cases it would have to be ground by hand. Forming them slightly larger than buns.",
            "In-N-Out uses a 25-30% fat beef patty which is not easily available at a local grocery store, in many cases it would have to be ground by hand. Forming them slightly larger than buns. I do this by placing the 2 ounce beef in between 2 pieces of parchment paper then taking my large cast iron skillet and applying a little force to smash the beef into a patty. You will want to form them into a perfect circle with your hand if they do not come out right after the initial smash. Figure 1: A recipe for preparing a cheeseburger (adapted from the cooking instructions available at https: \/\/www.instructables.com\/id\/In-N-Out-Double-Double-Cheeseburger-Copycat). Each basic in- gredient (entity) is highlighted by a different color in the text and with bounding boxes on the accompanying images. Over the course of the recipe instructions, ingredients interact with each other, change their states by each cooking action (underlined in the text), which in turn alter the visual and physical properties of entities. For instance, the tomato changes it form by being sliced up and then stacked on a hamburger bun.",
            "Over the course of the recipe instructions, ingredients interact with each other, change their states by each cooking action (underlined in the text), which in turn alter the visual and physical properties of entities. For instance, the tomato changes it form by being sliced up and then stacked on a hamburger bun. lows to keep track of entities, understand their inter- actions and accordingly update their states across time. We report that our proposed approach signi\ufb01- cantly improves upon previously published results on visual reasoning tasks in RecipeQA, which test understanding causal and temporal relations from images and text. We further show that the dynamic entity representations can capture semantics of the state information in the corresponding steps. 2 Visual Reasoning in RecipeQA In our study, we particularly focus on the visual reasoning tasks of RecipeQA, namely visual cloze, visual coherence, and visual ordering tasks, each of which examines a different reasoning skill2. We brie\ufb02y describe these tasks below. Visual Cloze. In the visual cloze task, the question is formed by a sequence of four images from consecutive steps of a recipe where one of them is replaced by a placeholder.",
            "We brie\ufb02y describe these tasks below. Visual Cloze. In the visual cloze task, the question is formed by a sequence of four images from consecutive steps of a recipe where one of them is replaced by a placeholder. A model should select the correct one from a multiple-choice list of four answer candidates to \ufb01ll in the missing piece. In that regard, the task inherently requires aligning visual and textual information and understanding 2We intentionally leave the textual cloze task out from our experiments as the questions in this task does not necessarily need multimodality. temporal relationships between the cooking actions and the entities. Visual Coherence. The visual coherence task tests the ability to identify the image within a sequence of four images that is inconsistent with the text instructions of a cooking recipe. To succeed in this task, a model should have a clear understanding of the procedure described in the recipe and at the same time connect language and vision. Visual Ordering. The visual ordering task is about grasping the temporal \ufb02ow of visual events with the help of the given recipe text.",
            "To succeed in this task, a model should have a clear understanding of the procedure described in the recipe and at the same time connect language and vision. Visual Ordering. The visual ordering task is about grasping the temporal \ufb02ow of visual events with the help of the given recipe text. The questions show a set of four images from the recipe and the task is to sort jumbled images into the correct order. Here, a model needs to infer the temporal relations between the images and align them with the recipe steps. 3 Procedural Reasoning Networks In the following, we explain our Procedural Reason- ing Networks model. Its architecture is based on a bi-directional attention \ufb02ow (BiDAF) model (Gard- ner et al., 2018)3, but also equipped with an explicit reasoning module that acts on entity-speci\ufb01c rela- 3Our implementation is based on the implementation pub- licly available in AllenNLP (Gardner et al., 2018).",
            "CNN CNN CNN LSTM LSTM LSTM Step 1: Ingredients 8-12 oz (225-350g) gingersnap cookies (depending on how much crust you like!) 1\/4 cup (57g) butter, melted (or slightly more if you're going full-hog on the crust) 24 oz.. (680g) cream cheese, softened 15 oz. (425g) pumpkin puree 2\/3 cup (75g) sugar 4 eggs 1 teaspoon vanilla 1\/4 cup (30g) \ufb02our Pinch of salt Freshly ground cinnamon, ginger and nutmeg to taste (I use 1\/2 teaspoon each!) Optional: fresh ground pepper - I know it sounds weird, but it adds depth to the spice pro\ufb01le! In a mixer or food processor, combine the softened cream cheese, pumpkin puree, sugar, and vanilla extract until well blended. Add the eggs, one at a time, mixing after each until just incorporated. Combine \ufb02our and spices and slowly add to the liquid mixture. Pour mixture into crust.",
            "Add the eggs, one at a time, mixing after each until just incorporated. Combine \ufb02our and spices and slowly add to the liquid mixture. Pour mixture into crust. Step 3:\u00a0The Filling Bake the pumpkin cheesecake for 80-90 minutes, until the center is almost set., and barely jiggles in the middle. Use a knife to gently loosen the crust from the edge of the pan. Allow cheesecake to cool before removing the rim of the pan.\u00a0 Refrigerate for at least 4 hours and up to overnight. If you are traveling with the cheesecake, leave the pan in tact until ready to eat! You're gonna love this one, I just know it! Step 4:\u00a0Bake Step 2: The Crust CNN LSTM Char Embed Embed CNN Concat BiLSTM Embed CNN Concat BiLSTM Char Embed fresh ground pepper gingersnap cookies ground cinnamon pumpkin\u00a0puree cream\u00a0cheese nutmeg vanilla ginger butter sugar eggs \ufb02our salt Embed Preheat your oven to 350F (180C). Using a food processor (or a mallet and a baggie - go for it!",
            "Using a food processor (or a mallet and a baggie - go for it!), turn your gingersnaps into crumbs! Add butter to crumbs and process until well incorporated.\u00a0 (If you're using the mallet method, you can use a fork for this part!) I like to line just the bottom of a 9\" springform pan with parchment, but that is optional.\u00a0 Pat the crust mixture into your pan, covering just the bottom, or going up the sides as far as you dare! If you're going full-crust, it's a good idea to par- bake your crust (meaning bake it before \ufb01lling) for 5-10 mins.\u00a0 Embed CNN Concat BiLSTM Char Embed Char Embed Embed CNN Concat BiLSTM Bi-Attention Bi-Attention Entities  (Ingredients) R-RNN R-RNN R-RNN R-RNN Question (Visual Coherence Task) Answer Candidate Concat BiLSTM BiLSTM Similarity Char Embed CNN Concat BiLSTM Embed Recipe\u00a0 (4 Steps) MLP CNN MLP Figure 2: An illustration of our Procedural Reasoning Networks (PRN).",
            "For a sample question from visual coher- ence task in RecipeQA, while reading the cooking recipe, the model constantly performs updates on the representa- tions of the entities (ingredients) after each step and makes use of their representations along with the whole recipe when it scores a candidate answer. Please refer to the main text for more details. tional memory units. Fig. 2 shows an overview of the network architecture. It consists of \ufb01ve main modules: An input module, an attention module, a reasoning module, a modeling module, and an out- put module. Note that the question answering tasks we consider here are multimodal in that while the context is a procedural text, the question and the multiple choice answers are composed of images. 1. Input Module extracts vector representations of inputs at different levels of granularity by using several different encoders. 2. Reasoning Module scans the procedural text and tracks the states of the entities and their re- lations through a recurrent relational memory core unit (Santoro et al., 2018). 3. Attention Module computes context-aware query vectors and query-aware context vectors as well as query-aware memory vectors. 4.",
            "3. Attention Module computes context-aware query vectors and query-aware context vectors as well as query-aware memory vectors. 4. Modeling Module employs two multi- layered RNNs to encode previous layers out- puts. 5. Output Module scores a candidate answer from the given multiple-choice list. At a high level, as the model is reading the cooking recipe, it continually updates the internal memory representations of the entities (ingredients) based on the content of each step \u2013 it keeps track of changes in the states of the entities, providing an entity-centric summary of the recipe. The response to a question and a possible answer depends on the representation of the recipe text as well as the last states of the entities. All this happens in a series of implicit relational reasoning steps and there is no need for explicitly encoding the state in terms of a prede\ufb01ned vocabulary. 3.1 Input Module Let the triple (R, Q, A) be a sample input. Here, R denotes the input recipe which contains textual instructions composed of N words in total. Q represents the question that consists of a sequence of M images.",
            "3.1 Input Module Let the triple (R, Q, A) be a sample input. Here, R denotes the input recipe which contains textual instructions composed of N words in total. Q represents the question that consists of a sequence of M images. A denotes an answer that is either a single image or a series of L images depending on the reasoning task. In particular, for the visual cloze and the visual coherence type questions, the answer contains a single image (L = 1) and for the visual ordering task, it includes a sequence. We encode the input recipe R at character, word, and step levels. Character-level embedding layer uses a convolutional neural network, namely Char- CNN model by Kim (2014), which outputs charac- ter level embeddings for each word and alleviates the issue of out-of-vocabulary (OOV) words. In word embedding layer, we use a pretrained GloVe model (Pennington et al., 2014) and extract word- level embeddings4.",
            "In word embedding layer, we use a pretrained GloVe model (Pennington et al., 2014) and extract word- level embeddings4. The concatenation of the char- acter and the word embeddings are then fed to a two-layer highway network (Srivastava et al., 2015) to obtain a contextual embedding for each word in the recipe. This results in the matrix R\u2032 \u2208R2d\u00d7N. On top of these layers, we have another layer that encodes the steps of the recipe in an individual manner. Speci\ufb01cally, we obtain a step-level con- 4We also consider pretrained ELMo embeddings (Peters et al., 2018) in our experiments but found out that the perfor- mance gain does not justify the computational overhead.",
            "textual embedding of the input recipe containing T steps as S = (s1, s2, . . . , sT ) where si repre- sents the \ufb01nal state of a BiLSTM encoding the i-th step of the recipe obtained from the character and word-level embeddings of the tokens exist in the corresponding step. We represent both the question Q and the answer A in terms of visual embeddings. Here, we employ a pretrained ResNet-50 model (He et al., 2016) trained on ImageNet dataset (Deng et al., 2009) and represent each image as a real-valued 2048-d vector using features from the penultimate average- pool layer. Then these embeddings are passed \ufb01rst to a multilayer perceptron (MLP) and then its out- puts are fed to a BiLSTM. We then form a matrix Q\u2032 \u2208R2d\u00d7M for the question by concatenating the cell states of the BiLSTM.",
            "Then these embeddings are passed \ufb01rst to a multilayer perceptron (MLP) and then its out- puts are fed to a BiLSTM. We then form a matrix Q\u2032 \u2208R2d\u00d7M for the question by concatenating the cell states of the BiLSTM. For the visual ordering task, to represent the sequence of images in the answer with a single vector, we additionally use a BiLSTM and de\ufb01ne the answering embedding by the summation of the cell states of the BiLSTM. Finally, for all tasks, these computations produce answer embeddings denoted by a \u2208R2d\u00d71. 3.2 Reasoning Module As mentioned before, comprehending a cooking recipe is mostly about entities (basic ingredients) and actions (cooking activities) described in the recipe instructions. Each action leads to changes in the states of the entities, which usually affects their visual characteristics. A change rarely oc- curs in isolation; in most cases, the action affects multiple entities at once. Hence, in our reasoning module, we have an explicit memory component implemented with relational memory units (San- toro et al., 2018).",
            "A change rarely oc- curs in isolation; in most cases, the action affects multiple entities at once. Hence, in our reasoning module, we have an explicit memory component implemented with relational memory units (San- toro et al., 2018). This helps us to keep track of the entities, their state changes and their relations in relation to each other over the course of the recipe (see Fig. 3). As we will examine in more detail in Section 4, it also greatly improves the interpretabil- ity of model outputs. Speci\ufb01cally, we set up the memory with a mem- ory matrix E \u2208RdE\u00d7K by extracting K entities (ingredients) from the \ufb01rst step of the recipe5. We initialize each memory cell ei representing a spe- ci\ufb01c entity by its CharCNN and pre-trained GloVe embeddings6. From now on, we will use the terms 5The \ufb01rst steps of the recipes in RecipeQA commonly contain a list of ingredients. 6Multi-word entities (e.g.",
            "From now on, we will use the terms 5The \ufb01rst steps of the recipes in RecipeQA commonly contain a list of ingredients. 6Multi-word entities (e.g. minced garlic) are represented by the average embedding vector of the words that they con- tain, and OOV words are expressed with the average word memory cells and entities interchangeably through- out the paper. Since the input recipe is given in the form of a procedural text decomposed into a number of steps, we update the memory cells after each step, re\ufb02ecting the state changes happened on the entities. This update procedure is modelled via a relational recurrent neural network (R-RNN), re- cently proposed by Santoro et al. (2018).",
            "This update procedure is modelled via a relational recurrent neural network (R-RNN), re- cently proposed by Santoro et al. (2018). It is built on a 2-dimensional LSTM model whose matrix of cell states represent our memory matrix E. Here, each row i of the matrix E refers to a speci\ufb01c entity ei and is updated after each recipe step t as follows: \u03c6i,t = R-RNN(\u03c6i,t\u22121, st) (1) where st denotes the embedding of recipe step t and \u03c6i,t = (hi,t, ei,t) is the cell state of the R-RNN at step t with hi,t and ei,t being the i-th row of the hidden state of the R-RNN and the dynamic representation of entity ei at the step t, respectively. The R-RNN model exploits a multi-headed self- attention mechanism (Vaswani et al., 2017) that allows memory cells to interact with each other and attend multiple locations simultaneously during the update phase. In Fig. 3, we illustrate how this interaction takes place in our relational memory module by consider- ing a sample cooking recipe and by presenting how the attention matrix changes throughout the recipe.",
            "In Fig. 3, we illustrate how this interaction takes place in our relational memory module by consider- ing a sample cooking recipe and by presenting how the attention matrix changes throughout the recipe. In particular, the attention matrix at a speci\ufb01c time shows the attention \ufb02ow from one entity (memory cell) to another along with the attention weights to the corresponding recipe step (offset column). The color intensity shows the magnitude of the at- tention weights. As can be seen from the \ufb01gure, the internal representations of the entities are ac- tively updated at each step. Moreover, as argued in (Santoro et al., 2018), this can be interpreted as a form of relational reasoning as each update on a speci\ufb01c memory cell is operated in relation to oth- ers. Here, we should note that it is often dif\ufb01cult to make sense of these attention weights. However, we observe that the attention matrix changes very gradually near the completion of the recipe. 3.3 Attention Module Attention module is in charge of linking the ques- tion with the recipe text and the entities present in the recipe.",
            "However, we observe that the attention matrix changes very gradually near the completion of the recipe. 3.3 Attention Module Attention module is in charge of linking the ques- tion with the recipe text and the entities present in the recipe. It takes the matrices Q\u2032 and R\u2032 from the input module, and E from the reasoning module vector of all the words.",
            "We'll start with a nice piece of roast, mine was 1 kilo and a half, but you can do less if you want.We'll have to cut the pieces so that it eventually \ufb01t in the bottle. This depends entirely from the size of the bottle itself, that said remember the meat\u00a0will shrink in the oven. Step 1: Slicin', Dicin'... salt oil potatoes rosemary thyme crushed garlic pork tenderloin blackpepper Then comes the phase that is known in italian as \"Pillottare\". Using a mortar, grind together the spices, the salt, the crushed garlic and add a drop or two of olive oil so that the mixture sticks together After that, take a knife, stab the meat and start \ufb01lling the cavities with the spices. When you're \ufb01nished it should look like your meat had grown a beard. Quickly clean the potatoes and the onion and chop them in medium sized pieces. Put half an inch of Olive oil in the pan and put everything in it. Add the remaining spices and, if you like, add some more.",
            "Quickly clean the potatoes and the onion and chop them in medium sized pieces. Put half an inch of Olive oil in the pan and put everything in it. Add the remaining spices and, if you like, add some more. Preheat the oven to 180C (356F) and then put this baby to roast. Turn it from time to time so that both sides cook evenly. I kept it one hour and ten, but it depends really from the size of your roast. You can always go old school and check with a toothpic from time to time. Bottle has to be clean, so after washing and drying it, and right before putting the meat in it, boil some water and pour it in for a quick rinse off. To avoid breaking the bottle pour some cold water in it and pour the boiling water into the cold water. You do not need much of it, just a cup or so, quickly rinse the bottle and throw the water away.\u00a0 Wait till the meat is cold, then put it into the freshly sterilized bottle and cover in olive oil. The meat has to rest for at least two days, then you can start eating it.",
            "Wait till the meat is cold, then put it into the freshly sterilized bottle and cover in olive oil. The meat has to rest for at least two days, then you can start eating it. Step 2: ... and Spicin' Step 3: Bring Company! Step 4: Burn Baby Burn! Step 5: Ready the Bottle. Step 6: Put the Piggies to Sleep. step 1 step 2 step 3 step 4 step 5 step 6 salt oil potatoes rosemary thyme crushed garlic pork tenderloin blackpepper Time Recipe: Oil Bottled Pork Tenderloin attending from attending to 0.0 0.2 0.4 0.6 0.8 1.0 Figure 3: Sample visualizations of the self-attention weights demonstrating both the interactions among the ingre- dients and between the ingredients and the textual instructions throughout the steps of a sample cooking recipe from RecipeQA (darker colors imply higher attention weights). The attention maps do not change much after the third step as the steps after that mostly provide some redundant information about the completed recipe.",
            "The attention maps do not change much after the third step as the steps after that mostly provide some redundant information about the completed recipe. and constructs the question-aware recipe represen- tation G and the question-aware entity representa- tion Y. Following the attention \ufb02ow mechanism described in (Seo et al., 2017a), we speci\ufb01cally calculate attentions in four different directions: (1) from question to recipe, (2) from recipe to question, (3) from question to entities, and (4) from entities to question. The \ufb01rst two of these attentions require comput- ing a shared af\ufb01nity matrix SR \u2208RN\u00d7M with SR i,j indicating the similarity between i-th recipe word and j-th image in the question estimated by SR i,j = w\u22a4 R[R\u2032 i; Q\u2032 j; R\u2032 i \u25e6Q\u2032 j] (2) where w\u22a4 R is a trainable weight vector, \u25e6and [; ] de- note elementwise multiplication and concatenation operations, respectively. Recipe-to-question attention determines the im- ages within the question that is most relevant to each word of the recipe.",
            "Recipe-to-question attention determines the im- ages within the question that is most relevant to each word of the recipe. Let \u02dcQ \u2208R2d\u00d7N repre- sent the recipe-to-question attention matrix with its i-th column being given by \u02dcQi = P j aijQ\u2032 j where the attention weight is computed by ai = softmax(SR i ) \u2208RM. Question-to-recipe attention signi\ufb01es the words within the recipe that have the closest similarity to each image in the question, and construct an attended recipe vector given by \u02dcr = P i biR\u2032 i with the attention weight is calculated by b = softmax(maxcol(SR)) \u2208RN where maxcol de- notes the maximum function across the column. The question-to-recipe matrix is then obtained by replicating \u02dcr N times across the column, giving \u02dcR \u2208R2d\u00d7N.",
            "The question-to-recipe matrix is then obtained by replicating \u02dcr N times across the column, giving \u02dcR \u2208R2d\u00d7N. Then, we construct the question aware represen- tation of the input recipe, G, with its i-th column Gi \u2208R8d\u00d7N denoting the \ufb01nal embedding of i-th word given by Gi = [R\u2032 i; \u02dcQi; R\u2032 i \u25e6\u02dcQi; R\u2032 i \u25e6\u02dcRi; ] . (3) Attentions from question to entities, and from entities to question are computed in a way similar to the ones described above. The only difference is that it uses a different shared af\ufb01nity matrix to be computed between the memory encoding entities E and the question Q\u2032. These attentions are then used to construct the question aware representation of entities, denoted by Y, that links and integrates the images in the question and the entities in the input recipe. 3.4 Modeling Module Modeling module takes the question-aware repre- sentations of the recipe G and the entities Y, and forms their combined vector representation.",
            "3.4 Modeling Module Modeling module takes the question-aware repre- sentations of the recipe G and the entities Y, and forms their combined vector representation. For this purpose, we \ufb01rst use a two-layer BiLSTM to read the question-aware recipe G and to encode the interactions among the words conditioned on the question. For each direction of BiLSTM , we use its hidden state after reading the last token as its output. In the end, we obtain a vector embedding c \u2208R2d\u00d71. Similarly, we employ a second BiL- STM, this time, over the entities Y, which results in another vector embedding f \u2208R2dE\u00d71. Finally, these vector representations are concatenated and then projected to a \ufb01xed size representation using o = \u03d5o([c; f]) \u2208R2d\u00d71 where \u03d5o is a multilayer perceptron with tanh activation function.",
            "3.5 Output Module The output module takes the output of the mod- eling module, encoding vector embeddings of the question-aware recipe and the entities Y, and the embedding of the answer A, and returns a simi- larity score which is used while determining the correct answer. Among all the candidate answer, the one having the highest similarity score is cho- sen as the correct answer. To train our proposed procedural reasoning network, we employ a hinge ranking loss (Collobert et al., 2011), similar to the one used in (Yagcioglu et al., 2018), given below. L = max{0, \u03b3 \u2212cos(o, a+) + cos(o, a\u2212)} (4) where \u03b3 is the margin parameter, a+ and a\u2212are the correct and the incorrect answers, respectively. 4 Experiments In this section, we describe our experimental setup and then analyze the results of the proposed Proce- dural Reasoning Networks (PRN) model. 4.1 Entity Extraction Given a recipe, we automatically extract the entities from the initial step of a recipe by using a dictionary of ingredients.",
            "4 Experiments In this section, we describe our experimental setup and then analyze the results of the proposed Proce- dural Reasoning Networks (PRN) model. 4.1 Entity Extraction Given a recipe, we automatically extract the entities from the initial step of a recipe by using a dictionary of ingredients. While determining the ingredients, we exploit Recipe1M (Marin et al., 2018) and Kaggle Whats Cooking Recipes (Yummly, 2015) datasets, and form our dictionary using the most commonly used ingredients in the training set of RecipeQA. For the cases when no entity can be extracted from the recipe automatically (20 recipes in total), we manually annotate those recipes with the related entities. 4.2 Training Details In our experiments, we separately trained models on each task, as well as we investigated multi-task learning where a single model is trained to solve all these tasks at once. In total, the PRN architecture consists of \u223c12M trainable parameters. We imple- mented our models in PyTorch (Paszke et al., 2017) using AllenNLP library (Gardner et al., 2018).",
            "In total, the PRN architecture consists of \u223c12M trainable parameters. We imple- mented our models in PyTorch (Paszke et al., 2017) using AllenNLP library (Gardner et al., 2018). We used Adam optimizer with a learning rate of 1e-4 with an early stopping criteria with the patience set to 10 indicating that the training procedure ends after 10 iterations if the performance would not improve. We considered a batch size of 32 due to our hardware constraints. In the multi-task setting, batches are sampled round-robin from all tasks, where each batch is solely composed of examples from one task. We performed our experiments on a system containing four NVIDIA GTX-1080Ti GPUs, and training a single model took around 2 hours. We employed the same hyperparameters for all the baseline systems. We plan to share our code and model implementation after the review process. 4.3 Baselines We compare our model with several baseline models as described below. We note that the results of the \ufb01rst two are previously reported in (Yagcioglu et al., 2018).",
            "We plan to share our code and model implementation after the review process. 4.3 Baselines We compare our model with several baseline models as described below. We note that the results of the \ufb01rst two are previously reported in (Yagcioglu et al., 2018). Hasty Student (Yagcioglu et al., 2018) is a heuristics-based simple model which ignores the recipe and gives an answer by examining only the question and the answer set using distances in the visual feature space. Impatient Reader (Hermann et al., 2015) is a simple neural model that takes its name from the fact that it repeatedly computes attention over the recipe after observing each image in the query. BiDAF (Seo et al., 2017a) is a strong reading comprehension model that employs a bi-directional attention \ufb02ow mechanism to obtain a question- aware representation and bases its predictions on this representation. Originally, it is a span-selection model from the input context. Here, we adapt it to work in a multimodal setting and answer multiple choice questions instead.",
            "Originally, it is a span-selection model from the input context. Here, we adapt it to work in a multimodal setting and answer multiple choice questions instead. BiDAF w\/ static memory is an extended version of the BiDAF model which resembles our proposed PRN model in that it includes a memory unit for the entities. However, it does not make any updates on the memory cells. That is, it uses the static entity embeeddings initialized with GloVe word vectors. We propose this baseline to test the signi\ufb01cance of the use of relational memory updates. 4.4 Results Table 1 presents the quantitative results for the vi- sual reasoning tasks in RecipeQA. In single-task training setting, PRN gives state-of-the-art results compared to other neural models. Moreover, it achieves the best performance on average. These results demonstrate the importance of having a dy- namic memory and keeping track of entities ex- tracted from the recipe. In multi-task training set-",
            "Vanilla-Apricot Shortbread Cookies Add to the whipped butter 1 cup of  baker's sugar. Stir until the sugar and  butter mix thoroughly. Add the whole  egg and the egg yolk and stir well. To\ufb00ee Bottomed Brownies Cut the brownie into small squares,  cleaning your knife after each cut. The topload of cocoa powder makes  this dessert so very rich that you don't  need much, and there will be ... Cherry Almond Torrone (Italian  Nougat) I used a knife, spatula, and pizza  roller. Use what you've got. Corn  starch and butter will help to prevent  sticking.... Apple Pie ...the apple pie \ufb01lling should not have  the skins on them, BUT... I made this  one for a friend of mine who is a health  conscious women and she insisted on  me leaving the skins on for all the  nutritional values.... Henderson's Sauce After it has been simmering for around  5 minutes, it is time to add some other  ingredients.",
            "Add all these being; Add  around 1 soup-spoon of sugar (1 soup  spoon brown or 2 soup spoons  white).... Absolutely Amazing Cream of  Celery Soup Add cream, lemon juice, hot sauce,  salt and pepper. Reheat and simmer  for about \ufb01ve minutes. ... Miniature Doughnut Coconut  Creatures Chill a can of coconut milk or cream in  the fridge overnight. When you\u2019re  ready to make the whipped cream,  open the can and scoop out the  hardened coconut. ... Mango Mint Ice Tea Take the measured amount of water  and heat it till hot. I used the  microwave here. You can heat the  water even on the stove top. To the  hot water add the Black tea powder  or the Black tea bags. Creme Brulee Recipe Place the ramekins into a pan with  high sides and carefully \ufb01ll the pan  with hot water until half way up the  sides of the ramekins. Make sure  not to splash any water into the  custard.",
            "Creme Brulee Recipe Place the ramekins into a pan with  high sides and carefully \ufb01ll the pan  with hot water until half way up the  sides of the ramekins. Make sure  not to splash any water into the  custard. bread Step: 4 Entity: water Step: 3 Entity: water Step: 1 Entity: cream Step: 6 Entity: cream Step: 2 Entity: sugar Step: 6 Entity: sugar Step: 5 Entity: butter Step: 6 Entity: butter Step: 3 Entity: sugar Food Categories vegetarian-and-vegan snacks-and-appetizers salad cocktails-and-mocktails sandwiches pizza soups-and-stews pie coffee canning-and-preserves bbq-and-grilling dessert bacon homebrew cupcakes cake breakfast pasta main-course beverages cookies recipes candy Step: 4 Entity: sugar (perfect) Lemon Meringue Pie ... Add half the sugar (150g) and whisk  again ... Figure 4: t-SNE visualizations of learned embeddings from each memory snapshot mapping to each entity and their corresponding states from each step for visual cloze task.",
            "Single-task Training Multi-task Training Model Cloze Coherence Ordering Average Cloze Coherence Ordering All Human\u2217 77.60 81.60 64.00 74.40 \u2013 \u2013 \u2013 \u2013 Hasty Student 27.35 65.80 40.88 44.68 \u2013 \u2013 \u2013 \u2013 Impatient Reader 27.36 28.08 26.74 27.39 \u2013 \u2013 \u2013 \u2013 BIDAF 53.95 48.82 62.42 55.06 44.62 36.00 63.93 48.67 BIDAF w\/ static memory 51.82 45.88 60.90 52.87 47.81 40.23 62.94 50.59 PRN 56.31 53.64 62.77 57.57 46.45 40.58 62.67 50.17 \u2217Taken from the RecipeQA project website, based on 100 questions sampled randomly from the validation set. Table 1: Quantitative comparison of the proposed PRN model against the baselines.",
            "Table 1: Quantitative comparison of the proposed PRN model against the baselines. ting where a single model is trained to solve all the tasks at once, PRN and BIDAF w\/ static memory perform comparably and give much better results than BIDAF. Note that the model performances in the multi-task training setting are worse than single-task performances. We believe that this is due to the nature of the tasks that some are more dif\ufb01cult than the others. We think that the perfor- mance could be improved by employing a carefully selected curriculum strategy (McCann et al., 2018). In Fig. 4, we illustrate the entity embeddings space by projecting the learned embeddings from the step-by-step memory snapshots through time with t-SNE to 3-d space from 200-d vector space. Color codes denote the categories of the cook- ing recipes. As can be seen, these step-aware embeddings show clear clustering of these cate- gories. Moreover, within each cluster, the entities are grouped together in terms of their state charac- teristics.",
            "Color codes denote the categories of the cook- ing recipes. As can be seen, these step-aware embeddings show clear clustering of these cate- gories. Moreover, within each cluster, the entities are grouped together in terms of their state charac- teristics. For instance, in the zoomed parts of the \ufb01gure, chopped and sliced, or stirred and whisked entities are placed close to each other. Fig. 5 demonstrates the entity arithmetics us- ing the learned embeddings from each entity step. Here, we show that the learned embedding from the memory snapshots can effectively capture the con- textual information about the entities at each time point in the corresponding step while taking into account of the recipe data. This basic arithmetic operation suggests that the proposed model can successfully capture the semantics of each entity\u2019s state in the corresponding step7. 5 Related Work In recent years, tracking entities and their state changes have been explored in the literature from a variety of perspectives. In an early work, Henaff et al. (2017) proposed a dynamic memory based network which updates entity states using a gat- ing mechanism while reading the text. Bansal et al.",
            "In an early work, Henaff et al. (2017) proposed a dynamic memory based network which updates entity states using a gat- ing mechanism while reading the text. Bansal et al. (2017) presented a more structured memory augmented model which employs memory slots for representing both entities and their relations. Pavez et al. (2018) suggested a conceptually simi- lar model in which the pairwise relations between attended memories are utilized to encode the world 7We used Gensim for calculating entity arithmetics using cosine distances between entity embeddings.",
            "Step 1:\u00a0 This is a cheap and easy method of an ancient cooking technique known as clay pot cooking using a common terra cotta \ufb02owerpot and saucer. You can spend over $100 on a clay cooker at a gourmet kitchen gadget store, or about $20 at a garden supply. You choose. Some of you may already have the pot lying in your yard, garage or shed. Once you try this you will probably be cooking all kinds of things in it! \u00a0 onions (Flowerpot Chicken) Step 3:\u00a0Prepare Vegetables. Chop your vegetables while the pot is soaking. You can use whatever you like for this, root vegetables mixed with onions are always a nice base. This time I used leeks, bell peppers, garlic and red onions. : onions (Flowerpot Chicken) :: Step 1:\u00a0 This is a cheap and easy method of an ancient cooking technique known as clay pot cooking using a common terra cotta \ufb02owerpot and saucer. You can spend over $100 on a clay cooker at a gourmet kitchen gadget store, or about $20 at a garden supply. You choose.",
            "You can spend over $100 on a clay cooker at a gourmet kitchen gadget store, or about $20 at a garden supply. You choose. Some of you may already have the pot lying in your yard, garage or shed. Once you try this you will probably be cooking all kinds of things in it! \u00a0 tomatoes (Flowerpot Chicken) ? : Step 1: Prepping the Vegetables. The \ufb01rst step is to have all the Vegetables prepped and ready to go in the pan, so \ufb01nely dice the Garlic, onions and Peppers. Don't worry about mixing them up in the bowl, all of these items are going to be sauteed in a small amount of oil at the next stage. Picture 1. Finely dice up the Garlic, you want it to be almost puree consistency. Picture 2. Finely dice up the Onions, this doesn't need to be as \ufb01ne as the garlic but you should ensure that they are all roughly the same size. Picture 3. Lastly dice up the bell pepper, I show you how i cut this in the video, but i will go over it quickly.",
            "Picture 3. Lastly dice up the bell pepper, I show you how i cut this in the video, but i will go over it quickly. Firstly i take off the four walls of the pepper, \ufb02atten them then cut them in to strips, then simply cut the other way so i have them diced. tomatoes (Chilli Con Carne) Step 1:\u00a0Ingredients ... pepperoni (I used what was left in a package which was enough for one layer) 1\/2 onion 2 roma tomatoes dried rosemary shredded mozarella and parmesan fresh savory, basil, tarragon, and thyme 2 or 3 cloves of garlic\u00a0 salt (sea or kosher salt are best) and pepper\u00a0 Slice the tomatoes and onion as thin as is reasonable, slice the garlic as thin as possible. Thoroughly wash the fresh herbs and pull the leaves from the stems. Discard the stems.",
            "Thoroughly wash the fresh herbs and pull the leaves from the stems. Discard the stems. tomatoes (Seven Layer Seven Grain Bread) Step 1: Gather Your Ingredients... ... 1 teaspoon dried oregano, 1\/8 teaspoon red pepper \ufb02akes (see step \ufb01ve for a bit of humor on this note), 3\/4 to 1 cup wine - Honestly, folks, don't be too particular about the wine. Red or white is \ufb01ne. (you may substitute chicken broth, or even add broth in addition to the wine. Be creative!)(you may substitute chicken broth, or even add broth in addition to the wine. Be creative!) 1 - 28 ounce can diced tomatoes (save the juice!) 1\/2 teaspoon dried Porcino mushrooms (Optional, see step #2) tomatoes (How to Make Chicken Cacciatore) Step 1:\u00a0 This is absolutely mind-blowingly good. Goat basically tastes like lamb, but is far leaner. (Lamb is the fattiest of the red meats.)",
            "Goat basically tastes like lamb, but is far leaner. (Lamb is the fattiest of the red meats.) It's very popular in a variety of different countries' cuisines, but for some reason has yet to gain a real following in the US. This recipe is inspired by the curried goat roti from Penny's Caribbean Cafe. While Penny doesn't share her secrets, this tastes awfully similar. Go get yourself some goat (or lamb if you must) and try it out! water (Caribbean Curried Goat) Step 4:\u00a0Add Everything Else. Add the rest of the curry powder and stir things about. When it starts to stick again add the water and deglaze again. Pour in just enough water to cover the meat, and leave a cup full of water near the pot to re\ufb01ll as it boils off. You want the meat to stay wet during the entire cooking process.\u00a0 In the picture below I've dropped in another boullion cube because they didn't all make it in with the onions.",
            "You want the meat to stay wet during the entire cooking process.\u00a0 In the picture below I've dropped in another boullion cube because they didn't all make it in with the onions. The details really don't matter too much in this dish - it cooks long enough that you've got LOTS of leeway to taste and modify.. : water (Caribbean Curried Goat) :: Step 1:\u00a0 All that sounded logic to me, and instead of looking on the net how others did it I started thinking how Bricobart would build such a device - I mean a bbq, not an anti-troll gun. And since I didn't want to spend any money I decided to build it from scratch.The project failed in the \ufb01rst trial, but ran like a small dog chased by a beeswarm in the second. Enjoy my poor men's vertical birdcage-based bbq! milk (Birdcage-BQ) ? : Step 3: Cooking. Melt the butter and add 1\/3 cup chopped onions. When the onions are cooked add the bacon bits. Now add the potatoes back to the pot and mash the potato mixture.",
            "milk (Birdcage-BQ) ? : Step 3: Cooking. Melt the butter and add 1\/3 cup chopped onions. When the onions are cooked add the bacon bits. Now add the potatoes back to the pot and mash the potato mixture. I use a potato masher or you can just use a fork. You still want it lumpy but the potatoes will help thicken the soup. Pour the milk and mix well. Add salt and pepper and heat until it is a slow boil. Remove from the stove and add the cheese and stir until melted. If you add the cheese too early it will go to the bottom and burn milk (Potato Soup for One) Step 2:\u00a0Meat Sauce Preheat oven to 180 degrees celsius. Brown off the mince in a large pan, depending on the fat content of the meat, you may or may not need a little oil. Drain the mince onto some paper towel to remove any oil and then place back in the pan. Add 4 slices of chopped prosciutto (or bacon\/pancetta) and fry for a few minutes.",
            "Drain the mince onto some paper towel to remove any oil and then place back in the pan. Add 4 slices of chopped prosciutto (or bacon\/pancetta) and fry for a few minutes. Add beef stock, tomato sauce, nutmeg, bayleaf and oregano. Simmer for at least 30 minutes. milk (Family Size Lasagne) Step 1:\u00a0Potato Prep + Seasonings Make sure all potatoes are peeled and cut into chunks. In a saucepan over medium heat, drop in the tablespoon of butter, the red pepper \ufb02akes and Italian seasoning. Let the butter melt and stir the seasonings around until they start smelling nice. :) milk (Potato Soup) Figure 5: Step-aware entity representations can be used to discover the changes occurred in the states of the ingredients between two different recipe steps. The difference vector between two entities can then be added to other entities to \ufb01nd their next states. For instance, in the \ufb01rst example, the difference vector encodes the chopping action done on onions. In the second example, it encodes the pouring action done on the water.",
            "The difference vector between two entities can then be added to other entities to \ufb01nd their next states. For instance, in the \ufb01rst example, the difference vector encodes the chopping action done on onions. In the second example, it encodes the pouring action done on the water. When these vectors are added to the representations of raw tomatoes and milk, the three most likely next states capture the semantics of state changes in an accurate manner. state. The main difference between our approach and these works is that by utilizing relational mem- ory core units we also allow memories to interact with each other during each update. Perez and Liu (2017) showed that similar ideas can be used to compile supporting memories in tracking dialogue state. Wang et al. (2017) has shown the importance of coreference signals for reading comprehension task. More recently, Dhin- gra et al. (2018) introduced a specialized recur- rent layer which uses coreference annotations for improving reading comprehension tasks. On lan- guage modeling task, Ji et al.",
            "More recently, Dhin- gra et al. (2018) introduced a specialized recur- rent layer which uses coreference annotations for improving reading comprehension tasks. On lan- guage modeling task, Ji et al. (2017) proposed a language model which can explicitly incorporate entities while dynamically updating their represen- tations for a variety of tasks such as language mod- eling, coreference resolution, and entity prediction. Our work builds upon and contributes to the growing literature on tracking states changes in procedural text. Bosselut et al. (2018) presented a neural model that can learn to explicitly predict state changes of ingredients at different points in a cooking recipe. Dalvi et al. (2018) proposed an- other entity-aware model to track entity states in scienti\ufb01c processes. Tandon et al. (2018) demon- strated that the prediction quality can be boosted by including hard and soft constraints to eliminate un- likely or favor probable state changes. In a follow- up work, Du et al. (2019) exploited the notion of label consistency in training to enforce similar pre- dictions in similar procedural contexts. Das et al.",
            "In a follow- up work, Du et al. (2019) exploited the notion of label consistency in training to enforce similar pre- dictions in similar procedural contexts. Das et al. (2019) proposed a model that dynamically con- structs a knowledge graph while reading the proce- dural text to track the ever-changing entities states. As discussed in the introduction, however, these previous methods use a strong inductive bias and assume that state labels are present during training. In our study, we deliberately focus on unlabeled procedural data and ask the question: Can multi- modality help to identify and provide insights to understanding state changes. 6 Conclusion We have presented a new neural architecture called Procedural Reasoning Networks (PRN) for multi- modal understanding of step-by-step instructions. Our proposed model is based on the successful BiDAF framework but also equipped with an ex- plicit memory unit that provides an implicit mecha-",
            "nism to keep track of the changes in the states of the entities over the course of the procedure. Our experimental analysis on visual reasoning tasks in the RecipeQA dataset shows that the model signi\ufb01- cantly improves the results of the previous models, indicating that it better understands the procedural text and the accompanying images. Additionally, we carefully analyze our results and \ufb01nd that our approach learns meaningful dynamic representa- tions of entities without any entity-level supervi- sion. Although we achieve state-of-the-art results on RecipeQA, clearly there is still room for im- provement compared to human performance. We also believe that the PRN architecture will be of value to other visual and textual sequential reason- ing tasks. Acknowledgements We thank the anonymous reviewers and area chairs for their invaluable feedback. This work was sup- ported by TUBA GEBIP fellowship awarded to E. Erdem; and by the MMVC project via an Institu- tional Links grant (Project No. 217E054) under the Newton-Katip C\u00b8 elebi Fund partnership funded by the Scienti\ufb01c and Technological Research Council of Turkey (TUBITAK) and the British Council.",
            "217E054) under the Newton-Katip C\u00b8 elebi Fund partnership funded by the Scienti\ufb01c and Technological Research Council of Turkey (TUBITAK) and the British Council. We also thank NVIDIA Corporation for the donation of GPUs used in this research. References Trapit Bansal, Arvind Neelakantan, and Andrew Mc- Callum. 2017. RelNet: End-to-End Modeling of En- tities & Relations. In NeurIPS Workshop on Auto- mated Knowledge Base Construction (AKBC). Antoine Bosselut, Corin Ennis, Omer Levy, Ari Holtz- man, Dieter Fox, and Yejin Choi. 2018. Simulat- ing Action Dynamics with Neural Process Networks. In Proceedings of the International Conference on Learning Representations (ICLR). Danqi Chen, Jason Bolton, and Christopher D Man- ning. 2016. A Thorough examination of the CNN\/Daily Mail Reading Comprehension Task. In Proceedings of the Annual Meeting of the Associ- ation for Computational Linguistics (ACL), pages 2358\u20132367.",
            "Danqi Chen, Jason Bolton, and Christopher D Man- ning. 2016. A Thorough examination of the CNN\/Daily Mail Reading Comprehension Task. In Proceedings of the Annual Meeting of the Associ- ation for Computational Linguistics (ACL), pages 2358\u20132367. Ronan Collobert, Jason Weston, L\u00b4eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research, 12:2493\u20132537. Bhavana Dalvi, Lifu Huang, Niket Tandon, Wen-tau Yih, and Peter Clark. 2018. Tracking state changes in procedural text: a challenge dataset and models for process paragraph comprehension. In Proceed- ings of the Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies. Rajarshi Das, Tsendsuren Munkhdalai, Xingdi Yuan, Adam Trischler, and Andrew McCallum. 2019.",
            "In Proceed- ings of the Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies. Rajarshi Das, Tsendsuren Munkhdalai, Xingdi Yuan, Adam Trischler, and Andrew McCallum. 2019. Building Dynamic Knowledge Graphs from Text us- ing Machine Reading Comprehension. In Proceed- ings of the International Conference on Learning Representations (ICLR). Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 248\u2013255. Bhuwan Dhingra, Qiao Jin, Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. 2018. Neural Models for Reasoning over Multiple Mentions using Coreference. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies (NAACL-HLT).",
            "2018. Neural Models for Reasoning over Multiple Mentions using Coreference. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies (NAACL-HLT). Xinya Du, Bhavana Dalvi Mishra, Niket Tandon, An- toine Bosselut, Wen-tau Yih, Peter Clark, and Claire Cardie. 2019. Be consistent! improving procedural text comprehension using label consistency. In Pro- ceedings of the Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies (NAACL- HLT). Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Pe- ters, Michael Schmitz, and Luke Zettlemoyer. 2018. AllenNLP: A deep semantic natural language pro- cessing platform. In Proceedings of Workshop for NLP Open Source Software (NLP-OSS), pages 1\u2013 6, Melbourne, Australia. Association for Computa- tional Linguistics.",
            "2018. AllenNLP: A deep semantic natural language pro- cessing platform. In Proceedings of Workshop for NLP Open Source Software (NLP-OSS), pages 1\u2013 6, Melbourne, Australia. Association for Computa- tional Linguistics. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Rearning for Image Recognition. In Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition, pages 770\u2013778. Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, and Yann LeCun. 2017. Tracking The World State with Recurrent Entity Networks. In Pro- ceedings of the International Conference on Learn- ing Representations (ICLR). Karl Moritz Hermann, Tomas Kocisky, Edward Grefen- stette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching Machines to Read and Comprehend.",
            "Karl Moritz Hermann, Tomas Kocisky, Edward Grefen- stette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching Machines to Read and Comprehend. In Proceedings of the Ad- vances in Neural Information Processing Systems (NeurIPS), pages 1693\u20131701. Schmidhuber J. Hochreiter, S. 1997. Long Short-Term Memory. Neural computation, 9(8):1735\u20131780.",
            "Mohit Iyyer, Varun Manjunatha, Anupam Guha, Yoga- rshi Vyas, Jordan Boyd-Graber, Hal Daum\u00b4e III, and Larry Davis. 2017. The amazing mysteries of the gutter: Drawing inferences between panels in comic book narratives. In Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR). Yangfeng Ji, Chenhao Tan, Sebastian Martschat, Yejin Choi, and Noah A Smith. 2017. Dynamic Entity Representations in Neural Language Models. In Proceedings of the Conference on Empirical Meth- ods in Natural Language Processing (EMNLP). Robin Jia and Percy Liang. 2017. Adversarial Ex- amples for Evaluating Reading Comprehension Sys- tems. In Proceedings of the Conference on Em- pirical Methods in Natural Language Processing (EMNLP). Samira Ebrahimi Kahou, Adam Atkinson, Vincent Michalski, Akos Kadar, Adam Trischler, and Yoshua Bengio. 2017. FigureQA: An Annotated Figure Dataset for Visual Reasoning.",
            "Samira Ebrahimi Kahou, Adam Atkinson, Vincent Michalski, Akos Kadar, Adam Trischler, and Yoshua Bengio. 2017. FigureQA: An Annotated Figure Dataset for Visual Reasoning. In Proceedings of the International Conference on Learning Represen- tations Workshop (ICLR Workshop). Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Ha- jishirzi. 2017. Are You Smarter Than A Sixth Grader? Textbook Question Answering for Multi- modal Machine Comprehension. In Proceedings of the IEEE Conference on Computer Vision and Pat- tern Recognition (CVPR). Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classi\ufb01cation. In Proceedings of the Con- ference on Empirical Methods in Natural Language Processing (EMNLP). Javier Marin, Aritro Biswas, Ferda O\ufb02i, Nicholas Hynes, Amaia Salvador, Yusuf Aytar, Ingmar We- ber, and Antonio Torralba.",
            "Javier Marin, Aritro Biswas, Ferda O\ufb02i, Nicholas Hynes, Amaia Salvador, Yusuf Aytar, Ingmar We- ber, and Antonio Torralba. 2018. Recipe1M: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images. arXiv preprint arXiv:1810.06553. Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2018. The natural language de- cathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic Differentiation in pytorch. In NIPS-W. Juan Pavez, Hector Allende, and Hector Allende-Cid. 2018. Working memory networks: Augmenting memory networks with a relational reasoning mod- ule.",
            "2017. Automatic Differentiation in pytorch. In NIPS-W. Juan Pavez, Hector Allende, and Hector Allende-Cid. 2018. Working memory networks: Augmenting memory networks with a relational reasoning mod- ule. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 1000\u20131009. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global Vectors for Word Representation. In Proceedings of the Conference on Empirical Methods in Natural Language Process- ing (EMNLP), pages 1532\u20131543. Julien Perez and Fei Liu. 2017. Dialog state tracking, a machine reading approach using memory network. In Proceedings of the 15th Conference of the Euro- pean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 305\u2013314. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations.",
            "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies (NAACL-HLT), pages 2227\u20132237. Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, and Tim- othy Lillicrap. 2018. Relational Recurrent Neural Networks. In Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), pages 7299\u20137310. M. Schuster and K. K. Paliwal. 1997. Bidirectional re- current neural networks. IEEE Transactions on Sig- nal Processing, 45(11):2673\u20132681. Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017a.",
            "Bidirectional re- current neural networks. IEEE Transactions on Sig- nal Processing, 45(11):2673\u20132681. Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017a. Bidirectional Atten- tion Flow for Machine Comprehension. In Proceed- ings of the International Conference on Learning Representations (ICLR). Minjoon Seo, Sewon Min, Ali Farhadi, and Hannaneh Hajishirzi. 2017b. Query-Reduction Networks for Question Answering. In Proceedings of the Inter- national Conference on Learning Representations (ICLR). R. K. Srivastava, K. Greff, and J. Schmidhuber. 2015. Highway networks. In Proceedings of the Interna- tional Conference on Machine Learning (ICML). Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. 2015. End-To-End Memory Networks.",
            "2015. Highway networks. In Proceedings of the Interna- tional Conference on Machine Learning (ICML). Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. 2015. End-To-End Memory Networks. In Proceed- ings of the Advances in Neural Information Process- ing Systems (NeurIPS), pages 2440\u20132448. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. 2016. Rethink- ing the Inception Architecture for Computer Vision. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), pages 2818\u20132826. Niket Tandon, Bhavana Dalvi, Joel Grus, Wen-tau Yih, Antoine Bosselut, and Peter Clark. 2018. Reasoning about actions and state changes by injecting com- monsense knowledge. In Proceedings of the Con- ference on Empirical Methods in Natural Language Processing (EMNLP).",
            "Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. 2016. MovieQA: Understanding Stories in Movies Through Question-Answering. In Proceedings of the IEEE Conference on Computer Vision and Pat- tern Recognition (CVPR), pages 4631\u20134640. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), pages 5998\u20136008. Hai Wang, Takeshi Onishi, Kevin Gimpel, and David McAllester. 2017. Emergent predication structure in hidden state vectors of neural readers. In Proceed- ings of the 2nd Workshop on Representation Learn- ing for NLP, pages 26\u201336, Vancouver, Canada. As- sociation for Computational Linguistics.",
            "2017. Emergent predication structure in hidden state vectors of neural readers. In Proceed- ings of the 2nd Workshop on Representation Learn- ing for NLP, pages 26\u201336, Vancouver, Canada. As- sociation for Computational Linguistics. Jason Weston, Antoine Bordes, Sumit Chopra, Alexan- der M Rush, Bart van Merri\u00a8enboer, Armand Joulin, and Tomas Mikolov. 2016. Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks. In Proceedings of the International Confer- ence on Learning Representations (ICLR). Jason Weston, Sumit Chopra, and Antoine Bordes. 2015. Memory Networks. In Proceedings of the International Conference on Learning Representa- tions (ICLR). Semih Yagcioglu, Aykut Erdem, Erkut Erdem, and Na- zli Ikizler-Cinbis. 2018. RecipeQA: A Challenge Dataset for Multimodal Comprehension of Cook- ing Recipes. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).",
            "2018. RecipeQA: A Challenge Dataset for Multimodal Comprehension of Cook- ing Recipes. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). Yummly. 2015. Kaggle Whats Cooking? https: \/\/www.kaggle.com\/c\/whats-cooking\/data. [Accessed: 2018-05-31]."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1909.08859.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 13479.999816894531,
    "avg_doclen_est": 184.65753173828125
}
