{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Adversarial Learning for Chinese NER from Crowd Annotations\u2217 Yaosheng Yang1, Meishan Zhang4, Wenliang Chen1 Wei Zhang2, Haofen Wang3, Min Zhang1 1School of Computer Science and Technology, Soochow University, China 2Alibaba Group and 3Shenzhen Gowild Robotics Co. Ltd 4School of Computer Science and Technology, Heilongjiang University, China 1ysyang@stu.suda.edu.cn, {wlchen, minzhang}@suda.edu.cn 4mason.zms@gmail.com, 2lantu.zw@alibaba-inc.com, 3wang haofen@gowild.cn Abstract To quickly obtain new labeled data, we can choose crowd- sourcing as an alternative way at lower cost in a short time. But as an exchange, crowd annotations from non-experts may be of lower quality than those from experts. In this paper, we propose an approach to performing crowd annotation learning for Chinese Named Entity Recognition (NER) to make full use of the noisy sequence labels from multiple annotators.",
            "But as an exchange, crowd annotations from non-experts may be of lower quality than those from experts. In this paper, we propose an approach to performing crowd annotation learning for Chinese Named Entity Recognition (NER) to make full use of the noisy sequence labels from multiple annotators. In- spired by adversarial learning, our approach uses a common Bi-LSTM and a private Bi-LSTM for representing annotator- generic and -speci\ufb01c information. The annotator-generic in- formation is the common knowledge for entities easily mas- tered by the crowd. Finally, we build our Chinese NE tagger based on the LSTM-CRF model. In our experiments, we cre- ate two data sets for Chinese NER tasks from two domains. The experimental results show that our system achieves better scores than strong baseline systems. Introduction There has been signi\ufb01cant progress on Named Entity Recog- nition (NER) in recent years using models based on machine learning algorithms (Zhao and Kit 2008; Collobert et al. 2011; Lample et al. 2016).",
            "Introduction There has been signi\ufb01cant progress on Named Entity Recog- nition (NER) in recent years using models based on machine learning algorithms (Zhao and Kit 2008; Collobert et al. 2011; Lample et al. 2016). As with other Natural Language Processing (NLP) tasks, building NER systems typically re- quires a massive amount of labeled training data which are annotated by experts. In real applications, we often need to consider new types of entities in new domains where we do not have existing annotated data. For such new types of en- tities, however, it is very hard to \ufb01nd experts to annotate the data within short time limits and hiring experts is costly and non-scalable, both in terms of time and money. In order to quickly obtain new training data, we can use crowdsourcing as one alternative way at lower cost in a short time. But as an exchange, crowd annotations from non- experts may be of lower quality than those from experts. It is one biggest challenge to build a powerful NER system on such a low quality annotated data.",
            "But as an exchange, crowd annotations from non- experts may be of lower quality than those from experts. It is one biggest challenge to build a powerful NER system on such a low quality annotated data. Although we can obtain high quality annotations for each input sentence by majority voting, it can be a waste of human labors to achieve such a goal, especially for some ambiguous sentences which may require a number of annotations to reach an agreement. Thus \u2217The corresponding author is Wenliang Chen. Copyright c\u20dd2018, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved. majority work directly build models on crowd annotations, trying to model the differences among annotators, for exam- ple, some of the annotators may be more trustful (Rodrigues, Pereira, and Ribeiro 2014; Nguyen et al. 2017). Here we focus mainly on the Chinese NER, which is more dif\ufb01cult than NER for other languages such as English for the lack of morphological variations such as capitalization and in particular the uncertainty in word segmentation.",
            "2017). Here we focus mainly on the Chinese NER, which is more dif\ufb01cult than NER for other languages such as English for the lack of morphological variations such as capitalization and in particular the uncertainty in word segmentation. The Chinese NE taggers trained on news domain often perform poor in other domains. Although we can alleviate the prob- lem by using character-level tagging to resolve the problem of poor word segmentation performances (Peng and Dredze 2015), still there exists a large gap when the target domain changes, especially for the texts of social media. Thus, in order to get a good tagger for new domains and also for the conditions of new entity types, we require large amounts of labeled data. Therefore, crowdsourcing is a reasonable solu- tion for these situations. In this paper, we propose an approach to training a Chi- nese NER system on the crowd-annotated data. Our goal is to extract additional annotator independent features by ad- versarial training, alleviating the annotation noises of non- experts.",
            "In this paper, we propose an approach to training a Chi- nese NER system on the crowd-annotated data. Our goal is to extract additional annotator independent features by ad- versarial training, alleviating the annotation noises of non- experts. The idea of adversarial training in neural networks has been used successfully in several NLP tasks, such as cross-lingual POS tagging (Kim et al. 2017) and cross- domain POS tagging (Gui et al. 2017). They use it to reduce the negative in\ufb02uences of the input divergences among dif- ferent domains or languages, while we use adversarial train- ing to reduce the negative in\ufb02uences brought by different crowd annotators. To our best knowledge, we are the \ufb01rst to apply adversarial training for crowd annotation learning. In the learning framework, we perform adversarial train- ing between the basic NER and an additional worker dis- criminator. We have a common Bi-LSTM for representing annotator-generic information and a private Bi-LSTM for representing annotator-speci\ufb01c information.",
            "In the learning framework, we perform adversarial train- ing between the basic NER and an additional worker dis- criminator. We have a common Bi-LSTM for representing annotator-generic information and a private Bi-LSTM for representing annotator-speci\ufb01c information. We build an- other label Bi-LSTM by the crowd-annotated NE label se- quence which re\ufb02ects the mind of the crowd annotators who learn entity de\ufb01nitions by reading the annotation guidebook. The common and private Bi-LSTMs are used for NER, while the common and label Bi-LSTMs are used as inputs for the worker discriminator. The parameters of the com- mon Bi-LSTM are learned by adversarial training, maximiz- ing the worker discriminator loss and meanwhile minimiz- arXiv:1801.05147v1  [cs.CL]  16 Jan 2018",
            "ing the NER loss. Thus the resulting features of the common Bi-LSTM are worker invariant and NER sensitive. For evaluation, we create two Chinese NER datasets in two domains: dialog and e-commerce. We require the crowd annotators to label the types of entities, including person, song, brand, product, and so on. Identifying these entities is useful for chatbot and e-commerce platforms (Kl\u00a8uwer 2011). Then we conduct experiments on the newly created datasets to verify the effectiveness of the proposed adversar- ial neural network model. The results show that our system outperforms very strong baseline systems. In summary, we make the following contributions: \u2022 We propose a crowd-annotation learning model based on adversarial neural networks. The model uses labeled data created by non-experts to train a NER classi\ufb01er and simul- taneously learns the common and private features among the non-expert annotators. \u2022 We create two data sets in dialog and e-commerce do- mains by crowd annotations. The experimental results show that the proposed approach performs the best among all the comparison systems.",
            "\u2022 We create two data sets in dialog and e-commerce do- mains by crowd annotations. The experimental results show that the proposed approach performs the best among all the comparison systems. Related Work Our work is related to three lines of research: Sequence la- beling, Adversarial training, and Crowdsourcing. Sequence labeling. NER is widely treated as a sequence la- beling problem, by assigning a unique label over each sen- tential word (Ratinov and Roth 2009). Early studies on se- quence labeling often use the models of HMM, MEMM, and CRF (Lafferty et al. 2001) based on manually-crafted discrete features, which can suffer the feature sparsity prob- lem and require heavy feature engineering. Recently, neural network models have been successfully applied to sequence labeling (Collobert et al. 2011; Huang, Xu, and Yu 2015; Lample et al. 2016). Among these work, the model which uses Bi-LSTM for feature extraction and CRF for decoding has achieved state-of-the-art performances (Huang, Xu, and Yu 2015; Lample et al.",
            "2016). Among these work, the model which uses Bi-LSTM for feature extraction and CRF for decoding has achieved state-of-the-art performances (Huang, Xu, and Yu 2015; Lample et al. 2016), which is exploited as the base- line model in our work. Adversarial Training. Adversarial Networks have achieved great success in computer vision such as image genera- tion (Denton et al. 2015; Ganin et al. 2016). In the NLP community, the method is mainly exploited under the set- tings of domain adaption (Zhang, Barzilay, and Jaakkola 2017; Gui et al. 2017), cross-lingual (Chen et al. 2016; Kim et al. 2017) and multi-task learning (Chen et al. 2017; Liu, Qiu, and Huang 2017). All these settings involve the feature divergences between the training and test examples, and aim to learn invariant features across the divergences by an additional adversarial discriminator, such as domain dis- criminator.",
            "2017; Liu, Qiu, and Huang 2017). All these settings involve the feature divergences between the training and test examples, and aim to learn invariant features across the divergences by an additional adversarial discriminator, such as domain dis- criminator. Our work is similar to these work but is applies on crowdsourcing learning, aiming to \ufb01nd invariant features among different crowdsourcing workers. Crowdsourcing. Most NLP tasks require a massive amount of labeled training data which are annotated by experts. However, hiring experts is costly and non-scalable, both in terms of time and money. Instead, crowdsourcing is another solution to obtain labeled data at a lower cost but with rela- tive lower quality than those from experts. Snow et al. (2008) collected labeled results for several NLP tasks from Amazon Mechanical Turk and demonstrated that non-experts annota- tions were quite useful for training new systems. In recent years, a series of work have focused on how to use crowd- sourcing data ef\ufb01ciently in tasks such as classi\ufb01cation (Felt et al. 2015; Bi et al.",
            "In recent years, a series of work have focused on how to use crowd- sourcing data ef\ufb01ciently in tasks such as classi\ufb01cation (Felt et al. 2015; Bi et al. 2014), and compare quality of crowd and expert labels (Dumitrache, Aroyo, and Welty 2017). In sequence labeling tasks, Dredze, Talukdar, and Cram- mer (2009) viewed this task as a multi-label problem while Rodrigues, Pereira, and Ribeiro (2014) took workers iden- tities into account by assuming that each sentential word was tagged correctly by one of the crowdsourcing workers and proposed a CRF-based model with multiple annotators. Nguyen et al. (2017) introduced a crowd representation in which the crowd vectors were added into the LSTM-CRF model at train time, but ignored them at test time. In this paper, we apply adversarial training on crowd annotations on Chinese NER in new domains, and achieve better perfor- mances than previous studies on crowdsourcing learning.",
            "In this paper, we apply adversarial training on crowd annotations on Chinese NER in new domains, and achieve better perfor- mances than previous studies on crowdsourcing learning. Baseline: LSTM-CRF We use a neural CRF model as the baseline system (Ratinov and Roth 2009), treating NER as a sequence labeling prob- lem over Chinese characters, which has achieved state-of- the-art performances (Peng and Dredze 2015). To this end, we explore the BIEO schema to convert NER into sequence labeling, following Lample et al. (2016), where sentential character is assigned with one unique tag. Concretely, we tag the non-entity character by label \u201cO\u201d, the beginning charac- ter of an entity by \u201cB-XX\u201d, the ending character of an entity by \u201cE-XX\u201d and the other character of an entity by \u201cI-XX\u201d, where \u201cXX\u201d denotes the entity type. We build high-level neural features from the input char- acter sequence by a bi-directional LSTM (Lample et al. 2016). The resulting features are combined and then are fed into an output CRF layer for decoding.",
            "We build high-level neural features from the input char- acter sequence by a bi-directional LSTM (Lample et al. 2016). The resulting features are combined and then are fed into an output CRF layer for decoding. In summary, the baseline model has three main components. First, we make vector representations for sentential characters x1x2 \u00b7 \u00b7 \u00b7 xn, transforming the discrete inputs into low-dimensional neu- ral inputs. Second, feature extraction is performed to obtain high-level features hner 1 hner 2 \u00b7 \u00b7 \u00b7 hner n , by using a bi-directional LSTM (Bi-LSTM) structure together with a linear trans- formation over x1x2 \u00b7 \u00b7 \u00b7 xn. Third, we apply a CRF tag- ging module over hner 1 hner 2 \u00b7 \u00b7 \u00b7 hner n , obtaining the \ufb01nal output NE labels. The overall framework of the baseline model is shown by the right part of Figure 1. Vector Representation of Characters To represent Chinese characters, we simply exploit a neu- ral embedding layer to map discrete characters into the low- dimensional vector representations.",
            "The overall framework of the baseline model is shown by the right part of Figure 1. Vector Representation of Characters To represent Chinese characters, we simply exploit a neu- ral embedding layer to map discrete characters into the low- dimensional vector representations. The goal is achieved by a looking-up table EW , which is a model parameter and will be \ufb01ne-tuned during training. The looking-up ta- ble can be initialized either by random or by using a pre- trained embeddings from large scale raw corpus. For a given Chinese character sequence c1c2 \u00b7 \u00b7 \u00b7 cn, we obtain the vec-",
            "x1 x2 ...... xn\u22121 xn hprivate 1 hprivate 2 ...... hprivate n\u22121 hprivate n hcommon n hcommon n\u22121 ...... hcommon 2 hcommon 1 hlabel n hlabel n\u22121 ...... hlabel 2 hlabel 1 x\u20321 x\u20322 ...... x\u2032n\u22121 x\u2032n ...... hner 2 hner 1 hner n\u22121 hner n L ...... oner 1 oner 2 oner n\u22121 oner n ...... L-PER B-PER O O w1 w2 ...... wn\u22121 wn \u00afy1 \u00afy2 ...... \u00afyn\u22121 \u00afyn ...... hworker 2 hworker 1 hworker n\u22121 hworker n L hworker oworker worker Bi-LSTM Bi-LSTM Bi-LSTM CNN Baseline Worker-Adversarial Figure 1: The framework of the proposed model, which consists of two parts. tor representation of each sentential character by: xt = look-up(ct, EW ), t \u2208[1, n].",
            "tor representation of each sentential character by: xt = look-up(ct, EW ), t \u2208[1, n]. Feature Extraction Based on the vector sequence x1x2 \u00b7 \u00b7 \u00b7 xn, we extract higher-level features hner 1 hner 2 \u00b7 \u00b7 \u00b7 hner n by using a bidirec- tional LSTM module and a simple feed-forward neural layer, which are then used for CRF tagging at the next step. LSTM is a type of recurrent neural network (RNN), which is designed for solving the exploding and dimin- ishing gradients of basic RNNs (Graves and Schmidhu- ber 2005). It has been widely used in a number of NLP tasks, including POS-tagging (Huang, Xu, and Yu 2015; Ma and Hovy 2016), parsing (Dyer et al. 2015) and machine translation (Wu et al. 2016), because of its strong capabili- ties of modeling natural language sentences.",
            "2015) and machine translation (Wu et al. 2016), because of its strong capabili- ties of modeling natural language sentences. By traversing x1x2 \u00b7 \u00b7 \u00b7 xn by order and reversely, we ob- tain the output features hprivate 1 hprivate 2 \u00b7 \u00b7 \u00b7 hprivate n of the bi- LSTM, where hprivate t = \u2212\u2192 h t \u2295\u2190\u2212 h t. Here we refer this Bi- LSTM as private in order to differentiate it with the com- mon Bi-LSTM over the same character inputs which will be introduced in the next section. Further we make an integration of the output vectors of bi-directional LSTM by a linear feed-forward neural layer, resulting in the features hner 1 hner 2 \u00b7 \u00b7 \u00b7 hner n by equation: hner t = Whprivate t + b, (1) where W and b are both model parameters. CRF Tagging Finally we feed the resulting features hner t , t \u2208[1, n] into a CRF layer directly for NER decoding. CRF tagging is one globally normalized model, aiming to \ufb01nd the best output sequence considering the dependencies between successive labels.",
            "CRF Tagging Finally we feed the resulting features hner t , t \u2208[1, n] into a CRF layer directly for NER decoding. CRF tagging is one globally normalized model, aiming to \ufb01nd the best output sequence considering the dependencies between successive labels. In the sequence labeling setting for NER, the output label of one position has a strong dependency on the label of the previous position. For example, the label before \u201cI-XX\u201d must be either \u201cB-XX\u201d or \u201cI-XX\u201d, where \u201cXX\u201d should be exactly the same. CRF involves two parts for prediction. First we should compute the scores for each label based hner t , resulting in oner t , whose dimension is the number of output labels. The other part is a transition matrix T which de\ufb01nes the scores of two successive labels. T is also a model parameter. Based on oner t and T, we use the Viterbi algorithm to \ufb01nd the best- scoring label sequence.",
            "The other part is a transition matrix T which de\ufb01nes the scores of two successive labels. T is also a model parameter. Based on oner t and T, we use the Viterbi algorithm to \ufb01nd the best- scoring label sequence. We can formalize the CRF tagging process as follows: oner t = Wnerhner t , t \u2208[1, n] score(X, y) = n X t=1 (ot,yt + Tyt\u22121,yt) yner = arg max y \u0000score(X, y)) \u0001 , (2) where score(\u00b7) is the scoring function for a given output la- bel sequence y = y1y2 \u00b7 \u00b7 \u00b7 yn based on input X, yner is the resulting label sequence, Wner is a model parameter. Training To train model parameters, we exploit a negative log- likelihood objective as the loss function.",
            "Training To train model parameters, we exploit a negative log- likelihood objective as the loss function. We apply softmax over all candidate output label sequences, thus the probabil- ity of the crowd-annotated label sequence is computed by: p(\u00afy|X) = exp \u0000score(X, \u00afy) \u0001 P y\u2208YX exp \u0000score(X, y) \u0001, (3) where \u00afy is the crowd-annotated label sequences and YX is all candidate label sequence of input X. Based on the above formula, the loss function of our base- line model is: loss(\u0398, X, \u00afy) = \u2212log p(\u00afy|X), (4) where \u0398 is the set of all model parameters. We use standard back-propagation method to minimize the loss function of the baseline CRF model.",
            "Worker Adversarial Adversarial learning has been an effective mechanism to re- solve the problem of the input features between the training and test examples having large divergences (Goodfellow et al. 2014; Ganin et al. 2016). It has been successfully applied on domain adaption (Gui et al. 2017), cross-lingual learn- ing (Chen et al. 2016) and multi-task learning (Liu, Qiu, and Huang 2017). All settings involve feature shifting between the training and testing. In this paper, our setting is different. We are using the annotations from non-experts, which are noise and can in- \ufb02uence the \ufb01nal performances if they are not properly pro- cessed. Directly learning based on the resulting corpus may adapt the neural feature extraction into the biased annota- tions. In this work, we assume that individual workers have their own guidelines in mind after short training. For exam- ple, a perfect worker can annotate highly consistently with an expert, while common crowdsourcing workers may be confused and have different understandings on certain con- texts.",
            "In this work, we assume that individual workers have their own guidelines in mind after short training. For exam- ple, a perfect worker can annotate highly consistently with an expert, while common crowdsourcing workers may be confused and have different understandings on certain con- texts. Based on the assumption, we make an adaption for the original adversarial neural network to our setting. Our adaption is very simple. Brie\ufb02y speaking, the original adversarial learning adds an additional discriminator to clas- sify the type of source inputs, for example, the domain cate- gory in the domain adaption setting, while we add a discrim- inator to classify the annotation workers. Solely the features from the input sentence is not enough for worker classi\ufb01- cation. The annotation result of the worker is also required. Thus the inputs of our discriminator are different. Here we exploit both the source sentences and the crowd-annotated NE labels as basic inputs for the worker discrimination. In the following, we describe the proposed adversarial learning module, including both the submodels and the train- ing method.",
            "Thus the inputs of our discriminator are different. Here we exploit both the source sentences and the crowd-annotated NE labels as basic inputs for the worker discrimination. In the following, we describe the proposed adversarial learning module, including both the submodels and the train- ing method. As shown by the left part of Figure 1, the submodel consists of four parts: (1) a common Bi-LSTM over input characters; (2) an additional Bi-LSTM to en- code crowd-annotated NE label sequence; (3) a convolu- tional neural network (CNN) to extract features for worker discriminator; (4) output and prediction. Common Bi-LSTM over Characters To build the adversarial part, \ufb01rst we create a new bi- directional LSTM, named by the common Bi-LSTM: hcommon 1 hcommon 2 \u00b7 \u00b7 \u00b7 hcommon n = Bi-LSTM(x1x2 \u00b7 \u00b7 \u00b7 xn). (5) As shown in Figure 1, this Bi-LSTM is constructed over the same input character representations of the private Bi- LSTM, in order to extract worker independent features.",
            "(5) As shown in Figure 1, this Bi-LSTM is constructed over the same input character representations of the private Bi- LSTM, in order to extract worker independent features. The resulting features of the common Bi-LSTM are used for both NER and the worker discriminator, different with the features of private Bi-LSTM which are used for NER only. As shown in Figure 1, we concatenate the outputs of the common and private Bi-LSTMs together, and then feed the results into the feed-forward combination layer of the NER part. Thus Formula 1 can be rewritten as: hner t = W(hcommon t \u2295hprivate t ) + b, (6) where W is wider than the original combination because the newly-added hcommon t . Noticeably, although the resulting common features are used for the worker discriminator, they actually have no ca- pability to distinguish the workers. Because this part is ex- ploited to maximize the loss of the worker discriminator, it will be interpreted in the later training subsection. These fea- tures are invariant among different workers, thus they can have less noises for NER.",
            "Because this part is ex- ploited to maximize the loss of the worker discriminator, it will be interpreted in the later training subsection. These fea- tures are invariant among different workers, thus they can have less noises for NER. This is the goal of adversarial learning, and we hope the NER being able to \ufb01nd useful features from these worker independent features. Additional Bi-LSTM over Annotated NER Labels In order to incorporate the annotated NE labels to predict the exact worker, we build another bi-directional LSTM (named by label Bi-LSTM) based on the crowd-annotated NE label sequence. This Bi-LSTM is used for worker discriminator only. During the decoding of the testing phase, we will never have this Bi-LSTM, because the worker discriminator is no longer required. Assuming the crowd-annotated NE label sequence an- notated by one worker is \u00afy = \u00afy1\u00afy2 \u00b7 \u00b7 \u00b7 \u00afyn, we exploit a looking-up table EL to obtain the corresponding sequence of their vector representations x\u20321x\u20322 \u00b7 \u00b7 \u00b7 x\u2032n, similar to the method that maps characters into their neural representa- tions.",
            "Concretely, for one NE label \u00afyt (t \u2208[1, n]), we obtain its neural vector by: x\u2032t = look-up(\u00afyt, EL). Next step we apply bi-directional LSTM over the se- quence x\u20321x\u20322 \u00b7 \u00b7 \u00b7 x\u2032n, which can be formalized as: hlabel 1 hlabel 2 \u00b7 \u00b7 \u00b7 hlabel n = Bi-LSTM(x\u2032 1x\u2032 2 \u00b7 \u00b7 \u00b7 x\u2032 n). (7) The resulting feature sequence is concatenated with the out- puts of the common Bi-LSTM, and further be used for worker classi\ufb01cation. CNN Following, we add a convolutional neural network (CNN) module based on the concatenated outputs of the common Bi-LSTM and the label Bi-LSTM, to produce the \ufb01nal fea- tures for worker discriminator. A convolutional operator with window size 5 is used, and then max pooling strategy is applied over the convolution sequence to obtain the \ufb01nal \ufb01xed-dimensional feature vector.",
            "A convolutional operator with window size 5 is used, and then max pooling strategy is applied over the convolution sequence to obtain the \ufb01nal \ufb01xed-dimensional feature vector. The whole process can be described by the following equations: hworker t = hcommon t \u2295hlabel t \u02dchworker t = tanh(Wcnn[hworker t\u22122 , hworker t\u22121 , \u00b7 \u00b7 \u00b7 , hworker t+2 ]) hworker = max-pooling(\u02dchworker 1 \u02dchworker 2 \u00b7 \u00b7 \u00b7 \u02dchworker n ) (8) where t \u2208[1, n] and Wcnn is one model parameter. We ex- ploit zero vector to paddle the out-of-index vectors. Output and Prediction After obtaining the \ufb01nal feature vector for the worker dis- criminator, we use it to compute the output vector, which scores all the annotation workers. The score function is de- \ufb01ned by: oworker = Wworkerhworker, (9) where Wworker is one model parameter and the output di- mension equals the number of total non-expert annotators.",
            "The prediction is to \ufb01nd the worker which is responsible for this annotation. Adversarial Training The training objective with adversarial neural network is different from the baseline model, as it includes the ex- tra worker discriminator. Thus the new objective includes two parts, one being the negative log-likelihood from NER which is the same as the baseline, and the other being the negative the log-likelihood from the worker discriminator. In order to obtain the negative log-likelihood of the worker discriminator, we use softmax to compute the prob- ability of the actual worker \u00afz as well, which is de\ufb01ned by: p(\u00afz|X, \u00afy) = exp(oworker \u00afz ) P z exp(oworker z ), (10) where z should enumerate all workers.",
            "Based on the above de\ufb01nition of probability, our new ob- jective is de\ufb01ned as follows: R(\u0398, \u0398\u2032, X, \u00afy, \u00afz) = loss(\u0398, X, \u00afy) \u2212loss(\u0398, \u0398\u2032, X) = \u2212log p(\u00afy|X) + log p(\u00afz|X, \u00afy), (11) where \u0398 is the set of all model parameters related to NER, and \u0398\u2032 is the set of the remaining parameters which are only related to the worker discriminator, X, \u00afy and \u00afz are the in- put sentence, the crowd-annotated NE labels and the cor- responding annotator for this annotation, respectively. It is worth noting that the parameters of the common Bi-LSTM are included in the set of \u0398 by de\ufb01nition. In particular, our goal is not to simply minimize the new objective.",
            "It is worth noting that the parameters of the common Bi-LSTM are included in the set of \u0398 by de\ufb01nition. In particular, our goal is not to simply minimize the new objective. Actually, we aim for a saddle point, \ufb01nding the parameters \u0398 and \u0398\u2032 satisfying the following conditions: \u02c6\u0398 = arg min \u0398 R(\u0398, \u0398\u2032, X, \u00afy, \u00afz) \u02c6\u0398\u2032 = arg max \u0398\u2032 R(\u02c6\u0398, \u0398\u2032, X, \u00afy, \u00afz) (12) where the \ufb01rst equation aims to \ufb01nd one \u0398 that minimizes our new objective R(\u00b7), and the second equation aims to \ufb01nd one \u0398\u2032 maximizing the same objective. Intuitively, the \ufb01rst equation of Formula 12 tries to min- imize the NER loss, but at the same time maximize the worker discriminator loss by the shared parameters of the common Bi-LSTM.",
            "Intuitively, the \ufb01rst equation of Formula 12 tries to min- imize the NER loss, but at the same time maximize the worker discriminator loss by the shared parameters of the common Bi-LSTM. Thus the resulting features of common Bi-LSTM actually attempt to hurt the worker discrimina- tor, which makes these features worker independent since they are unable to distinguish different workers. The second equation tries to minimize the worker discriminator loss by its own parameter \u0398\u2032. We use the standard back-propagation method to train the model parameters, the same as the baseline model. In order to incorporate the term of the argmax part of Formula 12 , we follow the previous work of adversarial training (Ganin et al. 2016; Chen et al. 2016; Liu, Qiu, and Huang 2017), by introducing a gradient reverse layer between the com- mon Bi-LSTM and the CNN module, whose forward does nothing but the backward simply negates the gradients.",
            "2016; Chen et al. 2016; Liu, Qiu, and Huang 2017), by introducing a gradient reverse layer between the com- mon Bi-LSTM and the CNN module, whose forward does nothing but the backward simply negates the gradients. #Sent AvgLen Kappa DL-PS 16,948 9.21 0.6033 UC-MT 2,337 34.97 0.7437 UC-UQ 2,300 7.69 0.7529 Table 1: Statistics of labeled datasets. Experiments Data Sets With the purpose of obtaining evaluation datasets from crowd annotators, we collect the sentences from two do- mains: Dialog and E-commerce domain. We hire undergrad- uate students to annotate the sentences. They are required to identify the prede\ufb01ned types of entities in the sentences. To- gether with the guideline document, the annotators are edu- cated some tips in \ufb01fteen minutes and also provided with 20 exemplifying sentences. Labeled Data: DL-PS. In Dialog domain (DL), we collect raw sentences from a chatbot application.",
            "To- gether with the guideline document, the annotators are edu- cated some tips in \ufb01fteen minutes and also provided with 20 exemplifying sentences. Labeled Data: DL-PS. In Dialog domain (DL), we collect raw sentences from a chatbot application. And then we ran- domly select 20K sentences as our pool and hire 43 students to annotate the sentences. We ask the annotators to label two types of entities: Person-Name and Song-Name. The anno- tators label the sentences independently. In particular, each sentence is assigned to three annotators for this data. Al- though the setting can be wasteful of labor, we can use the resulting dataset to test several well-known baselines such as majority voting. After annotation, we remove some illegal sentences re- ported by the annotators. Finally, we have 16,948 sentences annotated by the students. Table 1 shows the information of annotated data. The average Kappa value among the anno- tators is 0.6033, indicating that the crowd annotators have moderate agreement on identifying entities on this data.",
            "Finally, we have 16,948 sentences annotated by the students. Table 1 shows the information of annotated data. The average Kappa value among the anno- tators is 0.6033, indicating that the crowd annotators have moderate agreement on identifying entities on this data. In order to evaluate the system performances, we create a set of corpus with gold annotations. Concretely, we ran- domly select 1,000 sentences from the \ufb01nal dataset and let two experts generate the gold annotations. Among them, we use 300 sentences as the development set and the remain- ing 700 as the test set. The rest sentences with only student annotations are used as the training set. Labeled data: EC-MT and EC-UQ. In E-commerce do- main (EC), we collect raw sentences from two types of texts: one is titles of merchandise entries (EC-MT) and another is user queries (EC-UQ). The annotators label \ufb01ve types of entities: Brand, Product, Model, Material, and Speci\ufb01ca- tion.",
            "The annotators label \ufb01ve types of entities: Brand, Product, Model, Material, and Speci\ufb01ca- tion. These \ufb01ve types of entities are very important for E- commerce platform, for example building knowledge graph of merchandises. Five students participate the annotations for this domain since the number of sentences is small. We use the similar strategy as DL-PS to annotate the sentences, except that only two annotators are assigned for each sen- tence, because we aim to test the system performances under very small duplicated annotations. Finally, we obtain 2,337 sentences for EC-MT and 2,300 for EC-UQ. Table 1 shows the information of annotated results. Similarly, we produce the development and test datasets for system evaluation, by randomly selecting 400 sentences and letting two experts to generate the groundtruth",
            "annotations. Among them, we use 100 sentences as the de- velopment set and the remaining 300 as the test set. The rest sentences with only crowdsourcing annotations are used as the training set. Unlabeled data. The vector representations of characters are basic inputs of our baseline and proposed models, which are obtained by the looking-up table EW . As introduced be- fore, we can use pretrained embeddings from large-scale raw corpus to initialize the table. In order to pretrain the character embeddings, we use one large-scale unlabeled data from the user-generated content in Internet. Totally, we ob- tain a number of 5M sentences. Finally, we use the tool word2vec1 to pretrain the character embeddings based on the unlabeled dataset in our experiments. Settings For evaluation, we use the entity-level metrics of Precision (P), Recall (R), and their F1 value in our experiments, treat- ing one tagged entity as correct only when it matches the gold entity exactly. There are several hyper-parameters in the baseline LSTM- CRF and our \ufb01nal models. We set them empirically by the development performances.",
            "There are several hyper-parameters in the baseline LSTM- CRF and our \ufb01nal models. We set them empirically by the development performances. Concretely, we set the dimen- sion size of the character embeddings by 100, the dimension size of the NE label embeddings by 50, and the dimension sizes of all the other hidden features by 200. We exploit online training with a mini-batch size 128 to learn model parameters. The max-epoch iteration is set by 200, and the best-epoch model is chosen according to the de- velopment performances. We use RMSprop (Tieleman and Hinton 2012) with a learning rate 10\u22123 to update model pa- rameters, and use l2-regularization by a parameter 10\u22125. We adopt the dropout technique to avoid over\ufb01tting by a drop value of 0.2. Comparison Systems The proposed approach (henceforward referred to as AL- Crowd) is compared with the following systems: \u2022 CRF: We use the Crfsuite2 tool to train a model on the crowdsourcing labeled data.",
            "Comparison Systems The proposed approach (henceforward referred to as AL- Crowd) is compared with the following systems: \u2022 CRF: We use the Crfsuite2 tool to train a model on the crowdsourcing labeled data. As for the feature settings, we use the supervised version of Zhao and Kit (2008). \u2022 CRF-VT: We use the same settings of the CRF system, except that the training data is the voted version, whose groundtruths are produced by majority voting at the char- acter level for each annotated sentence. \u2022 CRF-MA: The CRF model proposed by Rodrigues, Pereira, and Ribeiro (2014), which uses a prior distributa- tion to model multiple crowdsourcing annotators. We use the source code provided by the authors. \u2022 LSTM-CRF: Our baseline system trained on the crowd- sourcing labeled data. \u2022 LSTM-CRF-VT: Our baseline system trained on the voted corpus, which is the same as CRF-VT.",
            "We use the source code provided by the authors. \u2022 LSTM-CRF: Our baseline system trained on the crowd- sourcing labeled data. \u2022 LSTM-CRF-VT: Our baseline system trained on the voted corpus, which is the same as CRF-VT. 1https:\/\/code.google.com\/archive\/p\/word2vec 2http:\/\/www.chokkan.org\/software\/crfsuite\/ Model P R F1 CRF 89.48 70.38 78.79 CRF-VT 85.16 65.07 73.77 CRF-MA 72.83 90.79 80.82 LSTM-CRF 90.50 79.97 84.91 LSTM-CRF-VT 88.68 75.51 81.57 LSTM-Crowd 86.40 83.43 84.89 ALCrowd 89.56 82.70 85.99 Table 2: Main results on the DL-PS data.",
            "Model Data: EC-MT P R F1 CRF 75.12 66.67 70.64 LSTM-CRF 75.02 72.84 73.91 LSTM-Crowd 73.81 75.18 74.49 ALCrowd 76.33 74.00 75.15 Data: EC-UQ CRF 65.45 55.33 59.96 LSTM-CRF 71.96 66.55 69.15 LSTM-Crowd 67.51 71.10 69.26 ALCrowd 74.72 68.60 71.53 Table 3: Main results on the EC-MT and EC-UQ datasets. \u2022 LSTM-Crowd: The LSTM-CRF model with crowd anno- tation learning proposed by Nguyen et al. (2017). We use the source code provided by the authors. The \ufb01rst three systems are based on the CRF model using traditional handcrafted features, and the last three systems are based on the neural LSTM-CRF model.",
            "(2017). We use the source code provided by the authors. The \ufb01rst three systems are based on the CRF model using traditional handcrafted features, and the last three systems are based on the neural LSTM-CRF model. Among them, CRF-MA, LSTM-Crowd and our system with adversarial learning (ALCrowd) are based on crowd annotation learning that directly trains the model on the crowd-annotations. Five systems, including CRF, CRF-MA, LSTM-CRF, LSTM- Crowd, and ALCrowd, are trained on the original version of labeled data, while CRF-VT and LSTM-CRF-VT are trained on the voted version. Since CRF-VT, CRF-MA and LSTM- CRF-VT all require ground-truth answers for each training sentence, which are dif\ufb01cult to be produced with only two annotations, we do not apply the three models on the two EC datasets. Main Results In this section, we show the model performances of our proposed crowdsourcing learning system (ALCrowd), and meanwhile compare it with the other systems mentioned above.",
            "Main Results In this section, we show the model performances of our proposed crowdsourcing learning system (ALCrowd), and meanwhile compare it with the other systems mentioned above. Table 2 shows the experimental results on the DL- PS datasets and Table 3 shows the experiment results on the EC-MT and EC-UQ datasets, respectively. The results of CRF and LSTM-CRF mean that the crowd annotation is an alternative solution with low cost for la- beling data that could be used for training a NER system even there are some inconsistencies. Compared with CRF, LSTM-CRF achieves much better performances on all the three data, showing +6.12 F1 improvement on DL-PS, +4.51 on EC-MT, and +9.19 on EC-UQ. This indicates that LSTM-",
            "Data:DL-PS Data:EC-MT Data:EC-UQ 60 65 70 75 80 85 Random Pretrained Figure 2: Comparisons by using different character embed- dings, where the Y-axis shows the F1 values CRF is a very strong baseline system, demonstrating the ef- fectiveness of neural network. Interestingly, when compared with CRF and LSTM-CRF, CRF-VT and LSTM-CRF-VT trained on the voted version perform worse in the DL-PS dataset. This trend is also men- tioned in Nguyen et al. (2017). This fact shows that the ma- jority voting method might be unsuitable for our task. There are two possible reasons accounting for the observation. On the one hand, simple character-level voting based on three annotations for each sentence may be still not enough. In the DL-PS dataset, even with only two prede\ufb01ned entity types, one character can have nine NE labels. Thus the majority- voting may be incapable of handling some cases. While the cost by adding more annotations for each sentence would be greatly increased.",
            "In the DL-PS dataset, even with only two prede\ufb01ned entity types, one character can have nine NE labels. Thus the majority- voting may be incapable of handling some cases. While the cost by adding more annotations for each sentence would be greatly increased. On the other hand, the lost informa- tion produced by majority-voting may be important, at least the ambiguous annotations denote that the input sentence is dif\ufb01cult for NER. The normal CRF and LSTM-CRF mod- els without discard any annotations can differentiate these dif\ufb01cult contexts through learning. Three crowd-annotation learning systems provide bet- ter performances than their counterpart systems, (CRF-MA VS CRF) and (LSTM-Crowd\/ALCrowd VS LSTM-CRF). Compared with the strong baseline LSTM-CRF, ALCrowd shows its advantage with +1.08 F1 improvements on DL-PS, +1.24 on EC-MT, and +2.38 on EC-UQ, respectively. This indicates that adding the crowd-annotation learning is quite useful for building NER systems.",
            "This indicates that adding the crowd-annotation learning is quite useful for building NER systems. In addition, ALCrowd also outperforms LSTM-Crowd on all the datasets consistently, demonstrating the high effectiveness of ALCrowd in extract- ing worker independent features. Among all the systems, ALCrowd performs the best, and signi\ufb01cantly better than all the other models (the p-value is below 10\u22125 by using t-test). The results indicate that with the help of adversarial training, our system can learn a better feature representation from crowd annotation. Discussion Impact of Character Embeddings. First, we investigate the effect of the pretrained character embeddings in our pro- posed crowdsourcing learning model. The comparison re- sults are shown in Figure 2, where Random refers to the random initialized character embeddings, and Pretrained refers to the embeddings pretrained on the unlabeled data. According to the results, we \ufb01nd that our model with the pretrained embeddings signi\ufb01cantly outperforms that using \uf07d \uf07d \uf0ec\uf0a9\uf02a \uf0ed\uf0a5\uf099\uf027 \uf07d \uf07d \uf07d",
            "According to the results, we \ufb01nd that our model with the pretrained embeddings signi\ufb01cantly outperforms that using \uf07d \uf07d \uf0ec\uf0a9\uf02a \uf0ed\uf0a5\uf099\uf027 \uf07d \uf07d \uf07d \uf07d \uf07d \uf07d \uf0ec\uf0a9\uf02a \uf0ed\uf0a5\uf099\uf027 \uf07d \uf07d \uf0ec\uf0a9\uf02a \uf0ed\uf0a5\uf099\uf027 \uf07d \uf07d \uf0ec\uf0a9\uf02a \uf0ed\uf0a5\uf099\uf027 \uf07d \uf07d \uf0ec\uf0a9\uf02a \uf0ed\uf0a5\uf099\uf027 \uf07d \uf07d \uf0ec\uf0a9\uf02a \uf0ed\uf0a5\uf099\uf027 \uf07d \uf07d \uf07d \uf07d \uf07d \uf07d \uf0ec\uf0a9\uf02a \uf0ed\uf0a5\uf099\uf027 \uf060\uf0f4\uf022\uf022\uf084\uf0d7 \uf060\uf0f4\uf022\uf022\uf084\uf0d7",
            "\uf07d \uf07d \uf07d \uf07d \uf0ec\uf0a9\uf02a \uf0ed\uf0a5\uf099\uf027 \uf060\uf0f4\uf022\uf022\uf084\uf0d7 \uf060\uf0f4\uf022\uf022\uf084\uf0d7 \uf060\uf0f4\uf022 \uf022 \uf022\uf022 \uf022 \uf022\uf084\uf0d7 \uf060\uf0f4\uf022\uf022\uf084\uf0d7 \uf0d7 \uf0d7 \uf0e5 \uf0e5 \uf0e5\uf0e5 \uf0e5 \uf0e5 \uf060\uf0f4\uf022\uf022\uf084\uf0d7 \uf060\uf0f4\uf022\uf022\uf084\uf0d7 \uf0d7 \uf0d7 \uf0e5 \uf0e5 \uf0e5\uf0e5 \uf0e5 \uf0e5 \uf060\uf0f4\uf022 \uf022 \uf022\uf022 \uf022 \uf022\uf084\uf0d7 \uf0d7 \uf0d7 \uf0e5 \uf0e5 \uf0e5\uf0e5 \uf0e5 \uf0e5 Figure 3: Case studies of different systems,",
            "\uf060\uf0f4\uf022 \uf022 \uf022\uf022 \uf022 \uf022\uf084\uf0d7 \uf0d7 \uf0d7 \uf0e5 \uf0e5 \uf0e5\uf0e5 \uf0e5 \uf0e5 Figure 3: Case studies of different systems, where named entities are illustrated by square brackets. the random embeddings, demonstrating that the pretrained embeddings successfully provide useful information. Case Studies. Second, we present several case studies in or- der to study the differences between our baseline and the worker adversarial models. We conduct a closed test on the training set, the results of which can be regarded as modi\ufb01- cations of the training corpus, since there exist inconsistent annotations for each training sentence among the different workers. Figure 3 shows the two examples from the DL-PS dataset, which compares the outputs of the baseline and our \ufb01nal models, as well as the majority-voting strategy. In the \ufb01rst case, none of the annotations get the cor- rect NER result, but our proposed model can capture it. The result of LSTM-CRF is the same as majority-voting.",
            "In the \ufb01rst case, none of the annotations get the cor- rect NER result, but our proposed model can capture it. The result of LSTM-CRF is the same as majority-voting. In the second example, the output of majority-voting is the worst, which can account for the reason why the same model trained on the voted corpus performs so badly, as shown in Table 2. The model of LSTM-CRF fails to recognize the named entity \u201cXiexie\u201d because of not trusting the second annotation, treating it as one noise annotation. Our proposed model is able to recognize it, because of its ability of extract- ing worker independent features. Conclusions In this paper, we presented an approach to performing crowd annotation learning based on the idea of adversarial training for Chinese Named Entity Recognition (NER). In our ap- proach, we use a common and private Bi-LSTMs for rep- resenting annotator-generic and -speci\ufb01c information, and learn a label Bi-LSTM from the crowd-annotated NE label sequences. Finally, the proposed approach adopts a LSTM- CRF model to perform tagging.",
            "Finally, the proposed approach adopts a LSTM- CRF model to perform tagging. In our experiments, we cre- ate two data sets for Chinese NER tasks in the dialog and e- commerce domains. The experimental results show that the proposed approach outperforms strong baseline systems.",
            "Acknowledgments This work is supported by the National Natural Science Foundation of China (Grant No. 61572338, 61525205, and 61602160). This work is also partially supported by the joint research project of Alibaba and Soochow University. Wen- liang is also partially supported by Collaborative Innovation Center of Novel Software Technology and Industrialization. References [2014] Bi, W.; Wang, L.; Kwok, J. T.; and Tu, Z. 2014. Learning to predict from crowdsourced data. In UAI, 82\u2013 91. [2016] Chen, X.; Sun, Y.; Athiwaratkun, B.; Cardie, C.; and Weinberger, K. 2016. Adversarial deep averaging networks for cross-lingual sentiment classi\ufb01cation. arXiv preprint arXiv:1606.01614. [2017] Chen, X.; Shi, Z.; Qiu, X.; and Huang, X. 2017. Ad- versarial multi-criteria learning for chinese word segmenta- tion.",
            "arXiv preprint arXiv:1606.01614. [2017] Chen, X.; Shi, Z.; Qiu, X.; and Huang, X. 2017. Ad- versarial multi-criteria learning for chinese word segmenta- tion. arXiv preprint arXiv:1704.07556. [2011] Collobert, R.; Weston, J.; Bottou, L.; Karlen, M.; Kavukcuoglu, K.; and Kuksa, P. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research 12:2493\u20132537. [2015] Denton, E. L.; Chintala, S.; Fergus, R.; et al. 2015. Deep generative image models using a laplacian pyramid of adversarial networks. In NIPS, 1486\u20131494. [2009] Dredze, M.; Talukdar, P. P.; and Crammer, K. 2009. Sequence learning from data with multiple labels.",
            "In NIPS, 1486\u20131494. [2009] Dredze, M.; Talukdar, P. P.; and Crammer, K. 2009. Sequence learning from data with multiple labels. In Work- shop Co-Chairs, 39. [2017] Dumitrache, A.; Aroyo, L.; and Welty, C. 2017. Crowdsourcing ground truth for medical relation extraction. arXiv preprint arXiv:1701.02185. [2015] Dyer, C.; Ballesteros, M.; Ling, W.; Matthews, A.; and Smith, N. A. 2015. Transition-based dependency pars- ing with stack long short-term memory. In ACL, 334\u2013343. [2015] Felt, P.; Black, K.; Ringger, E. K.; Seppi, K. D.; and Haertel, R. 2015. Early gains matter: A case for prefer- ring generative over discriminative crowdsourcing models. In HLT-NAACL, 882\u2013891.",
            "2015. Early gains matter: A case for prefer- ring generative over discriminative crowdsourcing models. In HLT-NAACL, 882\u2013891. [2016] Ganin, Y.; Ustinova, E.; Ajakan, H.; Germain, P.; Larochelle, H.; Laviolette, F.; Marchand, M.; and Lempit- sky, V. 2016. Domain-adversarial training of neural net- works. Journal of Machine Learning Research 17(59):1\u201335. [2014] Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y. 2014. Generative adversarial nets. In NIPS, 2672\u20132680. [2005] Graves, A., and Schmidhuber, J. 2005. Framewise phoneme classi\ufb01cation with bidirectional lstm and other neural network architectures. Neural Networks 18(5):602\u2013 610.",
            "[2005] Graves, A., and Schmidhuber, J. 2005. Framewise phoneme classi\ufb01cation with bidirectional lstm and other neural network architectures. Neural Networks 18(5):602\u2013 610. [2017] Gui, T.; Zhang, Q.; Huang, H.; Peng, M.; and Huang, X. 2017. Part-of-speech tagging for twitter with adversarial neural networks. In Proceedings of the 2017 Conference on EMNLP, 2401\u20132410. Copenhagen, Denmark: Association for Computational Linguistics. [2015] Huang, Z.; Xu, W.; and Yu, K. 2015. Bidirec- tional lstm-crf models for sequence tagging. arXiv preprint arXiv:1508.01991. [2017] Kim, J.-K.; Kim, Y.-B.; Sarikaya, R.; and Fosler- Lussier, E. 2017. Cross-lingual transfer learning for pos tagging without cross-lingual resources.",
            "[2017] Kim, J.-K.; Kim, Y.-B.; Sarikaya, R.; and Fosler- Lussier, E. 2017. Cross-lingual transfer learning for pos tagging without cross-lingual resources. In Proceedings of the 2017 Conference on EMNLP, 2822\u20132828. Copenhagen, Denmark: Association for Computational Linguistics. [2011] Kl\u00a8uwer, T. 2011. From chatbots to dialog systems. Conversational agents and natural language interaction: Techniques and Effective Practices 1\u201322. [2001] Lafferty, J.; McCallum, A.; Pereira, F.; et al. 2001. Conditional random \ufb01elds: Probabilistic models for seg- menting and labeling sequence data. In ICML, volume 1, 282\u2013289. [2016] Lample, G.; Ballesteros, M.; Subramanian, S.; Kawakami, K.; and Dyer, C. 2016. Neural architectures for named entity recognition. In NAACL, 260\u2013270.",
            "[2016] Lample, G.; Ballesteros, M.; Subramanian, S.; Kawakami, K.; and Dyer, C. 2016. Neural architectures for named entity recognition. In NAACL, 260\u2013270. [2017] Liu, P.; Qiu, X.; and Huang, X. 2017. Adversarial multi-task learning for text classi\ufb01cation. In Proceedings of the 55th ACL, 1\u201310. Vancouver, Canada: Association for Computational Linguistics. [2016] Ma, X., and Hovy, E. 2016. End-to-end sequence labeling via bi-directional lstm-cnns-crf. In Proceedings of the 54th ACL, 1064\u20131074. [2017] Nguyen, A. T.; Wallace, B.; Li, J. J.; Nenkova, A.; and Lease, M. 2017. Aggregating and predicting sequence labels from crowd annotations. In Proceedings of the 55th ACL, volume 1, 299\u2013309.",
            "2017. Aggregating and predicting sequence labels from crowd annotations. In Proceedings of the 55th ACL, volume 1, 299\u2013309. [2015] Peng, N., and Dredze, M. 2015. Named entity recog- nition for chinese social media with jointly trained embed- dings. In Proceedings of the EMNLP, 548\u2013554. [2009] Ratinov, L., and Roth, D. 2009. Design challenges and misconceptions in named entity recognition. In Pro- ceedings of the CoNLL-2009, 147\u2013155. [2014] Rodrigues, F.; Pereira, F.; and Ribeiro, B. 2014. Se- quence labeling with multiple annotators. Machine Learning 95(2):165\u2013181. [2008] Snow, R.; O\u2019Connor, B.; Jurafsky, D.; and Ng, A. Y. 2008. Cheap and fast\u2014but is it good?: evaluating non-expert annotations for natural language tasks.",
            "Machine Learning 95(2):165\u2013181. [2008] Snow, R.; O\u2019Connor, B.; Jurafsky, D.; and Ng, A. Y. 2008. Cheap and fast\u2014but is it good?: evaluating non-expert annotations for natural language tasks. In Proceedings of the conference on EMNLP, 254\u2013263. Association for Computa- tional Linguistics. [2012] Tieleman, T., and Hinton, G. 2012. Lecture 6.5- rmsprop: Divide the gradient by a running average of its re- cent magnitude. COURSERA: Neural networks for machine learning 4(2):26\u201331. [2016] Wu, Y.; Schuster, M.; Chen, Z.; Le, Q. V.; Norouzi, M.; Macherey, W.; Krikun, M.; Cao, Y.; Gao, Q.; Macherey, K.; et al. 2016. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144.",
            "2016. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144. [2017] Zhang, Y.; Barzilay, R.; and Jaakkola, T. 2017. Aspect-augmented adversarial networks for domain adapta- tion. arXiv preprint arXiv:1701.00188.",
            "[2008] Zhao, H., and Kit, C. 2008. Unsupervised segmenta- tion helps supervised learning of character tagging for word segmentation and named entity recognition. In IJCNLP, 106\u2013111."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1801.05147.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 11071.99984741211,
    "avg_doclen_est": 178.5806427001953
}
