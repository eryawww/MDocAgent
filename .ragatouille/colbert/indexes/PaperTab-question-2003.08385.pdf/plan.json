{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "X-stance: A Multilingual Multi-Target Dataset for Stance Detection Jannis Vamvas1 Rico Sennrich1,2 1Department of Computational Linguistics, University of Zurich 2School of Informatics, University of Edinburgh {vamvas,sennrich}@cl.uzh.ch Abstract We extract a large-scale stance detection dataset from comments written by can- didates of elections in Switzerland. The dataset consists of German, French and Italian text, allowing for a cross-lingual evaluation of stance detection. It contains 67 000 comments on more than 150 po- litical issues (targets). Unlike stance de- tection models that have speci\ufb01c target is- sues, we use the dataset to train a single model on all the issues. To make learn- ing across targets possible, we prepend to each instance a natural question that represents the target (e.g. \u201cDo you sup- port X?\u201d). Baseline results from multi- lingual BERT show that zero-shot cross- lingual and cross-target transfer of stance detection is moderately successful with this approach.",
            "\u201cDo you sup- port X?\u201d). Baseline results from multi- lingual BERT show that zero-shot cross- lingual and cross-target transfer of stance detection is moderately successful with this approach. 1 Introduction In recent years many datasets have been cre- ated for the task of automated stance detection, advancing natural language understanding sys- tems for political science, opinion research and other application areas. Typically, such bench- marks (Mohammad et al., 2016a) are composed of short pieces of text commenting on politicians or public issues and are manually annotated with their stance towards a target entity (e.g. Climate Change, or Trump). However, they are limited in scope on multiple levels (K\u00fc\u00e7\u00fck and Can, 2020). First of all, it is questionable how well cur- rent stance detection methods perform in a cross- lingual setting, as the multilingual datasets avail- Copyright c\u20dd2020 for this paper by its authors.",
            "First of all, it is questionable how well cur- rent stance detection methods perform in a cross- lingual setting, as the multilingual datasets avail- Copyright c\u20dd2020 for this paper by its authors. Use permit- ted under Creative Commons License Attribution 4.0 Interna- tional (CC BY 4.0) able today are relatively small, and speci\ufb01c to a single target (Taul\u00e9 et al., 2017, 2018). Further- more, speci\ufb01c models tend to be developed for each single target or pair of targets (Sobhani et al., 2017). Concerns have been raised that cross-target performance is often considerably lower than fully supervised performance (K\u00fc\u00e7\u00fck and Can, 2020). In this paper we propose a much larger dataset that combines multilinguality and a multitude of topics and targets. X-stance comprises more than 150 questions about Swiss politics and more than 67k answers given by candidates running for polit- ical of\ufb01ce in Switzerland. Questions are available in four languages: English, Swiss Standard Ger- man, French, and Italian.",
            "X-stance comprises more than 150 questions about Swiss politics and more than 67k answers given by candidates running for polit- ical of\ufb01ce in Switzerland. Questions are available in four languages: English, Swiss Standard Ger- man, French, and Italian. The language of a com- ment depends on the candidate\u2019s region of origin. We have extracted the data from the voting ad- vice application Smartvote. Candidates respond to questions mainly in categorical form (yes \/ rather yes \/ rather no \/ no). They can also submit a free- text comment to justify or explain their categorical answer. An example is given in Figure 1. We transform the dataset into a stance detec- tion task by interpreting the question as a natural- language representation of the target, and the com- mentary as the input to be classi\ufb01ed. The dataset is split into a multilingual train- ing set and into several test sets to evaluate zero- shot cross-lingual and cross-target transfer. To provide a baseline, we \ufb01ne-tune a multilingual BERT model (Devlin et al., 2019) on X-stance.",
            "To provide a baseline, we \ufb01ne-tune a multilingual BERT model (Devlin et al., 2019) on X-stance. We show that the baseline accuracy is comparable to previous stance detection benchmarks while leav- ing ample room for improvement. In addition, the model can generalize to a degree both cross- lingually and in a cross-target setting. We have made the dataset and the code for re- producing the baseline models publicly available.1 1http:\/\/doi.org\/10.5281\/zenodo.3831317 arXiv:2003.08385v2  [cs.CL]  10 Jun 2020",
            "Question #3414 \u2013 Available in all languages Soll der Bundesrat ein Frei- handelsabkommen mit den USA anstreben? La Suisse devrait-elle conclure un accord de libre-\u00e9change avec les Etats-Unis? Comment #26597 (German) Label: FAVOR Should Switzerland strive for a free trade agreement with the USA? Mit unserem zweitwichtigsten Handels- partner sollten wir ein Freihandels- abkommen haben. Comment #21421 (French) Label: AGAINST Les accords de libre-\u00e9change menacent la qualit\u00e9 des produits suisses. [With our second most important trading partner we should have a free trade agreement.] [The free trade agreements jeopardize the quality of the Swiss products.] Figure 1: Example of a question and two answers in the X-stance dataset. The answers were submitted by electoral candidates on a voting advice website. The author of the German comment was in favor of the issue; the author of the French comment against. Both authors use comments to explain their respective stance.",
            "The answers were submitted by electoral candidates on a voting advice website. The author of the German comment was in favor of the issue; the author of the French comment against. Both authors use comments to explain their respective stance. 2 Related Work Multilingual Stance Detection In the context of the IberEval shared tasks, two related multilingual datasets have been created (Taul\u00e9 et al., 2017, 2018). Both are a collection of annotated Spanish and Catalan tweets. Crucially, the tweets in both languages focus on the same issue (Catalan inde- pendence); given this fact they are the \ufb01rst truly multilingual stance detection datasets known to us. With regard to the languages covered by X-stance, only monolingual datasets seem to be available. For French, a collection of tweets on French presidential candidates has been an- notated with stance (Lai et al., 2020). Simi- larly, two datasets of Italian tweets on the occa- sion of the 2016 constitutional referendum have been created (Lai et al., 2018, 2020).",
            "Simi- larly, two datasets of Italian tweets on the occa- sion of the 2016 constitutional referendum have been created (Lai et al., 2018, 2020). With re- gard to German, a corpus of 270 sentences has been annotated with \ufb01ne-grained stance and atti- tude information (Clematide et al., 2012). Fur- thermore, \ufb01ne-grained stance detection has been qualitatively studied on a large corpus of Facebook posts (Klenner et al., 2017). Multi-Target Stance Detection The SemEval- 2016 task on detecting stance in tweets (Moham- mad et al., 2016b) offers data concerning multi- ple targets (Atheism, Climate Change, Feminism, Hillary Clinton, and Abortion). In the supervised subtask A, participants tended to develop a target- speci\ufb01c model for each of those targets. In sub- task B cross-target transfer to the target \u201cDonald Trump\u201d was tested, for which no annotated train- ing data were provided.",
            "In the supervised subtask A, participants tended to develop a target- speci\ufb01c model for each of those targets. In sub- task B cross-target transfer to the target \u201cDonald Trump\u201d was tested, for which no annotated train- ing data were provided. While this required the development of more universal models, their per- formance was generally much lower. Sobhani et al. (2017) introduced a multi-target stance dataset which provides two targets per in- stance. For example, a model designed in this framework is supposed to simultaneously classify a tweet with regard to Clinton and with regard to Trump. While in theory the framework allows for more than two targets, it is still restricted to a \ufb01- nite and clearly de\ufb01ned set of targets. It focuses on modeling the dependencies of multiple targets within the same text sample, while our approach focuses on learning stance detection from many samples with many different targets. Representation Learning for Stance Detection In a target-speci\ufb01c setting, Ghosh et al. (2019) perform a systematic evaluation of stance detec- tion approaches.",
            "Representation Learning for Stance Detection In a target-speci\ufb01c setting, Ghosh et al. (2019) perform a systematic evaluation of stance detec- tion approaches. They also evaluate BERT (Devlin et al., 2019) and \ufb01nd that it consistently outper- forms previous approaches. However, they only experiment with a single- segment encoding of the input, preventing cross- target transfer of the model. Augenstein et al. (2016) propose a conditional encoding approach to encode both the target and the tweet as se- quences. They use a bidirectional LSTM to condi- tion the encoding of the tweets on the encoding of the target, and then apply a nonlinear projection on",
            "Topic Questions Answers Digitisation 2 1168 Economy 23 6899 Education 16 7639 Finances 15 3980 Foreign Policy 16 4393 Immigration 19 6270 Infrastructure & Environment 31 9590 Security 20 5193 Society 17 6275 Welfare 15 8508 Total (training topics) 174 59 915 Healthcare 11 4711 Political System 9 2645 Total (held-out topics) 20 7356 Table 1: Number of questions and answers per topic. the conditionally encoded tweet. This allows them to train a model that can generalize to previously unseen targets. 3 The X-stance Dataset 3.1 Task De\ufb01nition The input provided by X-stance is two-fold: (A) a natural language question concerning a politi- cal issue; (B) a natural language commentary on a speci\ufb01c stance towards the question. The label to be predicted is either \u2018favor\u2019 or \u2018against\u2018. This corresponds to a standard estab- lished by Mohammad et al. (2016a).",
            "The label to be predicted is either \u2018favor\u2019 or \u2018against\u2018. This corresponds to a standard estab- lished by Mohammad et al. (2016a). However, X-stance differs from that dataset in that it lacks a \u2018neither\u2019 class; all comments refer to either a \u2018fa- vor\u2019 or an \u2018against\u2018 position. The task posed by X-stance is thus a binary classi\ufb01cation task. As an evaluation metric we report the macro- average of the F1-score for \u2018favor\u2019 and the F1- score for \u2018against\u2019, similar to Mohammad et al. (2016b). We use this metric mainly to strengthen comparability with the previous benchmarks. 3.2 Data Collection Provenance We downloaded the questions and answers via the Smartvote API2. The downloaded data cover 175 communal, cantonal and national elections between 2011 and 2020. All candidates in an election who participate in Smartvote are asked the same set of questions, but 2https:\/\/smartvote.ch depending on the locale they see translated ver- sions of the questions.",
            "All candidates in an election who participate in Smartvote are asked the same set of questions, but 2https:\/\/smartvote.ch depending on the locale they see translated ver- sions of the questions. They can answer each question with either \u2018yes\u2019, \u2018rather yes\u2019, \u2018rather no\u2019, or \u2018no\u2019. They can supplement each answer with a comment of at most 500 characters. The questions asked on Smartvote have been edited by a team of political scientists. They are intended to cover a broad range of political is- sues relevant at the time of the election. A de- tailed documentation of the design of Smartvote and the editing process of the questions is provided by Thurman and Gasser (2009). Preprocessing We merged the two labels on each pole into a single label: \u2018yes\u2019 and \u2018rather yes\u2019 were combined into \u2018favor\u2019; \u2018rather no\u2019, or \u2018no\u2019 into \u2018against\u2018. This improves the consistency of the data and the comparability to previous stance detection datasets. We did not further preprocess the text of the comments.",
            "This improves the consistency of the data and the comparability to previous stance detection datasets. We did not further preprocess the text of the comments. Language Identi\ufb01cation As the API does not provide the language of comments, we employed a language identi\ufb01er to automatically annotate this information. We used the langdetect li- brary (Shuyo, 2010). For each responder we clas- si\ufb01ed all the comments jointly, assuming that re- sponders did not switch code during the answering of the questionnaire. We applied the identi\ufb01er in a two-step approach. In the \ufb01rst run we allowed the identi\ufb01er to out- put all 55 languages that it supports out of the box, plus Romansh, the fourth of\ufb01cial language in Switzerland3. We found that no Romansh com- ments were detected and that all unexpected out- puts were misclassi\ufb01cations of German, French or Italian comments. We further concluded that little or no Swiss German comments are in the dataset; otherwise, some of them would have manifested themselves via misclassi\ufb01cations (e.g. as Dutch).",
            "We further concluded that little or no Swiss German comments are in the dataset; otherwise, some of them would have manifested themselves via misclassi\ufb01cations (e.g. as Dutch). In the second run, drawing from these conclu- sions, we restricted the identi\ufb01er\u2019s set of choices to English, French, German and Italian. Filtering We pre-\ufb01ltered the questions and an- swers to improve the quality of the dataset. To keep the domain of the data surveyable, we set a focus on national-level questions. Therefore, all 3Namely the Rumantsch Grischun variety; the lan- guage pro\ufb01le was created using resources from the Zurich Parallel Corpus Collection (Gra\u00ebn et al., 2019) and the Quotidiana corpus (https:\/\/github.com\/ ProSvizraRumantscha\/corpora).",
            "Intra-target (New answers to known questions) Cross-question (New questions within known topics) Cross-topic DE Train: Test: Valid: 33 850 2871 3479 Test: 3143 Test: 5269 FR Train: Test: Valid: 11 790 1055 1284 Test: 1170 Test: 1914 IT Test: 1173 Test: (110) Test: (173) Table 2: Number of answer instances in the training, validation and test sets. The upper left corner represents a multilingually supervised task, where training, validation and test data are from exactly the same domain. The top- to-bottom axis gives rise to a cross-lingual transfer task, where a model trained on German and French is evaluated on Italian answers to the same questions. The left-to-right axis represents a continuous shift of domain: In the middle column, the model is tested on previously unseen questions that belong to the same topics as seen during training. In the right column the model encounters unseen answers to unseen questions within an unseen topic. The two test sets in parentheses are too small for a signi\ufb01cant evaluation.",
            "In the right column the model encounters unseen answers to unseen questions within an unseen topic. The two test sets in parentheses are too small for a signi\ufb01cant evaluation. questions and corresponding answers pertaining to national elections were included. In the context of communal and cantonal elec- tions, candidates have answered both local ques- tions and a subset of the national questions. Of those elections, we only considered answers to the questions that also had been asked in a national election. They were only used to augment the training set while the validation and test sets were restricted to answers from national elections. We discarded the fewer than 20 comments clas- si\ufb01ed as English. Furthermore, we discarded in- stances that met any of the following conditions: \u2022 Question is not a closed question or does not address a clearly de\ufb01ned political issue. \u2022 No comment was submitted by the candidate or the comment is shorter than 50 characters. \u2022 Comment starts with \u201cbut\u201d or a similar indi- cator that the comment is not self-contained. \u2022 Comment contains a URL. In total, a \ufb01fth of the comments were \ufb01ltered out.",
            "\u2022 Comment starts with \u201cbut\u201d or a similar indi- cator that the comment is not self-contained. \u2022 Comment contains a URL. In total, a \ufb01fth of the comments were \ufb01ltered out. Topics The questions have been organized by the Smartvote editors into categories (such as \u201cEconomy\u201d). We further consolidated the pre- de\ufb01ned categories into 12 broad topics (Table 1). Compliance The dataset is shared under a CC BY-NC 4.0 license. Copyright remains with www.smartvote.ch. Given the sensitive nature of the data, we in- crease the anonymity of the data by hashing the respondents\u2019 IDs. No personal attributes of the re- spondents are included in the dataset. We provide a data statement (Bender and Friedman, 2018) in Appendix B. 3.3 Data Split We held out the topics \u201cHealthcare\u201d and \u201cPolitical System\u201d from the training data and created a sepa- rate cross-topic test set that contains the questions and answers related to those topics.",
            "3.3 Data Split We held out the topics \u201cHealthcare\u201d and \u201cPolitical System\u201d from the training data and created a sepa- rate cross-topic test set that contains the questions and answers related to those topics. Furthermore, in order to test cross-question generalization performance within previously seen topics, we manually selected 16 held-out ques- tions that are distributed over the remaining 10 topics. We selected the held-out questions man- ually because we wanted to make sure that they are truly unseen and that no paraphrases of the ques- tions are found in the training set. We designated Italian as a test-only language, since relatively few comments have been written in Italian. From the remaining German and French data we randomly selected a percentage of respon- dents as validation or as test respondents. As a result we received one training set, one val- idation set and four test sets. The sizes of the sets are listed in Table 2. We did not consider test sets that are cross-lingual and cross-target at the same time, as they would have been too small to yield signi\ufb01cant results.",
            "Digitisation Economy Education Finances Foreign Policy Immigration Infrastructure Security Society Welfare Healthcare Political System Proportion of class \u2018favor\u2019 25% 50% 75% 100% held-out mean Figure 2: Proportion of \u2018favor\u2019 labels per question, grouped by topic. While the proportion of favorable answers varies from question to question, it is balanced overall. 3.4 Analysis Some observations regarding the composition of X-stance can be made. Class Distribution Figure 2 visualizes the pro- portion of \u2018favor\u2019 and \u2018against\u2018 stances for each target in the dataset. The ratio differs between questions but is relatively equally distributed across the topics. In particular, the questions in the held-out topics (with a \u2018favor\u2019 ratio of 49.4%) have a similar class distribution as the questions in other topics (with a \u2018favor\u2019 ratio of 50.0%). Linguistic Properties Not every question is unique; some questions are paraphrases describing the same political issue.",
            "Linguistic Properties Not every question is unique; some questions are paraphrases describing the same political issue. For example, in the 2015 election, the candidates were asked: \u201cShould the consumption of cannabis as well as its possession for personal use be legalised?\u201d Four years later they were asked: \u201cShould cannabis use be legal- ized?\u201d However, we do not see any need to con- solidate those duplicates because they contribute to the diversity of the training data. We further observe that while some questions in the dataset are quite short, some questions are rather convoluted. For example, a typical long question reads: Some 1% of direct payments to Swiss agricul- ture currently go to organic farming operations. Should this proportion be increased at the ex- pense of standard farming operations as part of Switzerland\u2019s 2014-2017 agricultural policy? Such longer questions might be more challenging to process semantically. Languages The X-stance dataset has more Ger- man samples than French samples. The language ratio of about 3:1 is consistent across all train- ing and test sets.",
            "Such longer questions might be more challenging to process semantically. Languages The X-stance dataset has more Ger- man samples than French samples. The language ratio of about 3:1 is consistent across all train- ing and test sets. Given the two languages it is possible to either train two monolingual mod- els or to train a single model in a multi-source setup (McDonald et al., 2011). We choose a multi- source baseline because M-BERT is known to ben- e\ufb01t from multilingual training data both in a super- vised and in a cross-lingual scenario (Kondratyuk and Straka, 2019). 4 Baseline Experiments We evaluate four baselines to obtain an impression of the dif\ufb01culty of the task. 4.1 Majority Class Baselines The \ufb01rst pair of baselines uses the most frequent class in the training set for prediction. Speci\ufb01- cally, the global majority class baseline predicts the most frequent class across all training targets while the target-wise majority class baseline pre- dicts the class that is most frequent for a given tar- get question.",
            "Speci\ufb01- cally, the global majority class baseline predicts the most frequent class across all training targets while the target-wise majority class baseline pre- dicts the class that is most frequent for a given tar- get question. The latter can only be applied to the intra-target test sets. 4.2 Bag-of-Words Baseline As a second baseline, we train a fastText bag-of- words linear classi\ufb01er (Joulin et al., 2017). For each comment, we select the translation of the question that matches its language, and concate- nate it to the comment. We tokenize the text using the Europarl preprocessing tools (Koehn, 2005). The \u2018against\u2019 class was slightly upsampled in",
            "the training data so that the classes are balanced when summing over all questions and topics. We use the standard settings provided by the fastText library.4 Optimal hyperparameters from the following range were determined based on the validation accuracy: \u2022 Learning rate: 0.1, 0.2, 1 \u2022 Number of epochs: 5, 50 The word vectors were set to a size of 300. We do not initialize them with pre-trained multilingual embeddings since preliminary experiments did not show a bene\ufb01cial effect. 4.3 Multilingual BERT Baseline As our main baseline model we \ufb01ne-tune multilin- gual BERT (M-BERT) on the task (Devlin et al., 2019) which has been pre-trained jointly in 104 languages5 and has established itself as a state of the art for various multilingual tasks (Wu and Dredze, 2019; Pires et al., 2019). Within the \ufb01eld of stance detection, BERT can outperform both feature-based and other neural approaches in a monolingual English setting (Ghosh et al., 2019).",
            "Within the \ufb01eld of stance detection, BERT can outperform both feature-based and other neural approaches in a monolingual English setting (Ghosh et al., 2019). Architecture In the context of BERT we in- terpret the X-stance task as sequence pair clas- si\ufb01cation inspired by natural language inference tasks (Bowman et al., 2015). We follow the pro- cedure outlined by Devlin et al. (2019) for such tasks. We designate the question as segment A and the comment as segment B. The two segments are separated with the special token [SEP], and the special token [CLS] is prepended to the se- quence. The \ufb01nal hidden state corresponding to [CLS] is then classi\ufb01ed by a linear layer. We \ufb01ne-tune the full model with a cross-entropy loss, using the AllenNLP library (Gardner et al., 2018) as a basis for our implementation. Training As above, we balanced out the num- ber of classes in the training set.",
            "We \ufb01ne-tune the full model with a cross-entropy loss, using the AllenNLP library (Gardner et al., 2018) as a basis for our implementation. Training As above, we balanced out the num- ber of classes in the training set. We use a batch size of 16 and a maximum sequence length of 512 subwords, and performed a grid search over the following hyperparameters based on the validation accuracy: \u2022 Learning rate: 5e-5, 3e-5, 2e-5 \u2022 Number of epochs: 3, 4 4https:\/\/github.com\/facebookresearch\/ fastText 5https:\/\/github.com\/google-research\/ bert\/blob\/master\/multilingual.md DE FR IT Majority class (global) 33.1 34.8 34.4 Majority class (target-wise) 60.8 65.1 59.3 fastText 69.9 71.2 53.7 M-BERT 76.8 76.6 70.2 Table 3: Baseline scores in the cross-lingual setting.",
            "No Italian samples were seen during training, mak- ing this a case of zero-shot cross-lingual transfer. The scores are reported as the macro-average of the F1- scores for \u2018favor\u2019 and for \u2018against\u2019. The grid search was repeated independently for every variant that we test in the following sub- sections. Furthermore, the standard recommenda- tions for \ufb01ne-tuning BERT were used: Adam with \u03b21 = 0.9 and \u03b22 = 0.999; an L2 weight decay of 0.01; a learning rate warmup over the \ufb01rst 10% of the steps; and a linear decay of the learning rate. A dropout probability of 0.1 was set on all layers. Results Table 3 shows the results for the cross- lingual setting. M-BERT performs consistently better than the previous baselines. Even the zero- shot performance in Italian, while signi\ufb01cantly lower than the supervised scores, is much better than the target-wise majority class baseline. Results for the cross-target setting are given in Table 4.",
            "M-BERT performs consistently better than the previous baselines. Even the zero- shot performance in Italian, while signi\ufb01cantly lower than the supervised scores, is much better than the target-wise majority class baseline. Results for the cross-target setting are given in Table 4. Similar to the cross-lingual setting, model performance drops in the cross-target setting, but M-BERT remains the strongest baseline and eas- ily surpasses the majority class baselines. Fur- thermore, the cross-question score of M-BERT is slightly lower than the cross-topic score. 4.4 How Important is Consistent Language? The default setup preserves horizontal language consistency in that the language of the questions always corresponds to the language of the com- ments. For example, the Italian test instances are combined with the Italian version of the questions, even though during training the model has only ever seen the German and French version of them. An alternative concept is vertical language con- sistency, whereby the questions are consistently presented in one language, regardless of the com- ment.",
            "An alternative concept is vertical language con- sistency, whereby the questions are consistently presented in one language, regardless of the com- ment. To test whether horizontal or vertical con- sistency is more helpful, we train and evaluate M-BERT on a dataset variant where all questions are in their English version. We chose English as a lingua franca because it had the largest share of data during the pre-training of M-BERT.",
            "Intra-target Cross-question Cross-topic DE FR Mean DE FR Mean DE FR Mean Majority class (global) 33.1 34.8 33.9 36.4 37.9 37.1 32.1 33.8 32.9 Majority class (target-wise) 60.8 65.1 62.9 - - - - - - fastText 69.9 71.2 70.5 62.0 65.6 63.7 63.1 65.5 64.3 M-BERT 76.8 76.6 76.6 68.5 68.4 68.4 68.9 70.9 69.9 Table 4: Baseline scores in the cross-target setting. For each test set we separately report a German and a French score, as well as their harmonic mean. Results are shown in Table 5. While the effect is negligible in most settings, cross-lingual perfor- mance increases when all questions are in English. 4.5 How Important are the Segments?",
            "Results are shown in Table 5. While the effect is negligible in most settings, cross-lingual perfor- mance increases when all questions are in English. 4.5 How Important are the Segments? In order to rule out that only the questions or only the comments are necessary to optimally solve the task, we conduct some additional experiments: \u2022 Only use a single segment containing the comment, removing the questions from the training and test data (missing questions). \u2022 Only use the question and remove the com- ment (missing comments). In both cases the performance decreases across all evaluation settings (Table 5). The loss in performance is much higher when comments are missing, indicating that the comments contain the most important information about stance. As can be expected, the score achieved without comments is only slightly different from the target-wise ma- jority class baseline. But there is also a loss in performance when the questions are missing, which underlines the im- portance of pairing both pieces of text. The effect of missing questions is especially strong in the su- pervised and cross-lingual settings.",
            "But there is also a loss in performance when the questions are missing, which underlines the im- portance of pairing both pieces of text. The effect of missing questions is especially strong in the su- pervised and cross-lingual settings. To illustrate this, we provide in Table A8 some examples of comments that occur with multiple different tar- gets in the training set. Those examples can ex- plain why the target can be essential for disam- biguating a stance detection problem. On the other hand, the effect of omitting the questions is less pronounced in the cross-target settings. The above single-segment experiments tell us that both the comment and the question provide crucial information. But it is possible that the M-BERT model, even though trained on both seg- ments, mainly looks at a single segment at test time. To rule this out, we probe the model with randomized data at test time: \u2022 Test the model on versions of the test sets where the comments remain in place but the questions are shuf\ufb02ed randomly (random questions). We make sure that the random questions come from the same test set and language as the original questions.",
            "We make sure that the random questions come from the same test set and language as the original questions. \u2022 Keep the questions in place and randomize the comments (random comments). Again we shuf\ufb02e the comments only within test set boundaries. The results in Table 5 show that the performance of the model decreases in both cases, con\ufb01rming that it learns to take into account both segments. 4.6 How Important are Spelled-Out Targets? Finally we test whether the target really needs to be represented by natural language (e.g. \u201cDo you support X?\u201d). An alternative is to represent the target with a trainable embedding instead. In order to \ufb01t target embeddings smoothly into our architecture, we represent each target type with a different reserved symbol from the M-BERT vocabulary. Segment A is then set to this symbol instead of a natural language question. The results for this experiment are listed in the bottom row of Table 5. An M-BERT model that learns target embeddings instead of encoding a question performs clearly worse in the supervised and cross-lingual settings.",
            "Segment A is then set to this symbol instead of a natural language question. The results for this experiment are listed in the bottom row of Table 5. An M-BERT model that learns target embeddings instead of encoding a question performs clearly worse in the supervised and cross-lingual settings. From this we conclude that spelled-out natural language questions pro- vide important linguistic detail that can help in stance detection. 5 Discussion Our experiments show that M-BERT achieves a reasonable accuracy on X-stance, outperforming majority class baselines and a fastText classi\ufb01er.",
            "Supervised Cross-Lingual Cross-Question Cross-Topic M-BERT 76.6 70.2 68.4 69.9 \u2014 with English questions 76.1 71.7 68.5 69.4 \u2014 with missing questions 73.2 67.1 67.8 69.3 \u2014 with missing comments 64.2 60.5 51.1 48.6 \u2014 with random questions 56.0 52.5 47.7 48.5 \u2014 with random comments 50.7 50.7 48.2 48.7 \u2014 with target embeddings 70.1 66.0 68.4 69.0 Table 5: Results for additional experiments. The cross-lingual score is the F1-score on the Italian test set. For the supervised, cross-question and cross-topic settings we report the harmonic mean of the German and French scores. Dataset Evaluation Score SemEval-2016 Ghosh et al. (2019) 75.1 MPCHI Ghosh et al.",
            "For the supervised, cross-question and cross-topic settings we report the harmonic mean of the German and French scores. Dataset Evaluation Score SemEval-2016 Ghosh et al. (2019) 75.1 MPCHI Ghosh et al. (2019) 75.6 X-stance this paper 76.6 Table 6: Performance of BERT-like models on differ- ent supervised stance detection benchmarks. To put the supervised score into context we list scores that variants of BERT have achieved on other stance detection datasets in Table 6. It seems that the supervised part of X-stance has a similar dif\ufb01culty as the SemEval-2016 (Mohammad et al., 2016a) or MPCHI (Sen et al., 2018) datasets on which BERT has previously been evaluated. On the other hand, in the cross-lingual and cross-target settings, the mean score drops by 6\u20138 percentage points compared to the supervised set- ting; while zero-shot transfer is possible to a de- gree, it can still be improved.",
            "On the other hand, in the cross-lingual and cross-target settings, the mean score drops by 6\u20138 percentage points compared to the supervised set- ting; while zero-shot transfer is possible to a de- gree, it can still be improved. The additional experiments (Table 5) validate the results and show that the sequence-pair clas- si\ufb01cation approach to stance detection is justi\ufb01ed. It is interesting to see what errors the M-BERT model makes. Table A7 presents instances where it predicts the wrong label with a high con\ufb01dence. These examples indicate that many comments ex- press their stance only on a very implicit level, and thus hint at a potential weakness of the dataset. Because on the voting advice platform the label is explicitly shown to readers in addition to the com- ments, the comments do not need to express the stance explicitly. Manual annotation could eliminate very im- plicit samples in a future version of the dataset. However, the sheer size and breadth of the dataset could not realistically be achieved with manual an- notation, and, in our view, largely compensates for the implicitness of the texts.",
            "Manual annotation could eliminate very im- plicit samples in a future version of the dataset. However, the sheer size and breadth of the dataset could not realistically be achieved with manual an- notation, and, in our view, largely compensates for the implicitness of the texts. 6 Conclusion We have presented a new dataset for political stance detection called X-stance. The dataset ex- tends over a broad range of topics and issues re- garding national Swiss politics. This diversity of topics opens up an opportunity to further study multi-target learning. Moreover, being partly Swiss Standard German, partly French and Ital- ian, the dataset promotes a multilingual approach to stance detection. By compiling formal commentary by politicians on political questions, we add a new text genre to the \ufb01eld of stance detection. We also propose a question\u2013answer format that allows us to condi- tion stance detection models on a target naturally. Our baseline results with multilingual BERT show that the model has some capability to per- form zero-shot transfer to unseen languages and to unseen targets (both within a topic and to un- seen topics). However, there is some gap in per- formance that future work could address.",
            "Our baseline results with multilingual BERT show that the model has some capability to per- form zero-shot transfer to unseen languages and to unseen targets (both within a topic and to un- seen topics). However, there is some gap in per- formance that future work could address. We ex- pect that the X-stance dataset could furthermore be a valuable resource for \ufb01elds such as argument mining, argument search or topic classi\ufb01cation. Acknowledgments This work was funded by the Swiss Na- tional Science Foundation (project MUTAMUR; no. 176727). We would like to thank Isabelle Augenstein, Anne G\u00f6hring and the anonymous re- viewers for helpful feedback.",
            "References Isabelle Augenstein, Tim Rockt\u00e4schel, Andreas Vla- chos, and Kalina Bontcheva. 2016. Stance detec- tion with bidirectional conditional encoding. In Pro- ceedings of the 2016 Conference on Empirical Meth- ods in Natural Language Processing, pages 876\u2013 885, Austin, Texas. Association for Computational Linguistics. Emily M. Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587\u2013604. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large anno- tated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empiri- cal Methods in Natural Language Processing, pages 632\u2013642, Lisbon, Portugal. Association for Compu- tational Linguistics.",
            "2015. A large anno- tated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empiri- cal Methods in Natural Language Processing, pages 632\u2013642, Lisbon, Portugal. Association for Compu- tational Linguistics. Simon Clematide, Stefan Gindl, Manfred Klenner, Ste- fanos Petrakis, Robert Remus, Josef Ruppenhofer, Ulli Waltinger, and Michael Wiegand. 2012. MLSA \u2014 a multi-layered reference corpus for German sen- timent analysis. In Proceedings of the Eighth In- ternational Conference on Language Resources and Evaluation (LREC\u201912), pages 3551\u20133556, Istanbul, Turkey. European Language Resources Association (ELRA). Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing.",
            "European Language Resources Association (ELRA). Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics. Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Pe- ters, Michael Schmitz, and Luke Zettlemoyer. 2018. AllenNLP: A deep semantic natural language pro- cessing platform. In Proceedings of Workshop for NLP Open Source Software (NLP-OSS), pages 1\u2013 6, Melbourne, Australia. Association for Computa- tional Linguistics. Shalmoli Ghosh, Prajwal Singhania, Siddharth Singh, Koustav Rudra, and Saptarshi Ghosh. 2019.",
            "Association for Computa- tional Linguistics. Shalmoli Ghosh, Prajwal Singhania, Siddharth Singh, Koustav Rudra, and Saptarshi Ghosh. 2019. Stance detection in web and social media: a comparative study. In International Conference of the Cross- Language Evaluation Forum for European Lan- guages, pages 75\u201387. Springer. Johannes Gra\u00ebn, Tannon Kew, Anastassia Shaitarova, and Martin Volk. 2019. Modelling large parallel cor- pora: The zurich parallel corpus collection. In Pro- ceedings of the 7th Workshop on Challenges in the Management of Large Corpora (CMLC), pages 1\u20138. Leibniz-Institut f\u00fcr Deutsche Sprache. Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2017. Bag of tricks for ef\ufb01cient text classi\ufb01cation.",
            "Leibniz-Institut f\u00fcr Deutsche Sprache. Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2017. Bag of tricks for ef\ufb01cient text classi\ufb01cation. In Proceedings of the 15th Con- ference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Pa- pers, pages 427\u2013431, Valencia, Spain. Association for Computational Linguistics. Manfred Klenner, Don Tuggener, and Simon Clematide. 2017. Stance detection in Facebook posts of a German right-wing party. In Proceed- ings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 31\u201340, Valencia, Spain. Association for Computational Linguistics. Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. Machine Translation Summit, 2005, pages 79\u201386. Dan Kondratyuk and Milan Straka. 2019.",
            "Association for Computational Linguistics. Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. Machine Translation Summit, 2005, pages 79\u201386. Dan Kondratyuk and Milan Straka. 2019. 75 lan- guages, 1 model: Parsing universal dependencies universally. In Proceedings of the 2019 Confer- ence on Empirical Methods in Natural Language Processing and the 9th International Joint Confer- ence on Natural Language Processing (EMNLP- IJCNLP), pages 2779\u20132795, Hong Kong, China. As- sociation for Computational Linguistics. Dilek K\u00fc\u00e7\u00fck and Fazli Can. 2020. Stance detection: A survey. ACM Comput. Surv., 53(1). Mirko Lai, Alessandra Teresa Cignarella, Delia Iraz\u00fa Hern\u00e1ndez Far\u00edas, Cristina Bosco, Viviana Patti, and Paolo Rosso. 2020. Multilingual stance detection in social media political debates. Com- puter Speech & Language, page 101075.",
            "2020. Multilingual stance detection in social media political debates. Com- puter Speech & Language, page 101075. Mirko Lai, Viviana Patti, Giancarlo Ruffo, and Paolo Rosso. 2018. Stance evolution and twitter interac- tions in an italian political debate. In International Conference on Applications of Natural Language to Information Systems, pages 15\u201327. Springer. Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-source transfer of delexicalized dependency parsers. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Process- ing, pages 62\u201372, Edinburgh, Scotland, UK. Asso- ciation for Computational Linguistics. Saif Mohammad, Svetlana Kiritchenko, Parinaz Sob- hani, Xiaodan Zhu, and Colin Cherry. 2016a. A dataset for detecting stance in tweets. In Proceed- ings of the Tenth International Conference on Lan- guage Resources and Evaluation (LREC\u201916), pages 3945\u20133952, Portoro\u017e, Slovenia.",
            "2016a. A dataset for detecting stance in tweets. In Proceed- ings of the Tenth International Conference on Lan- guage Resources and Evaluation (LREC\u201916), pages 3945\u20133952, Portoro\u017e, Slovenia. European Language Resources Association (ELRA). Saif Mohammad, Svetlana Kiritchenko, Parinaz Sob- hani, Xiaodan Zhu, and Colin Cherry. 2016b. SemEval-2016 task 6: Detecting stance in tweets. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 31\u2013 41, San Diego, California. Association for Compu- tational Linguistics.",
            "Telmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual BERT? In Pro- ceedings of the 57th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 4996\u2013 5001, Florence, Italy. Association for Computa- tional Linguistics. Anirban Sen, Manjira Sinha, Sandya Mannarswamy, and Shourya Roy. 2018. Stance classi\ufb01cation of multi-perspective consumer health information. In Proceedings of the ACM India Joint International Conference on Data Science and Management of Data, pages 273\u2013281. Nakatani Shuyo. 2010. Language detection library for java. Parinaz Sobhani, Diana Inkpen, and Xiaodan Zhu. 2017. A dataset for multi-target stance detection. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Lin- guistics: Volume 2, Short Papers, pages 551\u2013557, Valencia, Spain. Association for Computational Lin- guistics.",
            "2017. A dataset for multi-target stance detection. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Lin- guistics: Volume 2, Short Papers, pages 551\u2013557, Valencia, Spain. Association for Computational Lin- guistics. Mariona Taul\u00e9, M Ant\u00f2nia Mart\u00ed, Francisco Rangel, Paolo Rosso, Cristina Bosco, and Viviana Patti. 2017. Overview of the task on stance and gen- der detection in tweets on catalan independence at ibereval 2017. In 2nd Workshop on Evaluation of Human Language Technologies for Iberian Lan- guages, IberEval 2017, volume 1881, pages 157\u2013 177. Mariona Taul\u00e9, Francisco Rangel, M Ant\u00f2nia Mart\u00ed, and Paolo Rosso. 2018. Overview of the task on multimodal stance detection in tweets on catalan #1oct referendum. In 3rd Workshop on Evaluation of Human Language Technologies for Iberian Lan- guages, IberEval 2018, volume 2150, pages 149\u2013 166.",
            "2018. Overview of the task on multimodal stance detection in tweets on catalan #1oct referendum. In 3rd Workshop on Evaluation of Human Language Technologies for Iberian Lan- guages, IberEval 2018, volume 2150, pages 149\u2013 166. James Thurman and Urs Gasser. 2009. Three case studies from switzerland: Smartvote. Berkman Cen- ter Research Publications. Shijie Wu and Mark Dredze. 2019. Beto, bentz, be- cas: The surprising cross-lingual effectiveness of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 833\u2013844, Hong Kong, China. Association for Com- putational Linguistics.",
            "A Examples Question Comment Gold Label Prob. Bef\u00fcrworten Sie eine vollst\u00e4ndige Liberalisierung der Gesch\u00e4fts\u00f6ffnungszeiten? Ausser Sonntag. Dies sollte ein Ruhetag bleiben k\u00f6nnen. FAVOR 0.001 [Are you in favour of a complete liberalisation of business hours for shops?] [Except Sunday. That should remain a day of rest.] Soll die Schweiz innerhalb der n\u00e4chsten vier Jahre EU-Beitrittsverhandlungen aufnehmen? In den n\u00e4chsten vier Jahren ist dies wohl un- realistisch. FAVOR 0.005 [Should Switzerland embark on negotiations in the next four years to join the EU?] [For the next four years this is probably unrealis- tic.] Bef\u00fcrworten Sie einen Ausbau des Landschaftss- chutzes? Wenn es darum geht erneuerbare Energien zu f\u00f6rdern, ist sogar eine Lockerung angebracht. AGAINST 0.006 [Are you in favour of extending landscape protec- tion?] [When it comes to promoting renewable energy, even a relaxation is appropriate.]",
            "Wenn es darum geht erneuerbare Energien zu f\u00f6rdern, ist sogar eine Lockerung angebracht. AGAINST 0.006 [Are you in favour of extending landscape protec- tion?] [When it comes to promoting renewable energy, even a relaxation is appropriate.] La Suisse devrait-elle engager des n\u00e9gociations pour un accord de libre \u00e9change avec les Etats- Unis? Il faut cependant en parall\u00e8le veiller \u00e0 ce que la Suisse ne soit pas mise de c\u00f4t\u00e9 par les Etats-Unis ! AGAINST 0.010 [Should Switzerland start negotiations with the USA on a free trade agreement?] [At the same time it must be ensured that Switzer- land is not sidelined by the United States!] Table A7: Some classi\ufb01cation errors where the predicted probability of the correct label is especially low. The examples have been taken from the validation set. Comment . . . is favorable towards target . . . but against target . . . Ich will offene Grenzen f\u00fcr Waren und selbstverantwortliche m\u00fcndige B\u00fcrger.",
            "The examples have been taken from the validation set. Comment . . . is favorable towards target . . . but against target . . . Ich will offene Grenzen f\u00fcr Waren und selbstverantwortliche m\u00fcndige B\u00fcrger. Der Staat hat kein Recht, uns einzuschr\u00e4nken. Soll die Schweiz mit den USA Verhand- lungen \u00fcber ein Freihandelsabkommen aufnehmen? Soll die Schweiz das Schengen- Abkommen mit der EU k\u00fcndigen und wieder verst\u00e4rkte Personenkontrollen direkt an der Grenze einf\u00fchren? [I want open borders for goods and re- sponsible citizens. The state has no right to restrict us.] [Should Switzerland start negotiations with the USA on a free trade agree- ment?] [Should Switzerland terminate the Schengen Agreement with the EU and reintroduce increased identity checks directly on the border?] Hier gilt der Grundsatz der Eigenver- antwortung und Selbstbestimmung des Unternehmens! Sind Sie f\u00fcr eine vollst\u00e4ndige Liberal- isierung der Laden\u00f6ffnungszeiten?",
            "Hier gilt der Grundsatz der Eigenver- antwortung und Selbstbestimmung des Unternehmens! Sind Sie f\u00fcr eine vollst\u00e4ndige Liberal- isierung der Laden\u00f6ffnungszeiten? W\u00fcrden Sie die Einf\u00fchrung einer Frauenquote in Verwaltungsr\u00e4ten b\u00f6rsenkotierter Unternehmen bef\u00fcr- worten? [The principle of personal responsibil- ity and corporate self-regulation applies here!] [Are you in favour of the complete lib- eralization of shop opening times?] [Would you support the introduction of a woman\u2019s quota for the Boards of Di- rectors of listed companies?] Table A8: Two comments that imply a positive stance towards one target issue but a negative stance towards another target issue. Such cases can be found in the dataset because respondents have copy-pasted some comments. These examples have been extracted from the training set.",
            "B Data Statement Curation rationale In order to study the automatic detection of stances on political issues, questions and candidate responses on the voting advice application smartvote.ch were downloaded. Mainly data pertaining to national-level issues were included to reduce variability. Language variety The training set consists of questions and answers in Swiss Standard German and Swiss French (74.1% de-CH; 25.9% fr-CH). The test sets also contain questions and answers in Swiss Italian (67.1% de-CH; 24.7% fr-CH; 8.2% it-CH). The questions have also been translated into English. Speaker demographic (answers) \u2022 Candidates for communal, cantonal or national elections in Switzerland who have \ufb01lled out an online questionnaire. \u2022 Age: 18 or older \u2013 mixed. \u2022 Gender: Unknown \u2013 mixed. \u2022 Race\/ethnicity: Unknown \u2013 mixed. \u2022 Native language: Unknown \u2013 mixed. \u2022 Socioeconomic status: Unknown \u2013 mixed. \u2022 Different speakers represented: 7581. \u2022 Presence of disordered speech: Unknown. Speech situation \u2022 The questions were edited and translated by political scientists for a public voting advice website.",
            "\u2022 Native language: Unknown \u2013 mixed. \u2022 Socioeconomic status: Unknown \u2013 mixed. \u2022 Different speakers represented: 7581. \u2022 Presence of disordered speech: Unknown. Speech situation \u2022 The questions were edited and translated by political scientists for a public voting advice website. \u2022 The answers were written between 2011 and 2020 by the users of the website. Text characteristics Questions, answers, arguments and comments regarding political issues."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-2003.08385.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 10132.000122070312,
    "avg_doclen_est": 180.92857360839844
}
