{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "arXiv:1911.01680v2  [cs.CL]  30 May 2020 Improving Slot Filling by Utilizing Contextual Information Amir Pouran Ben Veyseh*1, Franck Dernoncourt2, and Thien Huu Nguyen1 1Department of Computer and Information Science, University of Oregon, Eugene, Oregon, USA 2Adobe Research, San Jose, CA, USA {apouranb, thien}@cs.uoregon.edu franck.dernoncourt@adobe.com Abstract Slot Filling (SF) is one of the sub-tasks of Spo- ken Language Understanding (SLU) which aims to extract semantic constituents from a given natural language utterance. It is formu- lated as a sequence labeling task. Recently, it has been shown that contextual information is vital for this task. However, existing models employ contextual information in a restricted manner, e.g., using self-attention. Such meth- ods fail to distinguish the effects of the con- text on the word representation and the word label.",
      "Recently, it has been shown that contextual information is vital for this task. However, existing models employ contextual information in a restricted manner, e.g., using self-attention. Such meth- ods fail to distinguish the effects of the con- text on the word representation and the word label. To address this issue, in this paper, we propose a novel method to incorporate the con- textual information in two different levels, i.e., representation level and task-speci\ufb01c (i.e., la- bel) level. Our extensive experiments on three benchmark datasets on SF show the effective- ness of our model leading to new state-of-the- art results on all three benchmark datasets for the task of SF. 1 Introduction Slot Filling (SF) is the task of identifying the se- mantic constituents expressed in natural language utterance. It is one of the sub-tasks of spoken lan- guage understanding (SLU) and plays a vital role in personal assistant tools such as Siri, Alexa, and Goolge Assistant. This task is formulated as a sequence labeling problem.",
      "It is one of the sub-tasks of spoken lan- guage understanding (SLU) and plays a vital role in personal assistant tools such as Siri, Alexa, and Goolge Assistant. This task is formulated as a sequence labeling problem. For instance, in the given sentence \u201cPlay Signe Anderson chant music that is newest.\u201d, the goal is to identify \u201cSigne An- derson\u201d as \u201cartist\u201d, \u201cchant music\u201d as \u201cmusic-item\u201d and \u201cnewest\u201d as \u201csort\u201d. Early work on SF has employed fea- ture engineering methods to train statistical models, e.g., Conditional Random Field (Raymond and Riccardi, 2007). Later, deep learning emerged as a promising approach for SF * This work was done when the \ufb01rst author was an intern at Adobe Research. (Yao et al., 2014; Peng et al., 2015; Liu and Lane, 2016). The success of deep models could be attributed to pre-trained word embeddings to gen- eralize words and deep learning architectures to compose the word embeddings to induce effective representations.",
      "(Yao et al., 2014; Peng et al., 2015; Liu and Lane, 2016). The success of deep models could be attributed to pre-trained word embeddings to gen- eralize words and deep learning architectures to compose the word embeddings to induce effective representations. In addition to improving word representation using deep models, (Liu and Lane, 2016) showed that incorporating the context of each word into its representation could improve the results. Concretely, the effect of using context in word representation is two-fold: (1) Representation Level: As the meaning of the word is dependent on its context, incorporating the contextual information is vital to represent the true meaning of the word in the sentence (2) Task Level: For SF, the label of the word is related to the other words in the sentence and providing information about the other words, in prediction layer, could improve the performance. Unfortunately, the existing work employs the context in a restricted manner, e.g., via attention mechanism, which obfuscates the model about the two aforementioned effects of the contextual information.",
      "Unfortunately, the existing work employs the context in a restricted manner, e.g., via attention mechanism, which obfuscates the model about the two aforementioned effects of the contextual information. In order to address the limitations of the prior work to exploit the context for SF, in this paper, we propose a multi-task setting to train the model. More speci\ufb01cally, our model is encouraged to ex- plicitly ensure the two aforementioned effects of the contextual information for the task of SF. In particular, in addition to the main sequence label- ing task, we introduce new sub-tasks to ensure each effect. Firstly, in the representation level, we enforce the consistency between the word repre- sentations and its context. This enforcement is achieved via increasing the Mutual Information (MI) between these two representations. Secondly, in the task level, we propose two new sub-tasks: (1) To predict the label of the word solely from its context and (2) To predict which labels exist",
      "in the given sentence in a multi-label classi\ufb01cation setting. By doing so, we encourage the model to encode task-speci\ufb01c features in the context of each word. Our extensive experiments on three bench- mark datasets, empirically prove the effectiveness of the proposed model leading to new the state-of- the-art results on all three datasets. 2 Related Work In the literature, Slot Filling (SF), is categorized as one of the sub-tasks of spoken language under- standing (SLU). Early work employed feature en- gineering for statistical models, e.g., Conditional Random Field (Raymond and Riccardi, 2007). Due to the lack of generalisation ability of fea- ture based models, deep learning based models su- perseded them (Yao et al., 2014; Peng et al., 2015; Kurata et al., 2016; Hakkani-T\u00a8ur et al., 2016).",
      "Also, joint models to simultaneously predict the intent of the utterance and to extract the semantic slots has also gained a lot of attention (Guo et al., 2014; Liu and Lane, 2016; Zhang and Wang, 2016; Wang et al., 2018; Goo et al., 2018; Qin et al., 2019; E et al., 2019). In addition to the supervised setting, recently other setting such as progressive learning (Shen et al., 2019) or zero-shot learning has also been studied (Shah et al., 2019). To the best of our knowledge, none of the existing work introduces a multi-task learning solely for the SF to incorporate the con- textual information in both representation and task levels. 3 Model Our model is trained in a multi-task setting in which the main task is slot \ufb01lling to identify the best possible sequence of labels for the given sen- tence. In the \ufb01rst auxiliary task we aim to increase consistency between the word representation and its context. The second auxiliary task is to en- hance task speci\ufb01c information in contextual infor- mation.",
      "In the \ufb01rst auxiliary task we aim to increase consistency between the word representation and its context. The second auxiliary task is to en- hance task speci\ufb01c information in contextual infor- mation. In this section, we explain each of these tasks in more details. 3.1 Slot Filling Formally, the input to SF model is a sequence of words X = [x1, x2, . . . , xn] and our goal is to pre- dict the sequence of labels Y = [y1, y2, . . . , yn]. In our model, the word xi is represented by vec- tor ei which is the concatenation of the pre-trained word embedding and POS tag embedding of the word xi. In order to obtain a more abstract repre- sentation of the words, we employ a Bi-directional Long Short-Term Memory (BiLSTM) over the word representations E = [e1, e2, . . . , en] to gen- erate the abstract vectors H = [h1, h2, . . . , hn].",
      ". . , en] to gen- erate the abstract vectors H = [h1, h2, . . . , hn]. The vector hi is the \ufb01nal representation of the word xi and is fed into a two-layer feed forward neural net to compute the label scores si for the given word, si = FF(hi). As the task of SF is formulated as a sequence labeling task, we exploit a conditional random \ufb01eld (CRF) layer as the \ufb01- nal layer of SF prediction. More speci\ufb01cally, the predicted label scores S = [s1, s2, . . . , sn] are pro- vided as emission score to the CRF layer to predict the label sequence \u02c6Y = [\u02c6y1, \u02c6y2, . . . , \u02c6yn]. To train the model, the negative log-likelihood is used as the loss function for SF prediction, i.e., Lpred. 3.2 Consistency between Word and Context In this sub-task we aim to increase the consistency between the word representation and its context.",
      ". , \u02c6yn]. To train the model, the negative log-likelihood is used as the loss function for SF prediction, i.e., Lpred. 3.2 Consistency between Word and Context In this sub-task we aim to increase the consistency between the word representation and its context. To obtain the context of each word, we use max pooling over the outputs of the BiLSTM for all words of the sentence excluding the word itself, hc i = MaxPooling(h1, h2, ..., hn/hi). We aim to increase the consistency between vectors hi and hc i. To this end, we propose to maximize the Mu- tual Information (MI) between the word represen- tation and its context. In information theory, MI evaluates how much information we know about one random variable if the value of another vari- able is revealed.",
      "In information theory, MI evaluates how much information we know about one random variable if the value of another vari- able is revealed. Formally, the mutual information between two random variable X1 and X2 is ob- tained by: MI(X1, X2) = Z X1 Z X2 P(X1, X2)\u2217 log P(X1, X2) P(X1)P(X2)dX1dX2 (1) Using this de\ufb01nition of MI, we can reformu- late the MI equation as KL-Divergence between the joint distribution PX1X2 = P(X1, X2) and the product of marginal distributions PX1 N X2 = P(X1)P(X2): MI(X1, X2) = DKL(PX1X2||PX1 N X2) (2) Based on this understanding of MI, if the two random variables are dependent then the mutual in- formation between them (i.e. the KL-Divergence",
      "in equation 2) would be the highest. Consequently, if the representations hi and hc i are encouraged to have large mutual information, we expect them to share more information. Computing the KL-Divergence in equation 2 could be prohibitively expensive (Belghazi et al., 2018), so we need to estimate it. To this end, we exploit the adversarial method introduced in (Hjelm et al., 2019). In this method, a discrim- inator is employed to distinguish between sam- ples from the joint distribution and the product of the marginal distributions to estimate the KL- Divergence in equation 2. In our case, the sam- ple from joint distribution is the concatenation [hi : hc i] and the sample from the product of the marginal distribution is the concatenation [hi : hc j] where hc j is a context vector randomly chosen from the words in the mini-batch. Formally: Ldisc = 1 n\u03a3n i=1 \u2212(log(D[h, hc i])+ log(1 \u2212D([hi, hc j]))) (3) Where D is the discriminator.",
      "Formally: Ldisc = 1 n\u03a3n i=1 \u2212(log(D[h, hc i])+ log(1 \u2212D([hi, hc j]))) (3) Where D is the discriminator. This loss is added to the \ufb01nal loss function of the model. 3.3 Prediction by Contextual Information In addition to increasing consistency between the word representation and its context representation, we aim to increase the task-speci\ufb01c information in contextual representations. To this end, we train the model on two auxiliary tasks. The \ufb01rst one aims to use the context of each word to predict the label of that word. The goal of the second auxil- iary task is to use the global context information to predict sentence level labels. We describe each of these tasks in more details in the following sub- sections. Predicting Word Label In this sub-task, we use the context representa- tions of each word to predict its label. It will in- crease the information encoded in the context of the word about the label of the word. We use the same context vector hc i for the i-th word as described in the previous section.",
      "It will in- crease the information encoded in the context of the word about the label of the word. We use the same context vector hc i for the i-th word as described in the previous section. This vector is fed into a two-layer feed forward neural network with a softmax layer at the end to output the proba- bilities for each class, Pi(.|{x1, x2, ..., xn}/xi) = softmax(FF(hc i)). Finally, we use the following negative log-likelihood as the loss function to be optimized during training: Lwp = 1 n\u03a3n i=1 \u2212log(Pi(yi|{x1, x2, ..., xn}/xi)) (4) Predicting Sentence Labels The word label prediction enforces the context of each word to contain information about its label but it lacks a global view about the entire sen- tence. In order to increase the global informa- tion about the sentence in the representation of the words, we aim to predict the labels existing in a sentence from the representations of its words. More speci\ufb01cally, we introduce a new sub-task to predict which labels exit in the given sentence.",
      "In order to increase the global informa- tion about the sentence in the representation of the words, we aim to predict the labels existing in a sentence from the representations of its words. More speci\ufb01cally, we introduce a new sub-task to predict which labels exit in the given sentence. We formulate this task as a multi-label classi\ufb01cation problem. Formally, for each sentence, we predict the binary vector Y s = [ys 1, ys 2, ..., ys |L|] where L is the set of all possible word labels. In the vector Y s, ys i is 1 if the sentence X contains i-th label from the label set L otherwise it is 0. To predict vector Y s, we \ufb01rst compute the rep- resentation of the sentence. This representation is obtained by max pooling over the outputs of the BiLSTM, H = MaxPooling(h1, h2, ..., hn).",
      "To predict vector Y s, we \ufb01rst compute the rep- resentation of the sentence. This representation is obtained by max pooling over the outputs of the BiLSTM, H = MaxPooling(h1, h2, ..., hn). Af- terwards, the vector H is fed into a two-layer feed forward neural net with a sigmoid activation func- tion at the end to compute the probability distribu- tion of Y s(i.e., Pk(.|x1, x2, ..., xn) = \u03c3k(FF(H)) for k-th label in L). Note that since this task is a multi-label classi\ufb01cation, the number of neurons at the \ufb01nal layer is equal to |L|.",
      "Note that since this task is a multi-label classi\ufb01cation, the number of neurons at the \ufb01nal layer is equal to |L|. We optimize the following binary cross-entropy loss: Lsp = 1 |L|\u03a3|L| k=1 \u2212(ys k \u2217log(Pk(ys k|x1, x2, ..., xn))+ (1 \u2212ys k) \u2217log(1 \u2212Pk(ys k|x1, x2, ..., xn))) (5) Finally, to train the entire model we optimize the following combined loss: L = Lpred + \u03b1Ldiscr + \u03b2Lwp + \u03b3Lsp (6) where \u03b1, \u03b2 and \u03b3 are the trade-off parameters to be tuned based on the development set performance. 4 Experiments 4.1 Dataset and Parameters We evaluate our model on three SF datasets. Namely, we employ ATIS (Hemphill et al.,",
      "1990), SNIPS (Coucke et al., 2018) and EditMe (Manuvinakurike et al., 2018). ATIS and SNIPS are two widely adopted SF dataset and EditMe is a SF dataset for editing images with four slot labels (i.e., Action, Object, Attribute and Value). The statistics of the datasets are presented in the Appendix A. Based on the experiments on EditMe development set, the following parameters are selected: GloVe embedding with 300 dimensions to initialize word embedding ; 200 dimensions for the all hidden layers in LSTM and feed forward neural net; 0.1 for trade-off parameters \u03b1, \u03b2 and \u03b3; and Adam optimizer with learning rate 0.001. Following previous work, we use F1-score to evaluate the model. 4.2 Baselines We compare our model with other deep learning based models for SF.",
      "Following previous work, we use F1-score to evaluate the model. 4.2 Baselines We compare our model with other deep learning based models for SF. Namely, we compare the pro- posed model with Joint Seq (Hakkani-T\u00a8ur et al., 2016), Attention-Based (Liu and Lane, 2016), Sloted-Gated (Goo et al., 2018), SF-ID (E et al., 2019), CAPSULE-NLU (Zhang et al., 2019), and SPTID (Qin et al., 2019). Note that we com- pare our model with the single-task version of these baselines. We also compare our model with other sequence labeling models which are not speci\ufb01cally proposed for SF. Namely, we com- pare the model with CVT(Clark et al., 2018) and GCDT(Liu et al., 2019). CVT aims to improve in- put representation using improving partial views and GCDT exploits contextual information to en- hance word representations via concatenation of context and word representation.",
      "CVT aims to improve in- put representation using improving partial views and GCDT exploits contextual information to en- hance word representations via concatenation of context and word representation. 4.3 Results Table 1 reports the performance of the model and baselines. The proposed model outperforms all baselines in all datasets. Among all baselines, GCDT achieves best results on two out of three datasets. This superiority shows the importance of explicitly incorporating the contextual informa- tion into word representation for SF. However, the proposed model improve the performance substan- tially on all datasets by explicitly encouraging the consistency between word and its context in repre- sentation level and task-speci\ufb01c (i.e., label) level. Also, Table 1 shows that EditMe dataset is more challenging than the other datasets, despite fewer slot types it has.",
      "Also, Table 1 shows that EditMe dataset is more challenging than the other datasets, despite fewer slot types it has. This dif\ufb01culty could be ad- dressed by the limited number of training exam- ples and more diversity in sentence structures in Model SNIPS ATIS EditMe Joint Seq(2016) 87.3 94.3 - Attention-Based(2016) 87.8 94.2 - Sloted-Gated(2018) 89.2 95.4 84.9 SF-ID(2019) 90.9 95.5 85.2 CAPSULE-NLU(2019) 91.8 95.2 84.6 SPTID(2019) 90.8 95.1 85.3 CVT(2018) 91.4 94.8 85.4 GCDT(2019) 92.0 95.1 85.6 Ours 93.6 95.8 87.2 Table 1: Performance of the model and baselines on the Test sets.",
      "Model SNIPS ATIS EditMe Full 93.6 95.8 87.2 Full - MI 92.9 95.3 84.2 Full - WP 91.7 94.9 83.2 Full - SP 92.5 95.2 84.1 Table 2: Test F1-score for the ablated models this dataset. 4.4 Ablation Study Our model consists of three major components: (1) MI: Increasing mutual information between word and its context representations (2) WP: Pre- dicting the label of the word using its context to in- crease word level task-speci\ufb01c information in the word context (3) SP: Predicting which labels exist in the given sentence in a multi-label classi\ufb01cation to increase sentence level task-speci\ufb01c informa- tion in word representations. In order to analyze the contribution of each of these components, we also evaluate the model performance when we re- move one of the components and retrain the model. The results are reported in Table 2. This Table shows that all components are required for the model to have its best performance.",
      "In order to analyze the contribution of each of these components, we also evaluate the model performance when we re- move one of the components and retrain the model. The results are reported in Table 2. This Table shows that all components are required for the model to have its best performance. Among all components, the word level prediction using the contextual information has the major contribution to the model performance. This fact shows that contextual information trained to be informative about the \ufb01nal task is necessary to obtain the rep- resentations which could boost the performance. 5 Conclusion In this work, we introduced a new deep model for the task of Slot Filling (SF). In a multi-task set- ting, our model increases the mutual information between the word representation and its context, improves label information in the context and pre- dicts which concepts are expressed in the given",
      "sentence. Our experiments on three benchmark datasets show the effectiveness of our model by achieving the state-of-the-art results on all datasets for the SF task. References Mohamed Ishmael Belghazi, Aristide Baratin, Sai Ra- jeswar, Sherjil Ozair, Yoshua Bengio, R. Devon Hjelm, and Aaron C. Courville. 2018. Mutual in- formation neural estimation. In ICML. Kevin Clark, Minh-Thang Luong, Christopher D Man- ning, and Quoc V Le. 2018. Semi-supervised sequence modeling with cross-view training. In EMNLP. Alice Coucke, Alaa Saade, Adrien Ball, Th\u00b4eodore Bluche, Alexandre Caulier, David Leroy, Cl\u00b4ement Doumouro, Thibault Gisselbrecht, Francesco Calta- girone, Thibaut Lavril, and et al. 2018. Snips voice platform: an embedded spoken language understand- ing system for private-by-design voice interfaces. In arXiv. Haihong E, Peiqing Niu, Zhongfu Chen, and Meina Song.",
      "2018. Snips voice platform: an embedded spoken language understand- ing system for private-by-design voice interfaces. In arXiv. Haihong E, Peiqing Niu, Zhongfu Chen, and Meina Song. 2019. A novel bi-directional interrelated model for joint intent detection and slot \ufb01lling. In ACL. Chih-Wen Goo, Guang Gao, Yun-Kai Hsu, Chih-Li Huo, Tsung-Chieh Chen, Keng-Wei Hsu, and Yun- Nung Chen. 2018. Slot-gated modeling for joint slot \ufb01lling and intent prediction. In NAACL-HLT. Daniel Guo, Gokhan Tur, Wen-tau Yih, and Geoffrey Zweig. 2014. Joint semantic utterance classi\ufb01cation and slot \ufb01lling with recursive neural networks. In SLT. Dilek Hakkani-T\u00a8ur, G\u00a8okhan T\u00a8ur, Asli Celikyilmaz, Yun-Nung Chen, Jianfeng Gao, Li Deng, and Ye- Yi Wang.",
      "In SLT. Dilek Hakkani-T\u00a8ur, G\u00a8okhan T\u00a8ur, Asli Celikyilmaz, Yun-Nung Chen, Jianfeng Gao, Li Deng, and Ye- Yi Wang. 2016. Multi-domain joint semantic frame parsing using bi-directional rnn-lstm. In Inter- speech. Charles T. Hemphill, John J. Godfrey, and George R. Doddington. 1990. The ATIS spoken language sys- tems pilot corpus. In Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania, June 24-27,1990. R Devon Hjelm, Alex Fedorov, Samuel Lavoie- Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. 2019. Learning deep representations by mutual information estimation and maximization. In ICLR. Gakuto Kurata, Bing Xiang, Bowen Zhou, and Mo Yu. 2016. Leveraging sentence-level information with encoder LSTM for semantic slot \ufb01lling. In EMNLP.",
      "Learning deep representations by mutual information estimation and maximization. In ICLR. Gakuto Kurata, Bing Xiang, Bowen Zhou, and Mo Yu. 2016. Leveraging sentence-level information with encoder LSTM for semantic slot \ufb01lling. In EMNLP. Bing Liu and Ian Lane. 2016. Attention-based recur- rent neural network models for joint intent detection and slot \ufb01lling. In arXiv. Yijin Liu, Fandong Meng, Jinchao Zhang, Jinan Xu, Yufeng Chen, and Jie Zhou. 2019. GCDT: A global context enhanced deep transition architecture for se- quence labeling. In ACL. Ramesh Manuvinakurike, Jacqueline Brixey, Trung Bui, Walter Chang, Doo Soon Kim, Ron Artstein, and Kallirroi Georgila. 2018. Edit me: A corpus and a framework for understanding natural languag In Proceedings of the Eleventh International Con- ference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).",
      "2018. Edit me: A corpus and a framework for understanding natural languag In Proceedings of the Eleventh International Con- ference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA). Baolin Peng, Kaisheng Yao, Li Jing, and Kam-Fai Wong. 2015. Recurrent neural networks with exter- nal memory for spoken language understanding. In NLPCC. Libo Qin, Wanxiang Che, Yangming Li, Haoyang Wen, and Ting Liu. 2019. A stack-propagation frame- work with token-level intent detection for spoken language understanding. In EMNLP. Christian Raymond and Giuseppe Riccardi. 2007. Gen- erative and discriminative algorithms for spoken lan- guage understanding. In ISCA. Darsh J Shah, Raghav Gupta, Amir A Fayazi, and Dilek Hakkani-Tur. 2019. Robust zero-shot cross-domain slot \ufb01lling with example values. arXiv. Yilin Shen, Xiangyu Zeng, and Hongxia Jin.",
      "2019. Robust zero-shot cross-domain slot \ufb01lling with example values. arXiv. Yilin Shen, Xiangyu Zeng, and Hongxia Jin. 2019. A progressive model to enable continual learning for semantic slot \ufb01lling. In EMNLP. Yu Wang, Yilin Shen, and Hongxia Jin. 2018. A bi- model based RNN semantic frame parsing model for intent detection and slot \ufb01lling. In NAANCL-HLT. Kaisheng Yao, Baolin Peng, Yu Zhang, Dong Yu, Ge- offrey Zweig, and Yangyang Shi. 2014. Spoken lan- guage understanding using long short-term memory neural networks. In SLT. Chenwei Zhang, Yaliang Li, Nan Du, Wei Fan, and Philip Yu. 2019. Joint slot \ufb01lling and intent detec- tion via capsule neural networks. In ACL. Xiaodong Zhang and Houfeng Wang. 2016. A joint model of intent determination and slot \ufb01lling for spo- ken language understanding.",
      "2019. Joint slot \ufb01lling and intent detec- tion via capsule neural networks. In ACL. Xiaodong Zhang and Houfeng Wang. 2016. A joint model of intent determination and slot \ufb01lling for spo- ken language understanding. In IJCAI.",
      "A Dataset Statistics In our experiments, we employ three bench- mark datasets, ATIS, SNIPS and EditMe. Table 3 presents the statistics of these three datasets. Moreover, in order to provide more insight into the EditMe dataset, we report the labels statistics of this dataset in Table 4. Dataset Train Dev Test SNIPS 13,084 700 700 ATIS 4,478 500 893 EditMe 1,737 497 559 Table 3: Total number of examples in test/dev/train splits of the datasets Label Train Dev Test Action 1,562 448 479 Object 4,676 1,447 1,501 Attribute 1,437 403 462 Value 507 207 155 Table 4: Label Statistics of EditMe dataset"
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1911.01680.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":5403,
  "avg_doclen":174.2903225806,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1911.01680.pdf"
    }
  }
}