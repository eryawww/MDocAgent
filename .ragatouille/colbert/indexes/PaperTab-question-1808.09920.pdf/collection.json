[
  "Question Answering by Reasoning Across Documents with Graph Convolutional Networks Nicola De Cao University of Edinburgh University of Amsterdam nicola.decao@gmail.com Wilker Aziz University of Amsterdam w.aziz@uva.nl Ivan Titov University of Edinburgh University of Amsterdam ititov@inf.ed.ac.uk Abstract Most research in reading comprehension has focused on answering questions based on in- dividual documents or even single paragraphs. We introduce a neural model which integrates and reasons relying on information spread within documents and across multiple docu- ments. We frame it as an inference problem on a graph. Mentions of entities are nodes of this graph while edges encode relations between different mentions (e.g., within- and cross- document coreference). Graph convolutional networks (GCNs) are applied to these graphs and trained to perform multi-step reasoning. Our Entity-GCN method is scalable and com- pact, and it achieves state-of-the-art results on a multi-document question answering dataset, WIKIHOP (Welbl et al., 2018). 1 Introduction The long-standing goal of natural language under- standing is the development of systems which can acquire knowledge from text collections.",
  "1 Introduction The long-standing goal of natural language under- standing is the development of systems which can acquire knowledge from text collections. Fresh in- terest in reading comprehension tasks was sparked by the availability of large-scale datasets, such as SQuAD (Rajpurkar et al., 2016) and CNN/Daily Mail (Hermann et al., 2015), enabling end-to-end training of neural models (Seo et al., 2017; Xiong et al., 2017; Shen et al., 2017). These systems, given a text and a question, need to answer the query relying on the given document. Recently, it has been observed that most questions in these datasets do not require reasoning across the doc- ument, but they can be answered relying on in- formation contained in a single sentence (Weis- senborn et al., 2017).",
  "Recently, it has been observed that most questions in these datasets do not require reasoning across the doc- ument, but they can be answered relying on in- formation contained in a single sentence (Weis- senborn et al., 2017). The last generation of large-scale reading comprehension datasets, such as a NarrativeQA (Ko\u02c7cisk\u00b4y et al., 2018), Trivi- aQA (Joshi et al., 2017), and RACE (Lai et al., 2017), have been created in such a way as to ad- dress this shortcoming and to ensure that systems query:\u00a0country\u00a0Thorildsplan\u00a0 candidates:\u00a0{Denmark,\u00a0Finland,\u00a0Sweden,\u00a0Italy,\u00a0...}\u00a0 answer:\u00a0Sweden\u00a0 Thorildsplan\u00a0is\u00a0a\u00a0small\u00a0park\u00a0in\u00a0Kristineberg\u00a0in\u00a0\u00a0 Stockholm,\u00a0named\u00a0in\u00a01925\u00a0after\u00a0the\u00a0writer\u00a0[..] Stockholm\u00a0is\u00a0the\u00a0capital\u00a0of\u00a0Sweden\u00a0\u00a0 and\u00a0the\u00a0most\u00a0populous\u00a0city\u00a0in\u00a0[..] Figure 1: A sample from WIKIHOP where multi-step reasoning and information combination from different documents is necessary to infer the correct answer.",
  "relying only on local information cannot achieve competitive performance. Even though these new datasets are challeng- ing and require reasoning within documents, many question answering and search applications re- quire aggregation of information across multiple documents. The WIKIHOP dataset (Welbl et al., 2018) was explicitly created to facilitate the devel- opment of systems dealing with these scenarios. Each example in WIKIHOP consists of a collec- tion of documents, a query and a set of candidate answers (Figure 1). Though there is no guaran- tee that a question cannot be answered by relying just on a single sentence, the authors ensure that it is answerable using a chain of reasoning crossing document boundaries. Though an important practical problem, the multi-hop setting has so far received little at- tention. The methods reported by Welbl et al. (2018) approach the task by merely concatenat- ing all documents into a single long text and train- ing a standard RNN-based reading comprehen- sion model, namely, BiDAF (Seo et al., 2017) and FastQA (Weissenborn et al., 2017).",
  "Docu- ment concatenation in this setting is also used in Weaver (Raison et al., 2018) and MHPGM (Bauer et al., 2018). The only published paper which arXiv:1808.09920v4  [cs.CL]  27 Sep 2022",
  "goes beyond concatenation is due to Dhingra et al. (2018), where they augment RNNs with jump-links corresponding to co-reference edges. Though these edges provide a structural bias, the RNN states are still tasked with passing the infor- mation across the document and performing multi- hop reasoning. Instead, we frame question answering as an inference problem on a graph representing the document collection. Nodes in this graph corre- spond to named entities in a document whereas edges encode relations between them (e.g., cross- and within-document coreference links or simply co-occurrence in a document). We assume that reasoning chains can be captured by propagat- ing local contextual information along edges in this graph using a graph convolutional network (GCN) (Kipf and Welling, 2017). The multi-document setting imposes scalabil- ity challenges. In realistic scenarios, a system needs to learn to answer a query for a given col- lection (e.g., Wikipedia or a domain-speci\ufb01c set of documents).",
  "The multi-document setting imposes scalabil- ity challenges. In realistic scenarios, a system needs to learn to answer a query for a given col- lection (e.g., Wikipedia or a domain-speci\ufb01c set of documents). In such scenarios one cannot af- ford to run expensive document encoders (e.g., RNN or transformer-like self-attention (Vaswani et al., 2017)), unless the computation can be pre- processed both at train and test time. Even if (similarly to WIKIHOP creators) one considers a coarse-to-\ufb01ne approach, where a set of potentially relevant documents is provided, re-encoding them in a query-speci\ufb01c way remains the bottleneck. In contrast to other proposed methods (e.g., (Dhingra et al., 2018; Raison et al., 2018; Seo et al., 2017)), we avoid training expensive document encoders. In our approach, only a small query encoder, the GCN layers and a simple feed-forward an- swer selection component are learned.",
  "In our approach, only a small query encoder, the GCN layers and a simple feed-forward an- swer selection component are learned. Instead of training RNN encoders, we use contextualized embeddings (ELMo) to obtain initial (local) rep- resentations of nodes. This implies that only a lightweight computation has to be performed on- line, both at train and test time, whereas the rest is preprocessed. Even in the somewhat contrived WIKIHOP setting, where fairly small sets of can- didates are provided, the model is at least 5 times faster to train than BiDAF.1 Interestingly, when we substitute ELMo with simple pre-trained word embeddings, Entity-GCN still performs on par 1When compared to the \u2018small\u2019 and hence fast BiDAF model reported in Welbl et al. (2018), which is 25% less ac- curate than our Entity-GCN. Larger RNN models are prob- lematic also because of GPU memory constraints. with many techniques that use expensive question- aware recurrent document encoders.",
  "(2018), which is 25% less ac- curate than our Entity-GCN. Larger RNN models are prob- lematic also because of GPU memory constraints. with many techniques that use expensive question- aware recurrent document encoders. Despite not using recurrent document encoders, the full Entity-GCN model achieves over 2% im- provement over the best previously-published re- sults. As our model is ef\ufb01cient, we also reported results of an ensemble which brings further 3.6% of improvement and only 3% below the human performance reported by Welbl et al. (2018). Our contributions can be summarized as follows: \u2022 we present a novel approach for multi-hop QA that relies on a (pre-trained) document encoder and information propagation across multiple documents using graph neural net- works; \u2022 we provide an ef\ufb01cient training technique which relies on a slower of\ufb02ine and a faster on-line computation that does not require ex- pensive document processing; \u2022 we empirically show that our algorithm is ef- fective, presenting an improvement over pre- vious results. 2 Method In this section we explain our method.",
  "2 Method In this section we explain our method. We \ufb01rst introduce the dataset we focus on, WIKIHOP by Welbl et al. (2018), as well as the task ab- straction. We then present the building blocks that make up our Entity-GCN model, namely, an en- tity graph used to relate mentions to entities within and across documents, a document encoder used to obtain representations of mentions in context, and a relational graph convolutional network that propagates information through the entity graph. 2.1 Dataset and Task Abstraction Data The WIKIHOP dataset comprises of tuples \u27e8q, Sq, Cq, a\u22c6\u27e9where: q is a query/question, Sq is a set of supporting documents, Cq is a set of candi- date answers (all of which are entities mentioned in Sq), and a\u22c6\u2208Cq is the entity that correctly answers the question. WIKIHOP is assembled as- suming that there exists a corpus and a knowledge base (KB) related to each other.",
  "WIKIHOP is assembled as- suming that there exists a corpus and a knowledge base (KB) related to each other. The KB contains triples \u27e8s, r, o\u27e9where s is a subject entity, o an ob- ject entity, and r a unidirectional relation between them. Welbl et al. (2018) used WIKIPEDIA as cor- pus and WIKIDATA (Vrande\u02c7ci\u00b4c, 2012) as KB. The KB is only used for constructing WIKIHOP: Welbl",
  "et al. (2018) retrieved the supporting documents Sq from the corpus looking at mentions of subject and object entities in the text. Note that the set Sq (not the KB) is provided to the QA system, and not all of the supporting documents are relevant for the query but some of them act as distractors. Queries, on the other hand, are not expressed in natural lan- guage, but instead consist of tuples \u27e8s, r, ?\u27e9where the object entity is unknown and it has to be in- ferred by reading the support documents. There- fore, answering a query corresponds to \ufb01nding the entity a\u22c6that is the object of a tuple in the KB with subject s and relation r among the provided set of candidate answers Cq. Task The goal is to learn a model that can iden- tify the correct answer a\u22c6from the set of support- ing documents Sq. To that end, we exploit the available supervision to train a neural network that computes scores for candidates in Cq. We estimate the parameters of the architecture by maximizing the likelihood of observations. For prediction, we then output the candidate that achieves the high- est probability.",
  "To that end, we exploit the available supervision to train a neural network that computes scores for candidates in Cq. We estimate the parameters of the architecture by maximizing the likelihood of observations. For prediction, we then output the candidate that achieves the high- est probability. In the following, we present our model discussing the design decisions that enable multi-step reasoning and an ef\ufb01cient computation. 2.2 Reasoning on an Entity Graph Entity graph In an of\ufb02ine step, we organize the content of each training instance in a graph con- necting mentions of candidate answers within and across supporting documents. For a given query q = \u27e8s, r, ?\u27e9, we identify mentions in Sq of the en- tities in Cq \u222a{s} and create one node per mention. This process is based on the following heuristic: 1. we consider mentions spans in Sq exactly matching an element of Cq \u222a{s}. Admit- tedly, this is a rather simple strategy which may suffer from low recall.",
  "This process is based on the following heuristic: 1. we consider mentions spans in Sq exactly matching an element of Cq \u222a{s}. Admit- tedly, this is a rather simple strategy which may suffer from low recall. 2. we use predictions from a coreference reso- lution system to add mentions of elements in Cq \u222a{s} beyond exact matching (including both noun phrases and anaphoric pronouns). In particular, we use the end-to-end corefer- ence resolution by Lee et al. (2017). 3. we discard mentions which are ambiguously resolved to multiple coreference chains; this may sacri\ufb01ce recall, but avoids propagating ambiguity. Figure 2: Supporting documents (dashed ellipses) or- ganized as a graph where nodes are mentions of ei- ther candidate entities or query entities. Nodes with the same color indicates they refer to the same entity (ex- act match, coreference or both).",
  "Figure 2: Supporting documents (dashed ellipses) or- ganized as a graph where nodes are mentions of ei- ther candidate entities or query entities. Nodes with the same color indicates they refer to the same entity (ex- act match, coreference or both). Nodes are connected by three simple relations: one indicating co-occurrence in the same document (solid edges), another connect- ing mentions that exactly match (dashed edges), and a third one indicating a coreference (bold-red line). To each node vi, we associate a continuous an- notation xi \u2208RD which represents an entity in the context where it was mentioned (details in Sec- tion 2.3). We then proceed to connect these men- tions i) if they co-occur within the same document (we will refer to this as DOC-BASED edges), ii) if the pair of named entity mentions is identical (MATCH edges\u2014these may connect nodes across and within documents), or iii) if they are in the same coreference chain, as predicted by the exter- nal coreference system (COREF edges).",
  "Note that MATCH edges when connecting mentions in the same document are mostly included in the set of edges predicted by the coreference system. Hav- ing the two types of edges lets us distinguish be- tween less reliable edges provided by the coref- erence system and more reliable (but also more sparse) edges given by the exact-match heuristic. We treat these three types of connections as three different types of relations. See Figure 2 for an illustration. In addition to that, and to prevent hav- ing disconnected graphs, we add a fourth type of relation (COMPLEMENT edge) between any two nodes that are not connected with any of the other relations. We can think of these edges as those in the complement set of the entity graph with re- spect to a fully connected graph. Multi-step reasoning Our model then ap- proaches multi-step reasoning by transforming node representations (Section 2.3 for details) with a differentiable message passing algorithm that propagates information through the entity",
  "graph. The algorithm is parameterized by a graph convolutional network (GCN) (Kipf and Welling, 2017), in particular, we employ relational-GCNs (Schlichtkrull et al., 2018), an ex- tended version that accommodates edges of differ- ent types. In Section 2.4 we describe the propaga- tion rule. Each step of the algorithm (also referred to as a hop) updates all node representations in parallel. In particular, a node is updated as a function of messages from its direct neighbours, and a mes- sage is possibly speci\ufb01c to a certain relation. At the end of the \ufb01rst step, every node is aware of ev- ery other node it connects directly to. Besides, the neighbourhood of a node may include mentions of the same entity as well as others (e.g., same- document relation), and these mentions may have occurred in different documents. Taking this idea recursively, each further step of the algorithm al- lows a node to indirectly interact with nodes al- ready known to their neighbours.",
  "Taking this idea recursively, each further step of the algorithm al- lows a node to indirectly interact with nodes al- ready known to their neighbours. After L layers of R-GCN, information has been propagated through paths connecting up to L + 1 nodes. We start with node representations {h(0) i }N i=1, and transform them by applying L layers of R- GCN obtaining {h(L) i }N i=1. Together with a rep- resentation q of the query, we de\ufb01ne a distribution over candidate answers and we train maximizing the likelihood of observations. The probability of selecting a candidate c \u2208Cq as an answer is then P(c|q, Cq, Sq) \u221dexp \u0012 max i\u2208Mc fo([q, h(L) i ]) \u0013 , (1) where fo is a parameterized af\ufb01ne transforma- tion, and Mc is the set of node indices such that i \u2208Mc only if node vi is a mention of c. The max operator in Equation 1 is necessary to select the node with highest predicted probability since a candidate answer is realized in multiple locations via different nodes.",
  "2.3 Node Annotations Keeping in mind we want an ef\ufb01cient model, we encode words in supporting documents and in the query using only a pre-trained model for contex- tualized word representations rather than training our own encoder. Speci\ufb01cally, we use ELMo2 (Pe- ters et al., 2018), a pre-trained bi-directional lan- 2The use of ELMo is an implementation choice, and, in principle, any other contextual pre-trained model could be used (Radford et al., 2018; Devlin et al., 2019). guage model that relies on character-based input representation. ELMo representations, differently from other pre-trained word-based models (e.g., word2vec (Mikolov et al., 2013) or GloVe (Pen- nington et al., 2014)), are contextualized since each token representation depends on the entire text excerpt (i.e., the whole sentence). We choose not to \ufb01ne tune nor propagate gradi- ents through the ELMo architecture, as it would have de\ufb01ed the goal of not having specialized RNN encoders.",
  "We choose not to \ufb01ne tune nor propagate gradi- ents through the ELMo architecture, as it would have de\ufb01ed the goal of not having specialized RNN encoders. In the experiments, we will also ablate the use of ELMo showing how our model behaves using non-contextualized word represen- tations (we use GloVe). Documents pre-processing ELMo encodings are used to produce a set of representations {xi}N i=1, where xi \u2208RD denotes the ith candidate mention in context. Note that these representa- tions do not depend on the query yet and no train- able model was used to process the documents so far, that is, we use ELMo as a \ufb01xed pre-trained en- coder. Therefore, we can pre-compute representa- tion of mentions once and store them for later use. Query-dependent mention encodings ELMo encodings are used to produce a query represen- tation q \u2208RK as well. Here, q is a concatena- tion of the \ufb01nal outputs from a bidirectional RNN layer trained to re-encode ELMo representations of words in the query.",
  "Here, q is a concatena- tion of the \ufb01nal outputs from a bidirectional RNN layer trained to re-encode ELMo representations of words in the query. The vector q is used to com- pute a query-dependent representation of mentions {\u02c6xi}N i=1 as well as to compute a probability distri- bution over candidates (as in Equation 1). Query- dependent mention encodings \u02c6xi = fx(q, xi) are generated by a trainable function fx which is pa- rameterized by a feed-forward neural network. 2.4 Entity Relational Graph Convolutional Network Our model uses a gated version of the original R-GCN propagation rule. At the \ufb01rst layer, all hidden node representation are initialized with the query-aware encodings h(0) i = \u02c6xi.",
  "2.4 Entity Relational Graph Convolutional Network Our model uses a gated version of the original R-GCN propagation rule. At the \ufb01rst layer, all hidden node representation are initialized with the query-aware encodings h(0) i = \u02c6xi. Then, at each layer 0 \u2264\u2113\u2264L, the update message u(\u2113) i to the ith node is a sum of a transformation fs of the cur- rent node representation h(\u2113) i and transformations of its neighbours: u(\u2113) i = fs(h(\u2113) i ) + 1 |Ni| X j\u2208Ni X r\u2208Rij fr(h(\u2113) j ) , (2)",
  "where Ni is the set of indices of nodes neighbour- ing the ith node, Rij is the set of edge annotations between i and j, and fr is a parametrized func- tion speci\ufb01c to an edge type r \u2208R. Recall the available relations from Section 2.2, namely, R = {DOC-BASED, MATCH, COREF, COMPLEMENT}. A gating mechanism regulates how much of the update message propagates to the next step. This provides the model a way to prevent completely overwriting past information. Indeed, if all neces- sary information to answer a question is present at a layer which is not the last, then the model should learn to stop using neighbouring information for the next steps. Gate levels are computed as a(\u2113) i = \u03c3 \u0010 fa \u0010 [u(\u2113) i , h(\u2113) i ] \u0011\u0011 , (3) where \u03c3(\u00b7) is the sigmoid function and fa a parametrized transformation.",
  "Gate levels are computed as a(\u2113) i = \u03c3 \u0010 fa \u0010 [u(\u2113) i , h(\u2113) i ] \u0011\u0011 , (3) where \u03c3(\u00b7) is the sigmoid function and fa a parametrized transformation. Ultimately, the up- dated representation is a gated combination of the previous representation and a non-linear transfor- mation of the update message: h(\u2113+1) i = \u03c6(u(\u2113) i ) \u2299a(\u2113) i + h(\u2113) i \u2299(1 \u2212a(\u2113) i ) , (4) where \u03c6(\u00b7) is any nonlinear function (we used tanh) and \u2299stands for element-wise multiplica- tion. All transformations f\u2217are af\ufb01ne and they are not layer-dependent (since we would like to use as few parameters as possible to decrease model complexity promoting ef\ufb01ciency and scalability). 3 Experiments In this section, we compare our method against re- cent work as well as preforming an ablation study using the WIKIHOP dataset (Welbl et al., 2018).",
  "3 Experiments In this section, we compare our method against re- cent work as well as preforming an ablation study using the WIKIHOP dataset (Welbl et al., 2018). See Appendix A in the supplementary material for a description of the hyper-parameters of our model and training details. WIKIHOP We use WIKIHOP for training, val- idation/development and test. The test set is not publicly available and therefore we measure per- formance on the validation set in almost all ex- periments. WIKIHOP has 43,738/ 5,129/ 2,451 query-documents samples in the training, valida- tion and test sets respectively for a total of 51,318 samples. Authors constructed the dataset as de- scribed in Section 2.1 selecting samples with a graph traversal up to a maximum chain length of 3 documents (see Table 1 for additional dataset statistics). WIKIHOP comes in two versions, a Min Max Avg. Median # candidates 2 79 19.8 14 # documents 3 63 13.7 11 # tokens/doc.",
  "WIKIHOP comes in two versions, a Min Max Avg. Median # candidates 2 79 19.8 14 # documents 3 63 13.7 11 # tokens/doc. 4 2,046 100.4 91 Table 1: WIKIHOP dataset statistics from Welbl et al. (2018): number of candidates and documents per sam- ple and document length. standard (unmasked) one and a masked one. The masked version was created by the authors to test whether methods are able to learn lexical abstrac- tion. In this version, all candidates and all men- tions of them in the support documents are re- placed by random but consistent placeholder to- kens. Thus, in the masked version, mentions are always referred to via unambiguous surface forms. We do not use coreference systems in the masked version as they rely crucially on lexical realization of mentions and cannot operate on masked tokens. 3.1 Comparison In this experiment, we compare our Enitity- GCN against recent prior work on the same task.",
  "We do not use coreference systems in the masked version as they rely crucially on lexical realization of mentions and cannot operate on masked tokens. 3.1 Comparison In this experiment, we compare our Enitity- GCN against recent prior work on the same task. We present test and development re- sults (when present) for both versions of the dataset in Table 2. From Welbl et al. (2018), we list an oracle based on human performance as well as two standard reading comprehension models, namely BiDAF (Seo et al., 2017) and FastQA (Weissenborn et al., 2017). We also com- pare against Coref-GRU (Dhingra et al., 2018), MHPGM (Bauer et al., 2018), and Weaver (Rai- son et al., 2018). Additionally, we include results of MHQA-GRN (Song et al., 2018), from a recent arXiv preprint describing concurrent work. They jointly train graph neural networks and recurrent encoders.",
  "Additionally, we include results of MHQA-GRN (Song et al., 2018), from a recent arXiv preprint describing concurrent work. They jointly train graph neural networks and recurrent encoders. We report single runs of our two best single models and an ensemble one on the un- masked test set (recall that the test set is not pub- licly available and the task organizers only report unmasked results) as well as both versions of the validation set. Entity-GCN (best single model without coref- erence edges) outperforms all previous work by over 2% points. We additionally re-ran BiDAF baseline to compare training time: when using a single Titan X GPU, BiDAF and Entity-GCN pro- cess 12.5 and 57.8 document sets per second, re- spectively. Note that Welbl et al. (2018) had to use BiDAF with very small state dimensionalities",
  "Model Unmasked Masked Test Dev Test Dev Human (Welbl et al., 2018) 74.1 \u2013 \u2013 \u2013 FastQA (Welbl et al., 2018) 25.7 \u2013 35.8 \u2013 BiDAF (Welbl et al., 2018) 42.9 \u2013 54.5 \u2013 Coref-GRU (Dhingra et al., 2018) 59.3 56.0 \u2013 \u2013 MHPGM (Bauer et al., 2018) \u2013 58.2 \u2013 \u2013 Weaver / Jenga (Raison et al., 2018) 65.3 64.1 \u2013 \u2013 MHQA-GRN (Song et al., 2018) 65.4 62.8 \u2013 \u2013 Entity-GCN without coreference (single model) 67.6 64.8 \u2013 70.5 Entity-GCN with coreference (single model) 66.4 65.3 \u2013 \u2013 Entity-GCN* (ensemble 5 models) 71.2 68.5 \u2013 71.6 Table 2: Accuracy of different models on WIKIHOP closed test set and public validation set.",
  "Our Entity-GCN outperforms recent prior work without learning any language model to process the input but relying on a pre- trained one (ELMo \u2013 without \ufb01ne-tunning it) and applying R-GCN to reason among entities in the text. * with coreference for unmasked dataset and without coreference for the masked one. (20), and smaller batch size due to the scalabil- ity issues (both memory and computation costs). We compare applying the same reductions.3 Even- tually, we also report an ensemble of 5 indepen- dently trained models. All models are trained on the same dataset splits with different weight ini- tializations. The ensemble prediction is obtained as arg max c 5Q i=1 Pi(c|q, Cq, Sq) from each model. 3.2 Ablation Study To help determine the sources of improvements, we perform an ablation study using the publicly available validation set (see Table 3). We per- form two groups of ablation, one on the embed- ding layer, to study the effect of ELMo, and one on the edges, to study how different relations af- fect the overall model performance.",
  "We per- form two groups of ablation, one on the embed- ding layer, to study the effect of ELMo, and one on the edges, to study how different relations af- fect the overall model performance. Embedding ablation We argue that ELMo is crucial, since we do not rely on any other context encoder. However, it is interesting to explore how our R-GCN performs without it. Therefore, in this experiment, we replace the deep contextualized embeddings of both the query and the nodes with GloVe (Pennington et al., 2014) vectors (insensi- tive to context). Since we do not have any compo- nent in our model that processes the documents, we expect a drop in performance. In other words, in this ablation our model tries to answer questions 3Besides, we could not run any other method we com- pare with combined with ELMo without reducing the dimen- sionality further or having to implement a distributed version. without reading the context at all.",
  "In other words, in this ablation our model tries to answer questions 3Besides, we could not run any other method we com- pare with combined with ELMo without reducing the dimen- sionality further or having to implement a distributed version. without reading the context at all. For example, in Figure 1, our model would be aware that \u201cStock- holm\u201d and \u201cSweden\u201d appear in the same document but any context words, including the ones encod- ing relations (e.g., \u201cis the capital of\u201d) will be hid- den. Besides, in the masked case all mentions be- come \u2018unknown\u2019 tokens with GloVe and therefore the predictions are equivalent to a random guess. Once the strong pre-trained encoder is out of the way, we also ablate the use of our R-GCN com- ponent, thus completely depriving the model from inductive biases that aim at multi-hop reasoning.",
  "Once the strong pre-trained encoder is out of the way, we also ablate the use of our R-GCN com- ponent, thus completely depriving the model from inductive biases that aim at multi-hop reasoning. The \ufb01rst important observation is that replacing ELMo by GloVe (GloVe with R-GCN in Table 3) still yields a competitive system that ranks far above baselines from (Welbl et al., 2018) and even above the Coref-GRU of Dhingra et al. (2018), in terms of accuracy on (unmasked) validation set. The second important observation is that if we then remove R-GCN (GloVe w/o R-GCN in Ta- ble 3), we lose 8.0 points. That is, the R-GCN component pushes the model to perform above Coref-GRU still without accessing context, but rather by updating mention representations based on their relation to other ones. These results high- light the impact of our R-GCN component.",
  "That is, the R-GCN component pushes the model to perform above Coref-GRU still without accessing context, but rather by updating mention representations based on their relation to other ones. These results high- light the impact of our R-GCN component. Graph edges ablation In this experiment we in- vestigate the effect of the different relations avail- able in the entity graph and processed by the R- GCN module. We start off by testing our stronger encoder (i.e., ELMo) in absence of edges connect- ing mentions in the supporting documents (i.e., us-",
  "Model unmasked masked full (ensemble) 68.5 71.6 full (single) 65.1 \u00b1 0.11 70.4 \u00b1 0.12 GloVe with R-GCN 59.2 11.1 GloVe w/o R-GCN 51.2 11.6 No R-GCN 62.4 63.2 No relation types 62.7 63.9 No DOC-BASED 62.9 65.8 No MATCH 64.3 67.4 No COREF 64.8 \u2013 No COMPLEMENT 64.1 70.3 Induced edges 61.5 56.4 Table 3: Ablation study on WIKIHOP validation set. The full model is our Entity-GCN with all of its com- ponents and other rows indicate models trained without a component of interest. We also report baselines using GloVe instead of ELMo with and without R-GCN. For the full model we report mean \u00b11 std over 5 runs. ing only self-loops \u2013 No R-GCN in Table 3).",
  "We also report baselines using GloVe instead of ELMo with and without R-GCN. For the full model we report mean \u00b11 std over 5 runs. ing only self-loops \u2013 No R-GCN in Table 3). The results suggest that WIKIPHOP genuinely requires multihop inference, as our best model is 6.1% and 8.4% more accurate than this local model, in un- masked and masked settings, respectively.4 How- ever, it also shows that ELMo representations cap- ture predictive context features, without being ex- plicitly trained for the task. It con\ufb01rms that our goal of getting away with training expensive doc- ument encoders is a realistic one. We then inspect our model\u2019s effectiveness in making use of the structure encoded in the graph. We start naively by fully-connecting all nodes within and across documents without distinguish- ing edges by type (No relation types in Table 3).",
  "We then inspect our model\u2019s effectiveness in making use of the structure encoded in the graph. We start naively by fully-connecting all nodes within and across documents without distinguish- ing edges by type (No relation types in Table 3). We observe only marginal improvements with re- spect to ELMo alone (No R-GCN in Table 3) in both the unmasked and masked setting suggest- ing that a GCN operating over a naive entity graph would not add much to this task and a more infor- mative graph construction and/or a more sophisti- cated parameterization is indeed needed. Next, we ablate each type of relations inde- pendently, that is, we either remove connections of mentions that co-occur in the same docu- ment (DOC-BASED), connections between men- tions matching exactly (MATCH), or edges pre- dicted by the coreference system (COREF). The 4Recall that all models in the ensemble use the same lo- cal representations, ELMo. \ufb01rst thing to note is that the model makes better use of DOC-BASED connections than MATCH or COREF connections.",
  "The 4Recall that all models in the ensemble use the same lo- cal representations, ELMo. \ufb01rst thing to note is that the model makes better use of DOC-BASED connections than MATCH or COREF connections. This is mostly because i) the majority of the connections are indeed between mentions in the same document, and ii) without connecting mentions within the same document we remove important information since the model is unaware they appear closely in the document. Secondly, we notice that coreference links and complement edges seem to play a more marginal role. Though it may be surprising for coreference edges, recall that the MATCH heuristic already cap- tures the easiest coreference cases, and for the rest the out-of-domain coreference system may not be reliable. Still, modelling all these different rela- tions together gives our Entity-GCN a clear advan- tage. This is our best system evaluating on the de- velopment. Since Entity-GCN seems to gain little advantage using the coreference system, we report test results both with and without using it. Surpris- ingly, with coreference, we observe performance degradation on the test set.",
  "This is our best system evaluating on the de- velopment. Since Entity-GCN seems to gain little advantage using the coreference system, we report test results both with and without using it. Surpris- ingly, with coreference, we observe performance degradation on the test set. It is likely that the test documents are harder for the coreference system.5 We do perform one last ablation, namely, we re- place our heuristic for assigning edges and their labels by a model component that predicts them. The last row of Table 3 (Induced edges) shows model performance when edges are not predeter- mined but predicted. For this experiment, we use a bilinear function fe(\u02c6xi, \u02c6xj) = \u03c3 \u0000\u02c6x\u22a4 i We\u02c6xj \u0001 that predicts the importance of a single edge connect- ing two nodes i, j using the query-dependent rep- resentation of mentions (see Section 2.3). The performance drops below \u2018No R-GCN\u2019 suggesting that it cannot learn these dependencies on its own.",
  "The performance drops below \u2018No R-GCN\u2019 suggesting that it cannot learn these dependencies on its own. Most results are stronger for the masked set- tings even though we do not apply the coreference resolution system in this setting due to masking. It is not surprising as coreferred mentions are la- beled with the same identi\ufb01er in the masked ver- sion, even if their original surface forms did not match (Welbl et al. (2018) used WIKIPEDIA links for masking). Indeed, in the masked version, an entity is always referred to via the same unique surface form (e.g., MASK1) within and across doc- uments. In the unmasked setting, on the other hand, mentions to an entity may differ (e.g., \u201cUS\u201d vs \u201cUnited States\u201d) and they might not be retrieved by the coreference system we are employing, mak- 5Since the test set is hidden from us, we cannot analyze this difference further.",
  "Relation Accuracy P@2 P@5 Avg. |Cq| Supports overall (ensemble) 68.5 81.0 94.1 20.4 \u00b1 16.6 5129 overall (single model) 65.3 79.7 92.9 20.4 \u00b1 16.6 5129 3 best member of political party 85.5 95.7 98.6 5.4 \u00b1 2.4 70 record label 83.0 93.6 99.3 12.4 \u00b1 6.1 283 publisher 81.5 96.3 100.0 9.6 \u00b1 5.1 54 3 worst place of birth 51.0 67.2 86.8 27.2 \u00b1 14.5 309 place of death 50.0 67.3 89.1 25.1 \u00b1 14.3 159 inception 29.9 53.2 83.1 21.9 \u00b1 11.0 77 Table 4: Accuracy and precision at K (P@K in the table) analysis overall and per query type.",
  "Avg. |Cq| indicates the average number of candidates with one standard deviation. ing the task harder for all models. Therefore, as we rely mostly on exact matching when constructing our graph for the masked case, we are more effec- tive in recovering coreference links on the masked rather than unmasked version.6 4 Error Analysis In this section we provide an error analysis for our best single model predictions. First of all, we look at which type of questions our model per- forms well or poorly. There are more than 150 query types in the validation set but we \ufb01ltered the three with the best and with the worst accu- racy that have at least 50 supporting documents and at least 5 candidates. We show results in Ta- ble 4. We observe that questions regarding places (birth and death) are considered harder for Entity- GCN.",
  "We show results in Ta- ble 4. We observe that questions regarding places (birth and death) are considered harder for Entity- GCN. We then inspect samples where our model fails while assigning highest likelihood and no- ticed two principal sources of failure i) a mismatch between what is written in WIKIPEDIA and what is annotated in WIKIDATA, and ii) a different degree of granularity (e.g., born in \u201cLondon\u201d vs \u201cUK\u201d could be considered both correct by a human but not when measuring accuracy). See Table 6 in the supplement material for some reported samples. Secondly, we study how the model performance degrades when the input graph is large. In particu- lar, we observe a negative Pearson\u2019s correlation (- 0.687) between accuracy and the number of candi- date answers. However, the performance does not decrease steeply. The distribution of the number of candidates in the dataset peaks at 5 and has an av- erage of approximately 20. Therefore, the model 6Though other systems do not explicitly link matching mentions, they similarly bene\ufb01t from masking (e.g., masks essentially single out spans that contain candidate answers).",
  "Therefore, the model 6Though other systems do not explicitly link matching mentions, they similarly bene\ufb01t from masking (e.g., masks essentially single out spans that contain candidate answers). does not see many samples where there are a large number of candidate entities during training. Dif- ferently, we notice that as the number of nodes in the graph increases, the model performance drops but more gently (negative but closer to zero Pear- son\u2019s correlation). This is important as document sets can be large in practical applications. See Fig- ure 3 in the supplemental material for plots. 5 Related Work In previous work, BiDAF (Seo et al., 2017), FastQA (Weissenborn et al., 2017), Coref- GRU (Dhingra et al., 2018), MHPGM (Bauer et al., 2018), and Weaver / Jenga (Raison et al., 2018) have been applied to multi-document ques- tion answering. The \ufb01rst two mainly focus on sin- gle document QA and Welbl et al. (2018) adapted both of them to work with WIKIHOP.",
  "The \ufb01rst two mainly focus on sin- gle document QA and Welbl et al. (2018) adapted both of them to work with WIKIHOP. They pro- cess each instance of the dataset by concatenat- ing all d \u2208Sq in a random order adding doc- ument separator tokens. They trained using the \ufb01rst answer mention in the concatenated document and evaluating exact match at test time. Coref- GRU, similarly to us, encodes relations between entity mentions in the document. Instead of us- ing graph neural network layers, as we do, they augment RNNs with jump links corresponding to pairs of corefereed mentions. MHPGM uses a multi-attention mechanism in combination with external commonsense relations to perform mul- tiple hops of reasoning. Weaver is a deep co- encoding model that uses several alternating bi- LSTMs to process the concatenated documents and the query. Graph neural networks have been shown suc- cessful on a number of NLP tasks (Marcheggiani and Titov, 2017; Bastings et al., 2017; Zhang et al.,",
  "2018a), including those involving document level modeling (Peng et al., 2017). They have also been applied in the context of asking questions about knowledge contained in a knowledge base (Zhang et al., 2018b). In Schlichtkrull et al. (2018), GCNs are used to capture reasoning chains in a knowl- edge base. Our work and unpublished concurrent work by Song et al. (2018) are the \ufb01rst to study graph neural networks in the context of multi- document QA. Besides differences in the architec- ture, Song et al. (2018) propose to train a combi- nation of a graph recurrent network and an RNN encoder. We do not train any RNN document en- coders in this work. 6 Conclusion We designed a graph neural network that oper- ates over a compact graph representation of a set of documents where nodes are mentions to en- tities and edges signal relations such as within and cross-document coreference. The model learns to answer questions by gathering evidence from different documents via a differentiable mes- sage passing algorithm that updates node repre- sentations based on their neighbourhood.",
  "The model learns to answer questions by gathering evidence from different documents via a differentiable mes- sage passing algorithm that updates node repre- sentations based on their neighbourhood. Our model outperforms published results where abla- tions show substantial evidence in favour of multi- step reasoning. Moreover, we make the model fast by using pre-trained (contextual) embeddings. Acknowledgments We would like to thank Johannes Welbl for help- ing to test our system on WIKIHOP. This project is supported by SAP Innovation Center Network, ERC Starting Grant BroadSem (678254) and the Dutch Organization for Scienti\ufb01c Re- search (NWO) VIDI 639.022.518. Wilker Aziz is supported by the Dutch Organisation for Scienti\ufb01c Research (NWO) VICI Grant nr. 277-89-002. References Jasmijn Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Sima\u2019an. 2017. Graph convolutional encoders for syntax-aware neural ma- chine translation.",
  "277-89-002. References Jasmijn Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Sima\u2019an. 2017. Graph convolutional encoders for syntax-aware neural ma- chine translation. In Proceedings of the 2017 Con- ference on Empirical Methods in Natural Language Processing, pages 1957\u20131967, Copenhagen, Den- mark. Association for Computational Linguistics. Lisa Bauer, Yicheng Wang, and Mohit Bansal. 2018. Commonsense for generative multi-hop question an- swering tasks. In Proceedings of the 2018 Confer- ence on Empirical Methods in Natural Language Processing, pages 4220\u20134230, Brussels, Belgium. Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing.",
  "Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics. Bhuwan Dhingra, Qiao Jin, Zhilin Yang, William Co- hen, and Ruslan Salakhutdinov. 2018. Neural mod- els for reasoning over multiple mentions using coref- erence. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 2 (Short Papers), pages 42\u201348, New Orleans, Louisiana. Association for Computa- tional Linguistics.",
  "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 2 (Short Papers), pages 42\u201348, New Orleans, Louisiana. Association for Computa- tional Linguistics. Karl Moritz Hermann, Tom\u00b4as Kocisk\u00b4y, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su- leyman, and Phil Blunsom. 2015. Teaching ma- chines to read and comprehend. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Sys- tems 2015, December 7-12, 2015, Montreal, Que- bec, Canada, pages 1693\u20131701. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale dis- tantly supervised challenge dataset for reading com- prehension.",
  "Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale dis- tantly supervised challenge dataset for reading com- prehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), pages 1601\u2013 1611, Vancouver, Canada. Association for Compu- tational Linguistics. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd Inter- national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. Thomas N. Kipf and Max Welling. 2017. Semi- supervised classi\ufb01cation with graph convolutional networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Pro- ceedings. OpenReview.net.",
  "Semi- supervised classi\ufb01cation with graph convolutional networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Pro- ceedings. OpenReview.net. Tom\u00b4a\u02c7s Ko\u02c7cisk\u00b4y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00b4abor Melis, and Edward Grefenstette. 2018. The NarrativeQA read- ing comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317\u2013 328. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. RACE: Large-scale ReAd- ing comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical",
  "Methods in Natural Language Processing, pages 785\u2013794, Copenhagen, Denmark. Association for Computational Linguistics. Kenton Lee, Luheng He, Mike Lewis, and Luke Zettle- moyer. 2017. End-to-end neural coreference reso- lution. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Process- ing, pages 188\u2013197, Copenhagen, Denmark. Asso- ciation for Computational Linguistics. Diego Marcheggiani and Ivan Titov. 2017. Encoding sentences with graph convolutional networks for se- mantic role labeling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1506\u20131515, Copenhagen, Denmark. Association for Computational Linguis- tics. Tom\u00b4as Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Distributed rep- resentations of words and phrases and their com- positionality. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013.",
  "2013. Distributed rep- resentations of words and phrases and their com- positionality. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Pro- ceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States, pages 3111\u2013 3119. Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova, and Wen-tau Yih. 2017. Cross-sentence n-ary relation extraction with graph LSTMs. Trans- actions of the Association for Computational Lin- guistics, 5:101\u2013115. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for word representation. In Proceedings of the 2014 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, Doha, Qatar. Association for Computational Linguistics.",
  "2014. GloVe: Global vectors for word representation. In Proceedings of the 2014 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, Doha, Qatar. Association for Computational Linguistics. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. In Proceedings of the 2018 Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long Papers), pages 2227\u20132237, New Orleans, Louisiana. Association for Computational Linguistics. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing with unsupervised learning. Technical re- port, OpenAI. Martin Raison, Pierre-Emmanuel Mazar\u00b4e, Rajarshi Das, and Antoine Bordes. 2018.",
  "2018. Improving language under- standing with unsupervised learning. Technical re- port, OpenAI. Martin Raison, Pierre-Emmanuel Mazar\u00b4e, Rajarshi Das, and Antoine Bordes. 2018. Weaver: Deep co- encoding of questions and documents for machine reading. In Proceedings of the International Con- ference on Machine Learning (ICML). Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natu- ral Language Processing, pages 2383\u20132392, Austin, Texas. Association for Computational Linguistics. Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2018. Modeling relational data with graph convolu- tional networks. In The Semantic Web, pages 593\u2013 607, Cham. Springer International Publishing.",
  "2018. Modeling relational data with graph convolu- tional networks. In The Semantic Web, pages 593\u2013 607, Cham. Springer International Publishing. Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention \ufb02ow for machine comprehension. In 5th Inter- national Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Con- ference Track Proceedings. OpenReview.net. Yelong Shen, Po-Sen Huang, Jianfeng Gao, and Weizhu Chen. 2017. Reasonet: Learning to stop reading in machine comprehension. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Hali- fax, NS, Canada, August 13 - 17, 2017, pages 1047\u2013 1055. ACM. Linfeng Song, Zhiguo Wang, Mo Yu, Yue Zhang, Radu Florian, and Daniel Gildea. 2018.",
  "ACM. Linfeng Song, Zhiguo Wang, Mo Yu, Yue Zhang, Radu Florian, and Daniel Gildea. 2018. Exploring Graph-structured Passage Representation for Multi- hop Reading Comprehension with Graph Neural Networks. ArXiv preprint, abs/1809.02040. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from over\ufb01tting. The Journal of Machine Learning Research, 15(1):1929\u20131958. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4- 9, 2017, Long Beach, CA, USA, pages 5998\u20136008.",
  "2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4- 9, 2017, Long Beach, CA, USA, pages 5998\u20136008. Denny Vrande\u02c7ci\u00b4c. 2012. Wikidata: A new platform for collaborative data collection. In Proceedings of the 21st International Conference on World Wide Web, pages 1063\u20131064. ACM. Dirk Weissenborn, Georg Wiese, and Laura Seiffe. 2017. Making neural QA as simple as possible but not simpler. In Proceedings of the 21st Confer- ence on Computational Natural Language Learning (CoNLL 2017), pages 271\u2013280, Vancouver, Canada. Association for Computational Linguistics. Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. 2018. Constructing datasets for multi-hop reading comprehension across documents. Transac- tions of the Association for Computational Linguis- tics, 6:287\u2013302.",
  "Caiming Xiong, Victor Zhong, and Richard Socher. 2017. Dynamic coattention networks for ques- tion answering. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Pro- ceedings. OpenReview.net. Yuhao Zhang, Peng Qi, and Christopher D. Manning. 2018a. Graph convolution over pruned dependency trees improves relation extraction. In Proceedings of the 2018 Conference on Empirical Methods in Nat- ural Language Processing, pages 2205\u20132215, Brus- sels, Belgium. Association for Computational Lin- guistics. Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexan- der J. Smola, and Le Song. 2018b. Variational reasoning for question answering with knowledge graph.",
  "Association for Computational Lin- guistics. Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexan- der J. Smola, and Le Song. 2018b. Variational reasoning for question answering with knowledge graph. In Proceedings of the Thirty-Second AAAI Conference on Arti\ufb01cial Intelligence, (AAAI-18), the 30th innovative Applications of Arti\ufb01cial Intel- ligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Arti\ufb01cial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 6069\u20136076. AAAI Press.",
  "A Implementation and Experiments Details A.1 Architecture See table 5 for an outline of Entity-GCN architec- tural detail. Here the computational steps 1. ELMo embeddings are a concatenation of three 1024-dimensional vectors resulting in 3072-dimensional input vectors {xi}N i=1. 2. For the query representation q, we apply 2 bi-LSTM layers of 256 and 128 hidden units to its ELMo vectors. The concatenation of the forward and backward states results in a 256-dimensional question representation. 3. ELMo embeddings of candidates are pro- jected to 256-dimensional vectors, concate- nated to the q, and further transformed with a two layers MLP of 1024 and 512 hidden units in 512-dimensional query aware entity representations {\u02c6xi}N i=1 \u2208R512. 4. All transformations f\u2217in R-GCN-layers are af\ufb01ne and they do maintain the input and out- put dimensionality of node representations the same (512-dimensional). 5.",
  "4. All transformations f\u2217in R-GCN-layers are af\ufb01ne and they do maintain the input and out- put dimensionality of node representations the same (512-dimensional). 5. Eventually, a 2-layers MLP with [256, 128] hidden units takes the concatenation between {h(L) i }N i=1 and q to predict the probability that a candidate node vi may be the answer to the query q (see Equation 1). During preliminary trials, we experimented with different numbers of R-GCN-layers (in the range 1-7). We observed that with WIKIHOP, for L \u22653 models reach essentially the same perfor- mance, but more layers increase the time required to train them. Besides, we observed that the gating mechanism learns to keep more and more informa- tion from the past at each layer making unneces- sary to have more layers than required.",
  "Besides, we observed that the gating mechanism learns to keep more and more informa- tion from the past at each layer making unneces- sary to have more layers than required. A.2 Training Details We train our models with a batch size of 32 for at most 20 epochs using the Adam opti- mizer (Kingma and Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.999 and a learning rate of 10\u22124. To help against over\ufb01tting, we employ dropout (drop rate \u22080, 0.1, 0.15, 0.2, 0.25) (Srivastava et al., 2014) and early-stopping on validation accuracy. We re- port the best results of each experiment based on accuracy on validation set. B Error Analysis In Table 6, we report three samples from WIKI- HOP development set where out Entity-GCN fails. In particular, we show two instances where our model presents high con\ufb01dence on the answer, and one where is not.",
  "B Error Analysis In Table 6, we report three samples from WIKI- HOP development set where out Entity-GCN fails. In particular, we show two instances where our model presents high con\ufb01dence on the answer, and one where is not. We commented these sam- ples explaining why our model might fail in these cases. C Ablation Study In Figure 3, we show how the model performance goes when the input graph is large. In particular, how Entity-GCN performs as the number of can- didate answers or the number of nodes increases. 0 10 20 30 40 50 60 70 0.0 0.2 0.4 0.6 0.8 1.0 (a) Candidates set size (x-axis) and accuracy (y-axis). Pear- son\u2019s correlation of \u22120.687 (p < 10\u22127). 0 50 100 150 200 0.0 0.2 0.4 0.6 0.8 1.0 (b) Nodes set size (x-axis) and accuracy (y-axis).",
  "0 50 100 150 200 0.0 0.2 0.4 0.6 0.8 1.0 (b) Nodes set size (x-axis) and accuracy (y-axis). Pearson\u2019s correlation of \u22120.385 (p < 10\u22127). Figure 3: Accuracy (blue) of our best single model with respect to the candidate set size (on the top) and nodes set size (on the bottom) on the validation set. Re- scaled data distributions (orange) per number of candi- date (top) and nodes (bottom). Dashed lines indicate average accuracy.",
  "Input - q, {vi}N i=1 query ELMo 3072-dim candidates ELMo 3072-dim 2 layers bi-LSTM [256, 128]-dim 1 layer FF 256-dim concatenation 512-dim 2 layer FF [1024, 512]-dim: : {\u02c6xi}N i=1 3 layers R-GCN 512-dim each (shared parameters) concatenation with q 768-dim 3 layers FF [256,128,1]-dim Output - probabilities over Cq Table 5: Model architecture. ID WH dev 2257 Gold answer 2003 (p = 14.1) Query inception (of) Derrty Entertainment Predicted answer 2000 (p = 15.8) Support 1 Derrty Entertainment is a record label founded by [...]. The \ufb01rst album released under Derrty Entertainment was Nelly \u2019s Country Grammar. Support 2 Country Grammar is the debut single by American rapper Nelly. The song was pro- duced by Jason Epperson.",
  "The \ufb01rst album released under Derrty Entertainment was Nelly \u2019s Country Grammar. Support 2 Country Grammar is the debut single by American rapper Nelly. The song was pro- duced by Jason Epperson. It was released in 2000, [...] (a) In this example, the model predicts the answer correctly. However, there is a mismatch between what is written in WIKIPEDIA and what is annotated in WIKIDATA. In WIKIHOP, answers are generated with WIKIDATA. ID WH dev 2401 Gold answer Adolph Zukor (p = 7.1e\u22124%) Query producer (of) Forbidden Paradise Predicted answer Jesse L. Lask (p = 99.9%) Support 1 Forbidden Paradise is a [...] drama \ufb01lm produced by Famous Players-Lasky [...] Support 2 Famous Players-Lasky Corporation was [...] from the merger of Adolph Zukor\u2019s Fa- mous Players Film Company [..] and the Jesse L. Lasky Feature Play Company. (b) In this sample, there is ambiguity between two entities since both are correct answers reading the passages but only one is marked as correct.",
  "(b) In this sample, there is ambiguity between two entities since both are correct answers reading the passages but only one is marked as correct. The model fails assigning very high probability to only on one of them. ID WH dev 3030 Gold answer Scania (p = 0.029%) Query place of birth (of) Erik Penser Predicted answer Esl\u00a8ov (p = 97.3%) Support 1 Nils Wilhelm Erik Penser (born August 22, 1942, in Esl\u00a8ov, Sk\u02daane) is a Swedish [...] Support 2 Sk\u02daane County, sometimes referred to as \u201c Scania County \u201d in English, is the [...] (c) In this sample, there is ambiguity between two entities since the city Esl\u00a8ov is located in the Scania County (English name of Sk\u02daane County). The model assigning high probability to the city and it cannot select the county. Table 6: Samples from WIKIHOP set where Entity-GCN fails. p indicates the predicted likelihood."
]