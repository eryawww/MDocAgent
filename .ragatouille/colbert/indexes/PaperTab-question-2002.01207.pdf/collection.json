[
  "1 Arabic Diacritic Recovery Using a Feature-Rich biLSTM Model Kareem Darwish, Ahmed Abdelali, Hamdy Mubarak, Mohamed Eldesouki Qatar Computing Research Institute. Hamad Bin Khalifa University, Doha. Qatar {kdarwish,aabdelali,hmubarak,mohamohamed}@hbku.edu.qa ( Received 11 January 2022) Abstract Diacritics (short vowels) are typically omitted when writing Arabic text, and readers have to reintroduce them to correctly pronounce words. There are two types of Arabic diacritics: the \ufb01rst are core-word diacritics (CW), which specify the lexical selection, and the second are case endings (CE), which typically appear at the end of the word stem and generally specify their syntactic roles. Recovering CEs is relatively harder than recovering core-word diacritics due to inter-word dependencies, which are often distant. In this paper, we use a feature-rich recurrent neural network model that uses a variety of linguistic and surface- level features to recover both core word diacritics and case endings.",
  "In this paper, we use a feature-rich recurrent neural network model that uses a variety of linguistic and surface- level features to recover both core word diacritics and case endings. Our model surpasses all previous state-of-the-art systems with a CW error rate (CWER) of 2.86% and a CE error rate (CEER) of 3.7% for Modern Standard Arabic (MSA) and CWER of 2.2% and CEER of 2.5% for Classical Arabic (CA). When combining diacritized word cores with case endings, the resultant word error rate is 6.0% and 4.3% for MSA and CA respectively. This highlights the e\ufb00ectiveness of feature engineering for such deep neural models. 1 Introduction Modern Standard Arabic (MSA) and Classical Arabic (CA) have two types of vowels, namely long vowels, which are explicitly written, and short vowels, aka di- acritics, which are typically omitted in writing but are reintroduced by readers to properly pronounce words.",
  "1 Introduction Modern Standard Arabic (MSA) and Classical Arabic (CA) have two types of vowels, namely long vowels, which are explicitly written, and short vowels, aka di- acritics, which are typically omitted in writing but are reintroduced by readers to properly pronounce words. Since diacritics disambiguate the sense of the words in context and their syntactic roles in sentences, automatic diacritic recovery is es- sential for applications such as text-to-speech and educational tools for language learners, who may not know how to properly verbalize words. Diacritics have two types, namely: core-word (CW) diacritics, which are internal to words and specify lexical selection; and case-endings (CE), which appear on the last letter of word stems, typically specifying their syntactic role. For example, the word \u201cktb\u201d1 (I. \u0010J\u00bb) can have multiple diacritized forms such as \u201ckatab\u201d (I. \u000b\u0010J\u000b\u00bb \u2013 meaning \u201che wrote\u201d) \u201ckutub\u201d (I. \f\u0010J\f\u00bb \u2013 \u201cbooks\u201d).",
  "\u0010J\u00bb) can have multiple diacritized forms such as \u201ckatab\u201d (I. \u000b\u0010J\u000b\u00bb \u2013 meaning \u201che wrote\u201d) \u201ckutub\u201d (I. \f\u0010J\f\u00bb \u2013 \u201cbooks\u201d). While \u201ckatab\u201d can only assume one CE, namely \u201cfatHa\u201d (\u201ca\u201d), \u201ckutub\u201d can accept the CEs: \u201cdamma\u201d (\u201cu\u201d) (nominal \u2013 ex. subject), \u201ca\u201d 1 Buckwalter encoding is used in this paper Buckwalter (2002) arXiv:2002.01207v1  [cs.CL]  4 Feb 2020",
  "2 (accusative \u2013 ex. object), \u201ckasra\u201d (\u201ci\u201d) (genitive \u2013 ex. PP predicate), or their nuna- tions. There are 14 diacritic combinations. When used as CEs, they typically convey speci\ufb01c syntactic information, namely: fatHa \u201ca\u201d for accusative nouns, past verbs and subjunctive present verbs; kasra \u201ci\u201d for genitive nouns; damma \u201cu\u201d for nomi- native nouns and indicative present verbs; sukun \u201co\u201d for jussive present verbs and imperative verbs. FatHa, kasra and damma can be preceded by shadda \u201c\u223c\u201d for gemination (consonant doubling) and/or converted to nunation forms following some grammar rules. In addition, according to Arabic orthography and phonology, some words take a virtual (null) \u201c#\u201d marker when they end with certain charac- ters (ex: long vowels). This applies also to all non-Arabic words (ex: punctuation, digits, Latin words, etc.).",
  "In addition, according to Arabic orthography and phonology, some words take a virtual (null) \u201c#\u201d marker when they end with certain charac- ters (ex: long vowels). This applies also to all non-Arabic words (ex: punctuation, digits, Latin words, etc.). Generally, function words, adverbs and foreign named entities (NEs) have set CEs (sukun, fatHa or virtual). Similar to other Semitic languages, Arabic allows \ufb02exible Verb-Subject-Object as well as Verb-Object-Subject constructs (Attia 2008). Such \ufb02exibility creates in- herent ambiguity, which is resolved by diacritics as in \u201cr>Y Emr Ely\u201d (\u00fa \u00ce\u00ab Q\u00d4\u00ab \u00f8 @P Omar saw Ali/Ali saw Omar). In the absence of diacritics it is not clear who saw whom.",
  "In the absence of diacritics it is not clear who saw whom. Similarly, in the sub-sentence \u201ckAn Alm&tmr AltAsE\u201d (\u00a9\u0083A\u0010J\u00cb@ Q\u00d6\u0010\u00df \u00f1\u00d6\u00cf@ \t\u00e0A\u00bf), if the last word, is a predicate of the verb \u201ckAn\u201d, then the sentence would mean \u201cthis conference was the ninth\u201d and would receive a fatHa (a) as a case ending. Con- versely, if it was an adjective to the \u201cconference\u201d, then the sentence would mean \u201cthe ninth conference was ...\u201d and would receive a damma (u) as a case ending. Thus, a consideration of context is required for proper disambiguation. Due to the inter-word dependence of CEs, they are typically harder to predict compared to core-word diacritics (Habash and Rambow 2007, Roth et al. 2008, Harrat et al. 2013, Ameur et al. 2015), with CEER of state-of-the-art systems being in double digits compared to nearly 3% for word-cores.",
  "2008, Harrat et al. 2013, Ameur et al. 2015), with CEER of state-of-the-art systems being in double digits compared to nearly 3% for word-cores. Since recovering CEs is akin to shallow parsing (Marton et al. 2010) and requires morphological and syntactic processing, it is a di\ufb03cult problem in Arabic NLP. In this paper, we focus on recovering both CW diacritics and CEs. We employ two separate Deep Neural Network (DNN) ar- chitectures for recovering both kinds of diacritic types. We use character-level and word-level bidirectional Long-Short Term Memory (biLSTM) based recurrent neu- ral models for CW diacritic and CE recovery respectively. We train models for both Modern Standard Arabic (MSA) and Classical Arabic (CA). For CW diacritics, the model is informed using word segmentation information and a unigram language model. We also employ a unigram language model to perform post correction on the model output.",
  "We train models for both Modern Standard Arabic (MSA) and Classical Arabic (CA). For CW diacritics, the model is informed using word segmentation information and a unigram language model. We also employ a unigram language model to perform post correction on the model output. We achieve word error rates for CW diacritics of 2.9% and 2.2% for MSA and CA. The MSA word error rate is 6% lower than the best results in the literature (the RDI diacritizer (Rashwan et al. 2015)). The CE model is trained with a rich set of surface, morphological, and syntactic features. The proposed features would aid the biLSTM model in capturing syntactic dependencies indicated by Part-Of-Speech (POS) tags, gender and number features, morphological patterns, and a\ufb03xes. We show that our model achieves a case ending error rate (CEER) of 3.7% for MSA and 2.5% for CA.",
  "We show that our model achieves a case ending error rate (CEER) of 3.7% for MSA and 2.5% for CA. For MSA, this CEER is more than 60% lower than other state-of-the-art systems such as Farasa and the RDI diacritizer, which are",
  "Arabic Diacritic Recovery Using a Feature-Rich biLSTM Model 3 trained on the same dataset and achieve CEERs of 10.7% and 14.4% respectively. The contributions of this paper are as follows: \u2022 We employ a character-level RNN model that is informed using word mor- phological information and a word unigram language model to recover CW diacritics. Our model beats the best state-of-the-art system by 6% for MSA. \u2022 We introduce a new feature rich RNN-based CE recovery model that achieves errors rates that are 60% lower than the current state-of-the-art for MSA. \u2022 We explore the e\ufb00ect of di\ufb00erent features, which may potentially be exploited for Arabic parsing. \u2022 We show the e\ufb00ectiveness of our approach for both MSA and CA. 2 Background Automatic diacritics restoration has been investigated for many di\ufb00erent language such as European languages (e.g.",
  "\u2022 We show the e\ufb00ectiveness of our approach for both MSA and CA. 2 Background Automatic diacritics restoration has been investigated for many di\ufb00erent language such as European languages (e.g. Romanian (Mihalcea 2002, Tu\ufb01\u00b8s and Ceau\u00b8su 2008), French (Zweigenbaum and Grabar 2002), and Croatian (\u02c7Santi\u00b4c et al. 2009)), African languages (e.g. Yorba (Orife 2018)), Southeast Asian languages (e.g. Viet- namese (Luu and Yamamoto 2012)), Semitic language (e.g. Arabic and Hebrew (Gal 2002)), and many others (De Pauw et al. 2007). For many languages, diacritic (or accent restoration) is limited to a handful of letters. However, for Semitic languages, diacritic recovery extends to most letters.",
  "Arabic and Hebrew (Gal 2002)), and many others (De Pauw et al. 2007). For many languages, diacritic (or accent restoration) is limited to a handful of letters. However, for Semitic languages, diacritic recovery extends to most letters. Many general approaches have been ex- plored for this problem including linguistically motivated rule-based approaches, machine learning approaches, such as Hidden Markov Models (HMM) (Gal 2002) and Conditional Random Fields (CRF) (Darwish et al. 2018), and lately deep learn- ing approaches such as Arabic (Abandah et al. 2015, Hifny 2018, Mubarak et al. 2019), Slovak (Hucko and Lacko 2018), and Yorba (Orife 2018). Aside from rule-based approaches (El-Sadany and Hashish 1989), di\ufb00erent meth- ods were used to recover diacritics in Arabic text. Using a hidden Markov model (HMM) (Gal 2002, Elshafei et al.",
  "Aside from rule-based approaches (El-Sadany and Hashish 1989), di\ufb00erent meth- ods were used to recover diacritics in Arabic text. Using a hidden Markov model (HMM) (Gal 2002, Elshafei et al. 2006) with an input character sequence, the model attempts to \ufb01nd the best state sequence given previous observations. Gal (2002) reported a 14% word error rate (WER) while Elshafei et al. (2006) achieved a 4.1% diacritic error rate (DER) on the Quran (CA). Vergyri and Kirchho\ufb00(2004) com- bined both morphological, acoustic, and contextual features to build a diacritizer trained on FBIS and LDC CallHome ECA collections. They reported a 9% (DER) without CE, and 28% DER with CE. Nelken and Shieber (2005) employed a cascade of a \ufb01nite state transducers. The cascade stacked a word language model (LM), a charachter LM, and a morphological model.",
  "Nelken and Shieber (2005) employed a cascade of a \ufb01nite state transducers. The cascade stacked a word language model (LM), a charachter LM, and a morphological model. The model achieved an accuracy of 7.33% WER without CE and and 23.61% WER with CE. Zitouni et al. (2006) employed a maximum entropy model for sequence classi\ufb01cation. The system was trained on the LDCs Arabic Treebank (ATB) and evaluated on a 600 articles from An-Nahar Newspaper (340K words) and achieved 5.5% DER and 18% WER on words without CE. Bebah et al. (2014) used a hybrid approach that utilizes the output of Alkhalil morphological Analyzer (Mohamed Ould Abdallahi Ould et al. 2011) to generate",
  "4 all possible out of context diacritizations of a word. Then, an HMM guesses the correct diacritized form. Similarly, Microsoft Arabic Toolkit Services (ATKS) dia- critizer (Said et al. 2013) uses a rule-based morphological analyzer that produces possible analyses and an HMM in conjunction with rules to guess the most likely analysis. They report WER of 11.4% and 4.4% with and without CE. MADAMIRA (Pasha et al. 2014) uses a combinations of morpho-syntactic features to rank a list of potential analyses provided by the Buckwalter Arabic Morphological Analyzer (BAMA) (Buckwalter 2004). An SVM trained on ATB selects the most probable analysis, including the diacritized form. MADAMIRA achieves 19.0% and 6.7% WER with and without CE respectively (Darwish et al. 2017). Farasa (Darwish et al.",
  "An SVM trained on ATB selects the most probable analysis, including the diacritized form. MADAMIRA achieves 19.0% and 6.7% WER with and without CE respectively (Darwish et al. 2017). Farasa (Darwish et al. 2017) uses an HMM to guess CW diacritics and an SVM-rank based model trained on morphological and syntactic features to guess CEs. Farasa achieves WER of 12.8% and 3.3% with and without CEs. More recent work employed di\ufb00erent neural architectures to model the diacriti- zation problem. Abandah et al. (2015) used a biLSTM recurrent neural network trained on the same dataset as (Zitouni et al. 2006). They explored one, two and three BiLSTM layers with 250 nodes in each layers, achieving WER of 9.1% in- cluding CE on ATB. Similar architectures were used but achieved lower results (Rashwan et al. 2015, Belinkov and Glass 2015).",
  "Similar architectures were used but achieved lower results (Rashwan et al. 2015, Belinkov and Glass 2015). Azmi and Almajed (2015) pro- vide a comprehensive survey on Arabic diacritization. A more recent survey by Osama Hamed (2017) concluded that reported results are often incomparable due to the usage of di\ufb00erent test sets. They concluded that a large unigram LM for CW diacritic recovery is competitive with many of the systems in the literature, which prompted us to utilize a unigram language model for post correction. As mentioned earlier, two conclusions can be drawn, namely: restoring CEs is more challenging than CW diacritic restoration; and combining multiple features typically improves CE restoration. In this paper, we expand upon the work in the literature by introducing feature- rich DNN models for restoring both CW and CE diacritics. We compare our models to multiple systems on the same test set. We achieve results that reduce diacritiza- tion error rates by more than half compared to the best SOTA systems.",
  "We compare our models to multiple systems on the same test set. We achieve results that reduce diacritiza- tion error rates by more than half compared to the best SOTA systems. We further conduct an ablation study to determine the relative e\ufb00ect of the di\ufb00erent features. As for Arabic, it is a Semitic language with derivational morphology. Arabic nouns, adjectives, adverbs, and verbs are typically derived from a closed set of 10,000 roots of length 3, 4, or rarely 5. Arabic nouns and verbs are derived from roots by applying templates to the roots to generate stems. Such templates may carry information that indicate morphological features of words such POS tag, gender, and number. For example, given a 3-letter root with 3 consonants CCC, a valid template may be CwACC , where the in\ufb01x \u201cwA\u201d (@\u00f0) is inserted, this template typically indicates an Arabic broken, or irregular, plural template for a noun of template CACC or CACCp if masculine or feminine respectively.",
  "Further, stems may accept pre\ufb01xes and/or su\ufb03xes to form words. Pre\ufb01xes include coordinating conjunctions, determiner, and prepositions, and su\ufb03xes include attached pronouns and gender and number markers.",
  "Arabic Diacritic Recovery Using a Feature-Rich biLSTM Model 5 3 Our Diacritizer 3.1 Training and Test Corpora For MSA, we acquired the diacritized corpus that was used to train the RDI (Rash- wan et al. 2015) diacritizer and the Farasa diacritizer (Darwish et al. 2017). The corpus contains 9.7M tokens with approximately 194K unique surface forms (ex- cluding numbers and punctuation marks). The corpus covers multiple genres such as politics and sports and is a mix of MSA and CA. This corpus is considerably larger than the Arabic Treebank (Maamouri et al. 2004) and is more consistent in its diacritization. For testing, we used the freely available WikiNews test set (Dar- wish et al. 2017), which is composed of 70 MSA WikiNews articles (18,300 tokens) and evenly covers a variety of genres including politics, economics, health, science and technology, sports, arts and culture.",
  "2017), which is composed of 70 MSA WikiNews articles (18,300 tokens) and evenly covers a variety of genres including politics, economics, health, science and technology, sports, arts and culture. For CA, we obtained a large collection of fully diacritized classical texts (2.7M tokens) from a book publisher, and we held-out a small subset of 5,000 sentences (approximately 400k words) for testing. Then, we used the remaining sentences to train the CA models. 3.2 Core Word Diacritization Features. Arabic words are typically derived from a limited set of roots by \ufb01tting them into so-called stem-templates (producing stems) and may accept a variety of pre\ufb01xes and su\ufb03xes such as prepositions, determiners, and pronouns (producing words). Word stems specify the lexical selection and are typically una\ufb00ected by the attached a\ufb03xes. We used 4 feature types, namely: \u2022 CHAR: the characters. \u2022 SEG: the position of the character in a word segment.",
  "Word stems specify the lexical selection and are typically una\ufb00ected by the attached a\ufb03xes. We used 4 feature types, namely: \u2022 CHAR: the characters. \u2022 SEG: the position of the character in a word segment. For example, given the word \u201cwAlktAb\u201d (H. A\u0010J\u00ba\u00cb@\u00f0 and the book/writers), which is composed of 3 segments \u201cw+Al+ktAb\u201d (H. A\u0010J\u00ba+\u00cb@+\u00f0). Letters were marked as \u201cB\u201d if they begin a segment, \u201cM\u201d if they are in the middle of a segment, \u201cE\u201d if they end a segment, and \u201cS\u201d if they are single letter segments. So for \u201cw+Al+ktAb\u201d, the corresponding character positions are \u201cS+BE+BMME\u201d. We used Farasa to perform segmentation, which has a reported segmentation accuracy of 99% on the WikiNews dataset (Darwish and Mubarak 2016). \u2022 PRIOR: diacritics seen in the training set per segment.",
  "We used Farasa to perform segmentation, which has a reported segmentation accuracy of 99% on the WikiNews dataset (Darwish and Mubarak 2016). \u2022 PRIOR: diacritics seen in the training set per segment. Since we used a char- acter level model, this feature informed the model with word level information. For example, the word \u201cktAb\u201d (H. A\u0010J\u00bb) was observed to have two diacritized forms in the training set, namely \u201ckitaAb\u201d (H. A\u000b\u0010J\u00bb\u000b \u2013 book) and \u201ckut\u223caAb\u201d (H. A\u000b\u000f\u0010J\f\u00bb \u2013 writers). The \ufb01rst letter in the word (\u201ck\u201d) accepted the diacritics \u201ci\u201d and \u201cu\u201d. Thus given a binary vector representing whether a character is allowed to assume any of the eight primitive Arabic diacritic marks (a, i, u, o, K, N, F, and \u223cin order), the \ufb01rst letter would be given the following vector",
  "6 \u201c01100000\u201d. If a word segment was never observed during training, the vector for all letters therein would be set to 11111111. This feature borrows infor- mation from HMM models, which have been fairly successful in diacritizing word cores. \u2022 CASE: whether the letter expects a core word diacritic or a case ending. Case endings are placed on only one letter in a word, which may or may not be the last letter in the word. This is a binary feature. DNN Model. Using a DNN model, particularly with a biLSTM (Schuster and Paliwal 1997), is advantageous because the model automatically explores the space of feature com- binations and is able to capture distant dependencies. A number of studies have explored various biLSTM architectures (Abandah et al. 2015, Rashwan et al. 2015, Belinkov and Glass 2015) including stacked biLSTMs con\ufb01rming their e\ufb00ectiveness. As shown in Figure 1, we employed a character-based biLSTM model with associ- ated features for each character.",
  "2015, Belinkov and Glass 2015) including stacked biLSTMs con\ufb01rming their e\ufb00ectiveness. As shown in Figure 1, we employed a character-based biLSTM model with associ- ated features for each character. Every input character had an associated list of m features, and we trained randomly initialized embeddings of size 50 for each feature. Then, we concatenated the feature embeddings vectors creating an m \u00d7 50 vector for each character, which was fed into the biLSTM layer of length 100. The output of the biLSTM layer was fed directly into a dense layer of size 100. We used early stopping with patience of 5 epochs, a learning rate of 0.001, a batch size of 256, and an Adamax optimizer. The input was the character sequence in a sentence with words being separated by word boundary markers (WB), and we set the maximum sentence length to 1,250 characters. 3.3 Case Ending Diacritization Features. Table 1 lists the features that we used for CE recovery.",
  "The input was the character sequence in a sentence with words being separated by word boundary markers (WB), and we set the maximum sentence length to 1,250 characters. 3.3 Case Ending Diacritization Features. Table 1 lists the features that we used for CE recovery. We used Farasa to perform segmentation and POS tagging and to determine stem-templates (Darwish et al. 2017). Farasa has a reported POS accuracy of 96% on the WikiNews dataset Dar- wish et al. (2017). Though the Farasa diacritizer utilizes a combination of some the features presented herein, namely segmentation, POS tagging, and stem templates, Farasa\u2019s SVM-ranking approach requires explicit speci\ufb01cation of feature combina- tions (ex. Prob(CE\u2225current word, prev word, prev CE)). Manual exploration of the feature space is undesirable, and ideally we would want our learning algorithm to do so automatically.",
  "Prob(CE\u2225current word, prev word, prev CE)). Manual exploration of the feature space is undesirable, and ideally we would want our learning algorithm to do so automatically. The \ufb02exibility of the DNN model allowed us to include many more surface level features such as a\ufb03xes, leading and trailing characters in words and stems, and the presence of words in large gazetteers of named entities. As we show later, these additional features signi\ufb01cantly lowered CEER.",
  "Arabic Diacritic Recovery Using a Feature-Rich biLSTM Model 7 Fig. 1. DNN model for core word diacritics DNN Model Figure 2 shows the architecture of our DNN algorithm. Every input word had an associated list of n features, and we trained randomly initialized embeddings of size 100 for each feature. Then, we concatenated the feature embeddings vectors creating an n\u00d7100 vector for each word. We fed these vectors into a biLSTM layer of 100 dimensions after applying a dropout of 75%, where dropout behaves like a regularlizer to avoid over\ufb01tting (Hinton et al. 2012). We conducted side experiments with lower dropout rates, but the higher dropout rate worked best. The output of the biLSTM layer was fed into a 100 dimensional dense layer with 15% dropout and softmax activation. We conducted side experiments where we added additional biLSTM layers and replaced softmax with a conditional random \ufb01eld layer, but we did not observe improvements. Thus, we opted for a simpler model. We used a validation set to determine optimal parameters such as dropout rate.",
  "We conducted side experiments where we added additional biLSTM layers and replaced softmax with a conditional random \ufb01eld layer, but we did not observe improvements. Thus, we opted for a simpler model. We used a validation set to determine optimal parameters such as dropout rate. Again, we used the \u201cAdamax\u201d optimizer with categorical cross entropy loss and a learning rate of 0.001. We also applied early stopping with patience of up to 5 consecutive epochs without improvement. 4 Experiments and Results 4.1 Core Word Experimental Setup For all the experiments conducted herein, we used the Keras toolkit (Chollet et al. 2015) with a TensorFlow backend (Abadi et al. 2015). We used the entirety of the",
  "8 Feature Example Explanation and Motivation word w+b+mktb+t+nA (A\tJ+\u0010J+\u001c.\u0010J\u00ba\u00d2+K.+\u00f0 \u2013 and in our library) Some words have a \ufb01xed set of observed CEs word POS CONJ+PREP+NOUN +NSUFF+PRON Some POS combinations allow a closed set of CEs gender/number feminine/singular Gender/number agreement (dis)allow certain attach- ments and may allow/exclude certain CEs stem mktb+p (\u0010\u00e9+J.\u0010J\u00ba\u00d3 \u2013 li- brary) We attach gender and number noun su\ufb03xes such the singular feminine marker \u201cp\u201d (\u0010\u00e9\u0007) because CEs appear on them. stem POS NOUN+NSUFF Same rationale as word POS pre\ufb01x(es) & POS w+b+ (+H. +\u00f0) & CONJ+PREP Certain pre\ufb01xes a\ufb00ect CE directly. For example, the PREP \u201cb+\u201d (+H. )",
  "+\u00f0) & CONJ+PREP Certain pre\ufb01xes a\ufb00ect CE directly. For example, the PREP \u201cb+\u201d (+H. ) is a preposition causing their noun predicates to assume the genitive case su\ufb03x(es) & POS \u201c+nA\u201d (A\tK+) & PRON Certain su\ufb03xes a\ufb00ect CE directly stem template mfEl+p (\u0010\u00e9+\u00ca\u00aa\t\u00ae\u00d3 \u2013 de- rived from the root \u201cktb\u201d I. \u0010J\u00bb) Some stem templates allow certain CEs and exclude others. Ex. the stem template \u201c>fEl\u201d (\u00c9\u00aa\t\u00af @) disallows tanween (\u201cN\u201d, \u201cK\u201d, \u201cF\u201d) word/stem head/tail char uni/bi-grams word: w (\u00f0), wb (H. \u00f0); stem: A (@), nA (A\tK) Such characters can capture some morphological and syntactic information. Ex. verbs in present tense typ- ically start with \u201c> ( @), n ( \t\u00e0), y (\u00f8  ), or t ( \u0010H)\u201d.",
  "Ex. verbs in present tense typ- ically start with \u201c> ( @), n ( \t\u00e0), y (\u00f8  ), or t ( \u0010H)\u201d. sukun word foreign NEs: ex. jwn ( \t\u00e0\u00f1k. \u2013 John) CE of certain words is strictly sukun. We built a list from training set. named entities NEs Named entities are more likely to have sukun as CE. We extracted the named entity list from the Farasa named entity recognizer (Darwish 2013, Darwish and Gao 2014). Table 1. Features with examples and motivation. training set as input, and we instructed Keras to use 5% of the data for tuning (validation). We included the CASE feature, which speci\ufb01es whether the letter accepts a normal diacritic or case ending, in all our setups. We conducted multiple experiment using di\ufb00erent features, namely: \u2022 CHAR: This is our baseline setup where we only used the characters as features. \u2022 CHAR+SEG: This takes the characters and their segmentation information as features.",
  "We conducted multiple experiment using di\ufb00erent features, namely: \u2022 CHAR: This is our baseline setup where we only used the characters as features. \u2022 CHAR+SEG: This takes the characters and their segmentation information as features. \u2022 CHAR+PRIOR: This takes the characters and their the observed dia- critized forms in the training set. \u2022 All: This setup includes all the features.",
  "Arabic Diacritic Recovery Using a Feature-Rich biLSTM Model 9 Fig. 2. DNN case ending model architecture We also optionally employed post correction. For words that were seen in training, if the model produced a diacritized form that was not seen in the training data, we assumed it was an error and replaced it with the most frequently observed diacritized form (using a unigram language model). We report two error rates, namely WER (at word level) and DER (at character level). We used relaxed scoring where we assumed an empty case to be equivalent to sukun, and we removed default diacritics \u2013 fatHa followed by alef, kasra followed by ya, and damma followed by wa. Using such scoring would allow to compare to other systems in the literature that may use di\ufb00erent diacritization conventions. Results and Error analysis For testing, we used the aforementioned WikiNews dataset to test the MSA dia- critizer and the held-out 5,000 sentences for CA. Table 2 shows WER and DER results using di\ufb00erent features with and without post correction.",
  "Results and Error analysis For testing, we used the aforementioned WikiNews dataset to test the MSA dia- critizer and the held-out 5,000 sentences for CA. Table 2 shows WER and DER results using di\ufb00erent features with and without post correction. MSA Results: For MSA, though the CHAR+PRIOR feature led to worse re- sults than using CHAR alone, the results show that combining all the features achieved the best results. Moreover, post correction improved results overall. We compare our results to \ufb01ve other systems, namely Farasa (Darwish et al. 2017), MADAMIRA (Pasha et al. 2014), RDI (Rashwan et al., 2015), MIT (Belinkow and Glass, 2015), and Microsoft ATKS (Said et al. 2013). Table 7 compares our system with others in the aforementioned systems. As the results show, our results beat the current state-of-the-art.",
  "10 MSA CA DNN DNN+Post DNN DNN+Post Model WER DER WER DER WER DER WER DER CHAR 3.5 1.1 3.3 1.0 5.1 2.1 2.7 1.0 CHAR+SEG 3.3 1.1 3.2 1.0 4.7 1.9 2.6 1.0 CHAR+PRIOR 3.8 1.2 3.7 1.1 3.8 1.6 2.3 0.9 ALL 3.0 1.0 2.9 0.9 3.6 1.5 2.2 0.9 Table 2. Core word diacritization results For error analysis, we analyzed all the errors (527 errors). The errors types along with examples of each are shown in Table 3. The most prominent error type arises from the selection of a valid diacritized form that does not match the context (40.8%).",
  "The errors types along with examples of each are shown in Table 3. The most prominent error type arises from the selection of a valid diacritized form that does not match the context (40.8%). Perhaps, including POS tags as a feature or augmenting the PRIOR fea- ture with POS tag information and a bigram language model may reduce the error rate further. The second most common error is due to transliterated foreign words including foreign named entities (23.5%). Such words were not observed during training. Further, Arabic Named entities account for 10.6% of the errors, where they were either not seen in training or they share identical non-diacritized forms with other words. Perhaps, building larger gazetteers of diacritized named entities may resolve NE related errors. In 10.8% of the cases, the diacritizer produced in completely incorrect diacritized forms. In some the cases (9.1%), though the dia- critizer produced a form that is di\ufb00erent from the reference, both forms were in fact correct. Most of these cases were due to variations in diacritization conventions (ex.",
  "In some the cases (9.1%), though the dia- critizer produced a form that is di\ufb00erent from the reference, both forms were in fact correct. Most of these cases were due to variations in diacritization conventions (ex. \u201cbare alef\u201d (A) at start of a word receiving a diacritic or not). Other cases include foreign words and some words where both diacritized forms are equally valid. CA Results: For CA results, the CHAR+SEG and CHAR+PRIOR performed better than using characters alone with CHAR+PRIOR performing better than CHAR+SEG. As in the case with MSA, combining all the features led to the best results. Post correction had a signi\ufb01cantly larger positive impact on results compared to what we observed for MSA. This may indicate that we need a larger training set. The best WER that we achieved for CW diacritics with post corrections is 2.2%.",
  "Post correction had a signi\ufb01cantly larger positive impact on results compared to what we observed for MSA. This may indicate that we need a larger training set. The best WER that we achieved for CW diacritics with post corrections is 2.2%. Since we did not have access to any publicly available system that is tuned for CA, we compared our best system to using our best MSA system to diacritize the CA test set, and the MSA diacritizer produced signi\ufb01cantly lower results with a WER of 8.5% (see Table 7). This highlights the large di\ufb00erence between MSA and CA and the need for systems that are speci\ufb01cally tuned for both. We randomly selected and analyzed 500 errors (5.2% of the errors). The errors types along with examples of each are shown in Table 6. The two most common errors involve the system producing completely correct diacritized forms (38.8%)",
  "Arabic Diacritic Recovery Using a Feature-Rich biLSTM Model 11 Error Freq. % Explanation Examples Wrong selec- tion 215 40.8 Homographs with di\ufb00er- ent diacritized forms \u201cqaSor\u201d (Q\u00e5\u0015\u0094\u000b\u0010\u00af \u2013 palace) vs. \u201cqaSar\u201d (Q\u00e5\u000b\u0094\u000b\u0010\u00af \u2013 he limited) Foreign word 124 23.5 transliterated words in- cluding 96 foreign named entities wiykiymaAnoyaA (A\u000bJ \u0015\tKA\u000b\u00d2J \u00ba\u000b K \u00f0\u000b \u2013 Wikima- nia) Invalid dia- critized form 57 10.8 invalid form ya*okur (Q\f\u00bb\u0015\tY\u000fK  \u2013 he men- tions) vs. ya*okar (Q\u000b\u00bb\u0015\tY\u000fK ) Named entity 56 10.6 Arabic named entities \u201cEab\u02dcAdiy\u201d (\u00f8  X\u000bA\u000b\u000fJ. \u000b\u00ab \u2013 name) vs.",
  "ya*okar (Q\u000b\u00bb\u0015\tY\u000fK ) Named entity 56 10.6 Arabic named entities \u201cEab\u02dcAdiy\u201d (\u00f8  X\u000bA\u000b\u000fJ. \u000b\u00ab \u2013 name) vs. \u201cEibAdiy\u201d (\u00f8  X\u000bA\u000bJ.\u00ab\u000b \u2013 my servants) both correct 48 9.1 Some words have mul- tiple valid diacritized forms \u201cwikAlap\u201d (\u0010\u00e9\u000b\u00cbA\u000b\u00bf\u00f0\u000b) and \u201cwakAlap\u201d (\u0010\u00e9\u000b\u00cbA\u000b\u00bf\u000b\u00f0 \u2013 agency) A\ufb03x diacriti- zation error 16 3.0 Some su\ufb01xes are erro- neously diacritized baAkt$A\ufb01him (\u00d1\u00ea\u000b\t\u00af\u000bA \u0011\u0082\u0010\u001c\u00bbA\u000bK. \u2013 with their discovery) Reference is wrong 10 1.9 the truth diacritics were incorrect Alo\ufb01yfaA (A\u000b\t\u00aeJ \t\u00ae\u000b \u0015\u00cb@ \u2013 FIFA) vs.",
  "\u2013 with their discovery) Reference is wrong 10 1.9 the truth diacritics were incorrect Alo\ufb01yfaA (A\u000b\t\u00aeJ \t\u00ae\u000b \u0015\u00cb@ \u2013 FIFA) vs. AlofayofaA (A\u000b\t\u00ae\u0015J \u000b\t\u00ae\u0015\u00cb@) dialectal word 1 0.2 dialectal word mawaAyiliy (\u00fa \u00ce\u000bK \u000b@\u000b\u00f1\u000b\u00d3 \u2013 my chant) Table 3. Error analysis: Core word error types for MSA or correct forms that don\u2019t match the context (31.4%). The relatively higher per- centage of completely incorrect guesses, compared to MSA, may point to the higher lexical diversity of classical Arabic. As for MSA, we suspect that adding additional POS information and employing a word bigram to constrain the PRIOR feature may help reduce selection errors. Another prominent error is related to the diacrit- ics that appear on attached su\ufb03xes, particularly pronouns, which depend on the choice of case ending (13.2%). Errors due to named entities are slightly fewer than",
  "12 those seen for MSA (8.8%). A noticeable number of mismatches between the guess and the reference are due to partial diacritization of the reference (4.4%). We plan to conduct an extra round of checks on the test set. 4.2 Case Ending Experimental Setup We conducted multiple experiments to determine the relative e\ufb00ect of the di\ufb00erent features as follows: \u2022 word: This is our baseline setup, which uses word surface forms only. \u2022 word-surface: This setup uses the word surface forms, stems, pre\ufb01xes, and su\ufb03xes (including noun su\ufb03xes). This simulates the case when no POS tag- ging information is available. \u2022 word-POS: This includes the word surface form and POS information, in- cluding gender and number of stems, pre\ufb01xes, and su\ufb03xes. \u2022 word-morph: This includes words and their stem templates to capture mor- phological patterns. \u2022 word-surface-POS-morph: This setup uses all the features (surface, POS, and morphological).",
  "\u2022 word-morph: This includes words and their stem templates to capture mor- phological patterns. \u2022 word-surface-POS-morph: This setup uses all the features (surface, POS, and morphological). \u2022 all-misc: This uses all features plus word and stem leading and trailing char- acter unigrams and bigrams in addition to sukun words and named entities. For testing MSA, we used the aforementioned WikiNews dataset. Again, we compared our results to \ufb01ve other systems, namely Farasa (Darwish et al. 2017), MADAMIRA (Pasha et al. 2014), RDI (Rashwan et al., 2015), MIT (Belinkow and Glass, 2015), and Microsoft ATKS (Said et al. 2013). For CA testing, we used the 5,000 sentences that we set aside. Again, we compared to our best MSA system. Results and Error Analysis Table 8 lists the results of our setups compared to other systems. MSA Results: As the results show, our baseline DNN system outperforms all state-of-the-art systems.",
  "Again, we compared to our best MSA system. Results and Error Analysis Table 8 lists the results of our setups compared to other systems. MSA Results: As the results show, our baseline DNN system outperforms all state-of-the-art systems. Further, adding more features yielded better results over- all. Surface-level features resulted in the most gain, followed by POS tags, and lastly stem templates. Further, adding head and tail characters along with a list of sukun words and named entities led to further improvement. Our proposed feature-rich system has a CEER that is approximately 61% lower than any of the state-of-the-art systems. Figure 3 shows CE distribution and prediction accuracy. For the four basic mark- ers kasra, fatHa, damma and sukun, which appear 27%, 14%, 9% and 10% respec- tively, the system has CEER of \u223c1% for each. Detecting the virtual CE mark is a fairly easy task. All other CE markers represent 13% with almost negligible errors.",
  "Detecting the virtual CE mark is a fairly easy task. All other CE markers represent 13% with almost negligible errors. Table 4 lists a thorough breakdown of all errors accounting for at 1% of the errors along with the most common reasons of the errors and examples illustrating these",
  "Arabic Diacritic Recovery Using a Feature-Rich biLSTM Model 13 reasons. For example, the most common error type involves guessing a fatHa (a) instead of damma (u) or vice versa (19.3%). The most common reasons for this error type, based on inspecting the errors, were due to: POS errors (ex. a word is tagged as a verb instead of a noun); and a noun is treated as a subject instead of an object or vice versa. The table details the rest of the error types. Overall, some of the errors are potentially \ufb01xable using better POS tagging, improved detection of non-Arabized foreign names, and detection of indeclinability. However, some errors are more di\ufb03cult and require greater understanding of semantics such as improper attachment, incorrect idafa, and confusion between subject and object. Perhaps, such semantic errors can be resolved using parsing. CA Results: The results show that the POS tagging features led to the most improvements followed by the surface features. Combining all features led to the best results with WER of 2.5%.",
  "Perhaps, such semantic errors can be resolved using parsing. CA Results: The results show that the POS tagging features led to the most improvements followed by the surface features. Combining all features led to the best results with WER of 2.5%. As we saw for CW diacritics, using our best MSA system to diacritize CA led to signi\ufb01cantly lower results with CEER of 8.9%. Figure 4 shows CE distribution and prediction accuracy. For the four basic mark- ers fatHa, kasra, damma and sukun, which appear 18%, 14%, 13% and 8% respec- tively, the system has CEER \u223c0.5% for each. Again, detecting the virtual CE mark was a fairly easy task. All other CE markers representing 20% have negligible errors. Table 5 lists all the error types, which account for at least 1% of the errors, along with their most common causes and explanatory examples. The error types are sim- ilar to those observed for MSA.",
  "All other CE markers representing 20% have negligible errors. Table 5 lists all the error types, which account for at least 1% of the errors, along with their most common causes and explanatory examples. The error types are sim- ilar to those observed for MSA. Some errors are more syntactic and morphological in nature and can be addressed using better POS tagging and identi\ufb01cation of in- declinability, particularly as they relate to named entities and nouns with feminine markers. Other errors such as incorrect attachment, incorrect idafa, false subject, and confusion between subject and object can perhaps bene\ufb01t from the use of pars- ing. As with the core-word errors for CA, the reference has some errors (ex. {a,i,o} \u21d2#), and extra rounds of reviews of the reference are in order. 4.3 Full Diacritization Results Table 9 compares the full word diacritization (CW+CE) of our best setup to other systems in the literature.",
  "{a,i,o} \u21d2#), and extra rounds of reviews of the reference are in order. 4.3 Full Diacritization Results Table 9 compares the full word diacritization (CW+CE) of our best setup to other systems in the literature. As the results show for MSA, our overall diacritization WER is 6.0% while the state of the art system has a WER of 12.2%. As for CA, our best system produced an error rate of 4.3%, which is signi\ufb01cantly better than using our best MSA system to diacritize CA. 5 Conclusion and Future Work In this paper, we presented a feature-rich DNN approach for MSA CW and CE recovery that produces a word level error for MSA of 6.0%, which is more than 50% lower than state-of-the-art systems (6.0% compared to 12.2%) and word error rate of 4.3% for CA. Speci\ufb01cally, we used biLSTM-based model with a variety of surface, morphological, and syntactic features.",
  "Speci\ufb01cally, we used biLSTM-based model with a variety of surface, morphological, and syntactic features. Reliable NLP tools may be required to generate some of these features, and such tools may not be readily available for",
  "14 i K ~i ~K a F ~a ~F u N ~u ~N # o Wrong Correct Prediction Percentage(%) 0 10 20 30 40 0.81 0.30 0.08 0.00 1.12 0.10 0.06 0.00 1.08 0.21 0.16 0.00 0.26 1.10 Fig. 3. Case endings distribution and prediction accuracy for MSA other language varieties, such as dialectal Arabic. However, we showed the e\ufb03cacy of di\ufb00erent varieties of features, such as surface level-features, and they can help improve diacritization individually. Further, though some errors may be overcome using improved NLP tools (ex. better POS tagging), semantic errors, such incorrect attchment, are more di\ufb03cult to \ufb01x. Perhaps, using dependency parsing may help overcome some semantic errors.",
  "Further, though some errors may be overcome using improved NLP tools (ex. better POS tagging), semantic errors, such incorrect attchment, are more di\ufb03cult to \ufb01x. Perhaps, using dependency parsing may help overcome some semantic errors. As for feature engineering, the broad categories of features, such as surface, syntactic, and morphological features, may likely carry- over to other languages, language-speci\ufb01c feature engineering may be require to handle the speci\ufb01city of each language. Lastly, since multiple diacritization con- ventions may exist, as in the case of Arabic, adopting one convention consistently is important for training a good system and for properly testing it. Though we have mostly achieved this for MSA, the CA dataset requires more checks to insure greater consistency. For future work, we want to explore the e\ufb00ectiveness of augmenting our CW model with POS tagging information and a bigram language model. Further, we plan to create a multi reference diacritization test set to handle words that have multiple valid diacritized forms.",
  "For future work, we want to explore the e\ufb00ectiveness of augmenting our CW model with POS tagging information and a bigram language model. Further, we plan to create a multi reference diacritization test set to handle words that have multiple valid diacritized forms. For CE, we want to examine the e\ufb00ectiveness of the proposed features for Arabic parsing. We plan to explore: character-level convolutional neural networks that may capture sub-word morphological features; pre-trained embeddings; and attention mechanisms to focus on salient features. We also plan to explore joint modeling for both core word and case ending diacritics.",
  "Arabic Diacritic Recovery Using a Feature-Rich biLSTM Model 15 i K ~i ~K a F ~a ~F u N ~u ~N # o Wrong Correct Prediction Percentage(%) 0 10 20 30 40 0.4 0.2 0.0 0.0 0.6 0.1 0.1 0.0 0.5 0.1 0.1 0.0 0.1 0.1 Fig. 4.",
  "4. Case endings distribution and prediction accuracy for CA References Mart\u00b4\u0131n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Je\ufb00rey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geo\ufb00rey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Man\u00b4e, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Van- houcke, Vijay Vasudevan, Fernanda Vi\u00b4egas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensor\ufb02ow.org.",
  "2015. TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensor\ufb02ow.org. Gheith A Abandah, Alex Graves, Balkees Al-Shagoor, Alaa Arabiyat, Fuad Jamour, and Majid Al-Taee. 2015. Automatic diacritization of arabic text using recurrent neural networks. International Journal on Document Analysis and Recognition (IJDAR), 18(2):183\u2013197. Mohamed Seghir Hadj Ameur, Youcef Moulahoum, and Ahmed Guessoum. 2015. Restora- tion of arabic diacritics using a multilevel statistical model. In IFIP Interna- tional Conference on Computer Science and its Applications x000D , pages 181\u2013192. Springer. Mohammed Attia. 2008. Handling Arabic morphological and syntactic ambiguity within the LFG framework with a view to machine translation. Ph.D. Thesis. School of Languages, Linguistics and Cultures, The University of Manchester, UK. Aqil M Azmi and Reham S Almajed.",
  "Handling Arabic morphological and syntactic ambiguity within the LFG framework with a view to machine translation. Ph.D. Thesis. School of Languages, Linguistics and Cultures, The University of Manchester, UK. Aqil M Azmi and Reham S Almajed. 2015. A survey of automatic arabic diacritization techniques. Natural Language Engineering, 21(03):477\u2013495. Mohamed Bebah, Chennou\ufb01Amine, Mazroui Azzeddine, and Lakhouaja Abdelhak. 2014.",
  "16 Hybrid approaches for automatic vowelization of arabic texts. arXiv preprint arXiv:1410.2646. Yonatan Belinkov and James Glass. 2015. Arabic diacritization with recurrent neural networks. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2281\u20132285, Lisbon, Portugal. Tim Buckwalter. 2002. Buckwalter {Arabic} morphological analyzer version 1.0. LDC catalog number LDC2002L49, ISBN 1-58563-257-0. Tim Buckwalter. 2004. Buckwalter arabic morphological analyzer version 2.0. LDC catalog number LDC2004L02, ISBN 1-58563-324-0. Fran\u00b8cois Chollet et al. 2015. Keras. https://keras.io. Kareem Darwish. 2013. Named entity recognition using cross-lingual resources: Arabic as an example.",
  "Fran\u00b8cois Chollet et al. 2015. Keras. https://keras.io. Kareem Darwish. 2013. Named entity recognition using cross-lingual resources: Arabic as an example. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 1558\u20131567. Kareem Darwish, Ahmed Abdelali, Hamdy Mubarak, Younes Samih, and Mohammed Attia. 2018. Diacritization of moroccan and tunisian arabic dialects: A crf approach. In OSACT 3: The 3rd Workshop on Open-Source Arabic Corpora and Processing Tools, page 62. Kareem Darwish and Wei Gao. 2014. Simple e\ufb00ective microblog named entity recognition: Arabic as an example. In LREC, pages 2513\u20132517. Kareem Darwish and Hamdy Mubarak. 2016. Farasa: A new fast and accurate arabic word segmenter.",
  "Simple e\ufb00ective microblog named entity recognition: Arabic as an example. In LREC, pages 2513\u20132517. Kareem Darwish and Hamdy Mubarak. 2016. Farasa: A new fast and accurate arabic word segmenter. In Proceedings of the Tenth International Conference on Language Re- sources and Evaluation (LREC 2016), Paris, France. European Language Resources Association (ELRA). Kareem Darwish, Hamdy Mubarak, and Ahmed Abdelali. 2017. Arabic diacritization: Stats, rules, and hacks. In Proceedings of the Third Arabic Natural Language Pro- cessing Workshop, pages 9\u201317. Guy De Pauw, Peter W Wagacha, and Gilles-Maurice De Schryver. 2007. Automatic diacritic restoration for resource-scarce languages. In International Conference on Text, Speech and Dialogue, pages 170\u2013179. Springer. Tarek A. El-Sadany and Mohamed A. Hashish. 1989. An arabic morphological system.",
  "Automatic diacritic restoration for resource-scarce languages. In International Conference on Text, Speech and Dialogue, pages 170\u2013179. Springer. Tarek A. El-Sadany and Mohamed A. Hashish. 1989. An arabic morphological system. IBM Systems Journal, 28(4):600\u2013612. Moustafa Elshafei, Husni Al-Muhtaseb, and Mansour Alghamdi. 2006. Statistical methods for automatic diacritization of arabic text. In The Saudi 18th National Computer Conference. Riyadh, volume 18, pages 301\u2013306. Ya\u2019akov Gal. 2002. An hmm approach to vowel restoration in arabic and hebrew. In Proceedings of the ACL-02 workshop on Computational approaches to Semitic lan- guages, pages 1\u20137. Association for Computational Linguistics. Nizar Habash and Owen Rambow. 2007. Arabic diacritization through full morphological tagging.",
  "In Proceedings of the ACL-02 workshop on Computational approaches to Semitic lan- guages, pages 1\u20137. Association for Computational Linguistics. Nizar Habash and Owen Rambow. 2007. Arabic diacritization through full morphological tagging. In Human Language Technologies 2007: The Conference of the North Amer- ican Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers, pages 53\u201356. Association for Computational Linguistics. Salima Harrat, Mourad Abbas, Karima Meftouh, and Kamel Smaili. 2013. Diacritics Restoration for Arabic Dialects. In INTERSPEECH 2013 - 14th Annual Conference of the International Speech Communication Association, Lyon, France. ISCA. Y. Hifny. 2018. Hybrid lstm/maxent networks for arabic syntactic diacritics restoration. IEEE Signal Processing Letters, 25(10):1515\u20131519.",
  "ISCA. Y. Hifny. 2018. Hybrid lstm/maxent networks for arabic syntactic diacritics restoration. IEEE Signal Processing Letters, 25(10):1515\u20131519. Geo\ufb00rey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. 2012. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580.",
  "Arabic Diacritic Recovery Using a Feature-Rich biLSTM Model 17 A. Hucko and P. Lacko. 2018. Diacritics restoration using deep neural networks. In 2018 World Symposium on Digital Intelligence for Systems and Machines (DISA), pages 195\u2013200. Tuan Anh Luu and Kazuhide Yamamoto. 2012. A pointwise approach for vietnamese dia- critics restoration. In 2012 International Conference on Asian Language Processing, pages 189\u2013192. IEEE. Mohammed Maamouri, Ann Bies, Tim Buckwalter, and Wigdan Mekki. 2004. The penn arabic treebank: building a large-scale annotated arabic corpus. In NEMLAR Con- ference on Arabic Language Resources and Tools, pages 102\u2013109. Yuval Marton, Nizar Habash, and Owen Rambow. 2010. Improving arabic dependency parsing with lexical and in\ufb02ectional morphological features. In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 13\u201321.",
  "2010. Improving arabic dependency parsing with lexical and in\ufb02ectional morphological features. In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 13\u201321. Association for Computational Linguistics. Rada F Mihalcea. 2002. Diacritics restoration: Learning from letters versus learning from words. In International Conference on Intelligent Text Processing and Computa- tional Linguistics, pages 339\u2013348. Springer. Bebah Mohamed Ould Abdallahi Ould, Abdeloua\ufb01Meziane, Azzeddine Mazroui, and Abdelhak Lakhouaja. 2011. Alkhalil morphosys. In 7th International Computing Conference in Arabic, pages 66\u201373, Riyadh, Saudi Arabia. Hamdy Mubarak, Ahmed Abdelali, Hassan Sajjad, Younes Samih, and Kareem Darwish. 2019. Highly e\ufb00ective Arabic diacritization using sequence to sequence modeling.",
  "Hamdy Mubarak, Ahmed Abdelali, Hassan Sajjad, Younes Samih, and Kareem Darwish. 2019. Highly e\ufb00ective Arabic diacritization using sequence to sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the As- sociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2390\u20132395, Minneapolis, Minnesota. Association for Computational Linguistics. Rani Nelken and Stuart M Shieber. 2005. Arabic diacritization using weighted \ufb01nite-state transducers. In Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 79\u201386. Association for Computational Linguistics. Iroro Orife. 2018. Attentive sequence-to-sequence learning for diacritic restoration of yor\\ub\\\u2019a language text. arXiv preprint arXiv:1804.00832. Torsten Zesch Osama Hamed. 2017. A Survey and Comparative Study of Arabic Diacriti- zation Tools.",
  "arXiv preprint arXiv:1804.00832. Torsten Zesch Osama Hamed. 2017. A Survey and Comparative Study of Arabic Diacriti- zation Tools. JLCL, 32(1):27\u201347. Arfath Pasha, Mohamed Al-Badrashiny, Mona Diab, Ahmed El Kholy, Ramy Eskander, Nizar Habash, Manoj Pooleery, Owen Rambow, and Ryan M Roth. 2014. Madamira: A fast, comprehensive tool for morphological analysis and disambiguation of arabic. In LREC-2014, Reykjavik, Iceland. Mohsen Rashwan, Ahmad Al Sallab, M. Raafat, and Ahmed Rafea. 2015. Deep learn- ing framework with confused sub-set resolution architecture for automatic arabic diacritization. In IEEE Transactions on Audio, Speech, and Language Processing, pages 505\u2013516. Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab, and Cynthia Rudin. 2008.",
  "In IEEE Transactions on Audio, Speech, and Language Processing, pages 505\u2013516. Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab, and Cynthia Rudin. 2008. Arabic morphological tagging, diacritization, and lemmatization using lexeme models and feature ranking. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, pages 117\u2013120. Association for Computational Linguistics. Ahmed Said, Mohamed El-Sharqwi, Achraf Chalabi, and Eslam Kamal. 2013. A hybrid approach for arabic diacritization. In Natural Language Processing and Information Systems, pages 53\u201364, Berlin, Heidelberg. Springer Berlin Heidelberg. Nikola \u02c7Santi\u00b4c, Jan \u02c7Snajder, and Bojana Dalbelo Ba\u02c7si\u00b4c. 2009. Automatic diacritics restora-",
  "18 tion in croatian texts. INFuture2009: Digital Resources and Knowledge Sharing, pages 309\u2013318. Mike Schuster and Kuldip K Paliwal. 1997. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 45(11):2673\u20132681. Dan Tu\ufb01\u00b8s and Alexandru Ceau\u00b8su. 2008. Diac+: A professional diacritics recovering system. Proceedings of LREC 2008. Dimitra Vergyri and Katrin Kirchho\ufb00. 2004. Automatic diacritization of arabic for acous- tic modeling in speech recognition. In Proceedings of the workshop on computational approaches to Arabic script-based languages, COLING\u201904, pages 66\u201373, Geneva, Switzerland. Association for Computational Linguistics. Imed Zitouni, Je\ufb00rey S Sorensen, and Ruhi Sarikaya. 2006. Maximum entropy based restoration of arabic diacritics.",
  "Association for Computational Linguistics. Imed Zitouni, Je\ufb00rey S Sorensen, and Ruhi Sarikaya. 2006. Maximum entropy based restoration of arabic diacritics. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 577\u2013584. Association for Computational Linguis- tics. Pierre Zweigenbaum and Natalia Grabar. 2002. Restoring accents in unknown biomedical words: application to the french mesh thesaurus. International Journal of Medical Informatics, 67(1-3):113\u2013126.",
  "Arabic Diacritic Recovery Using a Feature-Rich biLSTM Model 19 Error Count % Most Common Causes a \u21d4u 133 19.3 POS error: ex. \u201cka$afa\u201d ( \u000b\t\u00ad \u000b\u0011\u0082\u000b\u00bb \u2013 he exposed) vs. \u201cka$ofu\u201d ( \f\t\u00ad \u0015\u0011\u0082\u000b\u00bb \u2013 exposure) & Subject vs. object: ex. \u201ctuwHy mivolu\u201d ( \f\u00c9\u0015\u0011J\u00d3\u000b \u00fa k\u000b\u00f1\f\u0010K \u2013 such indicates) vs. \u201ctuwHy mivola\u201d ( \u000b\u00c9\u0015\u0011J\u00d3\u000b \u00fa k\u000b\u00f1\f\u0010K \u2013 she indicates such) i \u21d4a 130 18.9 Incorrect attachment (due to coordinating con- junction or distant attachment): ex. \u201cAlogaza Alomusay\u02dcili lilidumuEi \u2013 wa+AlraSaSi vs.",
  "\u201cAlogaza Alomusay\u02dcili lilidumuEi \u2013 wa+AlraSaSi vs. wa+AlraSaSa (\u0090A \u000b\u0093\u000bQ\u00cb@\u00f0 \u00a8\u000b \u00f1\f\u00d3 \fY\u00ca\u00cb\u000b \u000b\u00c9\u000f\u000bJ  \u000b\u0082\f\u00dc\u0015\u00cf@ \u000b\tPA\u000b\t\u00aa\u00cb@ \u2013 tear gas and bullets) where bullets were attached incorrectly to tear instead of gas & indeclinability such as foreign words and feminine names: ex. \u201ckaAnuwni\u201d ( \t\u00e0\u000b \u00f1\f\tKA\u000b\u00bf \u2013 Cyrillic month name) vs. \u201ckaAuwna\u201d ( \u000b\t\u00e0\u00f1\f\tKA\u000b\u00bf) i \u21d4u 95 13.8 POS error of previous word: ex. \u201ctadahowuru wa- DoEihi\u201d (\u00e9\u000b\u00aa\u000b \u0015\t\u0093\u000b\u00f0 \fP \f \u00f1\u0015\u00eb \u000bY\u000b\u0010K \u2013 deterioration of his situa- tion \u2013 situtation is part of idafa construct) vs.",
  "\u201ctadahowuru wa- DoEihi\u201d (\u00e9\u000b\u00aa\u000b \u0015\t\u0093\u000b\u00f0 \fP \f \u00f1\u0015\u00eb \u000bY\u000b\u0010K \u2013 deterioration of his situa- tion \u2013 situtation is part of idafa construct) vs. \u201ctada- howara waDoEihu\u201d (\f\u00e9\f\u00aa \u0015\t\u0093\u000b\u00f0 \u000bP \u000b\u00f1\u0015\u00eb \u000b\tY\u000b\u0010K \u2013 his situation de- teriorated \u2013 situation is subject) & Incorrect attach- ment (due to coordinating conjunction or distant at- tachment): (as example for i \u21d4a) a \u21d4o 60 8.7 Foreign named entities: ex. \u201csiyraAloyuna\u201d ( \u000b\t\u00e0\u00f1\fJ \u0015\u00cb@\u000bQ\u001e \u0083\u000b \u2013 Siera Leon) vs. \u201csiyraAloyuno\u201d ( \u0015\t\u00e0\u00f1\fJ \u0015\u00cb@\u000bQ\u001e \u0083\u000b) i \u21d4K 27 4.0 Incorrect Idafa: \u201cliAt\u02dcifaqi haaA Alo>usobuwE\u201d (\u00a8\u00f1\fJ.",
  "\u0015\u0083 \f B@ @ \u000b\tY\u000b\u00eb \u0010\u0086\u000bA\u000b\t\u00ae\u0010K\u000bB\u000b \u2013 this week\u2019s agree- ment) vs. \u201cliAt\u02dcifaqK haaA Alo>usobuwE\u201d (\u00a8\u00f1\fJ. \u0015\u0083 \f B\u0015@ @ \u000b\tY\u000b\u00eb \u0010\u0086\u0013A\u000b\t\u00ae\u000f\u000b\u0010KB\u000b \u2013 to an agreement this week) K \u21d4N 29 4.2 Subject vs. object (as in a \u21d4u) and Incorrect attach- ment (as in i \u21d4a) F \u21d4N 25 3.7 Words ending with feminine marker \u201cp\u201d or \u201cAt\u201d: ex.",
  "object (as in a \u21d4u) and Incorrect attach- ment (as in i \u21d4a) F \u21d4N 25 3.7 Words ending with feminine marker \u201cp\u201d or \u201cAt\u201d: ex. \u201cmuHaADarap\u201d (\u0010\u00e8\u000bQ\u00e5\u000b\t\u0095A\u000bm \f\u00d7 \u2013 lecture) i \u21d4o 22 3.2 Foreign named entities (as in a \u21d4o) F \u21d4a 16 2.3 Incorrect Idafa (as in i \u21d4K) u \u21d4o 14 2.0 Foreign named entities (as in a \u21d4o) F \u21d4K 9 1.3 Words ending with feminine marker (as in F \u21d4N) K \u21d4a 8 1.2 Incorrect Idafa (as in i \u21d4K) Table 4. MSA case errors accounting from more than 1% of errors",
  "20 Error Count % Most Common Causes a \u21d4u 2,907 28.4 Subject vs. object: ex. \u201cwafaqa yawoma\u201d (\u000b\u00d0 \u0015\u00f1\u000bK  \u000b\u0010\u0087\u000b\t\u00af\u000b\u00f0 \u2013 he matches the day) vs. ex. \u201cwafaqa yawomu\u201d (\f\u00d0 \u0015\u00f1\u000bK  \u000b\u0010\u0087\u000b\t\u00af\u000b\u00f0 \u2013 the day matches) & False subject (object behaves like subject in passive tense): ex. \u201cyufar\u02dciqu qaDaA\u2019a\u201d (\u000bZA \u000b\t\u0092\u000b\u0010\u00ae\u0015\u00cb@ \f\u0010\u0086\u000f\u000bQ\u000b\t\u00ae\fK  \u2013 he separates the make up) vs. \u201cyufar\u02dcaqu qaDaA\u2019u\u201d (\fZA \u000b\t\u0092\u000b\u0010\u00ae\u0015\u00cb@ \u0014\u0010\u0086\u000b\u000fQ\u000b\t\u00ae\fK  \u2013 the make up is separated) & In- correct attachment (due to coordinating conjunction): ex. \u201cf+a>aEohadu\u201d ( \u000bY\u000b\u00ea\u0015\u00ab \u000b A\u000b\t\u00af \u2013 so I entrust) vs.",
  "\u201cf+a>aEohadu\u201d ( \u000bY\u000b\u00ea\u0015\u00ab \u000b A\u000b\t\u00af \u2013 so I entrust) vs. \u201cf+a>aEohadu\u201d ( \fY\u00ea\u000b\u0015\u00ab \u000b A\u000b\t\u00af) i \u21d4u 1,316 12.9 Incorrect attachment (due to coordinating conjunctions or distant attachment): (as in a \u21d4u) i \u21d4a 1,019 10.0 Incorrect attachment (as in a \u21d4u) & Indeclinability such as foreign words and feminine names: ex. \u201c>ajoyaAdiyni\u201d ( \t\u00e1\u000bK X\u000bA\u000bJ  \u0015k. \u000b @ \u2013 Ajyadeen (city name)) vs. \u201c>ajoyaAiyna\u201d ( \u000b\t\u00e1K X\u000bA\u000bJ  \u0015k. \u000b @) a \u21d4# 480 4.7 Problem with reference where the case for some words, par- ticularly non-Arabic names, is not provided in the refer- ence: ex.",
  "@) a \u21d4# 480 4.7 Problem with reference where the case for some words, par- ticularly non-Arabic names, is not provided in the refer- ence: ex. \u201c<isoHaAq\u201d ( \u0010\u0086A\u000bm\u0019\u0015\u0085@ \u000b \u2013 Issac) vs. \u201c<isoHaAqa\u201d ( \u000b\u0010\u0086A\u000bm\u0019\u0015\u0085@ \u000b) u \u21d4# 426 4.2 same problems as in a \u21d4# K \u21d4i 371 3.6 Incorrect Idafa: ex. \u201cEaTaA\u2019i Alofaqiyh\u201d (\u00e9J \u0010\u00ae\u000b \u000b\t\u00ae\u0015\u00cb@ Z\u000bA \u000b\u00a2\u000b\u00ab \u2013 the providence of the jurist) vs. \u201cEaTaA\u2019K Alofaqiyh\u201d (\u00e9J \u0010\u00ae\u000b \u000b\t\u00ae\u0015\u00cb@ Z\u0013A \u000b\u00a2\u000b\u00ab \u2013 Ataa the jurist) K \u21d4a 328 3.2 words ending with feminine marker: ex.",
  "\u201cEaTaA\u2019K Alofaqiyh\u201d (\u00e9J \u0010\u00ae\u000b \u000b\t\u00ae\u0015\u00cb@ Z\u0013A \u000b\u00a2\u000b\u00ab \u2013 Ataa the jurist) K \u21d4a 328 3.2 words ending with feminine marker: ex. \u201ctayomiyap\u201d (\u0010\u00e9\u000bJ \u00d2\u000b\u0015J \u000b\u0010K \u2013Taymiya) & Indeclinability: ex. \u201cbi<i$obiyliy\u02dcap\u201d (\u0010\u00e9\u000b\u000fJ \u00ca\u000bJ J.\u000b \u0015\u0011\u0083A \u000bK.\u000b \u000b\u00f0 \u2013 and in Lisbon) u \u21d4o 300 2.9 confusion between past, present, and imperative moods of verbs and preceding markers (imperative \u201claA\u201d vs. nega- tion \u201claA): ex. \u201claA tano$ariHu\u201d ( \fhQ\u000b\u00e5\u000b\u0011\u0084\u0015\tJ\u000b\u0010K B \u2013 does not open up) vs.",
  "nega- tion \u201claA): ex. \u201claA tano$ariHu\u201d ( \fhQ\u000b\u00e5\u000b\u0011\u0084\u0015\tJ\u000b\u0010K B \u2013 does not open up) vs. \u201claA tano$ariHo\u201d ( \u0015hQ\u000b\u00e5\u000b\u0011\u0084\u0015\tJ\u000b\u0010K B \u2013 do not open up) a \u21d4o 278 2.7 confusion between past, present, and imperative moods of verbs (as in u \u21d4o) K \u21d4N 253 2.5 Incorrect attachment (as in i \u21d2u) N \u21d4u 254 2.5 Incorrect Idafa (as in K \u21d2i) F \u21d4N 235 2.3 words ending with feminine marker (as in K \u21d2a) i \u21d4o 195 1.9 Di\ufb00ering conventions concerning handling two consecutive letters with sukun: ex. \u201cEano Aboni\u201d ( \t\u00e1\u000b \u0015K.@ \u0015\t\u00e1\u000b\u00ab \u2013 on the authority of the son of) vs. \u201cEani Aboni\u201d ( \t\u00e1\u000b \u0015K.",
  "\u201cEano Aboni\u201d ( \t\u00e1\u000b \u0015K.@ \u0015\t\u00e1\u000b\u00ab \u2013 on the authority of the son of) vs. \u201cEani Aboni\u201d ( \t\u00e1\u000b \u0015K.@ \t\u00e1\u000b \u000b\u00ab) i \u21d4# 178 1.7 same errors as for a \u21d2# o \u21d4# 143 1.4 same errors as for a \u21d2# Table 5. CA case errors accounting from more than 1% of errors",
  "Arabic Diacritic Recovery Using a Feature-Rich biLSTM Model 21 Error Freq. % Explanation Examples Invalid dia- critized form 195 38.8 invalid form \u201c>aqosaAm\u201d (\u00d0A \u000b\u0082\u0015\u0010\u00af@ \u000b \u2013 portions) vs. \u201c>aqasaAm\u201d (\u00d0A \u000b\u0082\u000b\u0010\u00af \u000b @) Wrong selec- tion 157 31.4 Homographs with di\ufb00er- ent diacritized forms \u201craAfoE\u201d (\u00a9\u0015\u0010\u00af\u000bP \u2013 lifting) vs. \u201crafaE\u201d (\u00a9\u000b\t\u00af\u000bP \u2013 he lifted) A\ufb03x diacriti- zation error 66 13.2 Some a\ufb03xes are erro- neously diacritized \u201cbaladhu\u201d (\f\u00e8Y\u000b\u00ca\u000bK. \u2013 his country, where country is subject of verb) vs. \u201cbal- adhi\u201d (\u00e8\u000bY\u000b\u00ca\u000bK.",
  "\u2013 his country, where country is subject of verb) vs. \u201cbal- adhi\u201d (\u00e8\u000bY\u000b\u00ca\u000bK. \u2013 his country, where country is subject or object of preposition) Named enti- ties 44 8.8 Named entities \u201cAlr\u02dcayob\u201d (I. \u0015K  \u000b\u000fQ\u00cb@ \u2013 Arrayb) vs. \u201cAlr\u02dciyab\u201d (I. \u000bK \u000f\u000bQ\u00cb@)) Problems with reference 22 4.4 Some words in the refer- ence were partially dia- critized \u201cnuEoTaY\u201d (\u00f9 \u000b\u00a2\u0015\u00aa\f\tK \u2013 we are given) vs. \u201cnETY\u201d (\u00f9\u00a2\u00aa\tK)) Guess has no diacritics 9 1.8 system did not produce any diacritics \u201cmhnd\u201d (Y\tJ\u00ea\u00d3 \u2013 sword) vs.",
  "\u201cnETY\u201d (\u00f9\u00a2\u00aa\tK)) Guess has no diacritics 9 1.8 system did not produce any diacritics \u201cmhnd\u201d (Y\tJ\u00ea\u00d3 \u2013 sword) vs. \u201cmuhan\u02dcad\u201d (Y\u000b\u000f\tJ\u000b\u00ea\f\u00d3)) Di\ufb00erent valid forms 7 1.4 Some words have mul- tiple valid diacritized forms \u201cmaA}op\u201d (\u0010\u00e9\u0015 KA\u000b\u00d3 \u2013 hun- dred) and \u201cmiA}op\u201d (\u0010\u00e9\u000b KA\u00d3\u000b) Misspelled word 1 0.2 \u201clbAlmsjd\u201d (Yj. \u0082\u00d6\u00cfAJ.\u00cb) vs. \u201clbAlmsjd\u201d (Yj. \u0082\u00d6\u00cfAK. \u2013 in the mosque)) Table 6. Error analysis: Core word error types for CA",
  "22 Error Rate System WER DER MSA Our system 2.9 0.9 (Rashwan et al. 2015) 3.0 1.0 Farasa 3.3 1.1 Microsoft ATKS 5.7 2.0 MADAMIRA 6.7 1.9 (Belinkov and Glass 2015) 14.9 3.9 CA Our system 2.2 0.9 Our best MSA system on CA 8.5 3.7 Table 7. Comparing our system to state-of-the-art systems \u2013 Core word diacritics",
  "Arabic Diacritic Recovery Using a Feature-Rich biLSTM Model 23 Setup CEER% MSA word (baseline) 9.1 word-surface 5.7 word-POS 7.0 word-morph 7.6 word-surface-POS-morph 5.2 all-misc 3.7 Microsoft ATKS 9.5 Farasa 10.4 RDI (Rashwan et al. 2015) 14.0 MIT (Belinkov and Glass 2015) 15.3 MADAMIRA (Pasha et al. 2014) 15.9 CA word (baseline) 4.0 word-surface 3.3 word-POS 3.1 word-morph 3.7 word-surface-POS-morph 2.9 all-misc 2.5 Our best MSA system on CA 8.9 Table 8. MSA Results and comparison to other systems",
  "24 Setup WER% MSA Our System 6.0 Microsoft ATKS 12.2 Farasa 12.8 RDI (Rashwan et al. 2015) 16.0 MADAMIRA (Pasha et al. 2014) 19.0 MIT (Belinkov and Glass 2015) 30.5 CA Our system 4.3 Our best MSA system on CA 14.7 Table 9. Comparison to other systems for full diacritization"
]