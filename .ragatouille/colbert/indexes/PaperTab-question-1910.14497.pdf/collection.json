[
  "Probabilistic Bias Mitigation in Word Embeddings Hailey Joren Harvard College Cambridge, MA hjoren@ucsd.edu David Alvarez-Melis Massachusetts Institute of Technology Cambridge, MA dalvmel@mit.edu Abstract It has been shown that word embeddings derived from large corpora tend to incorporate biases present in their training data. Various methods for mitigating these biases have been proposed, but recent work has demonstrated that these methods hide but fail to truly remove the biases, which can still be observed in word nearest-neighbor statistics. In this work we propose a probabilistic view of word embedding bias. We leverage this framework to present a novel method for mitigating bias which relies on probabilistic observations to yield a more robust bias mitigation algorithm. We demonstrate that this method effectively reduces bias according to three separate measures of bias while maintaining embedding quality across various popular benchmark semantic tasks. 1 Introduction Word embeddings, or vector representations of words, are an important component of Natural Language Processing (NLP) models and necessary for many downstream tasks.",
  "We demonstrate that this method effectively reduces bias according to three separate measures of bias while maintaining embedding quality across various popular benchmark semantic tasks. 1 Introduction Word embeddings, or vector representations of words, are an important component of Natural Language Processing (NLP) models and necessary for many downstream tasks. However, word embeddings, including embeddings commonly deployed for public use, have been shown to exhibit unwanted societal stereotypes and biases, raising concerns about disparate impact on axes of gender, race, ethnicity, and religion [1, 2]. The impact of this bias has manifested in a range of downstream tasks, ranging from autocomplete suggestions [3] to advertisement delivery [4], increasing the likelihood of amplifying harmful biases through the use of these models. The most well-established method thus far for mitigating bias1 relies on projecting target words2 onto a bias subspace (such as a gender subspace) and subtracting out the difference between the resulting distances [1]. On the other hand, the most popular metric for measuring bias is the WEAT statistic [2], which compares the cosine similarities between groups of words.",
  "On the other hand, the most popular metric for measuring bias is the WEAT statistic [2], which compares the cosine similarities between groups of words. However, WEAT has been recently shown to overestimate bias as a result of implicitly relying on similar frequencies for the target words [5], and Gonen and Goldberg [6] demonstrated that evidence of bias can still be recovered after geometric bias mitigation by examining the neighborhood of a target word among socially-biased words. In response to this, we propose an alternative framework for bias mitigation in word embeddings that approaches this problem from a probabilistic perspective. The motivation for this approach is two-fold. First, most popular word embedding algorithms are probabilistic at their core \u2013 i.e., they are trained (explicitly or implicitly [7]) to minimize some form of word co-occurrence probabilities. Thus, we argue that a framework for measuring and treating bias in these embeddings should take into account, in addition to their geometric aspect, their probabilistic nature too.",
  "Thus, we argue that a framework for measuring and treating bias in these embeddings should take into account, in addition to their geometric aspect, their probabilistic nature too. On the other hand, 1We intentionally do not reference the resulting embeddings as \"debiased\" or free from all gender bias, and prefer the term \"mitigating bias\" rather that \"debiasing,\" to guard against the misconception that the resulting embeddings are entirely \"safe\" and need not be critically evaluated for bias in downstream tasks. 2Throughout this paper, we use word interchangeably with the vector representing the word in an embedding. Workshop on Human-Centric Machine Learning at the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada. (non-archival) arXiv:1910.14497v2  [cs.CL]  26 Apr 2020",
  "the issue of bias has also been approached (albeit in different contexts) in the fairness literature, where various intuitive notions of equity such as equalized odds have been formalized through probabilistic criteria. By considering analogous criteria for the word embedding setting, we seek to draw connections between these two bodies of work. We present experiments on various bias mitigation benchmarks and show that our framework is comparable to state-of-the-art alternatives according to measures of geometric bias mitigation and that it performs far better according to measures of neighborhood bias. For fair comparison, we focus on mitigating a binary gender bias in pre-trained word embeddings using SGNS (skip-gram with negative-sampling), though we note that this framework and methods could be extended to other types of bias and word embedding algorithms. 2 Background Geometric Bias Mitigation Geometric bias mitigation uses the cosine distances between words to both measure and remove gender bias [1]. This method implicitly de\ufb01nes bias as a geometric asym- metry between words when projected onto a subspace, such as the gender subspace constructed from a set of gender pairs such as P = {(he, she), (man, woman), (king, queen)...}.",
  "This method implicitly de\ufb01nes bias as a geometric asym- metry between words when projected onto a subspace, such as the gender subspace constructed from a set of gender pairs such as P = {(he, she), (man, woman), (king, queen)...}. The projection of a vector v onto B (the subspace) is de\ufb01ned by vB = Pk j=1(v \u00b7 bj)bj where a subspace B is de\ufb01ned by k orthogonal unit vectors B = b1, ..., bk.",
  "The projection of a vector v onto B (the subspace) is de\ufb01ned by vB = Pk j=1(v \u00b7 bj)bj where a subspace B is de\ufb01ned by k orthogonal unit vectors B = b1, ..., bk. WEAT The WEAT statistic [2] demonstrates the presence of biases in word embeddings with an effect size de\ufb01ned as the mean test statistic across the two word sets: meanx\u2208Xs(x, A, B) \u2212meany\u2208Y s(y, A, B) std_devw\u2208X\u222aY s(w, A, B) (1) Where s, the test statistic, is de\ufb01ned as: s(w, A, B) = meana\u2208Acos(w, a) \u2212meanb\u2208Bcos(w, a), and X,Y ,A, and B are groups of words for which the association is measured. Possible values range from \u22122 to 2 depending on the association of the words groups, and a value of zero indicates X and Y are equally associated with A and B. See Ethayarajh et al. [5] for further details on WEAT.",
  "Possible values range from \u22122 to 2 depending on the association of the words groups, and a value of zero indicates X and Y are equally associated with A and B. See Ethayarajh et al. [5] for further details on WEAT. RIPA The RIPA (relational inner product association) metric was developed as an alternative to WEAT, with the critique that WEAT is likely to overestimate the bias of a target attribute [5]. The RIPA metric formalizes the measure of bias used in geometric bias mitigation as the inner product association of a word vector v with respect to a relation vector b. The relation vector is constructed from the \ufb01rst principal component of the differences between gender word pairs. We report the absolute value of the RIPA metric as the value can be positive or negative according to the direction of the bias. A value of zero indicates a lack of bias, and the value is bound by [\u2212||w||, ||w||].",
  "We report the absolute value of the RIPA metric as the value can be positive or negative according to the direction of the bias. A value of zero indicates a lack of bias, and the value is bound by [\u2212||w||, ||w||]. Neighborhood Metric The neighborhood bias metric proposed by Gonen and Goldberg [6] quanti- \ufb01es bias as the proportion of male socially-biased words among the k nearest socially-biased male and female neighboring words, whereby biased words are obtained by projecting neutral words onto a gender relation vector. As we only examine the target word among the 1000 most socially-biased words in the vocabulary (500 male and 500 female), a word\u2019s bias is measured as the ratio of its neighborhood of socially-biased male and socially-biased female words, so that a value of 0.5 in this metric would indicate a perfectly unbiased word, and values closer to 0 and 1 indicate stronger bias. 3 A Probabilistic Framework for Bias Mitigation Our objective here is to extend and complement the geometric notions of word embedding bias described in the previous section with an alternative, probabilistic, approach.",
  "3 A Probabilistic Framework for Bias Mitigation Our objective here is to extend and complement the geometric notions of word embedding bias described in the previous section with an alternative, probabilistic, approach. Intuitively, we seek a notion of equality akin to that of demographic parity in the fairness literature, which requires that a decision or outcome be independent of a protected attribute such as gender. [8]. Similarly, when considering a probabilistic de\ufb01nition of unbiased in word embeddings, we can consider the conditional probabilities of word pairs, ensuring for example that p(doctor|man) \u2248p(doctor|woman), and can extend this probabilistic framework to include the neighborhood of a target word, addressing the potential pitfalls of geometric bias mitigation. 2",
  "Conveniently, most word embedding frameworks allow for immediate computation of the conditional probabilities P(w|c). Here, we focus our attention on the Skip-Gram method with Negative Sampling (SGNS) of Mikolov et al. [9], although our framework can be equivalently instantiated for most other popular embedding methods, owing to their core similarities [7, 10]. Leveraging this probabilistic nature, we construct a bias mitigation method in two steps, and examine each step as an independent method as well as the resulting composite method. Probabilistic Bias Mitigation This component of our bias mitigation framework seeks to enforce that the probability of prediction or outcome cannot depend on a protected class such as gender. We can formalize this intuitive goal through a loss function that penalizes the discrepancy between the conditional probabilities of a target word (i.e., one that should not be affected by the protected attribute) conditioned on two words describing the protected attribute (e.g., man and woman in the case of gender).",
  "We can formalize this intuitive goal through a loss function that penalizes the discrepancy between the conditional probabilities of a target word (i.e., one that should not be affected by the protected attribute) conditioned on two words describing the protected attribute (e.g., man and woman in the case of gender). That is, for every target word we seek to minimize: loss = X a,b\u2208P p(target|a) \u2212p(target|b) (2) where P = {(he, she), (man, woman), (king, queen), . . . } is a set of word pairs characterizing the protected attribute, akin to that used in previous work [1]. At this point, the speci\ufb01c form of the objective will depend on the type of word embeddings used. For our expample of SGNS, recall that this algorithm models the conditional probability of a target word given a context word as a function of the inner product of their representations. Though an exact method for calculating the conditional probability includes summing over conditional probability of all the words in the vocabulary, we can use the estimation of log conditional probability proposed by Mikolov et al.",
  "Though an exact method for calculating the conditional probability includes summing over conditional probability of all the words in the vocabulary, we can use the estimation of log conditional probability proposed by Mikolov et al. [9], i.e., log p(wO|wI) \u2248log \u03c3(v\u2032 wo T vwI) + Pk i=1[log \u03c3(\u2212v\u2032 wi T vwI)]. Nearest Neighbor Bias Mitigation Based on observations by Gonen and Goldberg [6], we extend our method to consider the composition of the neighborhood of socially-gendered words of a target word. We note that bias in a word embedding depends not only on the relationship between a target word and explicitly gendered words like man and woman, but also between a target word and socially-biased male or female words. Bolukbasi et al [1] proposed a method for eliminating this kind of indirect bias through geometric bias mitigation, but it is shown to be ineffective by the neighborhood metric [6]. Instead, we extend our method of bias mitigation to account for this neighborhood effect.",
  "Bolukbasi et al [1] proposed a method for eliminating this kind of indirect bias through geometric bias mitigation, but it is shown to be ineffective by the neighborhood metric [6]. Instead, we extend our method of bias mitigation to account for this neighborhood effect. Speci\ufb01cally, we examine the conditional probabilities of a target word given the k/2 nearest neighbors from the male socially-biased words as well as given the k/2 female socially-biased words (in sorted order, from smallest to largest). The groups of socially-biased words are constructed as described in the neighborhood metric. If the word is unbiased according to the neighborhood metric, these probabilities should be comparable. We then use the following as our loss function: loss = k/2 X i=0 p(target|mi) \u2212p(target|fi), (3) where m and f represent the male and female neighbors sorted by distance to the target word t (we use L1 distance). 4 Experiments We evaluate our framework on fastText embeddings trained on Wikipedia (2017), UMBC webbase corpus and statmt.org news dataset (16B tokens) [12].",
  "4 Experiments We evaluate our framework on fastText embeddings trained on Wikipedia (2017), UMBC webbase corpus and statmt.org news dataset (16B tokens) [12]. For simplicity, only the \ufb01rst 22000 words are used in all embeddings, though preliminary results indicate the \ufb01ndings extend to the full corpus. For our novel methods of mitigating bias, a shallow neural network is used to adjust the embedding. The single layer of the model is an embedding layer with weights initialized to those of the original embedding. For the composite method, these weights are initialized to those of the embedding after probabilistic bias mitigation. A batch of word indices is fed into the model, which are then embedded and for which a loss value is calculated, allowing back-propagation to adjust the embeddings. For each of the models, a \ufb01xed number of iterations is used to prevent over\ufb01tting, which can eventually 3",
  "Figure 1: Word embedding semantic quality benchmarks for each bias mitigation method (higher is better). See Jastrzkebski et al. [11] for details of each metric. RIPA Neighborhood |mean| |.5 \u2212mean| Original 2.895 0.323 Geometric 0.096 0.328 Simple Probabilistic 0.320 0.250 Nearest Neighbor 1.705 0.083 Composite NN + Prob 0.372 0.034 Table 1: Remaining Bias (as measured by RIPA and Neighborhood metrics) in fastText embeddings for base- line (top two rows) and our (bottom three) methods. Figure 2: Remaining Bias (WEAT score) hurt performance on the embedding benchmarks (See Figure 1). We evaluated the embedding after 1000 iterations, and stopped training if performance on a benchmark decreased signi\ufb01cantly. We construct a list of candidate words to debias, taken from the words used in the WEAT gender bias statistics. Words in this list should be gender neutral, and are related to the topics of career, arts, science, math, family and professions (see appendix).",
  "We construct a list of candidate words to debias, taken from the words used in the WEAT gender bias statistics. Words in this list should be gender neutral, and are related to the topics of career, arts, science, math, family and professions (see appendix). We note that this list can easily be expanded to include a greater proportion of words in the corpus. For example, Ethayarajh et al. [5] suggested a method for identifying inappropriately gendered words using unsupervised learning. We compare this method of bias mitigation with the no bias mitigation (\"Orig\"), geometric bias mitigation (\"Geo\"), the two pieces of our method alone (\"Prob\" and \"KNN\") and the composite method (\"KNN+Prob\"). We note that the composite method performs reasonably well according the the RIPA metric, and much better than traditional geometric bias mitigation according to the neighborhood metric, without signi\ufb01cant performance loss according to the accepted benchmarks. To our knowledge this is the \ufb01rst bias mitigation method to perform reasonably both on both metrics.",
  "To our knowledge this is the \ufb01rst bias mitigation method to perform reasonably both on both metrics. 5 Discussion We proposed a simple method of bias mitigation based on this probabilistic notions of fairness, and showed that it leads to promising results in various benchmark bias mitigation tasks. Future work should include considering a more rigorous de\ufb01nition and non-binary of bias and experimenting with various embedding algorithms and network architectures. 4",
  "Acknowledgements The authors would like to thank Tommi Jaakkola for stimulating discussions during the initial stages of this work. References [1] Tolga Bolukbasi, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. CoRR, abs/1607.06520, 2016. URL http://arxiv.org/abs/1607.06520. [2] Aylin Caliskan Islam, Joanna J. Bryson, and Arvind Narayanan. Semantics derived automatically from language corpora necessarily contain human biases. CoRR, abs/1608.07187, 2016. URL http://arxiv.org/abs/1608.07187. [3] Issie Lapowsky. Google autocomplete still makes vile suggestions, 2018. URL \"https: //www.wired.com/story/google-autocomplete-vile-suggestions/\". [4] Catherine E. Tucker Anja Lambrecht. Algorithmic bias?",
  "[3] Issie Lapowsky. Google autocomplete still makes vile suggestions, 2018. URL \"https: //www.wired.com/story/google-autocomplete-vile-suggestions/\". [4] Catherine E. Tucker Anja Lambrecht. Algorithmic bias? an empirical study into apparent gender-based discrimination in the display of stem career ads. SSRN, 2016. doi: https: //papers.ssrn.com/sol3/papers.cfm?abstract_id=2852260. [5] Kawin Ethayarajh, David Kristjanson Duvenaud, and Graeme Hirst. Understanding undesirable word embedding associations. In ACL, 2019. [6] Hila Gonen and Yoav Goldberg. Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them. CoRR, abs/1903.03862, 2019. URL http://arxiv.org/abs/1903.03862. [7] Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization.",
  "CoRR, abs/1903.03862, 2019. URL http://arxiv.org/abs/1903.03862. [7] Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Advances in neural information processing systems, pages 2177\u20132185, 2014. [8] Moritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervised learning. CoRR, abs/1610.02413, 2016. URL http://arxiv.org/abs/1610.02413. [9] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. CoRR, abs/1310.4546, 2013. URL http://arxiv.org/abs/1310.4546. [10] Tatsunori B Hashimoto, David Alvarez-Melis, and Tommi S Jaakkola. Word embeddings as metric recovery in semantic spaces.",
  "URL http://arxiv.org/abs/1310.4546. [10] Tatsunori B Hashimoto, David Alvarez-Melis, and Tommi S Jaakkola. Word embeddings as metric recovery in semantic spaces. Transactions of the Association for Computational Linguistics, 4:273\u2013286, 2016. [11] Stanislaw Jastrzkebski, Damian Lesniak, and Wojciech Marian Czarnecki. How to evaluate word embeddings? on importance of data ef\ufb01ciency and simple supervised tasks. CoRR, abs/1702.02170, 2017. URL http://arxiv.org/abs/1702.02170. [12] Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin. Advances in pre-training distributed word representations. CoRR, abs/1712.09405, 2017. URL http://arxiv.org/abs/1712.09405.",
  "Advances in pre-training distributed word representations. CoRR, abs/1712.09405, 2017. URL http://arxiv.org/abs/1712.09405. A Experiment Notes For Equation 4, as described in the original work, in regards to the k sample words wi is drawn from the corpus using the Unigram distribution raised to the 3/4 power. For reference, the most male socially-biased words include words such as:\u2019john\u2019, \u2019jr\u2019, \u2019mlb\u2019, \u2019dick\u2019, \u2019n\ufb02\u2019, \u2019c\ufb02\u2019, \u2019sgt\u2019, \u2019abbot\u2019, \u2019halfback\u2019, \u2019jock\u2019, \u2019mike\u2019, \u2019joseph\u2019,while the most female socially-biased words include words such as:\u2019feminine\u2019, \u2019marital\u2019, \u2019tatiana\u2019, \u2019pregnancy\u2019, \u2019eva\u2019, \u2019pageant\u2019, \u2019distress\u2019, \u2019cristina\u2019, \u2019ida\u2019, \u2019beauty\u2019, \u2019sexuality\u2019,\u2019fertility\u2019 5",
  "B Professions \u2019accountant\u2019, \u2019acquaintance\u2019, \u2019actor\u2019, \u2019actress\u2019, \u2019administrator\u2019, \u2019adventurer\u2019, \u2019advocate\u2019, \u2019aide\u2019, \u2019al- derman\u2019, \u2019ambassador\u2019, \u2019analyst\u2019, \u2019anthropologist\u2019, \u2019archaeologist\u2019, \u2019archbishop\u2019, \u2019architect\u2019, \u2019artist\u2019, \u2019assassin\u2019, \u2019astronaut\u2019, \u2019astronomer\u2019, \u2019athlete\u2019, \u2019attorney\u2019, \u2019author\u2019, \u2019baker\u2019, \u2019banker\u2019, \u2019barber\u2019, \u2019baron\u2019, \u2019barrister\u2019, \u2019bartender\u2019, \u2019biologist\u2019, \u2019bishop\u2019, \u2019bodyguard\u2019, \u2019boss\u2019, \u2019boxer\u2019, \u2019broadcaster\u2019, \u2019broker\u2019, \u2019businessman\u2019, \u2019butcher\u2019, \u2019butler\u2019, \u2019captain\u2019, \u2019caretaker\u2019, \u2019carpenter\u2019, \u2019cartoonist\u2019, \u2019cellist\u2019, \u2019chan- cellor\u2019, \u2019chaplain\u2019, \u2019character\u2019, \u2019chef\u2019, \u2019chemist\u2019, \u2019choreographer\u2019, \u2019cinematographer\u2019, \u2019citizen\u2019, \u2019cleric\u2019, \u2019clerk\u2019, \u2019coach\u2019, \u2019collector\u2019, \u2019colonel\u2019, \u2019columnist\u2019, \u2019comedian\u2019, \u2019comic\u2019, \u2019commander\u2019, \u2019commentator\u2019, \u2019commissioner\u2019, \u2019composer\u2019,",
  "\u2019chef\u2019, \u2019chemist\u2019, \u2019choreographer\u2019, \u2019cinematographer\u2019, \u2019citizen\u2019, \u2019cleric\u2019, \u2019clerk\u2019, \u2019coach\u2019, \u2019collector\u2019, \u2019colonel\u2019, \u2019columnist\u2019, \u2019comedian\u2019, \u2019comic\u2019, \u2019commander\u2019, \u2019commentator\u2019, \u2019commissioner\u2019, \u2019composer\u2019, \u2019conductor\u2019, \u2019confesses\u2019, \u2019congressman\u2019, \u2019constable\u2019, \u2019consultant\u2019, \u2019cop\u2019, \u2019correspondent\u2019, \u2019counselor\u2019, \u2019critic\u2019, \u2019crusader\u2019, \u2019curator\u2019, \u2019dad\u2019, \u2019dancer\u2019, \u2019dean\u2019, \u2019dentist\u2019, \u2019deputy\u2019, \u2019detective\u2019, \u2019diplomat\u2019, \u2019director\u2019, \u2019doctor\u2019, \u2019drummer\u2019, \u2019economist\u2019, \u2019editor\u2019, \u2019educator\u2019, \u2019employee\u2019, \u2019entertainer\u2019, \u2019entrepreneur\u2019, \u2019envoy\u2019, \u2019evangelist\u2019, \u2019farmer\u2019, \u2019\ufb01lmmaker\u2019, \u2019\ufb01nancier\u2019, \u2019\ufb01sherman\u2019, \u2019footballer\u2019, \u2019foreman\u2019, \u2019gangster\u2019, \u2019gardener\u2019, \u2019geologist\u2019, \u2019goalkeeper\u2019, \u2019guitarist\u2019, \u2019headmaster\u2019, \u2019historian\u2019, \u2019hooker\u2019, \u2019illustrator\u2019, \u2019industrialist\u2019,",
  "\u2019\ufb01nancier\u2019, \u2019\ufb01sherman\u2019, \u2019footballer\u2019, \u2019foreman\u2019, \u2019gangster\u2019, \u2019gardener\u2019, \u2019geologist\u2019, \u2019goalkeeper\u2019, \u2019guitarist\u2019, \u2019headmaster\u2019, \u2019historian\u2019, \u2019hooker\u2019, \u2019illustrator\u2019, \u2019industrialist\u2019, \u2019inspector\u2019, \u2019instructor\u2019, \u2019inventor\u2019, \u2019investigator\u2019, \u2019journalist\u2019, \u2019judge\u2019, \u2019jurist\u2019, \u2019landlord\u2019, \u2019lawyer\u2019, \u2019lecturer\u2019, \u2019legislator\u2019, \u2019librarian\u2019, \u2019lieutenant\u2019, \u2019lyricist\u2019, \u2019maestro\u2019, \u2019magician\u2019, \u2019magistrate\u2019, \u2019maid\u2019, \u2019manager\u2019, \u2019marshal\u2019, \u2019mathematician\u2019, \u2019mechanic\u2019, \u2019mid\ufb01elder\u2019, \u2019minister\u2019, \u2019missionary\u2019, \u2019monk\u2019, \u2019musician\u2019, \u2019nanny\u2019, \u2019narrator\u2019, \u2019naturalist\u2019, \u2019novelist\u2019, \u2019nun\u2019, \u2019nurse\u2019, \u2019observer\u2019, \u2019of\ufb01cer\u2019, \u2019organist\u2019, \u2019painter\u2019, \u2019pas- tor\u2019, \u2019performer\u2019, \u2019philanthropist\u2019, \u2019philosopher\u2019, \u2019photographer\u2019, \u2019physician\u2019, \u2019physicist\u2019, \u2019pianist\u2019,",
  "\u2019nun\u2019, \u2019nurse\u2019, \u2019observer\u2019, \u2019of\ufb01cer\u2019, \u2019organist\u2019, \u2019painter\u2019, \u2019pas- tor\u2019, \u2019performer\u2019, \u2019philanthropist\u2019, \u2019philosopher\u2019, \u2019photographer\u2019, \u2019physician\u2019, \u2019physicist\u2019, \u2019pianist\u2019, \u2019planner\u2019, \u2019playwright\u2019, \u2019poet\u2019, \u2019policeman\u2019, \u2019politician\u2019, \u2019preacher\u2019, \u2019president\u2019, \u2019priest\u2019, \u2019principal\u2019, \u2019prisoner\u2019, \u2019professor\u2019, \u2019programmer\u2019, \u2019promoter\u2019, \u2019proprietor\u2019, \u2019prosecutor\u2019, \u2019protagonist\u2019, \u2019provost\u2019, \u2019psychiatrist\u2019, \u2019psychologist\u2019, \u2019rabbi\u2019, \u2019ranger\u2019, \u2019researcher\u2019, \u2019sailor\u2019, \u2019saint\u2019, \u2019salesman\u2019, \u2019saxophonist\u2019, \u2019scholar\u2019, \u2019scientist\u2019, \u2019screenwriter\u2019, \u2019sculptor\u2019, \u2019secretary\u2019, \u2019senator\u2019, \u2019sergeant\u2019, \u2019servant\u2019, \u2019singer\u2019, \u2019skipper\u2019, \u2019sociologist\u2019, \u2019soldier\u2019, \u2019solicitor\u2019, \u2019soloist\u2019, \u2019sportsman\u2019, \u2019statesman\u2019, \u2019steward\u2019, \u2019student\u2019, \u2019substitute\u2019,",
  "\u2019secretary\u2019, \u2019senator\u2019, \u2019sergeant\u2019, \u2019servant\u2019, \u2019singer\u2019, \u2019skipper\u2019, \u2019sociologist\u2019, \u2019soldier\u2019, \u2019solicitor\u2019, \u2019soloist\u2019, \u2019sportsman\u2019, \u2019statesman\u2019, \u2019steward\u2019, \u2019student\u2019, \u2019substitute\u2019, \u2019superintendent\u2019, \u2019surgeon\u2019, \u2019surveyor\u2019, \u2019swimmer\u2019, \u2019teacher\u2019, \u2019technician\u2019, \u2019teenager\u2019, \u2019therapist\u2019, \u2019trader\u2019, \u2019treasurer\u2019, \u2019trooper\u2019, \u2019trumpeter\u2019, \u2019tutor\u2019, \u2019tycoon\u2019, \u2019violinist\u2019, \u2019vocalist\u2019, \u2019waiter\u2019, \u2019waitress\u2019, \u2019warden\u2019, \u2019warrior\u2019, \u2019worker\u2019, \u2019wrestler\u2019, \u2019writer\u2019 C WEAT Word Sets Words used for WEAT statistic, consisting of baseline bias tests and gender bias tests in the format X vs Y / A vs B Flowers vs Insects / Pleasant vs Unpleasant X: \"aster\", \"clover\", \"hyacinth\", \"marigold\", \"poppy\", \"azalea\", \"crocus\", \"iris\", \"orchid\", \"rose\", \"bluebell\", \"daffodil\", \"lilac\",",
  "\"clover\", \"hyacinth\", \"marigold\", \"poppy\", \"azalea\", \"crocus\", \"iris\", \"orchid\", \"rose\", \"bluebell\", \"daffodil\", \"lilac\", \"pansy\", \"tulip\", \"buttercup\", \"daisy\", \"lily\", \"peony\", \"violet\", \"carna- tion\", \"gladiola\", \"magnolia\", \"petunia\", \"zinnia\" Y: \"ant\", \"caterpillar\", \"\ufb02ea\", \"locust\", \"spider\", \"bedbug\", \"centipede\", \"\ufb02y\", \"maggot\", \"tarantula\", \"bee\", \"cockroach\", \"gnat\", \"mosquito\", \"termite\", \"beetle\", \"cricket\", \"hornet\", \"moth\", \"wasp\", \"black\ufb02y\", \"dragon\ufb02y\", \"horse\ufb02y\", \"roach\", \"weevil\" A: \"caress\", \"freedom\", \"health\", \"love\", \"peace\", \"cheer\", \"friend\", \"heaven\",",
  "\"wasp\", \"black\ufb02y\", \"dragon\ufb02y\", \"horse\ufb02y\", \"roach\", \"weevil\" A: \"caress\", \"freedom\", \"health\", \"love\", \"peace\", \"cheer\", \"friend\", \"heaven\", \"loyal\", \"pleasure\", \"diamond\", \"gentle\", \"honest\", \"lucky\", \"rainbow\", \"diploma\", \"gift\", \"honor\", \"miracle\", \"sunrise\", \"family\", \"happy\", \"laughter\", \"paradise\", \"vacation\" B: \"abuse\", \"crash\", \"\ufb01lth\", \"murder\", \"sickness\", \"accident\", \"death\", \"grief\", \"poison\", \"stink\", \"assault\", \"disaster\", \"hatred\", \"pollute\", \"tragedy\", \"divorce\", \"jail\", \"poverty\", \"ugly\", \"cancer\", \"kill\", \"rotten\", \"vomit\", \"agony\", \"prison\" Instruments vs Weapons / Pleasant vs Unpleasant: X: \"bagpipe\", \"cello\", \"guitar\",",
  "\"divorce\", \"jail\", \"poverty\", \"ugly\", \"cancer\", \"kill\", \"rotten\", \"vomit\", \"agony\", \"prison\" Instruments vs Weapons / Pleasant vs Unpleasant: X: \"bagpipe\", \"cello\", \"guitar\", \"lute\", \"trombone\", \"banjo\", \"clarinet\", \"harmonica\", \"mandolin\", \"trumpet\", \"bassoon\", \"drum\", \"harp\", \"oboe\", \"tuba\", \"bell\", \"\ufb01ddle\", \"harpsichord\", \"piano\", \"viola\", \"bongo\", \"\ufb02ute\", \"horn\", \"saxophone\", \"violin\" Y: \"arrow\", \"club\", \"gun\", \"missile\", \"spear\", \"ax\", \"dagger\", \"harpoon\", \"pistol\", \"sword\", \"blade\", \"dynamite\", \"hatchet\", \"ri\ufb02e\", \"tank\", \"bomb\", \"\ufb01rearm\", \"knife\", \"shotgun\", \"teargas\", \"cannon\", \"grenade\", \"mace\", \"slingshot\",",
  "\"sword\", \"blade\", \"dynamite\", \"hatchet\", \"ri\ufb02e\", \"tank\", \"bomb\", \"\ufb01rearm\", \"knife\", \"shotgun\", \"teargas\", \"cannon\", \"grenade\", \"mace\", \"slingshot\", \"whip\" 6",
  "A: \"caress\", \"freedom\", \"health\", \"love\", \"peace\", \"cheer\", \"friend\", \"heaven\", \"loyal\", \"pleasure\", \"diamond\", \"gentle\", \"honest\", \"lucky\", \"rainbow\", \"diploma\", \"gift\", \"honor\", \"miracle\", \"sunrise\", \"family\", \"happy\", \"laughter\", \"paradise\", \"vacation\" B: \"abuse\", \"crash\", \"\ufb01lth\", \"murder\", \"sickness\", \"accident\", \"death\", \"grief\", \"poison\", \"stink\", \"assault\", \"disaster\", \"hatred\", \"pollute\", \"tragedy\", \"divorce\", \"jail\", \"poverty\", \"ugly\", \"cancer\", \"kill\", \"rotten\", \"vomit\", \"agony\", \"prison\" Male vs Female / Career vs Family: X: \"brother\", \"father\", \"uncle\", \"grandfather\", \"son\", \"he\", \"his\", \"him\", \"man\", \"himself\", \"men\", \"husband\", \"boy\", \"uncle\",",
  "\"prison\" Male vs Female / Career vs Family: X: \"brother\", \"father\", \"uncle\", \"grandfather\", \"son\", \"he\", \"his\", \"him\", \"man\", \"himself\", \"men\", \"husband\", \"boy\", \"uncle\", \"nephew\", \"boyfriend\", \"king\", \"actor\" Y: \"sister\", \"mother\", \"aunt\", \"grandmother\", \"daughter\", \"she\", \"hers\", \"her\", \"woman\", \"herself\", \"women\", \"wife\", \"aunt\", \"niece\", \"girlfriend\", \"queen\", \"actress\" A: \"executive\", \"management\", \"professional\", \"corporation\", \"salary\", \"of\ufb01ce\", \"business\", \"career\", \"industry\", \"company\", \"promotion\", \"profession\", \"CEO\", \"manager\", \"coworker\", \"entrepreneur\" B: \"home\", \"parents\", \"children\", \"family\", \"cousins\", \"marriage\", \"wedding\", \"relatives\", \"grandpar- ents\", \"grandchildren\", \"nurture\", \"child\", \"toddler\",",
  "\"coworker\", \"entrepreneur\" B: \"home\", \"parents\", \"children\", \"family\", \"cousins\", \"marriage\", \"wedding\", \"relatives\", \"grandpar- ents\", \"grandchildren\", \"nurture\", \"child\", \"toddler\", \"infant\", \"teenager\" Math vs Art / Male vs Female: X: \"math\", \"algebra\", \"geometry\", \"calculus\", \"equations\", \"computation\", \"numbers\", \"addition\", \"trigonometry\", \"arithmetic\", \"logic\", \"proofs\", \"multiplication\", \"mathematics\" Y: \"poetry\", \"art\", \"Shakespeare\", \"dance\", \"literature\", \"novel\", \"symphony\", \"drama\", \"orchestra\", \"music\", \"ballet\", \"arts\", \"creative\", \"sculpture\" A: \"brother\", \"father\", \"uncle\", \"grandfather\", \"son\", \"he\", \"his\", \"him\", \"man\", \"himself\", \"men\", \"husband\", \"boy\", \"uncle\", \"nephew\", \"boyfriend\", \"king\",",
  "\"father\", \"uncle\", \"grandfather\", \"son\", \"he\", \"his\", \"him\", \"man\", \"himself\", \"men\", \"husband\", \"boy\", \"uncle\", \"nephew\", \"boyfriend\", \"king\", \"actor\" B: \"sister\", \"mother\", \"aunt\", \"grandmother\", \"daughter\", \"she\", \"hers\", \"her\", \"woman\", \"herself\", \"women\", \"wife\", \"aunt\", \"niece\", \"girlfriend\", \"queen\", \"actress\" Science vs Art / Male8 vs Female8: X:\"science\", \"technology\", \"physics\", \"chemistry\", \"Einstein\", \"NASA\", \"experiment\", \"astronomy\", \"biology\", \"aeronautics\", \"mechanics\", \"thermodynamics\" Y: \"poetry\", \"art\", \"Shakespeare\", \"dance\", \"literature\", \"novel\", \"symphony\", \"drama\", \"orchestra\", \"music\", \"ballet\", \"arts\", \"creative\", \"sculpture\" A: \"brother\", \"father\", \"uncle\", \"grandfather\", \"son\", \"he\",",
  "\"literature\", \"novel\", \"symphony\", \"drama\", \"orchestra\", \"music\", \"ballet\", \"arts\", \"creative\", \"sculpture\" A: \"brother\", \"father\", \"uncle\", \"grandfather\", \"son\", \"he\", \"his\", \"him\", \"man\", \"himself\", \"men\", \"husband\", \"boy\", \"uncle\", \"nephew\", \"boyfriend\" B: \"sister\", \"mother\", \"aunt\", \"grandmother\", \"daughter\", \"she\", \"hers\", \"her\", \"woman\", \"herself\", \"women\", \"wife\", \"aunt\", \"niece\", \"girlfriend\" 7"
]