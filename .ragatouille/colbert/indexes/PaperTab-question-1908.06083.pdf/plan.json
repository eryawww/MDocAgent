{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack Emily Dinan Facebook AI Research edinan@fb.com Samuel Humeau Facebook AI Research samuel.humeau@gmail.com Bharath Chintagunta Virginia Tech jaic4@vt.edu Jason Weston Facebook AI Research jase@fb.com Abstract The detection of offensive language in the con- text of a dialogue has become an increasingly important application of natural language pro- cessing. The detection of trolls in public fo- rums (Gal\u00b4an-Garc\u00b4\u0131a et al., 2016), and the de- ployment of chatbots in the public domain (Wolf et al., 2017) are two examples that show the necessity of guarding against adversarially offensive behavior on the part of humans. In this work, we develop a training scheme for a model to become robust to such human attacks by an iterative build it, break it, \ufb01x it strategy with humans and models in the loop. In de- tailed experiments we show this approach is considerably more robust than previous sys- tems.",
            "In this work, we develop a training scheme for a model to become robust to such human attacks by an iterative build it, break it, \ufb01x it strategy with humans and models in the loop. In de- tailed experiments we show this approach is considerably more robust than previous sys- tems. Further, we show that offensive lan- guage used within a conversation critically de- pends on the dialogue context, and cannot be viewed as a single sentence offensive detection task as in most previous work. Our newly col- lected tasks and methods will be made open source and publicly available. 1 Introduction The detection of offensive language has become an important topic as the online community has grown, as so too have the number of bad actors (Cheng et al., 2017). Such behavior includes, but is not limited to, trolling in public discussion fo- rums (Herring et al., 2002) and via social media (Silva et al., 2016; Davidson et al., 2017), employ- ing hate speech that expresses prejudice against a particular group, or offensive language specif- ically targeting an individual.",
            "Such actions can be motivated to cause harm from which the bad actor derives enjoyment, despite negative conse- quences to others (Bishop, 2014). As such, some bad actors go to great lengths to both avoid detec- tion and to achieve their goals (Shachaf and Hara, 2010). In that context, any attempt to automat- ically detect this behavior can be expected to be adversarially attacked by looking for weaknesses in the detection system, which currently can eas- ily be exploited as shown in (Hosseini et al., 2017; Gr\u00a8ondahl et al., 2018). A further example, rele- vant to the natural langauge processing commu- nity, is the exploitation of weaknesses in machine learning models that generate text, to force them to emit offensive language. Adversarial attacks on the Tay chatbot led to the developers shutting down the system (Wolf et al., 2017). In this work, we study the detection of offen- sive language in dialogue with models that are ro- bust to adversarial attack.",
            "Adversarial attacks on the Tay chatbot led to the developers shutting down the system (Wolf et al., 2017). In this work, we study the detection of offen- sive language in dialogue with models that are ro- bust to adversarial attack. We develop an auto- matic approach to the \u201cBuild it Break it Fix it\u201d strategy originally adopted for writing secure pro- grams (Ruef et al., 2016), and the \u201cBuild it Break it\u201d approach consequently adapting it for NLP (Et- tinger et al., 2017). In the latter work, two teams of researchers, \u201cbuilders\u201d and \u201cbreakers\u201d were used to \ufb01rst create sentiment and semantic role-labeling systems and then construct examples that \ufb01nd their faults. In this work we instead fully automate such an approach using crowdworkers as the humans- in-the-loop, and also apply a \ufb01xing stage where models are retrained to improve them. Finally, we repeat the whole build, break, and \ufb01x sequence over a number of iterations. We show that such an approach provides more and more robust systems over the \ufb01xing iterations.",
            "Finally, we repeat the whole build, break, and \ufb01x sequence over a number of iterations. We show that such an approach provides more and more robust systems over the \ufb01xing iterations. Analysis of the type of data collected in the itera- tions of the break it phase shows clear distribution changes, moving away from simple use of profan- ity and other obvious offensive words to utterances that require understanding of world knowledge, \ufb01gurative language, and use of negation to detect if they are offensive or not. Further, data collected in the context of a dialogue rather than a sentence without context provides more sophisticated at- tacks. We show that model architectures that use the dialogue context ef\ufb01ciently perform much bet- arXiv:1908.06083v1  [cs.CL]  17 Aug 2019",
            "ter than systems that do not, where the latter has been the main focus of existing research (Wulczyn et al., 2017; Davidson et al., 2017; Zampieri et al., 2019). Code for our entire build it, break it, \ufb01x it al- gorithm will be made open source, complete with model training code and crowdsourcing interface for humans. Our data and trained models will also be made available for the community. 2 Related Work The task of detecting offensive language has been studied across a variety of content classes. Perhaps the most commonly studied class is hate speech, but work has also covered bullying, aggression, and toxic comments (Zampieri et al., 2019). To this end, various datasets have been created to benchmark progress in the \ufb01eld. In hate speech detection, recently Davidson et al. (2017) com- piled and released a dataset of over 24,000 tweets labeled as containing hate speech, offensive lan- guage, or neither.",
            "To this end, various datasets have been created to benchmark progress in the \ufb01eld. In hate speech detection, recently Davidson et al. (2017) com- piled and released a dataset of over 24,000 tweets labeled as containing hate speech, offensive lan- guage, or neither. The TRAC shared task on Ag- gression Identi\ufb01cation, a dataset of over 15,000 Facebook comments labeled with varying levels of aggression, was released as part of a compe- tition (Kumar et al., 2018). In order to benchmark toxic comment detection, The Wikipedia Toxic Comments dataset (which we study in this work) was collected and extracted from Wikipedia Talk pages and featured in a Kaggle competition (Wul- czyn et al., 2017; Google, 2018). Each of these benchmarks examine only single-turn utterances, outside of the context in which the language ap- peared. In this work we recommend that future systems should move beyond classi\ufb01cation of sin- gular utterances and use contextual information to help identify offensive language.",
            "Each of these benchmarks examine only single-turn utterances, outside of the context in which the language ap- peared. In this work we recommend that future systems should move beyond classi\ufb01cation of sin- gular utterances and use contextual information to help identify offensive language. Many approaches have been taken to solve these tasks \u2013 from linear regression and SVMs to deep learning (Noever, 2018). The best performing sys- tems in each of the competitions mentioned above (for aggression and toxic comment classi\ufb01cation) used deep learning approaches such as LSTMs and CNNs (Kumar et al., 2018; Google, 2018). In this work we consider a large-pretrained transformer model which has been shown to perform well on many downstream NLP tasks (Devlin et al., 2018). The broad class of adversarial training is cur- rently a hot topic in machine learning (Goodfel- low et al., 2014). Use cases include training im- age generators (Brock et al., 2018) as well as im- age classi\ufb01ers to be robust to adversarial examples (Liu et al., 2019).",
            "Use cases include training im- age generators (Brock et al., 2018) as well as im- age classi\ufb01ers to be robust to adversarial examples (Liu et al., 2019). These methods \ufb01nd the break- ing examples algorithmically, rather than by us- ing humans breakers as we do. Applying the same approaches to NLP tends to be more challenging because, unlike for images, even small changes to a sentence can cause a large change in the mean- ing of that sentence, which a human can detect but a lower quality model cannot. Nevertheless algo- rithmic approaches have been attempted, for ex- ample in text classi\ufb01cation (Ebrahimi et al., 2018), machine translation (Belinkov and Bisk, 2018), di- alogue generation tasks (Li et al., 2017) and read- ing comprehension (Jia and Liang, 2017). The lat- ter was particularly effective at proposing a more dif\ufb01cult version of the popular SQuAD dataset.",
            "The lat- ter was particularly effective at proposing a more dif\ufb01cult version of the popular SQuAD dataset. As mentioned in the introduction, our approach takes inspiration from \u201cBuild it Break it\u201d ap- proaches which have been successfully tried in other domains (Ruef et al., 2016; Ettinger et al., 2017). Those approaches advocate \ufb01nding faults in systems by having humans look for insecurities (in software) or prediction failures (in models), but do not advocate an automated approach as we do here. Our work is also closely connected to the \u201cMechanical Turker Descent\u201d algorithm detailed in (Yang et al., 2018) where language to action pairs were collected from crowdworkers by incen- tivizing them with a game-with-a-purpose tech- nique: a crowdworker receives a bonus if their contribution results in better models than another crowdworker. We did not gamify our approach in this way, but still our approach has common- alities in the round-based improvement of models through crowdworker interaction.",
            "We did not gamify our approach in this way, but still our approach has common- alities in the round-based improvement of models through crowdworker interaction. 3 Baselines: Wikipedia Toxic Comments In this section we describe the publicly available data that we have used to bootstrap our build it break it \ufb01x it approach. We also compare our model choices with existing work and clarify the metrics chosen to report our results. Wikipedia Toxic Comments The Wikipedia Toxic Comments dataset (WTC) has been col- lected in a common effort from the Wikimedia Foundation and Jigsaw (Wulczyn et al., 2017) to identify personal attacks online. The data has been extracted from the Wikipedia Talk pages, dis- cussion pages where editors can discuss improve- ments to articles or other Wikipedia pages. We considered the version of the dataset that corre-",
            "sponds to the Kaggle competition: \u201cToxic Com- ment Classi\ufb01cation Challenge\u201d (Google, 2018) which features 7 classes of toxicity: toxic, se- vere toxic, obscene, threat, insult, identity hate and non-toxic. In the same way as in (Khatri et al., 2018), every label except non-toxic is grouped into a class OFFENSIVE while the non-toxic class is kept as the SAFE class. In order to compare our results to (Khatri et al., 2018), we similarly split this dataset to dedicate 10% as a test set. 80% are dedicated to train set while the remaining 10% is used for validation. Statistics on the dataset are shown in Table 1. Models We establish baselines using two mod- els. The \ufb01rst one is a binary classi\ufb01er built on top of a large pre-trained transformer model. We use the same architecture as in BERT (Devlin et al., 2018). We add a linear layer to the output of the \ufb01rst token ([CLS]) to produce a \ufb01nal binary classi- \ufb01cation.",
            "We use the same architecture as in BERT (Devlin et al., 2018). We add a linear layer to the output of the \ufb01rst token ([CLS]) to produce a \ufb01nal binary classi- \ufb01cation. We initialize the model using the weights provided by (Devlin et al., 2018) corresponding to \u201cBERT-base\u201d. The transformer is composed of 12 layers with hidden size of 768 and 12 atten- tion heads. We \ufb01ne-tune the whole network on the classi\ufb01cation task. We also compare it the fastText classi\ufb01er (Joulin et al., 2017) for which a given sentence is encoded as the average of individual word vectors that are pre-trained on a large cor- pus issued from Wikipedia. A linear layer is then applied on top to yield a binary classi\ufb01cation. Experiments We compare the two aforemen- tioned models with (Khatri et al., 2018) who con- ducted their experiments with a BiLSTM with GloVe pre-trained word vectors (Pennington et al., 2014).",
            "Experiments We compare the two aforemen- tioned models with (Khatri et al., 2018) who con- ducted their experiments with a BiLSTM with GloVe pre-trained word vectors (Pennington et al., 2014). Results are listed in Table 2 and we com- pare them using the weighted-F1, i.e. the sum of F1 score of each class weighted by their fre- quency in the dataset. We also report the F1 of the OFFENSIVE-class which is the metric we fa- vor within this work, although we report both. (Note that throughout the paper, the notation F1 is always referring to OFFENSIVE-class F1.) In- deed, in the case of an imbalanced dataset such as Wikipedia Toxic Comments where most sam- ples are SAFE, the weighted-F1 is closer to the F1 score of the SAFE class while we focus on detect- ing OFFENSIVE content. Our BERT-based model outperforms the method from Khatri et al. (2018); throughout the rest of the paper, we use the BERT- based architecture in our experiments.",
            "Our BERT-based model outperforms the method from Khatri et al. (2018); throughout the rest of the paper, we use the BERT- based architecture in our experiments. In particu- Train Valid Test SAFE 89.8% 89.7% 90.1% OFFENSIVE 10.2% 10.3% 9.1% Total 114656 15958 15957 Table 1: Dataset statistics for our splits of Wikipedia Toxic Comments. OFFENSIVE F1 Weighted F1 fastText 71.4% 94.8% BERT-based 83.4% 96.7% (Khatri et al., 2018) - 95.4% Table 2: Comparison between our models based on fastText and BERT with the BiLSTM used by (Khatri et al., 2018) on Wikipedia Toxic Comments. lar, we used this baseline trained on WTC to boot- strap our approach, to be described subsequently.",
            "lar, we used this baseline trained on WTC to boot- strap our approach, to be described subsequently. 4 Build it Break it Fix it Method In order to train models that are robust to adver- sarial behavior, we posit that it is crucial collect and train on data that was collected in an adversar- ial manner. We propose the following automated build it, break it, \ufb01x it algorithm: 1. Build it: Build a model capable of detect- ing OFFENSIVE messages. This is our best- performing BERT-based model trained on the Wikipedia Toxic Comments dataset de- scribed in the previous section. We refer to this model throughout as A0. 2. Break it: Ask crowdworkers to try to \u201cbeat the system\u201d by submitting messages that our system (A0) marks as SAFE but that the worker considers to be OFFENSIVE. 3. Fix it: Train a new model on these collected examples in order to be more robust to these adversarial attacks. 4. Repeat: Repeat, deploying the newly trained model in the break it phase, then \ufb01x it again. See Figure 1 for a visualization of this process.",
            "Fix it: Train a new model on these collected examples in order to be more robust to these adversarial attacks. 4. Repeat: Repeat, deploying the newly trained model in the break it phase, then \ufb01x it again. See Figure 1 for a visualization of this process. 4.1 Break it Details De\ufb01nition of OFFENSIVE Throughout data col- lection, we characterize OFFENSIVE messages for users as messages that would not be \u201cok to send in a friendly conversation with someone you just met online.\u201d We use this speci\ufb01c language in an",
            "A0 SAFE OFFENSIVE Build it  Existing data training Not broken: try again! Adversarial data Broken: add to new dataset training prediction deploy deploy Offensive Message breaker A0 A1 A0 + A1 Break it  (ROUND 2)  Fix it  prediction training Offensive Message breaker Break it  (ROUND 1)  (ROUND 1)  Figure 1: The build it, break it, \ufb01x it algorithm we use to iteratively train better models A0, . . . , AN. In exper- iments we perform N = 3 iterations of the break it, \ufb01x it loop for the single-turn utterance detection task, and a further iteration for the multi-turn task in a dialogue context setting. attempt to capture various classes of content that would be considered unacceptable in a friendly conversation, without imposing our own de\ufb01ni- tions of what that means. The phrase \u201cwith some- one you just met online\u201d was meant to mimic the setting of a public forum. Crowderworker Task We ask crowdworkers to try to \u201cbeat the system\u201d by submitting messages that our system marks as SAFE but that the worker considers to be OFFENSIVE.",
            "The phrase \u201cwith some- one you just met online\u201d was meant to mimic the setting of a public forum. Crowderworker Task We ask crowdworkers to try to \u201cbeat the system\u201d by submitting messages that our system marks as SAFE but that the worker considers to be OFFENSIVE. For a given round, workers earn a \u201cgame\u201d point each time they are able to \u201cbeat the system,\u201d or in other words, trick the model by submitting OFFENSIVE messages that the model marks as SAFE. Workers earn up to 5 points each round, and have two tries for each point: we allow multiple attempts per point so that workers can get feedback from the models and bet- ter understand their weaknesses. The points serve to indicate success to the crowdworker and mo- tivate to achieve high scores, but have no other meaning (e.g. no monetary value as in (Yang et al., 2018)). More details regarding the user interface and instructions can be found in Appendix B. Models to Break During round 1, workers try to break the baseline model A0, trained on Wikipedia Toxic Comments.",
            "no monetary value as in (Yang et al., 2018)). More details regarding the user interface and instructions can be found in Appendix B. Models to Break During round 1, workers try to break the baseline model A0, trained on Wikipedia Toxic Comments. For rounds i, i > 1, workers must break both the baseline model and the model from the previous \u201c\ufb01x it\u201d round, which we refer to as Ai\u22121. In that case, the worker must submit messages that both A0 and Ai\u22121 mark as SAFE but which the worker considers to be OFFENSIVE. 4.2 Fix it Details During the \u201c\ufb01x it\u201d round, we update the models with the newly collected adversarial data from the \u201cbreak it\u201d round. The training data consists of all previous rounds of data, so that model Ai is trained on all rounds n for n \u2264i, as well as the Wikipedia Toxic Com- ments data. We split each round of data into train, validation, and test partitions. The validation set is used for hyperparameter selection. The test sets are used to measure how robust we are to new adversarial attacks.",
            "We split each round of data into train, validation, and test partitions. The validation set is used for hyperparameter selection. The test sets are used to measure how robust we are to new adversarial attacks. With increasing round i, Ai should become more robust to increasingly com- plex human adversarial attacks. 5 Single-Turn Task We \ufb01rst consider a single-turn set-up, i.e. detec- tion of offensive language in one utterance, with no dialogue context or conversational history. 5.1 Data Collection Adversarial Collection We collected three rounds of data with the build it, break it, \ufb01x it algorithm described in the previous section. Each round of data consisted of 1000 examples, leading to 3000 single-turn adversarial examples in total. For the remainder of the paper, we refer to this method of data collection as the adversarial method. Standard Collection In addition to the adver- sarial method, we also collected data in a non- adversarial manner in order to directly compare the two set-ups.",
            "For the remainder of the paper, we refer to this method of data collection as the adversarial method. Standard Collection In addition to the adver- sarial method, we also collected data in a non- adversarial manner in order to directly compare the two set-ups. In this method \u2013 which we refer to as the standard method, we simply ask crowd- workers to submit messages that they consider to be OFFENSIVE. There is no model to break. In- structions are otherwise the same. In this set-up, there is no real notion of \u201crounds\u201d, but for the sake of comparison we re- fer to each subsequent 1000 examples collected in",
            "Single-Turn Adversarial (Round 1) and Standard Task OFFENSIVE Examples contains non-profane contains contains requires contains profanity offending words negation \ufb01gurative language world knowledge sarcasm Standard 13% 12% 12% 11% 8% 3% Adversarial 0% 5% 23% 19% 14% 6% Table 3: Language analysis of the single-turn standard and adversarial (round 1) tasks by human annotation of various language properties. Standard collection examples contain more words found in an offensive words list, while adversarial examples require more sophisticated language understanding. this manner as a \u201cround\u201d. We collect 3000 exam- ples \u2013 or three rounds of data. We refer to a model trained on rounds n \u2264i of the standard data as Si. 5.1.1 Task Formulation Details Since all of the collected examples are labeled as OFFENSIVE, to make this task a binary classi\ufb01ca- tion problem, we will also add SAFE examples to it.",
            "5.1.1 Task Formulation Details Since all of the collected examples are labeled as OFFENSIVE, to make this task a binary classi\ufb01ca- tion problem, we will also add SAFE examples to it. The \u201csafe data\u201d is comprised of utterances from the ConvAI2 chit-chat task (Dinan et al., 2019; Zhang et al., 2018) which consists of pairs of hu- mans getting to know each other by discussing their interests. Each utterance we used was re- viewed by two independent crowdworkers and la- beled as SAFE, with the same characterization of SAFE as described before. For each partition (train, validation, test), the \ufb01- nal task has a ratio of 9:1 SAFE to OFFENSIVE ex- amples, mimicking the division of the Wikipedia Toxic Comments dataset used for training our baseline models. Dataset statistics for the \ufb01nal task can be found in Table 5. We refer to these tasks \u2013 with both SAFE and OFFENSIVE examples \u2013 as the adversarial and standard tasks.",
            "Dataset statistics for the \ufb01nal task can be found in Table 5. We refer to these tasks \u2013 with both SAFE and OFFENSIVE examples \u2013 as the adversarial and standard tasks. 5.1.2 Model Training Details Using the BERT-based model architecture de- scribed in Section 3, we trained models on each round of the standard and adversarial tasks, multi-tasking with the Wikipedia Toxic Comments task. We weight the multi-tasking with a mixing parameter which is also tuned on the validation set. Finally, after training weights with the cross entropy loss, we adjust the \ufb01nal bias also using the validation set. We optimize for the sensitive class (i.e. OFFENSIVE-class) F1 metric on the standard and adversarial validation sets respectively. For each task (standard and adversarial), on round i, we train on data from all rounds n for n \u2264i and optimize for performance on the valida- tion sets n \u2264i. % with % with avg. # avg. # profanity \u201cnot\u201d chars tokens Std. (Rnds 1-3) 18.2 2.8 48.6 9.4 Adv.",
            "% with % with avg. # avg. # profanity \u201cnot\u201d chars tokens Std. (Rnds 1-3) 18.2 2.8 48.6 9.4 Adv. Rnd 1 2.6 5.8 53.7 10.3 Adv. Rnd 2 1.5 5.5 44.5 9 Adv. Rnd 3 1.2 9.8 45.7 9.3 Multi-turn Adv. 1.6 4.9 36.6 7.8 Table 4: Percent of OFFENSIVE examples in each task containing profanity, the token \u201cnot\u201d, as well as the av- erage number of characters and tokens in each exam- ple. Rows 1-4 are the single-turn task, and the last row is the multi-turn task. Later rounds have less profan- ity and more use of negation as human breakers have to \ufb01nd more sophisticated language to adversarially at- tack our models.",
            "Rows 1-4 are the single-turn task, and the last row is the multi-turn task. Later rounds have less profan- ity and more use of negation as human breakers have to \ufb01nd more sophisticated language to adversarially at- tack our models. Rounds {1, 2 and 3} Train Valid Test SAFE Examples 21,600 2700 2700 OFFENSIVE Examples 2400 300 300 Total Examples 24,000 3,000 3,000 Table 5: Dataset statistics for the single-turn rounds of the adversarial task data collection. There are three rounds in total all of identical size, hence the numbers above can be divided for individual statistics. The stan- dard task is an additional dataset of exactly the same size as above. 5.2 Experimental Results We conduct experiments comparing the adversar- ial and standard methods. We break down the re- sults into \u201cbreak it\u201d results comparing the data col- lected and \u201c\ufb01x it\u201d results comparing the models obtained.",
            "5.2 Experimental Results We conduct experiments comparing the adversar- ial and standard methods. We break down the re- sults into \u201cbreak it\u201d results comparing the data col- lected and \u201c\ufb01x it\u201d results comparing the models obtained. 5.2.1 Break it Phase Examples obtained from both the adversarial and standard collection methods were found to be clearly offensive, but we note several differences in the distribution of examples from each task, shown in Table 4. First, examples from the stan- dard task tend to contain more profanity. Using a list of common English obscenities and otherwise",
            "WTC Baseline Standard models Adversarial models Task Type Task Round A0 S1 S2 S3 A1 A2 A3 WTC - 83.3 80.6 81.1 82.1 81.3 78.9 78.0 Standard Task All (1-3) 68.1 83.3 85.8 88.0 83.0 85.3 83.7 Adversarial Task 1 0.0 51.7 69.3 68.6 71.8 79.0 78.2 2 0.0 10.8 26.4 31.8 0.0 64.4 62.1 3 0.0 12.3 17.1 13.7 32.1 0.0 59.9 All (1-3) 0.0 27.4 41.7 41.8 40.6 55.5 67.",
            "1 3 0.0 12.3 17.1 13.7 32.1 0.0 59.9 All (1-3) 0.0 27.4 41.7 41.8 40.6 55.5 67.6 Table 6: Test performance of best standard models trained on standard task rounds (models Si for each round i) and best adversarial models trained on adversarial task rounds (models Ai). All models are evaluated using OFFENSIVE-class F1 on each round of both the standard task and adversarial task. A0 is the baseline model trained on the existing Wiki Toxic Comments (WTC) dataset. Adversarial models prove to be more robust than standard ones against attack (Adversarial Task 1-3), while still performing reasonably on Standard and WTC tasks. bad words1, in Table 4 we calculate the percentage of examples in each task containing such obscen- ities, and see that the standard examples contain at least seven times as many as each round of the adversarial task.",
            "bad words1, in Table 4 we calculate the percentage of examples in each task containing such obscen- ities, and see that the standard examples contain at least seven times as many as each round of the adversarial task. Additionally, in previous works, authors have observed that classi\ufb01ers struggle with negations (Hosseini et al., 2017). This is borne out by our data: examples from the single-turn ad- versarial task more often contain the token \u201cnot\u201d than examples from the standard task, indicating that users are easily able to fool the classi\ufb01er with negations. We also anecdotally see \ufb01gurative language such as \u201csnakes hiding in the grass\u201d in the ad- versarial data, which contain no individually of- fensive words, the offensive nature is captured by reading the entire sentence. Other examples re- quire sophisticated world knowledge such as that many cultures consider eating cats to be offen- sive. To quantify these differences, we performed a blind human annotation of a sample of the data, 100 examples of standard and 100 examples of ad- versarial round 1. Results are shown in Table 3.",
            "To quantify these differences, we performed a blind human annotation of a sample of the data, 100 examples of standard and 100 examples of ad- versarial round 1. Results are shown in Table 3. Adversarial data was indeed found to contain less profanity, fewer non-profane but offending words (such as \u201cidiot\u201d), more \ufb01gurative language, and to require more world knowledge. We note that, as anticipated, the task becomes more challenging for the crowdworkers with each round, indicated by the decreasing average scores in Table 7. In round 1, workers are able to get past A0 most of the time \u2013 earning an average score of 4.56 out of 5 points per round \u2013 showcasing how susceptible this baseline is to adversarial at- 1https:\/\/github.com\/LDNOOBW\/List-of-Dirty-Naughty- Obscene-and-Otherwise-Bad-Words Single-Turn Multi Round 1 2 3 (\u201c4\u201d) Avg. score (0-5) 4.56 2.56 1.6 2.89 Table 7: Adversarial data collection worker scores.",
            "score (0-5) 4.56 2.56 1.6 2.89 Table 7: Adversarial data collection worker scores. Workers received a score out of 5 indicating how often (out of 5 rounds) they were able to get past our clas- si\ufb01ers within two tries. In later single-turn rounds it is harder to defeat our models, but switching to multi-turn makes this easier again as new attacks can be found by using the dialogue context. tack despite its relatively strong performance on the Wikipedia Toxic Comments task. By round 3, however, workers struggle to trick the system, earning an average score of only 1.6 out of 5. A \ufb01ner-grained assessment of the worker scores can be found in Table 11 in the appendix. 5.2.2 Fix it Phase Results comparing the performance of models trained on the adversarial (Ai) and standard (Si) tasks are summarized in Table 6, with further re- sults in Table 13 in Appendix A.2.",
            "5.2.2 Fix it Phase Results comparing the performance of models trained on the adversarial (Ai) and standard (Si) tasks are summarized in Table 6, with further re- sults in Table 13 in Appendix A.2. The adversar- ially trained models Ai prove to be more robust to adversarial attack: on each round of adversarial testing they outperform standard models Si. Further, note that the adversarial task becomes harder with each subsequent round. In particu- lar, the performance of the standard models Si rapidly deteriorates between round 1 and round 2 of the adversarial task. This is a clear indi- cation that models need to train on adversarially- collected data to be robust to adversarial behavior. Standard models (Si), trained on the standard data, tend to perform similarly to the adversarial",
            "models (Ai) as measured on the standard test sets, with the exception of training round 3, in which A3 fails to improve on this task, likely due to be- ing too optimized for adversarial tasks. The stan- dard models Si, on the other hand, are improving with subsequent rounds as they have more training data of the same distribution as the evaluation set. Similarly, our baseline model performs best on its own test set, but other models are not far behind. Finally, we remark that all scores of 0 in Table 6 are by design, as for round i of the adversarial task, both A0 and Ai\u22121 classi\ufb01ed each example as SAFE during the \u2018break it\u2019 data collection phase. 6 Multi-Turn Task In most real-world applications, we \ufb01nd that ad- versarial behavior occurs in context \u2013 whether it is in the context of a one-on-one conversation, a comment thread, or even an image. In this work we focus on offensive utterances within the con- text of two-person dialogues.",
            "In this work we focus on offensive utterances within the con- text of two-person dialogues. For dialogue safety we posit it is important to move beyond classify- ing single utterances, as it may be the case that an utterance is entirely innocuous on its own but extremely offensive in the context of the previous dialogue history. For instance, \u201cYes, you should de\ufb01nitely do it!\u201d is a rather inoffensive message by itself, but most would agree that it is a hurtful response to the question \u201cShould I hurt myself?\u201d 6.1 Task Implementation To this end, we collect data by asking crowdwork- ers to try to \u201cbeat\u201d our best single-turn classi\ufb01er (using the model that performed best on rounds 1- 3 of the adversarial task, i.e., A3), in addition to our baseline classi\ufb01er A0. The workers are shown truncated pieces of a conversation from the Con- vAI2 chit-chat task, and asked to continue the con- versation with OFFENSIVE responses that our clas- si\ufb01er marks as SAFE.",
            "The workers are shown truncated pieces of a conversation from the Con- vAI2 chit-chat task, and asked to continue the con- versation with OFFENSIVE responses that our clas- si\ufb01er marks as SAFE. As before, workers have two attempts per conversation to try to get past the classi\ufb01er and are shown \ufb01ve conversations per round. They are given a score (out of \ufb01ve) at the end of each round indicating the number of times they successfully fooled the classi\ufb01er. We collected 3000 offensive examples in this manner. As in the single-turn set up, we com- bine this data with SAFE examples with a ratio of 9:1 SAFE to OFFENSIVE for classi\ufb01er training. The safe examples are dialogue examples from ConvAI2 for which the responses were reviewed by two independent crowdworkers and labeled as SAFE, as in the s single-turn task set-up. We refer to this overall task as the multi-turn adversarial task. Dataset statistics are given in Table 9. 6.2 Models To measure the impact of the context, we train models on this dataset with and without the given context.",
            "We refer to this overall task as the multi-turn adversarial task. Dataset statistics are given in Table 9. 6.2 Models To measure the impact of the context, we train models on this dataset with and without the given context. We use the fastText and the BERT-based model described in Section 3. In addition, we build a BERT-based model variant that splits the last utterance (to be classi\ufb01ed) and the rest of the history into two dialogue segments. Each segment is assigned an embedding and the input provided to the transformer is the sum of word embedding and segment embedding, replicating the setup of the Next Sentence Prediction that is used in the training of BERT (Devlin et al., 2018). 6.3 Experimental Results 6.3.1 Break it Phase During data collection, we observed that workers had an easier time bypassing the classi\ufb01ers than in the single-turn set-up. See Table 7.",
            "6.3 Experimental Results 6.3.1 Break it Phase During data collection, we observed that workers had an easier time bypassing the classi\ufb01ers than in the single-turn set-up. See Table 7. In the single- turn set-up, the task at hand gets harder with each round \u2013 the average score of the crowdworkers de- creases from 4.56 in round 1 to 1.6 in round 3. De- spite the fact that we are using our best single-turn classi\ufb01er in the multi-turn set-up (A3), the task be- comes easier: the average score per round is 2.89. This is because the workers are often able to use contextual information to suggest something of- fensive rather than say something offensive out- right. See examples of submitted messages in Ta- ble 8. Having context also allows one to express something offensive more ef\ufb01ciently: the mes- sages supplied by workers in the multi-turn setting were signi\ufb01cantly shorter on average, see Table 4.",
            "See examples of submitted messages in Ta- ble 8. Having context also allows one to express something offensive more ef\ufb01ciently: the mes- sages supplied by workers in the multi-turn setting were signi\ufb01cantly shorter on average, see Table 4. 6.3.2 Fix it Phase During training, we multi-tasked the multi-turn adversarial task with the Wikipedia Toxic Com- ments task as well as the single-turn adversarial and standard tasks. We average the results of our best models from \ufb01ve different training runs. The results of these experiments are given in Table 10. As we observed during the training of our base- lines in Section 3, the fastText model architec- ture is ill-equipped for this task relative to our BERT-based architectures. The fastText model performs worse given the dialogue context (an average of 23.56 OFFENSIVE-class F1 relative",
            "Multi-Turn Adversarial Task Examples Context [P1:] greetings and salutations my friend [P2:] hi how are you doing? [P1:] i\u2019m well. enjoying some r r from the navy station [P2:] i love to cook. do you cook? [P1:] i do on occasion. i love to read more though. OFFENSIVE Response [P2:] reading the menu doesnt count Context [P1:] hey how are you [P2:] good but i keep thinking about the time i almost played for nba [P1:] aww why did not you play [P2:] lost my nerve i guess. what do you do? [P1:] oh that\u2019s to bad. i work at a gas station OFFENSIVE Response [P2:] i need to follow my dream to not work in a gas station Table 8: Examples from the multi-turn adversarial task. Responses can be offensive only in context.",
            "[P1:] oh that\u2019s to bad. i work at a gas station OFFENSIVE Response [P2:] i need to follow my dream to not work in a gas station Table 8: Examples from the multi-turn adversarial task. Responses can be offensive only in context. Multi-Turn Adversarial Task Train Valid Test SAFE Examples 21,600 2,700 2,700 OFFENSIVE Examples 2,400 300 300 Total Examples 24,000 3,000 3,000 Table 9: Multi-turn adversarial task data statistics.",
            "Multi-Turn Adversarial Task Train Valid Test SAFE Examples 21,600 2,700 2,700 OFFENSIVE Examples 2,400 300 300 Total Examples 24,000 3,000 3,000 Table 9: Multi-turn adversarial task data statistics. Multi-Turn Adversarial Task Results F1 Weighted-F1 fastText with context 23.6 \u00b1 1.9 85.9 \u00b1 0.5 without context 37.1 \u00b1 2.6 88.8 \u00b1 0.6 BERT-based (no segments) with context 60.5 \u00b1 1.3 92.2 \u00b1 0.3 without context 56.8 \u00b1 1.6 90.6 \u00b1 0.7 BERT-based (dialogue segments) with context 66.4 \u00b1 2.2 93.2 \u00b1 0.4 without context 59.0 \u00b1 2.5 91.2 \u00b1 0.8 Table 10: Results of experiments on the multi-turn ad- versarial task. We denote the average and one stan- dard deviation from the results of \ufb01ve runs.",
            "We denote the average and one stan- dard deviation from the results of \ufb01ve runs. Models that use the context as input (\u201cwith context\u201d) perform bet- ter. Encoding this in the architecture as well (via BERT dialogue segment features) gives us the best results. to 37.1) than without, likely because its bag-of- embeddings representation is too simple to take the context into account. We see the opposite with our BERT-based mod- els, indicating that more complex models are able to effectively use the contextual information to detect whether the response is SAFE or OFFEN- SIVE. With the simple BERT-based architecture (that does not split the context and the utterance into separate segments), we observe an average of a 3.7 point increase in OFFENSIVE-class F1 with the addition of context. When we use segments to separate the context from the utterance we are trying to classify, we observe an average of a 7.4 point increase in OFFENSIVE-class F1.",
            "When we use segments to separate the context from the utterance we are trying to classify, we observe an average of a 7.4 point increase in OFFENSIVE-class F1. Thus, it appears that the use of contextual information to identify OFFENSIVE language is critical to making these systems robust, and improving the model ar- chitecture to take account of this has large impact. 7 Conclusion We have presented an approach to build more ro- bust offensive language detection systems in the context of a dialogue. We proposed a build it, break it, \ufb01x it, and then repeat strategy, whereby humans attempt to break the models we built, and we use the broken examples to \ufb01x the models. We show this results in far more nuanced language than in existing datasets. The adversarial data in- cludes less profanity, which existing classi\ufb01ers can pick up on, and is instead offensive due to \ufb01gu- rative language, negation, and by requiring more world knowledge, which all make current classi- \ufb01ers fail. Similarly, offensive language in the con- text of a dialogue is also more nuanced than stand- alone offensive utterances.",
            "Similarly, offensive language in the con- text of a dialogue is also more nuanced than stand- alone offensive utterances. We show that classi- \ufb01ers that learn from these more complex examples are indeed more robust to attack, and that using the dialogue context gives improved performance if the model architecture takes it into account. In this work we considered a binary problem (offensive or safe). Future work could consider classes of offensive language separately (Zampieri et al., 2019), or explore other dialogue tasks, e.g. from social media or forums. Another interesting direction is to explore how our build it, break it, \ufb01x it strategy would similarly apply to make neural generative models safe (Henderson et al., 2018).",
            "References 2018. 6th International Conference on Learning Rep- resentations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceed- ings. OpenReview.net. Yonatan Belinkov and Yonatan Bisk. 2018. Synthetic and natural noise both break neural machine transla- tion. In (DBL, 2018). Jonathan Bishop. 2014. Representations of trolls in mass media communication: a review of media- texts and moral panics relating to internet trolling. International Journal of Web Based Communities, 10(1):7\u201324. Andrew Brock, Jeff Donahue, and Karen Simonyan. 2018. Large scale gan training for high \ufb01- delity natural image synthesis. arXiv preprint arXiv:1809.11096. Justin Cheng, Cristian Danescu-Niculescu-Mizil, Jure Leskovec, and Michael Bernstein. 2017. Anyone can become a troll: Causes of trolling behavior in online discussions. American Scientist, 105(3):152.",
            "Justin Cheng, Cristian Danescu-Niculescu-Mizil, Jure Leskovec, and Michael Bernstein. 2017. Anyone can become a troll: Causes of trolling behavior in online discussions. American Scientist, 105(3):152. Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated hate speech detection and the problem of offensive language. In Eleventh International AAAI Conference on Web and Social Media. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. CoRR, abs\/1810.04805. Emily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al. 2019. The second conversational intelligence challenge (convai2). arXiv preprint arXiv:1902.00098.",
            "2019. The second conversational intelligence challenge (convai2). arXiv preprint arXiv:1902.00098. Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. 2018. Hot\ufb02ip: White-box adversarial exam- ples for text classi\ufb01cation. In Proceedings of the 56th Annual Meeting of the Association for Com- putational Linguistics, ACL 2018, Melbourne, Aus- tralia, July 15-20, 2018, Volume 2: Short Papers, pages 31\u201336. Association for Computational Lin- guistics. Allyson Ettinger, Sudha Rao, Hal Daum\u00b4e III, and Emily M Bender. 2017. Towards linguistically gen- eralizable nlp systems: A workshop and shared task. arXiv preprint arXiv:1711.01505. Patxi Gal\u00b4an-Garc\u00b4\u0131a, Jos\u00b4e Gaviria de la Puerta, Car- los Laorden G\u00b4omez, Igor Santos, and Pablo Garc\u00b4\u0131a Bringas.",
            "arXiv preprint arXiv:1711.01505. Patxi Gal\u00b4an-Garc\u00b4\u0131a, Jos\u00b4e Gaviria de la Puerta, Car- los Laorden G\u00b4omez, Igor Santos, and Pablo Garc\u00b4\u0131a Bringas. 2016. Supervised machine learning for the detection of troll pro\ufb01les in twitter social network: Application to a real case of cyberbullying. Logic Journal of the IGPL, 24(1):42\u201353. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative ad- versarial nets. In Advances in neural information processing systems, pages 2672\u20132680. Google. 2018. Toxic comment classi\ufb01cation challenge. Tommi Gr\u00a8ondahl, Luca Pajola, Mika Juuti, Mauro Conti, and N Asokan. 2018. All you need is\u201d love\u201d: Evading hate-speech detection.",
            "2018. Toxic comment classi\ufb01cation challenge. Tommi Gr\u00a8ondahl, Luca Pajola, Mika Juuti, Mauro Conti, and N Asokan. 2018. All you need is\u201d love\u201d: Evading hate-speech detection. arXiv preprint arXiv:1808.09115. Peter Henderson, Koustuv Sinha, Nicolas Angelard- Gontier, Nan Rosemary Ke, Genevieve Fried, Ryan Lowe, and Joelle Pineau. 2018. Ethical challenges in data-driven dialogue systems. In Proceedings of the 2018 AAAI\/ACM Conference on AI, Ethics, and Society, AIES \u201918, pages 123\u2013129, New York, NY, USA. ACM. Susan Herring, Kirk Job-Sluder, Rebecca Scheckler, and Sasha Barab. 2002. Searching for safety online: Managing\u201d trolling\u201d in a feminist forum. The infor- mation society, 18(5):371\u2013384.",
            "ACM. Susan Herring, Kirk Job-Sluder, Rebecca Scheckler, and Sasha Barab. 2002. Searching for safety online: Managing\u201d trolling\u201d in a feminist forum. The infor- mation society, 18(5):371\u2013384. Hossein Hosseini, Sreeram Kannan, Baosen Zhang, and Radha Poovendran. 2017. Deceiving google\u2019s perspective api built for detecting toxic comments. arXiv preprint arXiv:1702.08138. Robin Jia and Percy Liang. 2017. Adversarial exam- ples for evaluating reading comprehension systems. In (Palmer et al., 2017), pages 2021\u20132031. Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2017. Bag of tricks for ef\ufb01cient text classi\ufb01cation. In Proceedings of the 15th Con- ference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Pa- pers, pages 427\u2013431.",
            "2017. Bag of tricks for ef\ufb01cient text classi\ufb01cation. In Proceedings of the 15th Con- ference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Pa- pers, pages 427\u2013431. Association for Computational Linguistics. Chandra Khatri, Behnam Hedayatnia, Rahul Goel, Anushree Venkatesh, Raefer Gabriel, and Arindam Mandal. 2018. Detecting offensive content in open-domain conversations using two stage semi- supervision. CoRR, abs\/1811.12900. Ritesh Kumar, Atul Kr. Ojha, Shervin Malmasi, and Marcos Zampieri. 2018. Benchmarking aggression identi\ufb01cation in social media. In Proceedings of the First Workshop on Trolling, Aggression and Cyber- bullying (TRAC-2018), pages 1\u201311, Santa Fe, New Mexico, USA. Association for Computational Lin- guistics. Jiwei Li, Will Monroe, Tianlin Shi, S\u00b4ebastien Jean, Alan Ritter, and Dan Jurafsky.",
            "Association for Computational Lin- guistics. Jiwei Li, Will Monroe, Tianlin Shi, S\u00b4ebastien Jean, Alan Ritter, and Dan Jurafsky. 2017. Adversarial learning for neural dialogue generation. In (Palmer et al., 2017), pages 2157\u20132169. Aishan Liu, Xianglong Liu, Jiaxin Fan, Yuqing Ma, Anlan Zhang, Huiyuan Xie, and Dacheng Tao. 2019. Perceptual-sensitive gan for generating adversarial patches.",
            "David Noever. 2018. Machine learning suites for online toxicity detection. arXiv preprint arXiv:1810.01869. Martha Palmer, Rebecca Hwa, and Sebastian Riedel, editors. 2017. Proceedings of the 2017 Confer- ence on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017. Association for Computa- tional Linguistics. Jeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Nat- ural Language Processing (EMNLP), pages 1532\u2013 1543. Andrew Ruef, Michael Hicks, James Parker, Dave Levin, Michelle L Mazurek, and Piotr Mardziel. 2016. Build it, break it, \ufb01x it: Contesting secure development. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communica- tions Security, pages 690\u2013703. ACM.",
            "2016. Build it, break it, \ufb01x it: Contesting secure development. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communica- tions Security, pages 690\u2013703. ACM. Pnina Shachaf and Noriko Hara. 2010. Beyond vandal- ism: Wikipedia trolls. Journal of Information Sci- ence, 36(3):357\u2013370. Leandro Silva, Mainack Mondal, Denzil Correa, Fabr\u00b4\u0131cio Benevenuto, and Ingmar Weber. 2016. An- alyzing the targets of hate in online social media. In Tenth International AAAI Conference on Web and Social Media. Marty J Wolf, K Miller, and Frances S Grodzinsky. 2017. Why we should have seen that coming: com- ments on microsoft\u2019s tay experiment, and wider im- plications. ACM SIGCAS Computers and Society, 47(3):54\u201364. Ellery Wulczyn, Nithum Thain, and Lucas Dixon. 2017.",
            "ACM SIGCAS Computers and Society, 47(3):54\u201364. Ellery Wulczyn, Nithum Thain, and Lucas Dixon. 2017. Ex machina: Personal attacks seen at scale. In Proceedings of the 26th International Conference on World Wide Web, WWW 2017, Perth, Australia, April 3-7, 2017, pages 1391\u20131399. ACM. Zhilin Yang, Saizheng Zhang, Jack Urbanek, Will Feng, Alexander H. Miller, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Mastering the dun- geon: Grounded language learning by mechanical turker descent. In (DBL, 2018). Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. 2019. Semeval-2019 task 6: Identifying and cate- gorizing offensive language in social media (offen- seval). arXiv preprint arXiv:1903.08983.",
            "2019. Semeval-2019 task 6: Identifying and cate- gorizing offensive language in social media (offen- seval). arXiv preprint arXiv:1903.08983. Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Per- sonalizing dialogue agents: I have a dog, do you have pets too? In Proceedings of the 56th Annual Meeting of the Association for Computational Lin- guistics, ACL 2018, Melbourne, Australia, July 15- 20, 2018, Volume 1: Long Papers, pages 2204\u20132213. Association for Computational Linguistics.",
            "A Additional Experimental Results A.1 Additional Break It Phase Results Additional results regarding the crowdworkers\u2019 ability to \u201cbeat\u201d the classi\ufb01ers are reported in Ta- ble 11. In particular, we report the percent of mes- sages sent by the crowdsource workers that were marked SAFE and OFFENSIVE by both A0 and Ai\u22121. We note that very infrequently (< 1% of the time) a message was marked OFFENSIVE by A0 but SAFE by Ai\u22121, showing that A0 was rela- tively ineffective at catching adversarial behavior. Single-Turn Multi Round 1 2 3 (\u201c4\u201d) Avg.",
            "Single-Turn Multi Round 1 2 3 (\u201c4\u201d) Avg. score (0-5) 4.56 2.56 1.6 2.89 A0: OFFENSIVE and - 0.6% 0.8% 1.4% Ai\u22121: SAFE A0: SAFE and - 44.7% 64.9% 17.7% Ai\u22121: OFFENSIVE A0: OFFENSIVE and 25.7% 23.7% 16.1% 4.1% Ai\u22121: OFFENSIVE A0: SAFE and 74.3% 31.1% 18.3% 76.8% Ai\u22121: SAFE Table 11: Adversarial data collection statistics. A0 is the baseline model, trained on the Wikipedia Toxic Comments dataset. Ai\u22121 is the model for round i, trained on the adversarial data for rounds n \u2264i \u22121. In the case of the multi-turn set-up, Ai\u22121 is A3.",
            "A0 is the baseline model, trained on the Wikipedia Toxic Comments dataset. Ai\u22121 is the model for round i, trained on the adversarial data for rounds n \u2264i \u22121. In the case of the multi-turn set-up, Ai\u22121 is A3. In Table 12, we report the categorization of ex- amples into classes of offensive language from the blind human annotation of round 1 of the single- turn adversarial and standard data. We observe that in the adversarial set-up, there were fewer ex- amples of bullying language but more examples targeting a protected class. A.2 Additional Fix It Phase Results We report F1, precision, and recall for the OFFEN- SIVE class, as well as weighted-F1 for models Si and Ai on the single-turn standard and adversar- ial tasks in Table 13. B Data Collection Interface Details During the adversarial data collection, we asked users to generate a message that \u201c[the user be- lieves] is not ok but that our system marks as ok,\u201d using the de\ufb01nition of \u201cok\u201d and \u201cnot ok\u201d described in the paper (i.e.",
            "B Data Collection Interface Details During the adversarial data collection, we asked users to generate a message that \u201c[the user be- lieves] is not ok but that our system marks as ok,\u201d using the de\ufb01nition of \u201cok\u201d and \u201cnot ok\u201d described in the paper (i.e. \u201cok to send in a friendly conver- sation with someone you just met online\u201d). In order to generate a variety of responses, dur- ing the single-turn adversarial collection, we pro- vided users with a topic to base their response on 50% of the time. The topics were pulled from a set of 1365 crowd-sourced open-domain dia- logue topics. Example topics include diverse top- ics such as commuting, Gouda cheese, music festi- vals, podcasts, bowling, and Arnold Schwarzeneg- ger. Users were able to earn up to \ufb01ve points per round, with two tries for each point (to allow them to get a sense of the models\u2019 weaknesses). Users were informed of their score after each message, and provided with bonuses for good effort.",
            "Users were able to earn up to \ufb01ve points per round, with two tries for each point (to allow them to get a sense of the models\u2019 weaknesses). Users were informed of their score after each message, and provided with bonuses for good effort. The points did not affect the user\u2019s compensation, but rather, were provided as a way of gamifying the data collection, as this has been showed to increase data quality (Yang et al., 2018). Please see an example image of the chat inter- face in Figure 2.",
            "Single-Turn Adversarial and Standard Task OFFENSIVE Examples (Round 1) protected non-protected class class bullying sexual violent Standard 16% 18% 60% 8% 10% Adversarial 25% 16% 28% 14% 15% Table 12: Human annotation of 100 examples from each the single-turn standard and adversarial (round 1) tasks into offensive classes. Figure 2: User interface for the single-turn adversarial collection.",
            "Baseline model Standard models Adversarial models A0 S1 S2 S3 A1 A2 A3 Wikipedia Toxic Comments f1 83.37 80.56 81.11 82.07 81.33 78.86 78.02 prec 85.29 81.18 78.37 82.17 78.55 73.27 71.35 recall 81.53 79.95 84.05 81.97 84.3 85.37 86.07 weighted f1 96.73 96.15 96.17 96.44 96.21 95.6 95.38 Standard Task Round 1 f1 67.43 82.8 85.57 87.31 82.07 84.11 81.42 prec 78.67 89.53 85.15 88.66 77.68 78.95 73.02 recall 59.0 77.0 86.0 86.0 87.0 90.0 92.0 weighted f1 93.93 96.69 97.11 97.",
            "15 88.66 77.68 78.95 73.02 recall 59.0 77.0 86.0 86.0 87.0 90.0 92.0 weighted f1 93.93 96.69 97.11 97.48 96.29 96.7 96.01 Round 2 f1 71.59 87.1 87.44 91.84 81.95 85.17 82.51 prec 82.89 94.19 87.88 93.75 80.0 81.65 74.8 recall 63.0 81.0 87.0 90.0 84.0 89.0 92.0 weighted f1 94.69 97.52 97.49 98.38 96.34 96.96 96.28 Round 3 f1 65.0 79.77 84.32 84.66 85.0 86.7 87.5 prec 86.67 91.03 91.76 89.89 85.0 85.",
            "96 96.28 Round 3 f1 65.0 79.77 84.32 84.66 85.0 86.7 87.5 prec 86.67 91.03 91.76 89.89 85.0 85.44 84.26 recall 52.0 71.0 78.0 80.0 85.0 88.0 91.0 weighted f1 93.76 96.2 96.99 97.02 97 97.32 97.44 All rounds f1 68.1 83.27 85.81 87.97 82.98 85.3 83.71 prec 82.46 91.6 88.07 90.78 80.76 81.9 77.03 recall 58.0 76.33 83.67 85.33 85.33 89.0 91.67 weighted f1 94.14 96.81 97.2 97.63 96.54 96.99 96.",
            "9 77.03 recall 58.0 76.33 83.67 85.33 85.33 89.0 91.67 weighted f1 94.14 96.81 97.2 97.63 96.54 96.99 96.57 Adversarial Task Round 1 f1 0.0 51.7 69.32 68.64 71.79 79.02 78.18 prec 0.0 80.85 80.26 84.06 73.68 77.14 71.67 recall 0.0 38.0 61.0 58.0 70.0 81.0 86.0 weighted f1 84.46 91.72 94.27 94.26 94.44 95.75 95.39 Round 2 f1 0.0 10.81 26.36 31.75 0.0 64.41 62.1 prec 0.0 54.55 58.62 76.92 0.0 74.03 65.",
            "39 Round 2 f1 0.0 10.81 26.36 31.75 0.0 64.41 62.1 prec 0.0 54.55 58.62 76.92 0.0 74.03 65.56 recall 0.0 6.0 17.0 20.0 0.0 57.0 59.0 weighted f1 84.61 86.36 88.07 89.04 84.2 93.33 92.63 Round 3 f1 0.0 12.28 17.09 13.67 32.12 0.0 59.88 prec 0.0 50.0 58.82 47.06 59.46 0.0 74.63 recall 0.0 7.0 10.0 8.0 22.0 0.0 50.0 weighted f1 84.86 86.46 87.07 86.54 88.72 84.51 92.7 All rounds f1 0.0 27.",
            "0 10.0 8.0 22.0 0.0 50.0 weighted f1 84.86 86.46 87.07 86.54 88.72 84.51 92.7 All rounds f1 0.0 27.42 41.71 41.75 40.62 55.53 67.59 prec 0.0 70.83 72.13 76.79 60.13 46.0 65.0 weighted f1 84.64 88.42 90.2 90.31 89.7 91.94 93.66 Table 13: Full table of results from experiments on the single-turn standard and adversarial tasks. F1, precision, and recall are reported for the OFFENSIVEclass, as well as weighted F1."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1908.06083.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 12061.000213623047,
    "avg_doclen_est": 172.3000030517578
}
