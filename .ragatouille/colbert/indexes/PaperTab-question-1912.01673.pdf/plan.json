{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "COSTRA 1.0: A Dataset of Complex Sentence Transformations Petra Baran\u02c7c\u00b4\u0131kov\u00b4a, Ond\u02c7rej Bojar Charles University Faculty of Mathematics and Physics Institute of Formal and Applied Linguistics {barancikova,bojar}@ufal.mff.cuni.cz Abstract We present COSTRA 1.0, a dataset of complex sentence transformations. The dataset is intended for the study of sentence-level embeddings beyond simple word alternations or standard paraphrasing. This \ufb01rst version of the dataset is limited to sentences in Czech but the construction method is universal and we plan to use it also for other languages. The dataset consists of 4,262 unique sentences with an average length of 10 words, illustrating 15 types of modi\ufb01cations, such as simpli\ufb01cation, generalization, or formal and informal language variation. The hope is that with this dataset, we should be able to test semantic properties of sentence embeddings and perhaps even to \ufb01nd some topologically interesting \u201cskeleton\u201d in the sentence embedding space.",
            "The hope is that with this dataset, we should be able to test semantic properties of sentence embeddings and perhaps even to \ufb01nd some topologically interesting \u201cskeleton\u201d in the sentence embedding space. A preliminary analysis using LASER, multi-purpose multi-lingual sentence embeddings suggests that the LASER space does not exhibit the desired properties. Keywords: sentence embeddings, sentence transformations, paraphrasing, semantic relations 1. Introduction Vector representations are essential in the majority of nat- ural language processing tasks. The popularity of word embeddings started with the introduction of word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) and their properties have been analyzed at length from var- ious aspects.",
            "The popularity of word embeddings started with the introduction of word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) and their properties have been analyzed at length from var- ious aspects. Studies of word embeddings range from word similarity (Hill et al., 2014; Faruqui and Dyer, 2014), over the ability to capture derivational relations (Musil et al., 2019), linear superposition of multiple senses (Arora et al., 2016), the ability to predict semantic hierarchies (Fu et al., 2014) or POS tags (Musil, 2019) up to data ef\ufb01ciency (Jastrzkebski et al., 2017). Several studies (Mikolov et al., 2013c; Mikolov et al., 2013b; Levy and Goldberg, 2014; Vylomova et al., 2015) show that word vector representations are capable of capturing meaningful syntactic and semantic regular- ities.",
            "These include, for example, male\/female relation demonstrated by the pairs \u201cman:woman\u201d, \u201cking:queen\u201d and the country\/capital relation (\u201cRussia:Moscow\u201d, \u201cJapan:Tokyo\u201d). These regularities correspond to simple arithmetic operations in the vector space. Sentence embeddings are becoming equally ubiquitous in NLP, with novel representations appearing almost ev- ery other week. With an overwhelming number of meth- ods to compute sentence vector representations, the study of their general properties becomes dif\ufb01cult. Furthermore, it is not entirely clear in which way the embeddings should be evaluated. In an attempt to bring together more traditional representa- tions of sentence meanings and the emerging vector repre- sentations, Bojar et al. (2019) introduce several aspects or desirable properties of sentence embeddings. One of them, \u201crelatability\u201d, highlights the correspondence of meaningful differences between sentences on the one hand and geomet- rical relations between their respective embeddings in the highly dimensional continuous vector space on the other hand. If we found such correspondence, we could apply geometrical operations in the space to induce meaningful changes in sentences.",
            "If we found such correspondence, we could apply geometrical operations in the space to induce meaningful changes in sentences. In this work, we present COSTRA, a new dataset of COm- plex Sentence TRAnsformations. In its \ufb01rst version, the dataset is limited to sample sentences in Czech. The goal is to support studies of semantic and syntactic relations be- tween sentences in the continuous space. Our dataset is the prerequisite for one of the possible ways of exploring sentence meaning relatability:1 We envision that the con- tinuous space of sentences induced by an ideal embedding method would exhibit topological similarity to the graph of sentence variations. For instance, one could argue that a subset of sentences could be organized along a linear scale re\ufb02ecting the formalness of the language used. Another set of sentences could form a partially ordered set of grad- ually less and less concrete statements. And yet another set, intersecting both of the previous ones in multiple sen- tences could be partially or linearly ordered according to the strength of the speaker\u2019s con\ufb01dence in the claim.",
            "And yet another set, intersecting both of the previous ones in multiple sen- tences could be partially or linearly ordered according to the strength of the speaker\u2019s con\ufb01dence in the claim. Our long term goal is to search for a sentence embedding method that exhibits this behaviour, i.e., that the topologi- cal map of the embedding space corresponds to meaningful operations or changes in the set of sentences of a language (or more languages at once). We prefer this behaviour to emerge, as it happened for word vector operations, but re- gardless if the behaviour is emergent or trained, we need a dataset of sentences illustrating these patterns. A large dataset could serve for training; a small one would pro- vide a test set. In either case, these sentences could provide a \u201cskeleton\u201d to the continuous space of sentence embed- dings.2 1The term \u201crelatability\u201d is used to indicate that we search for speci\ufb01c types of relations among sentences. The common term \u201crelatedness\u201d, in our opinion, suggests some vagueness on the re- lation type.",
            "The common term \u201crelatedness\u201d, in our opinion, suggests some vagueness on the re- lation type. We do not build a dataset of sentences related in just some way, we seek for a set of clear-cut, \u201corthogonal\u201d relations. 2The Czech word for \u201cskeleton\u201d is \u201ckostra\u201d. arXiv:1912.01673v2  [cs.CL]  16 Apr 2020",
            "Change Example of change % change of aspect Hunters have fallen asleep on a clearing. 4 opposite\/shifted meaning On a clearing, several hunters were dancing. 15 less formally Several deer stalkers kipped down on a clearing. 6 change into possibility Several hunters can sleep on a clearing. 4 ban Hunters are forbidden to sleep on a clearing. 4 exaggeration Crowds of hunters slept on a clearing. 7 concretization Several hunters dozed off after lunch on the Upper clearing. 15 generalization There were several men in a forest. 9 change of locality Several hunters slept in a cinema. 3 change of gender Several huntresses slept on a clearing. 2 Total 65 Table 1: Examples of transformations given to annotators for the source sentence Several hunters slept on a clearing. The third column shows how many of all the transformation suggestions collected in the \ufb01rst round closely mimic the particular example. The number is approximate as annotators typically call one transformation by several names, e.g. less formally, formality diminished, decrease of formality, not formal expressions, non-formal, less formal, formality decreased, ... The examples were translated to English for presentation purposes only.",
            "The number is approximate as annotators typically call one transformation by several names, e.g. less formally, formality diminished, decrease of formality, not formal expressions, non-formal, less formal, formality decreased, ... The examples were translated to English for presentation purposes only. The paper is structured as follows: Section 2. summarizes existing methods of sentence embeddings evaluation and related work. Section 3. describes our methodology for constructing our dataset. Section 4. details the obtained dataset and some \ufb01rst observations. We conclude and pro- vide the link to the dataset in Section 5. 2. Background As hinted above, there are many methods of converting a sentence into a vector in a highly dimensional space.",
            "Section 4. details the obtained dataset and some \ufb01rst observations. We conclude and pro- vide the link to the dataset in Section 5. 2. Background As hinted above, there are many methods of converting a sentence into a vector in a highly dimensional space. To name a few: BiLSTM with the max-pooling trained for natural language inference (Conneau et al., 2017), masked language modelling and next sentence prediction using bidirectional Transformer (Devlin et al., 2018), max- pooling last states of neural machine translation among many languages (Artetxe and Schwenk, 2018) or the en- coder \ufb01nal state in attentionless neural machine translation (C\u00b4\u0131fka and Bojar, 2018). The most common way of evaluating methods of sentence embeddings is extrinsic, using so-called \u2018transfer tasks\u2019, i.e., comparing embeddings via the performance in down- stream tasks such as paraphrasing, entailment, sentence sentiment analysis, natural language inference and other assignments. However, even simple bag-of-words (BOW) approaches often achieve competitive results on such tasks (Wieting et al., 2015). Adi et al.",
            "However, even simple bag-of-words (BOW) approaches often achieve competitive results on such tasks (Wieting et al., 2015). Adi et al. (2016) introduce intrinsic evaluation by measur- ing the ability of models to encode basic linguistic proper- ties of a sentence such as its length, word order, and word occurrences. These so-called \u2018probing tasks\u2019 are further ex- tended by a depth of the syntactic tree, top constituent or verb tense by Conneau et al. (2018). Both transfer and probing tasks are integrated into SentE- val (Conneau and Kiela, 2018) framework for sentence vec- tor representations. Perone et al. (2018) applied SentEval to eleven different encoding methods revealing that there is no consistently well-performing method across all tasks. SentEval was further criticized for pitfalls such as compar- ing different embedding sizes or correlation between tasks (Eger et al., 2019; Wieting and Kiela, 2019). Shi et al. (2016) show that NMT encoder is able to capture syntactic information about the source sentence.",
            "Shi et al. (2016) show that NMT encoder is able to capture syntactic information about the source sentence. Belinkov et al. (2017) examine the ability of NMT to learn morphol- ogy through POS and morphological tagging. Still, very little is known about the semantic properties of sentence embeddings. Interestingly, C\u00b4\u0131fka and Bojar (2018) observe that the better self-attention embeddings serve in NMT, the worse they perform in most of SentE- val tasks. Zhu et al. (2018) generate automatically sentence varia- tions such as: (1) Original sentence: A rooster pecked grain. (2) Synonym Substitution: A cock pecked grain. (3) Not-Negation: A rooster didn\u2019t peck grain. (4) Quanti\ufb01er-Negation: There was no rooster pecking grain. and compare their triplets by examining distances between their embeddings, i.e.",
            "(3) Not-Negation: A rooster didn\u2019t peck grain. (4) Quanti\ufb01er-Negation: There was no rooster pecking grain. and compare their triplets by examining distances between their embeddings, i.e. distance between (1) and (2) should be smaller than distances between (1) and (3), (2) and (3), similarly, (3) and (4) should be closer together than (1)\u2013(3) or (1)\u2013(4). In our previous study (Baran\u02c7c\u00b4\u0131kov\u00b4a and Bojar, 2019), we examined the effect of small sentence alternations in sen- tence vector spaces. We used sentence pairs automati- cally extracted from datasets for natural language inference SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018). We observed that the vector difference, familiar from word embeddings, serves reasonably well also in sen- tence embedding spaces. The examined relations were, however, very simple: a change of gender, number, the ad- dition of an adjective, etc.",
            "We observed that the vector difference, familiar from word embeddings, serves reasonably well also in sen- tence embedding spaces. The examined relations were, however, very simple: a change of gender, number, the ad- dition of an adjective, etc. The structure of the sentence and its wording remained almost identical. We would like to move to more interesting non-trivial sen- tence comparison, beyond those in Zhu et al. (2018) or Baran\u02c7c\u00b4\u0131kov\u00b4a and Bojar (2019), such as change of style of a sentence, the introduction of a small modi\ufb01cation that drastically changes the meaning of a sentence or reshuf\ufb02ing of words in a sentence so that its meaning is altered.",
            "Change Instructions paraphrase 1 Reformulation the sentence using different words paraphrase 2 Reformulation the sentence using other different words different meaning Shuf\ufb02e words in the sentence in order to get different meaning opposite meaning Reformulate the sentence to get a sentence with opposite meaning nonsense Shuf\ufb02e words in sentence to make grammatical sentence with no sense. E.g. A hen pecked grain. \u2192Grain pecked a hen. minimal change Signi\ufb01cantly change the meaning of the sentence using only a minimal alternation. generalization Make the sentence more general. gossip Rewrite the sentence in a gossip style \u2013 strongly exaggerated meaning on the sentence. formal sentence Rewrite the sentence in a more formal style. non-standard sentence Rewrite the sentence in non-standard, colloquial style. simple sentence Rewrite the sentence in a simplistic style, so even a person with a limited vocabulary could understand it. possibility Change the modality of the sentence into a possibility. ban Change the modality of the sentence into a ban. future Move the sentence into the future. past Move the sentence into the past.",
            "possibility Change the modality of the sentence into a possibility. ban Change the modality of the sentence into a ban. future Move the sentence into the future. past Move the sentence into the past. Table 2: Sentences transformations requested in the second round of annotation with the instructions to the annotators. The annotators were given no examples (with the exception of nonsense) not to be in\ufb02uenced as much as in the \ufb01rst round. Unfortunately, such a dataset cannot be generated automat- ically and it is not available to our best knowledge. We attempt to start \ufb01lling this gap with COSTRA 1.0. 3. Annotation We acquired the data in two rounds of annotation. In the \ufb01rst one, we were looking for original and uncommon sen- tence change suggestions. In the second one, we collected sentence alternations using ideas from the \ufb01rst round. The \ufb01rst and second rounds of annotation could be broadly called as collecting ideas and collecting data, respectively. 3.1. First Round: Collecting Ideas We manually selected 15 newspaper headlines.",
            "In the second one, we collected sentence alternations using ideas from the \ufb01rst round. The \ufb01rst and second rounds of annotation could be broadly called as collecting ideas and collecting data, respectively. 3.1. First Round: Collecting Ideas We manually selected 15 newspaper headlines. Eleven an- notators were asked to modify each headline up to 20 times and describe the modi\ufb01cation with a short3 name. They were given an example sentence and several of its possible alternations, see Table 1 on the preceding page. Unfortunately, these examples turned out to be highly in\ufb02u- ential on the annotators\u2019 decisions and they correspond to almost two-thirds of all of the modi\ufb01cations gathered in the \ufb01rst round. Other very common transformations include change of word order or transformation into an interroga- tive\/imperative sentence. Other suggested interesting alterations include change into a fairy-tale style, excessive use of diminutives\/vulgarisms or dadaism\u2014a swap of roles in the sentence so that the re- sulting sentence is grammatically correct but nonsensical in our world.",
            "Other suggested interesting alterations include change into a fairy-tale style, excessive use of diminutives\/vulgarisms or dadaism\u2014a swap of roles in the sentence so that the re- sulting sentence is grammatically correct but nonsensical in our world. Of these suggestions, we selected only the dadaistic swap of roles for the current exploration (see non- sense in Table 2). 3This requirement was not always respected. The annotators sometimes created very complex descriptions such as speci\ufb01ca- tion of information about the society affected by the presence of an alien. In total, we collected 984 sentences with 269 described unique changes. We use them as inspiration for the second round of annotation. 3.2. Second Round: Collecting Data Sentence Transformations We selected 15 modi\ufb01ca- tions types to collect COSTRA 1.0. They are presented in Table 2. We asked for two distinct paraphrases of each sentence be- cause we believe that a proper sentence embedding should put paraphrases close together in vector space. Several modi\ufb01cation types were explicitly selected to con- stitute a thorough test of embeddings.",
            "They are presented in Table 2. We asked for two distinct paraphrases of each sentence be- cause we believe that a proper sentence embedding should put paraphrases close together in vector space. Several modi\ufb01cation types were explicitly selected to con- stitute a thorough test of embeddings. In different meaning, the annotators should create a sentence with some other meaning using the same words as the original sentence. Other transformations that should be challenging for em- beddings include minimal change, in which the sentence meaning should be signi\ufb01cantly modi\ufb01ed by only mini- mal alternation, or nonsense, in which words of the source sentence should be rearranged into a grammatically correct sentence without any sense. Seed Data The source sentences for annotations were se- lected from the Czech data of Global Voices (Tiedemann, 2012) and OpenSubtitles4 (Lison and Tiedemann, 2016). We used two sources in order to have different styles of seed sentences, both journalistic and common spoken lan- guage. We considered only sentences with more than 5 and less than 15 words and we manually selected 150 of them for further annotation.",
            "We used two sources in order to have different styles of seed sentences, both journalistic and common spoken lan- guage. We considered only sentences with more than 5 and less than 15 words and we manually selected 150 of them for further annotation. This step was necessary to remove sentences that are: \u2022 too unreal, out of this world, such as: Jedno fotonov\u00b4y torp\u00b4edo a je z tebe vesm\u00b4\u0131rn\u00b4a topinka. \u201cOne photon torpedo and you\u2019re a space toast.\u201d 4http:\/\/www.opensubtitles.org\/",
            "Annotator # Annotations # Sentences # Impossible # Typos Avg. Sent. Length Avg. Time armadillo 69 1035 0 9 10.3 12:32 wolverine 42 598 32 13 9.6 14:32 honeybadger 39 584 1 28 10.4 30:38 gorilla 31 448 17 16 9.8 16:55 porcupine 31 465 0 6 11.3 8:55 lump\ufb01sh 23 329 16 4 8.4 13:28 crane 22 319 11 15 9.2 15:30 meerkat 17 241 14 17 9.1 27:36 axolotl 8 116 4 11 10.1 24:02 bullshark 6 90 0 2 9.8 20:59 \ufb02amingo 3 45 0 8 11.3 11:37 capybara 2 30 0 0 7.",
            "1 24:02 bullshark 6 90 0 2 9.8 20:59 \ufb02amingo 3 45 0 8 11.3 11:37 capybara 2 30 0 0 7.6 25:06 Total 293 4,300 95 129 9.9 19:50 Table 3: Statistics for individual annotators (anonymized as armadillo, ..., capybara). \u2022 photo captions (i.e. incomplete sentences), e.g.: Zvl\u00b4a\u02c7stn\u00b4\u0131 ekv\u00b4adorsk\u00b4y p\u02c7r\u00b4\u0131pad Correa vs. Crudo \u201cSpeci\ufb01c Ecuadorian case Correa vs. Crudo\u201d \u2022 too vague, overly dependent on the context: B\u02c7e\u02c7z tam a mluv na ni. \u201cGo there and speak to her.\u201d Many of the intended sentence transformations would be impossible to apply to such sentences and annotators\u2019 time would be wasted.",
            "Crudo\u201d \u2022 too vague, overly dependent on the context: B\u02c7e\u02c7z tam a mluv na ni. \u201cGo there and speak to her.\u201d Many of the intended sentence transformations would be impossible to apply to such sentences and annotators\u2019 time would be wasted. Even after such \ufb01ltering, it was still quite possible that the desired sentence modi\ufb01cation could not be achieved for a sentence. For such a case, we gave the annotators the option to enter the keyword IMPOSSIBLE instead of the particular (impossible) modi\ufb01cation. This option allowed to state that no such transformation is possible explicitly. At the same time, most of the trans- formations are likely to lead to a large number of possible outcomes. As documented in Bojar et al. (2013), a Czech sentence might have hundreds of thousands of paraphrases. To support some minimal exploration of this possible diver- sity, most of the sentences were assigned to several annota- tors. Spell-Checking The annotation is a challenging task and the annotators naturally make mistakes.",
            "(2013), a Czech sentence might have hundreds of thousands of paraphrases. To support some minimal exploration of this possible diver- sity, most of the sentences were assigned to several annota- tors. Spell-Checking The annotation is a challenging task and the annotators naturally make mistakes. Unfortunately, a single typo can signi\ufb01cantly in\ufb02uence the resulting em- bedding (Malykh et al., 2018). After collecting all the sen- tence variations, we applied the statistical spellchecker and grammar checker Korektor (Richter et al., 2012) in order to minimize in\ufb02uence of typos to performance of embedding methods. We manually inspected 519 errors identi\ufb01ed by Korektor and \ufb01xed 129 true errors. 4. Dataset Description In the second round, we collected 293 annotations from 12 annotators. After Korektor, there are 4262 unique sentences (including 150 seed sentences) that form the COSTRA 1.0 dataset. Statistics of individual annotators are available in Table 3.",
            "Dataset Description In the second round, we collected 293 annotations from 12 annotators. After Korektor, there are 4262 unique sentences (including 150 seed sentences) that form the COSTRA 1.0 dataset. Statistics of individual annotators are available in Table 3. The time needed to carry out one piece of annotation (i.e., to provide one seed sentence with all 15 transformations) was on average almost 20 minutes but some annotators easily Persons # Annotations Unique Sents. U.S. % 1 30 438 99,8% 2 30 851 97,3% 3 61 2545 94,3% 4 5 278 95,8% Total 126 4112 96,8% Table 4: The number of people annotating the same sen- tence. Most of the sentences have at least three different annotators. Unfortunately, 24 sentences were left without any annotation. needed even half an hour. Out of the 4262 distinct sen- tences, only 188 were recorded more than once.",
            "Most of the sentences have at least three different annotators. Unfortunately, 24 sentences were left without any annotation. needed even half an hour. Out of the 4262 distinct sen- tences, only 188 were recorded more than once. In other words, the chance of two annotators producing the same output string is quite low. The most repeated transforma- tions are by far past, future and ban. The least repeated is paraphrase with only one sentence repeated. Table 4 documents this in another way. The 293 annota- tions are split into groups depending on how many annota- tors saw the same input sentence: 30 annotations were an- notated by one person only, 30 annotations by two different persons, etc. The last column shows the number of unique outputs obtained in that group. Across all cases, 96.8% of produced strings were unique.5 In line with instructions, the annotators were using the IM- POSSIBLE option scarcely (95 times, i.e., only 2%).",
            "The last column shows the number of unique outputs obtained in that group. Across all cases, 96.8% of produced strings were unique.5 In line with instructions, the annotators were using the IM- POSSIBLE option scarcely (95 times, i.e., only 2%). It was also a case of 7 annotators only; the remaining 5 annota- tors were capable of producing all requested transforma- tions. The top three transformations considered unfeasible were different meaning (using the same set of words), past (esp. for sentences already in the past tense)6 and simple sentence. First Observations We embedded COSTRA sentences with LASER (Artetxe and Schwenk, 2018), the only cur- rently available off-the-shelf sentence embedding model for 5The number of unique outputs from single-person annotations is not 100% because one of the annotators wrongly produced the same sentence for both possibility and future transformation. 6The annotators clearly did not consider the option to express a more distant past lexically.",
            "id change transformation and its translation 0 source Je to prost\u02c7e blbost, zru\u02c7ste ten projekt, pros\u00b4\u0131m, a nemrhejte st\u00b4atn\u00b4\u0131mi pen\u02c7ezi. It\u2019s just crap, please cancel the project, and don\u2019t waste state money. 1 different meaning Zru\u02c7sen\u00b4\u0131 projektu je blbost a nesmysln\u00b4e mrh\u00b4an\u00b4\u0131 st\u00b4atn\u00b4\u0131mi pen\u02c7ezi. Cancellation of the project is stupid and pointless waste of state money. 2 formal sentence Ten projekt je od z\u00b4akladu \u02c7spatn\u00b4y, zru\u02c7ste jej pros\u00b4\u0131m a nepl\u00b4ytvejte st\u00b4atn\u00b4\u0131mi prost\u02c7redky. The project is fundamentally bad, please cancel it and don\u2019t waste state resources. 3 future Bude to ur\u02c7cit\u02c7e blbost, projekt bude nejsp\u00b4\u0131\u02c7s zru\u02c7sen a nebude se mrhat st\u00b4atn\u00b4\u0131mi pen\u02c7ezi.",
            "3 future Bude to ur\u02c7cit\u02c7e blbost, projekt bude nejsp\u00b4\u0131\u02c7s zru\u02c7sen a nebude se mrhat st\u00b4atn\u00b4\u0131mi pen\u02c7ezi. It will certainly be crap, the project will likely be canceled and state money will not be wasted. 4 generalization Projekt je blbost, zastavte jej a nemrhejte pen\u02c7ezi. The project is crap, stop it and don\u2019t waste money. 5 gossip Ten projekt je \u02c7s\u00b4\u0131lenost, m\u02c7el by se okam\u02c7zit\u02c7e zastavit a nepl\u00b4ytvat na n\u02c7em miliony z na\u02c7sich dan\u00b4\u0131. The project is crazy, it should immediately stop and not to waste millions on it from our taxes. 6 minimal change Je to blbost, p\u02c7reru\u02c7ste ten projekt, pros\u00b4\u0131m, a mrhejte st\u00b4atn\u00b4\u0131mi pen\u02c7ezi. It\u2019s crap, interrupt the project, please, and waste state money.",
            "6 minimal change Je to blbost, p\u02c7reru\u02c7ste ten projekt, pros\u00b4\u0131m, a mrhejte st\u00b4atn\u00b4\u0131mi pen\u02c7ezi. It\u2019s crap, interrupt the project, please, and waste state money. 7 nonsense Zru\u02c7ste st\u00b4atn\u00b4\u0131 pen\u00b4\u0131ze pros\u00b4\u0131m a nemrhejte prost\u00b4ymi blbostmi. Please cancel the state money and do not waste plain crap. 8 nonstandard sent. Je to kravina, pr\u00b4ace by se m\u02c7ela zastavit a nemrhat st\u00b4atn\u00b4\u0131ma pen\u02c7ezma. It\u2019s bullshit, work should stop and not waste government money. 9 opposite meaning Jedn\u00b4a se o promy\u02c7slen\u00b4y pl\u00b4an, je t\u02c7reba ho realizovat a uvolnit pen\u00b4\u0131ze ze st\u00b4atn\u00b4\u0131 kasy. It is a well-thought-out plan, it needs to be implemented and the money released from the Treasury.",
            "It is a well-thought-out plan, it needs to be implemented and the money released from the Treasury. 10 paraphrase Je to hloupost, zastavte ten projekt a u\u02c7set\u02c7rete st\u00b4atn\u00b4\u0131 pen\u00b4\u0131ze. It\u2019s stupid, stop the project and squander state money. 11 paraphrase Je to blbost, zastavte pr\u00b4ace a nepl\u00b4ytvejte ve\u02c7rejn\u00b4ymi prost\u02c7redky. It\u2019s stupid, stop the work and don\u2019t waste public funds. 12 past Byla to prost\u02c7e blbost, projekt byl zru\u02c7sen a nemrhalo se st\u00b4atn\u00b4\u0131mi pen\u02c7ezi. It was just crap, the project was canceled and state money weren\u2019t wasted. 13 possibility Tento projekt m\u02dau\u02c7ze b\u00b4yt blbost, mohl by se zru\u02c7sit a nem\u02c7elo by se zde mrhat st\u00b4atn\u00b4\u0131mi pen\u02c7ezi. This project can be stupid, could be canceled and state money shouldn\u2019t be wasted.",
            "This project can be stupid, could be canceled and state money shouldn\u2019t be wasted. 14 simple sentence Projekt je \u02c7spatn\u00b4y, zastavte jej, nepl\u00b4ytvejte st\u00b4atn\u00b4\u0131mi pen\u02c7ezi. The project is bad, stop it, don\u2019t waste state money. 15 ban Ten projekt nesm\u00b4\u0131 b\u00b4yt blbost, jeho realizace se nesm\u00b4\u0131 zru\u02c7sit a nesm\u00b4\u0131 b\u00b4yt na n\u02c7ej uvoln\u02c7eny st\u00b4atn\u00b4\u0131 pen\u00b4\u0131ze. The project mustn\u2019t be crap, its realization mustn\u2019t be canceled and state money mustn\u2019t be released for it. Figure 1: 2D visualization using PCA of a single annotation. Sentences corresponding to the numbers in the plot are listed under the visualization. Best viewed in colors. the Czech language.",
            "Figure 1: 2D visualization using PCA of a single annotation. Sentences corresponding to the numbers in the plot are listed under the visualization. Best viewed in colors. the Czech language. Having browsed a number of 2D visu- alizations (PCA and t-SNE) of the space, we have to con- clude that visually, LASER space does not seem to exhibit any of the desired topological properties discussed above, see Figure 1 for one example. Table 5 summarizes vector and string similarities between seed sentences and their transformations. It re\ufb02ects the lack of semantic relations in the LASER space \u2013 the embed-",
            "Transformation Vector Similarity String Similarity minimal change 0.945 0.887 past 0.915 0.864 future 0.909 0.859 opposite meaning 0.902 0.821 possibility 0.899 0.843 ban 0.895 0.819 nonsense 0.881 0.675 different meaning 0.869 0.699 nonstandard sentence 0.851 0.660 formal sentence 0.850 0.661 paraphrase 0.827 0.556 simple sentence 0.810 0.606 gossip 0.809 0.562 generalization 0.739 0.512 Table 5: Vector and string similarity between seed sen- tences and their transformations per category measured as average cosine similarity and average Levenshtein similar- ity, respectively. ding of minimal change transformation lies very close to the original sentence (average similarity of 0.945) even though the transformation substantially changed the meaning of the sentence. Tense changes and some form of negation or ban- ning also keep the vectors very similar.",
            "ding of minimal change transformation lies very close to the original sentence (average similarity of 0.945) even though the transformation substantially changed the meaning of the sentence. Tense changes and some form of negation or ban- ning also keep the vectors very similar. The lowest average similarity was observed for generaliza- tion (0.739) and gossip (0.809), which is not any bad sign. However the fact that paraphrases have much smaller sim- ilarity (0.827) than opposite meaning (0.902) documents that the vector space lacks in terms of \u201crelatability\u201d. The string similarity between two sentences s1 and s2 was computed as |s1|+|s2|\u2212dL(s1,s2) |s1|+|s2| , where dL represents Lev- enshtein distance. Pearson correlation of the average co- sine similarity between seed sentence embeddings and their transformation and average string similarity is 0.934, i.e., very strong correlation. This result suggests that LASER embeddings are super\ufb01cial and lack a deeper grasp into the meaning of sentences. 5.",
            "Pearson correlation of the average co- sine similarity between seed sentence embeddings and their transformation and average string similarity is 0.934, i.e., very strong correlation. This result suggests that LASER embeddings are super\ufb01cial and lack a deeper grasp into the meaning of sentences. 5. Conclusion and Future Work We presented COSTRA 1.0, a small corpus of complex transformations of Czech sentences. We plan to use this corpus to analyze a broad spectrum sen- tence embeddings methods to see to what extent the con- tinuous space they induce re\ufb02ects semantic relations be- tween sentences in our corpus. The very \ufb01rst analysis using LASER embeddings indicates a lack of \u201cmeaning relatabil- ity\u201d, i.e., the ability to move along a trajectory in the space in order to reach desired sentence transformations. Actu- ally, not even paraphrases appear in close neighbourhoods of embedded sentences. More \u201csemantic\u201d sentence embed- dings methods are thus to be sought for.",
            "Actu- ally, not even paraphrases appear in close neighbourhoods of embedded sentences. More \u201csemantic\u201d sentence embed- dings methods are thus to be sought for. The corpus is freely available at the following link: http:\/\/hdl.handle.net\/11234\/1-3123 Aside from extending the corpus in Czech and adding other language variants, we are also planning to wrap COSTRA 1.0 into an API such as SentEval making the evaluation of sentence embeddings in terms of \u201crelatability\u201d effortless. 6. Acknowledgment The research reported in this paper has been supported by the grant No. 19-26934X (NEUREM3) of the Czech Sci- ence Foundation and by the grant No. 825303 (Bergamot) of European Union\u2019s Horizon 2020 research and innovation programme. This work has been using language resources stored and distributed by the project No. LM2015071, LINDAT- CLARIN, of the Ministry of Education, Youth and Sports of the Czech Republic. 7.",
            "This work has been using language resources stored and distributed by the project No. LM2015071, LINDAT- CLARIN, of the Ministry of Education, Youth and Sports of the Czech Republic. 7. Bibliographical References Adi, Y., Kermany, E., Belinkov, Y., Lavi, O., and Gold- berg, Y. (2016). Fine-grained analysis of sentence embeddings using auxiliary prediction tasks. CoRR, abs\/1608.04207. Arora, S., Li, Y., Liang, Y., Ma, T., and Risteski, A. (2016). Linear algebraic structure of word senses, with applica- tions to polysemy. CoRR, abs\/1601.03764. Artetxe, M. and Schwenk, H. (2018). Massively multi- lingual sentence embeddings for zero-shot cross-lingual transfer and beyond. CoRR, abs\/1812.10464. Baran\u02c7c\u00b4\u0131kov\u00b4a, P. and Bojar, O. (2019).",
            "(2018). Massively multi- lingual sentence embeddings for zero-shot cross-lingual transfer and beyond. CoRR, abs\/1812.10464. Baran\u02c7c\u00b4\u0131kov\u00b4a, P. and Bojar, O. (2019). In search for linear relations in sentence embedding spaces. In Proceedings of the 19th Conference ITAT 2019: Slovensko\u02c7cesk\u00b4y NLP workshop (SloNLP 2019), pages 125\u2013132, Ko\u02c7sice, Slo- vakia. CreateSpace Independent Publishing Platform. Belinkov, Y., Durrani, N., Dalvi, F., Sajjad, H., and Glass, J. R. (2017). What do neural machine translation models learn about morphology? CoRR, abs\/1704.03471. Bojar, O., Mach\u00b4a\u02c7cek, M., Tamchyna, A., and Zeman, D. (2013). Scratching the surface of possible translations. In Text, Speech and Dialogue: 16th International Con- ference, TSD 2013.",
            "Bojar, O., Mach\u00b4a\u02c7cek, M., Tamchyna, A., and Zeman, D. (2013). Scratching the surface of possible translations. In Text, Speech and Dialogue: 16th International Con- ference, TSD 2013. Proceedings, pages 465\u2013474, Berlin \/ Heidelberg. Springer Verlag. Bojar, O., Bernardi, R., and Webber, B. (2019). Represen- tation of sentence meaning (a jnle special issue). Natural Language Engineering, 25(4). Bowman, S. R., Angeli, G., Potts, C., and Manning, C. D. (2015). A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Confer- ence on Empirical Methods in Natural Language Pro- cessing (EMNLP). Association for Computational Lin- guistics. C\u00b4\u0131fka, O. and Bojar, O. (2018). Are BLEU and Mean- ing Representation in Opposition?",
            "Association for Computational Lin- guistics. C\u00b4\u0131fka, O. and Bojar, O. (2018). Are BLEU and Mean- ing Representation in Opposition? In Proceedings of the 56th Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers), pages 1362\u20131371. Association for Computational Linguistics, Association for Computational Linguistics. Conneau, A. and Kiela, D. (2018). Senteval: An evalua- tion toolkit for universal sentence representations. arXiv preprint arXiv:1803.05449. Conneau, A., Kiela, D., Schwenk, H., Barrault, L., and Bordes, A. (2017). Supervised learning of universal sentence representations from natural language inference data. CoRR, abs\/1705.02364. Conneau, A., Kruszewski, G., Lample, G., Barrault, L., and Baroni, M. (2018). What you can cram into a single vec-",
            "tor: Probing sentence embeddings for linguistic proper- ties. CoRR, abs\/1805.01070. Devlin, J., Chang, M., Lee, K., and Toutanova, K. (2018). BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs\/1810.04805. Eger, S., R\u00a8uckl\u00b4e, A., and Gurevych, I. (2019). Pit- falls in the evaluation of sentence embeddings. CoRR, abs\/1906.01575. Faruqui, M. and Dyer, C. (2014). Community evaluation and exchange of word vectors at wordvectors.org. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 19\u201324, Baltimore, Maryland, June. Association for Computational Linguistics. Fu, R., Guo, J., Qin, B., Che, W., Wang, H., and Liu, T. (2014). Learning semantic hierarchies via word embed- dings.",
            "Association for Computational Linguistics. Fu, R., Guo, J., Qin, B., Che, W., Wang, H., and Liu, T. (2014). Learning semantic hierarchies via word embed- dings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1199\u20131209, Baltimore, Mary- land, June. Association for Computational Linguistics. Hill, F., Reichart, R., and Korhonen, A. (2014). Simlex- 999: Evaluating semantic models with (genuine) simi- larity estimation. CoRR, abs\/1408.3456. Jastrzkebski, S., Lesniak, D., and Czarnecki, W. M. (2017). How to evaluate word embeddings? on importance of data ef\ufb01ciency and simple supervised tasks. CoRR, abs\/1702.02170. Levy, O. and Goldberg, Y. (2014). Linguistic regulari- ties in sparse and explicit word representations.",
            "on importance of data ef\ufb01ciency and simple supervised tasks. CoRR, abs\/1702.02170. Levy, O. and Goldberg, Y. (2014). Linguistic regulari- ties in sparse and explicit word representations. In Pro- ceedings of the Eighteenth Conference on Computational Natural Language Learning, pages 171\u2013180, Ann Arbor, Michigan, June. Association for Computational Linguis- tics. Lison, P. and Tiedemann, J. (2016). OpenSubtitles2016: Extracting large parallel corpora from movie and TV subtitles. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC\u201916), pages 923\u2013929, Portoro\u02c7z, Slovenia, May. European Language Resources Association (ELRA). Malykh, V., Logacheva, V., and Khakhulin, T. (2018). Robust word vectors: Context-informed embeddings for noisy texts.",
            "European Language Resources Association (ELRA). Malykh, V., Logacheva, V., and Khakhulin, T. (2018). Robust word vectors: Context-informed embeddings for noisy texts. In Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User- generated Text, pages 54\u201363, Brussels, Belgium, November. Association for Computational Linguistics. Mikolov, T., Chen, K., Corrado, G. S., and Dean, J. (2013a). Ef\ufb01cient estimation of word representations in vector space. Mikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean, J. (2013b). Distributed representations of words and phrases and their compositionality. CoRR, abs\/1310.4546. Mikolov, T., Yih, W.-t., and Zweig, G. (2013c). Linguistic regularities in continuous space word representations.",
            "Distributed representations of words and phrases and their compositionality. CoRR, abs\/1310.4546. Mikolov, T., Yih, W.-t., and Zweig, G. (2013c). Linguistic regularities in continuous space word representations. In Proceedings of the 2013 Conference of the North Ameri- can Chapter of the Association for Computational Lin- guistics: Human Language Technologies, pages 746\u2013 751, Atlanta, Georgia, June. Association for Computa- tional Linguistics. Musil, T., Vidra, J., and Marecek, D. (2019). Derivational morphological relations in word embeddings. CoRR, abs\/1906.02510. Musil, T. (2019). Examining structure of word embed- dings with PCA. CoRR, abs\/1906.00114. Pennington, J., Socher, R., and Manning, C. (2014). Glove: Global vectors for word representation.",
            "(2019). Examining structure of word embed- dings with PCA. CoRR, abs\/1906.00114. Pennington, J., Socher, R., and Manning, C. (2014). Glove: Global vectors for word representation. In Pro- ceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u2013 1543, Doha, Qatar, October. Association for Computa- tional Linguistics. Perone, C. S., Silveira, R., and Paula, T. S. (2018). Evalua- tion of sentence embeddings in downstream and linguis- tic probing tasks. CoRR, abs\/1806.06259. Richter, M., Stra\u02c7n\u00b4ak, P., and Rosen, A. (2012). Korektor\u2013a system for contextual spell-checking and diacritics com- pletion.",
            "CoRR, abs\/1806.06259. Richter, M., Stra\u02c7n\u00b4ak, P., and Rosen, A. (2012). Korektor\u2013a system for contextual spell-checking and diacritics com- pletion. In Martin Kay et al., editors, Proceedings of the 24th International Conference on Computational Lin- guistics (Coling 2012), pages 1\u201312, Mumbai, India. IIT Bombay, Coling 2012 Organizing Committee. Shi, X., Padhi, I., and Knight, K. (2016). Does string- based neural MT learn source syntax? In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1526\u20131534, Austin, Texas, November. Association for Computational Linguistics. Tiedemann, J. (2012). Parallel data, tools and interfaces in OPUS. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC\u201912), pages 2214\u20132218, Istanbul, Turkey, May. European Language Resources Association (ELRA).",
            "(2012). Parallel data, tools and interfaces in OPUS. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC\u201912), pages 2214\u20132218, Istanbul, Turkey, May. European Language Resources Association (ELRA). Vylomova, E., Rimell, L., Cohn, T., and Baldwin, T. (2015). Take and took, gaggle and goose, book and read: Evaluating the utility of vector differences for lexical re- lation learning. CoRR, abs\/1509.01692. Wieting, J. and Kiela, D. (2019). No training required: Exploring random encoders for sentence classi\ufb01cation. CoRR, abs\/1901.10444. Wieting, J., Bansal, M., Gimpel, K., and Livescu, K. (2015). Towards universal paraphrastic sentence embed- dings. CoRR, abs\/1511.08198. Williams, A., Nangia, N., and Bowman, S. R. (2018).",
            "(2015). Towards universal paraphrastic sentence embed- dings. CoRR, abs\/1511.08198. Williams, A., Nangia, N., and Bowman, S. R. (2018). A broad-coverage challenge corpus for sentence under- standing through inference. In NAACL-HLT. Zhu, X., Li, T., and de Melo, G. (2018). Exploring seman- tic properties of sentence embeddings. In Proceedings of the 56th Annual Meeting of the Association for Computa- tional Linguistics (Volume 2: Short Papers), pages 632\u2013 637, Melbourne, Australia, July. Association for Com- putational Linguistics."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1912.01673.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 8776.99966430664,
    "avg_doclen_est": 175.5399932861328
}
