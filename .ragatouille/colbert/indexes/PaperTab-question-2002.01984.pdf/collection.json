[
  "UNCC Biomedical Semantic Question Answering Systems. BioASQ: Task-7B, Phase-B. Sai Krishna Telukuntla, Aditya Kapri & Wlodek Zadrozny College of Computing and Informatics (CCI), UNC Charlotte NC 28223, USA {stelukun,akapri,wzadrozn}@uncc.edu March 14, 2024 Abstract In this paper, we detail our submission to the 7th year BioASQ competition. We present our approach for Task-7b, Phase B, Exact Answering Task. These Question Answering (QA) tasks include Factoid, Yes/No, List Type Question answering. Our system is based on a contextual word embedding model. We have used a Bidirectional Encoder Representations from Transformers(BERT) based system, \ufb01ned tuned for biomedical question answering task using BioBERT. In the third test batch set, our system achieved the highest MRR score for Factoid Question Answering task. Also, for List type question answering task our system achieved the highest recall score in the fourth test batch set.",
  "In the third test batch set, our system achieved the highest MRR score for Factoid Question Answering task. Also, for List type question answering task our system achieved the highest recall score in the fourth test batch set. Along with our detailed approach, we present the results for our submissions, and also highlight identi\ufb01ed downsides for our current approach and ways to improve them in our future experiments. 1 Introduction BioASQ 1 is a biomedical document classi\ufb01cation, document retrieval, and question answering competition, currently in its seventh year. We provide an overview of our submissions to semantic question answering task (7b, Phase B) of BioASQ 7 (except for \u2019ideal answer\u2019 test, in which we did not participate this year). In this task systems are provided with biomedical questions and are required to submit ideal and 1 http://BioASQ.org/participate/challenges 1 arXiv:2002.01984v1  [cs.CL]  5 Feb 2020",
  "exact answers to those questions. We have used BioBERT [9] based system , see also Bidirectional Encoder Representations from Transformers(BERT) [4], and we \ufb01ne tuned it for the biomedical question answering task. Our system scored near the top for factoid questions for all the batches of the challenge. More speci\ufb01ally, in the third test batch set, our system achieved highest MRR score for Factoid Question Answering task. Also, for List-type question answering task our system achieved highest recall score in the fourth test batch set. Along with our detailed approach, we present the results for our submissions and also highlight identi\ufb01ed downsides for our current approach and ways to improve them in our future experiments. In last test batch results we placed 4th for List-type questions and 3rd for Factoid-type questions.) The QA task is organized in two phases. Phase A deals with retrieval of the relevant document, snippets, concepts, and RDF triples, and phase B deals with exact and ideal answer generations (which is a paragraph size summary of snippets). Exact answer generation is required for factoid, list, and yes/no type question.",
  "Phase A deals with retrieval of the relevant document, snippets, concepts, and RDF triples, and phase B deals with exact and ideal answer generations (which is a paragraph size summary of snippets). Exact answer generation is required for factoid, list, and yes/no type question. BioASQ organizers provide the training and testing data. The training data con- sists of questions, gold standard documents, snippets, concepts, and ideal answers (which we did not use in this paper, but we used last year [2]). The test data is split between phases A and B. The Phase A dataset consists of the questions, unique ids, question types. The Phase B dataset consists of the questions, golden standard documents, snippets, unique ids and question types. Exact answers for factoid type questions are evaluated using strict accuracy (the top answer), lenient accuracy (the top 5 answers), and MRR (Mean Reciprocal Rank) which takes into account the ranks of returned answers. Answers for the list type question are evaluated using precision, recall, and F-measure. 2 Related Work 2.1 BioAsq Sharma et al.",
  "Answers for the list type question are evaluated using precision, recall, and F-measure. 2 Related Work 2.1 BioAsq Sharma et al. [16] describe a system with two stage process for factoid and list type question answering. Their system extracts relevant entities and then runs supervised classi\ufb01er to rank the entities. Wiese et al. [18] propose neural network based model for Factoid and List-type question answering task. The model is based on Fast QA and predicts the answer span in the passage for a given question. The model is trained on SQuAD data set and \ufb01ne tuned on the BioASQ data. Dimitriadis et al. [5] proposed two stage process for Factoid question answering task. Their system uses general purpose tools such as Metamap, BeCas to identify candidate sentences. These candidate sentences are represented in the form of features, and are then 2",
  "ranked by the binary classi\ufb01er. Classi\ufb01er is trained on candidate sentences extracted from relevant questions, snippets and correct answers from BioASQ challenge. For factoid question answering task highest MRR achieved in the 6th edition of BioASQ competition is 0.4325. Our system is a neural network model based on contextual word embeddings [4] and achieved a MRR score 0.6103 in one of the test batches for Factoid Question Answering task. 2.2 A minimum background on BERT BERT stands for \u201dBidirectional Encoder Representations from Transformers\u201d [4] is a contextual word embedding model. Given a sentence as an input, contextual embedding for the words are returned. The BERT model was designed so it can be \ufb01ne tuned for 11 different tasks [4], including question answering tasks. For a question answering task, question and paragraph (context) are given as an input. A BERT standard is that question text and paragraph text are separated by a sepa- rator [Sep]. BERT question-answering \ufb01ne tuning involves adding softmax layer.",
  "For a question answering task, question and paragraph (context) are given as an input. A BERT standard is that question text and paragraph text are separated by a sepa- rator [Sep]. BERT question-answering \ufb01ne tuning involves adding softmax layer. Softmax layer takes contextual word embeddings from BERT as input and learns to identity answer span present in the paragraph (context). This process is represented in Figure 1. BERT was originally trained to perform tasks such as language model creation using masked words and next-sentence-prediction. In other words BERT weights are learned such that context is used in building the representation of the word, not just as a loss function to help learn a context-independent representation. For detailed understanding of BERT Architecture, please refer to the original BERT paper [4]. 2.2.1 Comparison of Word Embeddings and Contextual Word Embeddings A word embedding is a learned representation. It is represented in the form of vector where words that have the same meaning have a similar vector representation. Consider a word embedding model \u2019word2vec\u2019 [12] trained on a corpus.",
  "It is represented in the form of vector where words that have the same meaning have a similar vector representation. Consider a word embedding model \u2019word2vec\u2019 [12] trained on a corpus. Word embeddings generated from the model are context independent that is, word embed- dings are returned regardless of where the words appear in a sentence and regardless of e.g. the sentiment of the sentence. However, contextual word embedding models like BERT also takes context of the word into consideration. 2.3 Comparison of BERT and Bio-BERT BERT and BioBERT are very similar in terms of architecture. Difference is that BERT is pretrained on Wikipedia articles, whereas BioBERT version used in our 3",
  "Figure 1: BioBERT \ufb01ne tuned for question answering task experiments is pretrained on Wikipedia, PMC and PubMed articles. Therefore BioBERT model is expected to perform well with biomedical text, in terms of generating contextual word embeddings. BioBERT model used in our experiments is based on BERT-Base Architecture; BERT-Base has 12 transformer Layers where as BERT-Large has 24 transformer layers. Moreover contextual word embedding vector size is 768 for BERT-Base and more for BERT-large. According to [4] Bert-Large, \ufb01ne-tuned on SQuAD 1.1 question answering data [13] can achieve F1 Score of 90.9 for Question Answering task where as BERT-Base Fine-tuned on the same SQuAD question answering [13] data could achieve F1 score of 88.5. One downside of the current version BioBERT is that word-piece vocabulary 2 is the same as that of original BERT Model, as a result word-piece vocabulary does not include biomedical jargon. Lee et al.",
  "One downside of the current version BioBERT is that word-piece vocabulary 2 is the same as that of original BERT Model, as a result word-piece vocabulary does not include biomedical jargon. Lee et al. [9] created BioBERT, using the same pre-trained BERT released by Google, and hence in the word-piece vocabulary (vocab.txt), as a result biomedical jargon is not 2 vocab.txt and all other software and data is available in our GitHub Repo https://github. com/telukuntla/BioMedicalQuestionAnswering_UNCC 4",
  "included in word-piece vocabulary. Modifying word-piece vocabulary (vocab.txt) at this stage would loose original compatibility with BERT, hence it is left unmodi\ufb01ed. In our future work we would like to build pre-trained BERT model from scratch. We would pretrain the model with biomedical corpus (PubMed, PMC) and Wikipedia. Doing so would give us scope to create word piece vocabulary to include biomedical jargon and there are chances of model performing better with biomedical jargon being included in the word piece vocabulary. We will consider this scenario in the future, or wait for the next version of BioBERT. 3 Experiments: Factoid Question Answering Task For Factoid Question Answering task, we \ufb01ne tuned BioBERT [9] with question answering data and added new features. Fig. 1 shows the architecture of BioBERT \ufb01ne tuned for question answering tasks: Input to BioBERT is word tokenized em- beddings for question and the paragraph (Context). As per the BERT [4] standards, tokens [CLS] and [SEP] are appended to the tokenized input as illustrated in the \ufb01gure.",
  "As per the BERT [4] standards, tokens [CLS] and [SEP] are appended to the tokenized input as illustrated in the \ufb01gure. The resulting model has a softmax layer formed for predicting answer span indices in the given paragraph (Context). On test data, the \ufb01ne tuned model gener- ates n-best predictions for each question. For a question, n-best corresponds that n answers are returned as possible answers in the decreasing order of con\ufb01dence. Variable n is con\ufb01gurable. In our paper, any further mentions of answer returned by the model correspond to the top answer returned by the model. 3.1 Setup BioASQ provides the training data. This data is based on previous BioASQ compe- titions. Train data we have considered is aggregate of all train data sets till the 5th version of BioASQ competition. We cleaned the data, that is, question-answering data without answers are removed and left with a total count of 530 question an- swers.",
  "Train data we have considered is aggregate of all train data sets till the 5th version of BioASQ competition. We cleaned the data, that is, question-answering data without answers are removed and left with a total count of 530 question an- swers. The data is split into train and test data in the ratio of 94 to 6; that is, count of \u2019495\u2019 for training and \u201935\u2019 for testing. The original data format is converted to the BERT/BioBERT format, where BioBERT expects start index of the actual answer. The start index corresponds to the index of the answer text present in the paragraph/ Context. For \ufb01nding start index we used built-in python function \ufb01nd(). The function returns the lowest index of the actual answer present in the context(paragraph). If the answer is not found -1 is returned as the index. The ef\ufb01cient way of \ufb01nding start index is, if the paragraph (Context) has multiple instances of answer text, then start index of the answer should be that instance of answer text whose context actually matches with 5",
  "whats been asked in the question. Example (Question, Answer and Paragraph from [17]): Question: Which drug should be used as an antidote in benzodiazepine overdose? Answer: \u2019Flumazenil\u2019 Paragraph(context): \u201dFlumazenil use in benzodiazepine overdose in the UK: a retrospective survey of NPIS data. OBJECTIVE: Benzodiazepine (BZD) overdose (OD) continues to cause signi\ufb01cant morbidity and mortality in the UK. Flumazenil is an effective antidote but there is a risk of seizures, partic- ularly in those who have co-ingested tricyclic antidepressants. A study was undertaken to examine the frequency of use, safety and ef\ufb01cacy of \ufb02umazenil in the management of BZD OD in the UK. METHODS: A 2-year retrospective cohort study was performed of all enquiries to the UK National Poisons Information Service involving BZD OD.",
  "METHODS: A 2-year retrospective cohort study was performed of all enquiries to the UK National Poisons Information Service involving BZD OD. RESULTS: Flumazenil was administered to 80 patients in 4504 BZD- related enquiries, 68 of whom did not have ventilatory failure or had recognised contraindications to \ufb02umazenil. Factors associated with \ufb02umazenil use were increased age, severe poisoning and ventilatory failure. Co-ingestion of tricyclic antidepressants and chronic obstruc- tive pulmonary disease did not in\ufb02uence \ufb02umazenil administration. Seizure frequency in patients not treated with \ufb02umazenil was 0.3%\u201d. Actual answer is \u2019Flumazenil\u2019, but there are multiple instances of word \u2019Flu- mazenil\u2019. Ef\ufb01cient way to identify the start-index for \u2019Flumazenil\u2019(answer) is to \ufb01nd that particular instance of the word \u2019Flumazenil\u2019 which matches the context for the question. In the above example \u2019Flumazenil\u2019 highlighted in bold is the actual instance that matches question\u2019s context.",
  "In the above example \u2019Flumazenil\u2019 highlighted in bold is the actual instance that matches question\u2019s context. Unfortunately, we could not identify readily available tools that can achieve this goal. In our future work, we look forward to handling these scenarios effectively. Note: The creators of \u2019SQuAD\u2019 [13] have handled the task of identifying answer\u2019s start index effectively. But \u2019SQuAD\u2019 data set is much more general and does not include biomedical question answering data. 3.2 Training and error analysis During our training with the BioASQ data, learning rate is set to 3e-5, as mentioned in the BioBERT paper [9]. We started training the model with 495 available train data and 35 test data by setting number of epochs to 50. After training with these hyper-parameters training accuracy(exact match) was 99.3%(over\ufb01tting) and testing accuracy is only 4%. In the next iteration we reduced the number of epochs to 25 6",
  "then training accuracy is reduced to 98.5% and test accuracy moved to 5%. We further reduced number of epochs to 15, and the resulting training accuracy was 70% and test accuracy 15%. In the next iteration set number of epochs to 12 and achieved train accuracy of 57.7% and test accuracy 23.3%. Repeated the experiment with 11 epochs and found training accuracy to be 57.7% and test accuracy to be same 22%. In the next iteration we set number of epochs to \u20199\u2019 and found training accuracy of 48% and test accuracy of 15%. Hence optimum number of epochs is taken as 12 epochs. During our error analysis we found that on test data, model tends to return text in the beginning of the context(paragraph) as the answer. On analysing train data, we found that there are \u2019120\u2019(out of \u2019495\u2019) question answering data instances having start index:0, meaning 120( 25%) question answering data has \ufb01rst word(s) in the context(paragraph) as the answer. We removed 70% of those instances in order to make train data more balanced.",
  "We removed 70% of those instances in order to make train data more balanced. In the new train data set we are left with \u2019411\u2019 question answering data instances. This time we got the highest test accuracy of 26% at 11 epochs. We have submitted our results for BioASQ test batch-2, got strict accuracy of 32% and our system stood in 2nd place. Initially, hyper-parameter- \u2019batch size\u2019 is set to 400. Later it is tuned to \u201932\u2019. Although accuracy(exact answer match) remained at 26%, model generated concise and better answers at batch size 32, that is wrong answers are close to the expected answer in good number of cases. Example.(from [17]) Question: Which mutated gene causes Chediak Higashi Syndrome? Exact Answer: lysosomal traf\ufb01cking regulator gene. The answer returned by a model trained at 400 batch size is Autosomal-recessive complicated spastic paraplegia with a novel lysosomal traf\ufb01cking regulator, and from the one trained at 32 batch size is lysosomal traf\ufb01cking regulator.",
  "The answer returned by a model trained at 400 batch size is Autosomal-recessive complicated spastic paraplegia with a novel lysosomal traf\ufb01cking regulator, and from the one trained at 32 batch size is lysosomal traf\ufb01cking regulator. In further experiments, we have \ufb01ne tuned the BioBERT model with both SQuAD dataset (version 2.0) and BioAsq train data. For training on SQuAD, hyper parameters- Learning rate and number of epochs are set to 3e-3 and 3 respectively as mentioned in the paper [4]. Test accuracy of the model boosted to 44%. In one more experiment we trained model only on SQuAD dataset, this time test accuracy of the model moved to 47%. The reason model did not perform up to the mark when trained with SQuAD alongside BioASQ data could be that in formatted BioASQ data, start index for the answer is not accurate, and affected the overall accuracy. 7",
  "4 Our Systems and Their Performance on Factoid Ques- tions We have experimented with several systems and their variations, e.g. created by training with speci\ufb01c additional features (see next subsection). Here is their list and short descriptions. Unfortunately we did not pay attention to naming, and the systems evolved between test batches, so the overall picture can only be understood by looking at the details. When we started the experiments our objective was to see whether BioBERT and entailment-based techniques can provide value for in the context of biomedical question answering. The answer to both questions was a yes, quali\ufb01ed by many examples clearly showing the limitations of both methods. Therefore we tried to address some of these limitations using feature engineering with mixed results: some clear errors got corrected and new errors got introduced, without overall improvement but convincing us that in future experiments it might be worth trying feature engineering again especially if more training data were available. Overall we experimented with several approaches with the following aspects of the systems changing between batches, that is being absent or present: * training on BioAsq data vs. training on SQuAD * using the BioAsq snippets for context vs.",
  "Overall we experimented with several approaches with the following aspects of the systems changing between batches, that is being absent or present: * training on BioAsq data vs. training on SQuAD * using the BioAsq snippets for context vs. using the documents from the provided URLs for context * adding or not the LAT, i.e. lexical answer type, feature (see [8], [3] and an explanation in the subsection just below). For Yes/No questions (only) we experimented with the entailment methods. We will discuss the performance of these models below and in Section 6. But before we do that, let us discuss a feature engineering experiment which eventually produced mixed results, but where we feel it is potentially useful in future experiments. 4.1 LAT Feature considered and its impact (slightly negative) During error analysis we found that for some cases, answer being returned by the model is far away from what it is being asked in the Question. Example: (from [17]) Question: Hy\u2019s law measures failure of which organ? Actual Answer: Liver. The answer returned by one of our models was alanine aminotransferase, which is an enzyme.",
  "Example: (from [17]) Question: Hy\u2019s law measures failure of which organ? Actual Answer: Liver. The answer returned by one of our models was alanine aminotransferase, which is an enzyme. The model returns an enzyme, when the question asked for the organ 8",
  "name. To address this type of errors, we decided to try the concepts of Lexical Answer Type (LAT) and Focus Word, which was used in IBM Watson, see [6] for overview; [3] for technical details, and [8] for details on question analysis. In an example given in the last source we read: POETS & POETRY: He was a bank clerk in the Yukon before he published \u201dSongs of a Sourdough\u201d in 1907. The focus is the part of the question that is a reference to the answer. In the example above, the focus is \u201dhe\u201d. LATs are terms in the question that indicate what type of entity is being asked for. The headword of the focus is generally a LAT, but questions often contain additional LATs, and in the Jeopardy! domain, categories are an additional source of LATs. (...) In the example, LATs are \u201dhe\u201d, \u201dclerk\u201d, and \u201dpoet\u201d. For example in the question \u201dWhich plant does oleuropein originate from?\u201d ([17]). The LAT here is plant.",
  "domain, categories are an additional source of LATs. (...) In the example, LATs are \u201dhe\u201d, \u201dclerk\u201d, and \u201dpoet\u201d. For example in the question \u201dWhich plant does oleuropein originate from?\u201d ([17]). The LAT here is plant. For the BioAsq task we did not need to explicitly distinguish between the focus and the LAT concepts. In this example, the expectation is that answer returned by the model is a plant. Thus it is conceivable that the cosine distance between contextual embedding of word \u2019plant\u2019 in the question and contextual embedding for the answer present in the paragraph(context) is comparatively low. As a result model learns to adjust its weights during training phase and returns answers with low cosine distance with the LAT. We used Stanford CoreNLP [11] library to write rules for extracting lexical answer type present in the question, both \u2019parts of speech\u2019(POS) and dependency parsing functionality was used. We incorporated the Lexical Answer Type into one of our systems, UNCC QA1 in Batch 4. This system underperformed our system FACTOIDS by about 3% in the MRR measure, but corrected errors such as in the example above.",
  "We incorporated the Lexical Answer Type into one of our systems, UNCC QA1 in Batch 4. This system underperformed our system FACTOIDS by about 3% in the MRR measure, but corrected errors such as in the example above. 4.1.1 Assumptions and rules for deriving lexical answer type. There are different question types: Which, What, When, How etc. Each type of question is being handled differently and there are commonalities among the rules written for different question types. Question words are identi\ufb01ed through parts of speech tags: \u2019WDT\u2019, \u2019WRB\u2019 ,\u2019WP\u2019. We assumed that LAT is a Noun and follows the question word. Often it was also a subject (NSUBJ). This process is illustrated in Fig.2. 9",
  "Figure 2: A simple way of \ufb01nding the lexical answer types, LATs, of factoid questions: using POS tags to \ufb01nd the question word (e.g. \u2019which\u2019), and a dependency parse to \ufb01nd the LAT within the window of 3 words. If a noun is not found near the \u201dWh-\u201d word, we iterate looking for it, as in the second panel. LAT computation was governed by a few simple rules, e.g. when a question has multiple words that are \u2019Subjects (and Noun), a word that is in proximity to the question word is considered as LAT. These rules are different for each \u201dWh\u201d word. Namely, when the word immediately following the question word is a Noun, window size is set to 3. The window size 3 means we iterate through the next 3 words to check if any of the word is both Noun and Subject, If so, such word is considered the LAT; else the word that is present very next to the question word is considered as the LAT.",
  "The window size 3 means we iterate through the next 3 words to check if any of the word is both Noun and Subject, If so, such word is considered the LAT; else the word that is present very next to the question word is considered as the LAT. For questions with words Which , What, When; a Noun immediately following the question word is very often the LAT, e.g. \u2019enzyme\u2019 in Which enzyme is targeted by Evolocumab?. When the word immediately following the question word is not a Noun, e.g. in What is the function of the protein Magt1? the window size is set to 5, and we iterate through the next 5 words (if present) and search for the word that is both Noun and Subject. If present, the word is considered as the LAT; else, the Noun in close proximity to the question word and following it is returned as the LAT. For questions with question words: When, Who, Why, the LAT is a question 10",
  "word itself. For the word How\u2019, e.g. in How many selenoproteins are encoded in the human genome?, we look at the adjective and if we \ufb01nd one, we take it to be the LAT, otherwise the word \u2019How\u2019 is considered as the LAT. Perhaps because of using only very simple rules, the accuracy for LAT derivation is 75%; that is, in the remaining 25% of the cases the LAT word is identi\ufb01ed incorrectly. Worth noting is that the overall performance the system that used LATs was slightly inferior to the system without LATs, but the types of errors changed. The training used BioBERT with the LAT feature as part of the input string. The errors it introduces usually involve \ufb01nding the wrong element of the correct type e.g. wrong enzyme when two similar enzymes are described in the text, or \u2019neuron\u2019 when asked about a type of cell with a certain function, when the answer calls for a different cell category, adipocytes, and both are mentioned in the text.",
  "wrong enzyme when two similar enzymes are described in the text, or \u2019neuron\u2019 when asked about a type of cell with a certain function, when the answer calls for a different cell category, adipocytes, and both are mentioned in the text. We feel with more data and additional tuning or perhaps using an ensemble model, we might be able to keep the correct answers, and improve the results on the confusing examples like the one mentioned above. Therefore if we improve our LAT derivation logic, or have larger datasets, then perhaps the neural network techniques they will yield better results. 4.2 Impact of Training using BioAsq data (slightly negative) Training on BioAsq data in our entry in Batch 1 and Batch 2 under the name QA1 showed it might lead to over\ufb01tting. This happened both with (Batch 2) and without (Batch 1) hyperparameters tuning: abysmal 18% MRR in Batch 1, and slighly better one, 40% in Batch 2 (although in Batch 2 it was overall the second best result in MRR but 16% lower than the highest score).",
  "In Batch 3 (only), our UNCC QA3 system was \ufb01ne tuned on BioAsq and SQuAD 2.0 [13], and for data preprocessing Context paragraph is generated from relevant snippets provided in the test data. This system underperformed, by about 2% in MRR, our other entry UNCC QA1, which was also an overall category winner for this batch. The latter was also trained on SQuAD, but not on BioAsq. We suspect that the reason could be the simplistic nature of the \ufb01nd() function described in Section 3.1. So, this could be an area where a better algorithm for \ufb01nding the best occurrence of an entity could improve performance. 4.3 Impact of Using Context from URLs (negative) In some experiments, for context in testing, we used documents for which URL pointers are provided in BioAsq. However, our system UNCC QA3 underperformed our other system tested only on the provided snippets. 11",
  "Figure 3: An example of a using BioBERT with additional features: Contextual word embedding for Lexical Answer Type (LAT) given as feature along with the actual contextual embeddings for the words in question and the paragraph. This change produced mixed results and no overall improvement. In Batch 5 the underperformance was about 6% of MRR, compared to our best system UNCC QA1, and by 9% to the top performer. 5 Performance on Yes/No and List questions Our work focused on Factoid questions. But we also have done experiments on List-type and Yes/No questions. 5.1 Entailment improves Yes/No accuracy We started by answering always YES (in batch 2 and 3) to get the baseline perfor- mance. For batch 4 we used entailment. Our algorithm was very simple: Given a question we iterate through the candidate sentences and try to \ufb01nd any candidate sentence is contradicting the question (with con\ufb01dence over 50%), if so \u2019No\u2019 is returned as answer, else \u2019Yes\u2019 is returned. In batch 4 this strategy produced better 12",
  "than the BioAsq baseline performance, and compared to our other systems, the use of entailment increased the performance by about 13% (macro F1 score). We used \u2019AllenNlp\u2019 [7] entailment library to \ufb01nd entailment of the candidate sentences with question. 5.2 For List-type the URLs have negative impact Overall, we followed the similar strategy that\u2019s been followed for Factoid Question Answering task. We started our experiment with batch 2, where we submitted 20 best answers (with context from snippets). Starting with batch 3, we performed post processing: once models generate answer predictions (n-best predictions), we do post-processing on the predicted answers. In test batch 4, our system (called FACTOIDS) achieved highest recall score of 0.7033 but low precision of 0.1119, leaving open the question of how could we have better balanced the two measures. In the post-processing phase, we take the top 20 (batch 3) and top 5 (batch 4 and 5), predicted answers, tokenize them using common separators: \u2019comma\u2019 , \u2019and\u2019, \u2019also\u2019, \u2019as well as\u2019.",
  "In the post-processing phase, we take the top 20 (batch 3) and top 5 (batch 4 and 5), predicted answers, tokenize them using common separators: \u2019comma\u2019 , \u2019and\u2019, \u2019also\u2019, \u2019as well as\u2019. Tokens with characters count more than 100 are eliminated and rest of the tokens are added to the list of possible answers. BioASQ evaluation mechanism does not consider snippets with more than 100 characters as a valid answer. Considering lengthy snippets in to the list of answers would reduce the mean precision score. As a \ufb01nal step, duplicate snippets in the answer pool are removed.",
  "BioASQ evaluation mechanism does not consider snippets with more than 100 characters as a valid answer. Considering lengthy snippets in to the list of answers would reduce the mean precision score. As a \ufb01nal step, duplicate snippets in the answer pool are removed. For example, consider these top 3 answers predicted by the system (before post-processing): { \"text\": \"dendritic cells\", \"probability\": 0.7554540733426441, \"start_logit\": 8.466046333312988, \"end_logit\": 9.536355018615723 }, { \"text\": \"neutrophils, macrophages and distinct subtypes of dendritic cells\", \"probability\": 0.13806867348304214, \"start_logit\": 6.766478538513184, \"end_logit\": 9.536355018615723 }, { \"text\": \"macrophages and distinct subtypes of dendritic\", \"probability\": 0.013973475271178242, 13",
  "\"start_logit\": 6.766478538513184, \"end_logit\": 7.24576473236084 }, After execution of post-processing heuristics, the list of answers returned is as follows: [\"dendritic cells\"], [\"neutrophils\"], [\"macrophages\"], [\"distinct subtypes of dendritic cells\"] 6 Summary of our results The tables below summarize all our results. They show that the performance of our systems was mixed. The simple architectures and algorithm we used worked very well only in Batch 3. However, we feel we can built a better system based on this experience. In particular we observed both the value of contextual embeddings and of feature engineering (LAT), however we failed to combine them properly. 6.1 Factoid questions 6.1.1 Systems used in Batch 5 experiments System description for UNCC QA1: The system was \ufb01netuned on the SQuAD 2.0. For data preprocessing Context / paragraph was generated from relevant snippets provided in the test data. System description for QA1 : LAT feature was added and \ufb01netuned with SQuAD 2.0.",
  "For data preprocessing Context / paragraph was generated from relevant snippets provided in the test data. System description for QA1 : LAT feature was added and \ufb01netuned with SQuAD 2.0. For data preprocessing Context / paragraph was generated from relevant snippets provided in the test data. System description for UNCC QA3 : Fine tuning process is same as it is done for the system UNCC QA1 in test batch-5. Difference is during data preprocessing, Context/paragraph is generated from the relevant documents for which URLS are included in the test data. 6.2 List Questions For List-type questions, although post processing helped in the later batches, we never managed to obtain competitive precision, although our recall was good. 14",
  "Table 1: Factoid Questions. In Batch 3 we obtained the highest score. Also the relative distance between our best system and the top performing system shrunk between Batch 4 and 5. System Strict Accuracy Lenient Accuracy MRR Batch 1 QA1 0.1538 0.2308 0.1761 Top Competitor 0.4103 0.5385 0.4637 Batch 2 QA1 0.36 0.48 0.4033 Top Competitor 0.52 0.64 0.5667 Batch 3 UNCC QA1 0.4483 0.5862 0.5115 UNCC QA2 0.4138 0.5862 0.4856 UNCC QA3 0.4138 0.5862 0.4943 Top Competitor 0.36 0.48 0.5023 Batch 4 FACTOIDS 0.5294 0.7353 0.6103 UNCC QA1 0.4706 0.7353 0.5833 Top Competitor 0.5882 0.8235 0.",
  "48 0.5023 Batch 4 FACTOIDS 0.5294 0.7353 0.6103 UNCC QA1 0.4706 0.7353 0.5833 Top Competitor 0.5882 0.8235 0.6912 Batch 5 UNCC QA1 0.2857 0.4286 0.3305 UNCC QA3 0.2286 0.3143 0.2643 QA1 0.2286 0.3714 0.2938 Top Competitor 0.2857 0.5143 0.3638 6.3 Yes/No questions The only thing worth remembering from our performance is that using entailment can have a measurable impact (at least with respect to a weak baseline). The results (weak) are in Table 3.",
  "2857 0.5143 0.3638 6.3 Yes/No questions The only thing worth remembering from our performance is that using entailment can have a measurable impact (at least with respect to a weak baseline). The results (weak) are in Table 3. 7 Discussion, Future Experiments, and Conclusions 7.0.1 Summary: In contrast to 2018, when we submitted [2] to BioASQ a system based on extractive summarization (and scored very high in the ideal answer category), this year we mainly targeted factoid question answering task and focused on experimenting with 15",
  "Table 2: List Questions System Mean Precision Recall F-measure Batch 2 QA1 0.0471 0.2898 0.0786 Top Competitor 0.5826 0.4839 0.4732 Batch 3 UNCC QA1 0.0780 0.4711 0.1297 Top Competitor 0.4267 0.3058 0.3298 Batch 4 FACTOIDS 0.1119 0.7033 0.1893 UNCC QA1 0.1087 0.6968 0.1846 UNCC QA3 0.1087 0.6968 0.1846 Top Competitor 0.4841 0.5051 0.4604 Batch 5 UNCC QA1 0.2051 0.5127 0.2862 Top Competitor 0.5653 0.4131 0.4619 BIOBERT. After these experiments we see the promise of BIOBERT in QA tasks, but we also see its limitations. The latter we tried to address with mixed results using feature engineering.",
  "After these experiments we see the promise of BIOBERT in QA tasks, but we also see its limitations. The latter we tried to address with mixed results using feature engineering. Overall these experiments allowed us to secure a best and a second best score in different test batches. Along with Factoid-type question, we also tried Yes/No and List-type questions, and did reasonably well with our very simple approach. For Yes/No the moral worth remembering is that reasoning has a potential to in\ufb02uence results, as evidenced by our adding the AllenNLP entailment [7] system increased its performance. All our data and software is available at Github, in the previously referenced URL (end of Section 2). 7.0.2 Future experiments In the current model, we have a shallow neural network with a softmax layer for predicting answer span. Shallow networks however are not good at generalizations. In our future experiments we would like to create dense question answering neural network with a softmax layer for predicting answer span.",
  "7.0.2 Future experiments In the current model, we have a shallow neural network with a softmax layer for predicting answer span. Shallow networks however are not good at generalizations. In our future experiments we would like to create dense question answering neural network with a softmax layer for predicting answer span. The main idea is to get contextual word embedding for the words present in the question and paragraph (Context) and feed the contextual word embeddings retrieved from the last layer of BioBERT to the dense question answering network. The mentioned dense 16",
  "Table 3: Yes/No Questions System Accuracy F1 Yes F1 No Macro F1 Batch 1 QA1 0.7931 0.8846 \u2013 0.4423 Top Competitor 0.8276 0.8980 0.4444 0.6712 Batch 2 QA1 0.5667 0.7234 \u2013 0.3617 Top Competitor 0.8333 0.8387 0.8276 0.8331 Batch 3 QA1 0.7826 0.8780 \u2013 0.4390 UNCC QA3 0.7826 0.8780 \u2013 0.4390 Top Competitor 0.8696 0.9231 0.5714 0.7473 Batch 4 UNCC QA1 0.6087 0.7097 0.4000 0.5548 FACTOIDS 0.7391 0.8500 \u2013 0.4250 UNCC QA3 0.7391 0.8500 \u2013 0.4250 Top Competitor 0.8696 0.9143 0.",
  "4000 0.5548 FACTOIDS 0.7391 0.8500 \u2013 0.4250 UNCC QA3 0.7391 0.8500 \u2013 0.4250 Top Competitor 0.8696 0.9143 0.7273 0.8208 Batch 5 UNCC QA2 0.5429 0.7037 \u2013 0.3519 Top Competitor 0.8286 0.8500 0.8000 0.8250 layered question answering neural network need to be tuned for \ufb01nding right hyper parameters. An example of such architecture is shown in Fig.4. In one more experiment, we would like to add a better version of LAT contextual word embedding as a feature, along with the actual contextual word embeddings for question text, and Context and feed them as input to the dense question an- swering neural network. By this experiment, we would like to \ufb01nd if LAT feature is improving overall answer prediction accuracy.",
  "By this experiment, we would like to \ufb01nd if LAT feature is improving overall answer prediction accuracy. Adding LAT feature this way instead of feeding this word piece embedding directly to the BioBERT (as we did in our above experiments) would not downgrade the quality of contextual word embeddings generated form BioBERT\u2019. Quality contextual word embeddings would lead to ef\ufb01cient transfer learning and chances are that it would improve the model\u2019s answer prediction accuracy. We also see potential for incorporating domain speci\ufb01c inference into the task e.g. using the MedNLI dataset [15]. For all types of experiments it might be worth exploring clinical BERT embeddings [1], explicitly incorporating domain knowledge (e.g. [10]) and possibly deeper discourse representations (e.g. [14]). 17",
  "Figure 4: Proposed extension. In addition to using BioBERT we propose to add a network that would train on embeddings of questions (Emb Quesi) and paragraphs (Emb Pari) with additional LAT features, or more broadly other semantic features, like relations that can be derived from text References [1] Alsentzer, E., Murphy, J.R., Boag, W., Weng, W.H., Jin, D., Naumann, T., McDermott, M.: Publicly available clinical bert embeddings. arXiv preprint arXiv:1904.03323 (2019) [2] Bhandwaldar, A., Zadrozny, W.: UNCC QA: biomedical question answering system. In: Proceedings of the 6th BioASQ Workshop A challenge on large- scale biomedical semantic indexing and question answering. pp. 66\u201371 (2018) [3] Brown, E.W., Ferrucci, D., Lally, A., Zadrozny, W.W.",
  "In: Proceedings of the 6th BioASQ Workshop A challenge on large- scale biomedical semantic indexing and question answering. pp. 66\u201371 (2018) [3] Brown, E.W., Ferrucci, D., Lally, A., Zadrozny, W.W.: System and method for providing answers to questions (Sep 25 2012), US Patent 8,275,803 [4] Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: pre-training of deep bidirectional transformers for language understanding. In: NAACL-HLT (2018) 18",
  "[5] Dimitriadis, D., Tsoumakas, G.: Word embeddings and external resources for answer processing in biomedical factoid question answering. Journal of biomedical informatics p. 103118 (2019) [6] Ferrucci, D.A., Brown, E.W., Chu-Carroll, J., Fan, J., Gondek, D., Kalyanpur, A., Lally, A., Murdock, J.W., Nyberg, E., Prager, J.M., Schlaefer, N., Welty, C.A.: Building Watson: An Overview of the DeepQA Project. AI Magazine 31, 59\u201379 (2010) [7] Gardner, M., Grus, J., Neumann, M., Tafjord, O., Dasigi, P., Liu, N.F., Peters, M.E., Schmitz, M., Zettlemoyer, L.S.: AllenNLP: a deep semantic natural language processing platform.",
  ": AllenNLP: a deep semantic natural language processing platform. CoRR abs/1803.07640 (2018) [8] Lally, A., Prager, J.M., McCord, M.C., Boguraev, B.K., Patwardhan, S., Fan, J., Fodor, P., Chu-Carroll, J.: Question analysis: How Watson reads a clue. IBM Journal of Research and Development 56(3.4), 2\u20131 (2012) [9] Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C.H., Kang, J.: BioBERT: a pre-trained biomedical language representation model for biomedical text mining. CoRR abs/1901.08746 (2019) [10] Lu, M., Fang, Y., Yan, F., Li, M.: Incorporating domain knowledge into natural language inference on clinical texts.",
  "CoRR abs/1901.08746 (2019) [10] Lu, M., Fang, Y., Yan, F., Li, M.: Incorporating domain knowledge into natural language inference on clinical texts. IEEE Access 7, 57623\u201357632 (2019) [11] Manning, C.D., Surdeanu, M., Bauer, J., Finkel, J.R., Bethard, S., McClosky, D.: The stanford corenlp natural language processing toolkit. In: ACL (2014) [12] Mikolov, T., Chen, K., Corrado, G., Dean, J.: Ef\ufb01cient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 (2013) [13] Rajpurkar, P., Zhang, J., Lopyrev, K., Liang, P.S.: SQuAD: 100, 000+ Ques- tions for Machine Comprehension of Text.",
  ": SQuAD: 100, 000+ Ques- tions for Machine Comprehension of Text. In: EMNLP (2016) [14] Rao, S., Marcu, D., Knight, K., Daum\u00b4e III, H.: Biomedical event extraction using abstract meaning representation. In: BioNLP 2017. pp. 126\u2013135 (2017) [15] Romanov, A., Shivade, C.: Lessons from natural language inference in the clinical domain. arXiv preprint arXiv:1808.06752 (2018) [16] Sharma, V., Kulkarni, N., Pranavi, S., Bayomi, G., Nyberg, E., Mitamura, T.: BioAMA: towards an end to end BioMedical question answering system. In: BioNLP (2018) 19",
  "[17] Tsatsaronis, G., Balikas, G., Malakasiotis, P., Partalas, I., Zschunke, M., Alvers, M.R., Weissenborn, D., Krithara, A., Petridis, S., Polychronopoulos, D., Almirantis, Y., Pavlopoulos, J., Baskiotis, N., Gallinari, P., Artieres, T., Ngonga, A., Heino, N., Gaussier, E., Barrio-Alvers, L., Schroeder, M., Androutsopoulos, I., Paliouras, G.: An overview of the bioasq large- scale biomedical semantic indexing and question answering competition. BMC Bioinformatics 16, 138 (2015). https://doi.org/10.1186/s12859- 015-0564-6, http://www.biomedcentral.com/content/pdf/ s12859-015-0564-6.pdf [18] Wiese, G., Weissenborn, D., Neves, M.L.: Neural Question Answering at BioASQ 5B.",
  ": Neural Question Answering at BioASQ 5B. In: Cohen, K.B., Demner-Fushman, D., Ananiadou, S., Tsujii, J. (eds.) BioNLP 2017, Vancouver, Canada, August 4, 2017. pp. 76\u201379. Associa- tion for Computational Linguistics (2017). https://doi.org/10.18653/v1/W17- 2309, https://doi.org/10.18653/v1/W17-2309 20",
  "8 APPENDIX In this appendix we provide additional details about the implementations. 8.1 Systems and their descriptions: We used several variants of our systems when experimenting with the BioASQ problems. In retrospect, it would be much easier to understand the changes if we adopted some mnemonic conventions in naming the systems. So, we apologize for the names that do not re\ufb02ect the modi\ufb01cations, and necessitate this list. 8.1.1 Factoid Type Question Answering: We preprocessed the test data to convert test data to BioBERT format, We gen- erated Context/paragraph by either aggregating relevant snippets provided or by aggregating documents for which URLS are provided in the BioASQ test data. 8.1.2 Batch-1: 8.1.3 System description for QA1: We generated Context/paragraph by aggregating relevant snippets available in the test data and mapped it against the question text and question id. We ignored the content present in the documents (document URLS were provided in the original test data). The model is \ufb01netuned with BioASQ data.",
  "We ignored the content present in the documents (document URLS were provided in the original test data). The model is \ufb01netuned with BioASQ data. 8.1.4 Batch-2: 8.1.5 System description for QA1: data preprocessing is done in the same way as it is done for test batch-1. Model \ufb01ne tuned on BioASQ data. 8.1.6 Batch-3: 8.1.7 System description for UNCC QA 1: System is \ufb01netuned on the SQuAD 2.0 [reference]. For data preprocessing Context / paragraph is generated from relevant snippets provided in the test data. 21",
  "8.1.8 System description for UNCC QA3: System is \ufb01netuned on the SQuAD 2.0 [reference] and BioASQ dataset[].For data preprocessing Context / paragraph is generated from relevant snippets provided in the test data. 8.1.9 System description for UNCC QA2: Fine tuning process is same as for UNCC QA 1 . Difference is Context/paragraph is generated form from the relevant documents for which URLS are included in the test data. System UNCC QA 1 got the highest MRR score in the 3rd test batch set. 8.1.10 Batch-4: 8.1.11 System description for FACTOIDS: The System is \ufb01netuned on the SQuAD 2.0. For data preprocessing Context / paragraph is generated from relevant snippets provided in the test data. 8.1.12 System description for UNCC QA 1: LAT/ Focus word feature added and \ufb01ne tuned with SQuAD 2.0 [reference]. For data preprocessing Context / paragraph is generated from relevant snippets provided in the test data.",
  "8.1.12 System description for UNCC QA 1: LAT/ Focus word feature added and \ufb01ne tuned with SQuAD 2.0 [reference]. For data preprocessing Context / paragraph is generated from relevant snippets provided in the test data. 8.1.13 Batch-5: 8.1.14 System description for UNCC QA 1: The System is \ufb01netuned on the SQuAD 2.0. For data preprocessing Context / paragraph is generated from relevant snippets provided in the test data. 8.1.15 System description for QA1: LAT/ Focus word feature added and \ufb01ne tuned with SQuAD 2.0 [reference]. For data preprocessing Context / paragraph is generated from relevant snippets provided in the test data. 8.1.16 System description for UNCC QA3: Fine tuning process is same as it is done for the system UNCC QA 1 in test batch-5. Difference is during data preprocessing, Context/paragraph is generated form from the relevant documents for which URLS are included in the test data. 22",
  "8.1.17 List Type Questions: We attempted List type questions starting from test batch 2. Used similar approach that\u2019s been followed for Factoid Question answering task. For all the test batch sets, in the data pre processing phase Context/ paragraph is generated either by aggregating relevant snippets or by aggregating documents(URLS) provided in the BioASQ test data. For test batch-2, model (System: QA1) is \ufb01netuned on BioASQ data and submitted top 20 answers predicted by the model as the list of answers. system QA1 achieved low F-Measure score:0.0786 in the second test batch. In the further test batches for List type questions, we \ufb01netuned the model on Squad data set [reference], implemented post processing techniques (refer section 5.2) and achieved a better F-measure score: 0.2862 in the \ufb01nal test batch set.",
  "In the further test batches for List type questions, we \ufb01netuned the model on Squad data set [reference], implemented post processing techniques (refer section 5.2) and achieved a better F-measure score: 0.2862 in the \ufb01nal test batch set. In test batch-3 (Systems : QA1/UNCC QA 1/UNCC QA3/UNCC QA2) top 20 answers returned by the model is sent for post processing and in test batch 4 and 5 only top 5 answers are sent for post processing. System UNCC QA2(in batch 3) for List type question answering, Context is generated from documents for which URLS are provided in the BioASQ test data. for the rest of the systems (in test batch-3) for List Type question answering task snippets present in the BioaSQ test data are used to generate context. In test batch-4 (System : FACTOIDS/UNCC QA 1/UNCC QA3) top 5 answers returned by the model is sent for post processing. In case of system FACTOIDS snippets in the test data were used to generate context.",
  "In test batch-4 (System : FACTOIDS/UNCC QA 1/UNCC QA3) top 5 answers returned by the model is sent for post processing. In case of system FACTOIDS snippets in the test data were used to generate context. for systems UNCC QA 1 and UNCC QA3 context is generated from the documents for which URLS are provided in the BioASQ test data. In test batch-5 ( Systems: QA1/UNCC QA 1/UNCC QA3/UNCC QA2 ) our approach is the same as that of test batch-4 where top 5 answers returned by the model is sent for post processing. for all the systems (in test batch-5) context is generated from the snippets provided in the BioASQ test data. 8.1.18 Yes/No Type Questions: For the \ufb01rst 3 test batches, We have submitted answer Yes to all the questions. Later, we employed Sentence Entailment techniques(refer section 6.0) for the fourth and \ufb01fth test batch sets.",
  "8.1.18 Yes/No Type Questions: For the \ufb01rst 3 test batches, We have submitted answer Yes to all the questions. Later, we employed Sentence Entailment techniques(refer section 6.0) for the fourth and \ufb01fth test batch sets. Our Systems with Sentence Entailment approach (for Yes/ No question answering): UNCC QA 1(test batch-4), UNCC QA3(test batch-5). 8.2 Additional details for Yes/No Type Questions We used Textual Entailment in Batch 4 and 5 for Yes/No question type. The algorithm was very simple: Given a question we iterate through the candidate 23",
  "sentences, and look for any candidate sentences contradicting the question. If we \ufb01nd one \u2019No\u2019 is returned as answer, else \u2019Yes\u2019 is returned. (The con\ufb01dence for contradiction was set at 50%) We used AllenNLP [7] entailment library to \ufb01nd entailment of the candidate sentences with question. Flow Chart for Yes/No Question answer processing is shown in Fig.5 Figure 5: Flow Chart for Yes/No Question answer processing 24",
  "8.3 Assumptions, rules and logic \ufb02ow for deriving Lexical Answer Types from questions There are different question types, and we distinguished them based on the question words: Which, What, When, How etc. Each type of question is being handled differently and there are commonalities among the rules written for different ques- tion types. How are question words identi\ufb01ed? question words have parts of speech(POS): \u2019WDT\u2019, \u2019WRB\u2019, \u2019WP\u2019. Assumptions: 1) Lexical answer type (LAT) or focus word is of type Noun and follows the question word. 2) The LAT word is a Subject. (This clearly not always true, but we used a very simple method). Note: StanfordNLP dependency parsing tag for identifying subject is \u2019nsubj\u2019 or \u2019nsubjpass\u2019. 3) When a question has multiple words that are of type Subject (and Noun), a word that is in proximity to the question word is considered as LAT. 4) For questions with question words: When, Who, Why, the LAT is a question word itself that is, When, Who, Why respectively.",
  "4) For questions with question words: When, Who, Why, the LAT is a question word itself that is, When, Who, Why respectively. Rules and logic \ufb02ow to traverse a question: The three cases below describe the logic \ufb02ow of \ufb01nding LATs. The \ufb01gures show the grammatical structures used for this purpose. 8.3.1 Case-1: Question with question word How. For questions with question word \u2019How\u2019, the adjective that follows the question word is considered as LAT (need not follow immediately). If an adjective is absent, word \u2019How\u2019 is considered as LAT. When there are multiple words that are adjectives, a word in close proximity to the question word and follows it is returned as LAT. Note: The part of speech tag to identify adjectives is \u2019JJ\u2019. For Other possible question words like whose. LAT/Focus word is question words itself. Example Question: How many selenoproteins are encoded in the human genome? 8.3.2 Case-2: Questions with question words Which , What and all other possible question words; a \u2019Noun\u2019 immediately following the question word. 25",
  "Figure 6: Dependency parse visualization for the example \u2019how\u2019 question. Figure 7: Dependency parse visualization for the example \u2019which\u2019 question. Example Question: Which enzyme is targeted by Evolocumab? Here, Focus word/LAT is enzyme which is both Noun and Subject and immediately follows the question word. When the word immediately following the question word is a noun, the window size is set to 3. This size 3 means that we iterate through the next 3 words (if present) to check if any of the word is both \u2019Noun\u2019 and \u2019Subject\u2019, If so, the word is considered as LAT/Focus Word. Else the word that is present very next to the question word is considered as LAT. 8.3.3 Case-3: Questions with question words Which , What and all other possible question words; word immediately following the question word is not a \u2019Noun\u2019. Example Question: What is the function of the protein Magt1? Here, Focus word/LAT is function which is both Noun and Subject and does not immediately follow the question word.",
  "Example Question: What is the function of the protein Magt1? Here, Focus word/LAT is function which is both Noun and Subject and does not immediately follow the question word. When the very next word following the question word is not a Noun, window size is set to 5. Window size 5 corresponds that we iterate through the next 5 words (if present) and search for the word that is both Noun and Subject. If present, the word is considered as LAT. Else, the \u2019Noun\u2019 close proximity to the question word and follows it is returned as LAT. 26",
  "Figure 8: Dependency parse visualization for the question about the function of the protein Magt1. Ad we mentioned earlier, the accuracy for LAT derivation is 75 percent. But clearly the simple logic described above can be improved, as shown in [8], [3]. Whether this in turn produces improvements in this particular task is an open question. 8.4 Proposing Future Experiments In the current model, we have a shallow neural network with a softmax layer for predicting answer span. Shallow networks however are not good at generalizations. In our future experiments we would like to create dense question answering neural network with a softmax layer for predicting answer span. The main idea is to get contextual word embedding for the words present in the question and paragraph (Context) and feed the contextual word embeddings retrieved from the last layer of BioBERT to the dense question answering network. The mentioned dense layered question answering Neural network need to be tuned for \ufb01nding right hyper parameters. An example of such architecture is shown in Fig.4. In another experiment we would like to only feed contextual word embeddings for Focus word/ LAT, paragraph/ Context as input to the question answering neural network.",
  "An example of such architecture is shown in Fig.4. In another experiment we would like to only feed contextual word embeddings for Focus word/ LAT, paragraph/ Context as input to the question answering neural network. In this experiment we would neglect all embeddings for the question text except that of Focus word/ LAT. Our assumption and idea for considering focus word and neglecting remaining words in the question is that during training phase it would make more precise for the model to identify the focus of the question and map answers against the questions focus. To validate our assumption, we would like to take sample question answering data and \ufb01nd the cosine distance between contextual embedding of Focus word and that of the actual answer and verify if the cosine distance is comparatively low in most of the cases. In one more experiment, we would like to add a better version of LAT contextual word embedding as a feature, along with the actual contextual word embeddings for 27",
  "question text, and Context and feed them as input to the dense question answering neural network. By this experiment, we would like to \ufb01nd if LAT feature is im- proving overall answer prediction accuracy. Adding LAT feature this way instead of feeding Focus words word piece embedding directly (as we did in our above experiments) to the BioBERT would not downgrade the quality of contextual word embeddings generated form BioBERT\u2019. Quality contextual word embeddings would lead to ef\ufb01cient transfer learning and chances are that it would improve the model\u2019s answer prediction accuracy. 28"
]