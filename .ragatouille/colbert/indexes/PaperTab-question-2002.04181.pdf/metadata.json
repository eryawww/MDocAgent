{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Performance Comparison of Crowdworkers and NLP Tools on Named-Entity Recognition and Sentiment Analysis of Political Tweets Mona Jalal*, Kate K. Mays**, Lei Guo**, and Margrit Betke* *Department of Computer Science, Boston University **Division of Emerging Media Studies, Boston University {jalal,kkmays,guolei,betke}@bu.edu Abstract We report results of a comparison of the ac- curacy of crowdworkers and seven Natural Language Processing (NLP) toolkits in solv- ing two important NLP tasks, named-entity recognition (NER) and entity-level sentiment (ELS) analysis. We here focus on a challeng- ing dataset, 1,000 political tweets that were collected during the U.S. presidential primary election in February 2016. Each tweet refers to at least one of four presidential candidates, i.e., four named entities. The groundtruth, established by experts in political communi- cation, has entity-level sentiment information for each candidate mentioned in the tweet. We tested several commercial and open source tools.",
      "Each tweet refers to at least one of four presidential candidates, i.e., four named entities. The groundtruth, established by experts in political communi- cation, has entity-level sentiment information for each candidate mentioned in the tweet. We tested several commercial and open source tools. Our experiments show that, for our dataset of political tweets, the most accurate NER system, Google Cloud NL, performed al- most on par with crowdworkers, but the most accurate ELS analysis system, TensiStrength, did not match the accuracy of crowdworkers by a large margin of more than 30 percent points. 1 Introduction As social media, specially Twitter, takes on an in- \ufb02uential role in presidential elections in the U.S., natural language processing of political tweets (Mohammad et al., 2015) has the potential to help with nowcasting and forecasting of election re- sults as well as identifying the main issues with a candidate \u2013 tasks of much interest to journal- ists, political scientists, and campaign organizers (Farzindar and Inkpen, 2015). As a methodology to obtain training data for a machine learning sys- tem that analyzes political tweets, Sameki et al.",
      "As a methodology to obtain training data for a machine learning sys- tem that analyzes political tweets, Sameki et al. (2016) devised a crowdsourcing scheme with vari- able crowdworker numbers based on the dif\ufb01culty of the annotation task. They provided a dataset of tweets where the sentiments towards political can- didates were labeled both by experts in political communication and by crowdworkers who were likely not domain experts. Sameki et al. (2016) revealed that crowdworkers can match expert per- formance relatively accurately and in a budget- ef\ufb01cient manner. Given this result, the authors envisioned future work in which groundtruth la- bels would be crowdsourced for a large number of tweets and then used to design an automated NLP tool for political tweet analysis. The question we address here is: How accurate are existing NLP tools for political tweet analysis? These tools would provide a baseline performance that any new machine learning system for political tweet analysis would compete against. We here explore whether existing NLP systems can an- swer the questions \u201cWhat sentiment?\u201d and \u201cTo- wards whom?\u201d accurately for the dataset of po- litical tweets provided by Sameki et al.",
      "We here explore whether existing NLP systems can an- swer the questions \u201cWhat sentiment?\u201d and \u201cTo- wards whom?\u201d accurately for the dataset of po- litical tweets provided by Sameki et al. (2016). In our analysis, we include NLP tools with publicly- available APIs, even if the tools were not specif- ically designed for short texts like tweets, and, in particular, political tweets. Our experiments reveal that the task of entity- level sentiment analysis is dif\ufb01cult for existing tools to answer accurately while the recognition of the entity, here, which politician, was easier. 2 NLP Toolkits NLP toolkits typically have the following capabil- ities: tokenization, part-of-speech (PoS) tagging, chunking, named entity recognition and sentiment analysis. In a study by Pinto et al. (2016), it is shown that the well-known NLP toolkits NLTK (Bird and Loper, 2004), Stanford CoreNLP (Man- ning et al., 2014), and TwitterNLP (Ritter et al., 2011) have tokenization, PoS tagging and NER modules in their pipelines.",
      "There are two main approaches for NER: (1) rule-based and (2) statistical or machine learning arXiv:2002.04181v2  [cs.CL]  11 Aug 2020",
      "based. The most ubiquitous algorithms for se- quence tagging use Hidden Markov Models (Ju- rafsky and Martin, 2009), Maximum Entropy Markov Models (Jurafsky and Martin, 2009; Mc- Callum et al., 2000), or Conditional Random Fields (Sutton and McCallum, 2012). Recent works (Wang et al., 2016; Zhang and Liu, 2017) have used recurrent neural networks with attention modules for NER. Sentiment detection tools like SentiStrength (Thelwall et al., 2010) and TensiStrength (Thel- wall, 2017) are rule-based tools, relying on vari- ous dictionaries of emoticons, slangs, idioms, and ironic phrases, and set of rules that can detect the sentiment of a sentence overall or a targeted sen- timent. Given a list of keywords, TensiStrength (similar to SentiStrength) reports the sentiment to- wards selected entities in a sentence, based on \ufb01ve levels of relaxation and \ufb01ve levels of stress.",
      "Given a list of keywords, TensiStrength (similar to SentiStrength) reports the sentiment to- wards selected entities in a sentence, based on \ufb01ve levels of relaxation and \ufb01ve levels of stress. Among commercial NLP toolkits (e.g., AYLIEN; MS Text Analytics; IBM Watson NLU), we selected Google Cloud Natural Language and Rosette Text Analytics for our experiments, which, to the best of our knowledge, are the only publicly accessible commercial APIs for the task of entity-level sentiment analysis that is agnostic to the text domain. We also report results of Ten- siStrength (Thelwall, 2017), TwitterNLP (Ritter et al., 2011), spaCy, CogComp-NLP (Khashabi et al., 2018), and Stanford NLP NER (Finkel et al., 2005). 3 Dataset and Analysis Methodology We used the 1,000-tweet dataset by Sameki et al.",
      "3 Dataset and Analysis Methodology We used the 1,000-tweet dataset by Sameki et al. (2016) that contains the named-entities labels and entity-level sentiments for each of the four 2016 presidential primary candidates Bernie Sanders, Donald Trump, Hillary Clinton, and Ted Cruz, provided by crowdworkers, and by experts in po- litical communication, whose labels are consid- ered groundtruth. The crowdworkers were located in the US and hired on the Amazon Mechanical Turk platform. For the task of entity-level senti- ment analysis, a 3-scale rating of \u201cnegative,\u201d \u201cneu- tral,\u201d and \u201cpositive\u201d was used by the annotators. Sameki et al. (2016) proposed a decision tree approach for computing the number of crowd- workers who should analyze a tweet based on the dif\ufb01culty of the task. Tweets are labeled by 2, 3, 5, or 7 workers based on the dif\ufb01culty of the task and the level of disagreement between the crowdwork- ers.",
      "Tweets are labeled by 2, 3, 5, or 7 workers based on the dif\ufb01culty of the task and the level of disagreement between the crowdwork- ers. The model computes the number of workers based on how long a tweet is, the presence of a link in a tweet, and the number of present sarcasm signals. Sarcasm is often used in political tweets and causes disagreement between the crowdwork- ers. The tweets that are deemed to be sarcastic by the decision tree model, are expected to be more dif\ufb01cult to annotate, and hence are allocated more crowdworkers to work on. We conducted two sets of experiments. In the \ufb01rst set, we used TensiStrength, Google Cloud Natural Language, and Rosette Text Analytics, for entity-level sentiment analysis; in the second set, Google Cloud Natural Language, spaCy, Stanford NER, CogComp-NLP, and TwitterNLP, Rosette Text Analytics for named-entity recognition. In the experiments that we conducted with Twit- terNLP for named-entity recognition, we worked with the default values of the model.",
      "In the experiments that we conducted with Twit- terNLP for named-entity recognition, we worked with the default values of the model. Fur- thermore, we selected the 3-class Stanford NER model, which uses the classes \u201cperson,\u201d \u201corga- nization,\u201d and \u201clocation\u201d because it resulted in higher accuracy compared to the 7-class model. For CogComp-NLP NER we used Ontonotes 5.0 NER model (Weischedel et al., 2013). For spaCy NER we used the \u2018en core web lg\u2019 model. We report the experimental results for our two tasks in terms of the correct classi\ufb01cation rate (CCR). For sentiment analysis, we have a three- class problem (positive, negative, and neutral), where the classes are mutually exclusive. The CCR, averaged for a set of tweets, is de\ufb01ned to be the number of correctly-predicted sentiments over the number of groundtruth sentiments in these tweets. For NER, we consider that each tweet may reference up to four candidates, i.e., targeted entities.",
      "The CCR, averaged for a set of tweets, is de\ufb01ned to be the number of correctly-predicted sentiments over the number of groundtruth sentiments in these tweets. For NER, we consider that each tweet may reference up to four candidates, i.e., targeted entities. The CCR, averaged for a set of tweets, is the number of correctly predicted entities (can- didates) over the number of groundtruth entities (candidates) in this set. 4 Results and Discussion The dataset of 1,000 randomly selected tweets contains more than twice as many tweets about Trump than about the other candidates. In the named-entity recognition experiment, the average CCR of crowdworkers was 98.6%, while the CCR of the automated systems ranged from 77.2% to 96.7%. For four of the automated systems, de- tecting the entity Trump was more dif\ufb01cult than the other entities (e.g., spaCy 72.7% for the entity",
      "Figure 1: Incorrect NER by spaCy (top) and incorrect ELS analysis by Google Cloud (bottom) Table 1: Average Correct Classi\ufb01cation Rate (CCR) for named-entity recognition (NER) of four presidential can- didates and entity-level sentiment (ELS) analysis by NLP tools and crowdworkers All Entities Bernie Sanders Donald Trump Hillary Clinton Ted Cruz # Tweets 1,000 236 510 225 211 NER Rosette Text Analytics 77.2% 80.1% 79.8% 84.4% 60.2% TwitterNLP 81.2% 89.4% 76.5% 79.6% 85.3% CogComp-NLP 82.6% 83.9% 81.2% 83.9% 79.6% Stanford NER 83.2% 86.4% 76.8% 86.2% 83.4% spaCy 88.2% 91.1% 72.7% 92% 91.5% Google Cloud NL 96.7% 97.9% 96.1% 96.",
      "4% 76.8% 86.2% 83.4% spaCy 88.2% 91.1% 72.7% 92% 91.5% Google Cloud NL 96.7% 97.9% 96.1% 96.4% 97.1% mTurk crowdworkers 98.6% 100% 99% 97.3% 98.1% ELS Rosette Text Analytics 31.7% 38.5% 24.9% 40.4% 31.3% Google Cloud NL 43.2% 44.9% 40.8% 45.5% 44.8% TensiStrength 44.2% 52.1% 40.8% 43.6% 44.1% mTurk crowdworkers 74.7% 77.9% 71.7% 67.5% 80.5% Trump vs. above 91% for the other entities). An example of incorrect NER is shown in Figure 1 top.",
      "1% mTurk crowdworkers 74.7% 77.9% 71.7% 67.5% 80.5% Trump vs. above 91% for the other entities). An example of incorrect NER is shown in Figure 1 top. The dif\ufb01culties the automated tools had in NER may be explained by the fact that the tools were not trained on tweets, except for TwitterNLP, which was not in active development when the data was created (Farzindar and Inkpen, 2015). In the sentiment analysis experiments, we found that a tweet may contain multiple sentiments. The groundtruth labels contain 210 positive sen- timents, 521 neutral sentiments, and 305 negative sentiments to the candidates. We measured the CCR, across all tweets, to be 31.7% for Rosette Text Analytics, 43.2% for Google Cloud, 44.2% for TensiStrength, and 74.7% for the crowdwork- ers. This means the difference between the perfor- mance of the tools and the crowdworkers is signif- icant \u2013 more than 30 percent points.",
      "This means the difference between the perfor- mance of the tools and the crowdworkers is signif- icant \u2013 more than 30 percent points. Crowdworkers correctly identi\ufb01ed 62% of the neutral, 85% of the positive, and 92% of the nega- tive sentiments. Google Cloud correctly identi\ufb01ed 88% of the neutral sentiments, but only 3% of the positive, and 19% of the negative sentiments. Ten- siStrength correctly identi\ufb01ed 87.2% of the neutral sentiments, but 10.5% of the positive, and 8.1% of the negative sentiments. Rosette Text Analytics correctly identi\ufb01ed 22.7% of neutral sentiments, 38.1% of negative sentiments and 40.9% of posi- tive sentiments. The lowest and highest CCR per- tains to tweets about Trump and Sanders for both Google Cloud and TensiStrength, Trump and Clin- ton for Rosette Text Analytics, and Clinton and Cruz for crowdworkers. An example of incorrect ELS analysis is shown in Figure 1 bottom.",
      "The lowest and highest CCR per- tains to tweets about Trump and Sanders for both Google Cloud and TensiStrength, Trump and Clin- ton for Rosette Text Analytics, and Clinton and Cruz for crowdworkers. An example of incorrect ELS analysis is shown in Figure 1 bottom. 5 Conclusions and Future Work Our results show that existing NLP systems cannot accurately perform sentiment analysis of political tweets in the dataset we experimented with. La- beling by humans, even non-expert crowdworkers, yields accuracy results that are well above the re- sults of existing automated NLP systems. In future work we will therefore use a crowdworker-labeled dataset to train a new machine-learning based NLP system for tweet analysis. We will ensure that the training data is balanced among classes. Our plan is to use state-of-the-art deep neural networks and compare their performance for entity-level senti- ment analysis of political tweets.",
      "Acknowledgments Partial support of this work by the Hariri Insti- tute for Computing and Computational Science & Engineering at Boston University (to L.G.) and a Google Faculty Research Award (to M.B. and L.G.) is gratefully acknowledged. Additionally, we would like to thank Daniel Khashabi for his help in running the CogComp-NLP Python API and Mike Thelwal for his help with TensiStrength. We are also grateful to the Stanford NLP group for clarifying some of the questions we had with regards to the Stanford NER tool. References Amazon Mechanical Turk. https://www.mturk.com. Last accessed on May 12, 2018. AYLIEN. https://developer. aylien.com/text-api-demo. Last accessed on March 07, 2018. Steven Bird and Edward Loper. 2004. NLTK: The Natural Language Toolkit. In Proceedings of the 42nd Annual Meeting of the Association for Compu- tational Linguistics, Barcelona, Spain, July 21-26, 2004 - Poster and Demonstration. CogComp-NLP.",
      "2004. NLTK: The Natural Language Toolkit. In Proceedings of the 42nd Annual Meeting of the Association for Compu- tational Linguistics, Barcelona, Spain, July 21-26, 2004 - Poster and Demonstration. CogComp-NLP. http://nlp.cogcomp.org. Last accessed on May 17, 2018. Atefeh Farzindar and Diana Inkpen. 2015. Natural lan- guage processing for social media. Synthesis Lec- tures on Human Language Technologies, 8(2):1\u2013 166. Jenny Rose Finkel, Trond Grenager, and Christo- pher D. Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In ACL 2005, 43rd Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, 25-30 June 2005, University of Michigan, USA, pages 363\u2013370. Google Cloud Natural Language. https://cloud. google.com/natural-language. Last accessed on March 09, 2018.",
      "Google Cloud Natural Language. https://cloud. google.com/natural-language. Last accessed on March 09, 2018. IBM Watson NLU. IBM Watson Natu- ral Language Understanding. https://www. ibm.com/watson/services/natural-language-under- standing. Last accessed on March 07, 2018. Dan Jurafsky and James H. Martin. 2009. Speech and language processing: An introduction to natural language processing, computational linguistics, and speech recognition, Second Edition. Prentice Hall series in arti\ufb01cial intelligence. Prentice Hall, Pear- son Education International.",
      "2009. Speech and language processing: An introduction to natural language processing, computational linguistics, and speech recognition, Second Edition. Prentice Hall series in arti\ufb01cial intelligence. Prentice Hall, Pear- son Education International. Daniel Khashabi, Mark Sammons, Ben Zhou, Tom Redman, Christos Christodoulopoulos, Vivek Sriku- mar, Nicholas Rizzolo, Lev Ratinov, Guanheng Luo, Quang Do, Chen-Tse Tsai, Subhro Roy, Stephen Mayhew, Zhilli Feng, John Wieting, Xiaodong Yu, Yangqiu Song, Shashank Gupta, Shyam Upadhyay, Naveen Arivazhagan, Qiang Ning, Shaoshi Ling, and Dan Roth. 2018. Cogcompnlp: Your swiss army knife for nlp. In International Conference on Lan- guage Resources and Evaluation (LREC). Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David Mc- Closky. 2014.",
      "In International Conference on Lan- guage Resources and Evaluation (LREC). Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David Mc- Closky. 2014. The Stanford CoreNLP natural lan- guage processing toolkit. In Proceedings of the 52nd Annual Meeting of the Association for Compu- tational Linguistics: System Demonstrations, ACL 2014, June 22-27, 2014, Baltimore, MD, USA, Sys- tem Demonstrations, pages 55\u201360. Andrew McCallum, Dayne Freitag, and Fernando C. N. Pereira. 2000. Maximum entropy Markov mod- els for information extraction and segmentation. In Proceedings of the Seventeenth International Con- ference on Machine Learning (ICML 2000), Stan- ford University, Stanford, CA, USA, June 29 - July 2, 2000, pages 591\u2013598. Saif M. Mohammad, Xiaodan Zhu, Svetlana Kir- itchenko, and Joel D. Martin. 2015.",
      "Saif M. Mohammad, Xiaodan Zhu, Svetlana Kir- itchenko, and Joel D. Martin. 2015. Sentiment, emotion, purpose, and style in electoral tweets. In- formation Processing & Management, 51(4):480\u2013 499. MS Text Analytics. https://azure.microsoft.com/en- us/services/ cognitive-services/text-analytics, Last accessed on March 07, 2018. Alexandre Miguel Pinto, Hugo Gonc\u00b8alo Oliveira, and Ana Oliveira Alves. 2016. Comparing the perfor- mance of different NLP toolkits in formal and social media text. In 5th Symposium on Languages, Appli- cations and Technologies, SLATE 2016, June 20-21, 2016, Maribor, Slovenia, pages 3:1\u20133:16. Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named entity recognition in Tweets: An experimental study.",
      "Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named entity recognition in Tweets: An experimental study. In Proceedings of the 2011 Conference on Empirical Methods in Natural Lan- guage Processing, EMNLP 2011, 27-31 July 2011, John McIntyre Conference Centre, Edinburgh, UK, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1524\u20131534. Rosette Text Analytics. https://www.rosette.com. Last accessed on May 18, 2018. Mehrnoosh Sameki, Mattia Gentil, Kate K. Mays, Lei Guo, and Margrit Betke. 2016. Dynamic alloca- tion of crowd contributions for sentiment analysis during the 2016 U.S. presidential election. In The Fourth AAAI Conference on Human Computation and Crowdsourcing (HCOMP 2016), October 30- November 3, 2016. 10 pages. spaCy. http://spacy.io Last accessed on May 16, 2018. Stanford NER.",
      "10 pages. spaCy. http://spacy.io Last accessed on May 16, 2018. Stanford NER. https://nlp.stanford.edu/software/crf- ner.shtml. Last accessed on May 17, 2018.",
      "Charles A. Sutton and Andrew McCallum. 2012. An introduction to conditional random \ufb01elds. Founda- tions and Trends in Machine Learning, 4(4):267\u2013 373. TensiStrength. http://sentistrength.wlv.ac.uk/ ten- sistrength.html. Last accessed on March 09, 2018. Mike Thelwall. 2017. TensiStrength: Stress and re- laxation magnitude detection for social media texts. Information Processing & Management, 53(1):106\u2013 121. Mike Thelwall, Kevan Buckley, Georgios Paltoglou, Di Cai, and Arvid Kappas. 2010. Sentiment strength detection in short informal text. Journal of the American Society for Information Science and Tech- nology JASIST, 61(12):2544\u20132558. TwitterNLP. https://github.com/aritter/twitter nlp. Last accessed on May 17, 2018. Yequan Wang, Minlie Huang, Xiaoyan Zhu, and Li Zhao. 2016.",
      "TwitterNLP. https://github.com/aritter/twitter nlp. Last accessed on May 17, 2018. Yequan Wang, Minlie Huang, Xiaoyan Zhu, and Li Zhao. 2016. Attention-based LSTM for aspect- level sentiment classi\ufb01cation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pages 606\u2013615. Ralph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, et al. 2013. Ontonotes release 5.0 ldc2013t19. Linguistic Data Consortium, Philadel- phia, PA. Yue Zhang and Jiangming Liu. 2017. Attention mod- eling for targeted sentiment.",
      "2013. Ontonotes release 5.0 ldc2013t19. Linguistic Data Consortium, Philadel- phia, PA. Yue Zhang and Jiangming Liu. 2017. Attention mod- eling for targeted sentiment. In Proceedings of the 15th Conference of the European Chapter of the As- sociation for Computational Linguistics, Valencia, Spain, April 3-7, 2017, volume 2, pages 572\u2013577."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-2002.04181.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":4465,
  "avg_doclen":165.3703703704,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-2002.04181.pdf"
    }
  }
}