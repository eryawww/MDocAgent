[
  "RobBERT: a Dutch RoBERTa-based Language Model Pieter Delobelle1 and Thomas Winters1 and Bettina Berendt1,2 1 Department of Computer Science, KU Leuven 2 Faculty of Electrical Engineering and Computer Science, TU Berlin firstname.lastname@kuleuven.be Abstract Pre-trained language models have been dom- inating the \ufb01eld of natural language process- ing in recent years, and have led to signi\ufb01cant performance gains for various complex natu- ral language tasks. One of the most prominent pre-trained language models is BERT, which was released as an English as well as a multi- lingual version. Although multilingual BERT performs well on many tasks, recent studies show that BERT models trained on a single language signi\ufb01cantly outperform the multi- lingual version. Training a Dutch BERT model thus has a lot of potential for a wide range of Dutch NLP tasks. While previous approaches have used earlier implementations of BERT to train a Dutch version of BERT, we used RoBERTa, a robustly optimized BERT ap- proach, to train a Dutch language model called RobBERT.",
  "While previous approaches have used earlier implementations of BERT to train a Dutch version of BERT, we used RoBERTa, a robustly optimized BERT ap- proach, to train a Dutch language model called RobBERT. We measured its performance on various tasks as well as the importance of the \ufb01ne-tuning dataset size. We also evaluated the importance of language-speci\ufb01c tokenizers and the model\u2019s fairness. We found that Rob- BERT improves state-of-the-art results for var- ious tasks, and especially signi\ufb01cantly outper- forms other models when dealing with smaller datasets. These results indicate that it is a powerful pre-trained model for a large vari- ety of Dutch language tasks. The pre-trained and \ufb01ne-tuned models are publicly available to support further downstream Dutch NLP appli- cations. 1 Introduction The advent of neural networks in natural lan- guage processing (NLP) has signi\ufb01cantly im- proved state-of-the-art results within the \ufb01eld. Ini- tially, recurrent neural networks and long short- term memory networks dominated the \ufb01eld.",
  "1 Introduction The advent of neural networks in natural lan- guage processing (NLP) has signi\ufb01cantly im- proved state-of-the-art results within the \ufb01eld. Ini- tially, recurrent neural networks and long short- term memory networks dominated the \ufb01eld. Later, the transformer model caused a revolution in NLP by dropping the recurrent part and only keeping attention mechanisms (Vaswani et al., 2017). The transformer model led to other popular language models, e.g. GPT-2 (Radford et al., 2018, 2019). BERT (Devlin et al., 2019) improved over previ- ous models and recurrent networks by allowing the system to learn from input text in a bidirec- tional way, rather than only from left-to-right or the other way around. This model was later re- implemented, critically evaluated and improved in the RoBERTa model (Liu et al., 2019). These large-scale attention-based models pro- vide the advantage of being able to solve NLP tasks by having a common, expensive pre-training phase, followed by a smaller \ufb01ne-tuning phase.",
  "These large-scale attention-based models pro- vide the advantage of being able to solve NLP tasks by having a common, expensive pre-training phase, followed by a smaller \ufb01ne-tuning phase. The pre-training happens in an unsupervised way by providing large corpora of text in the desired language. The second phase only needs a rela- tively small annotated dataset for \ufb01ne-tuning to outperform previous popular approaches in one of a large number of possible language tasks. While language models are usually trained on English data, some multilingual models also ex- ist. These are usually trained on a large quan- tity of text in different languages. For example, Multilingual-BERT is trained on a collection of corpora in 104 different languages (Devlin et al., 2019), and generalizes language components well across languages (Pires et al., 2019). However, models trained on data from one speci\ufb01c language usually improve the performance of multilingual models for this particular language (Martin et al., 2019; de Vries et al., 2019).",
  "However, models trained on data from one speci\ufb01c language usually improve the performance of multilingual models for this particular language (Martin et al., 2019; de Vries et al., 2019). Training a RoBERTa model (Liu et al., 2019) on a Dutch dataset thus also potentially increases performances for many downstream Dutch NLP tasks. In this paper, we introduce RobBERT1, a Dutch RoBERTa-based pre-trained language model, and critically evaluate its performance on various language tasks against 1The model named itself RobBERT when it was prompted with \u201cIk heet <mask>BERT.\u201d (\u201cMy name is <mask>BERT.\u201d), which we found quite a suitable name. arXiv:2001.06286v2  [cs.CL]  16 Sep 2020",
  "other Dutch languages models. We also pro- pose several new tasks for testing the model\u2019s ze- roshot ability, evaluate its performance on smaller datasets, and for measuring the importance of a language-speci\ufb01c tokenizer. Finally, we provide an extensive fairness evaluation using recent tech- niques and a new translated dataset. 2 Related Work Transformer models have been successfully used for a wide range of language tasks. Initially, trans- formers were introduced for use in machine trans- lation, where they ef\ufb01ciently improved the state- of-the-art (Vaswani et al., 2017). This cornerstone was used in BERT, a transformer model obtain- ing state-of-the-art results for eleven natural lan- guage processing tasks, such as question answer- ing and natural language inference (Devlin et al., 2019). BERT is pre-trained with large corpora of text using two unsupervised tasks. The \ufb01rst task is called masked language modeling (MLM), mak- ing the model guess which word is masked in cer- tain position in the text.",
  "BERT is pre-trained with large corpora of text using two unsupervised tasks. The \ufb01rst task is called masked language modeling (MLM), mak- ing the model guess which word is masked in cer- tain position in the text. The second task is next sentence prediction, in which the model has to pre- dict if two sentences occur subsequent in the cor- pus, or randomly sampled from the corpus. These tasks allow the model to create internal represen- tations about a language, which could thereafter be reused for different language tasks. This archi- tecture has been shown to be a general language model that could be \ufb01ne-tuned with little data in a relatively ef\ufb01cient way for a very distinct range of tasks and still outperform previous architectures (Devlin et al., 2019). Transformer models are also capable of gen- erating contextualized word embeddings (Peters et al., 2018). Traditional word embeddings, e.g.",
  "Transformer models are also capable of gen- erating contextualized word embeddings (Peters et al., 2018). Traditional word embeddings, e.g. word2vec (Mikolov et al., 2013) and GloVe (Pen- nington et al., 2014), lack the capibility of differ- entiating words based on context (e.g. \u201ca stick\u201d versus \u201clet\u2019s stick to\u201d). Transformer models, like BERT, on the other hand automatically incorpo- rate the context a word occurs into its embedding. The attention mechanism in transformer en- coder models also allows for better resolution of coreferences between words (Joshi et al., 2019a). For example, in the sentence \u201cThe trophy doesnt \ufb01t in the suitcase because its too big.\u201d, the word \u201cit\u201d would refer to the the suitcase instead of the trophy if the last word was changed to \u201csmall\u201d (Levesque et al., 2012). Being able to resolve these corefer- ences is for example important for translation, as dependent words might change form, e.g. due to word gender.",
  "Being able to resolve these corefer- ences is for example important for translation, as dependent words might change form, e.g. due to word gender. While BERT has been shown to be a power- ful language model, it also received scrutiny on its training and pre-processing. The authors of RoBERTa (Liu et al., 2019) showed that while the NSP pre-training task made the model perform better, it was not due to its intended reason, as it might merely predict relatedness between corpus sentences rather than subsequent sentences. That Devlin et al. (2019) trained a better model when using NSP than without NSP is likely due to the model learning long-range dependencies that were longer than when just using single sentences. As such, the RoBERTa model uses only the MLM task, and uses multiple full sentences in every in- put. Other researchers later improved the NSP task by instead making the model predict for two subsequent sentences if they occur in the given or \ufb02ipped order in the corpus (Lan et al., 2019). Devlin et al.",
  "Other researchers later improved the NSP task by instead making the model predict for two subsequent sentences if they occur in the given or \ufb02ipped order in the corpus (Lan et al., 2019). Devlin et al. (2019) also presented a multi- lingual model (mBERT) with the same archi- tecture as BERT, but trained on Wikipedia cor- pora in 104 languages. Unfortunately, the qual- ity of these multilingual embeddings is consid- ered worse than their monolingual counterparts, as R\u00a8onnqvist et al. (2019) illustrated for German and English models in a generative setting. The monolingual French CamemBERT model (Mar- tin et al., 2019) also outperformed mBERT on all tasks. Brandsen et al. (2019) also outperformed mBERT on several Dutch tasks using their Dutch BERT-based language model, called BERT-NL, trained on the small SoNaR corpus (Oostdijk et al., 2013a). More recently, de Vries et al.",
  "(2019) also outperformed mBERT on several Dutch tasks using their Dutch BERT-based language model, called BERT-NL, trained on the small SoNaR corpus (Oostdijk et al., 2013a). More recently, de Vries et al. (2019) also showed similar results for Dutch using their BERTje model, outperforming multilingual BERT in a wide range of tasks, such as sentiment analy- sis and part-of-speech tagging by pre-training on multiple corpora. Since both these works are con- current with ours, we compare our results with BERTje and BERT-NL in this paper. 3 Pre-training RobBERT We pre-trained RobBERT using the RoBERTa training regime. We trained two different versions, one where only the pre-training corpus was re- placed with a Dutch corpus (RobBERT v1) and one where both the corpus and the tokenizer were re- placed with Dutch versions (RobBERT v2). These",
  "two versions allow to evaluate the importance of having a language-speci\ufb01c tokenizer. 3.1 Data We pre-trained our model on the Dutch section of the OSCAR corpus, a large multilingual corpus which was obtained by language classi\ufb01cation in the Common Crawl corpus (Ortiz Su\u00b4arez et al., 2019). This Dutch corpus is 39GB large, with 6.6 billion words spread over 126 million lines of text, where each line could contain multiple sen- tences. This corpus is thus much larger than the corpora used for similar Dutch BERT models, as BERTje used a 12GB corpus, and BERT-NL used the SoNaR-500 corpus (about 2.2GB). (de Vries et al., 2019; Brandsen et al., 2019). 3.2 Tokenizer For RobBERT v2, we changed the default byte pair encoding (BPE) tokenizer of RoBERTa to a Dutch tokenizer.",
  "(de Vries et al., 2019; Brandsen et al., 2019). 3.2 Tokenizer For RobBERT v2, we changed the default byte pair encoding (BPE) tokenizer of RoBERTa to a Dutch tokenizer. The vocabulary of the Dutch tokenizer was constructed using the Dutch sec- tion of the OSCAR corpus (Ortiz Su\u00b4arez et al., 2019) with the same byte-level BPE algorithm as RoBERTa (Liu et al., 2019). This tokenizer grad- ually builds its vocabulary by replacing the most common consecutive tokens with a new, merged token. We limited the vocabulary to 40k words, which is 10k words less than RobBERT v1, due to additional tokens including non-negligible num- ber of Unicode tokens that are not used in Dutch. These are likely caused due to misclassi\ufb01ed sen- tences during the creation of the OSCAR cor- pus (Ortiz Su\u00b4arez et al., 2019).",
  "These are likely caused due to misclassi\ufb01ed sen- tences during the creation of the OSCAR cor- pus (Ortiz Su\u00b4arez et al., 2019). 3.3 Training RobBERT shares its architecture with RoBERTa\u2019s base model, which itself is a replication and improvement over BERT (Liu et al., 2019). Like BERT, it\u2019s architecture consists of 12 self- attention layers with 12 heads (Devlin et al., 2019) with 117M trainable parameters. One difference with the original BERT model is due to the differ- ent pre-training task speci\ufb01ed by RoBERTa, us- ing only the MLM task and not the NSP task. During pre-training, it thus only predicts which words are masked in certain positions of given sentences. The training process uses the Adam op- timizer (Kingma and Ba, 2017) with polynomial decay of the learning rate lr = 10\u22126 and a ramp- up period of 1000 iterations, with hyperparame- ters \u03b21 = 0.9 and RoBERTa\u2019s default \u03b22 = 0.98.",
  "Additionally, a weight decay of 0.1 and a small dropout of 0.1 helps prevent the model from over- \ufb01tting (Srivastava et al., 2014). RobBERT was trained on a computing cluster with 4 Nvidia P100 GPUs per node, where the number of nodes was dynamically adjusted while keeping a \ufb01xed batch size of 8192 sentences. At most 20 nodes were used (i.e. 80 GPUs), and the median was 5 nodes. By using gradient ac- cumulation, the batch size could be set indepen- dently of the number of GPUs available, in order to maximally utilize the cluster. Using the Fairseq library (Ott et al., 2019), the model trained for two epochs, which equals over 16k batches in to- tal, which took about three days on the computing cluster. In between training jobs on the comput- ing cluster, 2 Nvidia 1080 Ti\u2019s also covered some parameter updates for RobBERT v2. 4 Evaluation We evaluated RobBERT on multiple downstream Dutch language tasks.",
  "In between training jobs on the comput- ing cluster, 2 Nvidia 1080 Ti\u2019s also covered some parameter updates for RobBERT v2. 4 Evaluation We evaluated RobBERT on multiple downstream Dutch language tasks. For testing text classi- \ufb01cation, we evaluate on sentiment analysis and on demonstrative and relative pronoun prediction. The latter task helps evaluating the zero-shot pre- diction abilities, i.e. using only the pre-trained model without any \ufb01ne-tuning. Both classi\ufb01ca- tion tasks are also used to measure how well Rob- BERT performs on smaller datasets, by only us- ing subsets of the data. For testing RobBERT\u2019s token tagging capabilities, we used both part-of- speech (POS) tagging and named entity recogni- tion (NER) tasks. 4.1 Sentiment Analysis We replicated the high-level sentiment analysis task used to evaluate BERT-NL (Brandsen et al., 2019) and BERTje (de Vries et al., 2019) to be able to compare our methods.",
  "4.1 Sentiment Analysis We replicated the high-level sentiment analysis task used to evaluate BERT-NL (Brandsen et al., 2019) and BERTje (de Vries et al., 2019) to be able to compare our methods. This task uses a dataset called Dutch Book Reviews dataset (DBRD), in which book reviews from hebban.nl are la- beled as positive or negative (van der Burgh and Verberne, 2019). Although the dataset contains 118,516 reviews, only 22,252 of these reviews are actually labeled as positive or negative, which are split in a 90% train and 10% test datasets. This dataset was released in a paper analysing the per- formance of an ULMFiT model (Universal Lan- guage Model Fine-tuning for Text Classi\ufb01cation model) (van der Burgh and Verberne, 2019). We \ufb01ne-tuned RobBERT on the \ufb01rst 10,000",
  "Table 1: Results of RobBERT \ufb01ne-tuned on several downstream classi\ufb01cation tasks, compared to the state of the art models for the tasks. For accuracy, we also report the 95% con\ufb01dence intervals. (Results annotated with * from van der Burgh and Verberne (2019), ** from de Vries et al. (2019), *** from Brandsen et al. (2019), **** from Allein et al. (2020)) 10k Full dataset Task + model ACC (95% CI) [%] F1 [%] ACC (95% CI) [%] F1 [%] Sentiment Analysis (DBRD) van der Burgh and Verberne (2019) \u2014 \u2014 93.8* \u2014 BERTje (de Vries et al., 2019) \u2014 \u2014 93.0** \u2014 BERT-NL (Brandsen et al., 2019) \u2014 \u2014 \u2014 84.0*** RobBERT v1 86.730 (85.32, 88.14) 86.729 94.422 (93.47,95.38) 94.",
  ", 2019) \u2014 \u2014 \u2014 84.0*** RobBERT v1 86.730 (85.32, 88.14) 86.729 94.422 (93.47,95.38) 94.422 RobBERT v2 94.379 (93.42, 95.33) 94.378 95.144 (94.25,96.04) 95.144 Die/Dat (Europarl) Baseline (Allein et al., 2020) \u2014 \u2014 75.03**** \u2014 mBERT (Devlin et al., 2019) 92.157 (92.06, 92.25) 90.898 98.285 (98.24,98.33) 98.033 BERTje (de Vries et al., 2019) 93.096 (92.84, 93.36) 91.279 98.268 (98.22,98.31) 98.014 RobBERT v1 97.006 (96.95, 97.07) 96.571 98.406 (98.36, 98.",
  "84, 93.36) 91.279 98.268 (98.22,98.31) 98.014 RobBERT v1 97.006 (96.95, 97.07) 96.571 98.406 (98.36, 98.45) 98.169 RobBERT v2 97.816 (97.76, 97.87) 97.514 99.232 (99.20, 99.26) 99.121 training examples as well as on the full dataset. While the ULMFiT model is \ufb01rst \ufb01ne-tuned us- ing the unlabeled reviews before training the clas- si\ufb01er (van der Burgh and Verberne, 2019), it is unclear whether the other BERT models utilized the unlabeled reviews for further pre-training (Sun et al., 2019) or only used the labeled data for \ufb01ne- tuning the pre-trained model. We did the latter, meaning further improvement is possible by ad- ditionally pre-training on unlabeled in-domain se- quences.",
  "We did the latter, meaning further improvement is possible by ad- ditionally pre-training on unlabeled in-domain se- quences. Another unknown is how these models dealt with reviews that were longer than the max- imum number of tokens, as the average book re- view length is 547 tokens, with 40% of the docu- ments being longer than our model could handle. For our experiments, we only gave the last tokens of a review as input, as we found the training per- formance to be better, likely due to containing a summarizing comments. We trained our model for 2000 iterations with a batch size of 128 and a warm-up of 500 iterations, reaching a learning rate of 10\u22125. The training took approx. 2 hours on 2 Nvidea 1080 Ti GPUs, the best-performing RobBERT v2 model was selected based on a val- idation accuracy of 0.994. We see that RobBERT outperforms the other BERT models.",
  "The training took approx. 2 hours on 2 Nvidea 1080 Ti GPUs, the best-performing RobBERT v2 model was selected based on a val- idation accuracy of 0.994. We see that RobBERT outperforms the other BERT models. Both ver- sions of RobBERT also outperform the state-of- the-art ULMFiT model, although the difference is only statistically signi\ufb01cant for RobBERT v2. 4.2 Die/Dat Disambiguation Since BERT models perform well on coreference resolution tasks (Joshi et al., 2019b), we pro- pose to evaluate RobBERT on the recently in- troduced \u201cdie/dat disambiguation\u201d task (Allein et al., 2020), as a novel way to evaluate the ze- roshot ability of Dutch BERT models. In Dutch, depending on the sentence, both \u201cdie\u201d and \u201cdat\u201d can be either demonstrative or relative pronouns; in addition they can also be used in a subordinat- ing conjunction, i.e. to introduce a clause.",
  "In Dutch, depending on the sentence, both \u201cdie\u201d and \u201cdat\u201d can be either demonstrative or relative pronouns; in addition they can also be used in a subordinat- ing conjunction, i.e. to introduce a clause. The use of either of these words depends on the gender of the word it refers to. Allein et al. (2020) presented multiple models trained on the Europarl (Koehn, 2005) and SoNaR corpora (Oostdijk et al., 2013b), achieving an accuracy of 75.03% on Europarl to 84.56% on SoNaR. For this task, we use the Dutch Europarl cor- pus (Koehn, 2005), with the \ufb01rst 1.3M sequences (head) for training and last 399k (tail) as test set. Every sequence containing \u201cdie\u201d or \u201cdat\u201d creates an example for every occurrence of either word by masking the occurrence. For the test set, this resulted in about 289k masked sentences. BERT-like models can solve this task using two different approaches.",
  "Every sequence containing \u201cdie\u201d or \u201cdat\u201d creates an example for every occurrence of either word by masking the occurrence. For the test set, this resulted in about 289k masked sentences. BERT-like models can solve this task using two different approaches. Since the task is about pre- dicting words, their default MLM task can be used to guess which of the two words is more probable in a particular masked position. This allows the comparison of zero-shot BERT models, i.e. with- out any \ufb01ne-tuning on the training data (Table 2).",
  "The second approach uses the masked sentences to create two versions by \ufb01lling the mask with either \u201cdie\u201d and \u201cdat\u201d, separate them using the [SEP] token and making the model predict which of the two sentences is correct. This \ufb01ne-tuning was per- formed using 4 Nvidia GTX 1080 Ti GPUs, taking 30 minutes for 13 epochs on 10k sequences and about 24 hours for 3 epochs on the full dataset. We did no hyperparameter tuning, as the initial hyper- parameters (lr = 10\u22125, \u03f5 = 10\u22129, warm-up of 250 steps, batch size of 32 (10k) or 128 (full dataset), dropout of 0.1) were satisfactory. To measure RobBERTs performance on smaller datasets, we trained the model twice for both the sentiment analysis task and the die/dat disam- biguation task, once with a subset of 10k utter- ances, and once with the full training dataset. Table 2: Performance of predicting die/dat as most likely candidate for a mask using zero-shot BERT mod- els (i.e.",
  "Table 2: Performance of predicting die/dat as most likely candidate for a mask using zero-shot BERT mod- els (i.e. without \ufb01ne-tuning) as well as a majority class predictor (ZeroR), tested on the 288,799 test set sen- tences Model Accuracy [%] ZeroR (majority class) 66.70 mBERT (Devlin et al., 2019) 90.21 BERTje (de Vries et al., 2019) 94.94 RobBERT v1 98.03 RobBERT v2 98.75 RobBERT outperforms previous models as well as other BERT models both with as well as with- out \ufb01ne-tuning (see Table 1 and Table 2). It is also able to reach similar performance using less data. The fact that both for the \ufb01ne-tuned and the zero- shot setting, RobBERT outperforms other BERT models is also an indication that the base model has internalised more knowledge about Dutch than the others, likely due to the improved pre-training regime and using a larger corpus.",
  "The fact that both for the \ufb01ne-tuned and the zero- shot setting, RobBERT outperforms other BERT models is also an indication that the base model has internalised more knowledge about Dutch than the others, likely due to the improved pre-training regime and using a larger corpus. We can also see that having a Dutch tokenizer strongly helps re- duce the error rate for this task, halving the error rate when \ufb01ne-tuned on the full dataset. The rea- son the BERT-based models outperform the pre- vious RNN-based approach is likely the encoders ability to better deal with coreference resolution (Joshi et al., 2019a), and by extension deciding which word the \u201cdie\u201d or \u201cdat\u201d belongs to. The fact that RobBERT strongly outperforms the other BERT models on subsets of the data indicates that it is a suitable candidate for Dutch tasks that only have limited data available. 4.3 Part-of-speech Tagging Part-of-speech (POS) tagging involves labeling to- kens rather than labeling sequences.",
  "4.3 Part-of-speech Tagging Part-of-speech (POS) tagging involves labeling to- kens rather than labeling sequences. For this, we used a different head with an classi\ufb01cation output for each token, all activated by a softmax function. When a word consists of multiple tokens, the \ufb01rst token is used for the the label of the word. We perform the same POS \ufb01ne-tuning regimes as RoBERTa (Liu et al., 2019) to evaluate Rob- BERT\u2019s performance. When \ufb01ne-tuning, we em- ploy a linearly decaying learning rate with a warm- up for 6% of the total optimisation steps (Liu et al., 2019). For all the encoder-based models in our evaluation, we also perform a limited hyperparam- eter search on the development set with learning rate lr \u2208{10\u22125, 2 \u00b7 10\u22125, 3 \u00b7 10e\u22125, 10\u22124} and batch size \u2208{16, 32, 48}, which is also based on RoBERTa\u2019s \ufb01ne-tuning.",
  "Table 3: POS tagging on Lassy UD. For accuracy, we also report the 95% con\ufb01dence intervals. Task + model ACC (95% CI) [%] Frog (Bosch et al., 2007) 91.7 (91.2, 92.2) mBERT (Devlin et al., 2019) 96.5 (96.2, 96.9) BERTje (de Vries et al., 2019) 96.3 (96.0, 96.7) RobBERT v1 96.4 (96.0, 96.7) RobBERT v2 96.4 (96.0, 96.7) To evaluate the POS-performance, we used the Universal Dependencies (UD) version of the Lassy dataset (Van Noord et al., 2013), containing 17 different POS tags. We compared its perfor- mance with Frog, a popular memory-based Dutch POS tagging approach, and with other BERT mod- els.",
  "We compared its perfor- mance with Frog, a popular memory-based Dutch POS tagging approach, and with other BERT mod- els. Surprisingly, multilingual BERT marginally outperformed both Dutch BERT models, although not statistically signi\ufb01cantly, with both RobBERT models in second place with an almost equal ac- curacy. The higher performance of multilingual BERT could be indicative that it bene\ufb01ts from transferable language structures from other lan- guages helping it to perform well for POS tagging. Alternatively, this could signal a limit of the UD Lassy dataset, or at least for the performance of BERT-like models on this dataset. We also evaluated the models on several smaller subsets of the training data, to illustrate how much data is needed to achieve acceptable results. For all models, the same hyperparameters obtained for",
  "RobBERT v1 RobBERT v2 mBERT BERTje Accuracy 0 0,2 0,4 0,6 0,8 1,0 # of labeled sequences 102 103 104 Accuracy on POS tagging in function of training size Figure 1: POS tagging accuracy on the test set for dif- ferent sizes of training sets. Table 3 are used for all subsets, under the as- sumption that using a subset of the training data also works well under the same hyperparameters. The hyperparameters which yielded the results of RobBERT v2 are lr = 10\u22124, batch size of 16 and dropout of 0.1. The separate development set was used to select the best-performing model after each epoch based , which had a cross-entropy loss of 0.172 on the development set. While all BERT models perform similarly after seeing all instances of the UD Lassy dataset, there is a clear difference when using smaller training sets (Figure 1). Rob- BERT v2 outperforms all other models when using only 1,000 data points or less, again showing that it is more capable of dealing with smaller datasets.",
  "Rob- BERT v2 outperforms all other models when using only 1,000 data points or less, again showing that it is more capable of dealing with smaller datasets. 4.4 Named Entity Recognition Named entity recognition (NER) is the task of la- beling named entities in a sentence. It is thus a token-level task, just like POS-tagging, meaning we can use the same setup and hyperparameter tuning as described in Subsection 4.3. We use the CoNLL-2002 dataset and evaluation script2, which use a four value BIO labeling, namely for organisations, locations, people and miscel- laneous (Tjong Kim Sang, 2002). The hyper- parameters yielding the results for RobBERT v2 are lr = 3 \u00b7 10\u22125, batch size of 32 and dropout of 0.1. The separate development set was used to select the best-performing model after each epoch.",
  "The hyper- parameters yielding the results for RobBERT v2 are lr = 3 \u00b7 10\u22125, batch size of 32 and dropout of 0.1. The separate development set was used to select the best-performing model after each epoch. As the F1 score is generally used for evaluation of this task, we opted to use this met- ric instead of cross-entropy loss for selecting the best-performing model, which had an F1 score of 0.8769 on the development set. We compared the 2Retrieved from https://www.clips.uantwerp en.be/conll2002/ner/ F1 scores on the NER task in Table 4. Table 4: NER for various models, F1 score calculated with the CoNLL 2002 evaluation script, except for \u2020 which used the Seqeval Python library, * from Wu and Dredze (2019), ** from Brandsen et al. (2019), *** from de Vries et al. (2019).",
  "(2019), *** from de Vries et al. (2019). Task + model F1 score [%] Frog (Bosch et al., 2007) 57.31 mBERT (Devlin et al., 2019) 84.19 mBERT (Wu and Dredze, 2019) 90.94* BERT-NL (Brandsen et al., 2019) 89.7\u2020** BERTje (de Vries et al., 2019) 88.3*** RobBERT v1 87.53 RobBERT v2 89.08 We can see that (Wu and Dredze, 2019) outper- forms all other BERT models using a multilingual BERT model with an F1 score of 90.94. When we used the token labeling \ufb01ne-tuning regime de- scribed earlier on multilingual BERT, we were only able to get to an F1 score of 84.19 using mul- tilingual BERT, thus being outperformed by the Dutch BERT models.",
  "When we used the token labeling \ufb01ne-tuning regime de- scribed earlier on multilingual BERT, we were only able to get to an F1 score of 84.19 using mul- tilingual BERT, thus being outperformed by the Dutch BERT models. One possibility is that the authors used a more optimal \ufb01ne-tuning regime, or that they trained their model longer. 5 RobBERT and Fairness As language models are trained on large cor- pora, this poses a risk that minorities and pro- tected groups are ill-represented, e.g. by en- coding stereotypes (Bolukbasi et al., 2016; Zhao et al., 2019; Gonen and Goldberg, 2019). In word embeddings, these studies often rely on analogies (Bolukbasi et al., 2016; Caliskan et al., 2017) or embedding analysis (Gonen and Gold- berg, 2019). These approaches are not directly transferable to BERT models, since the sentence the word occurs in in\ufb02uences its embedding.",
  "These approaches are not directly transferable to BERT models, since the sentence the word occurs in in\ufb02uences its embedding. Efforts to generalize these approaches often rely on templates (May et al., 2019; Kurita et al., 2019). These can be intentionally neutral (\u201c<mask> is a word\u201d) or they might resemble an analogy in tex- tual form (\u201c<mask> is a zookeeper.\u201d). One can then perform an association test between possible values for the <mask> slot, similar to a word em- bedding association test (Caliskan et al., 2017). In this section, we discuss two distinct potential origins of representational harm (Blodgett et al., 2020) a language model could exhibit, and eval- uate these on RobBERT v2. The two discussed behaviours are (i) stereotyping of gender roles in",
  "waard landmeter sportman actrice zakenman huisvrouw boxer \ufb01lantroop editor baron militair non non kennis ballerina socialite butler schoolhoofd <mask> gaat naar een <T>. <mask> werkt als een <T>. \u21e0 She (\"Zij\")     |    He (\"Hij\") \u21e2   \u21e1 She (\"Zij\") ranked higher \u21e3 He (\"Hij\") ranked higher \u21e0 She (\"Zij\")     |    He (\"Hij\") \u21e2   \u21e0 She (\"Zij\")     |    He (\"Hij\") \u21e2   Ranking difference \u221240 \u221230 \u221220 \u221210 0 Associated gender \u22121,0 \u22120,5 0 0,5 1,0 Associated gender \u22121,0 \u22120,5 0 0,5 1,0 Associated gender \u22121,0 \u22120,5 0 0,5 1,0 <mask> is een <T>. Figure 2: Ranking difference between gendered pro- nouns for various professions. Three templates were used to evaluate, were <T> is replaced by a profes- sion.",
  "Figure 2: Ranking difference between gendered pro- nouns for various professions. Three templates were used to evaluate, were <T> is replaced by a profes- sion. In the leftmost template, the pronoun and profes- sion refer to different entities. occupations and (ii) unequal predictive power for texts written by men and women. These exempli\ufb01- cations highlight how language models risk affect- ing the experience of the end user, or replicating and reinforcing stereotypes. 5.1 Gender Stereotyping To assess how gender stereotypes of professions are present, we performed a template-based asso- ciation test similar to Kurita et al. (2019) and the semantically unbleached templates of May et al. (2019). We used RobBERT\u2019s LM head\u2014trained during pre-training with the MLM task\u2014to \ufb01ll in the <mask> slot for each template, in the same manner as the zero-shot task described in Sub- section 4.2. These templates have a second slot, which is used to iterate over the professions.",
  "These templates have a second slot, which is used to iterate over the professions. For this list of professions and the gender pro- jection on the he-she axis, we base us on the work by Bolukbasi et al. (2016), who crowdsourced the associated gender for various professions. Ideally, we would use a similarly crowdsourced Dutch dataset. However, since this does not yet exist, we opted for manually translating these English professions using the guidelines established by the European Parliament for gender neutral pro- fessions (Dimitrios Papadimoulis, 2018), meaning that we opted for the inclusive form for neutral professions in English that do not have a neutral counterpart, but an inclusive binary male variant and a female variant with explicit gender (e.g. for lawyer: using \u201cadvocaat\u201d and not \u201cadvocate\u201d). In the rare case that an inclusive or neutral form translated to an exclusive binary form, we ex- cluded this profession. We evaluated three templates on RobBERT, in- cluding one control template without co-referent entities (\u201c<mask> goes to a <T>\u201d) (Figure 2).",
  "In the rare case that an inclusive or neutral form translated to an exclusive binary form, we ex- cluded this profession. We evaluated three templates on RobBERT, in- cluding one control template without co-referent entities (\u201c<mask> goes to a <T>\u201d) (Figure 2). For the control template, there should be no and indeed is no correlation between ranking differ- ence for both pronouns and the associated gen- der of a profession. Interestingly, none of the in- stances has a positive ranking difference, meaning the language model always ranks the male pro- noun as more likely. When the profession and <mask> slot refer to the same entity, the general assessment of the lan- guage model correlates with the associated gender. But again, RobBERT estimates that the male pro- noun is more likely in almost all cases, even when these professions have a gendered suf\ufb01x. Curi- ously, actress (\u201cactrice\u201d) is the only word where this is not the case. Since RobBERT estimates the male pronoun to be more likely even in the control template, we suspect that the effect is due to more coverage of men in the training corpus.",
  "Curi- ously, actress (\u201cactrice\u201d) is the only word where this is not the case. Since RobBERT estimates the male pronoun to be more likely even in the control template, we suspect that the effect is due to more coverage of men in the training corpus. 5.2 Unequal Predictive Performance Unfairness is particularly problematic if it leads to unequal predictive performance. This prob- lem has been demonstrated for decision support systems, including recidivism prediction (Angwin et al., 2016) and public employment services (All- hutter et al., 2020). Such predictions can be down- stream tasks of language understanding; for exam- ple when job resums are processed (Van Hautte et al., 2020). To review fairness in downstream tasks, we evaluated the sentiment analysis task on DBRD, a dataset with scraped book reviews. Although this task in itself may have low impact for end users, it still serves as an illustrative example of how \ufb01ne- tuned models can behave unfairly.",
  "To review fairness in downstream tasks, we evaluated the sentiment analysis task on DBRD, a dataset with scraped book reviews. Although this task in itself may have low impact for end users, it still serves as an illustrative example of how \ufb01ne- tuned models can behave unfairly. To investigate whether such bias might result for our \ufb01ne-tuned model, we analyzed its out- come for different values of a sensitive attribute (in this case gender), as is commonly done in fair machine learning research (Zemel et al., 2013; Hardt et al., 2016; Delobelle et al., 2020). To this end, we augmented the held-out test set of DBRD with gender as a sensitive attribute for each re- view3. Values were obtained from the reviews\u2019 author pro\ufb01les with a self-reported binary gender (\u2018man\u2019 or \u2018vrouw\u2019) (64%). The remaining 36% of reviews did not report author gender, and they were discarded for this evaluation.",
  "Values were obtained from the reviews\u2019 author pro\ufb01les with a self-reported binary gender (\u2018man\u2019 or \u2018vrouw\u2019) (64%). The remaining 36% of reviews did not report author gender, and they were discarded for this evaluation. Of the remain- 3We make this augmentation of DBRD available under CC-by-NC-SA at https://people.cs.kuleuven.b e/\u02dcpieter.delobelle/data.html.",
  "0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate 0.0 0.2 0.4 0.6 0.8 1.0 True Positive Rate Positive (> 3 stars) Male (AUC = 0.98) Female (AUC = 0.98) Threshold (male) Threshold (female) 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate 0.0 0.2 0.4 0.6 0.8 1.0 True Positive Rate Highly positive (5 stars) Male (AUC = 0.76) Female (AUC = 0.83) Threshold (male) Threshold (female) Figure 3: ROC of the \ufb01ne-tuned model to predict posi- tive reviews for male and female reviewers ing, gender-labelled, reviews, 76% were written by women. Thus, the dataset is unbalanced.",
  "Thus, the dataset is unbalanced. We quantify the gender difference with two metrics: (i) Demographic Parity Ratio (DPR), which expresses a relative difference between pre- dicted outcomes \u02c6y conditioned on the sensitive at- tribute a (Dwork et al., 2012), following P(\u02c6y | \u00aca) P(\u02c6y | a) , and (ii) Equal Opportunity (EO) Hardt et al. (2016), which in addition also conditions on the true outcome y, as a task-speci\ufb01c fairness measure (Dwork et al., 2012), following P(\u02c6y | \u00aca, y) \u2212P(\u02c6y | a, y). Hardt et al. (2016) also relate EO to the ROC curves to evaluate fairness when dealing with a bi- nary predictor and a score function. To derive a binary predictor, we used 0 as a threshold value. Figure 3 shows the single resulting predictor, with the ROC curves split on the sensitive attribute, for each of the two rating levels (over 3 resp. 5 stars).",
  "To derive a binary predictor, we used 0 as a threshold value. Figure 3 shows the single resulting predictor, with the ROC curves split on the sensitive attribute, for each of the two rating levels (over 3 resp. 5 stars). The results of Figure 3 show that there is small difference in opportunity, which is especially pro- nounced for the highly positive classi\ufb01er. For pos- itive reviews, the EO difference is 0.028 at the in- dicated threshold and DPR is 70.2%. The DPR would indicate an unfairness, as values below 80% are often considered unfair. However, this metric has received some criticism, and when including the true outcome in EO, the difference in probabil- ities is close to 0, which does not signal any unfair- ness. When taking into account the ROC curves (Figure 3), the EO score can be explained by the good predictive performance. When considering highly positive reviews, however, the differences become more pronounced and the model has bet- ter predictive performance for reviews written by women.",
  "When taking into account the ROC curves (Figure 3), the EO score can be explained by the good predictive performance. When considering highly positive reviews, however, the differences become more pronounced and the model has bet- ter predictive performance for reviews written by women. 6 Code The training and evaluation code of this paper as well as the RobBERT model and the \ufb01ne-tuned models are publicly available for download at ht tps://github.com/iPieter/RobBERT. 7 Limitations and Future Work There are several potential improvements for cre- ating a better pre-trained RobBERT-like model. First, since BERT-based models are still being ac- tively researched, one could potentially improve the training regime using new unsupervised pre- training tasks when they are discovered, e.g. sen- tence order prediction (Lan et al., 2019). Sec- ond, while RobBERT is trained on lines that con- tain multiple sentences, it does not put subsequent lines of the corpus after each other due to the shuf- \ufb02ed nature of the OSCAR corpus (Ortiz Su\u00b4arez et al., 2019).",
  "Sec- ond, while RobBERT is trained on lines that con- tain multiple sentences, it does not put subsequent lines of the corpus after each other due to the shuf- \ufb02ed nature of the OSCAR corpus (Ortiz Su\u00b4arez et al., 2019). This is unlike RoBERTa, which does put full sentences next to one another if they do not exceed the available sequence length, in or- der to learn the long-range dependencies between words that the original BERT learned using its controversial NSP task. Creating an unshuf\ufb02ed version of OSCAR might thus further improve the performance of the pre-trained model. Third, there might be some bene\ufb01t to modifying the to- kenizer to use morpheme-based tokens, as Dutch uses compound words. Fourth, one could improve model\u2019s fairness during pre-training. We illus- trated how representational harm in downstream tasks can affect the end user\u2019s experience, like the unequal predictive performance for the DBRD task. Various methods have been proposed to mit- igate unfair behaviour in AI models (Zemel et al., 2013; Delobelle et al., 2020).",
  "Various methods have been proposed to mit- igate unfair behaviour in AI models (Zemel et al., 2013; Delobelle et al., 2020). While we refrained from training fair pre-trained and \ufb01ne-tuned mod- els in this research, training such models could be an interesting contribution. In addition, with the increased attention on fairness in machine learn- ing, a broader view of the impact on other pro- tected groups due to large pre-trained language models is also called-for. The RobBERT model itself can be used in new settings to help future research. First, RobBERT could be used in a model that uses a BERT-like transformer stack for the encoder and a genera- tive model as a decoder (Raffel et al., 2019; Lewis",
  "et al., 2019) Second, RobBERT can serve as the basis for a large number of Dutch language tasks that we did not examine in this paper. Given Rob- BERT\u2019s state-of-the-art performance on small as well as on large datasets, it could help advance re- sults when \ufb01ne-tuned on new datasets. 8 Conclusion We introduced a new language model for Dutch based on RoBERTa, called RobBERT, and showed that it outperforms earlier approaches as well as other BERT-based language models for a several different Dutch language tasks. More speci\ufb01- cally, we found that RobBERT signi\ufb01cantly out- performed other BERT-like models when dealing with smaller datasets, making it a useful resource for a large range of application domains. We ex- pect this model to serve as a base for \ufb01ne-tuning on other tasks, and thus help foster new models that can advance results for Dutch language tasks. Acknowledgements Pieter Delobelle was supported by the Research Foundation - Flanders under EOS No.",
  "We ex- pect this model to serve as a base for \ufb01ne-tuning on other tasks, and thus help foster new models that can advance results for Dutch language tasks. Acknowledgements Pieter Delobelle was supported by the Research Foundation - Flanders under EOS No. 30992574 and received funding from the Flemish Gov- ernment under the Onderzoeksprogramma Arti- \ufb01cile Intelligentie (AI) Vlaanderen programme. Thomas Winters is a fellow of the Research Foundation-Flanders (FWO-Vlaanderen). Most computational resources and services used in this work were provided by the VSC (Flemish Super- computer Center), funded by the Research Foun- dation - Flanders (FWO) and the Flemish Govern- ment department EWI. We are especially grate- ful to Luc De Raedt for his guidance as well as for providing the facilities to complete this project. We are thankful to Liesbeth Allein and her super- visors for inspiring us to use the die/dat task. We are also grateful to Ott et al. (2019); Paszke et al.",
  "We are thankful to Liesbeth Allein and her super- visors for inspiring us to use the die/dat task. We are also grateful to Ott et al. (2019); Paszke et al. (2019); Haghighi et al. (2018); Wolf et al. (2019) for their software packages. References Liesbeth Allein, Artuur Leeuwenberg, and Marie- Francine Moens. 2020. Binary and Multitask Clas- si\ufb01cation Model for Dutch Anaphora Resolution: Die/Dat Prediction. arXiv:2001.02943 [cs]. Doris Allhutter, Florian Cech, Fabian Fischer, Gabriel Grill, and Astrid Mager. 2020. Algorithmic Pro\ufb01l- ing of Job Seekers in Austria: How Austerity Poli- tics Are Made Effective. Front. Big Data, 3:5. Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. Machine bias.",
  "Front. Big Data, 3:5. Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. Machine bias. Su Lin Blodgett, Solon Barocas, Hal Daum\u00b4e III, and Hanna Wallach. 2020. Language (Technology) is Power: A Critical Survey of \u201dBias\u201d in NLP. arXiv:2005.14050 [cs]. Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In Ad- vances in Neural Information Processing Systems, pages 4349\u20134357. Antal van den Bosch, Bertjan Busser, Sander Canisius, and Walter Daelemans. 2007. An ef\ufb01cient memory- based morphosyntactic tagger and parser for dutch. LOT Occasional Series, 7:191\u2013206.",
  "2007. An ef\ufb01cient memory- based morphosyntactic tagger and parser for dutch. LOT Occasional Series, 7:191\u2013206. Alex Brandsen, Anne Dirkson, Suzan Verberne, Maya Sappelli, Dung Manh Chu, and Kimberly Stoutjes- dijk. 2019. BERT-NL a set of language models pre- trained on the Dutch SoNaR corpus. Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. 2017. Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334):183\u2013186. Wietse de Vries, Andreas van Cranenburgh, Arianna Bisazza, Tommaso Caselli, Gertjan van Noord, and Malvina Nissim. 2019. BERTje: A Dutch BERT Model. arXiv:1912.09582 [cs]. Pieter Delobelle, Paul Temple, Gilles Perrouin, Beno\u02c6\u0131t Fr\u00b4enay, Patrick Heymans, and Bettina Berendt.",
  "2019. BERTje: A Dutch BERT Model. arXiv:1912.09582 [cs]. Pieter Delobelle, Paul Temple, Gilles Perrouin, Beno\u02c6\u0131t Fr\u00b4enay, Patrick Heymans, and Bettina Berendt. 2020. Ethical Adversaries: Towards Mitigat- ing Unfairness with Adversarial Machine Learning. arXiv:2005.06852 [cs, stat]. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics. Dimitrios Papadimoulis. 2018. Genderneutraal taalge- bruik in het Europees Parlement. Technical report, European Parlement.",
  "Associ- ation for Computational Linguistics. Dimitrios Papadimoulis. 2018. Genderneutraal taalge- bruik in het Europees Parlement. Technical report, European Parlement. Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. 2012. Fairness Through Awareness. In 3rd Innovations in Theoret- ical Computer Science Conference, pages 214\u2013226. ACM. Hila Gonen and Yoav Goldberg. 2019. Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them. In Proceedings of the 2019 Conference of the North",
  "American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 609\u2013614, Minneapolis, Minnesota. Association for Computa- tional Linguistics. Sepand Haghighi, Masoomeh Jasemi, Shaahin Hess- abi, and Alireza Zolanvari. 2018. PyCM: Multiclass confusion matrix library in Python. Journal of Open Source Software, 3(25):729. Moritz Hardt, Eric Price, ecprice, and Nati Srebro. 2016. Equality of Opportunity in Supervised Learn- ing. In NIPS, pages 3315\u20133323. Curran Associates. Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. 2019a. Spanbert: Improving pre-training by representing and predicting spans. Mandar Joshi, Omer Levy, Daniel S Weld, and Luke Zettlemoyer. 2019b. Bert for coreference reso- lution: Baselines and analysis.",
  "2019a. Spanbert: Improving pre-training by representing and predicting spans. Mandar Joshi, Omer Levy, Daniel S Weld, and Luke Zettlemoyer. 2019b. Bert for coreference reso- lution: Baselines and analysis. arXiv preprint arXiv:1908.09091. Diederik P. Kingma and Jimmy Ba. 2017. Adam: A Method for Stochastic Optimization. arXiv:1412.6980 [cs]. Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit, vol- ume 5, pages 79\u201386. [object Object]. Keita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black, and Yulia Tsvetkov. 2019. Measuring bias in contextualized word representations. In Proceed- ings of the First Workshop on Gender Bias in Natu- ral Language Processing, pages 166\u2013172, Florence, Italy. Association for Computational Linguistics.",
  "2019. Measuring bias in contextualized word representations. In Proceed- ings of the First Workshop on Gender Bias in Natu- ral Language Processing, pages 166\u2013172, Florence, Italy. Association for Computational Linguistics. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learn- ing of language representations. arXiv preprint arXiv:1909.11942. Hector Levesque, Ernest Davis, and Leora Morgen- stern. 2012. The winograd schema challenge. In Thirteenth International Conference on the Princi- ples of Knowledge Representation and Reasoning. Mike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.",
  "2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretrain- ing Approach. arXiv:1907.11692 [cs]. Louis Martin, Benjamin Muller, Pedro Javier Ortiz Su\u00b4arez, Yoann Dupont, Laurent Romary, \u00b4Eric Ville- monte de la Clergerie, Djam\u00b4e Seddah, and Beno\u02c6\u0131t Sagot. 2019. CamemBERT: A Tasty French Lan- guage Model. arXiv:1911.03894 [cs]. Chandler May, Alex Wang, Shikha Bordia, Samuel R. Bowman, and Rachel Rudinger. 2019. On Measuring Social Biases in Sentence Encoders. arXiv:1903.10561 [cs].",
  "Chandler May, Alex Wang, Shikha Bordia, Samuel R. Bowman, and Rachel Rudinger. 2019. On Measuring Social Biases in Sentence Encoders. arXiv:1903.10561 [cs]. Tomas Mikolov, Kai Chen, Greg Corrado, and Jef- frey Dean. 2013. Ef\ufb01cient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. Nelleke Oostdijk, Martin Reynaert, V\u00b4eronique Hoste, and Ineke Schuurman. 2013a. The construction of a 500-million-word reference corpus of contemporary written dutch. In Essential speech and language technology for Dutch, pages 219\u2013247. Springer. Nelleke Oostdijk, Martin Reynaert, V\u00b4eronique Hoste, and Ineke Schuurman. 2013b. The construction of a 500-million-word reference corpus of contemporary written Dutch. In Essential Speech and Language Technology for Dutch: Results by the STEVIN- Programme, chapter 13.",
  "2013b. The construction of a 500-million-word reference corpus of contemporary written Dutch. In Essential Speech and Language Technology for Dutch: Results by the STEVIN- Programme, chapter 13. Springer Verlag. Pedro Javier Ortiz Su\u00b4arez, Beno\u02c6\u0131t Sagot, and Laurent Romary. 2019. Asynchronous Pipeline for Process- ing Huge Corpora on Medium to Low Resource In- frastructures. In 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7), Cardiff, United Kingdom. Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. Fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019: Demonstrations. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019.",
  "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. PyTorch: An imperative style, high-performance deep learning library. In Ad- vances in Neural Information Processing Systems, pages 8024\u20138035. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, Doha, Qatar. Association for Computational Linguistics. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations.",
  "Association for Computational Linguistics. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. In Proceedings of the 2018 Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long Papers), pages 2227\u20132237, New Orleans, Louisiana. Association for Computational Linguistics.",
  "Telmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual bert? arXiv preprint arXiv:1906.01502. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai- assets/researchcovers/languageunsupervised/language understanding paper. pdf. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog, 1(8). Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the Lim- its of Transfer Learning with a Uni\ufb01ed Text-to-Text Transformer.",
  "2019. Exploring the Lim- its of Transfer Learning with a Uni\ufb01ed Text-to-Text Transformer. arXiv:1910.10683 [cs, stat]. Samuel R\u00a8onnqvist, Jenna Kanerva, Tapio Salakoski, and Filip Ginter. 2019. Is multilingual BERT \ufb02u- ent in language generation? In Proceedings of the First NLPL Workshop on Deep Learning for Natural Language Processing, pages 29\u201336, Turku, Finland. Link\u00a8oping University Electronic Press. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from over\ufb01tting. The journal of machine learning research, 15(1):1929\u20131958. Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. 2019. How to \ufb01ne-tune bert for text classi\ufb01cation?",
  "The journal of machine learning research, 15(1):1929\u20131958. Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. 2019. How to \ufb01ne-tune bert for text classi\ufb01cation? In China National Conference on Chinese Compu- tational Linguistics, pages 194\u2013206. Springer. Erik F. Tjong Kim Sang. 2002. Introduction to the conll-2002 shared task: Language-independent named entity recognition. In Proceedings of the 6th Conference on Natural Language Learning - Vol- ume 20, COLING-02, page 14, USA. Association for Computational Linguistics. Benjamin van der Burgh and Suzan Verberne. 2019. The merits of Universal Language Model Fine- tuning for Small Datasets \u2013 a case with Dutch book reviews. arXiv:1910.00896 [cs]. Jeroen Van Hautte, Vincent Schelstraete, and Mika\u00a8el Wornoo. 2020. Leveraging the inherent hierarchy of vacancy titles for automated job ontology expansion.",
  "arXiv:1910.00896 [cs]. Jeroen Van Hautte, Vincent Schelstraete, and Mika\u00a8el Wornoo. 2020. Leveraging the inherent hierarchy of vacancy titles for automated job ontology expansion. In Proceedings of the 6th International Workshop on Computational Terminology, pages 37\u201342, Mar- seille, France. European Language Resources Asso- ciation. Gertjan Van Noord, Gosse Bouma, Frank Van Eynde, Daniel De Kok, Jelmer Van der Linde, Ineke Schuur- man, Erik Tjong Kim Sang, and Vincent Vandeghin- ste. 2013. Large scale syntactic annotation of writ- ten Dutch: Lassy, pages 147\u2013164. Springer. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need.",
  "Springer. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar- nett, editors, Advances in Neural Information Pro- cessing Systems 30, pages 5998\u20136008. Curran As- sociates, Inc. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, R\u2019emi Louf, Morgan Funtow- icz, and Jamie Brew. 2019. HuggingFace\u2019s trans- formers: State-of-the-art natural language process- ing. ArXiv, abs/1910.03771. Shijie Wu and Mark Dredze. 2019.",
  "2019. HuggingFace\u2019s trans- formers: State-of-the-art natural language process- ing. ArXiv, abs/1910.03771. Shijie Wu and Mark Dredze. 2019. Beto, Bentz, Be- cas: The Surprising Cross-Lingual Effectiveness of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 833\u2013844, Hong Kong, China. Association for Com- putational Linguistics. Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. 2013. Learning fair represen- tations. In International Conference on Machine Learning, pages 325\u2013333. Jieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cot- terell, Vicente Ordonez, and Kai-Wei Chang. 2019. Gender Bias in Contextualized Word Embeddings. arXiv:1904.03310 [cs]."
]