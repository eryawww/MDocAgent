{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Extractive Summarization of EHR Discharge Notes Emily Alsentzer emilya@mit.edu Anne Kim anneykim@mit.edu Abstract Patient summarization is essential for clinicians to provide coordinated care and practice e\ufb00ec- tive communication. Automated summariza- tion has the potential to save time, standard- ize notes, aid clinical decision making, and re- duce medical errors. Here we provide an up- per bound on extractive summarization of dis- charge notes and develop an LSTM model to sequentially label topics of history of present illness notes. We achieve an F1 score of 0.876, which indicates that this model can be em- ployed to create a dataset for evaluation of ex- tractive summarization methods. 1. Introduction Summarization of patient information is essential to the practice of medicine. Clinicians must synthesize infor- mation from diverse data sources to communicate with colleagues and provide coordinated care.",
      "1. Introduction Summarization of patient information is essential to the practice of medicine. Clinicians must synthesize infor- mation from diverse data sources to communicate with colleagues and provide coordinated care. Examples of clinical summarization are abundant in practice; pa- tient hando\ufb00summaries facilitate provider shift change, progress notes provide a daily status update for a pa- tient, oral case presentations enable transfer of infor- mation from overnight admission to the care team and attending, and discharge summaries provide informa- tion about a patient\u2019s hospital visit to their primary care physician and other outpatient providers (Feblowitz et al., 2011). Informal, unstructured, or poor quality summaries can lead to communication failures and even medical errors, yet clinical instruction on how to formulate clinical sum- maries is ad hoc and informal. Non-existent or limited search functionality, fragmented data sources, and lim- ited visualizations in electronic health records (EHRs) make summarization challenging for providers (Chris- tensen & Grimsmo, 2008; Singh et al., 2013; Natarajan et al., 2010).",
      "Furthermore, while dictation of EHR notes Submitted for MIT\u2019s 6.872 class project, Copyright 2017 by the authors. allows clinicians to more e\ufb03ciently document informa- tion at the point of care, the stream of consciousness- like writing can hinder the readability of notes. Kri- palani et al. show that discharge summaries are often lacking key information, including treatment progression and follow-up protocols, which can hinder communica- tion between hospital and community based clinicians (Kripalani et al., 2007). Recently, St. Thomas Hospi- tal in Nashville, TN stipulated that discharge notes be written within 48 hours of discharge following incidences where improper care was given to readmitted patients because the discharge summary for the previous admis- sion was not completed(Alsentzer). Automated summary generation has the potential to save clinician time, avoid medical errors, and aid clin- ical decision making. By organizing and synthesizing a patient\u2019s salient medical history, algorithms for pa- tient summarization can enable better communication and care, particularly for chronically ill patients, whose medical records often contain hundreds of notes.",
      "By organizing and synthesizing a patient\u2019s salient medical history, algorithms for pa- tient summarization can enable better communication and care, particularly for chronically ill patients, whose medical records often contain hundreds of notes. In this work, we explore the automatic summarization of dis- charge summary notes, which are critical to ensuring continuity of care after hospitalization. We (1) provide an upper bound on extractive summarization by assess- ing how much information in the discharge note can be found in the rest of the patient\u2019s EHR notes and (2) develop a classi\ufb01er for labeling the topics of history of present illness notes, a narrative section in the discharge summary that describes the patient\u2019s prior history and current symptoms. Such a classi\ufb01er can be used to create topic speci\ufb01c evaluation sets for methods that perform extractive summarization. These aims are critical steps in ultimately developing methods that can automate dis- charge summary creation. 2. Related Work In the broader \ufb01eld of summarization, automization was meant to standardize output while also saving time and e\ufb00ort.",
      "These aims are critical steps in ultimately developing methods that can automate dis- charge summary creation. 2. Related Work In the broader \ufb01eld of summarization, automization was meant to standardize output while also saving time and e\ufb00ort. Pioneering strategies in summarization started by extracting \u201dsigni\ufb01cant\u201d sentences in the whole corpus to build an abstract where \u201dsigni\ufb01cant\u201d sentences were de\ufb01ned by the number of frequently occurring words (Luhn, 1958). These initial methods did not consider arXiv:1810.12085v1  [cs.IR]  26 Oct 2018",
      "Extractive Summarization of Electronic Health Record Discharge Notes word meaning or syntax at either the sentence or para- graph level, which made them crude at best. More ad- vanced extractive heuristics like topic modeling (Allah- yari & Kochut, 2016), cue word dictionary approaches (Edmundson, 1969), and title methods (Ferreira et al., 2013) for scoring content in a sentence followed soon after. For example, topic modeling extends initial fre- quency methods by assigning topics scores by frequency of topic signatures, clustering sentences with similar top- ics, and \ufb01nally extracting the centroid sentence, which is considered the most representative sentence (Allahyari et al., 2017). Recently, abstractive summarization ap- proaches using sequence-to-sequence methods have been developed to generate new text that synthesizes origi- nal text(Paulus et al., 2017; Nallapati et al., 2016; Liu & Pan, 2016; Rush et al., 2015); however, the \ufb01eld of abstractive summarization is quite young.",
      "Existing approaches within the \ufb01eld of electronic health record summarization have largely been extractive and indicative, meaning that summaries point to important pieces in the original text rather than replacing the orig- inal text altogether. Few approaches have been deployed in practice, and even fewer have demonstrated impact on quality of care and outcomes (Pivovarov, 2016). Summa- rization strategies have ranged from extraction of rele- vant sentences from the original text to form the sum- mary (Moen et al., 2016), topic modeling of EHR notes using Latent Dirichlet allocation (LDA) or bayesian net- works (Pivovarov, 2016), and knowledge based heuristic systems (Goldstein & Shahar, 2016). To our knowledge, there is no literature to date on extractive or abstractive EHR summarization using neural networks. 3. Methods 3.1. Data MIMIC-III is a freely available, deidenti\ufb01ed database containing electronic health records of patients admitted to an Intensive Care Unit (ICU) at Beth Israel Deaconess Medical Center between 2001 and 2012.",
      "3. Methods 3.1. Data MIMIC-III is a freely available, deidenti\ufb01ed database containing electronic health records of patients admitted to an Intensive Care Unit (ICU) at Beth Israel Deaconess Medical Center between 2001 and 2012. The database contains all of the notes associated with each patient\u2019s time spent in the ICU as well as 55,177 discharge reports and 4,475 discharge addendums for 41,127 distinct pa- tients. Only the original discharge reports were included in our analyses. Each discharge summary was divided into sections (Date of Birth, Sex, Chief Complaint, Ma- jor Surgical or Invasive Procedure, History of Present Illness, etc.) using a regular expression. 3.2. Upper Bound on Summarization Extractive summarization of discharge summaries relies on the assumption that the information in the discharge summary is documented elsewhere in the rest of the pa- tient\u2019s notes. However, sometimes clinicians will docu- ment information in the discharge summary that may have been discussed throughout the hospital visit, but was never documented in the EHR.",
      "However, sometimes clinicians will docu- ment information in the discharge summary that may have been discussed throughout the hospital visit, but was never documented in the EHR. Thus, our \ufb01rst aim was to determine the upper bound of extractive summa- rization. For each patient, we compared the text of the discharge summary to the remaining notes for the patient\u2019s current admission as well as their entire medical record. Con- cept Unique Identi\ufb01ers (CUIs) from the Uni\ufb01ed Med- ical Language System (UMLS) were compared in or- der to assess whether clinically relevant concepts in the discharge summary could be located in the remaining notes (NLM, 2009). CUIs were extracted using Apache cTAKES (Savova et al., 2010) and \ufb01ltered by removing the CUIs that are already subsumed by a longer span- ning CUI. For example, CUIs for \u201dhead\u201d and \u201dache\u201d were removed if a CUI existed for \u201dhead ache\u201d in order to ex- tract the most clinically relevant CUIs.",
      "For example, CUIs for \u201dhead\u201d and \u201dache\u201d were removed if a CUI existed for \u201dhead ache\u201d in order to ex- tract the most clinically relevant CUIs. In order to understand which sections of the discharge summaries would be the easiest or most di\ufb03cult to sum- marize, we performed the same CUI overlap comparison for the chief complaint, major surgical or invasive proce- dure, discharge medication, and history of present illness sections of the discharge note separately. We calculated which fraction of the CUIs in each section were located in the rest of the patient\u2019s note for a speci\ufb01c hospital stay. We also calculated what percent of the genders recorded in the discharge summary were also recorded in the structured data for the patient. 3.3. Labeling History of Present Illness Notes 3.3.1. Annotation We developed a classi\ufb01er to label topics in the history of present illness (HPI) notes, including demographics, di- agnosis history, and symptoms/signs, among others.",
      "3.3. Labeling History of Present Illness Notes 3.3.1. Annotation We developed a classi\ufb01er to label topics in the history of present illness (HPI) notes, including demographics, di- agnosis history, and symptoms/signs, among others. A random sample of 515 history of present illness notes was taken, and each of the notes was manually annotated by one of eight annotators using the software Multi- document Annotation Environment (MAE) (Rim, 2016). MAE provides an interactive GUI for annotators and ex- ports the results of each annotation as an XML \ufb01le with text spans and their associated labels for additional pro- cessing. 40% of the HPI notes were labeled by clinicians and 60% by non-clinicians. Table 1 shows the instruc- tions given to the annotators for each of the 10 labels. The entire HPI note was labeled with one of the labels, and instructions were given to label each clause in a sen- tence with the same label when possible.",
      "Extractive Summarization of Electronic Health Record Discharge Notes Table 1. HPI Categories and Annotation Instructions Demographics Age, gender, race/ethnicity, language, social history etc. DiagnosisHistory Diagnoses prior to current symptoms MedicationHistory Medications or treatment prior to current symptoms ProcedureHistory Procedures prior to current symptoms Symptoms/Signs Current Signs/Symptoms; chief complaint Vitals/Labs Any vitals or labs related to current care episode Procedures/Results Procedures including CT, CXR, etc. and their results Meds/Treatments Medication or treatment ad- ministered in current care episode Movement Description of patient admit- ted/transferred between hos- pitals or care units Other Does not \ufb01t into any other category 3.3.2. Model Our LSTM model was adopted from prior work by Der- noncourt et al (Dernoncourt et al., 2016). Whereas the Dernoncourt model jointly classi\ufb01ed each sentence in a medical abstract, here we jointly classify each word in the HPI summary.",
      "Model Our LSTM model was adopted from prior work by Der- noncourt et al (Dernoncourt et al., 2016). Whereas the Dernoncourt model jointly classi\ufb01ed each sentence in a medical abstract, here we jointly classify each word in the HPI summary. Our model consists of four layers: a token embedding layer, a word contextual represen- tation layer, a label scoring layer, and a label sequence optimization layer (Figure 1). In the following descriptions, lowercase italics is used to denote scalars, lowercase bold is used to denote vectors, and uppercase italics is used to denote matrices. Token Embedding Layer: In the token embedding layer, pretrained word embeddings are combined with learned character embeddings to create a hybrid token embedding for each word in the HPI note. The word embeddings, which are direct mappings from word xi to vector t, were pretrained using word2vec (Mikolov et al., 2013a;b;c) on all of the notes in MIMIC (v30) and only the discharge notes. Both the continuous bag of words (CBOW) and skip gram models were explored.",
      "Both the continuous bag of words (CBOW) and skip gram models were explored. Let z1:l be the sequence of characters comprising the word xi. Each character is mapped to its embedding ci, and all embeddings are input into a bidirectional LSTM, which ultimately outputs c, the character embedding of the word xi. The output of the token embedding layer is the vector e, which is the result of concatenation of the word em- bedding, t, and the character embedding, c. Contextual Representation Layer: The contextual representation layer takes as input the sequence of word embeddings, e1:k, and outputs an embedding of the contextual representation for each word in the HPI note. The word embeddings are fed into a bi-directional LSTM, which outputs hj, a concatenation of the hidden states of the two LSTMs for each word. Label Scoring Layer: At this point, each word xi is associated with a hidden representation of the word, hj. In the label scoring layer, we use a fully connected neural network with one hidden layer to output a score associ- ated with each of the 10 categories for each word. Let W \u2208R10Xk and b \u2208R10.",
      "In the label scoring layer, we use a fully connected neural network with one hidden layer to output a score associ- ated with each of the 10 categories for each word. Let W \u2208R10Xk and b \u2208R10. We can compute a vector of scores s = W \u00b7 h + b where the ith component of s is the score of class i for a given word. Label Sequence Optimization Layer: The Label Se- quence Optimization Layer computes the probability of a labeling sequence and \ufb01nds the sequence with the high- est probability.",
      "Label Sequence Optimization Layer: The Label Se- quence Optimization Layer computes the probability of a labeling sequence and \ufb01nds the sequence with the high- est probability. In order to condition the label for each word on the labels of its neighbors, we employ a linear chain conditional random \ufb01eld (CRF) to de\ufb01ne a global score, Q, for a sequence of words and their associated scores s1, ..., sm and labels, y1, ..., yn: Q(y1, ..., yn) = b[y1] + m X t=1 st[yt] + m\u22121 X t=1 T[yt, yt+1] + \u03f5[ym] (1) where T is a transition matrix \u2208R10X10 and b, \u03f5 \u2208R10 are vectors of scores that describe the cost of beginning or ending with a label.",
      "The probability of a sequence of labels is calculated by applying a softmax layer to obtain a probability of a sequence of labels: P(y1, ..., yn) = eQ(y1,...,yn) P y1,...,yn eQ(y1,...,yn) (2) Cross-entropy loss, \u2212log(P(\u02c6y)), is used as the objective function where \u02c6y is the correct sequence of labels and the probability P(\u02c6y) is calculated according to the CRF. 3.3.3. Running the Model We evaluated our model on the 515 annotated history of present illness notes, which were split in a 70% train set, 15% development set, and a 15% test set. The model is trained using the Adam algorithm for gradient-based optimization (Kingma & Ba, 2014) with an initial learn- ing rate = 0.001 and decay = 0.9. A dropout rate of",
      "Extractive Summarization of Electronic Health Record Discharge Notes Figure 1. LSTM Model for sequential HPI word classi\ufb01ca- tion. Xi: token i, Z1 to Zl: characters of Xi, t: word em- beddings, c: character embeddings, e1:k: hybrid token em- beddings, h1:j: contextual representation embedings, s1:m : score vectors where s[i] is the score for category i, y1:m: output word labels. 0.5 was applied for regularization, and each batch size = 20. The model ran for 20 epochs and was halted early if there was no improvement after 3 epochs. We evaluated the impact of character embeddings, the choice of pretrained w2v embeddings, and the addition of learned word embeddings on model performance on the dev set. We report performance of the best performing model on the test set. 4. Results and Discussion Figure 2. Relationship between the number of non-discharge notes (top), number of non-discharge CUIs (middle), or the number of hours the patient spent outside the ICU (bottom) and the recall for each discharge summary. 4.1.",
      "4. Results and Discussion Figure 2. Relationship between the number of non-discharge notes (top), number of non-discharge CUIs (middle), or the number of hours the patient spent outside the ICU (bottom) and the recall for each discharge summary. 4.1. Upper Bound on Summarization For each of the 55,177 discharge summary reports in the MIMIC database, we calculated what fraction of the CUIs in the discharge summary could be found in the remaining notes for the patient\u2019s current admission",
      "Extractive Summarization of Electronic Health Record Discharge Notes (by hadm id) and in their entire longitudinal medical record (by subject id). Table 2 shows the CUI recall aver- aged across all discharge summaries by both subject id and hadm id. The low recall suggests that clinicians may incorporate information in the discharge note that had not been previously documented in the EHR. Fig- ure 2 plots the relationship between the number of non- discharge notes for each patient and the CUI recall (top) and the number of total CUIs in non-discharge notes and the CUI recall (middle). The number of CUIs is a proxy for the length of the notes, and as expected, the CUI re- call tends to be higher in patients with more and longer notes. The bottom panel in Figure 2 demonstrates that recall is not correlated with the patient\u2019s length of stay outside the ICU, which indicates that our upper bound calculation is not severely impacted by only having ac- cess to the patient\u2019s notes from their stay in the ICU.",
      "The bottom panel in Figure 2 demonstrates that recall is not correlated with the patient\u2019s length of stay outside the ICU, which indicates that our upper bound calculation is not severely impacted by only having ac- cess to the patient\u2019s notes from their stay in the ICU. Finally, Table 3 shows the recall for the sex, chief com- plaint, procedure, discharge medication, and HPI dis- charge summary sections averaged across all the dis- charge summaries. The procedure section has the high- est recall of 0.807, which is understandable because pro- cedures undergone during an inpatient stay are most likely to be documented in an EHR. The recall for each of these \ufb01ve sections is much higher than the overall re- call in Table 2, suggesting that extractive summarization may be easier for some sections of the discharge note. Overall, this upper bound analysis suggests that we may not be able to recreate a discharge summary with extrac- tive summarization alone.",
      "Overall, this upper bound analysis suggests that we may not be able to recreate a discharge summary with extrac- tive summarization alone. While CUI comparison allows for comparing medically relevant concepts, cTAKES\u2019s CUI labelling process is not perfect, and further work, perhaps through sophisticated regular expressions, is needed to de\ufb01ne the limits of extractive summarization. Table 2. Discharge CUI recall for each patient\u2019s current hospital admission (by hadm id) and their entire EHR (by subject id), averaged across all discharge summaries Comparison Average Recall By subject id 0.431 By hadm id 0.375 4.2. Labeling History of Present Illness Notes Table 4 compares dev set performance of the model us- ing various pretrained word embeddings, with and with- out character embeddings, and with pretrained versus learned word embeddings. The \ufb01rst row in each sec- tion is the performance of the model architecture de- scribed in the methods section for comparison. Models using word embeddings trained on the discharge sum- maries performed better than word embeddings trained Table 3. Average Recall for \ufb01ve sections of the discharge sum- mary.",
      "Models using word embeddings trained on the discharge sum- maries performed better than word embeddings trained Table 3. Average Recall for \ufb01ve sections of the discharge sum- mary. Recall for each patient\u2019s sex was calculated by exam- ining the structured data for the patient\u2019s current admission, and recall for the remaining sections was calculated by com- paring CUI overlap between the section and the remaining notes for the current admission. Section Average Recall Sex 0.675 Chief Complaint 0.720 Procedure 0.807 Discharge Medication 0.580 HPI 0.665 on all MIMIC notes, likely because the discharge sum- mary word embeddings better captured word use in dis- charge summaries alone. Interestingly, the continuous bag of words embeddings outperformed skip gram em- beddings, which is surprising because the skip gram ar- chitecture typically works better for infrequent words (Google, 2013). As expected, inclusion of character em- beddings increases performance by approximately 3%. The model with word embeddings learned in the model achieves the highest performance on the dev set (0.886), which may be because the pretrained worm embeddings were trained on a previous version of MIMIC.",
      "As expected, inclusion of character em- beddings increases performance by approximately 3%. The model with word embeddings learned in the model achieves the highest performance on the dev set (0.886), which may be because the pretrained worm embeddings were trained on a previous version of MIMIC. As a re- sult, some words in the discharge summaries, such as mi-spelled words or rarer diseases and medications, did not have associated word embeddings. Performing a sim- ple spell correction on out of vocab words may improve performance with pretrained word embeddings. We evaluated the best performing model on the test set. The Learned Word Embeddings model achieved an ac- curacy of 0.88 and an F1-Score of 0.876 on the test set. Table 5 shows the precision, recall, F1 score, and support for each of the ten labels, and Figure 3 shows the con- fusion matrix illustrating which labels were frequently misclassi\ufb01ed.",
      "Table 5 shows the precision, recall, F1 score, and support for each of the ten labels, and Figure 3 shows the con- fusion matrix illustrating which labels were frequently misclassi\ufb01ed. The demographics and patient movement labels achieved the highest F1 scores (0.96 and 0.93 re- spectively) while the vitals/labs and medication history labels had the lowest F1 scores (0.40 and 0.66 respec- tively). The demographics section consistently occurs at the beginning of the HPI note, and the patient move- ment section uses a limited vocab (transferred, admit- ted, etc.), which may explain their high F1 scores. On the other hand, the vitals/labs and medication history sections had the lowest support, which may explain why they were more challenging to label. Words that belonged to the diagnosis history, patient movement, and procedure/results sections were fre- quently labeled as symptoms/signs (Figure 3). Diagno- sis history sections may be labeled frequently as symp- toms/signs because symptoms/diseases can either be de-",
      "Extractive Summarization of Electronic Health Record Discharge Notes scribed as part of the patient\u2019s diagnosis history or cur- rent symptoms depending on when the symptom/disease occurred. However, many of the misclassi\ufb01cation errors may be due to inconsistency in manual labelling among annotators. For example, sentences describing both pa- tient movement and patient symptoms (e.g. \u201dthe patient was transferred to the hospital for his hypertension\u201d) were labeled entirely as \u2019patient movement\u2019 by some annotators while other annotators labeled the di\ufb00erent clauses of the sentence separately as \u2019patient movement\u2019 and \u2019symptoms/signs.\u2019 Further standardization among annotators is needed to avoid these misclassi\ufb01cations. Future work is needed to obtain additional manual an- notations where each HPI note is annotated by multiple annotators. This will allow for calculation of Cohen\u2019s kappa, which measures inter-annotator agreement, and comparison of clinician and non-clinician annotator reli- ability. Future work is also needed to better understand com- monly mislabeled categories and explore alternative model architectures.",
      "This will allow for calculation of Cohen\u2019s kappa, which measures inter-annotator agreement, and comparison of clinician and non-clinician annotator reli- ability. Future work is also needed to better understand com- monly mislabeled categories and explore alternative model architectures. Here we perform word level label prediction, which can result in phrases that contain mul- tiple labels. For example, the phrase \u201dhistory of neck pain\u201d can be labeled with both \u2019diagnosis history\u2019 and \u2019symptoms/signs\u2019 labels. Post-processing is needed to create a \ufb01nal label prediction for each phrase. While phrase level prediction may resolve these challenges, it is di\ufb03cult to segment the HPI note into phrases for pre- diction, as a single phrase may truly contain multiple labels. Segmentation of sentences by punctuation, con- junctions, and prepositions may yield the best phrase chunker for discharge summary text. Finally, supplementing the word embeddings in our LSTM model with CUIs may further improve perfor- mance. While word embeddings do well in learning the contextual context of words, CUIs allow for more explicit incorporation of medical domain expertise.",
      "Finally, supplementing the word embeddings in our LSTM model with CUIs may further improve perfor- mance. While word embeddings do well in learning the contextual context of words, CUIs allow for more explicit incorporation of medical domain expertise. By concate- nating the CUI for each word with its hybrid token em- bedding, we may be able to leverage both data driven and ontology driven approaches. 5. Conclusion In this paper we developed a CUI-based upper bound on extractive summarization of discharge summaries and presented a NN architecture that jointly classi\ufb01es words in history of present illness notes. We demonstrate that our model can achieve excellent performance on a small dataset with known heterogeneity among annotators. This model can be applied to the 55,000 discharge sum- maries in MIMIC to create a dataset for evaluation of extractive summarization methods. Table 4. F1-Scores on the development set under various model architectures. The \ufb01rst model in each section is the same model described in the methods section. Top perform- ing models in each section are in bold.",
      "Table 4. F1-Scores on the development set under various model architectures. The \ufb01rst model in each section is the same model described in the methods section. Top perform- ing models in each section are in bold. Model F1 w2v CBOW; Discharge Summaries 0.873 w2v Skip Gram; Discharge Summaries 0.831 w2v CBOW; All MIMIC Notes 0.862 w2v Skip Gram; All MIMIC Notes 0.809 With Character Embeddings 0.873 Without Character Embeddings 0.847 Pretrained Word Embeddings 0.873 Learned Word Embeddings 0.886 Table 5. Precision (P), Recall (R), F1 Score, and Number of supporting examples for each of the 10 label categories for the Learned Word Embedding model on the test set.",
      "Precision (P), Recall (R), F1 Score, and Number of supporting examples for each of the 10 label categories for the Learned Word Embedding model on the test set. Label P R F1 Support Demographics 0.96 0.95 0.96 1489 DiagnosisHistory 0.83 0.82 0.83 3156 MedicationHistory 0.80 0.56 0.66 557 ProcedureHistory 0.85 0.87 0.86 1835 Symptoms/Signs 0.81 0.83 0.82 2235 Vitals/Labs 0.83 0.26 0.40 486 Procedures/Results 0.86 0.72 0.78 1114 Meds/Treatments 0.93 0.87 0.90 4165 PatientMovement 0.89 0.97 0.93 10933 Other 0.90 0.85 0.88 1815 Average/Total 0.88 0.88 0.88 27785 Figure 3.",
      "Confusion Matrix for the Learned Word Embed- ding model on the test set. True labels are on the x-axis, and predicted labels are on the y-axis.",
      "Extractive Summarization of Electronic Health Record Discharge Notes Acknowledgments We would like to thank our annotators, Andrew Goldberg, Laurie Alsentzer, Elaine Goldberg, Andy Alsentzer, Grace Lo, and Josh Donis. We would also like to acknowledge Pete Szolovits for his guidance and for providing the pretrained word embeddings and Tris- tan Naumann for providing the MIMIC CUIs. References Allahyari, Mehdi and Kochut, Krys. Semantic tagging using topic models exploiting wikipedia category net- work. In Semantic Computing (ICSC), 2016 IEEE Tenth International Conference on, pp. 63\u201370. IEEE, 2016. Allahyari, Mehdi, Pouriyeh, Seyedamin, Asse\ufb01, Mehdi, Safaei, Saeid, Trippe, Elizabeth D, Gutierrez, Juan B, and Kochut, Krys. Text summarization techniques: A brief survey. arXiv preprint arXiv:1707.02268, 2017. Alsentzer, L. personal communication. Christensen, T. and Grimsmo, A.",
      "Text summarization techniques: A brief survey. arXiv preprint arXiv:1707.02268, 2017. Alsentzer, L. personal communication. Christensen, T. and Grimsmo, A. (n.d.). Instant avail- ability of patient records, but diminished availability of patient information: A multi-method study of gps use of electronic patient records. BMC Medical Infor- matics and Decision Making, 8(12), 2008. Dernoncourt, Franck, Young Lee, Ji, and Szolovits, Pe- ter. Neural networks for joint sentence classi\ufb01cation in medical paper abstracts. pp. 694700, 12 2016. Edmundson, H. P. New methods in automatic ex- tracting. J. ACM, 16(2):264\u2013285, April 1969. ISSN 0004-5411. doi: 10.1145/321510.321519. URL http: //doi.acm.org/10.1145/321510.321519.",
      "J. ACM, 16(2):264\u2013285, April 1969. ISSN 0004-5411. doi: 10.1145/321510.321519. URL http: //doi.acm.org/10.1145/321510.321519. Feblowitz, J. C., Wright, A., Singh, H., Samal, L., and Sittig, D. F. Summarization of clinical information: A conceptual model. Journal of Biomedical Informatics, 44:688\u2013699, 2011. Ferreira, Rafael, de Souza Cabral, Luciano, Lins, Rafael Dueire, e Silva, Gabriel Pereira, Freitas, Fred, Cavalcanti, George DC, Lima, Rinaldo, Simske, Steven J, and Favaro, Luciano. Assessing sentence scoring techniques for extractive text summarization. Expert systems with applications, 40(14):5755\u20135764, 2013. Goldstein, A. and Shahar, Y. An automated knowledge- based textual summarization system for longitudi- nal, multivariate clinical data.",
      "Expert systems with applications, 40(14):5755\u20135764, 2013. Goldstein, A. and Shahar, Y. An automated knowledge- based textual summarization system for longitudi- nal, multivariate clinical data. Journal of Biomedi- cal Informatics, 61(Supplement C):159 \u2013 175, 2016. ISSN 1532-0464. doi: https://doi.org/10.1016/j.jbi. 2016.03.022. URL http://www.sciencedirect.com/ science/article/pii/S1532046416300156. Google. Google Code Archive word2vec, 2013. URL https://code.google.com/archive/p/word2vec/. Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Kripalani, Sunil, Lefevre, Frank, Phillips, Christopher, V Williams, Mark, Basaviah, Preetha, and Baker, David.",
      "arXiv preprint arXiv:1412.6980, 2014. Kripalani, Sunil, Lefevre, Frank, Phillips, Christopher, V Williams, Mark, Basaviah, Preetha, and Baker, David. De\ufb01cits in communication and information transfer between hospital-based and primary care physicians. JAMA : the journal of the American Med- ical Association, 297:831\u201341, 03 2007. Liu, P. and Pan, X. Google Research Blog text summarization with tensor\ufb02ow, 2016. URL https://research.googleblog.com/2016/08/ text-summarization-with-tensorflow.html. Luhn, H. P. The automatic creation of literature ab- stracts. IBM J. Res. Dev., 2(2):159\u2013165, April 1958. ISSN 0018-8646. doi: 10.1147/rd.22.0159. URL http: //dx.doi.org/10.1147/rd.22.0159.",
      "Dev., 2(2):159\u2013165, April 1958. ISSN 0018-8646. doi: 10.1147/rd.22.0159. URL http: //dx.doi.org/10.1147/rd.22.0159. Mikolov, Tomas, Chen, Kai, Corrado, Greg, and Dean, Je\ufb00rey. E\ufb03cient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013a. Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg S, and Dean, Je\ufb00. Distributed representations of words and phrases and their compositionality. In Ad- vances in neural information processing systems, pp. 3111\u20133119, 2013b. Mikolov, Tomas, Yih, Wen-tau, and Zweig, Geo\ufb00rey. Linguistic regularities in continuous space word rep- resentations. In hlt-Naacl, volume 13, pp.",
      "Mikolov, Tomas, Yih, Wen-tau, and Zweig, Geo\ufb00rey. Linguistic regularities in continuous space word rep- resentations. In hlt-Naacl, volume 13, pp. 746\u2013751, 2013c. Moen, H., Peltonen, L.M., Heimonen, J., Airola, A., Pahikkala, T., Salakoski, T., and Salantera, T. Com- parison of automatic summarisation methods for clinical free text notes. Arti\ufb01cial Intelligence in Medicine, 67(Supplement C):25 \u2013 37, 2016. ISSN 0933-3657. doi: https://doi.org/10.1016/j.artmed. 2016.01.003. URL http://www.sciencedirect.com/ science/article/pii/S0933365716000051. Nallapati, Ramesh, Zhou, Bowen, Gulcehre, Caglar, Xi- ang, Bing, et al. Abstractive text summarization using sequence-to-sequence rnns and beyond.",
      "Nallapati, Ramesh, Zhou, Bowen, Gulcehre, Caglar, Xi- ang, Bing, et al. Abstractive text summarization using sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023, 2016.",
      "Extractive Summarization of Electronic Health Record Discharge Notes Natarajan, K., Stein, D., Jain, S., and Elhadad, N. (n.d.). An analysis of clinical queries in an elec- tronic health record search utility. International Jour- nal of Medical Informatics, 79(7):515\u2013522, 2010. NLM. UMLS Reference Manual. National Library of Medicine, 2009. Paulus, Romain, Xiong, Caiming, and Socher, Richard. A deep reinforced model for abstractive summariza- tion. arXiv preprint arXiv:1705.04304, 2017. Pivovarov, R. Electronic Health Record Summarization Over Heterogenous and Irregularly Sampled Clinical Data. PhD thesis, Department of Biomedical Infor- matics, Columbia University, 2016. Rim, Kyeongmin. Mae2: Portable annotation tool for general natural language use. in proceedings of the 12th joint acl-iso workshop on interoperable semantic annotation. 05 2016.",
      "Rim, Kyeongmin. Mae2: Portable annotation tool for general natural language use. in proceedings of the 12th joint acl-iso workshop on interoperable semantic annotation. 05 2016. Rush, Alexander M, Chopra, Sumit, and Weston, Jason. A neural attention model for abstractive sentence sum- marization. arXiv preprint arXiv:1509.00685, 2015. Savova, Guergana K, Masanz, James J, Ogren, Philip V, Zheng, Jiaping, Sohn, Sunghwan, Kipper-Schuler, Karin C, and Chute, Christopher G. Mayo clinical text analysis and knowledge extraction system (ctakes): architecture, component evaluation and applications. Journal of the American Medical Informatics Associ- ation, 17(5):507\u2013513, 2010. Singh, H., Spitzmueller, C., Petersen, N. J., Sawhney, M. K., and Sittig, D. F. (n.d.). Information overload and missed test results in electronic health record- based settings.",
      "Singh, H., Spitzmueller, C., Petersen, N. J., Sawhney, M. K., and Sittig, D. F. (n.d.). Information overload and missed test results in electronic health record- based settings. JAMA Internal Medicine, 173(8), 2013."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1810.12085.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":7342,
  "avg_doclen":174.8095238095,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1810.12085.pdf"
    }
  }
}