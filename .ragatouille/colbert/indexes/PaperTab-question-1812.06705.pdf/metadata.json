{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Conditional BERT Contextual Augmentation Xing Wu1,2, Shangwen Lv1,2, Liangjun Zang1\u2020, Jizhong Han1, Songlin Hu1,2\u2020 Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China University of Chinese Academy of Sciences, Beijing, China {wuxing,lvshangwen,zangliangjun,hanjizhong,husonglin}@iie.ac.cn Abstract We propose a novel data augmentation method for labeled sentences called con- ditional BERT contextual augmentation. Data augmentation methods are often ap- plied to prevent over\ufb01tting and improve generalization of deep neural network models. Recently proposed contextual augmentation augments labeled sentences by randomly replacing words with more varied substitutions predicted by language model. BERT demonstrates that a deep bidirectional language model is more pow- erful than either an unidirectional lan- guage model or the shallow concatena- tion of a forward and backward model. We retro\ufb01t BERT to conditional BERT by introducing a new conditional masked language model1 task. The well trained conditional BERT can be applied to en- hance contextual augmentation.",
      "We retro\ufb01t BERT to conditional BERT by introducing a new conditional masked language model1 task. The well trained conditional BERT can be applied to en- hance contextual augmentation. Experi- ments on six various different text classi- \ufb01cation tasks show that our method can be easily applied to both convolutional or re- current neural networks classi\ufb01er to obtain obvious improvement. 1 Introduction Deep neural network-based models are easy to over\ufb01t and result in losing their generalization due to limited size of training data. In order to ad- dress the issue, data augmentation methods are often applied to generate more training samples. 1The term \u201cconditional masked language model\u201d ap- peared once in original BERT paper, which indicates context- conditional, is equivalent to term \u201cmasked language model\u201d. In our paper, \u201cconditional masked language model\u201d indicates we apply extra label-conditional constraint to the \u201cmasked language model\u201d.",
      "1The term \u201cconditional masked language model\u201d ap- peared once in original BERT paper, which indicates context- conditional, is equivalent to term \u201cmasked language model\u201d. In our paper, \u201cconditional masked language model\u201d indicates we apply extra label-conditional constraint to the \u201cmasked language model\u201d. Recent years have witnessed great success in ap- plying data augmentation in the \ufb01eld of speech area(Jaitly and Hinton, 2013; Ko et al., 2015) and computer vision(Krizhevsky et al., 2012; Simard et al., 1998; Szegedy et al., 2015). Data augmen- tation in these areas can be easily performed by transformations like resizing, mirroring, random cropping, and color shifting. However, applying these universal transformations to texts is largely randomized and uncontrollable, which makes it impossible to ensure the semantic invariance and label correctness. For example, given a movie re- view \u201cThe actors is good\u201d, by mirroring we get \u201cdoog si srotca ehT\u201d, or by random cropping we get \u201cactors is\u201d, both of which are meaningless.",
      "For example, given a movie re- view \u201cThe actors is good\u201d, by mirroring we get \u201cdoog si srotca ehT\u201d, or by random cropping we get \u201cactors is\u201d, both of which are meaningless. Existing data augmentation methods for text are often loss of generality, which are developed with handcrafted rules or pipelines for speci\ufb01c do- mains. A general approach for text data augmen- tation is replacement-based method, which gener- ates new sentences by replacing the words in the sentences with relevant words (e.g. synonyms). However, words with synonyms from a hand- crafted lexical database likes WordNet(Miller, 1995) are very limited , and the replacement-based augmentation with synonyms can only produce limited diverse patterns from the original texts. To address the limitation of replacement-based meth- ods, Kobayashi(Kobayashi, 2018) proposed con- textual augmentation for labeled sentences by of- fering a wide range of substitute words, which are predicted by a label-conditional bidirectional lan- guage model according to the context.",
      "But con- textual augmentation suffers from two shortages: the bidirectional language model is simply shallow concatenation of a forward and backward model, and the usage of LSTM models restricts their pre- diction ability to a short range. BERT, which stands for Bidirectional Encoder Representations from Transformers, pre-trained arXiv:1812.06705v1  [cs.CL]  17 Dec 2018",
      "deep bidirectional representations by jointly con- ditioning on both left and right context in all lay- ers. BERT addressed the unidirectional constraint by proposing a \u201cmasked language model\u201d (MLM) objective by masking some percentage of the in- put tokens at random, and predicting the masked words based on its context. This is very similar to how contextual augmentation predict the replace- ment words. But BERT was proposed to pre-train text representations, so MLM task is performed in an unsupervised way, taking no label variance into consideration. This paper focuses on the replacement-based methods, by proposing a novel data augmentation method called conditional BERT contextual aug- mentation. The method applies contextual aug- mentation by conditional BERT, which is \ufb01ne- tuned on BERT. We adopt BERT as our pre-trained language model with two reasons. First, BERT is based on Transformer. Transformer provides us with a more structured memory for handling long-term dependencies in text. Second, BERT, as a deep bidirectional model, is strictly more powerful than the shallow concatenation of a left- to-right and right-to left model.",
      "First, BERT is based on Transformer. Transformer provides us with a more structured memory for handling long-term dependencies in text. Second, BERT, as a deep bidirectional model, is strictly more powerful than the shallow concatenation of a left- to-right and right-to left model. So we apply BERT to contextual augmentation for labeled sen- tences, by offering a wider range of substitute words predicted by the masked language model task. However, the masked language model pre- dicts the masked word based only on its context, so the predicted word maybe incompatible with the annotated labels of the original sentences. In or- der to address this issue, we introduce a new \ufb01ne- tuning objective: the \u201dconditional masked lan- guage model\u201d(C-MLM). The conditional masked language model randomly masks some of the to- kens from an input, and the objective is to predict a label-compatible word based on both its context and sentence label. Unlike Kobayashi\u2019s work, the C-MLM objective allows a deep bidirectional rep- resentations by jointly conditioning on both left and right context in all layers.",
      "Unlike Kobayashi\u2019s work, the C-MLM objective allows a deep bidirectional rep- resentations by jointly conditioning on both left and right context in all layers. In order to evaluate how well our augmentation method improves per- formance of deep neural network models, follow- ing Kobayashi(Kobayashi, 2018), we experiment it on two most common neural network struc- tures, LSTM-RNN and CNN, on text classi\ufb01ca- tion tasks. Through the experiments on six various different text classi\ufb01cation tasks, we demonstrate that the proposed conditional BERT model aug- ments sentence better than baselines, and condi- tional BERT contextual augmentation method can be easily applied to both convolutional or recur- rent neural networks classi\ufb01er. We further explore our conditional MLM tasks connection with style transfer task and demonstrate that our conditional BERT can also be applied to style transfer too. Our contributions are concluded as follows: \u2022 We propose a conditional BERT contextual augmentation method. The method allows BERT to augment sentences without break- ing the label-compatibility. Our conditional BERT can further be applied to style transfer task.",
      "Our contributions are concluded as follows: \u2022 We propose a conditional BERT contextual augmentation method. The method allows BERT to augment sentences without break- ing the label-compatibility. Our conditional BERT can further be applied to style transfer task. \u2022 Experimental results show that our approach obviously outperforms existing text data aug- mentation approaches. To our best knowledge, this is the \ufb01rst attempt to alter BERT to a conditional BERT or apply BERT on text generation tasks. 2 Related Work 2.1 Fine-tuning on Pre-trained Language Model Language model pre-training has attracted wide attention and \ufb01ne-tuning on pre-trained language model has shown to be effective for improv- ing many downstream natural language process- ing tasks. Dai(Dai and Le, 2015) pre-trained unlabeled data to improve Sequence Learning with recurrent networks. Howard(Howard and Ruder, 2018) proposed a general transfer learning method, Universal Language Model Fine-tuning (ULMFiT), with the key techniques for \ufb01ne-tuning a language model.",
      "Howard(Howard and Ruder, 2018) proposed a general transfer learning method, Universal Language Model Fine-tuning (ULMFiT), with the key techniques for \ufb01ne-tuning a language model. Radford(Radford et al., 2018) proposed that by generative pre-training of a lan- guage model on a diverse corpus of unlabeled text, large gains on a diverse range of tasks could be realized. Radford(Radford et al., 2018) achieved large improvements on many sentence-level tasks from the GLUE benchmark(Wang et al., 2018). BERT(Devlin et al., 2018) obtained new state-of- the-art results on a broad range of diverse tasks. BERT pre-trained deep bidirectional representa- tions which jointly conditioned on both left and right context in all layers, following by discrim- inative \ufb01ne-tuning on each speci\ufb01c task. Unlike previous works \ufb01ne-tuning pre-trained language model to perform discriminative tasks, we aim to apply pre-trained BERT on generative tasks",
      "by perform the masked language model(MLM) task. To generate sentences that are compatible with given labels, we retro\ufb01t BERT to conditional BERT, by introducing a conditional masked lan- guage model task and \ufb01ne-tuning BERT on the task. 2.2 Text Data Augmentation Text data augmentation has been extensively stud- ied in natural language processing. Sample-based methods includes downsampling from the major- ity classes and oversampling from the minority class, both of which perform weakly in prac- tice. Generation-based methods employ deep gen- erative models such as GANs(Goodfellow et al., 2014) or VAEs(Bowman et al., 2015; Hu et al., 2017), trying to generate sentences from a con- tinuous space with desired attributes of sentiment and tense. However, sentences generated in these methods are very hard to guarantee the quality both in label compatibility and sentence read- ability. In some speci\ufb01c areas (Jia and Liang, 2017; Xie et al., 2017; Ebrahimi et al., 2017). word replacement augmentation was applied.",
      "In some speci\ufb01c areas (Jia and Liang, 2017; Xie et al., 2017; Ebrahimi et al., 2017). word replacement augmentation was applied. Wang(Wang and Yang, 2015) proposed the use of neighboring words in continuous representations to create new instances for every word in a tweet to augment the training dataset. Zhang(Zhang et al., 2015) extracted all replaceable words from the given text and randomly choose r of them to be replaced, then substituted the replace- able words with synonyms from WordNet(Miller, 1995). Kolomiyets(Kolomiyets et al., 2011) re- placed only the headwords under a task-speci\ufb01c assumption that temporal trigger words usually occur as headwords. Kolomiyets(Kolomiyets et al., 2011) selected substitute words with top-K scores given by the Latent Words LM(Deschacht and Moens, 2009), which is a LM based on \ufb01xed length contexts.",
      "Kolomiyets(Kolomiyets et al., 2011) selected substitute words with top-K scores given by the Latent Words LM(Deschacht and Moens, 2009), which is a LM based on \ufb01xed length contexts. Fadaee(Fadaee et al., 2017) fo- cused on the rare word problem in machine trans- lation, replacing words in a source sentence with only rare words. A word in the translated sen- tence is also replaced using a word alignment method and a rightward LM. The work most similar to our research is Kobayashi(Kobayashi, 2018). Kobayashi used a \ufb01ll-in-the-blank context for data augmentation by replacing every words in the sentence with language model. In order to prevent the generated words from reversing the information related to the labels of the sen- tences, Kobayashi(Kobayashi, 2018) introduced a conditional constraint to control the replacement of words.",
      "In order to prevent the generated words from reversing the information related to the labels of the sen- tences, Kobayashi(Kobayashi, 2018) introduced a conditional constraint to control the replacement of words. Unlike previous works, we adopt a deep bidirectional language model to apply re- placement, and the attention mechanism within our model allows a more structured memory for handling long-term dependencies in text, which resulting in more general and robust improvement on various downstream tasks. 3 Conditional BERT Contextual Augmentation 3.1 Preliminary: Masked Language Model Task 3.1.1 Bidirectional Language Model In general, the language model(LM) models the probability of generating natural language sen- tences or documents. Given a sequence S of N to- kens, < t1, t2, ..., tN >, a forward language model allows us to predict the probability of the sequence as: p(t1, t2, ..., tN) = N Y i=1 p(ti|t1, t2, ..., ti\u22121).",
      "(1) Similarly, a backward language model allows us to predict the probability of the sentence as: p(t1, t2, ..., tN) = N Y i=1 p(ti|ti+1, ti+2, ..., tN). (2) Traditionally, a bidirectional language model a shallow concatenation of independently trained forward and backward LMs. 3.1.2 Masked Language Model Task In order to train a deep bidirectional language model, BERT proposed Masked Language Model (MLM) task, which was also referred to Cloze Task(Taylor, 1953). MLM task randomly masks some percentage of the input tokens, and then pre- dicts only those masked tokens according to their context. Given a masked token ti, the context is the tokens surrounding token ti in the sequence S, i.e. cloze sentence S\\{ti}. The \ufb01nal hidden vec- tors corresponding to the mask tokens are fed into an output softmax over the vocabulary to produce words with a probability distribution p(\u00b7|S\\{ti}). MLM task only predicts the masked words rather than reconstructing the entire input, which sug- gests that more pre-training steps are required for",
      "the model to converge. Pre-trained BERT can aug- ment sentences through MLM task, by predicting new words in masked positions according to their context. 3.2 Conditional BERT As shown in Fig 1, our conditional BERT shares the same model architecture with the original BERT. The differences are the input representation and training procedure. The input embeddings of BERT are the sum of the token embeddings, the segmentation embed- dings and the position embeddings. For the seg- mentation embeddings in BERT, a learned sen- tence A embedding is added to every token of the \ufb01rst sentence, and if a second sentence exists, a sentence B embedding will be added to every to- ken of the second sentence. However, the segmen- tation embeddings has no connection to the actual annotated labels of a sentence, like sense, senti- ment or subjectivity, so predicted word is not al- ways compatible with annotated labels. For ex- ample, given a positive movie remark \u201cthis ac- tor is good\u201d, we have the word \u201cgood\u201d masked.",
      "For ex- ample, given a positive movie remark \u201cthis ac- tor is good\u201d, we have the word \u201cgood\u201d masked. Through the Masked Language Model task by BERT, the predicted word in the masked position has potential to be negative word likes \u201dbad\u201d or \u201dboring\u201d. Such new generated sentences by sub- stituting masked words are implausible with re- spect to their original labels, which will be harm- ful if added to the corpus to apply augmentation. In order to address this issue, we propose a new task: \u201cconditional masked language model\u201d. 3.2.1 Conditional Masked Language Model The conditional masked language model randomly masks some of the tokens from the labeled sen- tence, and the objective is to predict the original vocabulary index of the masked word based on both its context and its label. Given a masked token ti, the context S\\{ti} and label y are both considered, aiming to calculate p(\u00b7|y, S\\{ti}), in- stead of calculating p(\u00b7|S\\{ti}).",
      "Given a masked token ti, the context S\\{ti} and label y are both considered, aiming to calculate p(\u00b7|y, S\\{ti}), in- stead of calculating p(\u00b7|S\\{ti}). Unlike MLM pre-training, the conditional MLM objective al- lows the representation to fuse the context infor- mation and the label information, which allows us to further train a label-conditional deep bidirec- tional representations. To perform conditional MLM task, we \ufb01ne- tune on pre-trained BERT. We alter the segmen- tation embeddings to label embeddings, which are learned corresponding to their annotated labels on labeled datasets. Note that the BERT are designed with segmentation embedding being embedding A or embedding B, so when a downstream task dataset with more than two labels, we have to adapt the size of embedding to label size com- patible. We train conditional BERT using con- ditional MLM task on labeled dataset. After the model has converged, it is expected to be able to predict words in masked position both considering the context and the label.",
      "We train conditional BERT using con- ditional MLM task on labeled dataset. After the model has converged, it is expected to be able to predict words in masked position both considering the context and the label. 3.3 Conditional BERT Contextual Augmentation After the conditional BERT is well-trained, we uti- lize it to augment sentences. Given a labeled sen- tence from the corpus, we randomly mask a few words in the sentence. Through conditional BERT, various words compatibly with the label of the sentence are predicted by conditional BERT. Af- ter substituting the masked words with predicted words, a new sentences is generated, which shares similar context and same label with original sen- tence. Then new sentences are added to original corpus. We elaborate the entire process in algo- rithm 1. Algorithm 1 Conditional BERT contextual aug- mentation algorithm. Fine-tuning on the pre- trained BERT , we retro\ufb01t BERT to conditional BERT using conditional MLM task on labeled dataset. After the model converged, we utilize it to augment sentences. New sentences are added into dataset to augment the dataset.",
      "Fine-tuning on the pre- trained BERT , we retro\ufb01t BERT to conditional BERT using conditional MLM task on labeled dataset. After the model converged, we utilize it to augment sentences. New sentences are added into dataset to augment the dataset. 1: Alter the segmentation embeddings to label embeddings 2: Fine-tune the pre-trained BERT using condi- tional MLM task on labeled dataset D until convergence 3: for each iteration i=1,2,...,M do 4: Sample a sentence s from D 5: Randomly mask k words 6: Using \ufb01ne-tuned conditional BERT to pre- dict label-compatible words on masked po- sitions to generate a new sentence S\u2032 7: end for 8: Add new sentences into dataset D to get aug- mented dataset D\u2032 9: Perform downstream task on augmented dataset D\u2032",
      "Figure 1: Model architecture of conditional BERT. The label embeddings in conditional BERT corre- sponding to segmentation embeddings in BERT, but their functions are different. 4 Experiment In this section, we present conditional BERT parameter settings and, following Kobayashi(Kobayashi, 2018), we apply dif- ferent augmentation methods on two types of neural models through six text classi\ufb01cation tasks. The pre-trained BERT model we used in our experiment is BERTBASE, with number of layers (i.e., Transformer blocks) L = 12, the hidden size H = 768, and the number of self-attention heads A = 12, total parameters = 110M. Detailed pre-train parameters setting can be found in original paper(Devlin et al., 2018). For each task, we perform the following steps independently. First, we evaluate the augmentation ability of original BERT model pre-trained on MLM task. We use pre-trained BERT to augment dataset, by predicted masked words only condition on context for each sentence. Second, we \ufb01ne-tune the original BERT model to a conditional BERT.",
      "First, we evaluate the augmentation ability of original BERT model pre-trained on MLM task. We use pre-trained BERT to augment dataset, by predicted masked words only condition on context for each sentence. Second, we \ufb01ne-tune the original BERT model to a conditional BERT. Well-trained conditional BERT augments each sentence in dataset by predicted masked words condition on both context and label. Third, we compare the performance of the two methods with Kobayashi\u2019s(Kobayashi, 2018) contextual aug- mentation results. Note that the original BERTs segmentation embeddings layer is compatible with two-label dataset. When the task-speci\ufb01c dataset is with more than two different labels, we should re-train a label size compatible label embeddings layer instead of directly \ufb01ne-tuning the pre-trained one. 4.1 Datasets Six benchmark classi\ufb01cation datasets are listed in table 1. Following Kim(Kim, 2014), for a dataset without validation data, we use 10% of its training set for the validation set. Summary statistics of six classi\ufb01cation datasets are shown in table 1.",
      "Following Kim(Kim, 2014), for a dataset without validation data, we use 10% of its training set for the validation set. Summary statistics of six classi\ufb01cation datasets are shown in table 1. Table 1: Summary statistics for the datasets after tokenization. c: Number of target classes. l: Aver- age sentence length. N: Dataset size. |V |: Vocab- ulary size. Test: Test set size (CV means there was no standard train/test split and thus 10-fold cross-validation was used).",
      "c: Number of target classes. l: Aver- age sentence length. N: Dataset size. |V |: Vocab- ulary size. Test: Test set size (CV means there was no standard train/test split and thus 10-fold cross-validation was used). Data c l N |V | Test SST5 5 18 11855 17836 2210 SST2 2 19 9613 16185 1821 Subj 2 23 10000 21323 CV TREC 6 10 5952 9592 500 MPQA 2 3 10606 6246 CV RT 2 21 10662 20287 CV SST(Socher et al., 2013) SST (Stanford Sentiment Treebank) is a dataset for sentiment classi\ufb01cation on movie reviews, which are annotated with \ufb01ve labels (SST5: very positive, positive, neutral, neg- ative, or very negative) or two labels (SST2: posi-",
      "tive or negative). Subj(Pang and Lee, 2004) Subj (Subjectivity dataset) is annotated with whether a sentence is subjective or objective. MPQA(Wiebe et al., 2005) MPQA Opinion Cor- pus is an opinion polarity detection dataset of short phrases rather than sentences, which con- tains news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations, etc.). RT(Pang and Lee, 2005) RT is another movie re- view sentiment dataset contains a collection of short review excerpts from Rotten Tomatoes col- lected by Bo Pang and Lillian Lee. TREC(Li and Roth, 2002) TREC is a dataset for classi\ufb01cation of the six question types (whether the question is about person, location, numeric in- formation, etc.). 4.2 Text classi\ufb01cation 4.2.1 Sentence Classi\ufb01er Structure We evaluate the performance improvement brought by conditional BERT contextual aug- mentation on sentence classi\ufb01cation tasks, so we need to prepare two common sentence classi\ufb01ers beforehand.",
      "4.2 Text classi\ufb01cation 4.2.1 Sentence Classi\ufb01er Structure We evaluate the performance improvement brought by conditional BERT contextual aug- mentation on sentence classi\ufb01cation tasks, so we need to prepare two common sentence classi\ufb01ers beforehand. For comparison, fol- lowing Kobayashi(Kobayashi, 2018), we adopt two typical classi\ufb01er architectures: CNN or LSTM-RNN. The CNN-based classi\ufb01er(Kim, 2014) has convolutional \ufb01lters of size 3, 4, 5 and word embeddings. All outputs of each \ufb01lter are concatenated before applied with a max-pooling over time, then fed into a two-layer feed-forward network with ReLU, followed by the softmax function. An RNN-based classi\ufb01er has a single layer LSTM and word embeddings, whose output is fed into an output af\ufb01ne layer with the softmax function. For both the architec- tures, dropout(Srivastava et al., 2014) and Adam optimization(Kingma and Ba, 2014) are applied during training.",
      "For both the architec- tures, dropout(Srivastava et al., 2014) and Adam optimization(Kingma and Ba, 2014) are applied during training. The train process is \ufb01nish by early stopping with validation at each epoch. 4.2.2 Hyper-parameters Setting Sentence classi\ufb01er hyper-parameters including learning rate, embedding dimension, unit or \ufb01l- ter size, and dropout ratio, are selected using grid- search for each task-speci\ufb01c dataset. We refer to Kobayashi\u2019s implementation in the released code2. 2https://github.com/pfnet\\ discretionary{-}{}{}research/contextual_ For BERT, all hyper-parameters are kept the same as Devlin(Devlin et al., 2018), codes in Tensor- \ufb02ow3 and PyTorch4 are all available on github and pre-trained BERT model can also be down- loaded. The number of conditional BERT train- ing epochs ranges in [1-50] and number of masked words ranges in [1-2].",
      "The number of conditional BERT train- ing epochs ranges in [1-50] and number of masked words ranges in [1-2]. 4.2.3 Baselines We compare the performance improvements ob- tained by our proposed method with the following baseline methods, \u201cw/\u201d means \u201cwith\u201d: \u2022 w/synonym: Words are randomly replaced with synonyms from WordNet(Miller, 1995). \u2022 w/context: Proposed by Kobayashi(Kobayashi, 2018), which used a bidirectional language model to apply contextual augmentation, each word was replaced with a probability. \u2022 w/context+label: Kobayashis contextual aug- mentation method(Kobayashi, 2018) in a label-conditional LM architecture. 4.2.4 Experiment Results Table 2 lists the accuracies of the all methods on two classi\ufb01er architectures. The results show that, for various datasets on different classi\ufb01er architectures, our conditional BERT contextual augmentation improves the model performances most. BERT can also augments sentences to some extent, but not as much as conditional BERT does.",
      "The results show that, for various datasets on different classi\ufb01er architectures, our conditional BERT contextual augmentation improves the model performances most. BERT can also augments sentences to some extent, but not as much as conditional BERT does. For we masked words randomly, the masked words may be label-sensitive or label-insensitive. If label-insensitive words are masked, words pre- dicted through BERT may not be compatible with original labels. The improvement over all bench- mark datasets also shows that conditional BERT is a general augmentation method for multi-labels sentence classi\ufb01cation tasks. 4.2.5 Effect of Number of Fine-tuning Steps We also explore the effect of number of train- ing steps to the performance of conditional BERT data augmentation. The \ufb01ne-tuning epoch setting ranges in [1-50], we list the \ufb01ne-tuning epoch of augmentation 3https://github.com/google-research/ bert 4https://github.com/huggingface/ pytorch-pretrained-BERT",
      "Table 2: Accuracies of different methods for various benchmarks on two classi\ufb01er architectures. C- BERT, which represents conditional BERT, performs best on two classi\ufb01er structures over six datasets. \u201cw/\u201d represents \u201cwith\u201d, lines marked with \u201c*\u201d are experiments results from Kobayashi(Kobayashi, 2018). Model SST5 SST2 Subj MPQA RT TREC Avg. CNN* 41.3 79.5 92.4 86.1 75.9 90.0 77.53 w/synonym* 40.7 80.0 92.4 86.3 76.0 89.6 77.50 w/context* 41.9 80.9 92.7 86.7 75.9 90.0 78.02 w/context+label* 42.1 80.8 93.0 86.7 76.1 90.5 78.20 w/BERT 41.5 81.9 92.9 87.7 78.2 91.8 79.",
      "02 w/context+label* 42.1 80.8 93.0 86.7 76.1 90.5 78.20 w/BERT 41.5 81.9 92.9 87.7 78.2 91.8 79.00 w/C-BERT 42.3 82.1 93.4 88.2 79.0 92.6 79.60 RNN* 40.2 80.3 92.4 86.0 76.7 89.0 77.43 w/synonym* 40.5 80.2 92.8 86.4 76.6 87.9 77.40 w/context* 40.9 79.3 92.8 86.4 77.0 89.3 77.62 w/context+label* 41.1 80.1 92.8 86.4 77.4 89.2 77.83 w/BERT 41.3 81.4 93.5 87.3 78.3 89.8 78.",
      "62 w/context+label* 41.1 80.1 92.8 86.4 77.4 89.2 77.83 w/BERT 41.3 81.4 93.5 87.3 78.3 89.8 78.60 w/C-BERT 42.6 81.9 93.9 88.0 78.9 91.0 79.38 conditional BERT to outperform BERT for var- ious benchmarks in table 3. The results show that our conditional BERT contextual augmenta- tion can achieve obvious performance improve- ment after only a few \ufb01ne-tuning epochs, which is very convenient to apply to downstream tasks. Table 3: Fine-tuning epochs of conditional BERT to outperform BERT for various benchmarks Model SST5 SST2 Subj MPQA RT TREC CNN 4 3 1 2 2 1 RNN 6 2 2 2 1 1 5 Connection to Style Transfer In this section, we further deep into the connection to style transfer and apply our well trained condi- tional BERT to style transfer task.",
      "Style transfer is de\ufb01ned as the task of rephrasing the text to con- tain speci\ufb01c stylistic properties without changing the intent or affect within the context(Prabhumoye et al., 2018). Our conditional MLM task changes words in the text condition on given label with- out changing the context. View from this point, the two tasks are very close. So in order to ap- ply conditional BERT to style transfer task, given a speci\ufb01c stylistic sentence, we break it into two steps: \ufb01rst, we \ufb01nd the words relevant to the style; second, we mask the style-relevant words, then use conditional BERT to predict new substitutes with sentence context and target style property. In order to \ufb01nd style-relevant words in a sentence, we refer to Xu(Xu et al., 2018), which proposed an attention-based method to extract the contri- bution of each word to the sentence sentimental label. For example, given a positive movie re- mark \u201cThis movie is funny and interesting\u201d, we \ufb01lter out the words contributes largely to the la- bel and mask them.",
      "For example, given a positive movie re- mark \u201cThis movie is funny and interesting\u201d, we \ufb01lter out the words contributes largely to the la- bel and mask them. Then through our conditional BERT contextual augmentation method, we \ufb01ll in the masked position by predicting words condi- tioning on opposite label and sentence context, re- sulting in \u201cThis movie is boring and dull\u201d. The words \u201cboring\u201d and \u201cdull\u201d contribute to the new sentence being labeled as negative style. We sam- ple some sentences from dataset SST2, transfer- ring them to the opposite label, as listed in table 4. 6 Conclusions and Future Work In this paper, we \ufb01ne-tune BERT to conditional BERT by introducing a novel conditional MLM task. After being well trained, the conditional BERT can be applied to data augmentation for sentence classi\ufb01cation tasks. Experiment results show that our model outperforms several base- line methods obviously. Furthermore, we demon- strate that our conditional BERT can also be ap- plied to style transfer task.",
      "Experiment results show that our model outperforms several base- line methods obviously. Furthermore, we demon- strate that our conditional BERT can also be ap- plied to style transfer task. In the future, (1)We will explore how to perform text data augmenta- tion on imbalanced datasets with pre-trained lan- guage model, (2) we believe the idea of condi- tional BERT contextual augmentation is universal and will be applied to paragraph or document level data augmentation.",
      "Table 4: Examples generated by conditional BERT on the SST2 dataset. To perform style transfer, we reverse the original label of a sentence, and conditional BERT output a new label compatible sentence. Original: there \u2019s no disguising this as one of the worst \ufb01lms of the summer . Generated: there \u2019s no disguising this as one of the best \ufb01lms of the summer . Original: it \u2019s probably not easy to make such a worthless \ufb01lm ... Generated: it \u2019s probably not easy to make such a stunning \ufb01lm ... Original: woody allen has really found his groove these days . Generated: woody allen has really lost his groove these days . References Samuel R Bowman, Luke Vilnis, Oriol Vinyals, An- drew M Dai, Rafal Jozefowicz, and Samy Ben- gio. 2015. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349. Andrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. pages 3079\u20133087.",
      "2015. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349. Andrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. pages 3079\u20133087. Koen Deschacht and Marie-Francine Moens. 2009. Semi-supervised semantic role labeling using the la- tent words language model. pages 21\u201329. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805. Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. 2017. Hot\ufb02ip: White-box adversarial exam- ples for nlp. arXiv preprint arXiv:1712.06751. Marzieh Fadaee, Arianna Bisazza, and Christof Monz. 2017. Data augmentation for low- resource neural machine translation.",
      "arXiv preprint arXiv:1712.06751. Marzieh Fadaee, Arianna Bisazza, and Christof Monz. 2017. Data augmentation for low- resource neural machine translation. arXiv preprint arXiv:1705.00440. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative ad- versarial nets. pages 2672\u20132680. Jeremy Howard and Sebastian Ruder. 2018. Universal language model \ufb01ne-tuning for text classi\ufb01cation. 1:328\u2013339. Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P Xing. 2017. Toward controlled generation of text. arXiv preprint arXiv:1703.00955. Navdeep Jaitly and Geoffrey E Hinton. 2013.",
      "2017. Toward controlled generation of text. arXiv preprint arXiv:1703.00955. Navdeep Jaitly and Geoffrey E Hinton. 2013. Vo- cal tract length perturbation (vtlp) improves speech recognition. 117. Robin Jia and Percy Liang. 2017. Adversarial exam- ples for evaluating reading comprehension systems. arXiv preprint arXiv:1707.07328. Yoon Kim. 2014. Convolutional neural net- works for sentence classi\ufb01cation. arXiv preprint arXiv:1408.5882. Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San- jeev Khudanpur. 2015. Audio augmentation for speech recognition. Sosuke Kobayashi. 2018.",
      "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San- jeev Khudanpur. 2015. Audio augmentation for speech recognition. Sosuke Kobayashi. 2018. Contextual augmentation: Data augmentation by words with paradigmatic re- lations. arXiv preprint arXiv:1805.06201. Oleksandr Kolomiyets, Steven Bethard, and Marie- Francine Moens. 2011. Model-portability experi- ments for textual temporal analysis. pages 271\u2013276. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin- ton. 2012. Imagenet classi\ufb01cation with deep convo- lutional neural networks. pages 1097\u20131105. Xin Li and Dan Roth. 2002. Learning question classi- \ufb01ers. pages 1\u20137. George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39\u2013 41.",
      "2002. Learning question classi- \ufb01ers. pages 1\u20137. George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39\u2013 41. Bo Pang and Lillian Lee. 2004. A sentimental educa- tion: Sentiment analysis using subjectivity summa- rization based on minimum cuts. page 271. Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit- ing class relationships for sentiment categorization with respect to rating scales. pages 115\u2013124. Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhutdinov, and Alan W Black. 2018. Style transfer through back-translation. arXiv preprint arXiv:1804.09000. Alec Radford, Karthik Narasimhan, Tim Sali- mans, and Ilya Sutskever. 2018. Improv- ing language understanding by generative pre- training. URL https://s3-us-west-2.",
      "Alec Radford, Karthik Narasimhan, Tim Sali- mans, and Ilya Sutskever. 2018. Improv- ing language understanding by generative pre- training. URL https://s3-us-west-2. amazon- aws. com/openai-assets/research-covers/language- unsupervised/language understanding paper. pdf. Patrice Y Simard, Yann A LeCun, John S Denker, and Bernard Victorri. 1998. Transformation invariance in pattern recognitiontangent distance and tangent propagation. pages 239\u2013274.",
      "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment tree- bank. pages 1631\u20131642. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from over\ufb01tting. The Journal of Machine Learning Research, 15(1):1929\u20131958. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du- mitru Erhan, Vincent Vanhoucke, and Andrew Ra- binovich. 2015. Going deeper with convolutions. pages 1\u20139. Wilson L Taylor. 1953. cloze procedure: A new tool for measuring readability. Journalism Bulletin, 30(4):415\u2013433. Alex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.",
      "Wilson L Taylor. 1953. cloze procedure: A new tool for measuring readability. Journalism Bulletin, 30(4):415\u2013433. Alex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461. William Yang Wang and Diyi Yang. 2015. That\u2019s so annoying!!!: A lexical and frame-semantic em- bedding based data augmentation approach to auto- matic categorization of annoying behaviors using# petpeeve tweets. pages 2557\u20132563. Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emo- tions in language. Language resources and evalua- tion, 39(2-3):165\u2013210. Ziang Xie, Sida I Wang, Jiwei Li, Daniel L\u00b4evy, Aiming Nie, Dan Jurafsky, and Andrew Y Ng. 2017.",
      "Language resources and evalua- tion, 39(2-3):165\u2013210. Ziang Xie, Sida I Wang, Jiwei Li, Daniel L\u00b4evy, Aiming Nie, Dan Jurafsky, and Andrew Y Ng. 2017. Data noising as smoothing in neural network language models. arXiv preprint arXiv:1703.02573. Jingjing Xu, Xu Sun, Qi Zeng, Xuancheng Ren, Xi- aodong Zhang, Houfeng Wang, and Wenjie Li. 2018. Unpaired sentiment-to-sentiment translation: A cycled reinforcement learning approach. arXiv preprint arXiv:1805.05181. Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text clas- si\ufb01cation. pages 649\u2013657."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1812.06705.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":8212,
  "avg_doclen":182.4888888889,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1812.06705.pdf"
    }
  }
}