[
  "Enhancing Pre-trained Chinese Character Representation with Word-aligned Attention Yanzeng Li, Bowen Yu, Mengge Xue, Tingwen Liu\u2217 Institute of Information Engineering, Chinese Academy of Sciences School of Cyber Security, University of Chinese Academy of Sciences {liyanzeng, yubowen, xuemengge, liutingwen}@iie.ac.cn Abstract Most Chinese pre-trained models take charac- ter as the basic unit and learn representation according to character\u2019s external contexts, ig- noring the semantics expressed in the word, which is the smallest meaningful utterance in Chinese. Hence, we propose a novel word- aligned attention to exploit explicit word in- formation, which is complementary to vari- ous character-based Chinese pre-trained lan- guage models. Speci\ufb01cally, we devise a pool- ing mechanism to align the character-level at- tention to the word level and propose to alle- viate the potential issue of segmentation error propagation by multi-source information fu- sion. As a result, word and character informa- tion are explicitly integrated at the \ufb01ne-tuning procedure.",
  "As a result, word and character informa- tion are explicitly integrated at the \ufb01ne-tuning procedure. Experimental results on \ufb01ve Chi- nese NLP benchmark tasks demonstrate that our method achieves signi\ufb01cant improvements against BERT, ERNIE and BERT-wwm. 1 Introduction Pre-trained language Models (PLM) such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), ERNIE (Sun et al., 2019), BERT-wwm (Cui et al., 2019) and XLNet (Yang et al., 2019) have been proven to capture rich language information from text and then bene\ufb01t many NLP applications by simple \ufb01ne-tuning, including sentiment classi\ufb01ca- tion (Pang et al., 2002), natural language infer- ence (Bowman et al., 2015), named entity recogni- tion (Sang and De Meulder, 2003) and so on.",
  "Generally, most popular PLMs prefer to use at- tention mechanism (Vaswani et al., 2017) to rep- resent the natural language, such as word-to-word self-attention for English. Unlike English, in Chi- nese, words are not separated by explicit delimiters. Since without word boundaries information, it is \u2217Corresponding author intuitive to model characters in Chinese tasks di- rectly. However, in most cases, the semantic of a single Chinese character is ambiguous. For exam- ple, the character \u201c\u62cd\u201d in word \u201c\u7403\u62cd(bat)\u201d and \u201c\u62cd\u5356(auction)\u201d has entirely different meanings. Moreover, several recent works have demonstrated that considering the word segmentation informa- tion can lead to better language understanding, and accordingly bene\ufb01ts various Chinese tasks (Wang et al., 2017; Li et al., 2018; Zhang and Yang, 2018; Gui et al., 2019; Mengge et al., 2019). All these factors motivate us to expand the character-level attention mechanism in Chinese PLMs to represent the semantics of words 1.",
  "All these factors motivate us to expand the character-level attention mechanism in Chinese PLMs to represent the semantics of words 1. To this end, there are two main challenges. (1) How to seamlessly integrate the segmentation information into character-based attention module of PLM is an important problem. (2) Gold-standard segmen- tation is rarely available in the downstream tasks, and how to effectively reduce the cascading noise caused by Chinese word segmentation (CWS) tools (Li et al., 2019) is another challenge. In this paper, we propose a new architec- ture, named Multi-source Word Aligned Attention (MWA), to solve the above issues. (1) Psycholin- guistic experiments (Bai et al., 2008; Meng et al., 2014) have shown that readers are likely to pay approximate attention to each character in one Chi- nese word. Drawing inspiration from such \ufb01nd- ings, we introduce a novel word-aligned attention, which could aggregate attention weight of char- acters in one word into a uni\ufb01ed value with the mixed pooling strategy (Yu et al., 2014).",
  "Drawing inspiration from such \ufb01nd- ings, we introduce a novel word-aligned attention, which could aggregate attention weight of char- acters in one word into a uni\ufb01ed value with the mixed pooling strategy (Yu et al., 2014). (2) For reducing segmentation error, we further extend our word-aligned attention with multi-source segmen- tation produced by various segmenters and deploy 1Considering the enormous cost of re-training a language model, we hope to incorporate word segmentation information to the \ufb01ne-tuning process to enhance performance, and leave how to improve the pre-training procedure for a future work. arXiv:1911.02821v2  [cs.CL]  29 Apr 2020",
  "\ud835\udc50! \ud835\udc50\" \ud835\udc50# \ud835\udc50$ \ud835\udc50% \ud835\udc50& Character-based Model Encoder Tokenizer \ud835\udc50! \ud835\udc50\" \ud835\udc50# \ud835\udc50$ \ud835\udc50% \ud835\udc50& Gain partition  \ud835\udf0b \u2131 \ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65(\ud835\udc44\ud835\udc3e3 \ud835\udc51) Apply partition \ud835\udf0b \ud835\udc53 \ud835\udc53 \ud835\udc53 \ud835\udc53:Mix Pooling Enhanced Character Representation A78 A8 Figure 1: Architecture of Word-aligned Attention a fusion function to pull together their disparate outputs. As shown in Table 1, different CWS tools may have different annotation granularity. Through comprehensive consideration of multi-granularity segmentation results, we can implicitly reduce the error caused by automatic annotation. Extensive experiments are conducted on various Chinese NLP tasks including sentiment classi\ufb01ca- tion, named entity recognition, sentence pair match- ing, natural language inference and machine read- ing comprehension. The results and analysis show that the proposed method boosts BERT, ERNIE and BERT-wwm signi\ufb01cantly on all the datasets 2.",
  "The results and analysis show that the proposed method boosts BERT, ERNIE and BERT-wwm signi\ufb01cantly on all the datasets 2. 2 Methodology 2.1 Character-level Pre-trained Encoder The primary goal of this work is to inject the word segmentation knowledge into character-level Chi- nese PLMs and enhance original models. Given the strong performance of deep Transformers trained on language modeling, we adopt BERT and its up- dated variants (ERNIE, BERT-wwm) as the basic encoder in this work, and the outputs from the last layer of encoder are treated as the character-level enriched contextual representations H. 2.2 Word-aligned Attention Although character-level Chinese PLM has remark- able ability to capture language knowledge from text, it neglects the semantic information expressed in the word level. Therefore we apply a word- aligned layer on top of the encoder to integrate the 2The source code of this paper can be obtained from https://github.com/lsvih/MWA.",
  "Therefore we apply a word- aligned layer on top of the encoder to integrate the 2The source code of this paper can be obtained from https://github.com/lsvih/MWA. Chinese \u5317\u4eac\u897f\u5c71\u68ee\u6797\u516c\u56ed Segmenter thulac \u5317\u4eac \u897f\u5c71 \u68ee\u6797 \u516c\u56ed ictclas \u5317\u4eac \u897f \u5c71 \u68ee\u6797 \u516c\u56ed hanlp \u5317\u4eac \u897f\u5c71 \u68ee\u6797\u516c\u56ed Table 1: Results of different popular CWS tools over \u201c\u5317\u4eac\u897f\u5c71\u68ee\u6797\u516c\u56ed(Beijing west mount forest park)\u201d. word boundary information into the representation of characters with an attention aggregation module.",
  "word boundary information into the representation of characters with an attention aggregation module. For an input sequence with n characters S = [c1, c2, ..., cn], where cj denotes the j-th charac- ter, CWS tool \u03c0 is used to partition S into non- overlapping word blocks: \u03c0(S) = [w1, w2, ..., wm], (m \u2264n) (1) where wi = {cs, cs+1, ..., cs+l\u22121} is the i-th seg- mented word with a length of l and s is the index of wi\u2019s \ufb01rst character in S. We apply self-attention op- eration with the representations of all input charac- ters to get the character-level attention score matrix Ac \u2208Rn\u00d7n. It can be formulated as: Ac = F(H) = softmax((KWk)(QWq)T \u221a d ) (2) where Q and K are both equal to the collective representation H at the last layer of the Chinese PLM, Wk \u2208Rd\u00d7d and Wq \u2208Rd\u00d7d are trainable parameters for projection.",
  "While Ac models the re- lationship between two arbitrarily characters with- out regard to the word boundary, we argue that incorporating word as atom in the attention can bet- ter represent the semantics, as the literal meaning of each individual character can be quite different from the implied meaning of the whole word, and the simple weighted sum in the character level may lose word and word sequence information. To address this issue, we propose to align Ac in the word level and integrate the inner-word at- tention. For ease of exposition, we rewrite Ac as [a1 c, a2 c, ..., an c ], where ai c \u2208Rn denotes the i-th row vector of Ac, that is, ai c represents the attention score vector of the i-th character. Then we deploy \u03c0 to segment Ac according to \u03c0(S).",
  "Then we deploy \u03c0 to segment Ac according to \u03c0(S). For example, if \u03c0(S) = [{c1, c2}, {c3}, ..., {cn\u22121, cn}], then \u03c0(Ac) = [{a1 c, a2 c}, {a3 c}, ..., {an\u22121 c , an c }] (3) In this way, an attention vector sequence is divided into several subsequences and each sub- sequence represents the attention of one word.",
  "Then, motivated by the psycholinguistic \ufb01nding that readers are likely to pay similar attention to each character in one Chinese word, we devise an appropriate aggregation module to fuse the inner- word attention. Concretely, we \ufb01rst transform {as c, ..., as+l\u22121 c } into one attention vector ai w for wi with the mixed pooling strategy (Yu et al., 2014) 3. Then we execute the piecewise upsampling opera- tion over each ai w to keep input and output dimen- sions unchanged for the sake of plug and play.",
  "Then we execute the piecewise upsampling opera- tion over each ai w to keep input and output dimen- sions unchanged for the sake of plug and play. The detailed process can be summarized as: ai w = \u03bb Maxpooling({as c, ..., as+l\u22121 c }) (4) + (1 \u2212\u03bb) Meanpooling({as c, ..., as+l\u22121 c }) \u02c6Ac[s : s + l \u22121] = el \u2297ai w (5) where \u03bb \u2208R1 is a weighting trainable variable to balance the mean and max pooling, el = [1, ..., 1]T represents a l-dimensional all-ones vector, l is the length of wi, el \u2297ai w = [ai w, ..., ai w] denotes the kronecker product operation between el and ai w, \u02c6Ac \u2208Rn\u00d7n is the aligned attention matrix. Eqs. 4 and 5 can help incorporate word segmentation in- formation into character-level attention calculation process, and determine the attention vector of one character from the perspective of the whole word, which is bene\ufb01cial for eliminating the attention bias caused by character ambiguity.",
  "Eqs. 4 and 5 can help incorporate word segmentation in- formation into character-level attention calculation process, and determine the attention vector of one character from the perspective of the whole word, which is bene\ufb01cial for eliminating the attention bias caused by character ambiguity. Finally, we can obtain the enhanced character representation produced by word-aligned attention as follows: \u02c6H = \u02c6AcVWv (6) where V = H, Wv \u2208Rd\u00d7d is a trainable pro- jection matrix.",
  "Finally, we can obtain the enhanced character representation produced by word-aligned attention as follows: \u02c6H = \u02c6AcVWv (6) where V = H, Wv \u2208Rd\u00d7d is a trainable pro- jection matrix. Besides, we also use multi-head attention (Vaswani et al., 2017) to capture informa- tion from different representation subspaces jointly, thus we have K different aligned attention matrices \u02c6A k c(1 \u2264k \u2264K) and corresponding representation \u02c6H k. With multi-head attention architecture, the output can be expressed as follows: H = Concat(\u02c6H 1, \u02c6H 2, ..., \u02c6H K)Wo (7) 2.3 Multi-source Word-aligned Attention As mentioned in Section 1, our proposed word- aligned attention relies on the segmentation results 3Other pooling methods such as max pooling or mean pooling also works. Here we choose mixed pooling because it has the advantages of distilling the global and the most prominent features in one word at the same time.",
  "Here we choose mixed pooling because it has the advantages of distilling the global and the most prominent features in one word at the same time. of CWS tool \u03c0. Unfortunately, a segmenter is usu- ally unreliable due to the risk of ambiguous and non-formal input, especially on out-of-domain data, which may lead to error propagation and an un- satisfactory model performance. In practice, the ambiguous distinction between morphemes and compound words leads to the cognitive divergence of words concepts, thus different \u03c0 may provide diverse \u03c0(S) with various granularities. To re- duce the impact of segmentation error and effec- tively mine the common knowledge of different seg- menters, it\u2019s natural to enhance the word-aligned attention layer with multi-source segmentation in- puts. Formally, assume that there are M popular CWS tools employed, we can obtain M different representations H1, ..., HM by Eq. 7.",
  "Formally, assume that there are M popular CWS tools employed, we can obtain M different representations H1, ..., HM by Eq. 7. Then we propose to fuse these semantically different repre- sentations as follows: \u02dcH = M X m=1 tanh(HmWg) (8) where Wg is a parameter matrix and \u02dcH denotes the \ufb01nal output of the MWA attention layer. 3 Experiments 3.1 Experiments Setup To test the applicability of the proposed MWA at- tention, we choose three publicly available Chinese pre-trained models as the basic encoder: BERT, ERNIE, and BERT-wwm. In order to make a fair comparison, we keep the same hyper-parameters (such maximum length, warm-up steps, initial learning rate, etc.) as suggested in BERT-wwm (Cui et al., 2019) for both baselines and our method on each dataset. We run the same experiment for \ufb01ve times and report the average score to ensure the reliability of results.",
  "as suggested in BERT-wwm (Cui et al., 2019) for both baselines and our method on each dataset. We run the same experiment for \ufb01ve times and report the average score to ensure the reliability of results. Besides, three popular CWS tools: thulac (Sun et al., 2016), ictclas (Zhang et al., 2003) and hanlp (He, 2014) are employed to segment sequence. The experiments are carried out on \ufb01ve Chinese NLP tasks and six public benchmark datasets: Sentiment Classi\ufb01cation (SC): We adopt ChnSentiCorp4 and weibo-100k sentiment dataset5 in this task. ChnSentiCorp dataset has about 10k sentences, which express positive or negative emo- tion. weibo-100k dataset contains 1.2M microblog 4https://github.com/pengming617/bert_ classification 5https://github.com/SophonPlus/ ChineseNlpCorpus/",
  "Dataset Task Max length Batch size Epoch lr\u2217 Dataset Size Train Dev Test ChnSentiCorp SC 256 16 3 3 \u00d7 10\u22125 9.2K 1.2K 1.2K weibo-100k 128 64 2 2 \u00d7 10\u22125 100K \u223c10K 10K ontonotes NER 256 16 5 3 \u00d7 10\u22125 15.7K 4.3K 4.3K LCQMC SPM 128 64 3 3 \u00d7 10\u22125 \u223c239K 8.8K 12.5K XNLI NLI 128 64 2 3 \u00d7 10\u22125 \u223c392K 2.5K 2.5K DRCD MRC 512 16 2 3 \u00d7 10\u22125 27K 3.5K 3.5K Table 2: Summary of datasets and the corresponding hyper-parameters setting. Reported learning rates\u2217are the initial values of BertAdam. texts and each microblog is tagged as positive or negative emotion.",
  "Reported learning rates\u2217are the initial values of BertAdam. texts and each microblog is tagged as positive or negative emotion. Named Entity Recognition (NER): this task is to test model\u2019s capacity of sequence tagging. We use a common public dataset Ontonotes 4.0 (Weischedel et al., 2011) in this task. Sentence Pair Matching (SPM): We use the most widely used dataset LCQMC (Liu et al., 2018) in this task, which aims to identify whether two questions are in a same intention. Natural Language Inference (NLI): this task is to exploit the contexts of text and concern infer- ence relationships between sentences. XNLI (Con- neau et al., 2018) is a cross-language language understanding dataset; we only use the Chinese language part of XNLI to evaluate the language un- derstanding ability. And we processed this dataset in the same way as ERNIE (Sun et al., 2019) did. Machine Reading Comprehension (MRC): MRC is a representative document-level modeling task which requires to answer the questions based on the given passages.",
  "And we processed this dataset in the same way as ERNIE (Sun et al., 2019) did. Machine Reading Comprehension (MRC): MRC is a representative document-level modeling task which requires to answer the questions based on the given passages. DRCD (Shao et al., 2018) is a public span-extraction Chinese MRC dataset, whose answers are spans in the document. We implement our model with PyTorch (Paszke et al., 2019), and all baselines are converted weights into PyTorch version. All experiments employ modi\ufb01ed Adam (Devlin et al., 2019) as optimizer with 0.01 weight decay and 0.1 warm- up ratio. All pre-trained models are con\ufb01gured to 12 layers and 768 hidden dimension. The detail settings are shown in Table 2. 3.2 Experiment Results Table 3 shows the performances on \ufb01ve classical Chinese NLP tasks with six public datasets. Gener- ally, our method consistently outperforms all base- lines on all \ufb01ve tasks, which demonstrates the effec- tiveness and universality of the proposed approach.",
  "3.2 Experiment Results Table 3 shows the performances on \ufb01ve classical Chinese NLP tasks with six public datasets. Gener- ally, our method consistently outperforms all base- lines on all \ufb01ve tasks, which demonstrates the effec- tiveness and universality of the proposed approach. Moreover, the Wilcoxon\u2019s test shows that a signi\ufb01- cant difference (p < 0.05) exits between our model and baseline models. In detail, on the two datasets of SC task, we ob- serve an average of 0.53% and 0.83% absolute im- provement in F1 score, respectively. SPM and NLI tasks can also gain bene\ufb01ts from our enhanced rep- resentation. For the NER task, our method obtains 0.92% improvement averagely over all baselines. Besides, introducing word segmentation informa- tion into the encoding of character sequences im- proves the MRC performance on average by 1.22 points and 1.65 points in F1 and Exact Match (EM) score respectively.",
  "Besides, introducing word segmentation informa- tion into the encoding of character sequences im- proves the MRC performance on average by 1.22 points and 1.65 points in F1 and Exact Match (EM) score respectively. We attribute such signi\ufb01cant gain in NER and MRC to the particularity of these two tasks. Intuitively, Chinese NER is correlated with word segmentation, and named entity bound- aries are also word boundaries. Thus the potential boundary information presented by the additional segmentation input can provide better guidance to label each character, which is consistent with the conclusion in (Zhang and Yang, 2018). Similarly, the span-extraction MRC task is to extract answer spans from document (Shao et al., 2018), which also faces the same word boundary problem as NER, and the long sequence in MRC exacerbates the problem. Therefore, our method gets a rela- tively greater improvement on the DRCD dataset. 3.3 Ablation Study To demonstrate the effectiveness of our multi- source fusion method, we carry out experiments on the DRCD dev set with different segmentation in- puts.",
  "Therefore, our method gets a rela- tively greater improvement on the DRCD dataset. 3.3 Ablation Study To demonstrate the effectiveness of our multi- source fusion method, we carry out experiments on the DRCD dev set with different segmentation in- puts. Besides, we also design two strong baselines by introducing a Transformer layer (1T) and a ran- dom tokenizer model (WArandom) to exclude the bene\ufb01ts from additional parameters. As shown in Table 4, adding additional parameters by introduc- ing an extra transformer layer can bene\ufb01t the PLMs. Compared with 1T and WArandom, our proposed word-aligned attention gives quite stable improve- ments no matter what CWS tool we use, which again con\ufb01rms the effectiveness and rationality of incorporating word segmentation information into character-level PLMs. Another observation is that",
  "Task SC NER SPM NLI MRC Dataset ChnSenti2,3 weibo-100k2 Ontonotes4 LCQMC2,3,4 XNLI1,2,3,4 DRCD2,3 [EM|F1] Prev. SOTA\u2020 93.1(2019a) - 74.89(2019b) 85.68(2019c) 67.5(2017d) 75.12(2019e) 87.26(2019e) BERT 94.72 97.31 79.18 86.50 78.19 85.57 91.16 +MWA 95.34(+0.62) 98.14(+0.83) 79.86(+0.68) 86.92(+0.42) 78.42(+0.23) 86.86(+1.29) 92.22(+1.06) BERT-wwm 94.38 97.36 79.28 86.11 77.92 84.11 90.46 +MWA 95.",
  "42(+0.23) 86.86(+1.29) 92.22(+1.06) BERT-wwm 94.38 97.36 79.28 86.11 77.92 84.11 90.46 +MWA 95.01(+0.63) 98.13(+0.77) 80.32(+1.04) 86.28(+0.17) 78.68(+0.76) 87.00(+2.89) 92.21(+1.75) ERNIE 95.17 97.30 77.74 87.27 78.04 87.85 92.85 +MWA 95.52(+0.35) 98.18(+0.88) 78.78(+1.04) 88.73(+1.46) 78.71(+0.67) 88.61(+0.76) 93.72(+0.87) Table 3: Evaluation results regarding each model on different datasets. Bold marks highest number among all models. Numbers in brackets indicate the absolute increase over baseline models.",
  "46) 78.71(+0.67) 88.61(+0.76) 93.72(+0.87) Table 3: Evaluation results regarding each model on different datasets. Bold marks highest number among all models. Numbers in brackets indicate the absolute increase over baseline models. Superscript number1,2,3,4 respectively represents that the corresponding dataset is also used by BERT (Devlin et al., 2019), BERT-wwm (Wu et al., 2016; Cui et al., 2019), ERNIE (Sun et al., 2019) and Glyce (Meng et al., 2019a), respectively. The results of all baselines are produced by our implementation or retrieved from original papers, and we report the higher one among them. The improvements over baselines are statistically signi\ufb01cant (p < 0.05). \u2020 denotes the results of previous state-of-the-art models on these datasets without using BERT.",
  "The improvements over baselines are statistically signi\ufb01cant (p < 0.05). \u2020 denotes the results of previous state-of-the-art models on these datasets without using BERT. Model BERT BERT-wwm ERNIE Original 92.06 91.68 92.61 +1T 92.37 92.22 93.42 +WArandom 91.83 90.33 92.12 +WAthulac 92.84 92.73 93.89 +WAictclas 93.05 92.90 93.75 +WAhanlp 92.91 93.21 93.91 +MWA 93.59 93.72 94.21 Table 4: F1 results of ablation experiments on the DRCD dev set. employing multiple segmenters and fusing them together could introduce richer segmentation infor- mation and further improve the performance.",
  "employing multiple segmenters and fusing them together could introduce richer segmentation infor- mation and further improve the performance. 3.4 Parameter Scale Analysis For fair comparison and demonstrating the im- provement of our model is not only rely on more trainable parameters, we also conduct experiments on the DRCD dev set to explore whether the per- formance keeps going-up with more parameters by introducing additional transformer blocks on top of the representations of PLMs. Model F1 Param. Number BERT-wwm 91.68 110M BERT-wwm+1T 92.23 110M+7.1M BERT-wwm+2T 91.99 110M+14.2M BERT-wwm+3T 91.68 110M+21.3M BERT-wwm+MWA 93.72 110M+7.6M Robust-BERT-wwm-ext-large 94.40 340M Table 5: Comparison on the DRCD dev set. The nT denotes the number of additional transformer layers.",
  "The nT denotes the number of additional transformer layers. In Table 5, +1T denotes that we introduce an- other one Transformer layer on top of BERT-wwm and +2T means additional 2 layers, M denotes million. As the experimental results showed, when the number of additional layers exceeds 1, the per- formance starts to decline, which demonstrates that using an extensive model on top of the PLM representations may not bring additional bene\ufb01ts. We can conclude that MWA doesn\u2019t introduce too many parameters, and MWA achieves better perfor- mance than +1T under the similar parameter num- bers. Besides, we also make comparison with the current best Chinese PLM: Robust-BERT-wwm- ext-large (Cui et al., 2019), a 24-layers Chinese PLM with 13.5 times more pre-training data and 3.1 times more parameters than BERT-wwm, ex- perimental results show that our model can achieve comparable performance, which again con\ufb01rms the effectiveness of incorporating word segmentation information into character-level PLMs.",
  "4 Conclusion In this paper, we develop a novel Multi-source Word Aligned Attention model (referred as MWA), which integrates word segmentation information into character-level self-attention mechanism to enhance the \ufb01ne-tuning performance of Chinese PLMs. We conduct extensive experiments on \ufb01ve NLP tasks with six public datasets. The proposed approach yields substantial improvements com- pared to BERT, BERT-wwm and ERNIE, demon- strating its effectiveness and universality. Further- more, the word-aligned attention can also be ap- plied to English PLMs to bridge the semantic gap between the whole word and the segmented Word- Piece tokens, which we leave for future work. Acknowledgement We would like to thank reviewers for their insight- ful comments. This work is supported by the Strate- gic Priority Research Program of Chinese Academy of Sciences, Grant No. XDC02040400.",
  "References Xuejun Bai, Guoli Yan, Simon P Liversedge, Chuanli Zang, and Keith Rayner. 2008. Reading spaced and unspaced chinese text: Evidence from eye move- ments. Journal of Experimental Psychology: Hu- man Perception and Performance, 34(5):1277. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large anno- tated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empiri- cal Methods in Natural Language Processing, pages 632\u2013642, Lisbon, Portugal. Association for Compu- tational Linguistics. Alexis Conneau, Ruty Rinott, Guillaume Lample, Ad- ina Williams, Samuel R. Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. Xnli: Evaluating cross- lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natu- ral Language Processing. Association for Computa- tional Linguistics.",
  "2018. Xnli: Evaluating cross- lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natu- ral Language Processing. Association for Computa- tional Linguistics. Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, and Guoping Hu. 2019. Pre-training with whole word masking for chinese bert. arXiv preprint arXiv:1906.08101. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics. Tao Gui, Ruotian Ma, Qi Zhang, Lujun Zhao, Yu-Gang Jiang, and Xuanjing Huang. 2019.",
  "Associ- ation for Computational Linguistics. Tao Gui, Ruotian Ma, Qi Zhang, Lujun Zhao, Yu-Gang Jiang, and Xuanjing Huang. 2019. Cnn-based chi- nese ner with lexicon rethinking. In Proceedings of the 28th International Joint Conference on Arti\ufb01cial Intelligence, pages 4982\u20134988. AAAI Press. Tao Gui, Yicheng Zou, Qi Zhang, Minlong Peng, Jinlan Fu, Zhongyu Wei, and Xuanjing Huang. 2019b. A lexicon-based graph neural network for chinese ner. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 1039\u2013 1049. Han He. 2014. HanLP: Han Language Processing. Qiang Huang, Jianhui Bu, Weijian Xie, Shengwen Yang, Weijia Wu, and Liping Liu. 2019c. Multi-task sentence encoding model for semantic retrieval in question answering systems.",
  "2014. HanLP: Han Language Processing. Qiang Huang, Jianhui Bu, Weijian Xie, Shengwen Yang, Weijia Wu, and Liping Liu. 2019c. Multi-task sentence encoding model for semantic retrieval in question answering systems. In 2019 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. Chia-Hsuan Lee and Hung-Yi Lee. 2019e. Cross- lingual transfer learning for question answering. Xiaoya Li, Yuxian Meng, Xiaofei Sun, Qinghong Han, Arianna Yuan, and Jiwei Li. 2019. Is word segmen- tation necessary for deep learning of chinese repre- sentations? In Proceedings of the 57th Annual Meet- ing of the Association for Computational Linguistics, pages 3242\u20133252. Yanzeng Li, Tingwen Liu, Diying Li, Quangang Li, Jin- qiao Shi, and Yanqiu Wang. 2018. Character-based bilstm-crf incorporating pos and dictionaries for chi- nese opinion target extraction.",
  "Yanzeng Li, Tingwen Liu, Diying Li, Quangang Li, Jin- qiao Shi, and Yanqiu Wang. 2018. Character-based bilstm-crf incorporating pos and dictionaries for chi- nese opinion target extraction. In Asian Conference on Machine Learning, pages 518\u2013533. Xin Liu, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Dongfang Li, and Buzhou Tang. 2018. Lcqmc: A large-scale chinese question matching corpus. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1952\u20131962. Hongxia Meng, Xuejun Bai, Chuanli Zang, and Guoli Yan. 2014. Landing position effects of coordinate and attributive structure compound words. Acta Psy- chologica Sinica, 46(1):36\u201349. Yuxian Meng, Wei Wu, Fei Wang, Xiaoya Li, Ping Nie, Fan Yin, Muyu Li, Qinghong Han, Xiaofei Sun, and Jiwei Li. 2019a.",
  "Yuxian Meng, Wei Wu, Fei Wang, Xiaoya Li, Ping Nie, Fan Yin, Muyu Li, Qinghong Han, Xiaofei Sun, and Jiwei Li. 2019a. Glyce: Glyph-vectors for chinese character representations. In NeurIPS 2019 : Thirty- third Conference on Neural Information Processing Systems, pages 2746\u20132757. Xue Mengge, Yu Bowen, Liu Tingwen, Wang Bin, Meng Erli, and Li Quangang. 2019. Porous lattice- based transformer encoder for chinese ner. Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classi\ufb01cation using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 79\u201386. Asso- ciation for Computational Linguistics.",
  "2002. Thumbs up?: sentiment classi\ufb01cation using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 79\u201386. Asso- ciation for Computational Linguistics. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Te- jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. Pytorch: An imperative style, high-performance deep learn- ing library. In Advances in Neural Information Pro- cessing Systems 32, pages 8024\u20138035. Curran Asso- ciates, Inc. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018.",
  "Curran Asso- ciates, Inc. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. In Proc. of NAACL. Erik F Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: language-independent named entity recognition. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4, pages 142\u2013147. Asso- ciation for Computational Linguistics.",
  "Chih Chieh Shao, Trois Liu, Yuting Lai, Yiying Tseng, and Sam Tsai. 2018. Drcd: a chinese machine reading comprehension dataset. arXiv preprint arXiv:1806.00920. Maosong Sun, Xinxiong Chen, Kaixu Zhang, Zhipeng Guo, and Zhiyuan Liu. 2016. Thulac: An ef\ufb01cient lexical analyzer for chinese. Technical report. Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua Wu. 2019. Ernie: Enhanced rep- resentation through knowledge integration. arXiv preprint arXiv:1904.09223. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need.",
  "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information pro- cessing systems, pages 5998\u20136008. Shaonan Wang, Jiajun Zhang, and Chengqing Zong. 2017. Exploiting word internal structures for generic Chinese sentence representation. In Pro- ceedings of the 2017 Conference on Empirical Meth- ods in Natural Language Processing, pages 298\u2013 303, Copenhagen, Denmark. Association for Com- putational Linguistics. Zhiguo Wang, Wael Hamza, and Radu Florian. 2017d. Bilateral multi-perspective matching for natural lan- guage sentences. In Twenty-Sixth International Joint Conference on Arti\ufb01cial Intelligence, pages 4144\u20134150.",
  "Zhiguo Wang, Wael Hamza, and Radu Florian. 2017d. Bilateral multi-perspective matching for natural lan- guage sentences. In Twenty-Sixth International Joint Conference on Arti\ufb01cial Intelligence, pages 4144\u20134150. Ralph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Ni- anwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, et al. 2011. Ontonotes 4.0. Linguistic Data Consortium LDC2011T03. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google\u2019s neural machine translation system: Bridging the gap between hu- man and machine translation. arXiv preprint arXiv:1609.08144.",
  "2016. Google\u2019s neural machine translation system: Bridging the gap between hu- man and machine translation. arXiv preprint arXiv:1609.08144. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car- bonell, Ruslan Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretrain- ing for language understanding. arXiv preprint arXiv:1906.08237. Dingjun Yu, Hanli Wang, Peiqiu Chen, and Zhihua Wei. 2014. Mixed pooling for convolutional neu- ral networks. In International Conference on Rough Sets and Knowledge Technology, pages 364\u2013375. Springer. Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun Liu. 2003. Hhmm-based chinese lexical analyzer ict- clas. In Proceedings of the second SIGHAN work- shop on Chinese language processing-Volume 17, pages 184\u2013187. Association for Computational Lin- guistics.",
  "2003. Hhmm-based chinese lexical analyzer ict- clas. In Proceedings of the second SIGHAN work- shop on Chinese language processing-Volume 17, pages 184\u2013187. Association for Computational Lin- guistics. Yue Zhang and Jie Yang. 2018. Chinese ner using lat- tice lstm. In Proceedings of the 56th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1554\u20131564."
]