{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "CM-Net: A Novel Collaborative Memory Network for Spoken Language Understanding Yijin Liu1\u2217, Fandong Meng2, Jinchao Zhang2, Jie Zhou2, Yufeng Chen1 and Jinan Xu1\u2020 1Beijing Jiaotong University, China 2Pattern Recognition Center, WeChat AI, Tencent Inc, China adaxry@gmail.com {fandongmeng, dayerzhang, withtomzhou}@tencent.com {chenyf, jaxu}@bjtu.edu.cn Abstract Spoken Language Understanding (SLU) mainly involves two tasks, intent detection and slot \ufb01lling, which are generally modeled jointly in existing works. However, most existing models fail to fully utilize co- occurrence relations between slots and intents, which restricts their potential performance. To address this issue, in this paper we propose a novel Collaborative Memory Network (CM-Net) based on the well-designed block, named CM-block.",
      "However, most existing models fail to fully utilize co- occurrence relations between slots and intents, which restricts their potential performance. To address this issue, in this paper we propose a novel Collaborative Memory Network (CM-Net) based on the well-designed block, named CM-block. The CM-block \ufb01rstly cap- tures slot-speci\ufb01c and intent-speci\ufb01c features from memories in a collaborative manner, and then uses these enriched features to enhance local context representations, based on which the sequential information \ufb02ow leads to more speci\ufb01c (slot and intent) global utterance representations. Through stacking multiple CM-blocks, our CM-Net is able to alternately perform information exchange among speci\ufb01c memories, local contexts and the global utterance, and thus incrementally enriches each other. We evaluate the CM-Net on two standard benchmarks (ATIS and SNIPS) and a self-collected corpus (CAIS). Experimental results show that the CM-Net achieves the state-of-the-art results on the ATIS and SNIPS in most of criteria, and signi\ufb01cantly outperforms the baseline models on the CAIS.",
      "Experimental results show that the CM-Net achieves the state-of-the-art results on the ATIS and SNIPS in most of criteria, and signi\ufb01cantly outperforms the baseline models on the CAIS. Additionally, we make the CAIS dataset publicly available for the research community 1. 1 Introduction Spoken Language Understanding (SLU) is a core component in dialogue systems. It typically aims to identify the intent and semantic constituents \u2217This work was done when Yijin Liu was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China \u2020Jinan Xu is the corresponding author of the paper. 1Code is available at: https://github.com/Adaxry/CM- Net. playlist_owner artist playlist music_item sort served_dish time_range temperature \u014f \u000b6ORW\u0003WDJV\f \u000b,QWHQW\u0003ODEHOV\f AddToPlaylist PlayMusic BookRestaurant GetWeather \u014f Figure 1: Statistical association of slot tags (on the left) and intent labels (on the right) in the SNIPS, where col- ors indicate different intents and thicknesses of lines indicate proportions.",
      "for a given utterance, which are referred as in- tent detection and slot \ufb01lling, respectively. Past years have witnessed rapid developments in di- verse deep learning models (Haffner et al., 2003; Sarikaya et al., 2011) for SLU. To take full ad- vantage of supervised signals of slots and intents, and share knowledge between them, most of ex- isting works apply joint models that mainly based on CNNs (Xu and Sarikaya, 2013; Gupta et al., 2019), RNNs (Guo et al., 2014a; Liu and Lane, 2016), and asynchronous bi-model (Wang et al., 2018). Generally, these joint models encode words convolutionally or sequentially, and then aggre- gate hidden states into a utterance-level represen- tation for the intent prediction, without interac- tions between representations of slots and intents. Intuitively, slots and intents from similar \ufb01elds tend to occur simultaneously, which can be ob- served from Figure 1 and Table 1.",
      "Intuitively, slots and intents from similar \ufb01elds tend to occur simultaneously, which can be ob- served from Figure 1 and Table 1. Therefore, it is bene\ufb01cial to generate the representations of slots and intents with the guidance from each other. Some works explore enhancing the slot \ufb01lling task arXiv:1909.06937v1  [cs.CL]  16 Sep 2019",
      "# Utterance Slot tag Intent 1 play Roy Orbison tunes now artist PlayMusic 2 add this Roy Orbison song onto Women of Comedy artist AddToPlaylist 3 book a spot for seven at a bar with chicken french served dish BookRestaurant 4 book french food for me and angeline at a restaurant cuisine BookRestaurant Table 1: Examples in SNIPS with annotations of intent label for the utterance and slot tags for partial words. unidirectionally with the guidance from intent rep- resentations via gating mechanisms (Goo et al., 2018; Li et al., 2018), while the predictions of in- tents lack the guidance from slots. Moreover, the capsule network with dynamic routing algorithms (Zhang et al., 2018a) is proposed to perform in- teractions in both directions. However, there are still two limitations in this model. The one is that the information \ufb02ows from words to slots, slots to intents and intents to words in a pipeline manner, which is to some extent limited in capturing com- plicated correlations among words, slots and in- tents.",
      "However, there are still two limitations in this model. The one is that the information \ufb02ows from words to slots, slots to intents and intents to words in a pipeline manner, which is to some extent limited in capturing com- plicated correlations among words, slots and in- tents. The other is that the local context informa- tion which has been shown highly useful for the slot \ufb01lling (Mesnil et al., 2014), is not explicitly modeled. In this paper, we try to address these issues, and thus propose a novel Collaborative Memory Network, named CM-Net. The main idea is to directly capture semantic relationships among words, slots and intents, which is conducted si- multaneously at each word position in a collabora- tive manner. Speci\ufb01cally, we alternately perform information exchange among the task-speci\ufb01c fea- tures referred from memories, local context rep- resentations and global sequential information via the well-designed block, named CM-block, which consists of three computational components: \u2022 Deliberate Attention: Obtaining slot- speci\ufb01c and intent-speci\ufb01c representations from memories in a collaborative manner.",
      "\u2022 Local Calculation: Updating local context representations with the guidances of the re- ferred slot and intent representations in the previous Deliberate Attention. \u2022 Global Recurrence: Generating speci\ufb01c (slot and intent) global sequential represen- tations based on local context representations from the previous Local Calculation. Above components in each CM-block are con- ducted consecutively, which are responsible for encoding information from different perspectives. Finally, multiple CM-blocks are stacked together, and construct our CM-Net. We \ufb01rstly conduct experiments on two popu- lar benchmarks, SNIPS (Coucke et al., 2018) and ATIS (Hemphill et al., 1990; Tur et al., 2010). Ex- perimental results show that the CM-Net achieves the state-of-the-art results in 3 of 4 criteria (e.g., intent detection accuracy on ATIS) on both bench- marks. Additionally, trials on our self-collected dataset, named CAIS, demonstrate the effective- ness and generalizability of the CM-Net.",
      "Additionally, trials on our self-collected dataset, named CAIS, demonstrate the effective- ness and generalizability of the CM-Net. Our main contributions are as follows: \u2022 We propose a novel CM-Net for SLU, which explicitly captures semantic correla- tions among words, slots and intents in a collaborative manner, and incrementally en- riches the speci\ufb01c features, local context rep- resentations and global sequential represen- tations through stacked CM-blocks. \u2022 Our CM-Net achieves the state-of-the-art re- sults on two major SLU benchmarks (ATIS and SNIPS) in most of criteria. \u2022 We contribute a new corpus CAIS with man- ual annotations of slot tags and intent labels to the research community. 2 Background In principle, the slot \ufb01lling is treated as a sequence labeling task, and the intent detection is a clas- si\ufb01cation problem.",
      "\u2022 We contribute a new corpus CAIS with man- ual annotations of slot tags and intent labels to the research community. 2 Background In principle, the slot \ufb01lling is treated as a sequence labeling task, and the intent detection is a clas- si\ufb01cation problem. Formally, given an utterance X = {x1, x2, \u00b7 \u00b7 \u00b7 , xN} with N words and its cor- responding slot tags Y slot = {y1, y2, \u00b7 \u00b7 \u00b7 , yN}, the slot \ufb01lling task aims to learn a parameterized mapping function f\u03b8 : X \u2192Y from input words to slot tags. For the intent detection, it is designed to predict the intent label \u02c6yint for the entire utter- ance X from the prede\ufb01ned label set Sint. Typically, the input utterance is \ufb01rstly encoded into a sequence of distributed representations X =",
      "{x1, x2, \u00b7 \u00b7 \u00b7 , xN} by character-aware and pre- trained word embeddings. Afterwards, the follow- ing bidirectional RNNs are applied to encode the embeddings X into context-sensitive representa- tions H = {h1, h2, \u00b7 \u00b7 \u00b7 , hN}. An external CRF (Lafferty et al., 2001) layer is widely utilized to calculate conditional probabilities of slot tags: p(yslot|H) = eF(H,yslot) P eyslot\u2208Yx eF(H,eyslot) (1) Here Yx is the set of all possible sequences of tags, and F(\u00b7) is the score function calculated by: F(h, y) = N X i=1 Ayi,yi+1 + N X i=1 Pi,yi (2) where A is the transition matrix that Ai,j indicates the score of a transition from i to j, and P is the score matrix output by RNNs. Pi,j indicates the score of the jth tag of the ith word in a sentence (Lample et al., 2016).",
      "Pi,j indicates the score of the jth tag of the ith word in a sentence (Lample et al., 2016). When testing, the Viterbi algorithm (Forney, 1973) is used to search the sequence of slot tags with maximum score: \u02c6yslot = arg max eyslot\u2208Yx F(H, eyslot) (3) As to the prediction of intent, the word-level hidden states H are \ufb01rstly summarized into a utterance-level representation vint via mean pool- ing (or max pooling or self-attention, etc.): vint = 1 N N X i=1 ht (4) The most probable intent label \u02c6yint is predicted by softmax normalization over the intent label set: \u02c6yint = arg max ey\u2208Sint P(ey|vint) P(ey = j|vint) = softmax(vint)[j] (5) Generally, both tasks are trained jointly to min- imize the sum of cross entropy from each individ- ual task.",
      "Formally, the loss function of the join model is computed as follows: L = (1 \u2212\u03bb) \u00b7 Lslot + \u03bb \u00b7 Lint Lint = \u2212 |Sint| X i=1 \u02c6yint i log(yint i ) Lslot = \u2212 N X j=1 |Sslot| X i=1 \u02c6yslot i,j log(yslot i,j ) (6) CM-block \u00b7\u00b7\u00b7 Slot Memory Intent Memory Inference Layer # 2 # 1 # L Embedding Layer CM-block x1 \u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 CM-block y1 yN yt \u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 y2 yint  slots  x2 xt xN Figure 2: Overview of our proposed CM-Net. The in- put utterance is \ufb01rstly encoded with the Embedding Layer (bottom), and then is transformed by multiple CM-blocks with the assistance of both slot and intent memories (on both sides). Finally we make predictions of slots and the intent in the Inference Layer (top).",
      "The in- put utterance is \ufb01rstly encoded with the Embedding Layer (bottom), and then is transformed by multiple CM-blocks with the assistance of both slot and intent memories (on both sides). Finally we make predictions of slots and the intent in the Inference Layer (top). where yint i and yslot i,j are golden labels, and \u03bb is hy- perparameter, and |Sint| is the size of intent label set, and similarly for |Sslot| . 3 CM-Net 3.1 Overview In this section, we start with a brief overview of our CM-Net and then proceed to introduce each module. As shown in Figure 2, the input utterance is \ufb01rstly encoded with the Embedding Layer, and then is transformed by multiple CM-blocks with the assistance of slot and intent memories, and \ufb01- nally make predictions in the Inference Layer. 3.2 Embedding Layers Pre-trained Word Embedding The pre-trained word embeddings has been indicated as a de-facto standard of neural network architectures for vari- ous NLP tasks.",
      "3.2 Embedding Layers Pre-trained Word Embedding The pre-trained word embeddings has been indicated as a de-facto standard of neural network architectures for vari- ous NLP tasks. We adapt the cased, 300d Glove2 (Pennington et al., 2014) to initialize word embed- dings, and keep them frozen. Character-aware Word Embedding It has been demonstrated that character level informa- tion (e.g. capitalization and pre\ufb01x) (Collobert et al., 2011) is crucial for sequence labeling. We use one layer of CNN followed by max pooling to generate character-aware word embeddings. 3.3 CM-block The CM-block is the core module of our CM-Net, which is designed with three computational com- 2https://nlp.stanford.edu/projects/glove/",
      "ponents: Deliberate Attention, Local Calculation and Global Recurrence respectively. Deliberate Attention To fully model semantic relations between slots and intents, we build the slot memory Mslot and intent memory Mint, and further devise a collab- orative retrieval approach. For the slot memory, it keeps |Sslot| slot cells which are randomly ini- tialized and updated as model parameters. Simi- larly for the intent memory. At each word position, we take the hidden state ht as query, and obtain slot feature hslot t and intent feature hint t from both memories by the deliberate attention mechanism, which will be illustrated in the following. Speci\ufb01cally for the slot feature hslot t , we \ufb01rstly get a rough intent representation ehint t by the word- aware attention with hidden state ht over the in- tent memory Mint, and then obtain the \ufb01nal slot feature hslot t by the intent-aware attention over the slot memory Mslot with the intent-enhanced representation [ht; ehint t ].",
      "Formally, the above- mentioned procedures are computed as follows: ehint t = ATT(ht, Mint) hslot t = ATT([ht; ehint t ], Mslot) (7) where ATT(\u00b7) is the query function calculated by the weighted sum of all cells mx i in memory Mx (x \u2208{slot, int}) : ATT(ht, Mx) = X i \u03b1imx i \u03b1i = exp(u\u22a4si) P j exp(u\u22a4sj) si = h\u22a4 t Wmx i (8) Here u and W are model parameters. We name the above calculations of two-round attentions (Equation 7) as \u201cdeliberate attention\u201d. The intent representation hint t is computed by the deliberate attention as well: ehslot t = ATT(ht, Mslot) hint t = ATT([ht; ehslot t ], Mint) (9) These two deliberate attentions are conducted simultaneously at each word position in such collaborative manner, which guarantees adequate knowledge diffusions between slots and intents. The retrieved slot features Hslot t and intent fea- tures Hint t are utilized to provide guidances for the next local calculation layer.",
      "The retrieved slot features Hslot t and intent fea- tures Hint t are utilized to provide guidances for the next local calculation layer. \u00b7\u00b7\u00b7 Hidden states Slot features Intent features \u00b7\u00b7\u00b7 ht-1 ht \u00b7\u00b7\u00b7 ht+1 \u00b7\u00b7\u00b7 ht-1 slot ht slot \u00b7\u00b7\u00b7 ht+1 slot ht-1 int ht int \u00b7\u00b7\u00b7 ht+1 int  Embeddings \u00b7\u00b7\u00b7 xt-1 xt \u00b7\u00b7\u00b7 xt+1 Bi-LSTM  Slot Memory  Intent Memory Global  Recurrence Deliberate Attention Local Calculation Output states Figure 3: The internal structure of our CM-Block, which is composed of deliberate attention, local cal- culation and global recurrent respectively. Local Calculation Local context information is highly useful for se- quence modeling (Kurata et al., 2016; Wang et al., 2016b). Zhang et al. (2018b) propose the S-LSTM to encode both local and sentence-level informa- tion simultaneously, and it has been shown more powerful for text representation when compared with the conventional BiLSTMs.",
      "Zhang et al. (2018b) propose the S-LSTM to encode both local and sentence-level informa- tion simultaneously, and it has been shown more powerful for text representation when compared with the conventional BiLSTMs. We extend the S- LSTM with slot-speci\ufb01c features Hslot t and intent- speci\ufb01c features Hslot t retrieved from memories. Speci\ufb01cally, at each input position t, we take the local window context \u03bet, word embedding xt, slot feature hslot t and intent feature hint t as inputs to conduct combinatorial calculation simultaneously. Formally, in the lth layer, the hidden state ht is updated as follows: \u03bel\u22121 t = [hl\u22121 t\u22121, hl\u22121 t ,",
      "Formally, in the lth layer, the hidden state ht is updated as follows: \u03bel\u22121 t = [hl\u22121 t\u22121, hl\u22121 t , hl\u22121 t+1] \u02c6il t = \u03c3(Wi 1\u03bel\u22121 t + Wi 2xt + Wi 3hslot t + Wi 4hint t ) \u02c6ol t = \u03c3(Wo 1\u03bel\u22121 t + Wo 2xt + Wo 3hslot t + Wo 4hint t ) \u02c6fl t = \u03c3(Wf 1\u03bel\u22121 t + Wf 2xt + Wf 3hslot t + Wf 4hint t ) \u02c6ll t = \u03c3(Wl 1\u03bel\u22121 t + Wl 2xt + Wl 3hslot t + Wl 4hint t ) \u02c6rl t = \u03c3(Wr 1\u03bel\u22121 t + Wr 2xt + Wr 3hslot t + Wr 4hint t ) ul t = tanh(Wu 1\u03bel\u22121 t + Wu 2xt + Wu 3hslot t + Wu 4hint t ) il t,",
      "fl t, ll t, rl t = softmax(\u02c6il t,\u02c6fl t,\u02c6ll t, \u02c6rl t) cl t = fl t \u2299cl\u22121 t + ll t \u2299cl\u22121 t\u22121 + rl t \u2299cl\u22121 t+1 + il t \u2299ul\u22121 t hl t = ol t \u2299tanh cl t (10)",
      "where \u03bel t is the concatenation of hidden states in a local window, and il t, fl t, ol t, ll t and rl t are gates to control information \ufb02ows, and Wx n (x \u2208 {i, o, f, l, r, u}, n \u2208{1, 2, 3, 4}) are model pa- rameters. More details about the state transition can be referred in (Zhang et al., 2018b). In the \ufb01rst CM-block, the hidden state ht is initialized with the corresponding word embedding. In other CM-blocks, the ht is inherited from the output of the adjacent lower CM-block. At each word position of above procedures, the hidden state is updated with abundant information from different perspectives, namely word embed- dings, local contexts, slots and intents represen- tations. The local calculation layer in each CM- block has been shown highly useful for both tasks, and especially for the slot \ufb01lling task, which will be validated in our experiments in Section 5.2.",
      "The local calculation layer in each CM- block has been shown highly useful for both tasks, and especially for the slot \ufb01lling task, which will be validated in our experiments in Section 5.2. Global Recurrence Bi-directional RNNs, especially the BiLSTMs (Hochreiter and Schmidhuber, 1997) are regarded to encode both past and future information of a sentence, which have become a dominant method in various sequence modeling tasks (Hammerton, 2003; Sundermeyer et al., 2012). The inherent na- ture of BiLSTMs is able to supplement global se- quential information, which is insuf\ufb01ciently mod- eled in the previous local calculation layer. Thus we apply an additional BiLSTMs layer upon the local calculation layer in each CM-block. By tak- ing the slot- and intent-speci\ufb01c local context rep- resentations as inputs, we can obtain more spe- ci\ufb01c global sequential representations.",
      "Thus we apply an additional BiLSTMs layer upon the local calculation layer in each CM-block. By tak- ing the slot- and intent-speci\ufb01c local context rep- resentations as inputs, we can obtain more spe- ci\ufb01c global sequential representations. Formally, it takes the hidden state hl\u22121 t inherited from the local calculation layer as input, and conduct recur- rent steps as follows: hl t = [ \u2212\u2192 hl t; \u2190\u2212 htl] \u2212\u2192 hl t = \u2212\u2212\u2212\u2212\u2192 LSTM(hl\u22121 t , \u2212\u2192 h l t\u22121; \u2212\u2192\u03b8 ) \u2190\u2212 hl t = \u2190\u2212\u2212\u2212\u2212 LSTM(hl\u22121 t , \u2190\u2212 h l t+1; \u2190\u2212\u03b8 ) (11) The output \u201cstates\u201d of the BiLSTMs are taken as \u201cstates\u201d input of the local calculation in next CM- block. The global sequential information encoded by the BiLSTMs is shown necessary and effective for both tasks in our experiments in Section 5.2.",
      "The global sequential information encoded by the BiLSTMs is shown necessary and effective for both tasks in our experiments in Section 5.2. 3.4 Inference Layer After multiple rounds of interactions among lo- cal context representations, global sequential in- formation, slot and intent features, we conduct Dataset SNIPS ATIS CAIS Vocab Size 11241 722 2146 Average Length 9.15 11.28 8.65 # Intents 7 18 11 # Slots 72 128 75 # Train Set 13084 4478 7995 # Validation Set 700 500 994 # Test Set 700 893 1012 Table 2: Dataset statistics. predictions upon the \ufb01nal CM-block.",
      "predictions upon the \ufb01nal CM-block. For the pre- dictions of slots, we take the hidden states H along with the retrieved slot Hslot representations (both are from the \ufb01nal CM-block) as input features, and then conduct predictions of slots similarly with the Equation (3) in Section 2: \u02c6yslot = arg max eyslot\u2208Yx F([H; Hslot], eyslot) (12) For the prediction of intent label, we \ufb01rstly aggre- gate the hidden state ht and the retrieved intent representation hint t at each word position (from the \ufb01nal CM-block as well) via mean pooling: vint = 1 N N X t [ht; hint t ] (13) and then take the summarized vector vint as input feature to conduct prediction of intent consistently with the Equation (5) in Section 2. 4 Experiments 4.1 Datasets and Metrics We evaluate our proposed CM-Net on three real- word datasets, and statistics are listed in Table 2.",
      "4 Experiments 4.1 Datasets and Metrics We evaluate our proposed CM-Net on three real- word datasets, and statistics are listed in Table 2. ATIS The Airline Travel Information Systems (ATIS) corpus (Hemphill et al., 1990) is the most widely used benchmark for the SLU research. Please note that, there are extra named entity fea- tures in the ATIS, which almost determine slot tags. These hand-crafted features are not gener- ally available in open domains (Zhang and Wang, 2016; Guo et al., 2014b), therefore we train our model purely on the training set without additional hand-crafted features. SNIPS SNIPS Natural Language Understanding benchmark 3 (Coucke et al., 2018) is collected in a crowsourced fashion by Snips. The intents of this 3https://github.com/snipsco/nlu- benchmark/tree/master/2017-06-custom-intent-engines",
      "Models SNIPS ATIS Slot (F1) Intent (Acc) Slot (F1) Intent (Acc) Joint GRU (Zhang and Wang, 2016) \u2013 \u2013 95.49 98.10 Self-Attention, Intent Gate(Li et al., 2018) \u2013 \u2013 96.52 98.77 Bi-model (Wang et al., 2018) \u2013 \u2013 96.89 98.99 Attention Bi-RNN (Liu and Lane, 2016) * 87.80 96.70 95.98 98.21 Joint Seq2Seq (Hakkani-T\u00a8ur et al., 2016) * 87.30 96.90 94.20 92.60 Slot-Gated (Intent Atten.) (Goo et al., 2018) 88.30 96.80 95.20 94.10 Slot-Gated (Full Atten.) (Goo et al., 2018) 88.80 97.00 94.80 93.60 CAPSULE-NLU(Zhang et al., 2018a) 91.80 97.70 95.",
      "(Goo et al., 2018) 88.80 97.00 94.80 93.60 CAPSULE-NLU(Zhang et al., 2018a) 91.80 97.70 95.20 95.00 Dilated CNN, Label-Recurrent (Gupta et al., 2019) 93.11 98.29 95.54 98.10 Sentence-State LSTM (Zhang et al., 2018b) \u2020 95.80 98.30 95.65 98.21 BiLSTMs + EMLoL (Siddhant et al., 2018) 93.29 98.83 95.62 97.42 BiLSTMs + EMLo (Siddhant et al., 2018) 93.90 99.29 95.42 97.30 Joint BERT (Chen et al., 2019) 97.00 98.60 96.10 97.50 CM-Net (Ours) 97.15 99.29 96.20 99.",
      "29 95.42 97.30 Joint BERT (Chen et al., 2019) 97.00 98.60 96.10 97.50 CM-Net (Ours) 97.15 99.29 96.20 99.10 Table 3: Results on test sets of the SNIPS and ATIS, where our CM-Net achieves state-of-the-art performances in most cases. \u201c*\u201d indicates that results are retrieved from Slot-Gated (Goo et al., 2018), and \u201c\u2020\u201d indicates our implementation. dataset are more balanced when compared with the ATIS. We split another 700 utterances for val- idation set following previous works (Goo et al., 2018; Zhang et al., 2018a). CAIS We collect utterances from the Chinese Arti\ufb01cial Intelligence Speakers (CAIS), and an- notate them with slot tags and intent labels. The training, validation and test sets are split by the distribution of intents, where detailed statistics are provided in the supplementary material.",
      "CAIS We collect utterances from the Chinese Arti\ufb01cial Intelligence Speakers (CAIS), and an- notate them with slot tags and intent labels. The training, validation and test sets are split by the distribution of intents, where detailed statistics are provided in the supplementary material. Since the utterances are collected from speaker systems in the real world, intent labels are partial to the PlayMusic option. We adopt the BIOES tag- ging scheme for slots instead of the BIO2 used in the ATIS, since previous studies have highlighted meaningful improvements with this scheme (Rati- nov and Roth, 2009) in the sequence labeling \ufb01eld. Metrics Slot \ufb01lling is typically treated as a se- quence labeling problem, and thus we take the conlleval 4 as the token-level F1 metric. The in- tent detection is evaluated with the classi\ufb01cation accuracy. Specially, several utterances in the ATIS are tagged with more than one labels. Following previous works (Tur et al., 2010; Zhang and Wang, 2016), we count an utterrance as a correct classi\ufb01- cation if any ground truth label is predicted.",
      "Specially, several utterances in the ATIS are tagged with more than one labels. Following previous works (Tur et al., 2010; Zhang and Wang, 2016), we count an utterrance as a correct classi\ufb01- cation if any ground truth label is predicted. 4https://www.clips.uantwerpen.be/conll2000/chunking/ conlleval.txt 4.2 Implementation Details All trainable parameters in our model are initial- ized by the method described in Glorot and Ben- gio (2010). We apply dropout (Srivastava et al., 2014) to the embedding layer and hidden states with a rate of 0.5. All models are optimized by the Adam optimizer (Kingma and Ba, 2014) with gradient clipping of 3 (Pascanu et al., 2013). The initial learning rate \u03b1 is set to 0.001, and decrease with the growth of training steps. We monitor the training process on the validation set and report the \ufb01nal result on the test set.",
      "The initial learning rate \u03b1 is set to 0.001, and decrease with the growth of training steps. We monitor the training process on the validation set and report the \ufb01nal result on the test set. One layer CNN with a \ufb01lter of size 3 and max pooling are utilized to generate 100d word embeddings. The cased 300d Glove is adapted to initialize word embeddings, and kept \ufb01xed when training. In auxiliary experi- ments, the output hidden states of BERT are taken as additional word embeddings and kept \ufb01xed as well. We share parameters of both memories with the parameter matrices in the corresponding soft- max layers, which can be taken as introducing su- pervised signals into the memories to some extent. We conduct hyper-parameters tuning for layer size (\ufb01nally set to 3) and loss weight \u03bb (\ufb01nally set to 0.5), and empirically set other parameters to the values listed in the supplementary material. 4.3 Main Results Main results of our CM-Net on the SNIPS and ATIS are shown in Table 3.",
      "4.3 Main Results Main results of our CM-Net on the SNIPS and ATIS are shown in Table 3. Our CM-Net achieves the state-of-the-art results on both datasets in terms of slot \ufb01lling F1 score and intent detection",
      "Figure 4: Investigations of the collaborative retrieval approach on slot \ufb01lling (on the left) and intent detection (on the right), where \u201cno slot2int\u201d indicates removing slow-aware attention for the intent representation, and similarly for \u201cno int2slot\u201d and \u201cneither\u201d. # Models SNIPS Slot (F1) Intent (Acc) 0 CM-Net 97.15 99.29 1 \u2013 slot memory 96.64 99.14 2 \u2013 intent memory 96.95 98.84 3 \u2013 local calculation 96.73 99.00 4 \u2013 global recurrence 96.80 98.57 Table 4: Ablation experiments on the SNIPS to in- vestigate the impacts of various components, where \u201c- slot memory\u201d indicates removing the slot memory and its interactions with other components correspond- ingly. Similarly for the other options. accuracy, except for the F1 score on the ATIS. We conjecture that the named entity feature in the ATIS has a great impact on the slot \ufb01lling result as illustrated in Section 4.1.",
      "Similarly for the other options. accuracy, except for the F1 score on the ATIS. We conjecture that the named entity feature in the ATIS has a great impact on the slot \ufb01lling result as illustrated in Section 4.1. Since the SNIPS is col- lected from multiple domains with more balanced labels when compared with the ATIS, the slot \ufb01ll- ing F1 score on the SNIPS is able to demonstrate the superiority of our CM-Net. It is noteworthy that the CM-Net achieves comparable results when compared with models that exploit additional language models (Siddhant et al., 2018; Chen et al., 2019). We conduct aux- iliary experiments by leveraging the well-known BERT (Devlin et al., 2018) as an external resource for a relatively fair comparison with those models, and report details in Section 5.3. 5 Analysis Since the SNIPS corpus is collected from multiple domains and its label distributions are more bal- anced when compared with the ATIS, we choose the SNIPS to elucidate properties of our CM-Net and conduct several additional experiments. 5.1 Whether Memories Promote Each Other?",
      "5 Analysis Since the SNIPS corpus is collected from multiple domains and its label distributions are more bal- anced when compared with the ATIS, we choose the SNIPS to elucidate properties of our CM-Net and conduct several additional experiments. 5.1 Whether Memories Promote Each Other? In the CM-Net, the deliberate attention mecha- nism is proposed in a collaborative manner to per- form information exchange between slots and in- tents. We conduct experiments to verify whether such kind of knowledge diffusion in both memo- ries can promote each other. More speci\ufb01cally, we remove one unidirectional diffusion (e.g. from slot to intent) or both in each experimental setup. The results are illustrated in Figure 4. We can observe obvious drops on both tasks when both directional knowledge diffusions are removed (CM-Net vs. neither). For the slot \ufb01lling task (left part in Figure 4), the F1 scores decrease slightly when the knowledge from slot to intent is blocked (CM-Net vs. \u201cno slot2int\u201d), and a more evident drop occurs when the knowledge from in- tent to slot is blocked (CM-Net vs.",
      "\u201cno slot2int\u201d), and a more evident drop occurs when the knowledge from in- tent to slot is blocked (CM-Net vs. \u201cno int2slot\u201d). Similar observations can be found for the intent detection task (right part in Figure 4). In conclusion, the bidirectional knowledge dif- fusion between slots and intents are necessary and effective to promote each other. 5.2 Ablation Experiments We conduct ablation experiments to investigate the impacts of various components in our CM-Net. In particular, we remove one component among slot memory, intent memory, local calculation and global recurrence. Results of different combina- tions are presented in Table 4. Once the slot memory and its corresponding interactions with other components are removed, scores on both tasks decrease to some extent, and a more obvious decline occurs for the slot \ufb01lling (row 1 vs. row 0), which is consistent with the conclusion of Section 5.1. Similar observations can be found for the intent memory (row 2).",
      "row 0), which is consistent with the conclusion of Section 5.1. Similar observations can be found for the intent memory (row 2). The local calculation layer is designed to capture bet- ter local context representations, which has an ev- ident impact on the slot \ufb01lling and slighter effect on the intent detection (row 3 vs. row 0). Opposite observations occur in term of global recurrence, which is supposed to model global sequential in- formation and thus has larger effect on the intent detection (row 4 vs. row 0). 5.3 Effects of Pre-trained Language Models Recently, there has been a growing body of works exploring neural language models that trained on",
      "Models SNIPS Slot (F1) Intent (Acc) BiLSTMs + EMLoL 93.29 98.83 BiLSTMs + EMLo 93.90 99.29 Joint BERT 97.00 98.60 CM-Net + BERT 97.31 99.32 Table 5: Results on the SNIPS benchmark with the as- sistance of pre-trained language model, where we es- tablish new state-of-the-art results on the SNIPS. Models CAIS Slot (F1) Intent (Acc) BiLSTMs + CRF 85.32 93.25 S-LSTM + CRF \u2020 85.74 94.36 CM-Net 86.16 94.56 Table 6: Results on our CAIS dataset, where \u201c\u2020\u201d indi- cates our implementation of the S-LSTM. massive corpora to learn contextual representa- tions (e.g. BERT (2018) and EMLo (2018)). In- spired by the effectiveness of language model em- beddings, we conduct experiments by leveraging the BERT as an additional feature.",
      "massive corpora to learn contextual representa- tions (e.g. BERT (2018) and EMLo (2018)). In- spired by the effectiveness of language model em- beddings, we conduct experiments by leveraging the BERT as an additional feature. The results emerged in Table 5 show that we establish new state-of-the-art results on both tasks of the SNIPS. 5.4 Evaluation on the CAIS We conduct experiments on our self-collected CAIS to evaluate the generalizability in different language. We apply two baseline models for com- parison, one is the popular BiLSTMs + CRF ar- chitecture (Huang et al., 2015) for sequence label- ing task, and the other one is the more powerful sententce-state LSTM (Zhang et al., 2018b). The results listed in Table 6 demonstrate the generaliz- ability and effectiveness of our CM-Net when han- dling various domains and different languages. 6 Related Work Memory Network Memory network is a gen- eral machine learning framework introduced by Weston et al.",
      "The results listed in Table 6 demonstrate the generaliz- ability and effectiveness of our CM-Net when han- dling various domains and different languages. 6 Related Work Memory Network Memory network is a gen- eral machine learning framework introduced by Weston et al. (2014), which have been shown effective in question answering (Weston et al., 2014; Sukhbaatar et al., 2015), machine transla- tion (Wang et al., 2016a; Feng et al., 2017), aspect level sentiment classi\ufb01cation (Tang et al., 2016), etc. For spoken language understanding, Chen et al. (2016) introduce memory mechanisms to en- code historical utterances. In this paper, we pro- pose two memories to explicitly capture the se- mantic correlations between slots and the intent in a given utterance, and devise a novel collaborative retrieval approach.",
      "(2016) introduce memory mechanisms to en- code historical utterances. In this paper, we pro- pose two memories to explicitly capture the se- mantic correlations between slots and the intent in a given utterance, and devise a novel collaborative retrieval approach. Interactions between slots and intents Consid- ering the semantic proximity between slots and in- tents, some works propose to enhance the slot \ufb01ll- ing task unidirectionally with the guidance of in- tent representations via gating mechanisms (Goo et al., 2018; Li et al., 2018). Intuitively, the slot representations are also instructive to the intent de- tection task and thus bidirectional interactions be- tween slots and intents are bene\ufb01cal for each other. Zhang et al. (2018a) propose a hierarchical cap- sule network to perform interactions from words to slots, slots to intents and intents to words in a pipeline manner, which is relatively limited in cap- turing the complicated correlations among them. In our CM-Net, information exchanges are per- formed simultaneously with knowledge diffusions in both directions.",
      "In our CM-Net, information exchanges are per- formed simultaneously with knowledge diffusions in both directions. The experiments demonstrate the superiority of our CM-Net in capturing the se- mantic correlations between slots and intents. Sentence-State LSTM Zhang et al. 2018b pro- pose a novel graph RNN named S-LSTM, which models sentence between words simultaneously. Inspired by the new perspective of state transi- tion in the S-LSTM, we further extend it with task-speci\ufb01c (i.e., slots and intents) representa- tions via our collaborative memories. In addition, the global information in S-LSTM is modeled by aggregating the local features with gating mecha- nisms, which may lose sight of sequential infor- mation of the whole sentence. Therefore, We ap- ply external BiLSTMs to supply global sequential features, which is shown highly necessary for both tasks in our experiments. 7 Conclusion We propose a novel Collaborative Memory Network (CM-Net) for jointly modeling slot \ufb01ll- ing and intent detection.",
      "Therefore, We ap- ply external BiLSTMs to supply global sequential features, which is shown highly necessary for both tasks in our experiments. 7 Conclusion We propose a novel Collaborative Memory Network (CM-Net) for jointly modeling slot \ufb01ll- ing and intent detection. The CM-Net is able to explicitly capture the semantic correlations among words, slots and intents in a collaborative manner, and incrementally enrich the information \ufb02ows with local context and global sequential informa- tion. Experiments on two standard benchmarks and our CAIS corpus demonstrate the effective- ness and generalizability of our proposed CM-Net. In addition, we contribute the new corpus (CAIS) to the research community.",
      "Acknowledgments Liu, Chen and Xu are supported by the National Natural Science Foundation of China (Contract 61370130, 61976015, 61473294 and 61876198), and the Beijing Municipal Natural Science Foun- dation (Contract 4172047), and the Interna- tional Science and Technology Cooperation Pro- gram of the Ministry of Science and Technology (K11F100010). We sincerely thank the anony- mous reviewers for their thorough reviewing and valuable suggestions. References Qian Chen, Zhu Zhuo, and Wen Wang. 2019. Bert for joint intent classi\ufb01cation and slot \ufb01lling. Yun-Nung Chen, Dilek Hakkani-T\u00a8ur, G\u00a8okhan T\u00a8ur, Jianfeng Gao, and Li Deng. 2016. End-to-end mem- ory networks with knowledge carryover for multi- turn spoken language understanding. In Interspeech, pages 3245\u20133249. Ronan Collobert, Jason Weston, L\u00b4eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.",
      "In Interspeech, pages 3245\u20133249. Ronan Collobert, Jason Weston, L\u00b4eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12(Aug):2493\u20132537. Alice Coucke, Alaa Saade, Adrien Ball, Thodore Bluche, Alexandre Caulier, David Leroy, Clment Doumouro, Thibault Gisselbrecht, Francesco Cal- tagirone, Thibaut Lavril, Mal Primet, and Joseph Dureau. 2018. Snips voice platform: an embedded spoken language understanding system for private- by-design voice interfaces. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805. Yang Feng, Shiyue Zhang, Andi Zhang, Dong Wang, and Andrew Abel. 2017.",
      "2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805. Yang Feng, Shiyue Zhang, Andi Zhang, Dong Wang, and Andrew Abel. 2017. Memory-augmented neu- ral machine translation. G David Forney. 1973. The viterbi algorithm. Pro- ceedings of the IEEE, 61(3):268\u2013278. Xavier Glorot and Yoshua Bengio. 2010. Understand- ing the dif\ufb01culty of training deep feedforward neu- ral networks. In Proceedings of the thirteenth in- ternational conference on arti\ufb01cial intelligence and statistics, pages 249\u2013256. Chih-Wen Goo, Guang Gao, Yun-Kai Hsu, Chih-Li Huo, Tsung-Chieh Chen, Keng-Wei Hsu, and Yun- Nung Chen. 2018. Slot-gated modeling for joint slot \ufb01lling and intent prediction.",
      "2018. Slot-gated modeling for joint slot \ufb01lling and intent prediction. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, Volume 2 (Short Pa- pers), pages 753\u2013757, New Orleans, Louisiana. As- sociation for Computational Linguistics. Daniel Guo, Gokhan Tur, Wen-tau Yih, and Geoffrey Zweig. 2014a. Joint semantic utterance classi\ufb01ca- tion and slot \ufb01lling with recursive neural networks. In 2014 IEEE Spoken Language Technology Work- shop (SLT), pages 554\u2013559. IEEE. Daniel Guo, Gokhan Tur, Wen-tau Yih, and Geoffrey Zweig. 2014b. Joint semantic utterance classi\ufb01ca- tion and slot \ufb01lling with recursive neural networks. In 2014 IEEE Spoken Language Technology Work- shop (SLT), pages 554\u2013559. IEEE. Arshit Gupta, John Hewitt, and Katrin Kirchhoff. 2019.",
      "In 2014 IEEE Spoken Language Technology Work- shop (SLT), pages 554\u2013559. IEEE. Arshit Gupta, John Hewitt, and Katrin Kirchhoff. 2019. Simple, fast, accurate intent classi\ufb01cation and slot labeling. Patrick Haffner, Gokhan Tur, and Jerry H Wright. 2003. Optimizing svms for complex call classi- \ufb01cation. In 2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP\u201903)., volume 1, pages I\u2013I. IEEE. Dilek Hakkani-T\u00a8ur, G\u00a8okhan T\u00a8ur, Asli Celikyilmaz, Yun-Nung Chen, Jianfeng Gao, Li Deng, and Ye- Yi Wang. 2016. Multi-domain joint semantic frame parsing using bi-directional rnn-lstm. In Inter- speech, pages 715\u2013719. James Hammerton. 2003. Named entity recognition with long short-term memory.",
      "2016. Multi-domain joint semantic frame parsing using bi-directional rnn-lstm. In Inter- speech, pages 715\u2013719. James Hammerton. 2003. Named entity recognition with long short-term memory. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4, pages 172\u2013175. Asso- ciation for Computational Linguistics. Charles T. Hemphill, John J. Godfrey, and George R. Doddington. 1990. The ATIS spoken language sys- tems pilot corpus. In Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania, June 24-27,1990. Sepp Hochreiter and Jrgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735\u20131780. Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidi- rectional LSTM-CRF models for sequence tagging. CoRR, abs/1508.01991. Diederik P Kingma and Jimmy Ba.",
      "Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidi- rectional LSTM-CRF models for sequence tagging. CoRR, abs/1508.01991. Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Gakuto Kurata, Bing Xiang, Bowen Zhou, and Mo Yu. 2016. Leveraging sentence-level information with encoder lstm for semantic slot \ufb01lling. arXiv preprint arXiv:1601.01530. John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random \ufb01elds: Prob- abilistic models for segmenting and labeling se- quence data.",
      "Guillaume Lample, Miguel Ballesteros, Sandeep Sub- ramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 260\u2013270. Association for Computational Lin- guistics. Changliang Li, Liang Li, and Ji Qi. 2018. A self- attentive model with gate mechanism for spoken lan- guage understanding. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing, pages 3824\u20133833, Brussels, Bel- gium. Association for Computational Linguistics. Bing Liu and Ian Lane. 2016. Attention-based recur- rent neural network models for joint intent detection and slot \ufb01lling. arXiv preprint arXiv:1609.01454.",
      "Association for Computational Linguistics. Bing Liu and Ian Lane. 2016. Attention-based recur- rent neural network models for joint intent detection and slot \ufb01lling. arXiv preprint arXiv:1609.01454. Gr\u00b4egoire Mesnil, Yann Dauphin, Kaisheng Yao, Yoshua Bengio, Li Deng, Dilek Hakkani-Tur, Xi- aodong He, Larry Heck, Gokhan Tur, Dong Yu, et al. 2014. Using recurrent neural networks for slot \ufb01ll- ing in spoken language understanding. IEEE/ACM Transactions on Audio, Speech, and Language Pro- cessing, 23(3):530\u2013539. Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the dif\ufb01culty of training recurrent neural networks. In International conference on machine learning, pages 1310\u20131318. Jeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. Glove: Global vectors for word representation.",
      "In International conference on machine learning, pages 1310\u20131318. Jeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Nat- ural Language Processing (EMNLP), pages 1532\u2013 1543. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. In NAACL, pages 2227\u20132237. Associ- ation for Computational Linguistics. Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proceedings of the thirteenth conference on compu- tational natural language learning, pages 147\u2013155. Association for Computational Linguistics. Ruhi Sarikaya, Geoffrey E Hinton, and Bhuvana Ram- abhadran. 2011. Deep belief nets for natural lan- guage call-routing.",
      "Association for Computational Linguistics. Ruhi Sarikaya, Geoffrey E Hinton, and Bhuvana Ram- abhadran. 2011. Deep belief nets for natural lan- guage call-routing. In 2011 IEEE International con- ference on acoustics, speech and signal processing (ICASSP), pages 5680\u20135683. IEEE. Aditya Siddhant, Anuj Goyal, and Angeliki Metalli- nou. 2018. Unsupervised transfer learning for spo- ken language understanding in intelligent agents. AAAI. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from over\ufb01tting. Journal of Machine Learning Re- search, 15:1929\u20131958. Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. 2015. End-to-end memory networks. In Advances in neural information processing systems, pages 2440\u20132448.",
      "Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. 2015. End-to-end memory networks. In Advances in neural information processing systems, pages 2440\u20132448. Martin Sundermeyer, Ralf Schl\u00a8uter, and Hermann Ney. 2012. Lstm neural networks for language modeling. In Thirteenth annual conference of the international speech communication association. Duyu Tang, Bing Qin, and Ting Liu. 2016. Aspect level sentiment classi\ufb01cation with deep memory net- work. arXiv preprint arXiv:1605.08900. Gokhan Tur, Dilek Hakkani-T\u00a8ur, and Larry Heck. 2010. What is left to be understood in atis? In 2010 IEEE Spoken Language Technology Workshop, pages 19\u201324. IEEE. Mingxuan Wang, Zhengdong Lu, Hang Li, and Qun Liu. 2016a. Memory-enhanced decoder for neural machine translation.",
      "In 2010 IEEE Spoken Language Technology Workshop, pages 19\u201324. IEEE. Mingxuan Wang, Zhengdong Lu, Hang Li, and Qun Liu. 2016a. Memory-enhanced decoder for neural machine translation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Lan- guage Processing, pages 278\u2013286, Austin, Texas. Association for Computational Linguistics. Xingyou Wang, Weijie Jiang, and Zhiyong Luo. 2016b. Combination of convolutional and recurrent neu- ral network for sentiment analysis of short texts. In Proceedings of COLING 2016, the 26th Inter- national Conference on Computational Linguistics: Technical Papers, pages 2428\u20132437. Yu Wang, Yilin Shen, and Hongxia Jin. 2018. A bi- model based RNN semantic frame parsing model for intent detection and slot \ufb01lling. In Proceedings of the 2018 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 309\u2013314, New Orleans, Louisiana.",
      "In Proceedings of the 2018 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 309\u2013314, New Orleans, Louisiana. Association for Computational Linguistics. Jason Weston, Sumit Chopra, and Antoine Bordes. 2014. Memory networks. Puyang Xu and Ruhi Sarikaya. 2013. Convolutional neural network based triangular crf for joint intent detection and slot \ufb01lling. In 2013 IEEE Workshop on Automatic Speech Recognition and Understand- ing, pages 78\u201383. IEEE. Chenwei Zhang, Yaliang Li, Nan Du, Wei Fan, and Philip S Yu. 2018a. Joint slot \ufb01lling and intent de- tection via capsule neural networks. arXiv preprint arXiv:1812.09471. Xiaodong Zhang and Houfeng Wang. 2016. A joint model of intent determination and slot \ufb01lling for spoken language understanding. In IJCAI, pages 2993\u20132999.",
      "Xiaodong Zhang and Houfeng Wang. 2016. A joint model of intent determination and slot \ufb01lling for spoken language understanding. In IJCAI, pages 2993\u20132999. Yue Zhang, Qi Liu, and Linfeng Song. 2018b. Sentence-state lstm for text representation. In Pro- ceedings of the 56th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers), pages 317\u2013327. Association for Computa- tional Linguistics."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1909.06937.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":10345,
  "avg_doclen":172.4166666667,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1909.06937.pdf"
    }
  }
}