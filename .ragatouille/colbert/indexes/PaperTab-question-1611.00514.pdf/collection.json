[
  "THE INTELLIGENT VOICE 2016 SPEAKER RECOGNITION SYSTEM Abbas Khosravani, Cornelius Glackin, Nazim Dugan, G\u00b4erard Chollet, Nigel Cannings Intelligent Voice Limited, St Clare House, 30-33 Minories, EC3N 1BP, London, UK ABSTRACT This paper presents the Intelligent Voice (IV) system submit- ted to the NIST 2016 Speaker Recognition Evaluation (SRE). The primary emphasis of SRE this year was on developing speaker recognition technology which is robust for novel lan- guages that are much more heterogeneous than those used in the current state-of-the-art, using signi\ufb01cantly less training data, that does not contain meta-data from those languages. The system is based on the state-of-the-art i-vector/PLDA which is developed on the \ufb01xed training condition, and the results are reported on the protocol de\ufb01ned on the develop- ment set of the challenge. Index Terms\u2014 Speaker Recognition, Speech Processing 1.",
  "The system is based on the state-of-the-art i-vector/PLDA which is developed on the \ufb01xed training condition, and the results are reported on the protocol de\ufb01ned on the develop- ment set of the challenge. Index Terms\u2014 Speaker Recognition, Speech Processing 1. INTRODUCTION Compared to previous years, the 2016 NIST speaker recog- nition evaluation (SRE) marked a major shift from English towards Austronesian and Chinese languages. The task like previous years is to perform speaker detection with the fo- cus on telephone speech data recorded over a variety of hand- set types. The main challenges introduced in this evaluation are duration and language variability. The potential variation of languages addressed in this evaluation, recording environ- ment, and variability of test segments duration in\ufb02uenced the design of our system. Our goal was to utilize recent advances in language normalization, domain adaptation, speech activ- ity detection and session compensation techniques to mitigate the adverse bias introduced in this year\u2019s evaluation. Over recent years, the i-vector representation of speech segments has been widely used by state-of-the-art speaker recognition systems [3].",
  "Our goal was to utilize recent advances in language normalization, domain adaptation, speech activ- ity detection and session compensation techniques to mitigate the adverse bias introduced in this year\u2019s evaluation. Over recent years, the i-vector representation of speech segments has been widely used by state-of-the-art speaker recognition systems [3]. The speaker recognition technol- ogy based on i-vectors currently dominates the research \ufb01eld due to its performance, low computational cost and the compatibility of i-vectors with machine learning techniques. This dominance is re\ufb02ected by the recent NIST i-vector ma- chine learning challenge [7] which was designed to \ufb01nd the most promising algorithmic approaches to speaker recogni- tion speci\ufb01cally on the basis of i-vectors [11, 18, 23, 12]. The outstanding ability of DNN for frame alignment which has achieved remarkable performance in text-independent speaker recognition for English data [13, 9], failed to provide even comparable recognition performance to the traditional GMM. Therefore, we concentrated on the cepstral based GMM/i-vector system.",
  "The outstanding ability of DNN for frame alignment which has achieved remarkable performance in text-independent speaker recognition for English data [13, 9], failed to provide even comparable recognition performance to the traditional GMM. Therefore, we concentrated on the cepstral based GMM/i-vector system. We outline in this paper the Intelligent Voice system, tech- niques and results obtained on the SRE 2016 development set that will mirror the evaluation condition as well as the timing report. Section 2 describes the data used for the system train- ing. The front-end and back-end processing of the system are presented in Sections 3 and 4 respectively. In Section 5, we describe experimental evaluation of the system on the SRE 2016 development set. Finally, we present a timing analysis of the system in Section 6. 2. TRAINING CONDITION The \ufb01xed training condition is used to build our speaker recognition system.",
  "In Section 5, we describe experimental evaluation of the system on the SRE 2016 development set. Finally, we present a timing analysis of the system in Section 6. 2. TRAINING CONDITION The \ufb01xed training condition is used to build our speaker recognition system. Only conversational telephone speech data from datasets released through the linguistic data consor- tium (LDC) have been used, including NIST SRE 2004-2010 and the Switchboard corpora (Switchboard Cellular Parts I and II, Switchboard2 Phase I,II and III) for different steps of system training. A more detailed description of the data used in the system training is presented in Table 1. We have also included the unlabelled set of 2472 telephone calls from both minor (Cebuano and Mandarin) and major (Tagalog and Cantonese) languages provided by NIST in the system train- ing. We will indicate when and how we used this set in the training in the following sections. Table 1. The description of the data used for training the speaker recognition system.",
  "We will indicate when and how we used this set in the training in the following sections. Table 1. The description of the data used for training the speaker recognition system. #Langs #Spks #Segs Male Female Male Female English 1 1925 2603 19556 25835 non-English 34 274 489 1428 2657 3. FRONT-END PROCESSING In this section we will provide a description of the main steps in front-end processing of our speaker recognition system in- cluding speech activity detection, acoustic and i-vector fea- ture extraction. arXiv:1611.00514v1  [cs.SD]  2 Nov 2016",
  "600 512 SoftMax Stacked  Filter-Bank \u2026 512 6 \u2026 512 512 HMM Speech Pause Fig. 1. The architecture of our DNN-HMM speech activity detection. 3.1. Speech Activity Detection The \ufb01rst stage of any speaker recognition system is to detect the speech content in an audio signal. An accurate speech activity detector (SAD) can improve the speaker recognition performance. Several techniques have been proposed for SAD, including unsupervised methods based on a thresh- olding signal energy, and supervised methods that train a speech/non-speech classi\ufb01er such as support vector machines (SVM) [16] and Gaussian mixture models (GMMs) [17]. Hidden markov models (HMMs) [19] have also been suc- cessful. Recently, it has been shown that DNN systems achieve impressive improvement in performance especially in low signal to noise ratios (SNRs) [21]. In our work we have utilized a two-class DNN-HMM classi\ufb01er to perform this task.",
  "Recently, it has been shown that DNN systems achieve impressive improvement in performance especially in low signal to noise ratios (SNRs) [21]. In our work we have utilized a two-class DNN-HMM classi\ufb01er to perform this task. The DNN-HMM hybrid con\ufb01guration with cross- entropy as the objective function has been trained with the back-propagation algorithm. The softmax layer produces posterior probabilities for speech and non-speech which were then converted into log-likelihoods. Using 2-state HMMs cor- responding to speech and non-speech, frame-wise decisions are made by Viterbi decoding. As input to the network, we fed 40-dimensional \ufb01lter-bank features along with 7 frames from each side. The network has 6 hidden layers with 512 units each. The architecture of our DNN-HMM SAD is shown in Figure 1. Approximately 100 hours of speech data from the Switchboard telephony data with word alignments as ground-truth were used to train our SAD. The DNN train- ing in performed on an NVIDIA TITAN X GPU, using Kaldi software [20].",
  "Approximately 100 hours of speech data from the Switchboard telephony data with word alignments as ground-truth were used to train our SAD. The DNN train- ing in performed on an NVIDIA TITAN X GPU, using Kaldi software [20]. Evaluated on 50 hours of telephone speech data from the same database, our DNN-HMM SAD indicated a frame-level miss-classi\ufb01cation (speech/non-speech) rate of 5.9% whereas an energy-based SAD did not perform better than 20%. 3.2. Acoustic Features For acoustic features we have experimented with differ- ent con\ufb01gurations of cepstral features. We have used 39- dimensional PLP features and 60-dimensional MFCC fea- tures (including their \ufb01rst and second order derivatives) as acoustic features. Moreover, our experiments indicated that the combination of these two feature sets performs particu- larly well in score fusion.",
  "We have used 39- dimensional PLP features and 60-dimensional MFCC fea- tures (including their \ufb01rst and second order derivatives) as acoustic features. Moreover, our experiments indicated that the combination of these two feature sets performs particu- larly well in score fusion. Both PLP and MFCC are extracted at 8kHz sample frequency using Kaldi [20] with 25 and 20 ms frame lengths, respectively, and a 10 ms overlap (other con\ufb01gurations are the same as Kaldi defaults). For each utter- ance, the features are centered using a short-term (3s window) cepstral mean and variance normalization (ST-CMVN). Fi- nally, we employed our DNN-HMM speech activity detector (SAD) to drop non-speech frames. 3.3. i-Vector Features Since the introduction of i-vectors in [3], the speaker recogni- tion community has seen a signi\ufb01cant increase in recognition performance.",
  "3.3. i-Vector Features Since the introduction of i-vectors in [3], the speaker recogni- tion community has seen a signi\ufb01cant increase in recognition performance. i-Vectors are low-dimensional representations of Baum-Welch statistics obtained with respect to a GMM, referred to as universal background model (UBM), in a sin- gle subspace which includes all characteristics of speaker and inter-session variability, named total variability matrix [3]. We trained on each acoustic feature a full covariance, gender- independent UBM model with 2048 Gaussians followed by a 600-dimensional i-vector extractor to establish our MFCC- and PLP-based i-vector systems. The unlabeled set of devel- opment data was used in the training of both the UBM and the i-vector extractor. The open-source Kaldi software has been used for all these processing steps [20]. It has been shown that successive acoustic observation vectors tend to be highly correlated. This may be problematic for maximum a posteriori (MAP) estimation of i-vectors. To investigating this issue, scaling the zero and \ufb01rst order Baum- Welch statistics before presenting them to the i-vector extrac- tor has been proposed.",
  "This may be problematic for maximum a posteriori (MAP) estimation of i-vectors. To investigating this issue, scaling the zero and \ufb01rst order Baum- Welch statistics before presenting them to the i-vector extrac- tor has been proposed. It turns out that a scale factor of 0.33 gives a slight edge, resulting in a better decision cost function [10]. This scaling factor has been performed in training the i-vector extractor as well as in the testing. 4. BACK-END PROCESSING This section provides the steps performed in back-end pro- cessing of our speaker recognition system. 4.1. Nearest-neighbor Discriminant Analysis (NDA) The nearest-neighbor discriminant analysis is a nonparamet- ric discriminant analysis technique which was proposed in [4], and recently used in speaker recognition [22]. The non- parametric within- and between-class scatter matrices \u02c6Sw and \u02c6Sb, respectively, are computed based on k nearest neighbor sample information. The NDA transform is then formed using eigenvectors of \u02c6S\u22121 w \u02c6Sb.",
  "The non- parametric within- and between-class scatter matrices \u02c6Sw and \u02c6Sb, respectively, are computed based on k nearest neighbor sample information. The NDA transform is then formed using eigenvectors of \u02c6S\u22121 w \u02c6Sb. It has been shown that as the number of nearest neighbors k approaches the number of samples in each class, the NDA essentially becomes the LDA projection. Based on the \ufb01nding in [22], NDA outperformed LDA due to",
  "-10 0 10 20 30 40 50 60 70 Duration 0 5 10 15 20 25 30 35 40 45 50 Frequency Fig. 2. The duration of test segments in the development set after dropping non-speech frames. the ability in capturing the local structure and boundary in- formation within and across different speakers. We applied a 600 \u00d7 400 NDA projection matrix computed using the 10 nearest sample information on centered i-vectors. The result- ing dimensionality reduced i-vectors are then whitened using both the training data and the unlabelled development set. 4.2. Short-Duration Variability Compensation The enrolment condition of the development set is supposed to provide at least 60 seconds of speech data for each tar- get speaker. Nevertheless, our SAD indicates that the speech content is as low as 26 seconds in some cases. The test seg- ments duration which ranges from 9 to 60 seconds of speech material can result in poor performance for lower duration segments.",
  "Nevertheless, our SAD indicates that the speech content is as low as 26 seconds in some cases. The test seg- ments duration which ranges from 9 to 60 seconds of speech material can result in poor performance for lower duration segments. As indicated in Figure 2, more than one third of the test segments have speech duration of less than 20 sec- onds. We have addressed this issue by proposing a short dura- tion variability compensation method. The proposed method works by \ufb01rst extracting from each audio segment in the un- labelled development set, a partial excerpt of 10 seconds of speech material with random selection of the starting point (Figure 3). Each audio \ufb01le in the unlabelled development set, with the extracted audio segment will result in two 400- dimensional i-vectors, one with at most 10 seconds of speech material. Considering each pair as one class, we computed a 400 \u00d7 390 LDA projection matrix to remove directions at- tributed to duration variability. Moreover, the projected i- vectors are also subjected to a within-class covariance nor- malization (WCCN) using the same class labels. 4.3.",
  "Moreover, the projected i- vectors are also subjected to a within-class covariance nor- malization (WCCN) using the same class labels. 4.3. Language Normalization Language-source normalization is an effective technique for reducing language dependency in the state-of-the-art i- vector/PLDA speaker recognition system [14]. It can be implemented by extending SN-LDA [15] in order to mitigate Fig. 3. Partial excerpt of 10 second speech duration from an audio speech \ufb01le. variations that separate languages. This can be accomplished by using the language label to identify different sources during training.",
  "It can be implemented by extending SN-LDA [15] in order to mitigate Fig. 3. Partial excerpt of 10 second speech duration from an audio speech \ufb01le. variations that separate languages. This can be accomplished by using the language label to identify different sources during training. Language Normalized-LDA (LN-LDA) uti- lizes a language-normalized within-speaker scatter matrix \u02c6SW which is estimated as the variability not captured by the between-speaker scatter matrix, \u02c6SW = ST \u2212\u02c6SB, (1) where ST and \u02c6SB are the total scatter and normalized between-speaker scatter matrices respectively, and are for- mulated as follows: ST = N X n=1 wnwn T , (2) where N is the total number of i-vectors and \u02c6SB = L X l=1 Sl X s=1 nl s( \u00afwl(s) \u2212\u00afwl)( \u00afwl(s) \u2212\u00afwl) T , (3) where L is the number of languages in the training set, Sl is the number of speakers in language l, \u00afwl(s) is the mean of nl s i-vectors from speaker s and language l and \ufb01nally \u00afwl is the mean of all i-vectors in language l. We applied a 390 \u00d7 300 SN-LDA projection matrix to reduce the i-vector dimensions down to 300.",
  "4.4. PLDA Probabilistic Linear Discriminant Analysis (PLDA) provides a powerful mechanism to distinguish between-speaker vari- ability, separating sources which characterizes speaker in- formation, from all other sources of undesired variability that characterize distortions. Since i-vectors are assumed to be generated by some generative model, we can break it down into statistically independent speaker- and session- components with Gaussian distributions [5, 8]. Although it has been shown that their distribution follow Student\u2019s t rather than Gaussian [8] distributions, length normalizing the entire set of i-vectors as a pre-processing step can approximately Gaussianize their distributions [5] and as a result improve the performance of Gaussian PLDA to that of heavy-tailed PLDA",
  "[8]. A standard Gaussian PLDA assumes that an i-vector w, is modelled according to w = m + Vy + \u03b5. (4) where, m is the mean of i-vectors, the columns of matrix V contains the basis for the between-speaker subspace, the la- tent identity variable y \u223cN(0, I) denotes the speaker factor that represents the identity of the speaker and the residual \u03b5 which is normally distributed with zero mean and full covari- ance matrix \u03a3, represents within-speaker variability. For each acoustic feature we have trained two PLDA mod- els. The \ufb01rst out-domain PLDA (Vout,\u03a3out) is trained us- ing the training set presented in Table 1, and the second in- domain PLDA (Vin,\u03a3in) was trained using the unlabelled development set. Our efforts to cluster the development set (e.g using the out-domain PLDA) was not very successful as it sounds that almost all of them are uttered by different speak- ers. Therefore, each i-vector was considered to be uttered by one speaker. We also set the number of speaker factors to 200. 4.5.",
  "Therefore, each i-vector was considered to be uttered by one speaker. We also set the number of speaker factors to 200. 4.5. Domain Adaptation Domain adaptation has gained considerable attention with the aim of compensating for cross-speech-source variability of in- domain and out-of-domain data. The framework presented in [6] for unsupervised adaptation of out-domain PLDA param- eters resulted in better performance for in-domain data. Using in-domain and out-domain PLDA trained in Section 4.4, we interpolated their parameters as follow: Vadapt = \u03b1Vin + (1 \u2212\u03b1)Vout \u03a3adapt = \u03b1\u03a3in + (1 \u2212\u03b1)\u03a3out. (5) We chose \u03b1 = 0.10 for making our submission. 4.6. Score Computation and Normalization For the one-segment enrolment condition, the speaker model is the length normalized i-vector of that segment, however, for the three-segment enrolment condition, we simply used a length-normalized mean vector of the length-normalizated i- vectors as the speaker model. Each speaker model is tested against each test segment as in the trial list.",
  "Each speaker model is tested against each test segment as in the trial list. For each two trial i-vectors w1 and w2, the PLDA score is computed as s = wT 1 Qw1 + wT 2 Qw2 + 2wT 1 Pw2 + c, (6) in which Q = S\u22121 T \u2212(ST \u2212SBS\u22121 T SB)\u22121, (7) P = S\u22121 T SB(ST \u2212SBS\u22121 T SB)\u22121. (8) and SB = VadaptVadapt T and ST = SB + \u03a3adapt. It has been shown and proved in our experiments that score nor- malization can have a great impact on the performance of the recognition system. We used the symmetric s-norm proposed in [8] which normalizes the score s of the pair (w1, w2) using the formula \u02c6s = s \u2212\u00b51 \u03c31 \u2212s \u2212\u00b52 \u03c32 (9) where the means \u00b51, \u00b52 and standard deviations \u03c31, \u03c32 are computed by matching w1 and w2 against the unlabelled set as the impostor speakers, respectively.",
  "4.7. Quality Measure Function It has been shown that there is a dependency between the value of the Cmin det threshold and the duration of both enrol- ment and test segments. Applying the quality measure func- tion (QMF) [18] enabled us to compensate for the shift in the Cmin det threshold due to the differences in speech duration. We conducted some experiments to estimate the dependency be- tween the Cmin det threshold shift on the duration of test segment and used the following QMF for PLDA ver\ufb01cation scores: QMF(t) = \u22120.2 \u221a t (10) where t is the duration of the test segment in seconds. 4.8. Calibration In the literature, the performance of speaker recognition is usually reported in terms of calibrated-insensitive equal error rate (EER) or the minimum decision cost function (Cmin det ). However, in real applications of speaker recognition there is a need to present recognition results in terms of calibrated log- likelihood-ratios. We have utilized the BOSARIS Toolkit [1] for calibration of scores. Cmin det provides an ideal reference value for judging calibration.",
  "However, in real applications of speaker recognition there is a need to present recognition results in terms of calibrated log- likelihood-ratios. We have utilized the BOSARIS Toolkit [1] for calibration of scores. Cmin det provides an ideal reference value for judging calibration. If Cdet \u2212Cmin det is minimized, then the system can be said to be well calibrated. The choice of target probability (Ptar) had a great im- pact on the performance of the calibration. However, we set Ptar = 0.0001 for our primary submission which performed the best on the development set. For our secondary submis- sion Ptar = 0.001 was used. 5. RESULTS AND DISCUSSION In this section we present the results obtained on the protocol provided by NIST on the development set which is supposed to mirror that of evaluation set. The results are shown in Ta- ble 2. The \ufb01rst part of the table indicates the result obtained by the primary system.",
  "The results are shown in Ta- ble 2. The \ufb01rst part of the table indicates the result obtained by the primary system. As can be seen, the fusion of MFCC and PLP (a simple sum of both MFCC and PLP scores) re- sulted in a relative improvement of almost 10%, as compared to MFCC alone, in terms of both Cdet and Cmin det . In order to quantify the contribution of the different system components we have de\ufb01ned different scenarios. In scenario A, we have analysed the effect of using LDA instead of NDA. As can be seen from the results, LDA outperforms NDA in the case of",
  "Table 2. Performance comparison of the Intelligent Voice speaker recognition system with various analysis on the development protocol of NIST SRE 2016. Unequalized Equalized Acoustic Features EER Cmin det Cdet EER Cmin det Cdet Primary MFCC 16.49 0.6633 0.6754 15.83 0.6650 0.6749 PLP 17.87 0.6857 0.6977 16.84 0.6914 0.6982 Fusion 16.04 0.6012 0.6107 14.93 0.6011 0.6267 Scenario A MFCC 16.82 0.6658 0.6794 16.42 0.6890 0.7021 PLP 16.98 0.6691 0.6881 16.28 0.6903 0.7092 Fusion 15.73 0.6153 0.6369 15.12 0.6587 0.6964 Scenario B MFCC 16.55 0.6735 0.6880 16.",
  "28 0.6903 0.7092 Fusion 15.73 0.6153 0.6369 15.12 0.6587 0.6964 Scenario B MFCC 16.55 0.6735 0.6880 16.10 0.6755 0.6945 PLP 18.27 0.6938 0.7141 16.97 0.7018 0.7299 Fusion 16.31 0.6075 0.6299 14.70 0.6259 0.6482 Scenario C MFCC 17.08 0.6767 0.6889 16.77 0.6677 0.6927 PLP 17.98 0.6857 0.6968 17.21 0.7001 0.7192 Fusion 16.59 0.6176 0.6264 15.70 0.6363 0.6680 Scenario D MFCC 17.42 0.6694 0.6833 16.54 0.6639 0.",
  "7192 Fusion 16.59 0.6176 0.6264 15.70 0.6363 0.6680 Scenario D MFCC 17.42 0.6694 0.6833 16.54 0.6639 0.6820 PLP 18.49 0.6851 0.7062 17.46 0.6852 0.7054 Fusion 17.03 0.6171 0.6315 15.73 0.6243 0.6410 Scenario E MFCC 16.65 0.6976 0.7124 16.24 0.6972 0.7122 PLP 18.48 0.7182 0.7324 17.49 0.7263 0.7480 Fusion 16.82 0.6343 0.6500 15.52 0.6471 0.6737 PLP, however, in fusion we can see that NDA resulted in bet- ter performance in terms of the primary metric.",
  "7263 0.7480 Fusion 16.82 0.6343 0.6500 15.52 0.6471 0.6737 PLP, however, in fusion we can see that NDA resulted in bet- ter performance in terms of the primary metric. In scenario B, we analysed the effect of using the short-duration com- pensation technique proposed in Section 4.2. Results indicate superior performance using this technique. In scenario C, we investigated the effects of language normalization on the per- formance of the system. If we replace LN-LDA with simple LDA, we can see performance degradation in MFCC as well as fusion, however, PLP seems not to be adversely affected. The effect of using QMF is also investigated in scenario D. Finally in scenario E, we can see the major improvement ob- tained through the use of the domain adaptation technique ex- plained in Section 4.5. For our secondary submission, we incorporated a disjoint portion of the labelled development set (10 out of 20 speakers) in either LN-LDA and in-domain PLDA training.",
  "For our secondary submission, we incorporated a disjoint portion of the labelled development set (10 out of 20 speakers) in either LN-LDA and in-domain PLDA training. We evaluated the system on almost 6k out of 24k trials from the other portion to avoid any over-\ufb01tting, par- ticularly important for the domain adaptation technique. This resulted in a relative improvement of 11% compared to the primary system in terms of the primary metric. However, the results can be misleading, since the recording condition may be the same for all speakers in the development set. 6. TIME ANALYSIS This section reports on the CPU execution time (single threaded), and the amount of memory used to process a single trial, which includes the time for creating models from the enrolment data and the time needed for processing the test segments. The analysis was performed on an Intel(R) Xeon(R) CPU E5-2670 2.60GHz. The results are shown in Table 3. We used the time command in Unix to report these results. The user time is the actual CPU time used in exe- cuting the process (single thread).",
  "The results are shown in Table 3. We used the time command in Unix to report these results. The user time is the actual CPU time used in exe- cuting the process (single thread). The real time is the wall clock time (the elapsed time including time slices used by other processes and the time the process spends blocked). The system time is also the amount of CPU time spent in the kernel within the process. We have also reported the memory allocated for each stage of execution. The most computation- ally intensive stage is the extraction of i-vectors (both MFCC- and PLP-based i-vectors), which also depends on the duration of the segments. For enrolment, we have reported the time",
  "Table 3. CPU execution time and the amount of memory required to process a single trial. segment dur speech dur stage user time system time real time memory(MB) Enrolment 140s 60s features 0.99s 0.02s 1.02s 12 SAD 8.10s 0.23s 2.26s 25.0 i-vectors 27.47s 2.20s 7.83s 4,014 Test 36s 25s features 0.52s 0.01s 0.54s 7.5 SAD 2.26s 0.09s 0.94s 25.35 i-vectors 26.02s 2.25s 7.9s 4,013 required to extract a model from a segment with a duration of 140 seconds and speech duration of 60 seconds. The time and memory required for front-end processing are negligible compared to the i-vector extraction stage, since they only include matrix operations. The time required for our SAD is also reported which increases linearly with the duration of segment. 7.",
  "The time and memory required for front-end processing are negligible compared to the i-vector extraction stage, since they only include matrix operations. The time required for our SAD is also reported which increases linearly with the duration of segment. 7. CONCLUSIONS AND PERSPECTIVES We have presented the Intelligent Voice speaker recognition system used for the NIST 2016 speaker recognition eval- uation. Our system is based on a score fusion of MFCC- and PLP-based i-vector/PLDA systems. We have described the main components of the system including, acoustic fea- ture extraction, speech activity detection, i-vector extraction as front-end processing, and language normalization, short- duration compensation, channel compensation and domain adaptation as back-end processing. For our future work, we intend to use the ALISP segmentation technique [2] in order to extract meaningful acoustic units so as to train supervised GMM or DNN models. 8. REFERENCES [1] N. Br\u00a8ummer and E. de Villiers. The bosaris toolkit user guide: Theory, algorithms and code for binary classi\ufb01er score processing. Documentation of BOSARIS toolkit, 2011.",
  "8. REFERENCES [1] N. Br\u00a8ummer and E. de Villiers. The bosaris toolkit user guide: Theory, algorithms and code for binary classi\ufb01er score processing. Documentation of BOSARIS toolkit, 2011. [2] G. Chollet, J. \u02c7Cernock`y, A. Constantinescu, S. Deligne, and F. Bimbot. Toward alisp: A proposal for automatic language independent speech processing. In Computa- tional Models of Speech Pattern Processing, pages 375\u2013 388. Springer, 1999. [3] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet. Front-end factor analysis for speaker veri\ufb01- cation. Audio, Speech, and Language Processing, IEEE Transactions on, 19(4):788\u2013798, 2011. [4] K. Fukunaga and J. Mantock. Nonparametric discrim- inant analysis.",
  "Audio, Speech, and Language Processing, IEEE Transactions on, 19(4):788\u2013798, 2011. [4] K. Fukunaga and J. Mantock. Nonparametric discrim- inant analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, (6):671\u2013678, 1983. [5] D. Garcia-Romero and C. Y. Espy-Wilson. Analysis of i-vector length normalization in speaker recognition sys- tems. In INTERSPEECH, pages 249\u2013252, 2011. [6] D. Garcia-Romero, A. McCree, S. Shum, N. Brum- mer, and C. Vaquero. Unsupervised domain adapta- tion for i-vector speaker recognition. In Proceedings of Odyssey, The Speaker and Language Recognition Work- shop, Joensuu, Finalnd, 2014.",
  "Unsupervised domain adapta- tion for i-vector speaker recognition. In Proceedings of Odyssey, The Speaker and Language Recognition Work- shop, Joensuu, Finalnd, 2014. [7] C. S. Greenberg, D. Bans\u00b4e, G. R. Doddington, D. Garcia-Romero, J. J. Godfrey, T. Kinnunen, A. F. Martin, A. McCree, M. Przybocki, and D. A. Reynolds. The nist 2014 speaker recognition i-vector machine learning challenge. In Proceedings of Odyssey, The Speaker and Language Recognition Workshop, Joensuu, Finland, 2014. [8] P. Kenny. Bayesian speaker veri\ufb01cation with heavy- tailed priors. In Proceedings of Odyssey, The Speaker and Language Recognition Workshop, page 14, 2010. [9] P. Kenny, V. Gupta, T. Stafylakis, P. Ouellet, and J. Alam. Deep neural networks for extracting baum- welch statistics for speaker recognition.",
  "[9] P. Kenny, V. Gupta, T. Stafylakis, P. Ouellet, and J. Alam. Deep neural networks for extracting baum- welch statistics for speaker recognition. In Proceed- ings of Odyssey, The Speaker and Language Recogni- tion Workshop, pages 293\u2013298, Joensuu, Finland, 2014. [10] P. Kenny, T. Stafylakis, P. Ouellet, M. J. Alam, and P. Dumouchel. Plda for speaker veri\ufb01cation with utter- ances of arbitrary duration. In 2013 IEEE International Conference on Acoustics, Speech and Signal Process- ing, pages 7649\u20137653. IEEE, 2013. [11] A. Khosravani and M. Homayounpour. Linearly con- strained minimum variance for robust i-vector based speaker recognition. In Proceedings of Odyssey, The Speaker and Language Recognition Workshop, pages 249\u2013253, Joensuu, Finland, 2014.",
  "[11] A. Khosravani and M. Homayounpour. Linearly con- strained minimum variance for robust i-vector based speaker recognition. In Proceedings of Odyssey, The Speaker and Language Recognition Workshop, pages 249\u2013253, Joensuu, Finland, 2014. [12] E. Khoury, L. El Shafey, M. Ferras, and S. Marcel. Hier- archical speaker clustering methods for the nist i-vector challenge. In Proceedings of Odyssey, The Speaker and Language Recognition Workshop, Joensuu, Finland, 2014.",
  "[13] Y. Lei, N. Scheffer, L. Ferrer, and M. McLaren. A novel scheme for speaker recognition using a phonetically- aware deep neural network. In 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1695\u20131699. IEEE, 2014. [14] M. McLaren, M. I. Mandasari, and D. A. van Leeuwen. Source normalization for language-independent speaker recognition using i-vectors. In Proceedings of Odyssey, The Speaker and Language Recognition Workshop, pages 55\u201361, Singapore, 2012. [15] M. Mclaren and D. Van Leeuwen. Source-normalized lda for robust speaker recognition using i-vectors from multiple speech sources. Audio, Speech, and Lan- guage Processing, IEEE Transactions on, 20(3):755\u2013 766, 2012. [16] N. Mesgarani, M. Slaney, and S. A. Shamma. Discrim- ination of speech from nonspeech based on multiscale spectro-temporal modulations.",
  "[16] N. Mesgarani, M. Slaney, and S. A. Shamma. Discrim- ination of speech from nonspeech based on multiscale spectro-temporal modulations. IEEE Transactions on Audio, Speech, and Language Processing, 14(3):920\u2013 930, 2006. [17] T. Ng, B. Zhang, L. Nguyen, S. Matsoukas, X. Zhou, N. Mesgarani, K. Vesel`y, and P. Matejka. Developing a speech activity detection system for the darpa rats pro- gram. In INTERSPEECH, pages 1969\u20131972, 2012. [18] S. Novoselov, T. Pekhovsky, and K. Simonchik. Stc speaker recognition system for the nist i-vector chal- lenge. In Proceedings of Odyssey, The Speaker and Lan- guage Recognition Workshop, pages 231\u2013240, Joensuu, Finland, 2014. [19] T. Pfau, D. P. Ellis, and A. Stolcke.",
  "In Proceedings of Odyssey, The Speaker and Lan- guage Recognition Workshop, pages 231\u2013240, Joensuu, Finland, 2014. [19] T. Pfau, D. P. Ellis, and A. Stolcke. Multispeaker speech activity detection for the icsi meeting recorder. In Au- tomatic Speech Recognition and Understanding, 2001. ASRU\u201901. IEEE Workshop on, pages 107\u2013110. IEEE, 2001. [20] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz, et al. The kaldi speech recognition toolkit. In IEEE 2011 workshop on automatic speech recognition and understanding. IEEE Signal Processing Society, 2011. [21] N. Ryant, M. Liberman, and J. Yuan. Speech activity detection on youtube using deep neural networks. In INTERSPEECH, pages 728\u2013731, 2013.",
  "IEEE Signal Processing Society, 2011. [21] N. Ryant, M. Liberman, and J. Yuan. Speech activity detection on youtube using deep neural networks. In INTERSPEECH, pages 728\u2013731, 2013. [22] S. O. Sadjadi, S. Ganapathy, and J. Pelecanos. The ibm 2016 speaker recognition system. In Odyssey 2016: The Speaker and Language Recognition Workshop, pages 174\u2013180, Bilbao, Spain, June 21-24 2016. [23] B. Vesnicer, J. Zganec-Gros, S. Dobrisek, and V. Struc. Incorporating duration information into i-vector-based speaker-recognition systems. In Proceedings of Odyssey, The Speaker and Language Recognition Work- shop, pages 241\u2013248, Joensuu, Finland, 2014."
]