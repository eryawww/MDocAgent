{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Generating Classical Chinese Poems from Vernacular Chinese Zhichao Yang1\u2217, Pengshan Cai1\u2217, Yansong Feng2, Fei Li1, Weijiang Feng3, Elena Suet-Ying Chiu1, Hong Yu1 1 University of Massachusetts, MA, USA {zhichaoyang, pengshancai}@umass.edu foxlf823@gmail.com chiu@llc.umass.edu hong yu@uml.edu 2 Institute of Computer Science and Technology, Peking University, China fengyansong@pku.edu.cn 3 College of Computer, National University of Defense Technology, China fengweijiang14@nudt.edu.cn Abstract Classical Chinese poetry is a jewel in the trea- sure house of Chinese culture. Previous poem generation models only allow users to employ keywords to interfere the meaning of gener- ated poems, leaving the dominion of gener- ation to the model. In this paper, we pro- pose a novel task of generating classical Chi- nese poems from vernacular, which allows users to have more control over the semantic of generated poems.",
      "In this paper, we pro- pose a novel task of generating classical Chi- nese poems from vernacular, which allows users to have more control over the semantic of generated poems. We adapt the approach of unsupervised machine translation (UMT) to our task. We use segmentation-based padding and reinforcement learning to address under- translation and over-translation respectively. According to experiments, our approach sig- ni\ufb01cantly improve the perplexity and BLEU compared with typical UMT models. Further- more, we explored guidelines on how to write the input vernacular to generate better poems. Human evaluation showed our approach can generate high-quality poems which are com- parable to amateur poems. 1 Introduction During thousands of years, millions of classical Chinese poems have been written. They contain ancient poets\u2019 emotions such as their apprecia- tion for nature, desiring for freedom and concerns for their countries. Among various types of clas- sical poetry, quatrain poems stand out. On the one hand, their aestheticism and terseness exhibit unique elegance. On the other hand, composing such poems is extremely challenging due to their phonological, tonal and structural restrictions.",
      "Among various types of clas- sical poetry, quatrain poems stand out. On the one hand, their aestheticism and terseness exhibit unique elegance. On the other hand, composing such poems is extremely challenging due to their phonological, tonal and structural restrictions. Most previous models for generating classical Chinese poems (He et al., 2012; Zhang and Lap- ata, 2014) are based on limited keywords or char- acters at \ufb01xed positions (e.g., acrostic poems). \u2217Equal contribution Since users could only interfere with the semantic of generated poems using a few input words, mod- els control the procedure of poem generation. In this paper, we proposed a novel model for classical Chinese poem generation. As illustrated in Figure 1, our model generates a classical Chinese poem based on a vernacular Chinese paragraph. Our ob- jective is not only to make the model generate aes- thetic and terse poems, but also keep rich semantic of the original vernacular paragraph. Therefore, our model gives users more control power over the semantic of generated poems by carefully writing the vernacular paragraph.",
      "Our ob- jective is not only to make the model generate aes- thetic and terse poems, but also keep rich semantic of the original vernacular paragraph. Therefore, our model gives users more control power over the semantic of generated poems by carefully writing the vernacular paragraph. Although a great number of classical poems and vernacular paragraphs are easily available, there exist only limited human-annotated pairs of poems and their corresponding vernacular translations. Thus, it is unlikely to train such poem generation model using supervised approaches. Inspired by unsupervised machine translation (UMT) (Lam- ple et al., 2018b), we treated our task as a transla- tion problem, namely translating vernacular para- graphs to classical poems. However, our work is not just a straight-forward application of UMT. In a training example for UMT, the length difference of source and target languages are usually not large, but this is not true in our task. Classical poems tend to be more con- cise and abstract, while vernacular text tends to be detailed and lengthy.",
      "In a training example for UMT, the length difference of source and target languages are usually not large, but this is not true in our task. Classical poems tend to be more con- cise and abstract, while vernacular text tends to be detailed and lengthy. Based on our observation on gold-standard annotations, vernacular paragraphs usually contain more than twice as many Chinese characters as their corresponding classical poems. Therefore, such discrepancy leads to two main problems during our preliminary experiments: (1) Under-translation: when summarizing vernac- ular paragraphs to poems, some vernacular sen- tences are not translated and ignored by our model. Take the last two vernacular sentences in Figure arXiv:1909.00279v1  [cs.CL]  31 Aug 2019",
      "Figure 1: An example of the training procedures of our model. Here we depict two procedures, namely back translation and language modeling. Back translation has two paths, namely ES \u2192DT \u2192ET \u2192DS and DT \u2192 ES \u2192DS \u2192ET . Language modeling also has two paths, namely ET \u2192DT and ES \u2192DS. Figure 1 shows only the former one for each training procedure. 1 as examples, they are not covered in the gener- ated poem. (2) Over-translation: when expand- ing poems to vernacular paragraphs, certain words are unnecessarily translated for multiple times. For example, the last sentence in the generated poem of Figure 1, as green as sapphire, is back- translated as as green as as as sapphire. Inspired by the phrase segmentation schema in classical poems (Ye, 1984), we proposed the method of phrase-segmentation-based padding to handle with under-translation. By padding po- ems based on the phrase segmentation custom of classical poems, our model better aligns po- ems with their corresponding vernacular para- graphs and meanwhile lowers the risk of under- translation. Inspired by Paulus et al.",
      "By padding po- ems based on the phrase segmentation custom of classical poems, our model better aligns po- ems with their corresponding vernacular para- graphs and meanwhile lowers the risk of under- translation. Inspired by Paulus et al. (2018), we designed a reinforcement learning policy to penal- ize the model if it generates vernacular paragraphs with too many repeated words. Experiments show our method can effectively decrease the possibility of over-translation. The contributions of our work are threefold: (1) We proposed a novel task for unsupervised Chinese poem generation from vernacular text. (2) We proposed using phrase-segmentation- based padding and reinforcement learning to ad- dress two important problems in this task, namely under-translation and over-translation. (3) Through extensive experiments, we proved the effectiveness of our models and explored how to write the input vernacular to inspire better po- ems. Human evaluation shows our models are able to generate high quality poems, which are compa- rable to amateur poems. 2 Related Works Classical Chinese Poem Generation Most pre- vious works in classical Chinese poem genera- tion focus on improving the semantic coherence of generated poems.",
      "Human evaluation shows our models are able to generate high quality poems, which are compa- rable to amateur poems. 2 Related Works Classical Chinese Poem Generation Most pre- vious works in classical Chinese poem genera- tion focus on improving the semantic coherence of generated poems. Based on LSTM, Zhang and La- pata (2014) purposed generating poem lines incre- mentally by taking into account the history of what has been generated so far. Yan (2016) proposed a polishing generation schema, each poem line is generated incrementally and iteratively by re\ufb01ning each line one-by-one. Wang et al. (2016) and Yi et al. (2018) proposed models to keep the generated poems coherent and semantically consistent with the user\u2019s intent. There are also researches that fo- cus on other aspects of poem generation. (Yang et al. (2018) explored increasing the diversity of gen- erated poems using an unsupervised approach. Xu et al. (2018) explored generating Chinese poems from images.",
      "There are also researches that fo- cus on other aspects of poem generation. (Yang et al. (2018) explored increasing the diversity of gen- erated poems using an unsupervised approach. Xu et al. (2018) explored generating Chinese poems from images. While most previous works gener- ate poems based on topic words, our work targets at a novel task: generating poems from vernacular Chinese paragraphs. Unsupervised Machine Translation Compared",
      "with supervised machine translation approaches (Cho et al., 2014; Bahdanau et al., 2015), un- supervised machine translation (Lample et al., 2018a,b) does not rely on human-labeled parallel corpora for training. This technique is proved to greatly improve the performance of low-resource languages translation systems. (e.g. English-Urdu translation). The unsupervised machine transla- tion framework is also applied to various other tasks, e.g. image captioning (Feng et al., 2019), text style transfer (Zhang et al., 2018), speech to text translation (Bansal et al., 2017) and clin- ical text simpli\ufb01cation (Weng et al., 2019). The UMT framework makes it possible to apply neu- ral models to tasks where limited human labeled data is available. However, in previous tasks that adopt the UMT framework, the abstraction levels of source and target language are the same. This is not the case for our task. Under-Translation & Over-Translation Both are troublesome problems for neural sequence- to-sequence models.",
      "However, in previous tasks that adopt the UMT framework, the abstraction levels of source and target language are the same. This is not the case for our task. Under-Translation & Over-Translation Both are troublesome problems for neural sequence- to-sequence models. Most previous related re- searches adopt the coverage mechanism (Tu et al., 2016; Mi et al., 2016; Sankaran et al., 2016). However, as far as we know, there were no suc- cessful attempt applying coverage mechanism to transformer-based models (Vaswani et al., 2017). 3 Model 3.1 Main Architecture We transform our poem generation task as an un- supervised machine translation problem.",
      "However, as far as we know, there were no suc- cessful attempt applying coverage mechanism to transformer-based models (Vaswani et al., 2017). 3 Model 3.1 Main Architecture We transform our poem generation task as an un- supervised machine translation problem. As il- lustrated in Figure 1, based on the recently pro- posed UMT framework (Lample et al., 2018b), our model is composed of the following components: \u2022 Encoder Es and decoder Ds for vernacular paragraph processing \u2022 Encoder Et and decoder Dt for classical poem processing where Es (or Et) takes in a vernacular paragraph (or a classical poem) and converts it into a hidden representation, and Ds (or Dt) takes in the hid- den representation and converts it into a vernac- ular paragraph (or a poem). Our model relies on a vernacular texts corpus S and a poem corpus T. We denote S and T as instances in S and T respec- tively. The training of our model relies on three proce- dures, namely parameter initialization, language modeling and back-translation. We will give de- tailed introduction to each procedure.",
      "The training of our model relies on three proce- dures, namely parameter initialization, language modeling and back-translation. We will give de- tailed introduction to each procedure. Parameter initialization As both vernacular and classical poem use Chinese characters, we initial- ize the character embedding of both languages in one common space, the same character in two lan- guages shares the same embedding. This initial- ization helps associate characters with their plau- sible translations in the other language. Language modeling It helps the model generate texts that conform to a certain language. A well- trained language model is able to detect and cor- rect minor lexical and syntactic errors. We train the language models for both vernacular and clas- sical poem by minimizing the following loss: Llm = E S\u2208S[\u2212log P(S|Ds(Es(SN))]+ E T\u2208T[\u2212log P(T|Dt(Et(TN))], (1) where SN (or TN) is generated by adding noise (drop, swap or blank a few words) in S (or T).",
      "Back-translation Based on a vernacular para- graph S, we generate a poem TS using Es and Dt, we then translate TS back into a vernacular para- graph STS = Ds(Et(TS)). Here, S could be used as gold standard for the back-translated paragraph STs. In this way, we could turn the unsupervised translation into a supervised task by maximizing the similarity between S and STS. The same also applies to using poem T as gold standard for its corresponding back-translation TST . We de\ufb01ne the following loss: Lbt = E S\u2208S[\u2212log P(S|Ds(Et(TS))]+ E T\u2208T[\u2212log P(T|Dt(Es(ST ))]. (2) Note that Lbt does not back propagate through the generation of TS and ST as we observe no im- provement in doing so. When training the model, we minimize the composite loss: L = \u03b11Llm + \u03b12Lbt, (3) where \u03b11 and \u03b12 are scaling factors.",
      "When training the model, we minimize the composite loss: L = \u03b11Llm + \u03b12Lbt, (3) where \u03b11 and \u03b12 are scaling factors. 3.2 Addressing Under-Translation and Over-Translation During our early experiments, we realize that the naive UMT framework is not readily applied to our task. Classical Chinese poems are featured for",
      "its terseness and abstractness. They usually focus on depicting broad poetic images rather than de- tails. We collected a dataset of classical Chinese poems and their corresponding vernacular transla- tions, the average length of the poems is 32.0 char- acters, while for vernacular translations, it is 73.3. The huge gap in sequence length between source and target language would induce over-translation and under-translation when training UMT models. In the following sections, we explain the two prob- lems and introduce our improvements. 3.2.1 Under-Translation By nature, classical poems are more concise and abstract while vernaculars are more detailed and lengthy, to express the same meaning, a vernacular paragraph usually contains more characters than a classical poem. As a result, when summarizing a vernacular paragraph S to a poem TS, TS may not cover all information in S due to its length limit. In real practice, we notice the generated poems usu- ally only cover the information in the front part of the vernacular paragraph, while the latter part is unmentioned. To alleviate under-translation, we propose phrase segmentation-based padding.",
      "In real practice, we notice the generated poems usu- ally only cover the information in the front part of the vernacular paragraph, while the latter part is unmentioned. To alleviate under-translation, we propose phrase segmentation-based padding. Speci\ufb01cally, we \ufb01rst segment each line in a classical poem into several sub-sequences, we then join these sub- sequences with the special padding tokens <p>. During training, the padded lines are used instead of the original poem lines. As illustrated in Figure 2, padding would create better alignments between a vernacular paragraph and a prolonged poem, making it more likely for the latter part of the ver- nacular paragraph to be covered in the poem. As we mentioned before, the length of the vernacu- lar translation is about twice the length of its cor- responding classical poem, so we pad each seg- mented line to twice its original length. According to Ye (1984), to present a stronger sense of rhythm, each type of poem has its unique phrase segmentation schema, for example, most seven-character quatrain poems adopt the 2-2-3 schema, i.e.",
      "According to Ye (1984), to present a stronger sense of rhythm, each type of poem has its unique phrase segmentation schema, for example, most seven-character quatrain poems adopt the 2-2-3 schema, i.e. each quatrain line contains 3 phrases, the \ufb01rst, second and third phrase contains 2, 2, 3 characters respectively. Inspired by this law, we segment lines in a poem according to the cor- responding phrase segmentation schema. In this way, we could avoid characters within the scope of a phrase to be cut apart, thus best preserve the semantic of each phrase.(Chang et al., 2008) Figure 2: A real example to show the effectiveness of our phrase-segmentation-based padding. Without padding, the vernacular paragraph could not be aligned well with the poem. Therefore, the text in South Yangtze ends but the grass and trees have not withered in red is not covered in the poem. By contrast, they are covered well after using our padding method.",
      "Without padding, the vernacular paragraph could not be aligned well with the poem. Therefore, the text in South Yangtze ends but the grass and trees have not withered in red is not covered in the poem. By contrast, they are covered well after using our padding method. 3.2.2 Over-Translation In NMT, when decoding is complete, the decoder would generate an <EOS>token, indicating it has reached the end of the output sequence. However, when expending a poem T into a vernacular Chi- nese paragraph ST , due to the conciseness nature of poems, after \ufb01nishing translating every source character in T, the output sequence ST may still be much shorter than the expected length of a poem\u2018s vernacular translation. As a result, the decoder would believe it has not \ufb01nished decoding. In- stead of generating the <EOS>token, the decoder would continue to generate new output characters from previously translated source characters. This would cause the decoder to repetitively output a piece of text many times.",
      "As a result, the decoder would believe it has not \ufb01nished decoding. In- stead of generating the <EOS>token, the decoder would continue to generate new output characters from previously translated source characters. This would cause the decoder to repetitively output a piece of text many times. To remedy this issue, in addition to minimizing the original loss function L, we propose to mini- mize a speci\ufb01c discrete metric, which is made pos- sible with reinforcement learning. We de\ufb01ne repetition ratio RR(S) of a para- graph S as: RR(S) = 1 \u2212vocab(S) len(S) , (4) where vocab(S) refers to the number of distinc- tive characters in S, len(S) refers the number of all characters in S. Obviously, if a gener- ated sequence contains many repeated characters, it would have high repetition ratio. Following the self-critical policy gradient training (Rennie et al.,",
      "Training set Validation set Test set # Poems 163K 19K 487 Average length of poems 32.0 32.0 32.0 # vernacular paragraphs 337K 19K 487 Average length of vernacular paragraphs 71.8 76.8 73.3 Table 1: Statistics of our dataset 2017), we de\ufb01ne the following loss function: Lrl = E S\u2208S[(RR(STS) \u2212\u03c4) log P(S|Ds(Et(TS))], (5) where \u03c4 is a manually set threshold. Intuitively, minimizing Lrl is equivalent to maximizing the conditional likelihood of the sequence S given STS if its repetition ratio is lower than the thresh- old \u03c4. Following (Wu et al., 2016), we revise the composite loss as: L\u2032 = \u03b11Llm + \u03b12Lbt + \u03b13Lrl, (6) where \u03b11, \u03b12, \u03b13 are scaling factors. 4 Experiment The objectives of our experiment are to explore the following questions: (1) How much do our mod- els improve the generated poems?",
      "4 Experiment The objectives of our experiment are to explore the following questions: (1) How much do our mod- els improve the generated poems? (Section 4.4) (2) What are characteristics of the input vernacu- lar paragraph that lead to a good generated poem? (Section 4.5) (3) What are weaknesses of gener- ated poems compared to human poems? (Section 4.6) To this end, we built a dataset as described in Section 4.1. Evaluation metrics and baselines are described in Section 4.2 and 4.3. For the imple- mentation details of building the dataset and mod- els, please refer to supplementary materials.1 4.1 Datasets Training and Validation Sets We collected a cor- pus of poems and a corpus of vernacular literature from online resources. The poem corpus contains 163K quatrain poems from Tang Poems and Song Poems, the vernacular literature corpus contains 337K short paragraphs from 281 famous books, the corpus covers various literary forms including prose, \ufb01ction and essay. Note that our poem cor- pus and a vernacular corpus are not aligned.",
      "Note that our poem cor- pus and a vernacular corpus are not aligned. We further split the two corpora into a training set and a validation set. 1Our data and code is publicly available at https://github.com/whaleloops/interpoetry Test Set From online resources, we collected 487 seven-character quatrain poems from Tang Poems and Song Poems, as well as their corresponding high quality vernacular translations. These poems could be used as gold standards for poems gener- ated from their corresponding vernacular transla- tions. Table 1 shows the statistics of our training, validation and test set. 4.2 Evaluation Metrics Perplexity Perplexity re\ufb02ects the probability a model generates a certain poem. Intuitively, a bet- ter model would yield higher probability (lower perplexity) on the gold poem. BLEU As a standard evaluation metric for ma- chine translation, BLEU (Papineni et al., 2001) measures the intersection of n-grams between the generated poem and the gold poem. A better gen- erated poem usually achieves higher BLEU score, as it shares more n-gram with the gold poem.",
      "A better gen- erated poem usually achieves higher BLEU score, as it shares more n-gram with the gold poem. Human evaluation While perplexity and BLEU are objective metrics that could be applied to large-volume test set, evaluating Chinese poems is after all a subjective task. We invited 30 human evaluators to join our human evaluation. The hu- man evaluators were divided into two groups. The expert group contains 15 people who hold a bach- elor degree in Chinese literature, and the amateur group contains 15 people who holds a bachelor de- gree in other \ufb01elds. All 30 human evaluators are native Chinese speakers. We ask evaluators to grade each generated poem from four perspectives: 1) Fluency: Is the gen- erated poem grammatically and rhythmically well formed, 2) Semantic coherence: Is the generated poem itself semantic coherent and meaningful, 3) Semantic preservability: Does the generated poem preserve the semantic of the modern Chi- nese translation, 4) Poeticness: Does the generated poem display the characteristic of a poem and does the poem build good poetic image.",
      "The grading scale for each perspective is from 1 to 5.",
      "ID Vernacular paragraph Generated poem 1 \u9752\u5c71\u9690\u9690\u7ea6\u7ea6\u7eff\u6c34\u5343\u91cc\u8fe2\u8fe2\uff0c\u79cb\u65f6\u5df2\u5c3d\u6c5f\u5357\u8349\u6728\u8fd8\u672a\u67af\u51cb\u3002\u4e8c \u5341\u56db\u6865\u660e\u6708\u6620\u7167\u5e7d\u5e7d\u6e05\u591c\uff0c\u4f60\u8fd9\u7f8e\u4eba\u73b0\u5728\u4f55\u5904\u6559\u4eba\u5439\u7bab\uff1f \u9752\u5c71\u9690\u9690\u7eff\u6c34\u5149\uff0c\u5343\u91cc\u79cb\u65f6\u5df2\u5c3d\u85cf\u3002 \u6c5f\u5357\u8349\u6728\u8fd8\u672a\u67af\uff0c\u4e8c\u5341\u56db\u6865\u5e7d\u591c\u9999\u3002 1 Blue peak is blur and faint, green river meaders thousands miles, the southern grass has not dried up in the autumn. Twenty-four hole bridges and bright moons shine in the clear night. where do you beautifully teach people to \ufb02ute? Blue peak blurs and green river shines, thousands miles away, autumn has been hidden. the southern grass has not dried up in the autumn. Twenty-four hole bridges smells beautiful and quiet tonight.",
      "where do you beautifully teach people to \ufb02ute? Blue peak blurs and green river shines, thousands miles away, autumn has been hidden. the southern grass has not dried up in the autumn. Twenty-four hole bridges smells beautiful and quiet tonight. 2 \u62c2\u8896\u8d77\u821e\u4e8e\u68a6\u4e2d\u5f98\u5f8a\uff0c\u76f8\u601d\u8513\u4e0a\u5fc3\u6249\u3002\u5979\u7737\u604b\u68a8\u82b1\u6cea\uff0c\u9759\u753b\u7ea2 \u5986\u7b49\u8c01\u5f52\uff0c\u7a7a\u7559\u4f0a\u4eba\u5f90\u5f90\u6194\u60b4\u3002 \u62c2\u8896\u8d77\u821e\u68a6\u5f98\u5f8a\uff0c\u76f8\u601d\u8513\u4e0a\u5fc3\u6249\u5f00\u3002 \u7389\u7737\u68a8\u82b1\u6cea\u75d5\u9759\uff0c\u753b\u7ea2\u7b49\u8c01\u5f52\u53bb\u6765\u3002 2 The sleeves danced in the dream, and the lovesickness was on the heart. She is in love with the tears of pears, and who is quietly wearing red makeup, only left alone to be languished slowly. The sleeves danced in the dream, the lovesickness appeared in the heart.",
      "She is in love with the tears of pears, and who is quietly wearing red makeup, only left alone to be languished slowly. The sleeves danced in the dream, the lovesickness appeared in the heart. Jade concerns tears of pears but the mark is still, wearing red makeup waiting for the one to come and go. 3 \u7a97\u5916\u7684\u9ebb\u96c0\u5728\u7535\u7ebf\u6746\u4e0a\u591a\u5634\uff0c\u4f60\u8bf4\u8fd9\u4e00\u53e5\u5f88\u6709\u590f\u5929\u7684\u611f\u89c9\u3002\u624b \u4e2d\u7684\u94c5\u7b14\u5728\u7eb8\u4e0a\u6765\u6765\u56de\u56de\uff0c\u6211\u7528\u51e0\u884c\u5b57\u5f62\u5bb9\u4f60\u662f\u6211\u7684\u8c01\u3002 \u7a97\u4e0b\u9ebb\u59d1\u706f\u706b\u591a\uff0c\u95f2\u4e2d\u8bf4\u4e0e\u4e07\u7f18\u4f55\u3002 \u590f\u9891\u624b\u628a\u94c5\u534e\u7eb8\uff0c\u6765\u5f80\u56de\u5934\u7528\u51e0\u591a\u3002 3 The sparrow outside the window is talking on the pole. You say this sentence makes you feel very summer. The pencil in my hand is writing back and forth on the paper. I only use a few lines to describe who you are to me.",
      "You say this sentence makes you feel very summer. The pencil in my hand is writing back and forth on the paper. I only use a few lines to describe who you are to me. Under the window lie sparrow girls in this prosperous city, chit chatting about the destiny of the world. Summer hands over many drawing canvas, Looking back and forth, how many do you need? 4 \u96e8\u5929\u7684\u5c4b\u74e6\uff0c\u6d6e\u6f3e\u6e7f\u6e7f\u7684\u6d41\u5149\uff0c\u7070\u800c\u6e29\u67d4\uff0c\u8fce\u5149\u5219\u5fae\u660e\uff0c\u80cc\u5149 \u5219\u5e7d\u9eef\uff0c\u5bf9\u4e8e\u89c6\u89c9\uff0c\u662f\u4e00\u79cd\u4f4e\u6c89\u7684\u5b89\u6170\u3002 \u96e8\u4f59\u5c4b\u74e6\u6d6e\u6f3e\u6e7f\uff0c\u6d41\u5149\u7070\u8272\u6696\u76f8\u8fce\u3002 \u5149\u5219\u5fae\u660e\u80cc\u5219\u8272\uff0c\u5e7d\u4eba\u9eef\u9eef\u5bf9\u98ce\u6e05\u3002 4 The rainy days of the roof tiles are soaking wet and wet, gray and gentle, Facing the light, it is slightly bright, Against the light, it is pitch dark, For the concept of vision, it is a deep comfort.",
      "The excess rain makes roof tiles rippling, ambilight gray is warm and welcoming. Light is slightly bright, against is pure color. The person hides in dark but faces wind breeze. 5 \u53ea\u8981\u5386\u53f2\u4e0d\u963b\u65ad\uff0c\u65f6\u95f4\u4e0d\u5012\u9000\uff0c\u4e00\u5207\u90fd\u4f1a\u8870\u8001\u3002\u8001\u5c31\u8001\u4e86\u5427\uff0c \u5b89\u8be6\u5730\u4ea4\u7ed9\u4e16\u754c\u4e00\u526f\u6148\u7965\u7f8e\u3002\u5047\u9970\u5929\u771f\u662f\u6700\u6b8b\u9177\u7684\u81ea\u6211\u7cdf\u8df5\u3002 \u53ea\u8981\u8bf8\u516c\u4e0d\u963b\u65f6\uff0c\u4e0d\u5012\u9000\u98df\u4e00\u5c18\u57c3\u3002 \u4f1a\u8870\u8001\u77e3\u5b89\u5206\u4e16\uff0c\u4e00\u526f\u6148\u7965\u5047\u6b64\u6765\u3002 5 As long as history does not block, time does not go backwards, everything will age. It is \ufb01ne to get old, and handing it to the world with kindness. Faking innocence is the cruelest self-destruction. As long as people do not block time, it will not go backwards and absorbs into a dust. People should stay chill and get old.",
      "It is \ufb01ne to get old, and handing it to the world with kindness. Faking innocence is the cruelest self-destruction. As long as people do not block time, it will not go backwards and absorbs into a dust. People should stay chill and get old. Faking innocence is not the way to go. Table 2: A few poems generated by our model from their corresponding vernacular paragraphs. 4.3 Baselines We compare the performance of the following models: (1) LSTM (Hochreiter and Schmidhu- ber, 1997); (2)Naive transformer (Vaswani et al., 2017); (3)Transformer + Anti OT (RL loss); (4)Transformer + Anti UT (phrase segmentation- based padding); (5)Transformer + Anti OT&UT. 4.4 Reborn Poems: Generating Poems from Vernacular Translations As illustrated in Table 2 (ID 1). Given the vernac- ular translation of each gold poem in test set, we generate \ufb01ve poems using our models. Intuitively, the more the generated poem resembles the gold poem, the better the model is.",
      "Given the vernac- ular translation of each gold poem in test set, we generate \ufb01ve poems using our models. Intuitively, the more the generated poem resembles the gold poem, the better the model is. We report mean perplexity and BLEU scores in Table 3 (Where +Anti OT refers to adding the reinforcement loss to mitigate over-\ufb01tting and +Anti UT refers to adding phrase segmentation-based padding to mit- igate under-translation), human evaluation results in Table 4.2 According to experiment results, perplexity, BLEU scores and total scores in human evalu- ation are consistent with each other. We ob- serve all BLEU scores are fairly low, we be- lieve it is reasonable as there could be multiple ways to compose a poem given a vernacular para- graph. Among transformer-based models, both +Anti OT and +Anti UT outperforms the naive transformer, while Anti OT&UT shows the best performance, this demonstrates alleviating under- translation and over-translation both helps gener- ate better poems. Speci\ufb01cally, +Anti UT shows bigger improvement than +Anti OT.",
      "Speci\ufb01cally, +Anti UT shows bigger improvement than +Anti OT. According to human evaluation, among the four perspectives, our Anti OT&UT brought most score improve- ment in Semantic preservability, this proves our improvement on semantic preservability was most obvious to human evaluators. All transformer- 2We did not use LSTM in human evaluation since its per- formance is worse as shown in Table 3.",
      "Model Perplexity BLEU BLEU-1 BLEU-2 BLEU-3 BLEU-4 LSTM 118.27 3.81 39.16 6.93 1.58 0.49 Transformer 105.79 5.50 40.92 8.02 2.46 1.11 +Anti OT 77.33 6.08 41.22 8.72 2.82 1.36 +Anti UT 74.21 6.34 42.20 9.04 2.96 1.44 +Anti OT&UT 65.58 6.57 42.53 8.98 2.96 1.46 Table 3: Perplexity and BLEU scores of generating poems from vernacular translations. Since perplexity and BLEU scores on the test set \ufb02uctuates from epoch to epoch, we report the mean perplexity and BLEU scores over 5 consecutive epochs after convergence.",
      "Since perplexity and BLEU scores on the test set \ufb02uctuates from epoch to epoch, we report the mean perplexity and BLEU scores over 5 consecutive epochs after convergence. Model Fluency Semantic coherence Semantic preservability Poeticness Total Transformer 2.63 2.54 2.12 2.46 9.75 +Anti OT 2.80 2.75 2.44 2.71 10.70 +Anti UT 2.82 2.82 2.86 2.85 11.35 +Anti OT&UT 3.21 3.27 3.27 3.28 13.13 Table 4: Human evaluation results of generating poems from vernacular translations. We report the mean scores for each evaluation metric and total scores of four metrics. based models outperform LSTM. Note that the av- erage length of the vernacular translation is over 70 characters, comparing with transformer-based models, LSTM may only keep the information in the beginning and end of the vernacular. We an- ticipated some score inconsistency between expert group and amateur group.",
      "Note that the av- erage length of the vernacular translation is over 70 characters, comparing with transformer-based models, LSTM may only keep the information in the beginning and end of the vernacular. We an- ticipated some score inconsistency between expert group and amateur group. However, after analyz- ing human evaluation results, we did not observed big divergence between two groups. 4.5 Interpoetry: Generating Poems from Various Literature Forms Chinese literature is not only featured for classi- cal poems, but also various other literature forms. Song lyric(\u5b8b\u8bcd), or ci also gained tremendous popularity in its palmy days, standing out in classi- cal Chinese literature. Modern prose, modern po- ems and pop song lyrics have won extensive praise among Chinese people in modern days. The goal of this experiment is to transfer texts of other lit- erature forms into quatrain poems. We expect the generated poems to not only keep the semantic of the original text, but also demonstrate terseness, rhythm and other characteristics of ancient poems.",
      "The goal of this experiment is to transfer texts of other lit- erature forms into quatrain poems. We expect the generated poems to not only keep the semantic of the original text, but also demonstrate terseness, rhythm and other characteristics of ancient poems. Speci\ufb01cally, we chose 20 famous fragments from four types of Chinese literature (5 fragments for each of modern prose, modern poems, pop song lyrics and Song lyrics). We try to As no ground truth is available, we resorted to human evaluation with the same grading standard in Section 4.4. Comparing the scores of different literature forms, we observe Song lyric achieves higher scores than the other three forms of modern liter- ature. It is not surprising as both Song lyric and quatrain poems are written in classical Chinese, while the other three literature forms are all in ver- nacular. Comparing the scores within the same litera- ture form, we observe the scores of poems gener- ated from different paragraphs tends to vary.",
      "Comparing the scores within the same litera- ture form, we observe the scores of poems gener- ated from different paragraphs tends to vary. After carefully studying the generated poems as well as their scores, we have the following observation: 1) In classical Chinese poems, poetic images (\u610f\u8c61) were widely used to express emotions and to build artistic conception. A certain poetic image usually has some \ufb01xed implications. For example, autumn is usually used to imply sadness and lone- liness. However, with the change of time, poetic images and their implications have also changed. According to our observation, if a vernacular para- graph contains more poetic images used in classi- cal literature, its generated poem usually achieves higher score. As illustrated in Table 2, both para- graph 2 and 3 are generated from pop song lyrics, paragraph 2 uses many poetic images from clas- sical literature (e.g. pear \ufb02owers, makeup), while paragraph 3 uses modern poetic images (e.g. spar- rows on the utility pole).",
      "pear \ufb02owers, makeup), while paragraph 3 uses modern poetic images (e.g. spar- rows on the utility pole). Obviously, compared with poem 2, sentences in poem 3 seems more confusing, as the poetic images in modern times may not \ufb01t well into the language model of classi- cal poems. 2) We also observed that poems generated from descriptive paragraphs achieve higher scores than from logical or philosophical paragraphs. For ex- ample, in Table 2, both paragraph 4 (more descrip- tive) and paragraph 5 (more philosophical) were",
      "Literature form Fluency Semantic coherence Semantic preservability Poeticness Total Prose 2.52 2.30 2.30 2.32 9.44 Modern poem 2.37 2.34 2.01 2.16 8.88 Pop song lyric 2.40 2.31 2.24 2.42 9.37 Song lyric 2.62 2.54 2.26 2.49 9.91 Table 5: Human evaluation results for generating poems from various literature forms. We show the results ob- tained from our best model (Transformer+Anti OT&UT). selected from famous modern prose. However, compared with poem 4, poem 5 seems semanti- cally more confusing. We offer two explanations to the above phenomenon: i. Limited by the 28- character restriction, it is hard for quatrain poems to cover complex logical or philosophical expla- nation. ii. As vernacular paragraphs are more de- tailed and lengthy, some information in a vernac- ular paragraph may be lost when it is summarized into a classical poem.",
      "ii. As vernacular paragraphs are more de- tailed and lengthy, some information in a vernac- ular paragraph may be lost when it is summarized into a classical poem. While losing some infor- mation may not change the general meaning of a descriptive paragraph, it could make a big differ- ence in a logical or philosophical paragraph. 4.6 Human Discrimination Test We manually select 25 generated poems from ver- nacular Chinese translations and pair each one with its corresponding human written poem. We then present the 25 pairs to human evaluators and ask them to differentiate which poem is generated by human poet.3 As demonstrated in Table 6, although the gen- eral meanings in human poems and generated po- ems seem to be the same, the wordings they em- ploy are quite different. This explains the low BLEU scores in Section 4.3. According to the test results in Table 7, human evaluators only achieved 65.8% in mean accuracy. This indicates the best generated poems are somewhat comparable to po- ems written by amateur poets.",
      "This explains the low BLEU scores in Section 4.3. According to the test results in Table 7, human evaluators only achieved 65.8% in mean accuracy. This indicates the best generated poems are somewhat comparable to po- ems written by amateur poets. We interviewed evaluators who achieved higher than 80% accuracy on their differentiation strate- gies. Most interviewed evaluators state they real- ize the sentences in a human written poem are usu- ally well organized to highlight a theme or to build a poetic image, while the correlation between sen- tences in a generated poem does not seem strong. As demonstrated in Table 6, the last two sen- tences in both human poems (marked as red) echo each other well, while the sentences in machine- 3We did not require the expert group\u2019s participation as many of them have known the gold poems already. Thus us- ing their judgments would be unfair. Human \u9ec4\u6c99\u789b\u91cc\u5ba2\u884c\u8ff7\uff0c\u56db\u671b\u4e91\u5929\u76f4\u4e0b\u4f4e\u3002 Within yellow sand moraine guest travels lost, looking around found sky and clouds low.",
      "Thus us- ing their judgments would be unfair. Human \u9ec4\u6c99\u789b\u91cc\u5ba2\u884c\u8ff7\uff0c\u56db\u671b\u4e91\u5929\u76f4\u4e0b\u4f4e\u3002 Within yellow sand moraine guest travels lost, looking around found sky and clouds low. \u4e3a\u8a00\u5730\u5c3d\u5929\u8fd8\u5c3d\uff0c\u884c\u5230\u5b89\u897f\u66f4\u5411\u897f\u3002 It is said that earth and sky ends here, however I need to travel more west than anxi. Machine \u5f02\u4e61\u5ba2\u5b50\u9ec4\u6c99\u8ff7\uff0c\u96c1\u8def\u8ff7\u5bd2\u4e91\u5411\u4f4e\u3002 Guest in yellow sand gets lost, geese are lost because clouds gets low. \u53ea\u9053\u5c71\u5ddd\u5230\u6b64\u5c3d\uff0c\u5b89\u897f\u8fd8\u8981\u66f4\u5411\u897f\u3002 It is said that mountains end here, however anxi is even more west. Human \u7edd\u57df\u4ece\u519b\u8ba1\u60d8\u7136\uff0c\u4e1c\u5357\u5e7d\u6068\u6ee1\u8bcd\u7b3a\u3002 It\u2019s hard to pay for the ambition of the military \ufb01eld, the anxiety of situation in southeast is all over poems.",
      "Human \u7edd\u57df\u4ece\u519b\u8ba1\u60d8\u7136\uff0c\u4e1c\u5357\u5e7d\u6068\u6ee1\u8bcd\u7b3a\u3002 It\u2019s hard to pay for the ambition of the military \ufb01eld, the anxiety of situation in southeast is all over poems. \u4e00\u7bab\u4e00\u5251\u5e73\u751f\u610f\uff0c\u8d1f\u5c3d\u72c2\u540d\u5341\u4e94\u5e74\u3002 A \ufb02ute and sword is all I care about in my life, 15 years have failed the reputation of \u201dmadman\u201d. Machine \u4ece\u519b\u7586\u573a\u5fd7\u96be\u916c\uff0c\u4ee4\u4eba\u6005\u671b\u4e1c\u5357\u5dde\u3002 It\u2019s hard to ful\ufb01ll my ambition on the military \ufb01eld, the situation in the southeast states are troublesome. \u5f62\u5bb9\u4ed7\u5251\u654c\u5e73\u620e\uff0c\u60c5\u6000\u6ce8\u6ee1\u8d4b\u96ea\u6101\u3002 I would like to use my sword to conquer my enemy, yet my feelings are full of worry like the snow. Table 6: Examples of generated poems and their cor- responding gold poems used in human discrimination test. generated poems seem more independent.",
      "Table 6: Examples of generated poems and their cor- responding gold poems used in human discrimination test. generated poems seem more independent. This gives us hints on the weakness of generated po- ems: While neural models may generate poems that resemble human poems lexically and syntac- tically, it\u2019s still hard for them to compete with hu- man beings in building up good structures. 5 Discussion Addressing Under-Translation In this part, we wish to explore the effect of different phrase seg- mentation schemas on our phrase segmentation- based padding. According to Ye (1984), most seven-character quatrain poems adopt the 2-2-3 segmentation schema. As shown in examples in Figure 3, we compare our phrase segmentation- based padding (2-2-3 schema) to two less common schemas (i.e. 2-3-2 and 3-2-2 schema) we report our experiment results in Table 8.",
      "Accuracy Value Min 52.0 Max 84.0 Mean 65.8 Table 7: The performance of human discrimination test. Figure 3: Examples of different padding schemas. The results show our 2-2-3 segmentation- schema greatly outperforms 2-3-2 and 3-2-2 schema in both perplexity and BLEU scores. Note that the BLEU scores of 2-3-2 and 3-2-2 schema remains almost the same as our naive baseline (Without padding). According to the observation, we have the following conclusions: 1) Although padding better aligns the vernacular paragraph to the poem, it may not improve the quality of the generated poem. 2) The padding tokens should be placed according to the phrase segmentation schema of the poem as it preserves the semantic within the scope of each phrase. Addressing Over-Translation To explore the ef- fect of our reinforcement learning policy on al- leviating over-translation, we calculate the rep- etition ratio of vernacular paragraphs generated from classical poems in our validation set.",
      "Addressing Over-Translation To explore the ef- fect of our reinforcement learning policy on al- leviating over-translation, we calculate the rep- etition ratio of vernacular paragraphs generated from classical poems in our validation set. We found naive transformer achieves 40.8% in rep- etition ratio, while our +Anti OT achieves 34.9%. Given the repetition ratio of vernacular paragraphs (written by human beings) in our validation set is 30.1%, the experiment results demonstrated our RL loss effectively alleviate over-translation, which in turn leads to better generated poems. 6 Conclusion In this paper, we proposed a novel task of gen- erating classical Chinese poems from vernacular paragraphs. We adapted the unsupervised machine translation model to our task and meanwhile pro- posed two novel approaches to address the under- translation and over-translation problems. Experi- ments show that our task can give users more con- trollability in generating poems.",
      "We adapted the unsupervised machine translation model to our task and meanwhile pro- posed two novel approaches to address the under- translation and over-translation problems. Experi- ments show that our task can give users more con- trollability in generating poems. In addition, our approaches are very effective to solve the prob- Padding schema Perplexity BLEU 2-2-3 74.21 6.34 2-3-2 83.12 5.49 3-2-2 85.66 5.75 Table 8: Perplexity and BLEU scores of different padding schemas. lems when the UMT model is directly used in this task. In the future, we plan to explore: (1) Apply- ing the UMT model in the tasks where the abstrac- tion levels of source and target languages are dif- ferent (e.g., unsupervised automatic summariza- tion); (2) Improving the quality of generated po- ems via better structure organization approaches.",
      "References Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate. In ICLR. Sameer Bansal, Herman Kamper, Adam Lopez, and Sharon Goldwater. 2017. Towards speech-to-text translation without speech recognition. In EACL. Pi-Chuan Chang, Michel Galley, and Christopher D. Manning. 2008. Optimizing chinese word seg- mentation for machine translation performance. In WMT@ACL. Kyunghyun Cho, Bart van Merrienboer, C\u00b8 aglar G\u00a8ulc\u00b8ehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representa- tions using rnn encoder-decoder for statistical ma- chine translation. In EMNLP. Yang Feng, Lin Ma, Wei Liu, and Jiebo Luo. 2019. Un- supervised image captioning. In The IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR).",
      "In EMNLP. Yang Feng, Lin Ma, Wei Liu, and Jiebo Luo. 2019. Un- supervised image captioning. In The IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR). Jing He, Ming Zhou, and Long Jiang. 2012. Generat- ing chinese classical poems with statistical machine translation models. In AAAI. Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9:1735\u2013 1780. Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc\u2019Aurelio Ranzato. 2018a. Unsupervised machine translation using monolingual corpora only. In ICLR. Guillaume Lample, Myle Ott, Alexis Conneau, Lu- dovic Denoyer, and Marc\u2019Aurelio Ranzato. 2018b. Phrase-based & neural unsupervised machine trans- lation. In EMNLP.",
      "In ICLR. Guillaume Lample, Myle Ott, Alexis Conneau, Lu- dovic Denoyer, and Marc\u2019Aurelio Ranzato. 2018b. Phrase-based & neural unsupervised machine trans- lation. In EMNLP. Haitao Mi, Baskaran Sankaran, Zhiguo Wang, and Abe Ittycheriah. 2016. Coverage embedding models for neural machine translation. In EMNLP. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2001. Bleu: a method for automatic eval- uation of machine translation. In ACL. Romain Paulus, Caiming Xiong, and Richard Socher. 2018. A deep reinforced model for abstractive sum- marization. In ICLR. Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross, and Vaibhava Goel. 2017. Self-critical sequence training for image captioning.",
      "In ICLR. Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross, and Vaibhava Goel. 2017. Self-critical sequence training for image captioning. The IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR), pages 1179\u20131195. Baskaran Sankaran, Haitao Mi, Yaser Al-Onaizan, and Abe Ittycheriah. 2016. Temporal attention model for neural machine translation. CoRR, abs/1608.02927. Zhaopeng Tu, Zhengdong Lu, Yang P. Liu, Xiaohua Liu, and Hang Li. 2016. Modeling coverage for neu- ral machine translation. In ACL. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NIPS.",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NIPS. Zhe Wang, Wei He, Hua Wu, Haiyang Wu, Wei Li, Haifeng Wang, and Enhong Chen. 2016. Chinese poetry generation with planning based neural net- work. In COLING. Wei-Hung Weng, Yu-An Chung, and Peter Szolovits. 2019. Unsupervised clinical language translation. In Proceedings of the 25th ACM SIGKDD Interna- tional Conference on Knowledge Discovery &#38; Data Mining, KDD \u201919, pages 3121\u20133131, New York, NY, USA. ACM.",
      "2019. Unsupervised clinical language translation. In Proceedings of the 25th ACM SIGKDD Interna- tional Conference on Knowledge Discovery &#38; Data Mining, KDD \u201919, pages 3121\u20133131, New York, NY, USA. ACM. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Gregory S. Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google\u2019s neural machine translation system: Bridging the gap between human and ma- chine translation. CoRR, abs/1609.08144.",
      "2016. Google\u2019s neural machine translation system: Bridging the gap between human and ma- chine translation. CoRR, abs/1609.08144. Linli Xu, Liang Jiang, Chuan Qin, Zhe Wang, and Dongfang Du. 2018. How images inspire poems: Generating classical chinese poetry from images with memory networks. In AAAI. Rui Yan. 2016. i, poet: Automatic poetry composition through recurrent neural networks with iterative pol- ishing schema. In IJCAI. Cheng Yang, Maosong Sun, Xiaoyuan Yi, and Wenhao Li. 2018. Stylistic chinese poetry generation via un- supervised style disentanglement. In EMNLP. Jiaying Ye. 1984. Poem Criticism with Jialin. Zhong hua shu ju, Beijing, China. Xiaoyuan Yi, Maosong Sun, Ruoyu Li, and Zonghan Yang. 2018. Chinese poetry generation with a work- ing memory model. In IJCAI.",
      "Zhong hua shu ju, Beijing, China. Xiaoyuan Yi, Maosong Sun, Ruoyu Li, and Zonghan Yang. 2018. Chinese poetry generation with a work- ing memory model. In IJCAI. Xingxing Zhang and Mirella Lapata. 2014. Chinese poetry generation with recurrent neural networks. In EMNLP. Zhirui Zhang, Shuo Ren, Shujie Liu, Jianyong Wang, Peng Chen, Mu Li, Ming Zhou, and Enhong Chen. 2018. Style transfer as unsupervised machine trans- lation. CoRR, abs/1808.07894."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1909.00279.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":9814,
  "avg_doclen":175.25,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1909.00279.pdf"
    }
  }
}