{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Shallow Syntax in Deep Water Swabha Swayamdipta\u2660\u2217 Matthew Peters\u2663 Brendan Roof\u2663 Chris Dyer\u2665 Noah A. Smith\u2666\u2663 \u2660Language Technologies Institute, Carnegie Mellon University \u2663Allen Institute for Arti\ufb01cial Intelligence \u2666Paul G. Allen School of Computer Science & Engineering, University of Washington \u2665Google DeepMind {swabhas,matthewp,brendanr}@allenai.org Abstract Shallow syntax provides an approximation of phrase-syntactic structure of sentences; it can be produced with high accuracy, and is compu- tationally cheap to obtain. We investigate the role of shallow syntax-aware representations for NLP tasks using two techniques. First, we enhance the ELMo architecture (Peters et al., 2018b) to allow pretraining on predicted shal- low syntactic parses, instead of just raw text, so that contextual embeddings make use of shallow syntactic context. Our second method involves shallow syntactic features obtained automatically on downstream task data. Nei- ther approach leads to a signi\ufb01cant gain on any of the four downstream tasks we consid- ered relative to ELMo-only baselines.",
            "Our second method involves shallow syntactic features obtained automatically on downstream task data. Nei- ther approach leads to a signi\ufb01cant gain on any of the four downstream tasks we consid- ered relative to ELMo-only baselines. Further analysis using black-box probes from Liu et al. (2019) con\ufb01rms that our shallow-syntax-aware contextual embeddings do not transfer to lin- guistic tasks any more easily than ELMo\u2019s em- beddings. We take these \ufb01ndings as evidence that ELMo-style pretraining discovers repre- sentations which make additional awareness of shallow syntax redundant. 1 Introduction The NLP community is revisiting the role of lin- guistic structure in applications with the advent of contextual word representations (CWRs) derived from pretraining language models on large cor- pora (Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018; Devlin et al., 2018).",
            "Re- cent work has shown that downstream task per- formance may bene\ufb01t from explicitly injecting a syntactic inductive bias into model architectures (Kuncoro et al., 2018), even when CWRs are also used (Strubell et al., 2018). However, high qual- ity linguistic structure annotation at a large scale remains expensive\u2014a trade-off needs to be made \u2217Work done during an internship at the Allen Institute for Arti\ufb01cial Intelligence.             The   man taught his   daughter  to     play golf . DT NN PRP$ NN TO VBD NN VB . NP NP NP VP VP S VP S NP NP NP VP VP Figure 1: A sentence with its phrase-syntactic tree (brown) and shallow syntactic (chunk) annotations (red). Nodes in the tree which percolate down as chunk labels are in red. Not all tokens in the sentence get chunk labels; e.g., punctuation is not part of a chunk. between the quality of the annotations and the computational expense of obtaining them.",
            "Nodes in the tree which percolate down as chunk labels are in red. Not all tokens in the sentence get chunk labels; e.g., punctuation is not part of a chunk. between the quality of the annotations and the computational expense of obtaining them. Shal- low syntactic structures (Abney, 1991; also called chunk sequences) offer a viable middle ground, by providing a \ufb02at, non-hierarchical approximation to phrase-syntactic trees (see Fig. 1 for an exam- ple). These structures can be obtained ef\ufb01ciently, and with high accuracy, using sequence labelers. In this paper we consider shallow syntax to be a proxy for linguistic structure. While shallow syntactic chunks are almost as ubiquitous as part-of-speech tags in standard NLP pipelines (Jurafsky and Martin, 2000), their rela- tive merits in the presence of CWRs remain un- clear. We investigate the role of these structures using two methods.",
            "While shallow syntactic chunks are almost as ubiquitous as part-of-speech tags in standard NLP pipelines (Jurafsky and Martin, 2000), their rela- tive merits in the presence of CWRs remain un- clear. We investigate the role of these structures using two methods. First, we enhance the ELMo architecture (Peters et al., 2018b) to allow pre- training on predicted shallow syntactic parses, in- stead of just raw text, so that contextual embed- dings make use of shallow syntactic context (\u00a72). Our second method involves classical addition of chunk features to CWR-infused architectures for four different downstream tasks (\u00a73). Shallow syntactic information is obtained automatically us- arXiv:1908.11047v1  [cs.CL]  29 Aug 2019",
            "ing a highly accurate model (97% F1 on stan- dard benchmarks). In both settings, we observe only modest gains on three of the four downstream tasks relative to ELMo-only baselines (\u00a74). Recent work has probed the knowledge encoded in CWRs and found they capture a surprisingly large amount of syntax (Blevins et al., 2018; Liu et al., 2019; Tenney et al., 2019). We further ex- amine the contextual embeddings obtained from the enhanced architecture and a shallow syntac- tic context, using black-box probes from Liu et al. (2019). Our analysis indicates that our shallow- syntax-aware contextual embeddings do not trans- fer to linguistic tasks any more easily than ELMo embeddings (\u00a74.2). Overall, our \ufb01ndings show that while shallow syntax can be somewhat useful, ELMo-style pre- training discovers representations which make ad- ditional awareness of shallow syntax largely re- dundant.",
            "Overall, our \ufb01ndings show that while shallow syntax can be somewhat useful, ELMo-style pre- training discovers representations which make ad- ditional awareness of shallow syntax largely re- dundant. 2 Pretraining with Shallow Syntactic Annotations We brie\ufb02y review the shallow syntactic struc- tures used in this work, and then present a model architecture to obtain embeddings from shallow Syntactic Context (mSynC). 2.1 Shallow Syntax Base phrase chunking is a cheap sequence- labeling\u2013based alternative to full syntactic pars- ing, where the sequence consists of non- overlapping labeled segments (Fig. 1 includes an example.) Full syntactic trees can be converted into such shallow syntactic chunk sequences us- ing a deterministic procedure (Jurafsky and Mar- tin, 2000). Tjong Kim Sang and Buchholz (2000) offered a rule-based transformation deriving non- overlapping chunks from phrase-structure trees as found in the Penn Treebank (Marcus et al., 1993). The procedure percolates some syntactic phrase nodes from a phrase-syntactic tree to the phrase in the leaves of the tree.",
            "The procedure percolates some syntactic phrase nodes from a phrase-syntactic tree to the phrase in the leaves of the tree. All overlapping embed- ded phrases are then removed, and the remainder of the phrase gets the percolated label\u2014this usu- ally corresponds to the head word of the phrase. In order to obtain shallow syntactic annotations on a large corpus, we train a BiLSTM-CRF model (Lample et al., 2016; Peters et al., 2017), which achieves 97% F1 on the CoNLL 2000 benchmark test set. The training data is obtained from the CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000), as well as the remaining sections (except \u00a723 and \u00a720) of the Penn Treebank, using the of\ufb01cial script for chunk generation.1 The stan- dard task de\ufb01nition from the shared task includes eleven chunk labels, as shown in Table 1.",
            "Label % Occurrence Noun Phrase (NP) 51.7 Verb Phrase (VP) 20.0 Prepositional Phrase (PP) 19.8 Adverbial Phrase (ADVP) 3.7 Subordinate Clause (SBAR) 2.1 Adjective Phrase (ADJP) 1.9 Verb Particles (PRT) 0.5 Conjunctive Phrase (CONJ) 0.06 Interjective Phrase (INTJ) 0.03 List Marker (LST) 0.01 Unlike Coordination Phrase (UCP) 0.002 Table 1: Shallow syntactic chunk phrase types from CoNLL 2000 shared task (Tjong Kim Sang and Buch- holz, 2000) and their occurrence % in the training data. 2.2 Pretraining Objective Traditional language models are estimated to max- imize the likelihood of each word xi given the words that precede it, p(xi | x<i). Given a corpus that is annotated with shallow syntax, we propose to condition on both the preceding words and their annotations.",
            "2.2 Pretraining Objective Traditional language models are estimated to max- imize the likelihood of each word xi given the words that precede it, p(xi | x<i). Given a corpus that is annotated with shallow syntax, we propose to condition on both the preceding words and their annotations. We associate with each word xi three additional variables (denoted ci): the indices of the beginning and end of the last completed chunk before xi, and its label. For example, in Fig. 2, c4 = \u27e83, 3, VP\u27e9 for x4 = the. Chunks, c are only used as condi- tioning context via p(xi | x<i, c\u2a7di); they are not predicted.2 Because the c labels depend on the entire sentence through the CRF chunker, condi- tioning each word\u2019s probability on any ci means that our model is, strictly speaking, not a language model, and it can no longer be meaningfully eval- uated using perplexity. A right-to-left model is constructed analo- gously, conditioning on c\u2a7ei alongside x>i.",
            "A right-to-left model is constructed analo- gously, conditioning on c\u2a7ei alongside x>i. Fol- 1https:\/\/www.clips.uantwerpen.be\/ conll2000\/chunking\/ 2A different objective could consider predicting the next chunks, along with the next word. However, this chunker would have access to strictly less information than usual, since the entire sentence would no longer be available.",
            "lowing Peters et al. (2018a), we use a joint objec- tive maximizing data likelihood objectives in both directions, with shared softmax parameters. 2.3 Pretraining Model Architecture Our model uses two encoders: eseq for encoding the sequential history (x<i), and esyn for shallow syntactic (chunk) history (c\u2a7di). For both, we use transformers (Vaswani et al., 2017), which consist of large feedforward networks equipped with mul- tiheaded self-attention mechanisms. h3 h2 h1 c1 c2 x3 x2 x1 Sequential Transformer Encoder eseq g1,2,NP g3,3,VP mSynC3 Shallow-Syntactic Transformer Encoder esyn uproj fproj fproj fproj fproj Not everyone  believes  the truth . NP NP \u2014 VP the  p believes,  believes  VP everyone,  Not,  Not everyone  NP ( ) Figure 2: Model architecture for pretraining with shal- low syntax. A sequential encoder converts the raw text into CWRs (shown in blue).",
            "NP NP \u2014 VP the  p believes,  believes  VP everyone,  Not,  Not everyone  NP ( ) Figure 2: Model architecture for pretraining with shal- low syntax. A sequential encoder converts the raw text into CWRs (shown in blue). Observed shallow syntac- tic structure (chunk boundaries and labels, shown in red) are combined with these CWRs in a shallow syn- tactic encoder to get contextualized representations for chunks (shown in orange). Both representations are passed through a projection layer to get mSynC em- beddings (details shown only in some positions, for clarity), used both for computing the data likelihood, as shown, as well as in downstream tasks. As inputs to eseq, we use a context-independent embedding, obtained from a CNN character en- coder (Kim et al., 2016) for each token xi. The outputs hi from eseq represent words in context. Next, we build representations for (observed) chunks in the sentence by concatenating a learned embedding for the chunk label with hs for the boundaries and applying a linear projection (fproj ).",
            "The outputs hi from eseq represent words in context. Next, we build representations for (observed) chunks in the sentence by concatenating a learned embedding for the chunk label with hs for the boundaries and applying a linear projection (fproj ). The output from fproj is input to esyn, the shallow syntactic encoder, and results in contex- tualized chunk representations, g. Note that the number of chunks in the sentence is less than or equal to the number of tokens. Each hi is now concatentated with gci, where gci corresponds to ci, the last chunk before posi- tion i. Finally, the output is given by mSynCi = uproj (hi, gci) = W \u22a4[hi; gci], where W is a model parameter. For training, mSynCi is used to compute the probability of the next word, using a sampled softmax (Bengio et al., 2003). For down- stream tasks, we use a learned linear weighting of all layers in the encoders to obtain a task-speci\ufb01c mSynC, following Peters et al. (2018a).",
            "For down- stream tasks, we use a learned linear weighting of all layers in the encoders to obtain a task-speci\ufb01c mSynC, following Peters et al. (2018a). Staged parameter updates Jointly training both the sequential encoder eseq, and the syn- tactic encoder esyn can be expensive, due to the large number of parameters involved. To reduce cost, we initialize our sequential CWRs h, using pretrained embeddings from ELMo-transformer. Once initialized as such, the encoder is \ufb01ne-tuned to the data likelihood objective (\u00a72.2). This results in a staged parameter update, which reduces train- ing duration by a factor of 10 in our experiments. We discuss the empirical effect of this approach in \u00a74.3. 3 Shallow Syntactic Features Our second approach incorporates shallow syn- tactic information in downstream tasks via token- level chunk label embeddings. Task training (and test) data is automatically chunked, and chunk boundary information is passed into the task model via BIOUL encoding of the labels.",
            "3 Shallow Syntactic Features Our second approach incorporates shallow syn- tactic information in downstream tasks via token- level chunk label embeddings. Task training (and test) data is automatically chunked, and chunk boundary information is passed into the task model via BIOUL encoding of the labels. We add randomly initialized chunk label embeddings to task-speci\ufb01c input encoders, which are then \ufb01ne- tuned for task-speci\ufb01c objectives. This approach does not require a shallow syntactic encoder or chunk annotations for pretraining CWRs, only a chunker. Hence, this can more directly measure the impact of shallow syntax for a given task.3 4 Experiments Our experiments evaluate the effect of shallow syntax, via contextualization (mSynC, \u00a72) and features (\u00a73). We provide comparisons with four baselines\u2014ELMo-transformer (Peters et al., 3In contrast, in \u00a72, the shallow-syntactic encoder itself, as well as predicted chunk quality on the large pretraining corpus could affect downstream performance.",
            "2018b), our reimplementation of the same, as well as two CWR-free baselines, with and without shal- low syntactic features. Both ELMo-transformer and mSynC are trained on the 1B word bench- mark corpus (Chelba et al., 2013); the latter also employs chunk annotations (\u00a72.1). Experimental settings are detailed in Appendix \u00a7A.1. 4.1 Downstream Task Transfer We employ four tasks to test the impact of shal- low syntax. The \ufb01rst three, namely, coarse and \ufb01ne-grained named entity recognition (NER), and constituency parsing, are span-based; the fourth is a sentence-level sentiment classi\ufb01cation task. Fol- lowing Peters et al. (2018a), we do not apply \ufb01ne- tuning to task-speci\ufb01c architectures, allowing us to do a controlled comparison with ELMo. Given an identical base architecture across models for each task, we can attribute any difference in per- formance to the incorporation of shallow syntax or contextualization.",
            "Given an identical base architecture across models for each task, we can attribute any difference in per- formance to the incorporation of shallow syntax or contextualization. Details of downstream ar- chitectures are provided below, and overall dataset statistics for all tasks is shown in the Appendix, Table 5. NER We use the English portion of the CoNLL 2003 dataset (Tjong Kim Sang and De Meul- der, 2003), which provides named entity annota- tions on newswire data across four different en- tity types (PER, LOC, ORG, MISC). A bidi- rectional LSTM-CRF architecture (Lample et al., 2016) and a BIOUL tagging scheme were used. Fine-grained NER The same architecture and tagging scheme from above is also used to predict \ufb01ne-grained entity annotations from OntoNotes 5.0 (Weischedel et al., 2011). There are 18 \ufb01ne- grained NER labels in the dataset, including regu- lar named entitities as well as entities such as date, time and common numerical entries.",
            "There are 18 \ufb01ne- grained NER labels in the dataset, including regu- lar named entitities as well as entities such as date, time and common numerical entries. Phrase-structure parsing We use the standard Penn Treebank splits, and adopt the span-based model from Stern et al. (2017). Following their approach, we used predicted part-of-speech tags from the Stanford tagger (Toutanova et al., 2003) for training and testing. About 51% of phrase- syntactic constituents align exactly with the pre- dicted chunks used, with a majority being single- width noun phrases. Given that the rule-based pro- cedure used to obtain chunks only propagates the phrase type to the head-word and removes all over- lapping phrases to the right, this is expected. We did not employ jack-kni\ufb01ng to obtain predicted chunks on PTB data; as a result there might be differences in the quality of shallow syntax anno- tations between the train and test portions of the data.",
            "We did not employ jack-kni\ufb01ng to obtain predicted chunks on PTB data; as a result there might be differences in the quality of shallow syntax anno- tations between the train and test portions of the data. Sentiment analysis We consider \ufb01ne- grained (5-class) classi\ufb01cation on Stan- ford Sentiment Treebank (Socher et al., 2013). The labels are negative, somewhat negative, neutral, positive and somewhat positive. Our model was based on the biattentive classi\ufb01cation network (McCann et al., 2017). We used all phrase lengths in the dataset for training, but test results are reported only on full sentences, following prior work. Results are shown in Table 2. Consistent with previous \ufb01ndings, CWRs offer large improvements across all tasks. Though helpful to span-level task models without CWRs, shallow syntactic fea- tures offer little to no bene\ufb01t to ELMo models. mSynC\u2019s performance is similar.",
            "Consistent with previous \ufb01ndings, CWRs offer large improvements across all tasks. Though helpful to span-level task models without CWRs, shallow syntactic fea- tures offer little to no bene\ufb01t to ELMo models. mSynC\u2019s performance is similar. This holds even for phrase-structure parsing, where (gold) chunks align with syntactic phrases, indicating that task- relevant signal learned from exposure to shallow syntax is already learned by ELMo. On sentiment classi\ufb01cation, chunk features are slightly harmful on average (but variance is high); mSynC again performs similarly to ELMo-transformer. Over- all, the performance differences across all tasks are small enough to infer that shallow syntax is not particularly helpful when using CWRs. 4.2 Linguistic Probes We further analyze whether awareness of shallow syntax carries over to other linguistic tasks, via probes from Liu et al. (2019). Probes are lin- ear models trained on frozen CWRs to make pre- dictions about linguistic (syntactic and semantic) properties of words and phrases.",
            "(2019). Probes are lin- ear models trained on frozen CWRs to make pre- dictions about linguistic (syntactic and semantic) properties of words and phrases. Unlike \u00a74.1, there is minimal downstream task architecture, bringing into focus the transferability of CWRs, as opposed to task-speci\ufb01c adaptation. 4.2.1 Probing Tasks The ten different probing tasks we used include CCG supertagging (Hockenmaier and Steedman, 2007), part-of-speech tagging from PTB (Mar- cus et al., 1993) and EWT (Universal Depeden- cies Silveira et al., 2014), named entity recogni- tion (Tjong Kim Sang and De Meulder, 2003),",
            "NER Fine-grained NER Constituency Parsing Sentiment Baseline (no CWR) 88.1 \u00b1 0.27 78.5 \u00b1 0.19 88.9 \u00b1 0.05 51.6 \u00b1 1.63 + shallow syn. features 88.6 \u00b1 0.22 78.9 \u00b1 0.13 90.8 \u00b1 0.14 51.1 \u00b1 1.39 ELMo-transformer (Peters et al., 2018b) 91.1 \u00b1 0.26 \u2014 93.7 \u00b1 0.00 \u2014 ELMo-transformer (our reimplementation) 91.5 \u00b1 0.25 85.7 \u00b1 0.08 94.1 \u00b1 0.06 53.0 \u00b1 0.72 + shallow syn. features 91.6 \u00b1 0.40 85.9 \u00b1 0.28 94.3 \u00b1 0.03 52.6 \u00b1 0.54 Shallow syn.",
            "features 91.6 \u00b1 0.40 85.9 \u00b1 0.28 94.3 \u00b1 0.03 52.6 \u00b1 0.54 Shallow syn. contextualization (mSynC) 91.5 \u00b1 0.19 85.9 \u00b1 0.20 94.1 \u00b1 0.07 53.0 \u00b1 1.07 Table 2: Test-set performance of ELMo-transformer (Peters et al., 2018b), our reimplementation, and mSynC, compared to baselines without CWR. Evaluation metric is F1 for all tasks except sentiment, which reports accu- racy. Reported results show the mean and standard deviation across 5 runs for coarse-grained NER and sentiment classi\ufb01cation and 3 runs for other tasks. CCG PTB POS EWT POS Chunk NER Sem. Tagging Gramm. Err. D Prep. Role Prep. Func. Event Fact.",
            "CCG PTB POS EWT POS Chunk NER Sem. Tagging Gramm. Err. D Prep. Role Prep. Func. Event Fact. ELMo-transformer 92.68 97.09 95.13 92.18 81.21 93.78 30.80 72.81 82.24 70.88 mSynC 92.03 96.91 94.64 96.89 79.98 93.03 30.86 70.83 82.67 70.39 Table 3: Test performance of ELMo-transformer (Peters et al., 2018b) vs. mSynC on several linguistic probes from Liu et al. (2019). In each case, performance of the best layer from the architecture is reported. Details on the probes can be found in \u00a74.2.1.",
            "mSynC on several linguistic probes from Liu et al. (2019). In each case, performance of the best layer from the architecture is reported. Details on the probes can be found in \u00a74.2.1. base-phrase chunking (Tjong Kim Sang and Buchholz, 2000), grammar error detection (Yan- nakoudakis et al., 2011), semantic tagging (Bjerva et al., 2016), preposition supersense identi\ufb01cation (Schneider et al., 2018), and event factuality de- tection (Rudinger et al., 2018). Metrics and ref- erences for each are summarized in Table 6. For more details, please see Liu et al. (2019). Results in Table 3 show ten probes. Again, we see the performance of baseline ELMo- transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks. As we would expect, on the probe for predicting chunk tags, mSynC achieves 96.9 F1 vs.",
            "Again, we see the performance of baseline ELMo- transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks. As we would expect, on the probe for predicting chunk tags, mSynC achieves 96.9 F1 vs. 92.2 F1 for ELMo-transformer, indicating that mSynC is in- deed encoding shallow syntax. Overall, the results further con\ufb01rm that explicit shallow syntax does not offer any bene\ufb01ts over ELMo-transformer. 4.3 Effect of Training Scheme We test whether our staged parameter train- ing (\u00a72.3) is a viable alternative to an end-to-end training of both esyn and eseq. We make a further distinction between \ufb01ne-tuning eseq vs. not updat- ing it at all after initialization (frozen). Down- stream validation-set F1 on \ufb01ne-grained NER, re- ported in Table 4, shows that the end-to-end strat- egy lags behind the others, perhaps indicating the need to train longer than 10 epochs.",
            "Down- stream validation-set F1 on \ufb01ne-grained NER, re- ported in Table 4, shows that the end-to-end strat- egy lags behind the others, perhaps indicating the need to train longer than 10 epochs. However, a single epoch on the 1B-word benchmark takes 36 hours on 2 Tesla V100s, making this prohibitive. Interestingly, the frozen strategy, which takes the Model Fine-grained NER F1 end-to-end ELMo 86.90 \u00b1 0.11 mSynC end-to-end 86.89 \u00b1 0.04 staged mSynC frozen 87.36 \u00b1 0.02 mSynC \ufb01ne-tuned 87.44 \u00b1 0.07 Table 4: Validation F1 for \ufb01ne-grained NER across syntactic pretraining schemes, with mean and standard deviations across 3 runs. least amount of time to converge (24 hours on 1 Tesla V100), also performs almost as well as \ufb01ne- tuning.",
            "least amount of time to converge (24 hours on 1 Tesla V100), also performs almost as well as \ufb01ne- tuning. 5 Conclusion We \ufb01nd that exposing CWR-based models to shal- low syntax, either through new CWR learning ar- chitectures or explicit pipelined features, has little effect on their performance, across several tasks. Linguistic probing also shows that CWRs aware of such structures do not improve task transferability. Our architecture and methods are general enough to be adapted for richer inductive biases, such as those given by full syntactic trees (RNNGs; Dyer et al., 2016), or to different pretraining objectives, such as masked language modeling (BERT; Devlin et al., 2018); we leave this pursuit to future work.",
            "References Steven P Abney. 1991. Parsing by chunks. In Principle-based parsing, pages 257\u2013278. Springer. Yoshua Bengio, R\u00b4ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic lan- guage model. Journal of Machine Learning Re- search, pages 1137\u20131155. Johannes Bjerva, Barbara Plank, and Johan Bos. 2016. Semantic tagging with deep residual networks. In Proc. of COLING. Terra Blevins, Omer Levy, and Luke Zettlemoyer. 2018. Deep rnns encode soft hierarchical syntax. In Proc. of ACL. Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robin- son. 2013. One billion word benchmark for mea- suring progress in statistical language modeling. ArXiv:1312.3005. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.",
            "2013. One billion word benchmark for mea- suring progress in statistical language modeling. ArXiv:1312.3005. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: pre-training of deep bidirectional transformers for language under- standing. ArXiv:1810.04805. Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A Smith. 2016. Recurrent neural network grammars. In Proc. of NAACL-HLT. Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Peters, Michael Schmitz, and Luke S. Zettlemoyer. 2017. AllenNLP: A deep semantic natural language processing platform. ArXiv:1803.07640. Julia Hockenmaier and Mark Steedman. 2007. CCG- bank: A corpus of ccg derivations and dependency structures extracted from the Penn Treebank.",
            "ArXiv:1803.07640. Julia Hockenmaier and Mark Steedman. 2007. CCG- bank: A corpus of ccg derivations and dependency structures extracted from the Penn Treebank. Com- putational Linguistics, 33(3). Jeremy Howard and Sebastian Ruder. 2018. Universal language model \ufb01ne-tuning for text classi\ufb01cation. In Proc. of ACL. Daniel Jurafsky and James H. Martin. 2000. Speech and Language Processing: An Introduction to Nat- ural Language Processing, Computational Linguis- tics, and Speech Recognition, 1st edition. Prentice Hall PTR, Upper Saddle River, NJ, USA. Yoon Kim, Yacine Jernite, David Sontag, and Alexan- der M. Rush. 2016. Character-aware neural lan- guage models. In Proceedings of the Thirtieth AAAI Conference on Arti\ufb01cial Intelligence, AAAI\u201916, pages 2741\u20132749. AAAI Press.",
            "2016. Character-aware neural lan- guage models. In Proceedings of the Thirtieth AAAI Conference on Arti\ufb01cial Intelligence, AAAI\u201916, pages 2741\u20132749. AAAI Press. Adhiguna Kuncoro, Chris Dyer, John Hale, Dani Yo- gatama, Stephen Clark, and Phil Blunsom. 2018. Lstms can learn syntax-sensitive dependencies well, but modeling structure makes them better. In Proc. of ACL. Guillaume Lample, Miguel Ballesteros, Sandeep Sub- ramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In Proc. of NAACL-HLT. Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew Peters, and Noah A. Smith. 2019. Lin- guistic knowledge and transferability of contextual representations. In Proc. of NAACL-HLT. Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993.",
            "2019. Lin- guistic knowledge and transferability of contextual representations. In Proc. of NAACL-HLT. Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn treebank. Computa- tional Linguistics, 19(2):313\u2013330. Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Con- textualized word vectors. In Proc. of NeurIPS, pages 6294\u20136305. Matthew Peters, Waleed Ammar, Chandra Bhagavat- ula, and Russell Power. 2017. Semi-supervised se- quence tagging with bidirectional language models. In Proc. of ACL. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word rep- resentations. In Proc. of NAACL-HLT.",
            "of ACL. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word rep- resentations. In Proc. of NAACL-HLT. Matthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. 2018b. Dissecting contextual word embeddings: Architecture and representation. In Proc. of EMNLP, pages 1499\u20131509. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing by generative pre-training. Rachel Rudinger, Aaron Steven White, and Benjamin Van Durme. 2018. Neural models of factuality. In Proc. of ACL. Nathan Schneider, Jena D Hwang, Vivek Srikumar, Jakob Prange, Austin Blodgett, Sarah R Moeller, Aviram Stern, Adi Bitan, and Omri Abend. 2018.",
            "In Proc. of ACL. Nathan Schneider, Jena D Hwang, Vivek Srikumar, Jakob Prange, Austin Blodgett, Sarah R Moeller, Aviram Stern, Adi Bitan, and Omri Abend. 2018. Comprehensive supersense disambiguation of En- glish prepositions and possessives. Natalia Silveira, Timothy Dozat, Marie-Catherine De Marneffe, Samuel R Bowman, Miriam Connor, John Bauer, and Christopher D Manning. 2014. A gold standard dependency corpus for English. In Proc. of LREC, pages 2897\u20132904. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment tree- bank. In Proc. of EMNLP. Mitchell Stern, Jacob Andreas, and Dan Klein. 2017. A minimal span-based neural constituency parser. In Proc. of ACL.",
            "Emma Strubell, Patrick Verga, Daniel Andor, David Weiss, and Andrew McCallum. 2018. Linguistically-informed self-attention for semantic role labeling. In Proc. of EMNLP. Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van Durme, Sam Bowman, Dipanjan Das, and Ellie Pavlick. 2019. What do you learn from context? Probing for sentence structure in contextu- alized word representations. In Proc. of ICLR. Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the CoNLL-2000 shared task: Chunking. In Proc. of CoNLL. Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proc. of NAACL. Association for Computational Linguistics. Kristina Toutanova, Dan Klein, Christopher D. Man- ning, and Yoram Singer.",
            "2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proc. of NAACL. Association for Computational Linguistics. Kristina Toutanova, Dan Klein, Christopher D. Man- ning, and Yoram Singer. 2003. Feature-rich part-of- speech tagging with a cyclic dependency network. In Proc. of NAACL. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proc. of NeurIPS, pages 5998\u20136008. Ralph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, et al. 2013. OntoNotes release 5.0 ldc2013t19. Linguistic Data Consortium, Philadel- phia, PA.",
            "2013. OntoNotes release 5.0 ldc2013t19. Linguistic Data Consortium, Philadel- phia, PA. Ralph Weischedel, Sameer Pradhan, Lance Ramshaw, Martha Palmer, Nianwen Xue, Mitchell Marcus, Ann Taylor, Craig Greenberg, Eduard Hovy, Robert Belvin, et al. 2011. OntoNotes Release 4.0. LDC2011T03, Philadelphia, Penn.: Linguistic Data Consortium. Helen Yannakoudakis, Ted Briscoe, and Ben Medlock. 2011. A new dataset and method for automatically grading esol texts. In Proc. of ACL. A Supplemental Material A.1 Hyperparameters ELMo-transformer Our baseline pretraining model was a reimplementation of that given in Pe- ters et al. (2018b). Hyperparameters were gener- ally identical, but we trained on only 2 GPUs with (up to) 4,000 tokens per batch.",
            "(2018b). Hyperparameters were gener- ally identical, but we trained on only 2 GPUs with (up to) 4,000 tokens per batch. This difference in batch size meant we used 6,000 warm up steps with the learning rate schedule of Vaswani et al. (2017). mSynC The function fseq is identical to the 6- layer biLM used in ELMo-transformer. fsyn, on the other hand, uses only 2 layers. The learned embeddings for the chunk labels have 128 dimen- sions and are concatenated with the two boundary h of dimension 512. Thus fproj maps 1024 + 128 dimensions to 512. Further, we did not perform weight averaging over several checkpoints. Shallow Syntax The size of the shallow syntac- tic feature embedding was 50 across all experi- ments, initialized uniform randomly. All model implementations are based on the AllenNLP library (Gardner et al., 2017).",
            "Task Train Heldout Test CoNLL 2003 NER (Tjong Kim Sang and De Meulder, 2003) 23,499 5,942 5,648 OntoNotes NER (Weischedel et al., 2013) 81,828 11,066 11,257 Penn TreeBank (Marcus et al., 1993) 39,832 1,700 2,416 Stanford Sentiment Treebank (Socher et al., 2013) 8,544 1,101 2,210 Table 5: Downstream dataset statistics describing the number of train, heldout and test set instances for each task.",
            "Task Dataset Metric CCG Supertagging CCGBank (Hockenmaier and Steedman, 2007) Accuracy PTB part-of-speech tagging PennTreeBank (Marcus et al., 1993) Accuracy EWT part-of-speech tagging Universal Dependencies (Silveira et al., 2014) Accuracy Chunking CoNLL 2000 (Tjong Kim Sang and Buchholz, 2000) F1 Named Entity Recognition CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003) F1 Semantic Tagging (Bjerva et al., 2016) Accuracy Grammar Error Detection First Certi\ufb01cate in English (Yannakoudakis et al., 2011) F1 Preposition Supersense Role STREUSLE 4.0 (Schneider et al., 2018) Accuracy Preposition Supersense Function STREUSLE 4.0 (Schneider et al., 2018) Accuracy Event Factuality Detection UDS It Happened v2 (Rudinger et al., 2018) Pearson R Table 6: Dataset and metrics for each probing task from Liu et al.",
            "(2019), corresponding to Table 3."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1908.11047.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 7450.999923706055,
    "avg_doclen_est": 173.27906799316406
}
