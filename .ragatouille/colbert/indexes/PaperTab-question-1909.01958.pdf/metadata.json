{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "From \u2018F\u2019 to \u2018A\u2019 on the N.Y. Regents Science Exams: An Overview of the Aristo Project Peter Clark, Oren Etzioni, Daniel Khashabi, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, Niket Tandon, Sumithra Bhakthavatsalam, Dirk Groeneveld, Michal Guerquin, Michael Schmitz Allen Institute for Arti\ufb01cial Intelligence, Seattle, WA, U.S.A. Abstract AI has achieved remarkable mastery over games such as Chess, Go, and Poker, and even Jeopardy!, but the rich variety of standardized exams has remained a landmark challenge. Even as recently as 2016, the best AI system could achieve merely 59.3% on an 8th Grade science exam (Schoenick et al., 2016). This article reports success on the Grade 8 New York Re- gents Science Exam, where for the \ufb01rst time a system scores more than 90% on the exam\u2019s non-diagram, multiple choice (NDMC) questions.",
      "This article reports success on the Grade 8 New York Re- gents Science Exam, where for the \ufb01rst time a system scores more than 90% on the exam\u2019s non-diagram, multiple choice (NDMC) questions. In addition, our Aristo system, building upon the success of recent language models, exceeded 83% on the corresponding Grade 12 Science Exam NDMC ques- tions. The results, on unseen test questions, are robust across different test years and different variations of this kind of test. They demonstrate that modern Natural Language Processing (NLP) methods can result in mastery on this task. While not a full solution to general question-answering (the questions are limited to 8th Grade multiple-choice science) it represents a signi\ufb01cant milestone for the \ufb01eld. In 2014, Project Aristo was launched with the goal of re- liably answering grade-school science questions, a stepping stone in the quest for systems that understood and could rea- son about science. The Aristo goal was highly ambitious, with the initial system scoring well below 50% even on 4th Grade multiple choice tests.",
      "The Aristo goal was highly ambitious, with the initial system scoring well below 50% even on 4th Grade multiple choice tests. With a glance at the questions, it is easy to see why: the questions are hard. For example, consider the following 8th Grade question: How are the particles in a block of iron affected when the block is melted? (A) The particles gain mass. (B) The particles contain less energy. (C) The particles move more rapidly. [correct] (D) The particles increase in volume. This question is challenging as it requires both scienti\ufb01c knowledge (particles move faster at higher temperatures) and common sense knowledge (melting involves raising temperature), and the ability to combine this information to- gether appropriately. Now, six years later, we are able to report that Aristo re- cently surpassed 90% on multiple choice questions from the Grade 8 New York Regents Science Exam, a major mile- stone and a re\ufb02ection on the tremendous progress of the NLP Figure 1: Progress over time of Aristo\u2019s scores on Regents 8th Grade Science (non-diagram, multiple choice questions, held-out test set).",
      "community as a whole. In this article, we review why this is signi\ufb01cant, how Aristo was able to achieve this score, and where the system still makes mistakes. We also explore what kinds of reasoning Aristo appears to be capable of doing, and what work still needs to be done to achieve the broader goals of the project. Why is this an important achievement? First, passing standardized tests has been a challenging AI benchmark for many years (Bringsjord and Schimanski, 2003; Brachman et al., 2005; Strickland, 2013). A good benchmark should test a variety of capabilities while also being clearly mea- surable, understandable, accessible, non-gameable, and suf- \ufb01ciently motivating. Standardized tests, while not a full test of machine intelligence, meet many of these practical re- quirements (Clark and Etzioni, 2016). They also appear to require several capabilities associated with intelligence, in- cluding language understanding, reasoning, and common- sense - although the extent to which such skills are needed is controversial (Davis, 2014).",
      "They also appear to require several capabilities associated with intelligence, in- cluding language understanding, reasoning, and common- sense - although the extent to which such skills are needed is controversial (Davis, 2014). We explore this in more de- tail this article. Second, although NLP has made dramatic advances in re- cent years with the advent of large-scale language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., arXiv:1909.01958v3  [cs.CL]  2 Feb 2021",
      "2018), and RoBERTa (Liu et al., 2019), many of the demon- strated successes have been on internal yardsticks gener- ated by the AI/NLP community itself, such as SQuAD (Ra- jpurkar et al., 2016), GLUE (Wang et al., 2019), and Trivi- aQA (Joshi et al., 2017). In contrast, the 8th Grade science exams are an external, independently-generated benchmark where we can compare machine performance with human performance. Aristo thus serves as a \u201cposter child\u201d for the remarkable and rapid advances achieved in NLP, applied to an easily accessible task. Finally, Aristo makes steps towards the AI Grand Chal- lenge of a system that can read a textbook chapter and an- swer the questions at the end of the chapter. This broader challenge dates back to the 1970s, and was reinvigorated in Raj Reddy\u2019s 1988 AAAI Presidential Address and subse- quent writing (Reddy, 1988, 2003). However, progress on this challenge has a checkered history.",
      "This broader challenge dates back to the 1970s, and was reinvigorated in Raj Reddy\u2019s 1988 AAAI Presidential Address and subse- quent writing (Reddy, 1988, 2003). However, progress on this challenge has a checkered history. Early attempts side- stepped the natural language understanding (NLU) task, in the belief that the main challenge lay in problem solving, e.g., (Larkin et al., 1980). In recent years there has been substantial progress in systems that can \ufb01nd factual answers in text, starting with IBM\u2019s Watson system (Ferrucci et al., 2010), and now with high-performing neural systems that can answer short questions provided they are given a text that contains the answer (e.g., Seo et al., 2016; Wang, Yan, and Wu, 2018). Aristo continues along this trajectory, but aims to also answer questions where the answer may not be written down explicitly. While not a full solution to the textbook grand challenge, Aristo is a further step along this path. At the same time, care is needed in interpreting Aristo\u2019s results.",
      "Aristo continues along this trajectory, but aims to also answer questions where the answer may not be written down explicitly. While not a full solution to the textbook grand challenge, Aristo is a further step along this path. At the same time, care is needed in interpreting Aristo\u2019s results. In particular, we make no claims that Aristo is an- swering questions in the way a person would (and is likely using different methods). Exams are designed with human reasoning in mind, to test certain human knowledge and rea- soning skills. But if the computer is answering questions in a different way, to what extent does it possess such skills? (Davis, 2014). To explore this, we examine the causes of some of Aristo\u2019s failures, and test whether Aristo has some of the semantic skills that appear necessary for good perfor- mance. We \ufb01nd evidence of several types of such systematic behavior, albeit not perfect, suggesting some form of rea- soning is occurring. Although still quite distant from human problem-solving, these emergent semantic skills are likely a key contributor to Aristo\u2019s scores reaching the 90% range.",
      "We \ufb01nd evidence of several types of such systematic behavior, albeit not perfect, suggesting some form of rea- soning is occurring. Although still quite distant from human problem-solving, these emergent semantic skills are likely a key contributor to Aristo\u2019s scores reaching the 90% range. As a brief history, the metric progress of the Aristo sys- tem on the Regents 8th Grade exams (non-diagram, multiple choice part, for a hidden, held-out test set) is shown in Fig- ure 1. The \ufb01gure shows the variety of techniques attempted, and mirrors the rapidly changing trajectory of the Natural Language Processing (NLP) \ufb01eld in general. Early work was dominated by information retrieval, statistical, and au- tomated rule extraction and reasoning methods (Clark et al., 2014, 2016; Khashabi et al., 2016; Khot, Sabharwal, and Clark, 2017; Khashabi et al., 2018).",
      "Later work has har- nessed state-of-the-art tools for large-scale language mod- eling and deep learning (Trivedi et al., 2019; Tandon et al., 2018), which have come to dominate the performance of the overall system and re\ufb02ects the stunning progress of the \ufb01eld Figure 2: A simpli\ufb01ed picture of Aristo\u2019s architecture. Aristo\u2019s eight solvers can be loosely grouped into statistical, reasoning, and language model-based approaches. of NLP as a whole. Finally, it is particularly \ufb01tting to report this result in the AI Magazine, as it is another step in the decades-long quest to ful\ufb01l the late Paul Allen\u2019s dream of a \u201cDigital Aris- totle\u201d, an \u201ceasy-to-use, all-encompassing knowledge store- house...to advance the \ufb01eld of AI.\u201d (Allen, 2012), a dream also set out in the AI Magazine in the Winter 2004 edition (Friedland et al., 2004).",
      "Aristo\u2019s success re\ufb02ects how much progress the \ufb01eld of NLP and AI as a whole has made in the intervening years. The Aristo System Aristo comprises of eight solvers, each of which attempts to independently answer a multiple choice question. Its suite of solvers has changed over the years, with new solvers be- ing added and redundant solvers being dropped to maintain a simple architecture. (Earlier solvers include use of Markov Logic Networks (Khot et al., 2015), reasoning over tables (Khashabi et al., 2016), and other neural approaches, now superceded by the language models.) As illustrated in Fig- ure 2, they can be loosely grouped into: 1. Statistical and information retrieval methods 2. Reasoning methods 3. Large-scale language model methods We now brie\ufb02y describe these solvers, with pointers to fur- ther information. Over the life of the project, the relative importance of the methods has shifted towards large-scale language methods, which now dominate the overall perfor- mance of the system.",
      "Large-scale language model methods We now brie\ufb02y describe these solvers, with pointers to fur- ther information. Over the life of the project, the relative importance of the methods has shifted towards large-scale language methods, which now dominate the overall perfor- mance of the system. Information Retrieval and Statistics The IR solver searches to see if the question along with an answer option is explicitly stated in the corpus. To do this, for each answer option ai, it sends q + ai as a query to a search engine, and returns the search engine\u2019s score for the top retrieved sentence s. This is repeated for all options ai to score them all, and the option with the highest score selected (Clark et al., 2016). The PMI solver uses pointwise mutual information (Church and Hanks, 1989) to measure the associations be- tween parts of q and parts of ai. PMI for two n-grams x 2",
      "Figure 3: The TupleInference Solver retrieves tuples rele- vant to the question, and constructs a support graph for each answer option (Here, the support graph for the choice (A) is shown). and y is de\ufb01ned as PMI(x, y) = log p(x,y) p(x)p(y). The larger the PMI, the stronger the association between x and y. The solver extracts unigrams, bigrams, trigrams, and skip- bigrams, and outputs the answer with the largest average PMI, calculated over all pairs of question and answer option n-grams (Clark et al., 2016). Finally, ACME (Abstract-Concrete Mapping Engine) searches for a cohesive link between a question q and can- didate answer ai using a large knowledge base of vector spaces that relate words in language to a set of 5000 sci- enti\ufb01c terms enumerated in a term bank. The key insight in ACME is that we can better assess lexical cohesion of a question and answer by pivoting through scienti\ufb01c termi- nology, rather than by simple co-occurence frequencies of question and answer words (Turney, 2017).",
      "The key insight in ACME is that we can better assess lexical cohesion of a question and answer by pivoting through scienti\ufb01c termi- nology, rather than by simple co-occurence frequencies of question and answer words (Turney, 2017). Reasoning Methods The TupleInference solver uses semi-structured knowledge in the form of tuples, extracted via Open Information Extrac- tion (Open IE) (Banko et al., 2007). TupleInference treats the reasoning task as searching for a graph that best con- nects the terms in the question with an answer choice via the knowledge; see Figure 3 for a simple illustrative example. To \ufb01nd the best support graph for each answer option, we de\ufb01ne the task as an optimization problem, and use Integer Linear Programming (ILP) to solve it. The answer choice with the highest scoring graph is then selected (Khot, Sab- harwal, and Clark, 2017). Multee (Trivedi et al., 2019) is a solver that repurposes existing textual entailment tools for question answering.",
      "The answer choice with the highest scoring graph is then selected (Khot, Sab- harwal, and Clark, 2017). Multee (Trivedi et al., 2019) is a solver that repurposes existing textual entailment tools for question answering. Textual entailment (TE) is the task of assessing if one text implies another, and there are several high-performing TE systems now available. Multee learns to combine their de- cisions, so it can determine how strongly a set of retrieved texts entails a particular question + answer option (Trivedi et al., 2019). The QR (qualitative reasoning) solver is designed to answer questions about qualitative in\ufb02uence, i.e., how more/less of one quantity affects another.",
      "The QR (qualitative reasoning) solver is designed to answer questions about qualitative in\ufb02uence, i.e., how more/less of one quantity affects another. Unlike the other solvers in Aristo, it is a specialist solver that only \ufb01res for a Partition Dataset Train Dev Test Total Regents 4th 127 20 109 256 Regents 8th 125 25 119 269 Regents 12th 665 282 632 1579 ARC-Easy 2251 570 2376 5197 ARC-Challenge 1119 299 1172 2590 Totals\u2020 4035 1151 4180 9366 \u2020ARC (Easy+Challenge) includes Regents 4th & 8th as a subset Table 1: Dataset partition sizes (number of questions). small subset of questions that ask about qualitative change. The solver uses a knowledge base of 50,000 (textual) state- ments about qualitative in\ufb02uence, e.g., \u201cA sunscreen with a higher SPF protects the skin longer.\u201d.",
      "small subset of questions that ask about qualitative change. The solver uses a knowledge base of 50,000 (textual) state- ments about qualitative in\ufb02uence, e.g., \u201cA sunscreen with a higher SPF protects the skin longer.\u201d. It has then been trained to reason using the BERT language model (Devlin et al., 2018), using a similar approach to that described below (Tafjord et al., 2019). Large-Scale Language Models The \ufb01eld of NLP has advanced substantially with the advent of large-scale language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2018), and RoBERTa (Liu et al., 2019). The AristoBERT solver applies BERT to multiple choice questions by treating the task as classi\ufb01cation: Given a question q with answer options ai and optional background knowledge Ki, we provide it to BERT as: [CLS] Ki [SEP] q [SEP] ai [SEP] for each option ai.",
      "The [CLS] output token is projected to a single logit and fed through a softmax layer across an- swer options, trained using cross entropy loss, and the high- est scoring option selected. AristoBERT uses three methods to apply BERT more ef- fectively. First, we retrieve and supply background knowl- edge Ki along with the question when using BERT, as de- scribed above. This provides the potential for BERT to \u201cread\u201d that background knowledge and apply it to the ques- tion, although the exact nature of how it uses background knowledge is more complex and less interpretable. Second, following (Sun et al., 2019), we \ufb01ne-tune BERT using a cur- riculum of several datasets, starting with RACE (a general reading comprehension dataset that is not science related (Lai et al., 2017)), followed by a collection of science train- ing sets: OpenbookQA (Mihaylov et al., 2018), ARC (Clark et al., 2018), and Regents questions (training partition).",
      "Fi- nally, we repeat this for three variants of BERT (cased, un- cased, cased whole-word), and ensemble the predictions. Finally, the AristoRoBERTa solver does the same with RoBERTa (Liu et al., 2019), a high-performing and opti- mized derivative of BERT trained on signi\ufb01cantly more text. In AristoRoBERTa, we simply replace the BERT model in AristoBERT with RoBERTa, repeating similar \ufb01ne-tuning steps. We ensemble two versions together, namely with and without the \ufb01rst \ufb01ne-tuning step using RACE. Experiments and Results We apply Aristo to the non-diagram, multiple choice (NDMC) questions in the science exams. Although ques- 3",
      "Figure 4: The results of each of the Aristo solvers, as well as the overall Aristo system, on each of the test sets. Most notably, Aristo achieves 91.6% accuracy in 8th Grade, and exceeds 83% in 12th Grade. Note that Aristo is a single system, run unchanged on each dataset (not re-tuned for each dataset). tions with diagrams are common,1 they are outside of our focus on language and reasoning. (For illustrative work on science diagrams, see (Krishnamurthy, Tafjord, and Kemb- havi, 2016)). We also omit questions that require a direct answer for two reasons. First, after removing questions with diagrams, they are rare: For example, of the 482 direct an- swer questions over 13 years of Regents 8th Grade Science exams, only 38 (<8%) do not involve a diagram. Second, they are complex, often requiring explanation and synthesis. Both diagram and direct-answer questions are natural topics for future work.",
      "Second, they are complex, often requiring explanation and synthesis. Both diagram and direct-answer questions are natural topics for future work. We evaluate Aristo using the New York Regents Science exam questions2, and also the ARC dataset, a larger cor- pus of science questions drawn from public resources across the country (Clark et al., 2018). The Regents exams are only produced for 4th, 8th, and 12th Grade students (corre- sponding to the end of elementary, middle, and high-school respectively), while the ARC questions span Grades 3 to 9. All questions are posed exactly as written, with no re- wording or rephrasing. The entire dataset is partitioned into train/dev/test parts (Table 1), and for the Regents questions we ensure that each exam is either completely in train, dev, or test but not split between them. (The non-Regents ARC questions do not have exam groupings.) All but 39 of the 9366 questions are 4-way multiple choice, the remaining 39 (<0.5%) being 3- or 5-way. A random score over the entire dataset is 25.02%.",
      "All but 39 of the 9366 questions are 4-way multiple choice, the remaining 39 (<0.5%) being 3- or 5-way. A random score over the entire dataset is 25.02%. Results The results are summarized in Figure 4, showing the per- formance of the solvers individually, and their combination in the full Aristo system. Note that Aristo is a single system run on the \ufb01ve datasets (not retuned for each dataset in turn). Most notably, Aristo\u2019s scores on the Regents Exams far exceed earlier performances (e.g., Schoenick et al., 2016; Clark et al., 2016), and represent a new high-point on sci- ence questions. 1Ratios of non-diagram multiple choice (NDMC), with- diagram multiple-choice (DMC), non-diagram direct answer (NDDA), and with-diagram direct answer (DDA) questions are ap- proximately 45/25/5/25 for Regents 4th Grade, 25/25/5/45 for 8th Grade, and 40/25/15/20 for 12th grade.",
      "2See https://www.nysedregents.org/ for the original exams. In addition, the results show the dramatic impact of new language modeling technology, embodied in AristoBERT and AristoRoBERTa, the scores for these two solvers dom- inating the performance of the overall system. Even on the ARC-Challenge questions, containing a wide variety of dif- \ufb01cult questions, the language modeling based solvers domi- nate. The general increasing trend of solver scores from left to right for each test set loosely re\ufb02ects the progression of the NLP \ufb01eld over the six years of the project. To further check that we have not over\ufb01t to our data, we also ran Aristo on the most recent years of the Regents Exams (4th and 8th Grade), years 2017-19, that were un- available at the start of the project and were not part of our datasets. We \ufb01nd similar scores (average 92.8% for the three Fourth Grade exams, 93.3% for the Eighth Grade), suggest- ing the system is not over\ufb01t.",
      "We \ufb01nd similar scores (average 92.8% for the three Fourth Grade exams, 93.3% for the Eighth Grade), suggest- ing the system is not over\ufb01t. On a combination of exam scores and lab work (weighted approximately 60:40), the NY State Education Department considers an overall score of 65% as \u201cMeeting the Stan- dards\u201d, and over 85% as \u201cMeeting the Standards with Dis- tinction\u201d3. As a somewhat loose comparison, if this rubric applies equally to the NDMC subset we have studied, this would mean Aristo has met the standard with distinction in 8th Grade NDMC Science (although clearly full science re- quires substantially more). Answer Only Performance Several authors have observed that for some multiple choice datasets, systems can still perform well even when ignoring the question body and looking only at the answer options (Gururangan et al., 2018; Poliak et al., 2018b). This sur- prising result is particularly true for crowdsourced datasets, where workers may use stock words or phrases (e.g., \u201cnot\u201d) in incorrect answer options that gives them away.",
      "This sur- prising result is particularly true for crowdsourced datasets, where workers may use stock words or phrases (e.g., \u201cnot\u201d) in incorrect answer options that gives them away. To mea- sure this phenomenon on our datasets, we trained and tested a new AristoRoBERTa model giving it only the answer op- tions, i.e., no question body nor retrieved knowledge. (With- out retraining the model scores slightly less, 35% overall vs. 38% with retraining). The results (test set) are shown in Figure 5, indicating that it is hard to select the right answer without reading the question. (Scores are slightly higher for 3https://www.nysedregents.org/grade8/science/618/home.html 4",
      "Figure 5: Scores when looking at the answer options only, compared with using the full questions. The (desirably) low scores/large drops indicate it is hard to guess the answer without reading the question. Figure 6: Aristo\u2019s scores drop a small amount (average 10%) when tested on adversarially generated 8-way MC questions. 12th Grade answer-only, possibly because the average an- swer length is longer, hence more potential for hidden pat- terns inside that hint at correctness/incorrectness). Adversarial Answer Options What if we add extra incorrect answer options to the ques- tions? If a system has mastery of the material, we would expect its score to be relatively unaffected by such modi\ufb01- cations. We can make this more challenging by doing this adversarially: try many different incorrect options until the system is fooled. If we do this, turning a 4-way MC question into 8-way with options chosen to fool Aristo, then retrain on this new dataset, we do observe an effect: the scores drop, although the drop is small (\u224810%), see Figure 6.",
      "If we do this, turning a 4-way MC question into 8-way with options chosen to fool Aristo, then retrain on this new dataset, we do observe an effect: the scores drop, although the drop is small (\u224810%), see Figure 6. This indi- cates that while Aristo performs well, it still has some blind spots that can be arti\ufb01cially uncovered through adversarial methods such as this. Analysis Despite the high scores, Aristo still makes occasional mis- takes. Because Aristo retrieves and then \u201creads\u201d corpus sen- tences to answer a question, we can inspect the retrieved knowledge when Aristo fails, and gain some insight as to where and why it makes errors. Did Aristo retrieve the \u201cright\u201d knowledge, but then choose the wrong answer? Or was the failure due (in part) to the retrieval step itself? We manually analyzed 30 random failures (of 248) in the en- tire dev set (Regents + ARC, 1151 dev set questions to- Figure 7: Case Study: Causes of error for 30 questions that Aristo answered incorrectly.",
      "We manually analyzed 30 random failures (of 248) in the en- tire dev set (Regents + ARC, 1151 dev set questions to- Figure 7: Case Study: Causes of error for 30 questions that Aristo answered incorrectly. tal), and found four main categories of failures, illustrated in Figure 7, that we now summarize. As the language model solvers have highest weight in Aristo, we conduct this anal- ysis for failures by AristoRoBERTa, but note these very fre- quently (\u223c90% of the time) equate to overall Aristo failures, and that when AristoRoBERTa fails, most (on average, 76%) of the other solvers also fail also. We did not discern any systematic patterns within these. Good Support for Correct Answer (13%) Surprisingly, only 4 of the 30 failures were cases where the retrieved knowledge supported the right answer option, but Aristo chose a wrong answer option. An example is: Which is the best unit to measure distances between Earth and other solar systems in the universe?",
      "Good Support for Correct Answer (13%) Surprisingly, only 4 of the 30 failures were cases where the retrieved knowledge supported the right answer option, but Aristo chose a wrong answer option. An example is: Which is the best unit to measure distances between Earth and other solar systems in the universe? (A) miles (B) kilometers (C) light years [correct] (D) as- tronomical units [selected] Here, although Aristo did retrieve good evidence for the cor- rect answer (C), namely: Distances between Earth and the stars are often mea- sured in terms of light-years. it still preferred the incorrect option (D) from the retrieved knowledge: In general, distances in the solar system are measured in astronomical units. Here, Aristo has confused distinguishing distances within the Solar System vs. distances between solar systems. (A confusion that a human might easily make too). This il- lustrates where Aristo has mis-applied its retrieved knowl- edge. However, such cases appear to be rare (4/30).",
      "distances between solar systems. (A confusion that a human might easily make too). This il- lustrates where Aristo has mis-applied its retrieved knowl- edge. However, such cases appear to be rare (4/30). In other words, for the fast majority of questions, if suitable knowl- edge is retrieved, then Aristo will answer correctly. No Support for the Correct Answer (57%) The largest cause of failure was simply when none of the retrieved sen- tences provide evidence for the correct answer. In such sit- uations, Aristo has little chance of answering correctly. For example: 5",
      "Although they belong to the same family, an eagle and a pelican are different. What is one difference be- tween them? (A) their preference for eating \ufb01sh (B) their ability to \ufb02y (C) their method of reproduction [se- lected] (D) their method of catching food [correct] As there are no corpus sentences comparing eagles and peli- cans, Aristo retrieves a rather random collection of unhelpful facts. Instead, what is needed here is to realize this is a com- parison question, then retrieve appropriate facts for pelicans and eagles individually, and them compare them, for exam- ple by using question decomposition methods (Wolfson et al., 2020). Reading Comprehension (27%) In the exams, there are a few \u201creading comprehension\u201d questions that primarily re- quire reasoning over the question content itself, rather than retrieving and applying science knowledge. In such situa- tions, retrieved knowledge is unlikely to be helpful. 8/30 failures fell into this category. One example is a question describing an experiment: A student wants to determine the effect of garlic on the growth of a fungus species.",
      "In such situa- tions, retrieved knowledge is unlikely to be helpful. 8/30 failures fell into this category. One example is a question describing an experiment: A student wants to determine the effect of garlic on the growth of a fungus species. Several samples of fungus cultures are grown in the same amount of agar and light. Each sample is given a different amount of garlic. What is the independent variable in this in- vestigation? (A) amount of agar (B) amount of light (C) amount of garlic [correct] (D) amount of growth [selected] Here, the answer is unlikely to be written down in a corpus, as a novel scenario is being described. Rather, it requires understanding the scenario itself. A second example is a meta-question about sentiment: Which statement is an opinion? (A) Many plants are green. (B) Many plants are beautiful. [correct] (C) Plants require sunlight. [selected] (D) Plants can grow in different places. Again, retrieval is unlikely to help here. Rather, the question asks for an analysis of the options themselves, something Aristo does not realize.",
      "(B) Many plants are beautiful. [correct] (C) Plants require sunlight. [selected] (D) Plants can grow in different places. Again, retrieval is unlikely to help here. Rather, the question asks for an analysis of the options themselves, something Aristo does not realize. Good Support for an Incorrect Answer (3%) Occasion- ally a failure occurs due to retrieved knowledge supporting an incorrect answer, e.g., if the question is ambiguous, or the retrieved knowledge is wrong. The single failure in this category that we observed was: Which of these objects will most likely \ufb02oat in water? (A) glass marble (B) steel ball (C) hard rubber ball [selected] (D) table tennis ball [correct] Here, Aristo retrieved evidence for both (C) and (D), e.g., for (C), Aristo\u2019s retrieval included: \u201cIt had like a rubber ball in it, which would maybe \ufb02oat up. . . \u201d here leading Aristo to select the wrong answer.",
      ". . \u201d here leading Aristo to select the wrong answer. Arguably, as this question is a comparative (\u201c...which most likely \ufb02oats?\u201d), Aristo should have rejected this in favor of the cor- rect answer (table tennis ball). However, as Aristo computes a con\ufb01dence for each option independently, it is unable to directly make these cross-option comparisons. Other Finally we point to one other interesting failure: About how long does it take for the Moon to complete one revolution around Earth? (A) 7 days (B) 30 days [correct] (C) 90 days (D) 365 days [selected] In this case, many relevant sentences were retrieved, includ- ing: \u2022 Because it takes the moon about 27.3 days to complete one orbit around the Earth... \u2022 It takes 27.3 days for the moon to complete one revolution around the earth. \u2022 The Moon completes one revolution around the Earth in 27.32166 days. However, Aristo does not realize 27.3 is \u201cabout\u201d 30, and hence answered the question incorrectly.",
      "\u2022 The Moon completes one revolution around the Earth in 27.32166 days. However, Aristo does not realize 27.3 is \u201cabout\u201d 30, and hence answered the question incorrectly. A Score Card for Aristo\u2019s Semantic Skills From informal tests, Aristo appears to be doing more than simply matching a question + answer option to a retrieved sentence. Rather, Aristo appears to recognize various lin- guistic and semantic phenomena, and respond appropriately. For example, if we add negation (a \u201cnot\u201d) to the question, Aristo almost always correctly changes its answer choice. Similarly if we replace \u201cincrease\u201d with \u201cdecrease\u201d in a ques- tion, Aristo will typically change its answer choice correctly, suggesting it has some latent knowledge of qualitative direc- tion. To quantify such skills more systematically, we per- formed \ufb01ve sets of tests on Aristo without \ufb01ne-tuning Aristo on those tests, i.e., the tests are zero-shot.",
      "To quantify such skills more systematically, we per- formed \ufb01ve sets of tests on Aristo without \ufb01ne-tuning Aristo on those tests, i.e., the tests are zero-shot. (From other ex- periments, we know that if we train Aristo on these tests it can perform them almost perfectly, but our interest here is how Aristo performs \u201cout of the box\u201d after training on the science exams). Each test probes a different semantic phe- nomenon of interest, as we now describe. Negation How well does Aristo handle negation? As a (limited) test, we generated a synthetic negation dataset (10k questions), where each question has a synthetic context (re- placing the retrieved sentences), plus a question about it, for example: Context: Alan is small. Alan is tall. Bob is big. Bob is tall. Charlie is big. Charlie is tall. David is small. David is short. Question: Which of the following is not tall? (A) Alan (B) Bob (C) Charlie (D) David [correct] We then test Aristo on this dataset without \ufb01ne-tuning on it.",
      "Charlie is tall. David is small. David is short. Question: Which of the following is not tall? (A) Alan (B) Bob (C) Charlie (D) David [correct] We then test Aristo on this dataset without \ufb01ne-tuning on it. Remarkably, Aristo score 94% on this dataset, suggesting at least in this particular formulation, Aristo has an under- standing of \u201cnot\u201d. 6",
      "Conjunction We test conjunction in a similar way, with questions such as: Context: Alan is red. Alan is big. Bob is blue. Bob is small. Charlie is blue. Charlie is big. David is red. David is small. Question: Which of the following is big and blue? (A) Alan (B) Bob (C) Charlie [correct] (D) David With questions containing two conjuncts (e.g., the one above), and again without any \ufb01ne-tuning on this data, Aristo scores 98%. If we increase the number of conjuncts in the question to 3, 4, and 5, Aristo scores 95%, 94%, and 80% respectively. If we use \ufb01ve conjuncts and a negation, for example: Context: Alan is red. Alan is big. Alan is light. Alan is old. Alan is tall. Bob is red. Bob is small. Bob is heavy. Bob is old. Bob is tall. Charlie is blue. Charlie is big. Charlie is light. Charlie is old. Charlie is tall.",
      "Alan is old. Alan is tall. Bob is red. Bob is small. Bob is heavy. Bob is old. Bob is tall. Charlie is blue. Charlie is big. Charlie is light. Charlie is old. Charlie is tall. David is red. David is small. David is heavy. David is young. David is tall. Question: Which of the following is old and red and light and big and not short? (A) Alan (B) Bob (C) Charlie (D) David (the correct answer is left as an exercise for the reader) Aristo remarkably still scores 75%. Although not perfect, this indicates some form of systematic handling of conjunc- tion plus negation is occurring. Polarity Polarity refers to Aristo\u2019s ability to correctly change its answer when a comparative in the question is \u201c\ufb02ipped\u201d. For example, given: Which human activity will likely have a negative effect on global stability? (A) decreasing water pollution levels (B) increasing world population growth. [correct] if we now switch \u201cnegative\u201d to \u201cpositive\u201d, Aristo should switch its answer from (B) to (A).",
      "For example, given: Which human activity will likely have a negative effect on global stability? (A) decreasing water pollution levels (B) increasing world population growth. [correct] if we now switch \u201cnegative\u201d to \u201cpositive\u201d, Aristo should switch its answer from (B) to (A). To score a point, Aristo must get both the original question and the \u201c\ufb02ipped\u201d ques- tion (with a changed answer) correct. To measure this, we use an existing qualitative dataset containing such pairs, called QuaRTz (Tafjord et al., 2019). As the QuaRTz ques- tions are 2-way multiple-choice, a random score for getting both right would be 25%. Remarkably, we \ufb01nd Aristo scores 67.1% (again with no \ufb01ne-tuning on QuaRTz), suggesting Aristo has some knowledge about the polarity of compara- tives. Note that this test also requires Aristo to get the orig- inal question right in the \ufb01rst place, thus the score re\ufb02ects both knowledge and polarity reasoning, a harder task than polarity alone.",
      "Note that this test also requires Aristo to get the orig- inal question right in the \ufb01rst place, thus the score re\ufb02ects both knowledge and polarity reasoning, a harder task than polarity alone. Factuality Event factuality refers to whether an event, mentioned in a textual context, did in fact occur (Rudinger, White, and Durme, 2018). For example: Figure 8: Aristo, with no \ufb01ne-tuning, passes probes for all but counting. If someone regretted that a particular thing happened (A) that thing might or might not have happened (B) that thing didn\u2019t happen (C) that thing happened [correct] Predicting factuality requires understanding what the con- text around an event implies about that event\u2019s occurrence. We tested Aristo on this task using the veradicity question dev set from DNC (Poliak et al., 2018a), converted to multi- ple choice format (394 questions). On this task, Aristo score 66.5%, again suggesting Aristo has some knowledge of how words affect the factiveness of the events that they modify.",
      "On this task, Aristo score 66.5%, again suggesting Aristo has some knowledge of how words affect the factiveness of the events that they modify. Counting Finally we ran Aristo on bAbI task 7, (a simple form of) counting (Weston et al., 2015), converted to multi- ple choice with four options, as below. For example: Daniel picked up the football. Daniel dropped the football. Daniel got the milk. How many objects is Daniel holding? (A) zero (B) one (C) two (D) three Aristo (again not \ufb01ne-tuned on this dataset) did badly at this task, scoring only 6%.4 This results is perhaps not surpris- ing, as this type of reasoning is not exempli\ufb01ed in any way in an any of Aristo\u2019s training data. Scorecard We can informally map these scores to a grade level to give Aristo a Score Card (Figure 8).",
      "Scorecard We can informally map these scores to a grade level to give Aristo a Score Card (Figure 8). The most strik- ing observation is that Aristo \u201cpasses\u201d all but counting, and has apparently acquired these skills through its general \ufb01ne- tuning on RACE and Science Exams, with no \ufb01ne-tuning at all on these speci\ufb01c probing datasets. Aristo does appear to be doing more than just sentence matching, but not quite in the systematic way a person would. These acquired latent skills are re\ufb02ected in the high scores Aristo achieves on the Science Exams. Discussion What can we conclude from this? Most signi\ufb01cantly, Aristo has achieved surprising success on a formidable problem, 4Lower than random guessing, because Aristo frequently se- lects option D (\u201cthree\u201d), an option which is (by chance) very rarely the right answer in this dataset. \u201cD\u201d is likely chosen due to a small random bias towards \u201cthree\u201d, and all questions looking stylisti- cally similar. Note there is no training on this (nor other) probing datasets, so Aristo is unaware of the answer distribution.",
      "\u201cD\u201d is likely chosen due to a small random bias towards \u201cthree\u201d, and all questions looking stylisti- cally similar. Note there is no training on this (nor other) probing datasets, so Aristo is unaware of the answer distribution. 7",
      "in particular by leveraging large-scale language models. The system thus serves as a demonstration of the stunning progress that NLP technology as a whole has made in the last two years. At the same time, exams themselves are an imperfect test of understanding science, and, despite their many use- ful properties, are also only a partial test of machine intelli- gence (Davis, 2014). Earlier, we highlighted several classes of problems Aristo does not handle well, even though its exam scores are high: questions requiring diverse pieces of evidence to be combined, reading comprehension (\u201cstory\u201d) questions, meta-questions, and arithmetic. Davis (2016) has similarly pointed out that as standardized tests are authored for people, not machines, they also don\u2019t directly test for things that are easy for people such as temporal reasoning, simple counting, and obviously impossible situations. These are problem types that Aristo is not familiar with and would be hard for it (but not for people). Clearly, science exams are just one of many different, partial indicators of progress in broader AI.",
      "These are problem types that Aristo is not familiar with and would be hard for it (but not for people). Clearly, science exams are just one of many different, partial indicators of progress in broader AI. Finally, we have only been using multiple- choice questions, a format that just requires ranking of an- swer choices, arguably allowing more use of weak evidence compared with (say) generating an answer, or even indepen- dently deciding if an answer is true or false (Clark et al., 2019). On the other hand, we do see clear evidence of system- atic semantic skill in Aristo. For example, Aristo not only answers this question correctly: City administrators can encourage energy conserva- tion by (A) lowering parking fees (B) decreasing the cost of gasoline. (C) lowering the cost of bus and subway fares. [cor- rect,selected] but \ufb02ipping \u201cdecreasing\u201d and \u201clowering\u201d causes it to cor- rectly change its answer: City administrators can encourage energy conserva- tion by (A) lowering parking fees (B) decreasing increasing the cost of gasoline.",
      "[cor- rect,selected] but \ufb02ipping \u201cdecreasing\u201d and \u201clowering\u201d causes it to cor- rectly change its answer: City administrators can encourage energy conserva- tion by (A) lowering parking fees (B) decreasing increasing the cost of gasoline. [cor- rect,selected] (C) lowering raising the cost of bus and subway fares. Our probes showed that such behavior is not just anecdo- tal but systematic, suggesting that some form of reasoning is occurring, but not in the traditional style of discrete sym- bol manipulation in a formally designed language (Brach- man and Levesque, 1985; Genesereth and Nilsson, 2012). Other work has similarly found that neural systems can learn systematic behavior (Lake and Baroni, 2017; Clark, Tafjord, and Richardson, 2020), and these emergent seman- tic skills are a key contributor to Aristo\u2019s scores reaching the 90% range. Large-scale language model architectures have brought a dramatic, new capability to the table that goes signi\ufb01cantly beyond just pattern matching and similarity as- sessment.",
      "Large-scale language model architectures have brought a dramatic, new capability to the table that goes signi\ufb01cantly beyond just pattern matching and similarity as- sessment. Related Work on Standardized Testing for AI Standardized Tests Standardized tests have long been proposed as challenge problems for AI (e.g., Bringsjord and Schimanski, 2003; Brachman et al., 2005; Clark and Etzioni, 2016; Piatetsky- Shapiro et al., 2006), as they appear to require signi\ufb01cant advances in AI technology while also being accessible, mea- surable, understandable, and motivating. Earlier work on standardized tests focused on specialized tasks, for example, SAT word analogies (Turney, 2006), GRE word antonyms (Mohammad et al., 2013), and TOEFL synonyms (Landauer and Dumais, 1997). More recently, there have been attempts at building systems to pass uni- versity entrance exams.",
      "More recently, there have been attempts at building systems to pass uni- versity entrance exams. Under NII\u2019s Todai project, several systems were developed for parts of the University of Tokyo Entrance Exam, including maths, physics, English, and his- tory (Strickland, 2013; Tainaka, 2013; Fujita et al., 2014), although in some cases questions were modi\ufb01ed or anno- tated before being given to the systems (e.g., Matsuzaki et al., 2014). Similarly, a smaller project worked on passing the Gaokao (China\u2019s college entrance exam) (e.g., Cheng et al., 2016; Guo et al., 2017). The Todai project was reported as ended in 2016, in part because of the challenges of building a machine that could \u201cgrasp meaning in a broad spectrum\u201d (Mott, 2016). Math Word Problems Substantial progress has been achieved on math word prob- lems. On plane geometry questions, (Seo et al., 2015) demonstrated an approach that achieve a 61% accuracy on SAT practice questions.",
      "Math Word Problems Substantial progress has been achieved on math word prob- lems. On plane geometry questions, (Seo et al., 2015) demonstrated an approach that achieve a 61% accuracy on SAT practice questions. The Euclid system (Hopkins et al., 2017) achieved a 43% recall and 91% precision on SAT \u201dclosed-vocabulary\u201d algebra questions, a limited subset of questions that nonetheless constitutes approximately 45% of a typical math SAT exam. Closed-vocabulary questions are those that do not reference real-world situations (e.g., \u201dwhat is the largest prime smaller than 100?\u201d or \u201dTwice the product of x and y is 8. What is the square of x times y?\u201d) Work on open-world math questions has continued, but re- sults on standardized tests have not been reported and thus it is dif\ufb01cult to benchmark the progress relative to human performance. See Amini et al. (2019) for a recent snapshot of the state of the art, and references to the literature on this problem.",
      "See Amini et al. (2019) for a recent snapshot of the state of the art, and references to the literature on this problem. Summary and Conclusion Answering science questions is a long-standing AI grand challenge (Reddy, 1988; Friedland et al., 2004). We have described Aristo, the \ufb01rst system to achieve a score of over 90% on the non-diagram, multiple choice part of the New York Regents 8th Grade Science Exam, demonstrating that modern NLP methods can result in mastery of this task. Al- though Aristo only answers multiple choice questions with- out diagrams, and operates only in the domain of science, it nevertheless represents an important milestone towards systems that can read and understand. The momentum on 8",
      "this task has been remarkable, with accuracy moving from roughly 60% to over 90% in just three years. In addition, the use of independently authored questions from a standardized test allows us to benchmark AI performance relative to hu- man students. Beyond the use of a broad vocabulary and scienti\ufb01c con- cepts, many of the benchmark questions intuitively appear to require some degree of reasoning to answer. For many years in AI, reasoning was thought of as discrete symbol manipu- lation. With the advent of deep learning, this notion of rea- soning has expanded, with systems performing challenging tasks using neural architectures rather than explicit repre- sentation languages. Similarly, we observe surprising per- formance on answering science questions, and on speci\ufb01c semantic phenomena such as those probed earlier. This sug- gests that the machine has indeed learned something about language and the world, and how to manipulate that knowl- edge, albeit neither symbolically nor discretely. Although an important milestone, this work is only a step on the long road toward a machine that has a deep under- standing of science and achieves Paul Allen\u2019s original dream of a Digital Aristotle.",
      "Although an important milestone, this work is only a step on the long road toward a machine that has a deep under- standing of science and achieves Paul Allen\u2019s original dream of a Digital Aristotle. A machine that has fully understood a textbook should not only be able to answer the multiple choice questions at the end of the chapter - it should also be able to generate both short and long answers to direct questions; it should be able to perform constructive tasks, e.g., designing an experiment for a particular hypothesis; it should be able to explain its answers in natural language and discuss them with a user; and it should be able to learn di- rectly from an expert who can identify and correct the ma- chine\u2019s misunderstandings. These are all ambitious tasks still largely beyond the current technology, but with the rapid progress happening in NLP and AI, solutions may arrive sooner than we expect. Acknowledgements We gratefully acknowledge the late Paul Allen\u2019s inspira- tion, passion, and support for research on this grand chal- lenge.",
      "Acknowledgements We gratefully acknowledge the late Paul Allen\u2019s inspira- tion, passion, and support for research on this grand chal- lenge. We also thank the many other contributors to Aristo, including Niranjan Balasubramanian, Matt Gardner, Peter Jansen, Jayant Krishnamurthy, Souvik Kundu, Todor Mi- haylov, Harsh Trivedi, Peter Turney, and the Beaker team at AI2, and to Ernie Davis, Gary Marcus, Raj Reddy, and many others for helpful feedback on this work. We also thank the anonymous reviewers for helpful comments that improved the paper. References Allen, P. 2012. Idea Man: A memoir by the Cofounder of Microsoft. Penguin. Amini, A.; Gabriel, S.; Lin, P.; Koncel-Kedziorski, R.; Choi, Y.; and Hajishirzi, H. 2019. MathQA: Towards Inter- pretable Math Word Problem Solving with Operation- Based Formalisms. In NAACL-HLT.",
      "2019. MathQA: Towards Inter- pretable Math Word Problem Solving with Operation- Based Formalisms. In NAACL-HLT. Banko, M.; Cafarella, M. J.; Soderland, S.; Broadhead, M.; and Etzioni, O. 2007. Open Information Extraction from the Web. In IJCAI. Brachman, R. J., and Levesque, H. J. 1985. Readings in Knowledge Representation. Morgan Kaufmann Publish- ers Inc. Brachman, R.; Gunning, D.; Bringsjord, S.; Genesereth, M.; Hirschman, L.; and Ferro, L. 2005. Selected Grand Chal- lenges in Cognitive Science. Technical report, MITRE Technical Report 05-1218. Bedford MA: The MITRE Corporation. Bringsjord, S., and Schimanski, B. 2003. What is Arti\ufb01cial Intelligence? Psychometric AI as an Answer. In IJCAI, 887\u2013893. Citeseer.",
      "Bringsjord, S., and Schimanski, B. 2003. What is Arti\ufb01cial Intelligence? Psychometric AI as an Answer. In IJCAI, 887\u2013893. Citeseer. Cheng, G.; Zhu, W.; Wang, Z.; Chen, J.; and Qu, Y. 2016. Taking up the Gaokao Challenge: An Information Re- trieval Approach. In IJCAI, 2479\u20132485. Church, K. W., and Hanks, P. 1989. Word Association Norms, Mutual Information and Lexicography. In 27th ACL, 76\u201383. Clark, P., and Etzioni, O. 2016. My Computer is an Honor Student - But how Intelligent is it? Standardized Tests as a Measure of AI. AI Magazine 37(1):5\u201312. Clark, P.; Balasubramanian, N.; Bhakthavatsalam, S.; Humphreys, K.; Kinkead, J.; Sabharwal, A.; and Tafjord, O.",
      "AI Magazine 37(1):5\u201312. Clark, P.; Balasubramanian, N.; Bhakthavatsalam, S.; Humphreys, K.; Kinkead, J.; Sabharwal, A.; and Tafjord, O. 2014. Automatic Construction of Inference- Supporting Knowledge Bases. In 4th Workshop on Automated Knowledge Base Construction (AKBC). Clark, P.; Etzioni, O.; Khot, T.; Sabharwal, A.; Tafjord, O.; Turney, P. D.; and Khashabi, D. 2016. Combining Re- trieval, Statistics, and Inference to Answer Elementary Science Questions. In AAAI, 2580\u20132586. Clark, P.; Cowhey, I.; Etzioni, O.; Khot, T.; Sabharwal, A.; Schoenick, C.; and Tafjord, O. 2018. Think you have Solved Question Answering? Try ARC, the AI2 Reason- ing Challenge. ArXiv abs/1803.05457.",
      "2018. Think you have Solved Question Answering? Try ARC, the AI2 Reason- ing Challenge. ArXiv abs/1803.05457. Clark, C.; Lee, K.; Chang, M.-W.; Kwiatkowski, T.; Collins, M.; and Toutanova, K. 2019. BoolQ: Exploring the Surprising Dif\ufb01culty of Natural Yes/No Questions. In NAACL-HLT. Clark, P.; Tafjord, O.; and Richardson, K. 2020. Transform- ers as Soft Reasoners over Language. In IJCAI. Davis, E. 2014. The Limitations of Standardized Science Tests as Benchmarks for Arti\ufb01cial Intelligence Research. ArXiv abs/1411.1629. Davis, E. 2016. How to Write Science Questions that are Easy for People and Hard for Computers. AI Magazine 37:13\u201322. Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.",
      "Davis, E. 2016. How to Write Science Questions that are Easy for People and Hard for Computers. AI Magazine 37:13\u201322. Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL. Ferrucci, D.; Brown, E.; Chu-Carroll, J.; Fan, J.; Gondek, D.; Kalyanpur, A. A.; Lally, A.; Murdock, J. W.; Nyberg, E.; Prager, J.; et al. 2010. Building Watson: An Overview of the DeepQA Project. AI magazine 31(3):59\u201379. Friedland, N. S.; Allen, P. G.; Matthews, G.; Witbrock, M.; Baxter, D.; Curtis, J.; Shepard, B.; Miraglia, P.; Angele, 9",
      "J.; Staab, S.; et al. 2004. Project Halo: Towards a Digital Aristotle. AI magazine 25(4):29\u201329. Fujita, A.; Kameda, A.; Kawazoe, A.; and Miyao, Y. 2014. Overview of Todai Robot Project and Evaluation Frame- work of its NLP-based Problem Solving. In LREC. Genesereth, M. R., and Nilsson, N. J. 2012. Logical Foundations of Arti\ufb01cial Intelligence. Morgan Kauf- mann. Guo, S.; Zeng, X.; He, S.; Liu, K.; and Zhao, J. 2017. Which is the Effective Way for Gaokao: Information Retrieval or Neural Networks? In EACL\u201917, 111\u2013120. Gururangan, S.; Swayamdipta, S.; Levy, O.; Schwartz, R.; Bowman, S. R.; and Smith, N. A. 2018. Annotation Arti- facts in Natural Language Inference Data. In NAACL.",
      "Gururangan, S.; Swayamdipta, S.; Levy, O.; Schwartz, R.; Bowman, S. R.; and Smith, N. A. 2018. Annotation Arti- facts in Natural Language Inference Data. In NAACL. Hopkins, M.; Petrescu-Prahova, C.; Levin, R.; Bras, R. L.; Herrasti, A.; and Joshi, V. 2017. Beyond Sentential Se- mantic Parsing: Tackling the Math SAT with a Cascade of Tree Transducers. In EMNLP. Joshi, M.; Choi, E.; Weld, D. S.; and Zettlemoyer, L. 2017. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In ACL\u201917. Vancou- ver, Canada: Association for Computational Linguistics. Khashabi, D.; Khot, T.; Sabharwal, A.; Clark, P.; Etzioni, O.; and Roth, D. 2016. Question Answering via Inte- ger Programming over Semi-Structured Knowledge.",
      "Khashabi, D.; Khot, T.; Sabharwal, A.; Clark, P.; Etzioni, O.; and Roth, D. 2016. Question Answering via Inte- ger Programming over Semi-Structured Knowledge. In IJCAI. Khashabi, D.; Khot, T.; Sabharwal, A.; and Roth, D. 2018. Question Answering as Global Reasoning over Semantic Abstractions. In AAAI. Khot, T.; Balasubramanian, N.; Gribkoff, E.; Sabharwal, A.; Clark, P.; and Etzioni, O. 2015. Exploring Markov Logic Networks for Question Answering. In EMNLP. Khot, T.; Sabharwal, A.; and Clark, P. F. 2017. Answering Complex Questions using Open Information Extraction. In ACL. Krishnamurthy, J.; Tafjord, O.; and Kembhavi, A. 2016. Semantic Parsing to Probabilistic Programs for Situated Question Answering. In EMNLP.",
      "Answering Complex Questions using Open Information Extraction. In ACL. Krishnamurthy, J.; Tafjord, O.; and Kembhavi, A. 2016. Semantic Parsing to Probabilistic Programs for Situated Question Answering. In EMNLP. Lai, G.; Xie, Q.; Liu, H.; Yang, Y.; and Hovy, E. 2017. RACE: Large-scale Reading Comprehension Dataset from Examinations. In EMNLP. Lake, B. M., and Baroni, M. 2017. Generalization without systematicity: On the compositional skills of sequence- to-sequence recurrent networks. In ICML. Landauer, T. K., and Dumais, S. T. 1997. A Solution to Plato\u2019s problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowl- edge. Psychological review 104(2):211. Larkin, J. H.; McDermott, J.; Simon, D. P.; and Simon, H. A. 1980.",
      "Psychological review 104(2):211. Larkin, J. H.; McDermott, J.; Simon, D. P.; and Simon, H. A. 1980. Models of Competence in Solving Physics Prob- lems. Cognitive Science 4:317\u2013345. Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V. 2019. RoBERTa: A Robustly Optimized BERT Pretrain- ing Approach. arXiv preprint arXiv:1907.11692. Matsuzaki, T.; Iwane, H.; Anai, H.; and Arai, N. H. 2014. The most Uncreative Examinee: a First Step toward Wide Coverage Natural Language Math Problem Solving. In AAAI\u201914. Mihaylov, T.; Clark, P.; Khot, T.; and Sabharwal, A. 2018. Can a Suit of Armor Conduct Electricity?",
      "In AAAI\u201914. Mihaylov, T.; Clark, P.; Khot, T.; and Sabharwal, A. 2018. Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering. In EMNLP. Mohammad, S. M.; Dorr, B. J.; Hirst, G.; and Turney, P. D. 2013. Computing Lexical Contrast. Computational Linguistics 39(3):555\u2013590. Mott, N. 2016. Todai Robot Gives Up on Get- ting Into the University of Tokyo. Inverse. (https://www.inverse.com/article/23761-todai-robot- gives-up-university-tokyo). Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M. P.; Clark, C.; Lee, K.; and Zettlemoyer, L. S. 2018. Deep Contextualized Word Representations. In NAACL.",
      "Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M. P.; Clark, C.; Lee, K.; and Zettlemoyer, L. S. 2018. Deep Contextualized Word Representations. In NAACL. Piatetsky-Shapiro, G.; Djeraba, C.; Getoor, L.; Grossman, R.; Feldman, R.; and Zaki, M. 2006. What are the Grand Challenges for Data Mining?: KDD-2006 Panel Report. ACM SIGKDD Explorations Newsletter 8(2):70\u201377. Poliak, A.; Haldar, A.; Rudinger, R.; Hu, J. E.; Pavlick, E.; White, A. S.; and Durme, B. V. 2018a. Collecting Di- verse Natural Language Inference Problems for Sentence Representation Evaluation. In EMNLP. Poliak, A.; Naradowsky, J.; Haldar, A.; Rudinger, R.; and Van Durme, B. 2018b.",
      "2018a. Collecting Di- verse Natural Language Inference Problems for Sentence Representation Evaluation. In EMNLP. Poliak, A.; Naradowsky, J.; Haldar, A.; Rudinger, R.; and Van Durme, B. 2018b. Hypothesis Only Baselines in Natural Language Inference. In StarSem. Rajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016. SQuAD: 100,000+ Questions for Machine Comprehen- sion of Text. In EMNLP. Reddy, R. 1988. Foundations and Grand Challenges of Arti\ufb01cial Intelligence: AAAI Presidential Address. AI Magazine 9(4). Reddy, R. 2003. Three Open Problems in AI. J. ACM 50:83\u201386. Rudinger, R.; White, A. S.; and Durme, B. V. 2018. Neural Models of Factuality. In NAACL-HLT.",
      "2003. Three Open Problems in AI. J. ACM 50:83\u201386. Rudinger, R.; White, A. S.; and Durme, B. V. 2018. Neural Models of Factuality. In NAACL-HLT. Schoenick, C.; Clark, P. F.; Tafjord, O.; Turney, P. D.; and Etzioni, O. 2016. Moving beyond the Turing Test with the Allen AI Science Challenge. CACM. Seo, M. J.; Hajishirzi, H.; Farhadi, A.; Etzioni, O.; and Mal- colm, C. 2015. Solving Geometry Problems: Combining Text and Diagram Interpretation. In EMNLP. Seo, M. J.; Kembhavi, A.; Farhadi, A.; and Hajishirzi, H. 2016. Bidirectional Attention Flow for Machine Compre- hension. ArXiv abs/1611.01603. Strickland, E. 2013. Can an AI get into the University of Tokyo?",
      "2016. Bidirectional Attention Flow for Machine Compre- hension. ArXiv abs/1611.01603. Strickland, E. 2013. Can an AI get into the University of Tokyo? IEEE Spectrum 50(9):13\u201314. Sun, K.; Yu, D.; Yu, D.; and Cardie, C. 2019. Improving Machine Reading Comprehension with General Reading Strategies. In NAACL-HLT. 10",
      "Tafjord, O.; Gardner, M.; Lin, K.; and Clark, P. 2019. QuaRTz: An Open-Domain Dataset of Qualitative Rela- tionship Questions. In EMNLP. Tainaka, M. 2013. The Todai Robot Project. NII Today 46. (http://www.nii.ac.jp/userdata/results/pr data/NII Today/ 60 en/all.pdf). Tandon, N.; Mishra, B. D.; Grus, J.; Yih, W.-t.; Bosselut, A.; and Clark, P. 2018. Reasoning about Actions and State Changes by Injecting Commonsense Knowledge. In EMNLP. Trivedi, H.; Kwon, H.; Khot, T.; Sabharwal, A.; and Bal- asubramanian, N. 2019. Repurposing Entailment for Multi-Hop Question Answering Tasks. In NAACL. Turney, P. D. 2006. Similarity of Semantic Relations. Computational Linguistics 32(3):379\u2013416.",
      "2019. Repurposing Entailment for Multi-Hop Question Answering Tasks. In NAACL. Turney, P. D. 2006. Similarity of Semantic Relations. Computational Linguistics 32(3):379\u2013416. Turney, P. D. 2017. Leveraging Term Banks for Answering Complex Questions: A Case for Sparse Vectors. arXiv preprint arXiv:1704.03543. Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and Bowman, S. R. 2019. GLUE: A Multi-task Benchmark and Analysis Platform for Natural Language Understand- ing. In ICLR. Wang, W.; Yan, M.; and Wu, C. 2018. Multi-Granularity Hi- erarchical Attention Fusion Networks for Reading Com- prehension and Question Answering. In ACL. Weston, J.; Bordes, A.; Chopra, S.; and Mikolov, T. 2015. Towards AI-Complete Question Answering: A Set of Pre- requisite Toy Tasks.",
      "In ACL. Weston, J.; Bordes, A.; Chopra, S.; and Mikolov, T. 2015. Towards AI-Complete Question Answering: A Set of Pre- requisite Toy Tasks. arXiv 1502.05698. Wolfson, T.; Geva, M.; Gupta, A.; Gardner, M.; Goldberg, Y.; Deutch, D.; and Berant, J. 2020. Break It Down: A Question Understanding Benchmark. TACL. 11"
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1909.01958.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":12283,
  "avg_doclen":170.5972222222,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1909.01958.pdf"
    }
  }
}