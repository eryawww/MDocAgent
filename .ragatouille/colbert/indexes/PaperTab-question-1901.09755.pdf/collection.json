[
  "Language Independent Sequence Labelling for Opinion Target Extraction\u2217\u2020 Rodrigo Agerri\u2021and German Rigau IXA NLP Group University of the Basque Country (UPV/EHU) Donostia-San Sebasti\u00b4an, Spain Abstract In this research note we present a language independent system to model Opinion Target Extraction (OTE) as a sequence labelling task. The system consists of a combination of clustering features implemented on top of a simple set of shallow local features. Experiments on the well known Aspect Based Sentiment Analysis (ABSA) benchmarks show that our approach is very competitive across languages, obtaining best results for six languages in seven di\ufb00erent datasets. Furthermore, the results provide further insights into the behaviour of clustering features for sequence labelling tasks. The system and models generated in this work are available for public use and to facilitate reproducibility of results.",
  "Furthermore, the results provide further insights into the behaviour of clustering features for sequence labelling tasks. The system and models generated in this work are available for public use and to facilitate reproducibility of results. Keywords: Opinion Target Extraction, Aspect Based Sentiment Analysis, Information Extrac- tion, Clustering, Semi-supervised learning, Natural Language Processing 1 Introduction Opinion Mining and Sentiment Analysis (OMSA) are crucial for determining opinion trends and attitudes about commercial products, companies reputation management, brand monitoring, or to track attitudes by mining social media, etc. Furthermore, given the explosion of information produced and shared via the Internet, especially in social media, it is simply not possible to keep up with the constant \ufb02ow of new information by manual methods. Early approaches to OMSA were based on document classi\ufb01cation, where the task was to determine the polarity (positive, negative, neutral) of a given document or review (Pang and Lee, 2008; Liu, 2012). A well known benchmark for polarity classi\ufb01cation at document level is that of Pang et al. (2002). Later on, a \ufb01ner-grained OMSA was deemed necessary.",
  "A well known benchmark for polarity classi\ufb01cation at document level is that of Pang et al. (2002). Later on, a \ufb01ner-grained OMSA was deemed necessary. This was motivated by the fact that in a given review more than one opinion about a variety of aspects or attributes of a given product is usually conveyed. Thus, Aspect Based Sentiment Analysis (ABSA) was de\ufb01ned as a task which consisted of identifying several components of a given opinion: the opinion holder, the target, the opinion expression (the textual expression conveying polarity) and the aspects or \u2217Please cite this paper as: R. Agerri, G. Rigau. Language Independent Sequence Labelling for Opinion Target Extraction, Arti\ufb01cial Intelligence (2018), 268: 65-85. https://doi.org/10.1016/j.artint.2018.12.002. c\u20dd2018. \u2020This manuscript is made available under the CC-BY-NC-ND 4.0 license http://creativecommons.org/ licenses/by-nc-nd/4.0/.",
  "c\u20dd2018. \u2020This manuscript is made available under the CC-BY-NC-ND 4.0 license http://creativecommons.org/ licenses/by-nc-nd/4.0/. Paper submitted 6 November 2017, Revised 30 November 2018, Accepted 6 Decem- ber 2018. \u2021Corresponding author: rodrigo.agerri@ehu.eus 1 arXiv:1901.09755v1  [cs.CL]  28 Jan 2019",
  "features. Aspects are mostly domain-dependent. In restaurant reviews, relevant aspects would include \u201cfood quality\u201d, \u201cprice\u201d, \u201cservice\u201d, \u201crestaurant ambience\u201d, etc. Similarly, if the reviews were about consumer electronics such as laptops, then aspects would include \u201csize\u201d, \u201cbattery life\u201d, \u201chard drive capacity\u201d, etc. In the review shown by Figure 1 there are three di\ufb00erent opinions about two di\ufb00erent aspects (categories) of the restaurant, namely, the \ufb01rst two opinions are about the quality of the food and the third one about the general ambience of the place. Furthermore, there are just two opinion targets because the target of the third opinion, the restaurant itself, remains implicit. Finally, each aspect is assigned a polarity; in this case all three opinion aspects are negative.",
  "Furthermore, there are just two opinion targets because the target of the third opinion, the restaurant itself, remains implicit. Finally, each aspect is assigned a polarity; in this case all three opinion aspects are negative. <sentence id=\"1016296:4\"> <text>Chow fun was dry; pork shu mai was more than usually greasy and had to share a table with loud and rude family</text> <Opinions> <Opinion target=\"Chow fun\" category=\"FOOD#QUALITY\" polarity=\"negative\" from=\"0\" to= \"8\"/> <Opinion target=\"pork shu mai\" category=\"FOOD#QUALITY\" polarity=\"negative\" from=\"18 \" to=\"30\"/> <Opinion target=\"NULL\" category=\"AMBIENCE#GENERAL\" polarity=\"negative\" from=\"0\" to= \"0\"/> </Opinions> </sentence> Figure 1: Aspect Based Sentiment Analysis example. In this work we focus on Opinion Target Extraction, which we model as a sequence labelling task.",
  "In this work we focus on Opinion Target Extraction, which we model as a sequence labelling task. In order to do so, we convert an annotated review such as the one in Figure 1 into the BIO scheme for learning sequence labelling models (Tjong Kim Sang, 2002). Example (1) shows the review in BIO format. Tokens in the review are tagged depending on whether they are at the beginning (B-target), inside (I-target) or outside (O) of the opinion target expression. Note that the third opinion target in Figure 1 is implicit. (1) Chow/B-target fun/I-target was/O dry/O; pork/B-target shu/I-target mai/I-target was/O more/O than/O usually/O greasy/O and/O had/O to/O share/O a/O table/O with/O loud/O and/O rude/O family/O. We learn language independent models which consist of a set of local, shallow features com- plemented with semantic distributional features based on clusters obtained from a variety of data sources.",
  "We learn language independent models which consist of a set of local, shallow features com- plemented with semantic distributional features based on clusters obtained from a variety of data sources. We show that our approach, despite the lack of hand-engineered, language-speci\ufb01c fea- tures, obtains state-of-the-art results in 7 datasets for 6 languages on the ABSA benchmarks (Pontiki et al., 2014, 2015, 2016). The main contribution of this research note is providing an extension or addendum to previous work on sequence labelling (Agerri and Rigau, 2016) by reporting additional experimental results as well as further insights on the performance of our model across languages on a di\ufb00erent NLP task such as Opinion Target Extraction (OTE). Thus, we empirically demonstrate the validity and strong performance of our approach for six languages in seven di\ufb00erent datasets of the restaurant domain. Every experiment and result presented in this note is novel.",
  "Thus, we empirically demonstrate the validity and strong performance of our approach for six languages in seven di\ufb00erent datasets of the restaurant domain. Every experiment and result presented in this note is novel. In this sense, we show that our approach is not only competitive across languages and domains for Named Entity Recognition, as shown by Agerri and Rigau (2016), but that it can be straight- forwardly adapted to di\ufb00erent tasks and domains such as OTE. Furthermore, we release the system and every model trained for public use and to facilitate reproducibility of results. 2",
  "2 Background Early approaches to Opinion Target Extraction (OTE) were unsupervised, although later on the vast majority of works have been based on supervised and deep learning models. To the best of our knowledge, the \ufb01rst work on OTE was published by Hu and Liu (2004). They created a new task which consisted of generating overviews of the main product features from a collection of customer reviews on consumer electronics. They addressed such task using an unsupervised algorithm based on association mining. Other early unsupervised approaches include Popescu and Etzioni (2005) which used a dependency parser to obtain more opinion targets, and Kim and Hovy (2006) which aimed at extracting opinion targets in newswire via Semantic Role Labelling. From a supervised perspective, Zhuang et al. (2006) presented an approach which learned the opinion target candidates and a combination of dependency and part-of-speech (POS) paths connecting such pairs. Their results improved the baseline provided by Hu and Liu (2004). Another in\ufb02uential work was Qiu et al.",
  "(2006) presented an approach which learned the opinion target candidates and a combination of dependency and part-of-speech (POS) paths connecting such pairs. Their results improved the baseline provided by Hu and Liu (2004). Another in\ufb02uential work was Qiu et al. (2011), an unsupervised algorithm called Double Propagation which roughly consists of incrementally augmenting a set of seeds via dependency parsing. Closer to our work, Jin et al. (2009), Li et al. (2010) and Jakob and Gurevych (2010) approached OTE as a sequence labelling task, modelling the opinion targets using the BIO scheme. The \ufb01rst approach implemented HMM whereas the last two proposed CRFs to solve the problem. In all three cases, their systems included extensive human-designed and linguistically motivated features, such as POS tags, lemmas, dependencies, constituent parsing structure, lexical patterns and semantic features extracted from WordNet (Fellbaum and Miller, 1998). Quite frequently these works used a third party dataset, or a subset of the original one, or created their own annotated data for their experiments.",
  "Quite frequently these works used a third party dataset, or a subset of the original one, or created their own annotated data for their experiments. The result was that it was di\ufb03cult to draw precise conclusions about the advantages or disadvantages of the proposed methods. In this context, the Aspect Based Sentiment Analysis (ABSA) tasks at SemEval (Pontiki et al., 2014, 2015, 2016) provided standard training and evaluation data thereby helping to establish a clear benchmark for the OTE task. Finally, it should be noted that there is a closely related task, namely, the SemEval 2016 task on Stance Detection1. Stance detection is related to ABSA, but there is a signi\ufb01cant di\ufb00erence. In ABSA the task is to determine whether a piece of text is positive, negative, or neutral with respect to an aspect and a given target (which in Stance Detection is called \u201cauthor\u2019s favorability\u201d towards a given target).",
  "In ABSA the task is to determine whether a piece of text is positive, negative, or neutral with respect to an aspect and a given target (which in Stance Detection is called \u201cauthor\u2019s favorability\u201d towards a given target). However, in Stance Detection the text may express opinion or sentiment about some other target, not mentioned in the given text, and the targets are prede\ufb01ned, whereas in ABSA the targets are open-ended. 2.1 ABSA Tasks at SemEval Three ABSA editions were held within the SemEval Evaluation Exercises between 2014 and 2016. The ABSA 2014 and 2015 tasks consisted of English reviews only, whereas in the 2016 task 7 more languages were added. Additionally, reviews from four domains were collected for the various sub- tasks across the three editions, namely, Consumer Electronics, Telecommunications, Museums and Restaurant reviews. In any case, the only constant in each of the ABSA editions was the inclusion, for the Opinion Target Extraction (OTE) sub-task, of restaurant reviews for every language.",
  "In any case, the only constant in each of the ABSA editions was the inclusion, for the Opinion Target Extraction (OTE) sub-task, of restaurant reviews for every language. Thus, for the experiments presented in this paper we decided to focus on the restaurant domain across 6 languages and the three di\ufb00erent ABSA editions. Similarly, this section will be focused on reviewing the OTE results for the restaurant domain. 1http://alt.qcri.org/semeval2016/task6/ 3",
  "The ABSA task consisted of identifying, for each opinion, the opinion target, the aspect re- ferred to by the opinion and the aspect\u2019s polarity. Figure 1 illustrates the original annotation of a restaurant review in the ABSA 2016 dataset. It should be noted that, out of the three opinion components, only the targets are explicitly represented in the text, which means that OTE can be independently modelled as a sequence labelling problem as shown by Example (1). It is particu- larly important to notice that the opinion expressions (\u201cdry\u201d, \u201cgreasy\u201d, \u201cloud and rude\u201d) are not annotated. Following previous approaches, the \ufb01rst competitive systems for OTE at ABSA were supervised. Among the participants (for English) in the three editions, one team (Toh and Wang, 2014; Toh and Su, 2015) was particularly successful.",
  "Following previous approaches, the \ufb01rst competitive systems for OTE at ABSA were supervised. Among the participants (for English) in the three editions, one team (Toh and Wang, 2014; Toh and Su, 2015) was particularly successful. For ABSA 2014 and 2015 they developed a CRF system with extensive handcrafted linguistic features: POS, head word, dependency relations, WordNet relations, gazetteers and Name Lists based on applying the Double Propagation algorithm (Qiu et al., 2011) on an initial list of 551 seeds. Interestingly, they also introduced word representation features based on Brown and K-mean clusters. For ABSA 2016, they improved their system by using the output of a Recurrent Neural Network (RNN) to provide additional features. The RNN is trained on the following input features: word embeddings, Name Lists and word clusters (Toh and Su, 2016). They were the best system in 2014 and 2016.",
  "The RNN is trained on the following input features: word embeddings, Name Lists and word clusters (Toh and Su, 2016). They were the best system in 2014 and 2016. In 2015 they obtained the second best result, in which the best system, a preliminary version of the one presented in this note, was submitted by the EliXa team (San Vicente et al., 2015). From 2015 onwards most works have been based on deep learning. Liu et al. (2015) applied RNNs on top of a variety of pre-trained word embeddings, while Jebbara and Cimiano (2016) presented an architecture in which a RNN based tagger is stacked on top of the features generated by a Convolutional Neural Network (CNN). These systems were evaluated on the 2014 and 2015 datasets, respectively, but they did not go beyond the state-of-the-art. Poria et al.",
  "These systems were evaluated on the 2014 and 2015 datasets, respectively, but they did not go beyond the state-of-the-art. Poria et al. (2016) presented a 7 layer deep CNN combining word embeddings trained on a 5 billion word corpus extracted from Amazon (McAuley and Leskovec, 2013), POS tag features and manually developed linguistic patterns based on syntactic analysis and SenticNet (Cambria et al., 2014) a concept-level knowledge based build for Sentiment Analysis applications. They only evaluate their system on the English 2014 ABSA data, obtaining best results up to date on that benchmark. More recently, Wang et al. (2017) proposed a coupled multi-layer attention (CMLA) network where each layer consists of a couple of attentions with tensor operators. Unlike previous ap- proaches, their system does not use complex linguistic-based features designed for one speci\ufb01c language. However, whereas previous successful approaches modelled OTE as an independent task, in the CMLA model the attentions interactively learn both the opinion targets and the opin- ion expressions.",
  "However, whereas previous successful approaches modelled OTE as an independent task, in the CMLA model the attentions interactively learn both the opinion targets and the opin- ion expressions. As opinion expressions are not available in the original ABSA datasets, they had to manually annotate the ABSA training and testing data with the required opinion expressions. Although Wang et al. (2017) did not release the datasets with the annotated opinion expressions, Figure 2 illustrates what these annotations would look like. Thus, two new attributes (pfrom and pto) annotate the opinion expressions for each of the three opinions (\u201cdry\u201d, \u201cgreasy\u201d and \u201cloud and rude\u201d, respectively). Using this new manual information to train their CMLA network they reported the best results so far for ABSA 2014 and 2015 (English only). Finally, Li and Lam (2017) develop a multi-task learning framework consisting of two LSTMs equipped with extended memories and neural memory operations. As Wang et al. (2017), they use opinion expressions annotations for a joint modelling of opinion targets and expressions. However, unlike Wang et al. (2017) they do not manually annotate the opinion expressions.",
  "As Wang et al. (2017), they use opinion expressions annotations for a joint modelling of opinion targets and expressions. However, unlike Wang et al. (2017) they do not manually annotate the opinion expressions. Instead they manually add sentiment lexicons and rules based on dependency parsing in order to \ufb01nd the opinion words required to train their system. Using this hand-engineered system, they report state of the 4",
  "<sentence id=\"1016296:4\"> <text>Chow fun was dry; pork shu mai was more than usually greasy and had to share a table with loud and rude family</text> <Opinions> <Opinion target=\"Chow fun\" category=\"FOOD#QUALITY\" polarity=\"negative\" from=\"0\" to= \"8\" pfrom=13 pto=16/> <Opinion target=\"pork shu mai\" category=\"FOOD#QUALITY\" polarity=\"negative\" from=\"18 \" to=\"30\" pfrom=53 pto=59/> <Opinion target=\"NULL\" category=\"AMBIENCE#GENERAL\" polarity=\"negative\" from=\"0\" to= \"0\" pfrom=90 pto=103/> </Opinions> </sentence> Figure 2: Adding opinion expression annotations to Example (1) in the ABSA 2016 training set. art results only for English on the ABSA 2016 dataset. They do not provide evaluation results on the 2014 and 2015 restaurant datasets.",
  "art results only for English on the ABSA 2016 dataset. They do not provide evaluation results on the 2014 and 2015 restaurant datasets. With respect to other languages, the IIT-T team presented systems for 4 out of the 7 languages in ABSA 2016, obtaining the best score for French and Dutch, second in Spanish but with very poor results for English, well below the baseline. The GTI team (`Alvarez-L\u00b4opez et al., 2016) implemented a CRF system using POS, lemmas and bigrams as features. They obtained the best result for Spanish and rather modest results for English. Summarizing, the most successful systems for OTE have been based on supervised approaches with rather elaborate, complex and linguistically inspired features. Poria et al. (2016) obtains best results on the ABSA 2014 data by means of a CNN with word embeddings trained on 5 billion words from Amazon, POS features, manual patterns based on syntactic analysis and SenticNet.",
  "Poria et al. (2016) obtains best results on the ABSA 2014 data by means of a CNN with word embeddings trained on 5 billion words from Amazon, POS features, manual patterns based on syntactic analysis and SenticNet. More recently, the CMLA deep learning model has established new state-of-the-art results for the 2015 dataset, whereas Li and Lam (2017) provide the state of the art for the 2016 benchmark. Thus, there is not currently a multilingual system that obtains competitive results across (at least) several of the languages included in ABSA. As usual, most of the work has been done for English, with the large majority of the previous systems providing results only for one of the three English ABSA editions and without exploring the multilingual aspect. This could be due to the complex and language-speci\ufb01c systems that performed best for English (Poria et al., 2016), or perhaps because the CMLA approach of Wang et al. (2017) would require, in addition to the opinion targets, the gold standard annotations of the opinion expressions for each of the 6 languages other than English in the ABSA datasets.",
  "(2017) would require, in addition to the opinion targets, the gold standard annotations of the opinion expressions for each of the 6 languages other than English in the ABSA datasets. 3 Methodology The work presented in this research note requires the following resources: (i) Aspect Based Sen- timent Analysis (ABSA) data for training and testing; (ii) large unlabelled corpora to obtain semantic distributional features from clustering lexicons; and (iii) a sequence labelling system. In this section we will describe each of the resources used. 3.1 ABSA Datasets Table 1 shows the ABSA datasets from the restaurants domain for English, Spanish, French, Dutch, Russian and Turkish. From left to right each row displays the number of tokens, number of targets 5",
  "Language ABSA No. of Tokens and Opinion Targets Train Test Token B-target I-target Token B-target I-target en 2014 47028 3687 1457 12606 1134 524 en 2015 18488 1199 538 10412 542 264 en 2016 28900 1743 797 9952 612 274 es 2016 35847 1858 742 13179 713 173 fr 2016 26777 1641 443 11646 650 239 nl 2016 24788 1231 331 7606 373 81 ru 2016 51509 3078 953 16999 952 372 tr 2016 12406 1374 516 1316 145 61 Table 1: ABSA SemEval 2014-2016 datasets for the restaurant domain. B-target indicates the number of opinion targets in each set; I-target refers to the number of multiword targets. and the number of multiword targets for each training and test set.",
  "B-target indicates the number of opinion targets in each set; I-target refers to the number of multiword targets. and the number of multiword targets for each training and test set. For English, it should be noted that the size of the 2015 set is less than half with respect to the 2014 dataset in terms of tokens, and only one third in number of targets. The French, Spanish and Dutch datasets are quite similar in terms of tokens although the number of targets in the Dutch dataset is comparatively smaller, possibly due to the tendency to construct compound terms in that language. The Russian dataset is the largest whereas the Turkish set is by far the smallest one. Additionally, we think it is also interesting to note the low number of targets that are multi- words. To provide a couple of examples, for Spanish only the %35.59 of the targets are multiwords whereas for Dutch the percentage goes down to %25.68.",
  "Additionally, we think it is also interesting to note the low number of targets that are multi- words. To provide a couple of examples, for Spanish only the %35.59 of the targets are multiwords whereas for Dutch the percentage goes down to %25.68. If we compare these numbers with the CoNLL 2002 data for Named Entity Recognition (NER), a classic sequence labelling task, we \ufb01nd that in the ABSA data there is less than half the number of multiword targets than the number of multiword entities that can be found in the CoNLL Spanish and Dutch data (%35.59 vs %74.33 for Spanish and %25.68 vs %44.96 for Dutch). 3.2 Unlabelled Corpora Apart from the manually annotated data, we also leveraged large, publicly available, unlabelled data to train the clusters: (i) Brown 1000 clusters and (ii) Clark and Word2vec clusters in the 100-800 range. In order to induce clusters from the restaurant domain we used the Yelp Academic Dataset2, from which three versions were created. First, the full dataset, containing 225M tokens.",
  "In order to induce clusters from the restaurant domain we used the Yelp Academic Dataset2, from which three versions were created. First, the full dataset, containing 225M tokens. Second, a subset consisting of \ufb01ltering out those categories that do not correspond directly to food related reviews (Kiritchenko et al., 2014). Thus, out of the 720 categories contained in the Yelp Academic Dataset, we kept the reviews from 173 of them. This Yelp food dataset contained 117M tokens in 997,721 reviews. Finally, we removed two more categories (Hotels and Hotels & Travel) from the Yelp food dataset to create the Yelp food-hotels subset containing around 102M tokens. For the rest of the languages we used their corresponding Wikipedia dumps. The pre-processing and tokenization is performed with the IXA pipes tools (Agerri et al., 2014). The number of words used for each dataset, language and cluster type are described in Table 2.",
  "For the rest of the languages we used their corresponding Wikipedia dumps. The pre-processing and tokenization is performed with the IXA pipes tools (Agerri et al., 2014). The number of words used for each dataset, language and cluster type are described in Table 2. For example, the \ufb01rst row reads \u201cYelp Academic Dataset containing 225M words was used; after pre-processing, 156M words were taken to induce Brown clusters, whereas Clark and Word2vec clusters were trained on the whole corpus\u201d. As explained in Agerri and Rigau (2016), we pre- 2http://www.yelp.com/dataset challenge 6",
  "million words in corpus million words for training Brown Clark Word2vec en Yelp Academic Dataset 225 156 225 225 Yelp food 117 82 117 117 Yelp food-hotels 102 73 102 102 Wikipedia (20141208) 1700 790 790 1700 es Wikipedia (20140810) 428 246 246 428 fr Wikipedia (20140804) 547 280 280 547 nl Wikipedia (20140804) 235 128 128 235 ru Wikipedia (20140727) 338 158 158 338 tr Wikipedia (20140806) 48 33 48 48 Table 2: Unlabeled corpora to induce clusters. For each corpus and cluster type the number of words (in millions) is speci\ufb01ed. Average training times: depending on the number of words, Brown clusters training time required between 5h and 48h. Word2vec required 1-4 hours whereas Clark clusters training lasted between 5 hours and 10 days. process the corpus before training Brown clusters, resulting in a smaller dataset than the original.",
  "Word2vec required 1-4 hours whereas Clark clusters training lasted between 5 hours and 10 days. process the corpus before training Brown clusters, resulting in a smaller dataset than the original. Additionally, due to e\ufb03ciency reasons, when the corpus is too large we use the pre-processed version to induce the Clark clusters. 3.3 System We use the sequence labeller implemented within IXA pipes (Agerri and Rigau, 2016). It learns supervised models based on the Perceptron algorithm (Collins, 2002). To avoid duplication of e\ufb00orts, it uses the Apache OpenNLP project implementation3 customized with its own features. By design, the sequence labeller aims to establish a simple and shallow feature set, avoiding any linguistic motivated features, with the objective of removing any reliance on costly extra gold annotations and/or cascading errors across annotations.",
  "By design, the sequence labeller aims to establish a simple and shallow feature set, avoiding any linguistic motivated features, with the objective of removing any reliance on costly extra gold annotations and/or cascading errors across annotations. The system consists of: (i) Local, shallow features based mostly on orthographic, word shape and n-gram features plus their context; and (ii) three types of simple clustering features, based on unigram matching: (i) Brown (Brown et al., 1992) clusters, taking the 4th, 8th, 12th and 20th node in the path; (ii) Clark (Clark, 2003) clusters and, (iii) Word2vec (Mikolov et al., 2013) clusters, based on K-means applied over the extracted word vectors using the skip-gram algorithm. Figure 3: Unigram matching in clustering features. 3http://opennlp.apache.org/ 7",
  "The clustering features look for the cluster class of the incoming token in one or more of the clustering lexicons induced following the three methods listed above. If found, then the class is added as feature (\u201cnot found\u201d otherwise). As we work on a 5 token window, for each token and clustering lexicon at least 5 features are generated. For Brown, the number of features generated depend on the number of nodes found in the path for each token and clustering lexicon used. Figure 3 depicts how our system relates, via clusters, unseen words with those words that have been seen as targets during the training process. Thus, the tokens \u2018french-onions\u2019 and \u2018salmon\u2019 would be annotated as opinion targets because they occur in the same clusters as seen words which in the training data are labeled as targets. The word representation features are combined and stacked using the clustering lexicons induced over the di\ufb00erent data sources listed in Table 2.",
  "The word representation features are combined and stacked using the clustering lexicons induced over the di\ufb00erent data sources listed in Table 2. In other words, stacking means adding various clustering features of the same type obtained from di\ufb00erent data sources (for example, using clusters trained on Yelp and on Wikipedia); combining refers to combining di\ufb00erent types of clustering features obtained from the same data source (e.g., using features from Brown and Clark clustering lexicons). To choose the best combination of clustering features we tried, via 5-fold cross validation on the training set, every possible permutation of the available Clark and Word2vec clustering lexicons obtained from the data sources. Once the best combination of Clark and Word2vec clustering lexicons per data source was found, we tried to combine them with the Brown clusters. The result is a rather simple but very competitive system that has proven to be highly successful in the most popular Named Entity Recognition and Classi\ufb01cation (NER) benchmarks, both in out- of-domain and in-domain evaluations. Furthermore, it was demonstrated that the system also performed robustly across languages without any language-speci\ufb01c tuning.",
  "Furthermore, it was demonstrated that the system also performed robustly across languages without any language-speci\ufb01c tuning. Details of the system\u2019s implementation, including detailed description of the local and clustering features, can be found in Agerri and Rigau (2016)4, including a section on how to combine the clustering features. A preliminary version of this system (San Vicente et al., 2015) was the winner of the OTE sub-task in the ABSA 2015 edition (English only). In the next section we show that this system obtains state-of-the-art results not only across domains and languages for NER, but also for other tasks such as Opinion Target Extraction. The results reported are obtained using the o\ufb03cial ABSA evaluation scripts (Pontiki et al., 2014, 2015, 2016). 4 Experimental Results In this section we report on the experiments performed using the system and data described above. First we will present the English results for the three ABSA editions as well as a comparison with previous work.",
  "4 Experimental Results In this section we report on the experiments performed using the system and data described above. First we will present the English results for the three ABSA editions as well as a comparison with previous work. After that we will do the same for 5 additional languages included in the ABSA 2016 edition: Dutch, French, Russian, Spanish and Turkish. The local and clustering features, as described in Section 3.3, are the same for every language and evaluation setting. The only change is the clustering lexicons used for the di\ufb00erent languages. As stated in section 3.3, the best cluster combination is chosen via 5-fold cross validation (CV) on the training data. We \ufb01rst try every permutation with the Clark and Word2vec clusters. Once the best combination is obtained, we then try with the Brown clusters obtaining thus the \ufb01nal model for each language and dataset. 4.1 English Table 3 provides detailed results on the Opinion Target Extraction (OTE) task for English. We show in bold our best model (ALL) chosen via 5-fold CV on the training data.",
  "4.1 English Table 3 provides detailed results on the Opinion Target Extraction (OTE) task for English. We show in bold our best model (ALL) chosen via 5-fold CV on the training data. Moreover, we also 4Table 3 and pages 68-71 8",
  "show the results of the best models using only one type of clustering feature, namely, the best Brown, Clark and Word2vec models, respectively. The \ufb01rst noteworthy issue is that the same model obtains the best results on the three English datasets. Second, it is also interesting to note the huge gains obtained by the clustering features, between 6-7 points in F1 score across the three ABSA datasets. Third, the results show that the combination of clustering features induced from di\ufb00erent data sources is crucial. Fourth, the clustering features improve the recall by 12-15 points in the 2015 and 2016 data, and around 7 points for 2014. Finally, while in 2014 the precision also increases, in the 2015 setting it degrades almost by 4 points in F1 score.",
  "Finally, while in 2014 the precision also increases, in the 2015 setting it degrades almost by 4 points in F1 score. 2014 2015 2016 Features P R F1 P R F1 P R F1 Local (L) 81.84 74.69 78.10 76.82 54.43 63.71 74.41 61.76 67.50 L + BY 77.84 84.57 81.07 71.73 63.65 67.45 74.49 71.08 72.74 L + CYF100-CYR200 82.91 84.30 83.60 73.25 61.62 66.93 74.12 72.06 73.07 L + W2VW400 76.82 82.10 79.37 74.42 59.04 65.84 73.04 65.52 69.08 L + ALL 81.15 87.30 84.11 72.90 69.00 70.90 73.33 73.69 73.51 Table 3: ABSA SemEval 2014-2016 English results.",
  "BY: Brown Yelp 1000 classes; CYF100- CYR200: Clark Yelp Food 100 classes and Clark Yelp Reviews 200 classes; W2VW400: Word2vec Wikipedia 400 classes; ALL: BY+CYF100-CYR200+W2VW400. Table 4 compares our results with previous work. MIN refers to the multi-task learning frame- work consisting of two LSTMs equipped with extended memories and neural memory operations with manually developed rules for detecting opinion expressions (Li and Lam, 2017). CNN- SenticNet is the 7 layer CNN with Amazon word embeddings, POS, linguistic rules based on syntax patterns and SenticNet (Poria et al., 2016). LSTM is a Long Short Term Memory neural network built on top of word embeddings as proposed by Liu et al. (2015). WDEmb (Yin et al., 2016) uses word and dependency path, linear context and dependency context embedding features the input to a CRF.",
  "LSTM is a Long Short Term Memory neural network built on top of word embeddings as proposed by Liu et al. (2015). WDEmb (Yin et al., 2016) uses word and dependency path, linear context and dependency context embedding features the input to a CRF. RNCRF is a joint model with CRF and a recursive neural network whereas CMLA is the Coupled Multilayer Attentions model described in section 2.1, both systems proposed by Wang et al. (2017). DLIREC-NLANGP is the winning system at ABSA 2014 and 2016 (Toh and Wang, 2014; Toh and Su, 2015, 2016) while the penultimate row refers to our own system for all the three benchmarks (details in Table 3). System ABSA 2014 ABSA 2015 ABSA 2016 MIN\u2217(Li and Lam, 2017) - - 73.44 CNN-SenticNet (Poria et al., 2016) 86.20 - - CNN-SenticNet\u2217(Poria et al., 2016) 87.",
  "2017) - - 73.44 CNN-SenticNet (Poria et al., 2016) 86.20 - - CNN-SenticNet\u2217(Poria et al., 2016) 87.17 - - LSTM (Liu et al., 2015) 81.15 64.30 - WDEmb (Yin et al., 2016) 84.31 69.12 - WDEmb\u2217(Yin et al., 2016) 84.97 69.73 - RNCRF (Wang et al., 2017) 84.05 67.06 - RNCRF\u2217(Wang et al., 2017) 85.29 70.73 - DLIREC-NLANGP (Toh et al., 2014-2016) 84.01 67.11 72.34 BY+CYF100-CYR200+W2VW400 84.11 70.90 73.51 Baseline 47.16 48.06 44.",
  ", 2014-2016) 84.01 67.11 72.34 BY+CYF100-CYR200+W2VW400 84.11 70.90 73.51 Baseline 47.16 48.06 44.07 Table 4: ABSA SemEval 2014-2016: Comparison of English results in terms of F1 scores; \u2217refers to models enriched with human-engineered linguistic features. 9",
  "The results of Table 4 show that our system, despite its simplicity, is highly competitive, obtaining the best results on the 2015 and 2016 datasets and a competitive performance on the 2014 benchmark. In particular, we outperform much more complex and language-speci\ufb01c approaches tuned via language-speci\ufb01c features, such as that of DLIREC-NLANGP. Furthermore, while the deep learning approaches (enriched with human-engineered linguistic features) obtain comparable or better results on the 2014 data, that is not the case for the 2015 and 2016 benchmarks, where our system outperforms also the MIN and CMLA models (systems which require manually added rules and gold-standard opinion expressions to obtain their best results, as explained in section 2.1). In this sense, this means that our system obtains better results than MIN and CMLA by learning the targets independently instead of jointly learning the target and those expressions that convey the polarity of the opinion, namely, the opinion expression.",
  "In this sense, this means that our system obtains better results than MIN and CMLA by learning the targets independently instead of jointly learning the target and those expressions that convey the polarity of the opinion, namely, the opinion expression. There seems to be also a correlation between the size of the datasets and performance, given that the results on the 2014 data are much higher than those obtained using the 2015 and 2016 datasets. This might be due to the fact that the 2014 training set is substantially larger, as detailed in Table 1. In fact, the smaller datasets seem to a\ufb00ect more the deep learning approaches (LSTM, WDEmb, RNCRF) where only the MIN and CMLA models obtain similar results to ours, albeit using manually added language-speci\ufb01c annotations. Finally, it would have been interesting to compare MIN, CNN-SenticNet and CMLA with our system on the three ABSA benchmarks, but their systems are not publicly available. 4.2 Multilingual We trained our system for 5 other languages on the ABSA 2016 datasets, using the same strategy as for English.",
  "4.2 Multilingual We trained our system for 5 other languages on the ABSA 2016 datasets, using the same strategy as for English. We choose the best Clark-Word2vec combination (with and without Brown clusters) via 5-cross validation on the training data. The features are exactly the same as those used for English, the only change is the data on which the clusters are trained. Table 5 reports on the detailed results obtained for each of the languages. In bold we show the best model chosen via 5-fold CV. Moreover, we also show the best models using only one of each of the clustering features. The \ufb01rst di\ufb00erence with respect to the English results is that the Brown clustering features are, in three out of \ufb01ve settings, detrimental to performance. Second, that combining clustering features is only bene\ufb01cial for Spanish. Third, the overall results are in general lower than those obtained in the 2016 English data.",
  "Second, that combining clustering features is only bene\ufb01cial for Spanish. Third, the overall results are in general lower than those obtained in the 2016 English data. Finally, the di\ufb00erence between the best results and the results using the Local features is lower than for English, even though the Local results are similar to those obtained with the English datasets (except for Turkish, but this is due to the signi\ufb01cantly smaller size of the data, as shown in Table 1). We believe that all these four issues are caused, at least partially, by the lack of domain-speci\ufb01c clustering features used for the multilingual experiments. In other words, while for the English experiments we leveraged the Yelp dataset to train the clustering algorithms, in the multilingual setting we \ufb01rst tried with already available clusters induced from the Wikipedia. Thus, it is to be expected that the gains obtained by clustering features obtained from domain-speci\ufb01c data such as Yelp would be superior to those achieved by the clusters trained on out-of-domain data. In spite of this, Table 6 shows that our system outperforms the best previous approaches across the \ufb01ve languages.",
  "In spite of this, Table 6 shows that our system outperforms the best previous approaches across the \ufb01ve languages. In some cases, such as Turkish and Russian, the best previous scores were the baselines provided by the ABSA organizers, but for Dutch, French and Spanish our system is signi\ufb01cantly better than current state-of-the-art. In particular, and despite using the same system for every language, we improve over GTI\u2019s submission, which implemented a CRF system with linguistic features speci\ufb01c to Spanish (`Alvarez-L\u00b4opez et al., 2016). 10",
  "Language Features Precision Recall F1 es Local (L) 79.17 59.19 67.74 L + BW 67.96 63.67 65.75 L + CW600 73.22 64.80 68.75 L + W2VW300 75.50 63.53 69.00 L + CW600 + W2VW300 75.36 65.22 69.92 fr Local (L) 66.92 66.41 66.67 L + BW 63.39 72.46 67.62 L + CW100 69.94 69.08 69.50 L + W2VW100 66.52 68.77 67.62 nl Local (L) 73.14 55.50 63.11 L + BW 68.59 57.37 62.48 L + CW100 66.94 65.15 66.03 L + W2VW400 68.27 64.61 66.39 ru Local (L) 64.87 61.87 63.33 L + BW 61.32 64.",
  "48 L + CW100 66.94 65.15 66.03 L + W2VW400 68.27 64.61 66.39 ru Local (L) 64.87 61.87 63.33 L + BW 61.32 64.60 62.92 L + CW500 64.21 66.91 65.53 L + W2VW700 64.41 64.81 64.61 tr Local (L) 56.82 51.72 54.15 L + BW 62.69 57.93 60.22 L + CW200 58.28 60.69 59.46 L + W2VW300 59.09 53.79 56.32 Table 5: ABSA SemEval 2016 multilingual results. 5 Discussion and Error Analysis Considering the simplicity of our approach, we obtain best results for 6 languages and 7 di\ufb00erent settings in the Opinion Target Extraction (OTE) benchmark for the restaurant domain using the ABSA 2014-2016 datasets.",
  "5 Discussion and Error Analysis Considering the simplicity of our approach, we obtain best results for 6 languages and 7 di\ufb00erent settings in the Opinion Target Extraction (OTE) benchmark for the restaurant domain using the ABSA 2014-2016 datasets. These results are obtained without linguistic or manually-engineered features, relying on in- jecting external knowledge from the combination of clustering features to obtain a robust system across languages, outperforming other more complex and language-speci\ufb01c systems. Furthermore, the feature set used is the same for every setting, reducing human intervention to a minimum and establishing a clear methodology for a fast and easy creation of competitive OTE multilingual taggers. The results also con\ufb01rm the behaviour of these clustering algorithms to provide features for sequence labelling tasks such as OTE and Named Entity Recognition (NER), as previously dis- cussed in Agerri and Rigau (2016). Thus, in every evaluation setting the best results using Brown clusters as features were obtained when data close to the application domain and text genre, even if relatively small, was used to train the Brown algorithm.",
  "Thus, in every evaluation setting the best results using Brown clusters as features were obtained when data close to the application domain and text genre, even if relatively small, was used to train the Brown algorithm. This can be clearly seen if we com- pare the English with the multilingual results. For English, the models including Brown clusters improve the Local features over 3-5 points in F1 score, whereas for Spanish, Dutch and Russian, they worsen performance. The reason is that for English the Yelp dataset is used whereas for the rest of languages the clusters are induced using the Wikipedia, e\ufb00ectively an out-of-domain corpus. The exception is Turkish, for which a 6 point gain in F1 score is obtained, but we believe that is probably due to the small size of the training data used for training the Local model. In contrast, Word2vec clusters clearly bene\ufb01t from larger amounts of data, as illustrated by the best English Word2vec model being the one trained using the Wikipedia, and not the Yelp dataset, which is closer to the application domain. Finally, the Clark algorithm seems to be the 11",
  "Language System F1 es GTI 68.51 L + CW600 + W2VW300 69.92 Baseline 51.91 fr IIT-T 66.67 L + CW100 69.50 Baseline 45.45 nl IIT-T 56.99 L + W2VW400 66.39 Baseline 50.64 ru Danii. 33.47 L + CW500 65.53 Baseline 49.31 tr L + BW 60.22 Baseline 41.86 Table 6: ABSA SemEval 2016: Comparison of multilingual results in terms of F1 scores. most versatile as it consistently outperforms the other two clustering methods in 4 out of the 8 evaluation settings presented. Summarizing: (i) Brown clusters perform better when leveraged from source data close to the application domain, even if small in size; (ii) Clark clusters are the most robust of the three with respect to the size and domain of the data used; and (iii) for Word2vec size is the crucial factor. The larger the source data the better the performance.",
  "The larger the source data the better the performance. Thus, instead of choosing over one clustering type or the other, our system provides a method to e\ufb00ectively combining them, depending on the data sources available, to obtain robust and language independent sequence labelling systems. Finally, results show that our models are particularly competitive when the amount of training data available is small, allowing us to compete with more complex systems including also manually- engineered features, as shown especially by the English results on the 2015 and 2016 data. 5.1 Error Analysis We will now discuss the shortcomings and most common errors performed by our system for the OTE task. By looking at the overall results in terms of precision and recall, it is possible to see the following patterns: With respect to the Local models, precision is consistently better than recall or, in other words, the coverage of the Local models is quite low. Tables 3 and 5 show that adding clustering features to the Local models allows to improve the recall for every evaluation setting, although with di\ufb00erent outcomes. Overall, precision su\ufb00ers, except for French5.",
  "Tables 3 and 5 show that adding clustering features to the Local models allows to improve the recall for every evaluation setting, although with di\ufb00erent outcomes. Overall, precision su\ufb00ers, except for French5. Furthermore, in three cases (English 2014, 2016 and Russian) precision is lower than recall, whereas the remaining 5 evaluations show that, despite large improvements in F1 score, most errors in our system are caused by false negatives, as it can be seen in Table 7. Table 8 displays the top 5 most common false positives and false negative errors for English, Spanish and French6. By inspecting our system\u2019s output, and both the test and training sets, we found out that there were three main sources of errors: (a) errors caused by ambiguity in the use of certain source forms that may or may not refer to an opinion target; (b) span errors, where the 5It also goes up for Turkish, but as already commented, we believe that due to the small size of the Turkish training set, clustering features allow to improve both precision and recall.",
  "6According to the authors\u2019 knowledge of languages to comment on speci\ufb01c examples from the data. 12",
  "2014 2015 2016 Error type en en en es fr nl ru tr FP 230 151 189 165 194 117 390 62 FN 143 169 163 248 202 132 312 65 Table 7: False Positives and Negatives for every ABSA 2014-2016 setting. target has only been partially annotated; and (c) unknown targets, which the system was unable to annotate by generalizing on the training data or clusters.",
  "target has only been partially annotated; and (c) unknown targets, which the system was unable to annotate by generalizing on the training data or clusters. 2014 2015 2016 en en en es fr FP place 21 place 16 place 16 comida 11 restaurant 13 money 6 food 6 food 16 restaurante 10 cuisine 9 spot 4 waitress 4 restaurant 11 atenci\u00b4on 7 terrasse 8 pizza 3 chicken 4 service 7 platos 6 repas 7 sushi 3 salmon 3 wait 3 servicio 4 plats 6 FN place 4 restaurant 8 place 7 restaurante 12 restaurant 5 food 3 place 7 sushi 3 platos 7 cuisine 5 waiting 2 food 5 restaurant 3 trato 6 carte 5 taste 2 Casa La Femme 4 Ray\u2019s 3 comida 6 plats 4 selection 2 The Four Seasons 3 menu 3 carta 6 table 3 Table 8: Top \ufb01ve false positive (FP) and negative (FN) errors for English, Spanish and French.",
  "With respect to type (a), it is useful to look at the most common errors for all three languages, namely, \u2018place\u2019, \u2018food\u2019 and \u2018restaurant\u2019, which are also among the top 5 most frequent targets in the gold standard sets. By looking at Examples (1-3) we would say that in all three cases \u2018place\u2019 should be annotated as opinion target. However, (2) is a false positive (FP), (3) is a false negative (FN) and (1) is an example from the training set in which \u2018place\u2019 is annotated as target. This is the case with many instances of \u2018place\u2019 for which there seems to be some inconsistency in the actual annotation of the training and test set examples7. Example (1): Avoid this place! Example (2): this place is a keeper! Example (3): it is great place to watch sporting events. For other frequent type (a) errors, ambiguity is the main problem.",
  "Example (1): Avoid this place! Example (2): this place is a keeper! Example (3): it is great place to watch sporting events. For other frequent type (a) errors, ambiguity is the main problem. Thus, in Spanish the use of \u2018comida\u20198 and \u2018restaurante\u20199 is highly ambiguous and causes many FPs and FNs because sometimes it is actually an opinion target whereas in many other other cases it is just referring to the meal or the restaurant themselves without expressing any opinion about them. The same phenomenon occurs for \u201cfood\u201d and \u201crestaurant\u201d in English and for \u2018cuisine\u2019 and \u2018restaurant\u2019 in French. Span type (b) errors are typically caused by long opinion targets such as \u201c\ufb01let mignon on top of spinach and mashed potatoes\u201d for which our system annotates \u201c\ufb01let\u201d and \u201cspinach\u201d as separate targets, or \u201cchicken curry and chicken tikka masala\u201d which is wrongly tagged as one target.",
  "These cases are di\ufb03cult because on the surface they look similar but the \ufb01rst one refers to one dish only, hence one target, whereas the second one refers to two separate dishes for which two di\ufb00erent 7Interannotator agreement (91% F1) was only reported for a small subset of the Spanish data. 8In English: \u201cfood\u201d or \u201cmeal\u201d, depending on the context. 9In English: \u201crestaurant\u201d. 13",
  "opinion targets should be annotated. Of course, these cases are particularly hurtful because they count as both FP and FN. Finally, type (c) errors are usually caused by lack of generalization of our system to deal with unknown targets. Example (4-7) contain various mentions to the \u201cRay\u2019s\u201d restaurant, which is in the top 5 errors for the English 2016 test set. Example (4): After 12 years in Seattle Ray\u2019s rates as the place we always go back to. Example (5): We were only in Seattle for one night and I\u2019m so glad we picked Rays for dinner! Example (6): I love Dungeness crabs and at Ray\u2019s you can get them served in about 6 di\ufb00erent ways! Example (7): Imagine my happy surprise upon \ufb01nding that the views are only the third-best thing about Ray\u2019s!",
  "Example (6): I love Dungeness crabs and at Ray\u2019s you can get them served in about 6 di\ufb00erent ways! Example (7): Imagine my happy surprise upon \ufb01nding that the views are only the third-best thing about Ray\u2019s! Example (8): Ray\u2019s is something of a Seattle institution Examples (4), (5) and (7) are FNs, (6) is a FP caused by wrongly identifying the target as \u201cRay\u2019s you\u201d, whereas (8) is not event annotated in the gold standard or by our system, although it should had been. 6 Concluding Remarks In this research note we provide additional empirical experimentation to Agerri and Rigau (2016), reporting best results for Opinion Target Extraction for 6 languages and 7 datasets using the same set of simple, shallow and language independent features. Furthermore, the results provide some interesting insights with respect to the use of clusters to inject external knowledge via semi- supervised features. First, Brown clusters are particularly bene\ufb01cial when trained on domain-related data.",
  "Furthermore, the results provide some interesting insights with respect to the use of clusters to inject external knowledge via semi- supervised features. First, Brown clusters are particularly bene\ufb01cial when trained on domain-related data. This seems to be the case in the multilingual setting, where the Brown clusters (trained on out-of- domain Wikipedia data) worsen the system\u2019s performance for every language except for Turkish. Second, the results also show that Clark and Word2vec improve results in general, even if induced on out-of-domain data. Thirdly, for best performance it is convenient to combine clusters obtained from diverse data sources, both from in- and out-of-domain corpora. Finally, the results indicate that, even when the amount of training data is small, such as in the 2015 and 2016 English benchmarks, our system\u2019s performance remains competitive thanks to the combination of clustering features. This, together with the lack of linguistic features, facilitates the easy and fast development of systems for new domains or languages. These considerations thus con\ufb01rm the hypotheses stated in Agerri and Rigau (2016) with respect to the use of clustering features to obtain robust sequence taggers across languages and tasks.",
  "These considerations thus con\ufb01rm the hypotheses stated in Agerri and Rigau (2016) with respect to the use of clustering features to obtain robust sequence taggers across languages and tasks. The system and models for every language and dataset are available as part of the ixa-pipe- opinion module for public use and reproducibility of results.10 Acknowledgments First, we would like to thank the anonymous reviewers for their comments to improve the paper. We would also like to thank I\u02dcnaki San Vicente for his help obtaining the Yelp data. This work has been supported by the Spanish Ministry of Economy and Competitiveness (MINECO/FEDER, 10https://github.com/ixa-ehu/ixa-pipe-opinion 14",
  "UE), under the projects TUNER (TIN2015-65308-C5-1-R) and CROSSTEXT (TIN2015-72646- EXP). References Agerri, R., Bermudez, J., and Rigau, G. (2014). IXA pipeline: E\ufb03cient and ready to use multilin- gual NLP tools. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC\u201914), pages 3823\u20133828, Reykjavik, Iceland. Agerri, R. and Rigau, G. (2016). Robust multilingual named entity recognition with shallow semi-supervised features. Arti\ufb01cial Intelligence, 238:63\u201382. `Alvarez-L\u00b4opez, T., Juncal-Mart\u00b4\u0131nez, J., Fern\u00b4andez-Gavilanes, M., Costa-Montenegro, E., and Gonz\u00b4alez-Casta\u02dcno, F. J. (2016).",
  "`Alvarez-L\u00b4opez, T., Juncal-Mart\u00b4\u0131nez, J., Fern\u00b4andez-Gavilanes, M., Costa-Montenegro, E., and Gonz\u00b4alez-Casta\u02dcno, F. J. (2016). Gti at semeval-2016 task 5: Svm and crf for aspect detec- tion and unsupervised aspect-based sentiment analysis. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 306\u2013311. Association for Computa- tional Linguistics. Brown, P. F., Desouza, P. V., Mercer, R. L., Pietra, V. J. D., and Lai, J. C. (1992). Class-based n-gram models of natural language. Computational linguistics, 18(4):467\u2013479. Cambria, E., Olsher, D., and Rajagopal, D. (2014). Senticnet 3: a common and common-sense knowledge base for cognition-driven sentiment analysis.",
  "Computational linguistics, 18(4):467\u2013479. Cambria, E., Olsher, D., and Rajagopal, D. (2014). Senticnet 3: a common and common-sense knowledge base for cognition-driven sentiment analysis. In Twenty-eighth AAAI conference on arti\ufb01cial intelligence. Clark, A. (2003). Combining distributional and morphological information for part of speech induction. In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics-Volume 1, pages 59\u201366. Association for Computational Linguistics. Collins, M. (2002). Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 1\u20138. Fellbaum, C. and Miller, G., editors (1998). Wordnet: An Electronic Lexical Database. MIT Press, Cambridge (MA). Hu, M. and Liu, B. (2004). Mining and summarizing customer reviews.",
  "Fellbaum, C. and Miller, G., editors (1998). Wordnet: An Electronic Lexical Database. MIT Press, Cambridge (MA). Hu, M. and Liu, B. (2004). Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168\u2013177. Jakob, N. and Gurevych, I. (2010). Extracting opinion targets in a single-and cross-domain setting with conditional random \ufb01elds. In Proceedings of the 2010 conference on empirical methods in natural language processing, pages 1035\u20131045. Association for Computational Linguistics. Jebbara, S. and Cimiano, P. (2016). Aspect-Based Relational Sentiment Analysis Using a Stacked Neural Network Architecture.",
  "Association for Computational Linguistics. Jebbara, S. and Cimiano, P. (2016). Aspect-Based Relational Sentiment Analysis Using a Stacked Neural Network Architecture. In ECAI 2016 - 22nd European Conference on Arti\ufb01cial In- telligence, 29 August-2 September 2016, The Hague, The Netherlands - Including Prestigious Applications of Arti\ufb01cial Intelligence (PAIS 2016), pages 1123\u2014-1131. Jin, W., Ho, H. H., and Srihari, R. K. (2009). A novel lexicalized hmm-based learning framework for web opinion mining. In Proceedings of the 26th annual international conference on machine learning, pages 465\u2013472. 15",
  "Kim, S.-M. and Hovy, E. (2006). Extracting opinions, opinion holders, and topics expressed in online news media text. In Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 1\u20138. Association for Computational Linguistics. Kiritchenko, S., Zhu, X., Cherry, C., and Mohammad, S. (2014). NRC-canada-2014: Detecting aspects and sentiment in customer reviews. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 437\u2013442, Dublin, Ireland. Association for Com- putational Linguistics and Dublin City University. Li, F., Han, C., Huang, M., Zhu, X., Xia, Y.-J., Zhang, S., and Yu, H. (2010). Structure-aware review mining and summarization. In Proceedings of the 23rd international conference on com- putational linguistics, pages 653\u2013661. Association for Computational Linguistics. Li, X. and Lam, W. (2017). Deep multi-task learning for aspect term extraction with memory interaction.",
  "In Proceedings of the 23rd international conference on com- putational linguistics, pages 653\u2013661. Association for Computational Linguistics. Li, X. and Lam, W. (2017). Deep multi-task learning for aspect term extraction with memory interaction. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2886\u20132892. Liu, B. (2012). Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies, 5(1):1\u2013167. Liu, P., Joty, S., and Meng, H. (2015). Fine-grained opinion mining with recurrent neural networks and word embeddings. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1433\u20131443. Association for Computational Linguistics. McAuley, J. and Leskovec, J. (2013). Hidden factors and hidden topics: understanding rating di- mensions with review text. In Proceedings of the 7th ACM conference on Recommender systems, pages 165\u2013172. ACM.",
  "McAuley, J. and Leskovec, J. (2013). Hidden factors and hidden topics: understanding rating di- mensions with review text. In Proceedings of the 7th ACM conference on Recommender systems, pages 165\u2013172. ACM. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. (2013). Distributed represen- tations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111\u20133119. Pang, B. and Lee, L. (2008). Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1\u2013135. Pang, B., Lee, L., and Vaithyanathan, S. (2002). Thumbs up? sentiment classi\ufb01cation using machine learning techniques. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 79\u201386. Association for Computational Linguistics.",
  "(2002). Thumbs up? sentiment classi\ufb01cation using machine learning techniques. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 79\u201386. Association for Computational Linguistics. Pontiki, M., Galanis, D., Papageorgiou, H., Androutsopoulos, I., Manandhar, S., AL-Smadi, M., Al-Ayyoub, M., Zhao, Y., Qin, B., De Clercq, O., Hoste, V., Apidianaki, M., Tannier, X., Loukachevitch, N., Kotelnikov, E., Bel, N., Jim\u00b4enez-Zafra, S. M., and Eryi\u02d8git, G. (2016). Semeval-2016 task 5: Aspect based sentiment analysis. In Proceedings of the 10th Interna- tional Workshop on Semantic Evaluation (SemEval-2016), pages 19\u201330, San Diego, California. Association for Computational Linguistics.",
  "(2016). Semeval-2016 task 5: Aspect based sentiment analysis. In Proceedings of the 10th Interna- tional Workshop on Semantic Evaluation (SemEval-2016), pages 19\u201330, San Diego, California. Association for Computational Linguistics. Pontiki, M., Galanis, D., Papageorgiou, H., Manandhar, S., and Androutsopoulos, I. (2015). Semeval-2015 task 12: Aspect based sentiment analysis. In Proceedings of the 9th Interna- tional Workshop on Semantic Evaluation (SemEval 2015), pages 486\u2013495, Denver, Colorado. Association for Computational Linguistics. 16",
  "Pontiki, M., Galanis, D., Pavlopoulos, J., Papageorgiou, H., Androutsopoulos, I., and Manandhar, S. (2014). Semeval-2014 task 4: Aspect based sentiment analysis. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27\u201335, Dublin, Ireland. Association for Computational Linguistics and Dublin City University. Popescu, A. M. and Etzioni, O. (2005). Extracting product features and opinions from reviews. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 339\u2013346. Poria, S., Cambria, E., and Gelbukh, A. (2016). Aspect extraction for opinion mining with a deep convolutional neural network. Knowledge-Based Systems, 108:42\u201349. Qiu, G., Liu, B., Bu, J., and Chen, C. (2011). Opinion word expansion and target extraction through double propagation. Computational linguistics, 37(1):9\u201327.",
  "Knowledge-Based Systems, 108:42\u201349. Qiu, G., Liu, B., Bu, J., and Chen, C. (2011). Opinion word expansion and target extraction through double propagation. Computational linguistics, 37(1):9\u201327. San Vicente, I. n., Saralegi, X., and Agerri, R. (2015). Elixa: A modular and \ufb02exible absa platform. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 748\u2013752, Denver, Colorado. Association for Computational Linguistics. Tjong Kim Sang, E. F. (2002). Introduction to the CoNLL-2002 shared task: Language- independent named entity recognition. In Proceedings of CoNLL-2002, pages 155\u2013158. Taipei, Taiwan. Toh, Z. and Su, J. (2015). Nlangp: Supervised machine learning system for aspect category classi\ufb01cation and opinion target extraction.",
  "In Proceedings of CoNLL-2002, pages 155\u2013158. Taipei, Taiwan. Toh, Z. and Su, J. (2015). Nlangp: Supervised machine learning system for aspect category classi\ufb01cation and opinion target extraction. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 496\u2013501. Association for Computational Linguistics. Toh, Z. and Su, J. (2016). Nlangp at semeval-2016 task 5: Improving aspect based sentiment analysis using neural network features. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval 2016), pages 282\u2013288. Toh, Z. and Wang, W. (2014). Dlirec: Aspect term extraction and term polarity classi\ufb01cation system. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 235\u2013240. Wang, W., Pan, S. J., Dahlmeier, D., and Xiao, X. (2017).",
  "In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 235\u2013240. Wang, W., Pan, S. J., Dahlmeier, D., and Xiao, X. (2017). Coupled multi-layer attentions for co-extraction of aspect and opinion terms. In AAAI, pages 3316\u20133322. Yin, Y., Wei, F., Dong, L., Xu, K., Zhang, M., and Zhou, M. (2016). Unsupervised word and dependency path embeddings for aspect term extraction. In Proceedings of the Twenty-Fifth In- ternational Joint Conference on Arti\ufb01cial Intelligence, IJCAI\u201916, pages 2979\u20132985. AAAI Press. Zhuang, L., Jing, F., and Zhu, X.-Y. (2006). Movie review mining and summarization. In Pro- ceedings of the 15th ACM international conference on Information and knowledge management, pages 43\u201350. ACM. 17"
]