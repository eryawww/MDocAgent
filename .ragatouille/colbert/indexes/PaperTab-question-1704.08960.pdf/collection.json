[
  "Neural Word Segmentation with Rich Pretraining Jie Yang\u2217and Yue Zhang\u2217and Fei Dong Singapore University of Technology and Design {jie yang, fei dong}@mymail.sutd.edu.sg yue zhang@sutd.edu.sg Abstract Neural word segmentation research has bene\ufb01ted from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has ex- ploited richer sources of external informa- tion, such as punctuation, automatic seg- mentation and POS. We investigate the ef- fectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most important submod- ule using rich external sources. Results show that such pretraining signi\ufb01cantly improves the model, leading to accura- cies competitive to the best methods on six benchmarks.",
  "Results show that such pretraining signi\ufb01cantly improves the model, leading to accura- cies competitive to the best methods on six benchmarks. 1 Introduction There has been a recent shift of research attention in the word segmentation literature from statisti- cal methods to deep learning (Zheng et al., 2013; Pei et al., 2014; Morita et al., 2015; Chen et al., 2015b; Cai and Zhao, 2016; Zhang et al., 2016b). Neural network models have been exploited due to their strength in non-sparse representation learn- ing and non-linear power in feature combination, which have led to advances in many NLP tasks. So far, neural word segmentors have given compara- ble accuracies to the best statictical models. With respect to non-sparse representation, character embeddings have been exploited as a foundation of neural word segmentors.",
  "So far, neural word segmentors have given compara- ble accuracies to the best statictical models. With respect to non-sparse representation, character embeddings have been exploited as a foundation of neural word segmentors. They serve to reduce sparsity of character ngrams, allowing, for example, \u201c\u732b(cat) \u8eba(lie) \u5728(in) \u5899\u89d2(corner)\u201d to be connected with \u201c\u72d7(dog) \u8e72(sit) \u5728(in) \u5899 \u2217Equal contribution. \u89d2(corner)\u201d (Zheng et al., 2013), which is infeasi- ble by using sparse one-hot character features. In addition to character embeddings, distributed rep- resentations of character bigrams (Mansur et al., 2013; Pei et al., 2014) and words (Morita et al., 2015; Zhang et al., 2016b) have also been shown to improve segmentation accuracies.",
  "With respect to non-linear modeling power, var- ious network structures have been exploited to represent contexts for segmentation disambigua- tion, including multi-layer perceptrons on \ufb01ve- character windows (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a), as well as LSTMs on characters (Chen et al., 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016; Zhang et al., 2016b). For structured learning and inference, CRF has been used for character sequence labelling mod- els (Pei et al., 2014; Chen et al., 2015b) and struc- tural beam search has been used for word-based segmentors (Cai and Zhao, 2016; Zhang et al., 2016b).",
  "Previous research has shown that segmentation accuracies can be improved by pretraining charac- ter and word embeddings over large Chinese texts, which is consistent with \ufb01ndings on other NLP tasks, such as parsing (Andor et al., 2016). Pre- training can be regarded as one way of leveraging external resources to improve accuracies, which is practically highly useful and has become a stan- dard practice in neural NLP. On the other hand, statistical segmentation research has exploited raw texts for semi-supervised learning, by collecting clues from raw texts more thoroughly such as mutual information and punctuation (Li and Sun, 2009; Sun and Xu, 2011), and making use of self- predictions (Wang et al., 2011; Liu and Zhang, 2012). It has also utilised heterogenous annota- tions such as POS (Ng and Low, 2004; Zhang and Clark, 2008) and segmentation under different arXiv:1704.08960v1  [cs.CL]  28 Apr 2017",
  "State Recognized words Partial word Incoming chars Next Action state0 [ ] \u03c6 [\u6211\u53bb\u8fc7\u706b\u8f66\u7ad9\u90a3\u8fb9] SEP state1 [ ] \u6211 [\u53bb\u8fc7\u706b\u8f66\u7ad9\u90a3\u8fb9] SEP state2 [\u6211] \u53bb [\u8fc7\u706b\u8f66\u7ad9\u90a3\u8fb9] SEP state3 [\u6211,\u53bb] \u8fc7 [\u706b\u8f66\u7ad9\u90a3\u8fb9] SEP state4 [\u6211,\u53bb,\u8fc7] \u706b [\u8f66\u7ad9\u90a3\u8fb9] APP state5 [\u6211,\u53bb,\u8fc7] \u706b\u8f66 [\u7ad9\u90a3\u8fb9] APP state6 [\u6211,\u53bb,\u8fc7] \u706b\u8f66\u7ad9 [\u90a3\u8fb9] SEP state7 [\u6211,\u53bb,\u8fc7,\u706b\u8f66\u7ad9] \u90a3 [\u8fb9] APP state8 [\u6211,\u53bb,\u8fc7,\u706b\u8f66\u7ad9] \u90a3\u8fb9 [ ] FIN state9 [\u6211,\u53bb,\u8fc7,\u706b\u8f66\u7ad9,\u90a3\u8fb9] \u03c6 [ ] - - Table 1: A transition based word segmentation example. standards (Jiang et al., 2009).",
  "standards (Jiang et al., 2009). To our knowledge, such rich external information has not been sys- tematically investigated for neural segmentation. We \ufb01ll this gap by investigating rich external pretraining for neural segmentation. Following Cai and Zhao (2016) and Zhang et al. (2016b), we adopt a globally optimised beam-search frame- work for neural structured prediction (Andor et al., 2016; Zhou et al., 2015; Wiseman and Rush, 2016), which allows word information to be mod- elled explicitly. Different from previous work, we make our model conceptually simple and modular, so that the most important sub module, namely a \ufb01ve-character window context, can be pretrained using external data. We adopt a multi-task learn- ing strategy (Collobert et al., 2011), casting each external source of information as a auxiliary clas- si\ufb01cation task, sharing a \ufb01ve-character window network. After pretraining, the character win- dow network is used to initialize the correspond- ing module in our segmentor.",
  "After pretraining, the character win- dow network is used to initialize the correspond- ing module in our segmentor. Results on 6 different benchmarks show that our method outperforms the best statistical and neu- ral segmentation models consistently, giving the best reported results on 5 datasets in different do- mains and genres. Our implementation is based on LibN3L1 (Zhang et al., 2016a). Code and mod- els can be downloaded from http://gitHub. com/jiesutd/RichWordSegmentor 2 Related Work Work on statistical word segmentation dates back to the 1990s (Sproat et al., 1996). State-of-the-art approaches include character sequence labeling models (Xue et al., 2003) using CRFs (Peng et al., 1https://github.com/SUTDNLP/LibN3L 2004; Zhao et al., 2006) and max-margin struc- tured models leveraging word features (Zhang and Clark, 2007; Sun et al., 2009; Sun, 2010).",
  "Semi- supervised methods have been applied to both character-based and word-based models, explor- ing external training data for better segmentation (Sun and Xu, 2011; Wang et al., 2011; Liu and Zhang, 2012; Zhang et al., 2013). Our work be- longs to recent neural word segmentation. To our knowledge, there has been no work in the literature systematically investigating rich external resources for neural word segmentation training. Closest in spirit to our work, Sun and Xu (2011) empirically studied the use of various external re- sources for enhancing a statistical segmentor, in- cluding character mutual information, access va- riety information, punctuation and other statisti- cal information. Their baseline is similar to ours in the sense that both character and word contexts are considered. On the other hand, their model is statistical while ours is neural. Consequently, they integrate external knowledge as features, while we integrate it by shared network parameters. Our re- sults show a similar degree of error reduction com- pared to theirs by using external data.",
  "On the other hand, their model is statistical while ours is neural. Consequently, they integrate external knowledge as features, while we integrate it by shared network parameters. Our re- sults show a similar degree of error reduction com- pared to theirs by using external data. Our model inherits from previous \ufb01ndings on context representations, such as character win- dows (Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a) and LSTMs (Chen et al., 2015b; Xu and Sun, 2016). Similar to Zhang et al. (2016b) and Cai and Zhao (2016), we use word context on top of character context. However, words play a relatively less important role in our model, and we \ufb01nd that word LSTM, which has been used by all previous neural segmentation work, is un- necessary for our model. Our model is conceptu- ally simpler and more modularised compared with",
  "S A hidden layer output \u8f66 \u7ad9 \u90a3 \u8fb9 w-k Recognized words Partial word Incoming chars w-2 \u6211 \u4e4b\u524d \u53bb \u8fc7 \u706b w-1 P c0 c1 . . . cm . . . . . . . . . . . . XW XP XC h Figure 1: Overall model. Zhang et al. (2016b) and Cai and Zhao (2016), allowing a central sub module, namely a \ufb01ve- character context window, to be pretrained. 3 Model Our segmentor works incrementally from left to right, as the example shown in Table 1. At each step, the state consists of a sequence of words that have been fully recognized, denoted as W = [w\u2212k, w\u2212k+1, ..., w\u22121], a current partially recog- nized word P, and a sequence of next incom- ing characters, denoted as C = [c0, c1, ..., cm], as shown in Figure 1.",
  "Given an input sentence, W and P are initialized to [ ] and \u03c6, respectively, and C contains all the input characters. At each step, a decision is made on c0, either appending it as a part of P, or seperating it as the beginning of a new word. The incremental process repeats until C is empty and P is null again (C = [ ], P = \u03c6). Formally, the process can be regarded as a state-transition process, where a state is a tuple S = \u27e8W, P, C\u27e9, and the transition actions include SEP (seperate) and APP (append), as shown by the deduction system in Figure 22. In the \ufb01gure, V denotes the score of a state, given by a neural network model. The score of the initial state (i.e. axiom) is 0, and the score of a non-axiom state is the sum of scores of all incre- mental decisions resulting in the state. Similar to Zhang et al.",
  "The score of the initial state (i.e. axiom) is 0, and the score of a non-axiom state is the sum of scores of all incre- mental decisions resulting in the state. Similar to Zhang et al. (2016b) and Cai and Zhao (2016), our model is a global structural model, using the over- all score to disambiguate states, which correspond to sequences of inter-dependent transition actions. Different from previous work, the structure of 2An end of sentence symbol \u27e8/s\u27e9is added to the input so that the last partial word can be put onto W as a full word before segmentation \ufb01nishes.",
  "Different from previous work, the structure of 2An end of sentence symbol \u27e8/s\u27e9is added to the input so that the last partial word can be put onto W as a full word before segmentation \ufb01nishes. Axiom: S = \u27e8[ ], \u03c6, C\u27e9, V = 0 Goal: S = \u27e8W, \u03c6, [ ]\u27e9, V = Vfinal SEP: S = \u27e8W, P, c0|C\u27e9, V S \u2032 = \u27e8W|P, c0, C\u27e9, V \u2032 = V + Score(S, SEP) APP: S = \u27e8W, P, c0|C\u27e9, V S \u2032 = \u27e8W, P \u2295c0, C\u27e9, V \u2032 = V + Score(S, APP) Figure 2: Deduction system, where \u2295denotes string concatenation. our scoring network is shown in Figure 1. It con- sists of three main layers. On the bottom is a rep- resentation layer, which derives dense representa- tions XW , XP and XC for W, P and C, respec- tively.",
  "our scoring network is shown in Figure 1. It con- sists of three main layers. On the bottom is a rep- resentation layer, which derives dense representa- tions XW , XP and XC for W, P and C, respec- tively. We compare various distributed represen- tations and neural network structures for learning XW , XP and XC, detailed in Section 3.1. On top of the representation layer, we use a hidden layer to merge XW , XP and XC into a single vector h = tanh(WhW \u00b7XW +WhP \u00b7XP +WhC\u00b7XC+bh) (1) The hidden feature vector h is used to represent the state S = \u27e8W, P, C\u27e9, for calculating the scores of the next action. In particular, a linear output layer with two nodes is employed: o = Wo \u00b7 h + bo (2) The \ufb01rst and second node of o represent the scores of SEP and APP given S, namely Score(S, SEP), Score(S, APP) respectively. 3.1 Representation Learning Characters.",
  "In particular, a linear output layer with two nodes is employed: o = Wo \u00b7 h + bo (2) The \ufb01rst and second node of o represent the scores of SEP and APP given S, namely Score(S, SEP), Score(S, APP) respectively. 3.1 Representation Learning Characters. We investigate two different ap- proaches to encode incoming characters, namely a window approach and an LSTM approach. For the former, we follow prior methods (Xue et al., 2003; Pei et al., 2014), using \ufb01ve-character window [c\u22122, c\u22121, c0, c1, c2] to represent incoming charac- ters. Shown in Figure 3, a multi-layer perceptron (MLP) is employed to derive a \ufb01ve-character win- dow vector DC from single-character vector rep- resentations Vc\u22122, Vc\u22121, Vc0, Vc1, Vc2.",
  "Shown in Figure 3, a multi-layer perceptron (MLP) is employed to derive a \ufb01ve-character win- dow vector DC from single-character vector rep- resentations Vc\u22122, Vc\u22121, Vc0, Vc1, Vc2. DC = MLP([Vc\u22122; Vc\u22121; Vc0; Vc1; Vc2]) (3) For the latter, we follow recent work (Chen et al., 2015b; Zhang et al., 2016b), using a bi- directional LSTM to encode input character se- quence.3 In particular, the bi-directional LSTM 3The LSTM variation with coupled input and forget gate but without peephole connections is applied (Gers and Schmidhuber, 2000)",
  "hidden vector [\u2190\u2212 hC(c0); \u2212\u2192 hC(c0)] of the next incom- ing character c0 is used to represent the coming characters [c0, c1, ...] given a state. Intuitively, a \ufb01ve-character window provides a local context from which the meaning of the middle character can be better disambiguated. LSTM, on the other hand, captures larger contexts, which can contain more useful clues for dismbiguation but also ir- relevant information. It is therefore interesting to investigate a combination of their strengths, by \ufb01rst deriving a locally-disambiguated version of c0, and then feed it to LSTM for a globally dis- ambiguated representation. Now with regard to the single-character vec- tor representation Vci(i \u2208[\u22122, 2]), we follow previous work and consider both character em- bedding ec(ci) and character-bigram embedding eb(ci, ci+1) , investigating the effect of each on the accuracies. When both ec(ci) and eb(ci, ci+1) are utilized, the concatenated vector is taken as Vci. Partial Word.",
  "When both ec(ci) and eb(ci, ci+1) are utilized, the concatenated vector is taken as Vci. Partial Word. We take a very simple approach to representing the partial word P, by using the em- bedding vectors of its \ufb01rst and last characters, as well as the embedding of its length. Length em- beddings are randomly initialized and then tuned in model training. XP has relatively less in\ufb02uence on the empirical segmentation accuracies. XP = [ec(P[0]); ec(P[\u22121]); el(LEN(P))] (4) Word. Similar to the character case, we investi- gate two different approaches to encoding incom- ing characters, namely a window approach and an LSTM approach. For the former, we follow prior methods (Zhang and Clark, 2007; Sun, 2010), us- ing the two-word window [w\u22122, w\u22121] to represent recognized words. A hidden layer is employed to derive a two-word vector XW from single word embeddings ew(w\u22122) and ew(w\u22121).",
  "A hidden layer is employed to derive a two-word vector XW from single word embeddings ew(w\u22122) and ew(w\u22121). XW = tanh(Ww[ew(w\u22122); ew(w\u22121)] + bw) (5) For the latter, we follow Zhang et al. (2016b) and Cai and Zhao (2016), using an uni-directional LSTM on words that have been recognized. 3.2 Pretraining Neural network models for NLP bene\ufb01t from pre- training of word/character embeddings, learning distributed sementic information from large raw texts for reducing sparsity. The three basic ele- ments in our neural segmentor, namely characters, character bigrams and words, can all be pretrained . . . . . . . . . . . . . \u00a0. \u00a0. . . . MLP ... ... ... punc. silver hete.",
  ". . . . . . . . . . . . \u00a0. \u00a0. . . . MLP ... ... ... punc. silver hete. POS shared parameters main training pretraining Bi-LSTM S A hidden layer output ... ... ... ... XW XP XC h DC Vc-2 Vc-1 Vc0 Vc1 Vc2 Figure 3: Shared character representation. over large unsegmented data. We pretrain the \ufb01ve- character window network in Figure 3 as an unit, learning the MLP parameter together with char- acter and bigram embeddings. We consider four types of commonly explored external data to this end, all of which have been studied for statisti- cal word segmentation, but not for neural network segmentors. Raw Text. Although raw texts do not contain ex- plicit word boundary information, statistics such as mutual information between consecutive char- acters can be useful features for guiding segmen- tation (Sun and Xu, 2011). For neural segmenta- tion, these distributional statistics can be implic- itly learned by pretraining character embeddings.",
  "For neural segmenta- tion, these distributional statistics can be implic- itly learned by pretraining character embeddings. We therefore consider a more explicit clue for pre- training our character window network, namely punctuations (Li and Sun, 2009). Punctuation can serve as a type of explicit mark- up (Spitkovsky et al., 2010), indicating that the two characters on its left and right belong to two different words. We leverage this source of infor- mation by extracting character \ufb01ve-grams exclud- ing punctuation from raw sentences, using them as inputs to classify whether there is punctuation before middle character. Denoting the resulting \ufb01ve character window as [c\u22122, c\u22121, c0, c1, c2], the MLP in Figure 3 is used to derive its representa- tion DC, which is then fed to a softmax layer for binary classi\ufb01cation: P(punc) = softmax(Wpunc \u00b7 DC + bpunc) (6) Here P(punc) indicates the probability of a punc- tuation mark existing before c0.",
  "Standard back- propagation training of the MLP in Figure 3 can be done jointly with the training of Wpunc and bpunc. After such training, the embedding Vci and MLP values can be used to initialize the corresponding parameters for DC in the main segmentor, before",
  "its training. Automatically Segmented Text. Large texts automatically segmented by a baseline segmen- tor can be used for self-training (Liu and Zhang, 2012) or deriving statistical features (Wang et al., 2011). We adopt a simple strategy, taking automatically segmented text as silver data to pretrain the \ufb01ve-character window net- work. Given [c\u22122, c\u22121, c0, c1.c2], DC is de- rived using the MLP in Figure 3, and then used to classify the segmentation of c0 into B(begining)/M(middle)/E(end)/S(single character word) labels. P(silver) = softmax(Wsilv \u00b7 DC + bsilv) (7) Here Wsilv and bsilv are model parameters. Train- ing can be done in the same way as training with punctuation. Heterogenous Training Data. Multiple segmen- tation corpora exist for Chinese, with different segmentation granularities. There has been inves- tigation on leveraging two corpora under differ- ent annotation standards to improve statistical seg- mentation (Jiang et al., 2009).",
  "Heterogenous Training Data. Multiple segmen- tation corpora exist for Chinese, with different segmentation granularities. There has been inves- tigation on leveraging two corpora under differ- ent annotation standards to improve statistical seg- mentation (Jiang et al., 2009). We try to utilize heterogenous treebanks by taking an external tree- bank as labeled data, training a B/M/E/S classi\ufb01er for the character windows network. P(hete) = softmax(Whete \u00b7 DC + bhete) (8) POS Data. Previous research has shown that POS information is closely related to segmentation (Ng and Low, 2004; Zhang and Clark, 2008). We ver- ify the utility of POS information for our seg- mentor by pretraining a classi\ufb01er that predicts the POS on each character, according to the character window representation DC. In particular, given [c\u22122, c\u22121, c0, c1, c2], the POS of the word that c0 belongs to is used as the output. P(pos) = softmax(Wpos \u00b7 DC + bpos) (9) Multitask Learning.",
  "In particular, given [c\u22122, c\u22121, c0, c1, c2], the POS of the word that c0 belongs to is used as the output. P(pos) = softmax(Wpos \u00b7 DC + bpos) (9) Multitask Learning. While each type of ex- ternal training data can offer one source of seg- mentation information, different external data can be complimentary to each other. We aim to in- ject all sources of information into the charac- ter window representation DC by using it as a shared representation for different classi\ufb01cation tasks. Neural model have been shown capable of doing multi-task learning via parameter sharing (Collobert et al., 2011).",
  "We aim to in- ject all sources of information into the charac- ter window representation DC by using it as a shared representation for different classi\ufb01cation tasks. Neural model have been shown capable of doing multi-task learning via parameter sharing (Collobert et al., 2011). Shown in Figure 3, in our Algorithm 1: Training Input : (xi, yi) Parameters: \u0398 Process: agenda \u2190(S = \u27e8[ ], \u03c6, Xi\u27e9, V = 0) for j in [0:LEN(Xi)] do beam = [] for \u02c6y in agenda do \u02c6y\u2032 = ACTION(\u02c6y, SEP) ADD(\u02c6y\u2032, beam) \u02c6y\u2032 = ACTION(\u02c6y, APP) ADD(\u02c6y\u2032, beam) end agenda \u2190TOP(beam, B) if yi j /\u2208agenda then \u02c6yj = BESTIN(agenda) UPDATE(yi j, \u02c6yj,\u0398) return end end \u02c6y = BESTIN(agenda) UPDATE(yi, \u02c6y,\u0398) return case, the output layer for each task is independent, but the hidden layer DC and all layers below DC are shared.",
  "For training with all sources above, we ran- domly sample sentences from the Punc./Auto- seg/Heter./POS sources with the ratio of 10/1/1/1, for each sentence in punctuation corpus we take only 2 characters (character before and after the punctuation) as input instances. 4 Decoding and Training To train the main segmentor, we adopt the global transition-based learning and beam-search strat- egy of Zhang and Clark (2011). For decoding, standard beam search is used, where the B best partial output hypotheses at each step are main- tained in an agenda. Initially, the agenda contains only the start state. At each step, all hypotheses in the agenda are expanded, by applying all possible actions and B highest scored resulting hypotheses are used as the agenda for the next step. For training, the same decoding process is ap- plied to each training example (xi, yi). At step j, if the gold-standard sequence of transition actions yi j falls out of the agenda, max-margin update is performed by taking the current best hypothesis \u02c6yj in the beam as a negative example, and yi j as",
  "Paramater Value Paramater Value \u03b1 0.01 size(ec) 50 \u03bb 10\u22128 size(eb) 50 p 0.2 size(ew) 50 \u03b7 0.2 size(el) 20 MLP layer 2 size(XC) 150 beam B 8 size(XP ) 50 size(h) 200 size(XW ) 100 Table 2: Hyper-parameter values. a positive example. The loss function is l(\u02c6yj, yi j) = max((score(\u02c6yj) + \u03b7 \u00b7 \u03b4(\u02c6yj, yi j) \u2212score(yi j)), 0), (10) where \u03b4(\u02c6yj, yi j) is the number of incorrect local decisions in \u02c6yj, and \u03b7 controls the score margin. The strategy above is early-update (Collins and Roark, 2004).",
  "The strategy above is early-update (Collins and Roark, 2004). On the other hand, if the gold- standard hypothesis does not fall out of the agenda until the full sentence has been segmented, a \ufb01- nal update is made between the highest scored hy- pothesis \u02c6y (non-gold standard) in the agenda and the gold-standard yi, using exactly the same loss function. Pseudocode for the online learning algo- rithm is shown in Algorithm 1. We use Adagrad (Duchi et al., 2011) to optimize model parameters, with an initial learning rate \u03b1. L2 regularization and dropout (Srivastava et al., 2014) on input are used to reduce over\ufb01tting, with a L2 weight \u03bb and a dropout rate p. All the pa- rameters in our model are randomly initialized to a value (\u2212r, r), where r = q 6.0 fanin+fanout (Ben- gio, 2012). We \ufb01ne-tune character and character bigram embeddings, but not word embeddings, ac- ccording to Zhang et al. (2016b).",
  "We \ufb01ne-tune character and character bigram embeddings, but not word embeddings, ac- ccording to Zhang et al. (2016b). 5 Experiments 5.1 Experimental Settings Data. We use Chinese Treebank 6.0 (CTB6) (Xue et al., 2005) as our main dataset. Train- ing, development and test set splits follow previ- ous work (Zhang et al., 2014). In order to ver- ify the robustness of our model, we additionally use SIGHAN 2005 bake-off (Emerson, 2005) and NLPCC 2016 shared task for Weibo segmentation (Qiu et al., 2016) as test datasets, where the stan- dard splits are used. For pretraining embedding of Source #Chars #Words #Sents Raw data Gigaword 116.5m \u2013 \u2013 Auto seg Gigaword 398.2m 238.6m 12.04m Hete.",
  "For pretraining embedding of Source #Chars #Words #Sents Raw data Gigaword 116.5m \u2013 \u2013 Auto seg Gigaword 398.2m 238.6m 12.04m Hete. People\u2019s Daily 10.14m 6.17m 104k POS People\u2019s Daily 10.14m 6.17m 104k Table 3: Statistics of external data. words, characters and character bigrams, we use Chinese Gigaword (simpli\ufb01ed Chinese sections)4, automatically segmented using ZPar 0.6 off-the- shelf (Zhang and Clark, 2007), the statictics of which are shown in Table 3. For pretraining character representations, we extract punctuation classi\ufb01cation data from the Gi- gaword corpus, and use the word-based ZPar and a standard character-based CRF model (Tseng et al., 2005) to obtain automatic segmentation results. We compare pretraining using ZPar results only and using results that both segmentors agree on. For heterogenous segmentation corpus and POS data, we use a People\u2019s Daily corpus of 5 months5.",
  "We compare pretraining using ZPar results only and using results that both segmentors agree on. For heterogenous segmentation corpus and POS data, we use a People\u2019s Daily corpus of 5 months5. Statistics are listed in Table 3. Evaluation. The standard word precision, recall and F1 measure (Emerson, 2005) are used to eval- uate segmentation performances. Hyper-parameter Values. We adopt commonly used values for most hyperparameters, but tuned the sizes of hidden layers on the development set. The values are summarized in Table 2. 5.2 Development Experiments We perform development experiments to verify the usefulness of various context representations, network con\ufb01gurations and different pretraining methods, respectively. 5.2.1 Context Representations The in\ufb02uence of character and word context rep- resentations are empirically studied by varying the network structures for XC and XW in Figure 1, re- spectively. All the experiments in this section are performed using a beam size of 8. Character Context.",
  "All the experiments in this section are performed using a beam size of 8. Character Context. We \ufb01x the word represen- tation XW to a 2-word window and compare dif- ferent character context representations. The re- sults are shown in Table 4, where \u201cno char\u201d rep- resents our model without XC, \u201c5-char window\u201d represents a \ufb01ve-character window context, \u201cchar LSTM\u201d represents character LSTM context and 4https://catalog.ldc.upenn.edu/LDC2011T13 5http://www.icl.pku.edu.cn/icl res",
  "Character P R F No char 82.19 87.20 84.62 5-char window 95.33 95.50 95.41 char LSTM 95.21 95.82 95.51 5-char window+LSTM 95.77 95.95 95.86 -char emb 95.20 95.19 95.20 -bichar emb 93.87 94.67 94.27 Table 4: In\ufb02uence of character contexts. \u201c5-char window + LSTM\u201d represents a combina- tion, detailed in Section 3.1. \u201c-char emb\u201d and \u201c- bichar emb\u201d represent the combined window and LSTM context without character and character- bigram information, respectively. As can be seen from the table, without char- acter information, the F-score is 84.62%, demon- strating the necessity of character contexts. Us- ing window and LSTM representations, the F- scores increase to 95.41% and 95.51%, respec- tively.",
  "Us- ing window and LSTM representations, the F- scores increase to 95.41% and 95.51%, respec- tively. A combination of the two lead to further improvement, showing that local and global char- acter contexts are indeed complementary, as hy- pothesized in Section 3.1. Finally, by removing character and character-bigram embeddings, the F-score decreases to 95.20% and 94.27%, respec- tively, which suggests that character bigrams are more useful compared to character unigrams. This is likely because they contain more distinct tokens and hence offer a larger parameter space. Word Context. The in\ufb02uence of various word contexts are shown in Table 5. Without using word information, our segmentor gives an F-score of 95.66% on the development data. Using a con- text of only w\u22121 (1-word window), the F-measure increases to 95.78%.",
  "Without using word information, our segmentor gives an F-score of 95.66% on the development data. Using a con- text of only w\u22121 (1-word window), the F-measure increases to 95.78%. This shows that word con- texts are far less important in our model com- pared to character contexts, and also compared to word contexts in previous word-based segmentors (Zhang et al., 2016b; Cai and Zhao, 2016). This is likely due to the difference in our neural net- work structures, and that we \ufb01ne-tune both charac- ter and character bigram embeddings, which sig- ni\ufb01cantly enlarges the adjustable parameter space as compared with Zhang et al. (2016b). The fact that word contexts can contribute relatively less than characters in a word is also not surprising in the sense that word-based neural segmentors do not outperform the best character-based mod- els by large margins.",
  "(2016b). The fact that word contexts can contribute relatively less than characters in a word is also not surprising in the sense that word-based neural segmentors do not outperform the best character-based mod- els by large margins. Given that character context is what we pretrain, our model relies more heavily Word P R F No word 95.50 95.83 95.66 1-word window 95.70 95.85 95.78 2-word window 95.77 95.95 95.86 3-word window 95.80 95.85 95.83 word LSTM 95.71 95.97 95.84 2-word window+LSTM 95.74 95.95 95.84 Table 5: In\ufb02uence of word contexts. on them. With both w\u22122 and w\u22121 being used for the context, the F-score further increases to 95.86%, showing that a 2-word window is useful by of- fering more contextual information. On the other hand, when w\u22123 is also considered, the F-score does not improve further.",
  "On the other hand, when w\u22123 is also considered, the F-score does not improve further. This is consistent with previous \ufb01ndings of statistical word segmentation (Zhang and Clark, 2007), which adopt a 2-word context. Interestingly, using a word LSTM does not bring further improvements, even when it is combined with a window context. This suggests that global word contexts may not offer crucial additional information compared with local word contexts. Intuitively, words are signi\ufb01cantly less polysemous compared with characters, and hence can serve as effective contexts even if used locally, to supplement a more crucial character context. 5.2.2 Stuctured Learning and Inference We verify the effectiveness of structured learning and inference by measuring the in\ufb02uence of beam size on the baseline segmentor. Figure 4 shows the F-scores against different numbers of training iter- ations with beam size 1,2,4,8 and 16, respectively. When the beam size is 1, the inference is local and greedy. As the size of the beam increases, more global structural ambiguities can be resolved since learning is designed to guide search.",
  "When the beam size is 1, the inference is local and greedy. As the size of the beam increases, more global structural ambiguities can be resolved since learning is designed to guide search. A contrast between beam sizes 1 and 2 demonstrates the use- fulness of structured learning and inference. As the beam size increases, the gain by doubling the beam size decreases. We choose a beam size of 8 for the remaining experiments for a tradeoff be- tween speed and accuracy. 5.2.3 Pretraining Results Table 6 shows the effectiveness of rich pretrain- ing of Dc on the development set. In particular, by using punctuation information, the F-score in- creases from 95.86% to 96.25%, with a relative error reduction of 9.4%. This is consistent with",
  "5 10 15 20 iteration 0.90 0.91 0.92 0.93 0.94 0.95 0.96 F1-value beam=1 beam=2 beam=4 beam=8 beam=16 Figure 4: F1 measure against the training epoch. Pretrain P R F ER% Baseline 95.77 95.95 95.86 0 +Punc. pretrain 96.36 96.13 96.25 -9.4 +Auto-seg pretrain 96.23 96.29 96.26 -9.7 +Heter-seg pretrain 96.28 96.27 96.27 -9.9 +POS pretrain 96.16 96.28 96.22 -8.7 +Multitask pretrain 96.54 96.42 96.48 -15.0 Table 6: In\ufb02uence of pretraining. the observation of Sun and Xu (2011), who show that punctuation is more effective compared with mutual information and access variety as semi- supervised data for a statistical word segmentation model.",
  "the observation of Sun and Xu (2011), who show that punctuation is more effective compared with mutual information and access variety as semi- supervised data for a statistical word segmentation model. With automatically-segmented data6, het- erogenous segmentation and POS information, the F-score increases to 96.26%, 96.27% and 96.22%, respectively, showing the relevance of all infor- mation sources to neural segmentation, which is consistent with observations made for statistical word segmentation (Jiang et al., 2009; Wang et al., 2011; Zhang et al., 2013). Finally, by integrat- ing all above information via multi-task learning, the F-score is further improved to 96.48%, with a 15.0% relative error reduction. 5.2.4 Comparision with Zhang et al. (2016b) Both our model and Zhang et al. (2016b) use global learning and beam search, but our network is different. Zhang et al. (2016b) utilizes the ac- tion history with LSTM encoder, while we use par- tial word rather than action information.",
  "(2016b) use global learning and beam search, but our network is different. Zhang et al. (2016b) utilizes the ac- tion history with LSTM encoder, while we use par- tial word rather than action information. Besides, the character and character bigram embeddings are \ufb01ne-tuned in our model while Zhang et al. (2016b) set the embeddings \ufb01xed during training. 6By using ZPar alone, the auto-segmented result is 96.02%, less than using results by matching ZPar and the CRF segmentor outputs. 10< 30 50 70 90 >110 Sentence length 0.94 0.95 0.96 0.97 0.98 F1-value Multitask Baseline Zhang et al. 2016 Figure 5: F1 measure against the sentence length. We study the F-measure distribution with respect to sentence length on our baseline model, multi- task pretraining model and Zhang et al. (2016b).",
  "2016 Figure 5: F1 measure against the sentence length. We study the F-measure distribution with respect to sentence length on our baseline model, multi- task pretraining model and Zhang et al. (2016b). In particular, we cluster the sentences in the de- velopment dataset into 6 categories based on their length and evaluate their F1-values, respectively. As shown in Figure 5, the models give different error distributions, with our models being more ro- bust to the sentence length compared with Zhang et al. (2016b). Their model is better on very short sentences, but worse on all other cases. This shows the relative advantages of our model. 5.3 Final Results Our \ufb01nal results on CTB6 are shown in Table 7, which lists the results of several current state-of- the-art methods. Without multitask pretraining, our model gives an F-score of 95.44%, which is higher than the neural segmentor of Zhang et al. (2016b), which gives the best accuracies among pure neural segments on this dataset.",
  "Without multitask pretraining, our model gives an F-score of 95.44%, which is higher than the neural segmentor of Zhang et al. (2016b), which gives the best accuracies among pure neural segments on this dataset. By us- ing multitask pretraining, the result increases to 96.21%, with a relative error reduction of 16.9%. In comparison, Sun and Xu (2011) investigated heterogenous semi-supervised learning on a state- of-the-art statistical model, obtaining a relative er- ror reduction of 13.8%. Our \ufb01ndings show that external data can be as useful for neural segmen- tation as for statistical segmentation. Our \ufb01nal results compare favourably to the best statistical models, including those using semi- supervised learning (Sun and Xu, 2011; Wang et al., 2011), and those leveraging joint POS and syntactic information (Zhang et al., 2014). In ad- dition, it also outperforms the best neural models, in particular Zhang et al. (2016b)*, which is a hy- brid neural and statistical model, integrating man-",
  "Models P R F Baseline 95.3 95.5 95.4 Punc. pretrain 96.0 95.6 95.8 Auto-seg pretrain 95.8 95.6 95.7 Multitask pretrain 96.4 96.0 96.2 Sun and Xu (2011) baseline 95.2 94.9 95.1 Sun and Xu (2011) multi-source semi 95.9 95.6 95.7 Zhang et al. (2016b) neural 95.3 94.7 95.0 Zhang et al. (2016b)* hybrid 96.1 95.8 96.0 Chen et al. (2015a) window 95.7 95.8 95.8 Chen et al. (2015b) char LSTM 96.2 95.8 96.0 Zhang et al. (2014) POS and syntax \u2013 \u2013 95.7 Wang et al.",
  "(2015b) char LSTM 96.2 95.8 96.0 Zhang et al. (2014) POS and syntax \u2013 \u2013 95.7 Wang et al. (2011) statistical semi 95.8 95.8 95.8 Zhang and Clark (2011) statistical 95.5 94.8 95.1 Table 7: Main results on CTB6. ual discrete features into their word-based neural model. We achieve the best reported F-score on this dataset. To our knowledge, this is the \ufb01rst time a pure neural network model outperforms all ex- isting methods on this dataset, allowing the use of external data 7. We also evaluate our model pre- trained only on punctuation and auto-segmented data, which do not include additional manual la- bels. The results on CTB test data show the accu- racy of 95.8% and 95.7%, respectivley, which are comparable with those statistical semi-supervised methods (Sun and Xu, 2011; Wang et al., 2011).",
  "The results on CTB test data show the accu- racy of 95.8% and 95.7%, respectivley, which are comparable with those statistical semi-supervised methods (Sun and Xu, 2011; Wang et al., 2011). They are also among the top performance meth- ods in Table 7. Compared with discrete semi- supervised methods (Sun and Xu, 2011; Wang et al., 2011), our semi-supervised model is free from hand-crafted features. In addition to CTB6, which has been the most commonly adopted by recent segmentation re- search, we additionally evaluate our results on the SIGHAN 2005 bakeoff and Weibo datasets, to ex- amine cross domain robustness. Different state- of-the-art methods for which results are recorded on these datasets are listed in Table 8. Most neu- ral models reported results only on the PKU 8 and MSR datasets of the bakeoff test sets, which are in simpli\ufb01ed Chinese.",
  "Different state- of-the-art methods for which results are recorded on these datasets are listed in Table 8. Most neu- ral models reported results only on the PKU 8 and MSR datasets of the bakeoff test sets, which are in simpli\ufb01ed Chinese. The AS and CityU corpora are in traditional Chinese, sourced from Taiwan and 7 We did not investigate the use of lexicons (Chen et al., 2015a,b) in our research, since lexicons might cover different OOV in the training and test data, and hence directly affecting the accuracies, which makes it relatively dif\ufb01cult to compare different methods fairly unless a single lexicon is used for all methods, as observed by Cai and Zhao (2016). 8We notice that both PKU dataset and our heterogenous data are based on the news of People\u2019s Daily. While the het- erogenous data only collect news from Febuary 1998 to June 1998, it does not contain the sentences in the dev and test datasets of PKU.",
  "8We notice that both PKU dataset and our heterogenous data are based on the news of People\u2019s Daily. While the het- erogenous data only collect news from Febuary 1998 to June 1998, it does not contain the sentences in the dev and test datasets of PKU. F1 measure PKU MSR AS CityU Weibo Multitask pretrain 96.3 97.5 95.7 96.9 95.5 Cai and Zhao (2016) 95.5 96.5 \u2013 \u2013 \u2013 Zhang et al. (2016b) 95.1 97.0 \u2013 \u2013 \u2013 Zhang et al. (2016b)* 95.7 97.7 \u2013 \u2013 \u2013 Pei et al. (2014) 95.2 97.2 \u2013 \u2013 \u2013 Sun et al. (2012) 95.4 97.4 \u2013 \u2013 \u2013 Zhang and Clark (2007) 94.5 97.2 94.6 95.1 \u2013 Zhang et al. (2006) 95.1 97.1 95.1 95.1 \u2013 Sun et al.",
  "(2006) 95.1 97.1 95.1 95.1 \u2013 Sun et al. (2009) 95.2 97.3 \u2013 94.6 \u2013 Sun (2010) 95.2 96.9 95.2 95.6 \u2013 Wang et al. (2014) 95.3 97.4 95.4 94.7 \u2013 Xia et al. (2016) \u2013 \u2013 \u2013 \u2013 95.4 Table 8: Main results on other test datasets. Hong Kong corpora, respectively. We map them into simpli\ufb01ed Chinese before segmentation. The Weibo corpus is in a yet different genre, being so- cial media text. Xia et al. (2016) achieved the best results on this dataset by using a statistical model with features learned using external lexicons, the CTB7 corpus and the People Daily corpus. Simi- lar to Table 7, our method gives the best accuracies on all corpora except for MSR, where it underper- forms the hybrid model of Zhang et al. (2016b) by 0.2%.",
  "Simi- lar to Table 7, our method gives the best accuracies on all corpora except for MSR, where it underper- forms the hybrid model of Zhang et al. (2016b) by 0.2%. To our knowledge, we are the \ufb01rst to re- port results for a neural segmentor on more than 3 datasets, with competitive results consistently. It veri\ufb01es that knowledge learned from a certain set of resources can be used to enhance cross-domain robustness in training a neural segmentor for dif- ferent datasets, which is of practical importance. 6 Conclusion We investigated rich external resources for en- hancing neural word segmentation, by building a globally optimised beam-search model that lever- ages both character and word contexts. Taking each type of external resource as an auxiliary clas- si\ufb01cation task, we use neural multi-task learning to pre-train a set of shared parameters for character contexts. Results show that rich pretraining leads to 15.4% relative error reduction, and our model gives results highly competitive to the best sys- tems on six different benchmarks.",
  "Results show that rich pretraining leads to 15.4% relative error reduction, and our model gives results highly competitive to the best sys- tems on six different benchmarks. Acknowledgments We thank the anonymous reviewers for their in- sightful comments and the support of NSFC 61572245. We would like to thank Meishan Zhang for his insightful discussion and assisting coding. Yue Zhang is the corresponding author.",
  "References Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman Ganchev, Slav Petrov, and Michael Collins. 2016. Globally nor- malized transition-based neural networks. In ACL. Association for Computational Linguistics, pages 2442\u20132452. https://doi.org/10.18653/v1/P16-1231. Yoshua Bengio. 2012. Practical recommendations for gradient-based training of deep architectures. In Neural networks: Tricks of the trade, Springer, pages 437\u2013478. Deng Cai and Hai Zhao. 2016. Neural word seg- mentation learning for chinese. In ACL. Associa- tion for Computational Linguistics, pages 409\u2013420. https://doi.org/10.18653/v1/P16-1039. Xinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xu- anjing Huang. 2015a. Gated recursive neu- ral network for chinese word segmentation. In ACL. Association for Computational Linguistics.",
  "Xinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xu- anjing Huang. 2015a. Gated recursive neu- ral network for chinese word segmentation. In ACL. Association for Computational Linguistics. https://doi.org/10.3115/v1/P15-1168. Xinchi Chen, Xipeng Qiu, Chenxi Zhu, Pengfei Liu, and Xuanjing Huang. 2015b. Long short- term memory neural networks for chinese word segmentation. In EMNLP. Association for Computational Linguistics, pages 1385\u20131394. https://doi.org/10.18653/v1/D15-1141. Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In ACL. As- sociation for Computational Linguistics, page 111. http://aclweb.org/anthology/P04-1015. Ronan Collobert, Jason Weston, L\u00b4eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.",
  "Ronan Collobert, Jason Weston, L\u00b4eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research 12(Aug):2493\u20132537. John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research 12(Jul):2121\u20132159. Thomas Emerson. 2005. The second international chi- nese word segmentation bakeoff. In Proceedings of the fourth SIGHAN workshop on Chinese language Processing. volume 133. Felix A Gers and J\u00a8urgen Schmidhuber. 2000. Recur- rent nets that time and count. In Neural Networks, 2000. IJCNN 2000, Proceedings of the IEEE-INNS- ENNS International Joint Conference on. IEEE, vol- ume 3, pages 189\u2013194. Wenbin Jiang, Liang Huang, and Qun Liu. 2009.",
  "IJCNN 2000, Proceedings of the IEEE-INNS- ENNS International Joint Conference on. IEEE, vol- ume 3, pages 189\u2013194. Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Automatic adaptation of annotation standards: Chinese word segmentation and pos tagging: a case study. In ACL-IJCNLP. Association for Computational Linguistics, pages 522\u2013530. http://aclweb.org/anthology/P09-1059. Zhongguo Li and Maosong Sun. 2009. Punctuation as implicit annotations for chinese word segmen- tation. Computational Linguistics 35(4):505\u2013512. http://aclweb.org/anthology/J09-4006. Yang Liu and Yue Zhang. 2012. Unsuper- vised domain adaptation for joint segmentation and pos-tagging. In COLING. pages 745\u2013754. http://aclweb.org/anthology/C12-2073. Mairgup Mansur, Wenzhe Pei, and Baobao Chang. 2013.",
  "In COLING. pages 745\u2013754. http://aclweb.org/anthology/C12-2073. Mairgup Mansur, Wenzhe Pei, and Baobao Chang. 2013. Feature-based neural language model and chi- nese word segmentation. In IJCNLP. pages 1271\u2013 1277. http://aclweb.org/anthology/I13-1181. Hajime Morita, Daisuke Kawahara, and Sadao Kurohashi. 2015. Morphological analysis for unsegmented languages using recurrent neu- ral network language model. In EMNLP. Association for Computational Linguistics. https://doi.org/10.18653/v1/D15-1276. Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of- speech tagging: One-at-a-time or all-at-once? word- based or character-based? In EMNLP. Associa- tion for Computational Linguistics, pages 277\u2013284. http://aclweb.org/anthology/W04-3236.",
  "Chinese part-of- speech tagging: One-at-a-time or all-at-once? word- based or character-based? In EMNLP. Associa- tion for Computational Linguistics, pages 277\u2013284. http://aclweb.org/anthology/W04-3236. Wenzhe Pei, Tao Ge, and Baobao Chang. 2014. Max-margin tensor neural network for chinese word segmentation. In ACL. Association for Computational Linguistics, pages 293\u2013303. https://doi.org/10.3115/v1/P14-1028. Fuchun Peng, Fangfang Feng, and Andrew McCallum. 2004. Chinese segmentation and new word detec- tion using conditional random \ufb01elds. In COLING. page 562. http://aclweb.org/anthology/C04-1081. Xipeng Qiu, Peng Qian, and Zhan Shi. 2016. Overview of the nlpcc-iccpol 2016 shared task: Chinese word segmentation for micro-blog texts. In International Conference on Computer Processing of Oriental Languages. Springer, pages 901\u2013906.",
  "2016. Overview of the nlpcc-iccpol 2016 shared task: Chinese word segmentation for micro-blog texts. In International Conference on Computer Processing of Oriental Languages. Springer, pages 901\u2013906. Valentin I Spitkovsky, Daniel Jurafsky, and Hiyan Al- shawi. 2010. Pro\ufb01ting from mark-up: Hyper-text annotations for guided parsing. In ACL. Associ- ation for Computational Linguistics, pages 1278\u2013 1287. http://aclweb.org/anthology/P10-1130. Richard Sproat, William Gale, Chilin Shih, and Nancy Chang. 1996. A stochastic \ufb01nite- state word-segmentation algorithm for chi- nese. Computational linguistics 22(3):377\u2013404. http://aclweb.org/anthology/J96-3004. Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from over\ufb01tting.",
  "Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from over\ufb01tting. Journal of Machine Learning Re- search 15(1):1929\u20131958. Weiwei Sun. 2010. Word-based and character- based word segmentation models: Comparison and combination. In COLING. pages 1211\u20131219. http://aclweb.org/anthology/C10-2139.",
  "Weiwei Sun and Jia Xu. 2011. Enhancing chinese word segmentation using unlabeled data. In EMNLP. As- sociation for Computational Linguistics, pages 970\u2013 979. http://aclweb.org/anthology/D11-1090. Xu Sun, Houfeng Wang, and Wenjie Li. 2012. Fast online training with frequency-adaptive learn- ing rates for chinese word segmentation and new word detection. In ACL. Association for Computational Linguistics, pages 253\u2013262. http://aclweb.org/anthology/P12-1027. Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshi- masa Tsuruoka, and Jun\u2019ichi Tsujii. 2009. A dis- criminative latent variable chinese segmenter with hybrid word/character information. In NAACL- HLT. Association for Computational Linguistics, pages 56\u201364. http://aclweb.org/anthology/N09- 1007. Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005.",
  "Association for Computational Linguistics, pages 56\u201364. http://aclweb.org/anthology/N09- 1007. Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A condi- tional random \ufb01eld word segmenter for sighan bake- off 2005. In Proceedings of the fourth SIGHAN workshop on Chinese language Processing. Mengqiu Wang, Rob Voigt, and Christopher D Man- ning. 2014. Two knives cut better than one: Chi- nese word segmentation with dual decomposition. In ACL. Association for Computational Linguistics, pages 193\u2013198. https://doi.org/10.3115/v1/P14- 2032. Yiou Wang, Yoshimasa Tsuruoka Jun\u2019ichi Kazama, Yoshimasa Tsuruoka, Wenliang Chen, Yujie Zhang, and Kentaro Torisawa. 2011. Improv- ing chinese word segmentation and pos tagging with semi-supervised methods using large auto- analyzed data. In IJCNLP.",
  "2011. Improv- ing chinese word segmentation and pos tagging with semi-supervised methods using large auto- analyzed data. In IJCNLP. pages 309\u2013317. http://www.aclweb.org/anthology/I11-1035. Sam Wiseman and Alexander M Rush. 2016. Sequence-to-sequence learning as beam-search optimization. In EMNLP. Association for Computational Linguistics, pages 1296\u20131306. http://aclweb.org/anthology/D16-1137. Qingrong Xia, Zhenghua Li, Jiayuan Chao, and Min Zhang. 2016. Word segmentation on micro-blog texts with external lexicon and heterogeneous data. In International Conference on Computer Process- ing of Oriental Languages. Springer. Jingjing Xu and Xu Sun. 2016. Dependency- based gated recursive neural network for chi- nese word segmentation. In ACL. Associa- tion for Computational Linguistics, page 567. https://doi.org/10.18653/v1/P16-2092.",
  "Jingjing Xu and Xu Sun. 2016. Dependency- based gated recursive neural network for chi- nese word segmentation. In ACL. Associa- tion for Computational Linguistics, page 567. https://doi.org/10.18653/v1/P16-2092. Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Marta Palmer. 2005. The penn chinese treebank: Phrase structure annotation of a large corpus. Natural lan- guage engineering 11(02):207\u2013238. Nianwen Xue et al. 2003. Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing 8(1):29\u201348. Longkai Zhang, Houfeng Wang, Xu Sun, and Mair- gup Mansur. 2013. Exploring representations from unlabeled data with co-training for chi- nese word segmentation. In EMNLP. Associa- tion for Computational Linguistics, pages 311\u2013321. http://aclweb.org/anthology/D13-1031.",
  "2013. Exploring representations from unlabeled data with co-training for chi- nese word segmentation. In EMNLP. Associa- tion for Computational Linguistics, pages 311\u2013321. http://aclweb.org/anthology/D13-1031. Meishan Zhang, Jie Yang, Zhiyang Teng, and Yue Zhang. 2016a. Libn3l: a lightweight package for neural nlp. In Proceedings of the Tenth International Conference on Language Resources and Evaluation. https://doi.org/10.1145/322234.322243. Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting Liu. 2014. Character-level chinese de- pendency parsing. In ACL. Association for Computational Linguistics, pages 1326\u20131336. https://doi.org/10.3115/v1/P14-1125. Meishan Zhang, Yue Zhang, and Guohong Fu. 2016b. Transition-based neural word segmentation. In ACL. Association for Computational Linguistics. https://doi.org/10.18653/v1/P16-1040.",
  "Meishan Zhang, Yue Zhang, and Guohong Fu. 2016b. Transition-based neural word segmentation. In ACL. Association for Computational Linguistics. https://doi.org/10.18653/v1/P16-1040. Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita. 2006. Subword-based tagging by conditional random \ufb01elds for chinese word segmentation. In NAACL. Association for Computational Linguistics, pages 193\u2013196. http://aclweb.org/anthology/N06-2049. Yue Zhang and Stephen Clark. 2007. Chi- nese segmentation with a word-based percep- tron algorithm. In ACL. Association for Com- putational Linguistics, volume 45, page 840. http://aclweb.org/anthology/P07-1106. Yue Zhang and Stephen Clark. 2008. Joint word seg- mentation and pos tagging using a single perceptron. In ACL. Association for Computational Linguistics, pages 888\u2013896.",
  "Yue Zhang and Stephen Clark. 2008. Joint word seg- mentation and pos tagging using a single perceptron. In ACL. Association for Computational Linguistics, pages 888\u2013896. http://aclweb.org/anthology/P08- 1101. Yue Zhang and Stephen Clark. 2011. Syntactic pro- cessing using the generalized perceptron and beam search. Computational linguistics 37(1):105\u2013151. https://doi.org/10.1162/coli a 00037. Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang Lu. 2006. Effective tag set selection in chinese word segmentation via conditional random \ufb01eld model- ing. In PACLIC. Citeseer, volume 20, pages 87\u201394. http://aclweb.org/anthology/Y06-1012. Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu. 2013. Deep learning for chinese word segmen- tation and pos tagging. In EMNLP.",
  "http://aclweb.org/anthology/Y06-1012. Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu. 2013. Deep learning for chinese word segmen- tation and pos tagging. In EMNLP. Associa- tion for Computational Linguistics, pages 647\u2013657. http://aclweb.org/anthology/D13-1061. Hao Zhou, Yue Zhang, Shujian Huang, and Ji- ajun Chen. 2015. A neural probabilistic structured-prediction model for transition-based dependency parsing. In ACL. Association for Computational Linguistics, pages 1213\u20131222. https://doi.org/10.3115/v1/P15-1117."
]