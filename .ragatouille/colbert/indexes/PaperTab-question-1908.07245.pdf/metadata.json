{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "arXiv:1908.07245v4  [cs.CL]  5 Jan 2020 GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge Luyao Huang, Chi Sun, Xipeng Qiu\u2217, Xuanjing Huang Shanghai Key Laboratory of Intelligent Information Processing, Fudan University School of Computer Science, Fudan University 825 Zhangheng Road, Shanghai, China {lyhuang18,sunc17,xpqiu,xjhuang}@fudan.edu.cn Abstract Word Sense Disambiguation (WSD) aims to \ufb01nd the exact sense of an ambiguous word in a particular context. Traditional supervised methods rarely take into consideration the lex- ical resources like WordNet, which are widely utilized in knowledge-based methods. Recent studies have shown the effectiveness of incor- porating gloss (sense de\ufb01nition) into neural networks for WSD. However, compared with traditional word expert supervised methods, they have not achieved much improvement. In this paper, we focus on how to better leverage gloss knowledge in a supervised neural WSD system. We construct context-gloss pairs and propose three BERT-based models for WSD.",
      "However, compared with traditional word expert supervised methods, they have not achieved much improvement. In this paper, we focus on how to better leverage gloss knowledge in a supervised neural WSD system. We construct context-gloss pairs and propose three BERT-based models for WSD. We \ufb01ne-tune the pre-trained BERT model on SemCor3.0 training corpus and the experimen- tal results on several English all-words WSD benchmark datasets show that our approach outperforms the state-of-the-art systems 1. 1 Introduction Word Sense Disambiguation (WSD) is a funda- mental task and long-standing challenge in Nat- ural Language Processing (NLP), which aims to \ufb01nd the exact sense of an ambiguous word in a par- ticular context (Navigli, 2009). Previous WSD ap- proaches can be grouped into two main categories: knowledge-based and supervised methods. Knowledge-based WSD methods rely on lex- ical resources like WordNet (Miller, 1995) and usually exploit two kinds of lexical knowledge.",
      "Previous WSD ap- proaches can be grouped into two main categories: knowledge-based and supervised methods. Knowledge-based WSD methods rely on lex- ical resources like WordNet (Miller, 1995) and usually exploit two kinds of lexical knowledge. The gloss, which de\ufb01nes a word sense mean- ing, is \ufb01rst utilized in Lesk algorithm (Lesk, 1986) and then widely taken into account in many other approaches (Banerjee and Pedersen, 2002; Basile et al., 2014). Besides, structural properties of semantic graphs are mainly used in graph-based algorithms (Agirre et al., 2014; Moro et al., 2014). \u2217Corresponding author. 1https://github.com/HSLCY/GlossBERT Traditional supervised WSD methods (Zhong and Ng, 2010; Shen et al., 2013; Iacobacci et al., 2016) focus on extracting manually designed features and then train a dedicated classi\ufb01er (word expert) for every target lemma.",
      "Although word expert supervised WSD meth- ods perform better, they are less \ufb02exible than knowledge-based methods in the all-words WSD task (Raganato et al., 2017a). Recent neural-based methods are devoted to dealing with this prob- lem. K\u02daageb\u00a8ack and Salomonsson (2016) present a supervised classi\ufb01er based on Bi-LSTM, which shares parameters among all word types except the last layer. Raganato et al. (2017a) convert WSD task to a sequence labeling task, thus building a uni\ufb01ed model for all polysemous words. However, neither of them can totally beat the best word ex- pert supervised methods. More recently, Luo et al. (2018b) propose to leverage the gloss information from WordNet and model the semantic relationship between the context and gloss in an improved memory net- work. Similarly, Luo et al. (2018a) introduce a (hierarchical) co-attention mechanism to generate co-dependent representations for the context and gloss.",
      "Similarly, Luo et al. (2018a) introduce a (hierarchical) co-attention mechanism to generate co-dependent representations for the context and gloss. Their attempts prove that incorporating gloss knowledge into supervised WSD approach is helpful, but they still have not achieved much improvement, because they may not make full use of gloss knowledge. In this paper, we focus on how to better lever- age gloss information in a supervised neural WSD system. Recently, the pre-trained language mod- els, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018), have shown their effective- ness to alleviate the effort of feature engineering. Especially, BERT has achieved excellent results in question answering (QA) and natural language inference (NLI). We construct context-gloss pairs",
      "Sentence with four targets: Your research stopped when a convenient assertion could be made. Context-Gloss Pairs of the target word [research] Label Sense Key [CLS] Your research ... [SEP] systematic investigation to ... [SEP] Yes research%1:04:00:: [CLS] Your research ... [SEP] a search for knowledge [SEP] No research%1:09:00:: [CLS] Your research ... [SEP] inquire into [SEP] No research%2:31:00:: [CLS] Your research ... [SEP] attempt to \ufb01nd out in a ... [SEP] No research%2:32:00:: Context-Gloss Pairs with weak supervision of the target word [research] Label Sense Key [CLS] Your \u201cresearch\u201d ... [SEP] research: systematic investigation to ... [SEP] Yes research%1:04:00:: [CLS] Your \u201cresearch\u201d ... [SEP] research: a search for knowledge [SEP] No research%1:09:00:: [CLS] Your \u201cresearch\u201d ...",
      "[SEP] research: systematic investigation to ... [SEP] Yes research%1:04:00:: [CLS] Your \u201cresearch\u201d ... [SEP] research: a search for knowledge [SEP] No research%1:09:00:: [CLS] Your \u201cresearch\u201d ... [SEP] research: inquire into [SEP] No research%2:31:00:: [CLS] Your \u201cresearch\u201d ... [SEP] research: attempt to \ufb01nd out in a ... [SEP] No research%2:32:00:: Table 1: The construction methods. The sentence is taken from SemEval-2007 WSD dataset. The ellipsis \u201c...\u201d indicates the remainder of the sentence or the gloss. from glosses of all possible senses (in WordNet) of the target word, thus treating WSD task as a sentence-pair classi\ufb01cation problem. We \ufb01ne-tune the pre-trained BERT model on SemCor3.0 train- ing corpus. Recently2, we are informed by Vial et al.",
      "We \ufb01ne-tune the pre-trained BERT model on SemCor3.0 train- ing corpus. Recently2, we are informed by Vial et al. (2019)3 that they also use BERT and incorpo- rate WordNet as lexical knowledge in their super- vised WSD system. But our work is much differ- ent from theirs. They exploit the semantic rela- tionships between senses such as synonymy, hy- pernymy and hyponymy and rely on pre-trained BERT word vectors (feature-based approach); we leverage gloss knowledge (sense de\ufb01nition) and use BERT through \ufb01ne-tuning procedures. Out of respect, we add their results in Table 3. However, the results of their feature-based approach in the same experimental setup (single training set and single model) are not as good as our \ufb01ne-tuning approach although their ensemble systems (with another training set WNGC) achieve better perfor- mance. In particular, our contribution is two-fold: 1. We construct context-gloss pairs and propose three BERT-based models for WSD. 2.",
      "In particular, our contribution is two-fold: 1. We construct context-gloss pairs and propose three BERT-based models for WSD. 2. We \ufb01ne-tune the pre-trained BERT model on SemCor3.0 training corpus, and the experimental results on several English all-words WSD bench- mark datasets show that our approach outperforms the state-of-the-art systems. 2 Methodology In this section, we describe our method in detail. 2after we submitted the \ufb01nal version to the conference 3their paper is available on arXiv after our \ufb01rst submission to the conference in May, 2019 2.1 Task De\ufb01nition In WSD, a sentence s usually consists of a series of words: {w1, \u00b7 \u00b7 \u00b7 , wm}, and some of the words {wi1, \u00b7 \u00b7 \u00b7 , wik} are targets {t1, \u00b7 \u00b7 \u00b7 , tk} need to be disambiguated. For each target t, its candi- date senses {c1, \u00b7 \u00b7 \u00b7 , cn} come from entries of its lemma in a pre-de\ufb01ned sense inventory (usually WordNet).",
      "For each target t, its candi- date senses {c1, \u00b7 \u00b7 \u00b7 , cn} come from entries of its lemma in a pre-de\ufb01ned sense inventory (usually WordNet). Therefore, WSD task aims to \ufb01nd the most suitable entry (symbolized as unique sense key) for each target in a sentence. See a sentence example in Table 1. 2.2 BERT BERT (Devlin et al., 2018) is a new language rep- resentation model, and its architecture is a multi- layer bidirectional Transformer encoder. BERT model is pre-trained on a large corpus and two novel unsupervised prediction tasks, i.e., masked language model and next sentence prediction tasks are used in pre-training. When incorporating BERT into downstream tasks, the \ufb01ne-tuning pro- cedure is recommended. We \ufb01ne-tune the pre- trained BERT model on WSD task. BERT(Token-CLS) Since every target in a sen- tence needs to be disambiguated to \ufb01nd its ex- act sense, WSD task can be regarded as a token- level classi\ufb01cation task.",
      "BERT(Token-CLS) Since every target in a sen- tence needs to be disambiguated to \ufb01nd its ex- act sense, WSD task can be regarded as a token- level classi\ufb01cation task. To incorporate BERT to WSD task, we take the \ufb01nal hidden state of the token corresponding to the target word (if more than one token, we average them) and add a clas- si\ufb01cation layer for every target lemma, which is the same as the last layer of the Bi-LSTM model (K\u02daageb\u00a8ack and Salomonsson, 2016).",
      "2.3 GlossBERT BERT can explicitly model the relationship of a pair of texts, which has shown to be bene\ufb01cial to many pair-wise natural language understanding tasks. In order to fully leverage gloss information, we propose GlossBERT to construct context-gloss pairs from all possible senses of the target word in WordNet, thus treating WSD task as a sentence- pair classi\ufb01cation problem. We describe our construction method with an example (See Table 1). There are four targets in this sentence, and here we take target word re- search as an example: Context-Gloss Pairs The sentence containing target words is denoted as context sentence. For each target word, we extract glosses of all N pos- sible senses (here N = 4) of the target word (research) in WordNet to obtain the gloss sen- tence. [CLS] and [SEP] marks are added to the context-gloss pairs to make it suitable for the in- put of BERT model. A similar idea is also used in aspect-based sentiment analysis (Sun et al., 2019).",
      "[CLS] and [SEP] marks are added to the context-gloss pairs to make it suitable for the in- put of BERT model. A similar idea is also used in aspect-based sentiment analysis (Sun et al., 2019). Context-Gloss Pairs with Weak Supervision Based on the previous construction method, we add weak supervised signals to the context-gloss pairs (see the highlighted part in Table 1). The signal in the gloss sentence aims to point out the target word, and the signal in the context sentence aims to emphasize the target word considering the situation that a target word may occur more than one time in the same sentence. Therefore, each target word has N context-gloss pair training instances (label \u2208{yes, no}). When testing, we output the probability of label = yes of each context-gloss pair and choose the sense corresponding to the highest probability as the pre- diction label of the target word. We experiment with three GlossBERT models: GlossBERT(Token-CLS) We use context-gloss pairs as input.",
      "When testing, we output the probability of label = yes of each context-gloss pair and choose the sense corresponding to the highest probability as the pre- diction label of the target word. We experiment with three GlossBERT models: GlossBERT(Token-CLS) We use context-gloss pairs as input. We highlight the target word by tak- ing the \ufb01nal hidden state of the token correspond- ing to the target word (if more than one token, we average them) and add a classi\ufb01cation layer (label \u2208{yes, no}). GlossBERT(Sent-CLS) We use context-gloss pairs as input.",
      "GlossBERT(Sent-CLS) We use context-gloss pairs as input. We take the \ufb01nal hidden state of the \ufb01rst token [CLS] as the representation of the whole sequence and add a classi\ufb01cation layer Dataset Total Noun Verb Adj Adv SemCor 226036 87002 88334 31753 18947 SE2 2282 1066 517 445 254 SE3 1850 900 588 350 12 SE07 455 159 296 0 0 SE13 1644 1644 0 0 0 SE15 1022 531 251 160 80 Table 2: Statistics of the different parts of speech anno- tations in English all-words WSD datasets. (label \u2208{yes, no}), which does not highlight the target word. GlossBERT(Sent-CLS-WS) We use context- gloss pairs with weak supervision as input. We take the \ufb01nal hidden state of the \ufb01rst token [CLS] and add a classi\ufb01cation layer (label \u2208{yes, no}), which weekly highlight the target word by the weak supervision.",
      "We take the \ufb01nal hidden state of the \ufb01rst token [CLS] and add a classi\ufb01cation layer (label \u2208{yes, no}), which weekly highlight the target word by the weak supervision. 3 Experiments 3.1 Datasets The statistics of the WSD datasets are shown in Table 2. Training Dataset Following previous work (Luo et al., 2018a,b; Raganato et al., 2017a,b; Iacobacci et al., 2016; Zhong and Ng, 2010), we choose SemCor3.0 as training corpus, which is the largest corpus manually annotated with WordNet sense for WSD. Evaluation Datasets We evaluate our method on several English all-words WSD datasets. For a fair comparison, we use the benchmark datasets proposed by Raganato et al.",
      "Evaluation Datasets We evaluate our method on several English all-words WSD datasets. For a fair comparison, we use the benchmark datasets proposed by Raganato et al. (2017b) which in- clude \ufb01ve standard all-words \ufb01ne-grained WSD datasets from the Senseval and SemEval com- petitions: Senseval-2 (SE2), Senseval-3 (SE3), SemEval-2007 (SE07), SemEval-2013 (SE13) and SemEval-2015 (SE15). Following Luo et al. (2018a), Luo et al. (2018b) and Raganato et al. (2017a), we choose SE07, the smallest among these test sets, as the development set. WordNet Since Raganato et al. (2017b) map all the sense annotations in these datasets from their original versions to WordNet 3.0, we extract word sense glosses from WordNet 3.0.",
      "Dev Test Datasets Concatenation of Test Datasets System SE07 SE2 SE3 SE13 SE15 Noun Verb Adj Adv All MFS baseline 54.5 65.6 66.0 63.8 67.1 67.7 49.8 73.1 80.5 65.5 Leskext+emb 56.7 63.0 63.7 66.2 64.6 70.0 51.1 51.7 80.6 64.2 Babelfy 51.6 67.0 63.5 66.4 70.3 68.9 50.7 73.2 79.8 66.4 IMS 61.3 70.9 69.3 65.3 69.5 70.5 55.8 75.6 82.9 68.9 IMS+emb 62.6 72.2 70.4 65.9 71.5 71.9 56.6 75.9 84.7 70.1 Bi-LSTM - 71.1 68.",
      "6 82.9 68.9 IMS+emb 62.6 72.2 70.4 65.9 71.5 71.9 56.6 75.9 84.7 70.1 Bi-LSTM - 71.1 68.4 64.8 68.3 69.5 55.9 76.2 82.4 68.4 Bi-LSTM+att.+LEX+P OS 64.8 72.0 69.1 66.9 71.5 71.5 57.5 75.0 83.8 69.9 GASext (Linear) - 72.4 70.1 67.1 72.1 71.9 58.1 76.4 84.7 70.4 GASext (Concatenation) - 72.2 70.5 67.2 72.6 72.2 57.7 76.6 85.0 70.6 CANs - 72.2 70.2 69.1 72.2 73.5 56.",
      "2 70.5 67.2 72.6 72.2 57.7 76.6 85.0 70.6 CANs - 72.2 70.2 69.1 72.2 73.5 56.5 76.6 80.3 70.9 HCAN - 72.8 70.3 68.5 72.8 72.7 58.2 77.4 84.1 71.1 SemCor, hypernyms (single) - - - - - - - - - 75.6 SemCor, hypernyms (ensemble)\u2020 69.5 77.5 77.4 76.0 78.3 79.6 65.9 79.5 85.5 76.7 SemCor+WNGC, hypernyms (single)\u2021 - - - - - - - - - 77.1 SemCor+WNGC, hypernyms (ensemble)\u2020 \u2021 73.4 79.7 77.8 78.7 82.6 81.4 68.7 83.",
      "1 SemCor+WNGC, hypernyms (ensemble)\u2020 \u2021 73.4 79.7 77.8 78.7 82.6 81.4 68.7 83.7 85.5 79.0 BERT(Token-CLS) 61.1 69.7 69.4 65.8 69.5 70.5 57.1 71.6 83.5 68.6 GlossBERT(Sent-CLS) 69.2 76.5 73.4 75.1 79.5 78.3 64.8 77.6 83.8 75.8 GlossBERT(Token-CLS) 71.9 77.0 75.4 74.6 79.3 78.3 66.5 78.6 84.4 76.3 GlossBERT(Sent-CLS-WS) 72.5 77.7 75.2 76.1 80.4 79.3 66.9 78.2 86.4 77.",
      "5 78.6 84.4 76.3 GlossBERT(Sent-CLS-WS) 72.5 77.7 75.2 76.1 80.4 79.3 66.9 78.2 86.4 77.0 Table 3: F1-score (%) for \ufb01ne-grained English all-words WSD on the test sets in the framework of Raganato et al. (2017b) (including the development set SE07). The six blocks list the MFS baseline, two knowledge-based sys- tems, two traditional word expert supervised systems, six recent neural-based systems, one BERT feature-based system and our systems, respectively. Results in \ufb01rst three blocks come from Raganato et al. (2017b), and others from the corresponding papers. \u2020 values are ensemble systems and \u2021 values are models trained on both SemCor and WNGC. Bold font indicates best single model system trained on SemCor, i.e. excludes \u2020 \u2021 values since it is meaningless to compare ensemble systems and models trained on two training sets with our single model trained on SemCor training set only.",
      "Bold font indicates best single model system trained on SemCor, i.e. excludes \u2020 \u2021 values since it is meaningless to compare ensemble systems and models trained on two training sets with our single model trained on SemCor training set only. . 3.2 Settings We use the pre-trained uncased BERTBASE model4 for \ufb01ne-tuning, because we \ufb01nd that BERTLARGE model performs slightly worse than BERTBASE in this task. The number of Trans- former blocks is 12, the number of the hidden layer is 768, the number of self-attention heads is 12, and the total number of parameters of the pre-trained model is 110M. When \ufb01ne-tuning, we use the development set (SE07) to \ufb01nd the optimal settings for our experiments. We keep the dropout probability at 0.1, set the number of epochs to 4. The initial learning rate is 2e-5, and the batch size is 64. 3.3 Results Table 3 shows the performance of our method on the English all-words WSD benchmark datasets. We compare our approach with previous methods.",
      "The initial learning rate is 2e-5, and the batch size is 64. 3.3 Results Table 3 shows the performance of our method on the English all-words WSD benchmark datasets. We compare our approach with previous methods. The \ufb01rst block shows the MFS baseline, which selects the most frequent sense in the training cor- pus for each target word. The second block shows two knowledge-based systems. Leskext+emb (Basile et al., 2014) is a variant of Lesk algorithm (Lesk, 1986) by calcu- 4https://storage.googleapis.com/bert models/2018 10 18/ uncased L-12 H-768 A-12.zip lating the gloss-context overlap of the target word. Babelfy (Moro et al., 2014) is a uni\ufb01ed graph- based approach which exploits the semantic net- work structure from BabelNet. The third block shows two word expert tradi- tional supervised systems. IMS (Zhong and Ng, 2010) is a \ufb02exible framework which trains SVM classi\ufb01ers and uses local features.",
      "The third block shows two word expert tradi- tional supervised systems. IMS (Zhong and Ng, 2010) is a \ufb02exible framework which trains SVM classi\ufb01ers and uses local features. And IMS+emb (Iacobacci et al., 2016) is the best con\ufb01guration of the IMS framework, which also integrates word embeddings as features. The fourth block shows several re- cent neural-based methods. Bi-LSTM (K\u02daageb\u00a8ack and Salomonsson, 2016) is a baseline for neural models. Bi-LSTM+att.+LEX+P OS (Raganato et al., 2017a) is a multi-task learning framework for WSD, POS tagging, and LEX with self-attention mechanism, which converts WSD to a sequence learning task. GASext (Luo et al., 2018b) is a variant of GAS which is a gloss-augmented variant of the memory network by extending gloss knowledge. CANs and HCAN (Luo et al., 2018a) are sentence-level and hierar- chical co-attention neural network models which leverage gloss knowledge.",
      "CANs and HCAN (Luo et al., 2018a) are sentence-level and hierar- chical co-attention neural network models which leverage gloss knowledge. The \ufb01fth block are feature-based BERT mod- els (Vial et al., 2019) which exploit the semantic relationships between senses such as synonymy,",
      "hypernymy and hyponymy, and use pre-trained BERT embeddings and transformer encoder lay- ers. It is worth noting that our \ufb01ne-tuning method is superior to their feature-based method under the same experimental setup (single model + SemCor training set). In the last block, we report the performance of our method. BERT(Token-CLS) is our baseline, which does not incorporate gloss information, and it performs slightly worse than previous traditional supervised methods and recent neural-based meth- ods. It proves that directly using BERT cannot ob- tain performance growth. The other three methods outperform other models by a substantial margin, which proves that the improvements come from leveraging BERT to better exploit gloss informa- tion. It is worth noting that our method achieves signi\ufb01cant improvements in SE07 and Verb over previous methods, which have the highest ambi- guity level among all datasets and all POS tags re- spectively according to Raganato et al. (2017b). Moreover, GlossBERT(Token-CLS) performs better than GlossBERT(Sent-CLS), which proves that highlighting the target word in the sentence is important.",
      "(2017b). Moreover, GlossBERT(Token-CLS) performs better than GlossBERT(Sent-CLS), which proves that highlighting the target word in the sentence is important. However, the weakly highlight- ing method GlossBERT(Sent-CLS-WS) performs best in most circumstances, which may result from its combination of the advantages of the other two methods. 3.4 Discussion There are two main reasons for the great improve- ments of our experimental results. First, we con- struct context-gloss pairs and convert WSD prob- lem to a sentence-pair classi\ufb01cation task which is similar to NLI tasks and train only one classi\ufb01er, which is equivalent to expanding the corpus. Sec- ond, we leverage BERT (Devlin et al., 2018) to better exploit the gloss information. BERT model shows its advantage in dealing with sentence-pair classi\ufb01cation tasks by its amazing improvement on QA and NLI tasks. This advantage comes from both of its two novel unsupervised prediction tasks.",
      "BERT model shows its advantage in dealing with sentence-pair classi\ufb01cation tasks by its amazing improvement on QA and NLI tasks. This advantage comes from both of its two novel unsupervised prediction tasks. Compared with traditional word expert super- vised methods, our GlossBERT shows its effec- tiveness to alleviate the effort of feature engineer- ing and does not require training a dedicated clas- si\ufb01er for every target lemma. Compared with re- cent neural-based methods, our solution is more intuitive and can make better use of gloss knowl- edge. Besides, our approach demonstrates that when we \ufb01ne-tune BERT on a downstream task, converting it into a sentence-pair classi\ufb01cation task may be a good choice. 4 Conclusion In this paper, we seek to better leverage gloss knowledge in a supervised neural WSD system. We propose a new solution to WSD by construct- ing context-gloss pairs and then converting WSD to a sentence-pair classi\ufb01cation task.",
      "4 Conclusion In this paper, we seek to better leverage gloss knowledge in a supervised neural WSD system. We propose a new solution to WSD by construct- ing context-gloss pairs and then converting WSD to a sentence-pair classi\ufb01cation task. We \ufb01ne-tune the pre-trained BERT model on SemCor3.0 train- ing corpus, and the experimental results on several English all-words WSD benchmark datasets show that our approach outperforms the state-of-the-art systems. Acknowledgments We would like to thank the anonymous reviewers for their valuable comments. The research work is supported by National Natural Science Foundation of China (No. 61751201 and 61672162), Shang- hai Municipal Science and Technology Commis- sion (16JC1420401 and 17JC1404100), Shanghai Municipal Science and Technology Major Project (No.2018SHZDZX01) and ZJLab. References Eneko Agirre, Oier L\u00b4opez de Lacalle, and Aitor Soroa. 2014. Random walks for knowledge-based word sense disambiguation.",
      "References Eneko Agirre, Oier L\u00b4opez de Lacalle, and Aitor Soroa. 2014. Random walks for knowledge-based word sense disambiguation. Computational Linguistics, 40(1):57\u201384. Satanjeev Banerjee and Ted Pedersen. 2002. An adapted lesk algorithm for word sense disambigua- tion using wordnet. In International conference on intelligent text processing and computational lin- guistics, pages 136\u2013145. Springer. Pierpaolo Basile, Annalina Caputo, and Giovanni Se- meraro. 2014. An enhanced lesk word sense dis- ambiguation algorithm through a distributional se- mantic model. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1591\u20131600. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805.",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805. Ignacio Iacobacci, Mohammad Taher Pilehvar, and Roberto Navigli. 2016. Embeddings for word sense disambiguation: An evaluation study. In Proceed- ings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers), volume 1, pages 897\u2013907.",
      "Mikael K\u02daageb\u00a8ack and Hans Salomonsson. 2016. Word sense disambiguation using a bidirectional lstm. arXiv preprint arXiv:1606.03568. Michael Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In Proceedings of the 5th annual international conference on Systems documentation, pages 24\u201326. Citeseer. Fuli Luo, Tianyu Liu, Zexue He, Qiaolin Xia, Zhi- fang Sui, and Baobao Chang. 2018a. Leveraging gloss knowledge in neural word sense disambigua- tion by hierarchical co-attention. In Proceedings of the 2018 Conference on Empirical Methods in Nat- ural Language Processing, pages 1402\u20131411. Fuli Luo, Tianyu Liu, Qiaolin Xia, Baobao Chang, and Zhifang Sui. 2018b. Incorporating glosses into neural word sense disambiguation.",
      "Fuli Luo, Tianyu Liu, Qiaolin Xia, Baobao Chang, and Zhifang Sui. 2018b. Incorporating glosses into neural word sense disambiguation. arXiv preprint arXiv:1805.08028. George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39\u2013 41. Andrea Moro, Alessandro Raganato, and Roberto Nav- igli. 2014. Entity linking meets word sense disam- biguation: a uni\ufb01ed approach. Transactions of the Association for Computational Linguistics, 2:231\u2013 244. Roberto Navigli. 2009. Word sense disambiguation: A survey. ACM computing surveys (CSUR), 41(2):10. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations.",
      "ACM computing surveys (CSUR), 41(2):10. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. In Proceedings of the 2018 Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long Papers), pages 2227\u20132237. Alessandro Raganato, Claudio Delli Bovi, and Roberto Navigli. 2017a. Neural sequence learning models for word sense disambiguation. In Proceedings of the 2017 Conference on Empirical Methods in Nat- ural Language Processing, pages 1156\u20131167. Alessandro Raganato, Jose Camacho-Collados, and Roberto Navigli. 2017b. Word sense disambigua- tion: A uni\ufb01ed evaluation framework and empiri- cal comparison.",
      "Alessandro Raganato, Jose Camacho-Collados, and Roberto Navigli. 2017b. Word sense disambigua- tion: A uni\ufb01ed evaluation framework and empiri- cal comparison. In Proceedings of the 15th Confer- ence of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 99\u2013110. Hui Shen, Razvan Bunescu, and Rada Mihalcea. 2013. Coarse to \ufb01ne grained sense disambiguation in wikipedia. In Second Joint Conference on Lexical and Computational Semantics (* SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, volume 1, pages 22\u201331. Chi Sun, Luyao Huang, and Xipeng Qiu. 2019. Utiliz- ing bert for aspect-based sentiment analysis via con- structing auxiliary sentence. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, Volume 1 (Long and Short Papers), pages 380\u2013385.",
      "Utiliz- ing bert for aspect-based sentiment analysis via con- structing auxiliary sentence. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, Volume 1 (Long and Short Papers), pages 380\u2013385. Lo\u00a8\u0131c Vial, Benjamin Lecouteux, and Didier Schwab. 2019. Sense vocabulary compression through the semantic knowledge of wordnet for neu- ral word sense disambiguation. arXiv preprint arXiv:1905.05677. Zhi Zhong and Hwee Tou Ng. 2010. It makes sense: A wide-coverage word sense disambiguation system for free text. In Proceedings of the ACL 2010 system demonstrations, pages 78\u201383."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1908.07245.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":6319,
  "avg_doclen":170.7837837838,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1908.07245.pdf"
    }
  }
}