{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Gating Mechanisms for Combining Character and Word-level Word Representations: An Empirical Study Jorge A. Balazs and Yutaka Matsuo Graduate School of Engineering The University of Tokyo {jorge, matsuo}@weblab.t.u-tokyo.ac.jp Abstract In this paper we study how different ways of combining character and word-level rep- resentations affect the quality of both \ufb01nal word and sentence representations. We pro- vide strong empirical evidence that model- ing characters improves the learned represen- tations at the word and sentence levels, and that doing so is particularly useful when repre- senting less frequent words. We further show that a feature-wise sigmoid gating mechanism is a robust method for creating representa- tions that encode semantic similarity, as it per- formed reasonably well in several word sim- ilarity datasets. Finally, our \ufb01ndings suggest that properly capturing semantic similarity at the word level does not consistently yield im- proved performance in downstream sentence- level tasks. Our code is available at https: //github.com/jabalazs/gating.",
      "Finally, our \ufb01ndings suggest that properly capturing semantic similarity at the word level does not consistently yield im- proved performance in downstream sentence- level tasks. Our code is available at https: //github.com/jabalazs/gating. 1 Introduction Incorporating sub-word structures like substrings, morphemes and characters to the creation of word representations signi\ufb01cantly increases their qual- ity as re\ufb02ected both by intrinsic metrics and performance in a wide range of downstream tasks (Bojanowski et al., 2017; Luong and Man- ning, 2016; Wu et al., 2016; Ling et al., 2015). The reason for this improvement is related to sub-word structures containing information that is usually ignored by standard word-level models. Indeed, when representing words as vectors ex- tracted from a lookup table, semantically related words resulting from in\ufb02ectional processes such as surf, sur\ufb01ng, and surfed, are treated as being independent from one another1.",
      "Indeed, when representing words as vectors ex- tracted from a lookup table, semantically related words resulting from in\ufb02ectional processes such as surf, sur\ufb01ng, and surfed, are treated as being independent from one another1. Further, word- level embeddings do not account for derivational 1Unless using pre-trained embeddings with a notion of sub- word information such as fastText (Bojanowski et al., 2017) processes resulting in syntactically-similar words with different meanings such as break, break- able, and unbreakable. This causes derived words, which are usually less frequent, to have lower- quality (or no) vector representations. Previous works have successfully combined character-level and word-level word representa- tions, obtaining overall better results than using only word-level representations. For example Lu- ong and Manning (2016) achieved state-of-the-art results in a machine translation task by represent- ing unknown words as a composition of their char- acters.",
      "For example Lu- ong and Manning (2016) achieved state-of-the-art results in a machine translation task by represent- ing unknown words as a composition of their char- acters. Botha and Blunsom (2014) created word representations by adding the vector representa- tions of the words\u2019 surface forms and their mor- phemes (\u2212\u2212\u2212\u2212\u2212\u2192 perfectly = \u2212\u2212\u2212\u2212\u2212\u2212\u2192 perfectly + \u2212\u2212\u2212\u2212\u2212\u2192 perfect + \u2212\u2192 ly), obtaining signi\ufb01cant improvements on intrinsic evaluation tasks, word similarity and machine translation. Lample et al. (2016) concatenated character-level and word-level representations for creating word representations, and then used them as input to their models for obtaining state-of-the- art results in Named Entity Recognition on several languages. What these works have in common is that the models they describe \ufb01rst learn how to repre- sent subword information, at character (Luong and Manning, 2016), morpheme (Botha and Blunsom, 2014), or substring (Bojanowski et al., 2017) lev- els, and then combine these learned representa- tions at the word level.",
      "The incorporation of in- formation at a \ufb01ner-grained hierarchy results in higher-quality modeling of rare words, morpho- logical processes, and semantics (Avraham and Goldberg, 2017). There is no consensus, however, on which com- bination method works better in which case, or how the choice of a combination method affects downstream performance, either measured intrin- sically at the word level, or extrinsically at the sen- arXiv:1904.05584v1  [cs.CL]  11 Apr 2019",
      "tence level. In this paper we aim to provide some intuitions about how the choice of mechanism for combining character-level with word-level representations in- \ufb02uences the quality of the \ufb01nal word representa- tions, and the subsequent effect these have in the performance of downstream tasks. Our contribu- tions are as follows: \u2022 We show that a feature-wise sigmoidal gating mechanism is the best at combining represen- tations at the character and word-level hierar- chies, as measured by word similarity tasks. \u2022 We provide evidence that this mechanism learns that to properly model increasingly in- frequent words, it has to increasingly rely on character-level information. \u2022 We \ufb01nally show that despite the increased ex- pressivity of word representations it offers, it has no clear effect in sentence represen- tations, as measured by sentence evaluation tasks. 2 Background We are interested in studying different ways of combining word representations, obtained from different hierarchies, into a single word represen- tation.",
      "2 Background We are interested in studying different ways of combining word representations, obtained from different hierarchies, into a single word represen- tation. Speci\ufb01cally, we want to study how combin- ing word representations (1) taken directly from a word embedding lookup table, and (2) obtained from a function over the characters composing them, affects the quality of the \ufb01nal word repre- sentations. Let W be a set, or vocabulary, of words with |W| elements, and C a vocabulary of characters with |C| elements. Further, let x = w1, . . . , wn; wi \u2208W be a sequence of words, and ci = ci 1, . . . , ci m; ci j \u2208C be the se- quence of characters composing wi.",
      "Further, let x = w1, . . . , wn; wi \u2208W be a sequence of words, and ci = ci 1, . . . , ci m; ci j \u2208C be the se- quence of characters composing wi. Each token wi can be represented as a vector v(w) i \u2208Rd ex- tracted directly from an embedding lookup table E(w) \u2208R|W|\u00d7d, pre-trained or otherwise, and as a vector v(c) i \u2208Rd built from the characters that compose it; in other words, v(c) i = f(ci), where f is a function that maps a sequence of characters to a vector. The methods for combining word and character- level representations we study, are of the form G(v(w) i , v(c) i ) = vi where vi is the \ufb01nal word rep- resentation. 2.1 Mapping Characters to Character-level Word Representations The function f is composed of an embedding layer, an optional context function, and an aggre- gation function.",
      "2.1 Mapping Characters to Character-level Word Representations The function f is composed of an embedding layer, an optional context function, and an aggre- gation function. The embedding layer transforms each char- acter ci j into a vector ri j of dimension dr, by directly taking it from a trainable embed- ding lookup table E(c) \u2208 R|C|\u00d7dr. We de- \ufb01ne the matrix representation of word wi as Ci = [ri 1, . . . , ri m], Ci \u2208Rm\u00d7dr. The context function takes Ci as input and returns a context-enriched matrix representation Hi = [hi 1, . . . , hi m], Hi \u2208Rm\u00d7dh, in which each hi j contains a measure of information about its context, and interactions with its neighbors. In particular, we chose to do this by feeding Ci to a Bidirectional LSTM (BiLSTM) (Graves and Schmidhuber, 2005; Graves et al., 2013)2.",
      "In particular, we chose to do this by feeding Ci to a Bidirectional LSTM (BiLSTM) (Graves and Schmidhuber, 2005; Graves et al., 2013)2. Informally, we can think of a Long Short- Term Memory Network (LSTM) (Hochre- iter and Schmidhuber, 1997) as a func- tion Rm\u00d7dr \u2192 Rm\u00d7dh that takes a matrix C = [r1, . . . , rm] as input and returns a context-enriched matrix representation H = [h1, . . . , hm], where each hj encodes information about the previous elements h1, . . . , hj\u221213. A BiLSTM is simply composed of 2 LSTMs, one that reads the input from left to right (for- ward), and another that does so from right to left (backward). The output of the forward and backward LSTMs are \u2212\u2192 H = [\u2212\u2192 h 1, . . . , \u2212\u2192 h m] and \u2190\u2212 H = [\u2190\u2212 h 1, . . .",
      "The output of the forward and backward LSTMs are \u2212\u2192 H = [\u2212\u2192 h 1, . . . , \u2212\u2192 h m] and \u2190\u2212 H = [\u2190\u2212 h 1, . . . , \u2190\u2212 h m] respectively. In the backward case the LSTM reads rm \ufb01rst and r1 last, therefore \u2190\u2212 h j will encode the context from \u2190\u2212 h j+1, . . . , \u2190\u2212 h m. The aggregation function takes the context- enriched matrix representation of word wi for both directions, \u2212\u2192 Hi and \u2190\u2212 Hi, and returns a single vector v(c) i \u2208Rdh. To do so we followed Miyamoto and Cho (2016), and de\ufb01ned the character-level repre- sentation v(c) i of word wi as the linear combination of the forward and backward last hidden states re- 2Other methods for encoding the characters\u2019 context, such as CNNs (Kim et al., 2016), could also be used.",
      "3In terms of implementation, the LSTM is applied iteratively to each element of the input sequence regardless of dimen- sion m, which means it accepts inputs of variable length, but we will use this notation for the sake of simplicity.",
      "s GloVe GloVe f Linear (d x 1) g x c + (1 - g) x w GloVe f GloVe Linear (d x d) g    c + (1 - g)     w w c c w w c c w c a t s c a t c a t s c c a t s a t s w c Character vector representation Pre-trained word vector representation Character-level word representation Gated word-character representation f f Figure 1: Character and Word-level combination methods. turned by the context function: v(c) i = W (c)[ \u2212\u2192 him; \u2190\u2212 hi1] + b(c) (1) where W (c) \u2208Rdh\u00d72dh and b(c) \u2208Rdh are train- able parameters, and [\u25e6; \u25e6] represents the concate- nation operation between two vectors. 2.2 Combining Character and Word-level Representations We tested three different methods for combining v(c) i with v(w) i : simple concatenation, a learned scalar gate (Miyamoto and Cho, 2016), and a learned vector gate (also referred to as feature- wise sigmoidal gate).",
      "Additionally, we compared these methods to two baselines: using pre-trained word vectors only, and using character-only fea- tures for representing words. See \ufb01g. 1 for a visual description of the proposed methods. word-only (w) considers only v(w) i and ig- nores v(c) i : vi = v(w) i (2) char-only (c) considers only v(c) i and ig- nores v(w) i : vi = v(c) i (3) concat (cat) concatenates both word and character-level representations: vi = [v(c) i ; v(w) i ] (4) scalar gate (sg) implements the scalar gating mechanism described by Miyamoto and Cho (2016): gi = \u03c3(w\u22a4v(w) i + b) (5) vi = giv(c) i + (1 \u2212gi)v(w) i (6) where w \u2208Rd and b \u2208R are trainable parameters, gi \u2208(0, 1), and \u03c3 is the sigmoid function.",
      "vector gate (vg): gi = \u03c3(W v(w) i + b) (7) vi = gi \u2299v(c) i + (1 \u2212gi) \u2299v(w) i (8) where W \u2208Rd\u00d7d and b \u2208Rd are trainable pa- rameters, gi \u2208(0, 1)d, \u03c3 is the element-wise sig- moid function, \u2299is the element-wise product for vectors, and 1 \u2208Rd is a vector of ones. The vector gate is inspired by Miyamoto and Cho (2016) and Yang et al. (2017), but is different to the former in that the gating mechanism acts upon each dimension of the word and character- level vectors, and different to the latter in that it does not rely on external sources of information for calculating the gating mechanism. Finally, note that word only and char only are special cases of both gating mecha- nisms: gi = 0 (scalar gate) and gi = 0 (vec- tor gate) correspond to word only; gi = 1 and gi = 1 correspond to char only.",
      "Finally, note that word only and char only are special cases of both gating mecha- nisms: gi = 0 (scalar gate) and gi = 0 (vec- tor gate) correspond to word only; gi = 1 and gi = 1 correspond to char only. 2.3 Obtaining Sentence Representations To enable sentence-level classi\ufb01cation we need to obtain a sentence representation from the word vectors vi. We achieved this by using a BiLSTM with max pooling, which was shown to be a good universal sentence encoding mechanism (Conneau et al., 2017). Let x = w1, . . . , wn, be an input sentence and V = [v1, . . . , vn] its matrix representation, where each vi was obtained by one of the methods de- scribed in section 2.2. S = [s1, . . . , sn] is the",
      "context-enriched matrix representation of x ob- tained by feeding V to a BiLSTM of output di- mension ds4. Lastly, s \u2208Rds is the \ufb01nal sen- tence representation of x obtained by max-pooling S along the sequence dimension. Finally, we initialized the word representations v(w) i using GloVe embeddings (Pennington et al., 2014), and \ufb01ne-tuned them during training. Refer to appendix A for details on the other hyperparam- eters we used. 3 Experiments 3.1 Experimental Setup We trained our models for solving the Nat- ural Language Inference (NLI) task in two datasets, SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018), and validated them in each corresponding development set (in- cluding the matched and mismatched development sets of MultiNLI). For each dataset-method combination we trained 7 models initialized with different random seeds, and saved each when it reached its best val- idation accuracy5.",
      "For each dataset-method combination we trained 7 models initialized with different random seeds, and saved each when it reached its best val- idation accuracy5. We then evaluated the quality of each trained model\u2019s word representations vi in 10 word similarity tasks, using the system created by Jastrzebski et al. (2017)6. Finally, we fed these obtained word vectors to a BiLSTM with max-pooling and evaluated the \ufb01nal sentence representations in 11 downstream trans- fer tasks (Conneau et al., 2017; Subramanian et al., 2018). 3.2 Datasets Word-level Semantic Similarity A desirable property of vector representations of words is that semantically similar words should have similar vector representations. Assessing whether a set of word representations possesses this quality is re- ferred to as the semantic similarity task. This is the most widely-used evaluation method for eval- uating word representations, despite its shortcom- ings (Faruqui et al., 2016).",
      "Assessing whether a set of word representations possesses this quality is re- ferred to as the semantic similarity task. This is the most widely-used evaluation method for eval- uating word representations, despite its shortcom- ings (Faruqui et al., 2016). This task consists of comparing the similar- ity between word vectors measured by a distance 4si = [\u2212\u2192 si; \u2190\u2212 si] for each i, and both \u2212\u2192 si and \u2190\u2212 si \u2208R ds 2 . 5We found that models validated on the matched development set of MultiNLI, rather than the mismatched, yielded best results, although the differences were not statistically signif- icant. 6https://github.com/kudkudak/word-embeddings-benchmarks/ tree/8fd0489 metric (usually cosine distance), with a similarity score obtained from human judgements. High cor- relation between these similarities is an indicator of good performance. A problem with this formulation though, is that the de\ufb01nition of \u201csimilarity\u201d often confounds the meaning of both similarity and relatedness.",
      "High cor- relation between these similarities is an indicator of good performance. A problem with this formulation though, is that the de\ufb01nition of \u201csimilarity\u201d often confounds the meaning of both similarity and relatedness. For example, cup and tea are related but dissimilar words, and this type of distinction is not always clear (Agirre et al., 2009; Hill et al., 2015). To face the previous problem, we tested our methods in a wide variety of datasets, including some that explicitly model relatedness (WS353R), some that explicitly consider similarity (WS353S, SimLex999, SimVerb3500), and some where the distinction is not clear (MEN, MTurk287, MTurk771, RG, WS353). We also included the RareWords (RW) dataset for evaluating the qual- ity of rare word representations. See appendix B for a more complete description of the datasets we used. Sentence-level Evaluation Tasks Unlike word- level representations, there is no consensus on the desirable properties sentence representations should have. In response to this, Conneau et al.",
      "See appendix B for a more complete description of the datasets we used. Sentence-level Evaluation Tasks Unlike word- level representations, there is no consensus on the desirable properties sentence representations should have. In response to this, Conneau et al. (2017) created SentEval7, a sentence representa- tion evaluation benchmark designed for assessing how well sentence representations perform in vari- ous downstream tasks (Conneau and Kiela, 2018). Some of the datasets included in SentEval cor- respond to sentiment classi\ufb01cation (CR, MPQA, MR, SST2, and SST5), subjectivity classi\ufb01cation (SUBJ), question-type classi\ufb01cation (TREC), rec- ognizing textual entailment (SICK E), estimating semantic relatedness (SICK R), and measuring textual semantic similarity (STS16, STSB). The datasets are described by Conneau et al. (2017), and we provide pointers to their original sources in the appendix table B.2. To evaluate these sentence representations SentEval trained a linear model on top of them, and evaluated their performance in the validation sets accompanying each dataset.",
      "The datasets are described by Conneau et al. (2017), and we provide pointers to their original sources in the appendix table B.2. To evaluate these sentence representations SentEval trained a linear model on top of them, and evaluated their performance in the validation sets accompanying each dataset. The only excep- tion was the STS16 task, in which our representa- tions were evaluated directly. 4 Word-level Evaluation 4.1 Word Similarity Table 1 shows the quality of word representations in terms of the correlation between word similarity 7https://github.com/facebookresearch/SentEval/tree/906b34a",
      "MEN MTurk287 MTurk771 RG65 RW SimLex999 SimVerb3500 WS353 WS353R WS353S SNLI w 71.78 35.40 49.05 61.80 18.43 19.17 10.32 39.27 28.01 53.42 c 9.85 -5.65 0.82 -5.28 17.81 0.86 2.76 -2.20 0.20 -3.87 cat 71.91 35.52 48.84 62.12 18.46 19.10 10.21 39.35 28.16 53.40 sg 70.49 34.49 46.15 59.75 18.24 17.20 8.73 35.86 23.48 50.83 vg 80.00 32.54 62.09 68.90 20.76 37.70 20.45 54.72 47.24 65.60 MNLI w 68.76 50.15 68.81 65.",
      "48 50.83 vg 80.00 32.54 62.09 68.90 20.76 37.70 20.45 54.72 47.24 65.60 MNLI w 68.76 50.15 68.81 65.83 18.43 42.21 25.18 61.10 58.21 70.17 c 4.84 0.06 1.95 -0.06 12.18 3.01 1.52 -4.68 -3.63 -3.65 cat 68.77 50.40 68.77 65.92 18.35 42.22 25.12 61.15 58.26 70.21 sg 67.66 49.58 68.29 64.84 18.36 41.81 24.57 60.13 57.09 69.41 vg 76.69 56.06 70.13 69.00 25.35 48.40 35.12 68.91 64.70 77.",
      "36 41.81 24.57 60.13 57.09 69.41 vg 76.69 56.06 70.13 69.00 25.35 48.40 35.12 68.91 64.70 77.23 Table 1: Word-level evaluation results. Each value corresponds to average Pearson correlation of 7 identical models initialized with different random seeds. Correlations were scaled to the [\u2212100; 100] range for easier reading. Bold values represent the best method per training dataset, per task; underlined values represent the best-performing method per task, independent of training dataset. For each task and dataset, every best-performing method was signi\ufb01cantly different to other methods (p < 0.05), except for w trained in SNLI at the MTurk287 task. Statistical signi\ufb01cance was obtained with a two-sided Welch\u2019s t-test for two independent samples without assuming equal variance (Welch, 1947). scores obtained by the proposed models and word similarity scores de\ufb01ned by humans.",
      "Statistical signi\ufb01cance was obtained with a two-sided Welch\u2019s t-test for two independent samples without assuming equal variance (Welch, 1947). scores obtained by the proposed models and word similarity scores de\ufb01ned by humans. First, we can see that for each task, character only models had signi\ufb01cantly worse performance than every other model trained on the same dataset. The most likely explanation for this is that these models are the only ones that need to learn word representations from scratch, since they have no access to the global semantic knowledge encoded by the GloVe embeddings. Further, bold results show the overall trend that vector gates outperformed the other meth- ods regardless of training dataset. This implies that learning how to combine character and word- level representations at the dimension level pro- duces word vector representations that capture a notion of word similarity and relatedness that is closer to that of humans. Additionally, results from the MNLI row in gen- eral, and underlined results in particular, show that training on MultiNLI produces word repre- sentations better at capturing word similarity. This is probably due to MultiNLI data being richer than that of SNLI.",
      "Additionally, results from the MNLI row in gen- eral, and underlined results in particular, show that training on MultiNLI produces word repre- sentations better at capturing word similarity. This is probably due to MultiNLI data being richer than that of SNLI. Indeed, MultiNLI data was gath- ered from various sources (novels, reports, let- ters, and telephone conversations, among others), rather than the single image captions dataset from which SNLI was created. Exceptions to the previous rule are models eval- uated in MEN and RW. The former case can be explained by the MEN dataset8 containing only words that appear as image labels in the ESP- 8https://staff.fni.uva.nl/e.bruni/MEN Game9 and MIRFLICKR-1M10 image datasets (Bruni et al., 2014), and therefore having data that is more closely distributed to SNLI than to MultiNLI.",
      "More notably, in the RareWords dataset (Lu- ong et al., 2013), the word only, concat, and scalar gate methods performed equally, despite having been trained in different datasets (p > 0.1), and the char only method performed signi\ufb01cantly worse when trained in MultiNLI. The vector gate, however, per- formed signi\ufb01cantly better than its counterpart trained in SNLI. These facts provide evidence that this method is capable of capturing linguis- tic phenomena that the other methods are unable to model. 4.2 Word Frequencies and Gating Values Figure 2 shows that for more common words the vector gate mechanism tends to favor only a few dimensions while keeping a low average gat- ing value across dimensions. On the other hand, values are greater and more homogeneous across dimensions in rarer words. Further, \ufb01g. 3 shows this mechanism assigns, on average, a greater gat- ing value to less frequent words, con\ufb01rming the \ufb01ndings by Miyamoto and Cho (2016), and Yang et al. (2017).",
      "Further, \ufb01g. 3 shows this mechanism assigns, on average, a greater gat- ing value to less frequent words, con\ufb01rming the \ufb01ndings by Miyamoto and Cho (2016), and Yang et al. (2017). In other words, the less frequent the word, the more this mechanism allows the character-level representation to in\ufb02uence the \ufb01nal word repre- sentation, as shown by eq. (8). A possible inter- pretation of this result is that exploiting charac- 9http://www.cs.cmu.edu/\u02dcbiglou/resources/ 10http://press.liacs.nl/mirflickr/",
      "about out up people think sustainable adversely immensely juvenile nominated immoderate inheritor misbehave picket portioned Common Uncommon Rare mean max mean max 0.0 0.2 0.4 0.6 0.8 1.0 Figure 2: Visualization of gating values for 5 common words (freq. \u223c20000), 5 uncommon words (freq. \u223c60), and 5 rare words (freq. \u223c2), appearing in both the RW and MultiNLI datasets. 0 250 500 750 1000 1250 1500 1750 Frequency Rank 0.25 0.30 0.35 0.40 0.45 0.50 0.55 Average Gating Value Figure 3: Average gating values for words appearing in both RW and MultiNLI. Words are sorted by decreas- ing frequency in MultiNLI. ter information becomes increasingly necessary as word-level representations\u2019 quality decrease. Another observable trend in both \ufb01gures is that gating values tend to be low on average. Indeed, it is possible to see in \ufb01g.",
      "Words are sorted by decreas- ing frequency in MultiNLI. ter information becomes increasingly necessary as word-level representations\u2019 quality decrease. Another observable trend in both \ufb01gures is that gating values tend to be low on average. Indeed, it is possible to see in \ufb01g. 3 that the average gating values range from 0.26 to 0.56. This result corrob- orates the \ufb01ndings by Miyamoto and Cho (2016), stating that setting g = 0.25 in eq. (6), was better than setting it to higher values. In summary, the gating mechanisms learn how to compensate the lack of expressivity of under- represented words by selectively combining their representations with those of characters. 5 Sentence-level Evaluation Table 2 shows the impact that different methods for combining character and word-level word rep- resentations have in the quality of the sentence representations produced by our models.",
      "5 Sentence-level Evaluation Table 2 shows the impact that different methods for combining character and word-level word rep- resentations have in the quality of the sentence representations produced by our models. We can observe the same trend mentioned in section 4.1, and highlighted by the differ- ence between bold values, that models trained in MultiNLI performed better than those trained in SNLI at a statistically signi\ufb01cant level, con\ufb01rm- ing the \ufb01ndings of Conneau et al. (2017). In other words, training sentence encoders on MultiNLI yields more general sentence representations than doing so on SNLI. The two exceptions to the previous trend, SICKE and SICKR, bene\ufb01ted more from models trained on SNLI. We hypothesize this is again due to both SNLI and SICK (Marelli et al., 2014) hav- ing similar data distributions11. Additionally, there was no method that signi\ufb01- cantly outperformed the word only baseline in classi\ufb01cation tasks.",
      "We hypothesize this is again due to both SNLI and SICK (Marelli et al., 2014) hav- ing similar data distributions11. Additionally, there was no method that signi\ufb01- cantly outperformed the word only baseline in classi\ufb01cation tasks. This means that the added ex- pressivity offered by explicitly modeling charac- ters, be it through concatenation or gating, was not signi\ufb01cantly better than simply \ufb01ne-tuning the pre-trained GloVe embeddings for this type of task. We hypothesize this is due to the con\ufb02ation of two effects. First, the fact that morphological processes might not encode important information for solving these tasks; and second, that SNLI and MultiNLI belong to domains that are too dissimi- lar to the domains in which the sentence represen- tations are being tested. On the other hand, the vector gate signif- icantly outperformed every other method in the STSB task when trained in both datasets, and in the STS16 task when trained in SNLI.",
      "On the other hand, the vector gate signif- icantly outperformed every other method in the STSB task when trained in both datasets, and in the STS16 task when trained in SNLI. This again hints at this method being capable of modeling phenomena at the word level, resulting in im- proved semantic representations at the sentence level. 6 Relationship Between Word- and Sentence-level Evaluation Tasks It is clear that the better performance the vector gate had in word similarity tasks did not trans- 11SICK was created from Flickr-8k (Rashtchian et al., 2010), and SNLI from its expanded version: Flickr30k (Young et al., 2014).",
      "Classi\ufb01cation Entailment Relatedness Semantic Textual Similarity CR MPQA MR SST2 SST5 SUBJ TREC SICKE SICKR\u2020 STS16\u2020 STSB\u2020 SNLI w 80.50 84.59 74.18 78.86 42.33 90.38 86.83 86.37 88.52 59.90\u2217 71.29\u2217 c 74.90\u2217 78.86\u2217 65.93\u2217 69.42\u2217 35.56\u2217 82.97\u2217 83.31\u2217 84.13\u2217 83.89\u2217 59.33\u2217 67.20\u2217 cat 80.44 84.66 74.31 78.37 41.34\u2217 90.28 85.80\u2217 86.40 88.44 59.90\u2217 71.24\u2217 sg 80.59 84.60 74.49 79.04 41.63\u2217 90.16 86.00 86.10\u2217 88.57 60.",
      "40 88.44 59.90\u2217 71.24\u2217 sg 80.59 84.60 74.49 79.04 41.63\u2217 90.16 86.00 86.10\u2217 88.57 60.05\u2217 71.34\u2217 vg 80.42 84.66 74.26 78.87 42.38 90.07 85.97 85.67 88.31\u2217 60.92 71.99 MNLI w 83.80 89.13 79.05 83.38 45.21 91.79 89.23 84.92 86.33 66.08 71.96\u2217 c 70.23\u2217 72.19\u2217 62.83\u2217 64.55\u2217 32.47\u2217 79.49\u2217 74.74\u2217 81.53\u2217 75.92\u2217 51.47\u2217 61.74\u2217 cat 83.96 89.12 79.23 83.70 45.",
      "47\u2217 79.49\u2217 74.74\u2217 81.53\u2217 75.92\u2217 51.47\u2217 61.74\u2217 cat 83.96 89.12 79.23 83.70 45.08\u2217 91.92 90.03 85.06 86.45 66.17 71.82\u2217 sg 83.88 89.06 79.22 83.71 45.26 91.66\u2217 88.83\u2217 84.96 86.40 65.49\u2217 71.87\u2217 vg 83.45\u2217 89.05 79.13 83.87 45.88 91.55\u2217 89.49 84.82 86.50 65.75 72.82 Table 2: Experimental results. Each value shown in the table is the average result of 7 identical models initialized with different random seeds. Values represent accuracy (%) unless indicated by \u2020, in which case they represent Pearson correlation scaled to the range [\u2212100, 100] for easier reading.",
      "82 Table 2: Experimental results. Each value shown in the table is the average result of 7 identical models initialized with different random seeds. Values represent accuracy (%) unless indicated by \u2020, in which case they represent Pearson correlation scaled to the range [\u2212100, 100] for easier reading. Bold values represent the best method per training dataset, per task; underlined values represent the best-performing method per task, independent of training dataset. Values marked with an asterisk (\u2217) are signi\ufb01cantly different to the average performance of the best model trained on the same dataset (p < 0.05). Results for every best-performing method trained on one dataset are signi\ufb01cantly different to the best-performing method trained on the other. Statistical signi\ufb01cance was obtained in the same way as described in table 1. late into overall better performance in downstream tasks.",
      "Results for every best-performing method trained on one dataset are signi\ufb01cantly different to the best-performing method trained on the other. Statistical signi\ufb01cance was obtained in the same way as described in table 1. late into overall better performance in downstream tasks. This con\ufb01rms previous \ufb01ndings indicating that intrinsic word evaluation metrics are not good predictors of downstream performance (Tsvetkov et al., 2015; Chiu et al., 2016; Faruqui et al., 2016; Gladkova and Drozd, 2016). Figure 4(b) shows that the word representa- tions created by the vector gate trained in MultiNLI had positively-correlated results within several word-similarity tasks. This hints at the generality of the word representations created by this method when modeling similarity and relat- edness. However, the same cannot be said about sentence-level evaluation performance; there is no clear correlation between word similarity tasks and sentence-evaluation tasks.",
      "This hints at the generality of the word representations created by this method when modeling similarity and relat- edness. However, the same cannot be said about sentence-level evaluation performance; there is no clear correlation between word similarity tasks and sentence-evaluation tasks. This is clearly il- lustrated by performance in the STSBenchmark, the only in which the vector gate was signif- icantly superior, not being correlated with perfor- mance in any word-similarity dataset. This can be interpreted simply as word-level representations capturing word-similarity not being a suf\ufb01cient condition for good performance in sentence-level tasks. In general, \ufb01g. 4 shows that there are no gen- eral correlation effects spanning both training datasets and combination mechanisms. For exam- ple, \ufb01g. 4(a) shows that, for both word-only and concat models trained in SNLI, performance in word similarity tasks correlates positively with performance in most sentence evaluation tasks, however, this does not happen as clearly for the same models trained in MultiNLI (\ufb01g. 4(b)).",
      "4(a) shows that, for both word-only and concat models trained in SNLI, performance in word similarity tasks correlates positively with performance in most sentence evaluation tasks, however, this does not happen as clearly for the same models trained in MultiNLI (\ufb01g. 4(b)). 7 Related Work 7.1 Gating Mechanisms for Combining Characters and Word Representations To the best of our knowledge, there are only two recent works that speci\ufb01cally study how to com- bine word and subword-level vector representa- tions. Miyamoto and Cho (2016) propose to use a trainable scalar gating mechanism capable of learning a weighting scheme for combin- ing character-level and word-level representations. They compared their proposed method to manu- ally weighting both levels; using characters only; words only; or their concatenation. They found that in some datasets a speci\ufb01c manual weight- ing scheme performed better, while in others the learned scalar gate did. Yang et al.",
      "They compared their proposed method to manu- ally weighting both levels; using characters only; words only; or their concatenation. They found that in some datasets a speci\ufb01c manual weight- ing scheme performed better, while in others the learned scalar gate did. Yang et al. (2017) further expand the gating concept by making the mechanism work at a \ufb01ner- grained level, learning how to weight each vector\u2019s dimensions independently, conditioned on exter- nal word-level features such as part-of-speech and named-entity tags. Similarly, they compared their proposed mechanism to using words only, charac- ters only, and a concatenation of both, with and without external features. They found that their vector gate performed better than the other meth- ods in all the reported tasks, and beat the state of the art in two reading comprehension tasks. Both works showed that the gating mechanisms assigned greater importance to character-level rep-",
      "MEN MT287 MT771 RG65 RW SL999 SV3500 WS353 WS353R WS353S CR MPQA MR SICKE SICKR SST2 SST5 STS16 STSB SUBJ TREC MEN MT287 MT771 RG65 RW SL999 SV3500 WS353 WS353R WS353S CR MPQA MR SICKE SICKR SST2 SST5 STS16 STSB SUBJ TREC Word Only MEN MT287 MT771 RG65 RW SL999 SV3500 WS353 WS353R WS353S CR MPQA MR SICKE SICKR SST2 SST5 STS16 STSB SUBJ TREC MEN MT287 MT771 RG65 RW SL999 SV3500 WS353 WS353R WS353S CR MPQA MR SICKE SICKR SST2 SST5 STS16 STSB SUBJ TREC Char Only MEN MT287 MT771 RG65 RW SL999 SV3500 WS353 WS353R WS353S CR MPQA MR SICKE SICKR SST2 SST5 STS16 STSB SUBJ TREC MEN MT287 MT771 RG65 RW SL999 SV3500 WS353 WS353R WS353S CR MPQA MR SICKE SICKR",
      "WS353 WS353R WS353S CR MPQA MR SICKE SICKR SST2 SST5 STS16 STSB SUBJ TREC MEN MT287 MT771 RG65 RW SL999 SV3500 WS353 WS353R WS353S CR MPQA MR SICKE SICKR SST2 SST5 STS16 STSB SUBJ TREC Concat MEN MT287 MT771 RG65 RW SL999 SV3500 WS353 WS353R WS353S CR MPQA MR SICKE SICKR SST2 SST5 STS16 STSB SUBJ TREC MEN MT287 MT771 RG65 RW SL999 SV3500 WS353 WS353R WS353S CR MPQA MR SICKE SICKR SST2 SST5 STS16 STSB SUBJ TREC Scalar Gate MEN MT287 MT771 RG65 RW SL999 SV3500 WS353 WS353R WS353S CR MPQA MR SICKE SICKR SST2 SST5 STS16 STSB SUBJ TREC MEN MT287 MT771 RG65 RW SL999 SV3500 WS353 WS353R WS353S CR MPQA MR SICKE SICKR SST2 SST5 STS16 STSB SUBJ TREC Word-level",
      "SICKR SST2 SST5 STS16 STSB SUBJ TREC MEN MT287 MT771 RG65 RW SL999 SV3500 WS353 WS353R WS353S CR MPQA MR SICKE SICKR SST2 SST5 STS16 STSB SUBJ TREC Word-level Sentence-level Vector Gate 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 (a) Models trained in SNLI. MEN MT287 MT771 RG65 RW SL999 SV3500 WS353 WS353R WS353S CR MPQA MR SICKE SICKR SST2 SST5 STS16 STSB SUBJ TREC MEN MT287 MT771 RG65 RW SL999 SV3500 WS353 WS353R WS353S CR MPQA MR SICKE SICKR SST2 SST5 STS16 STSB SUBJ TREC Word Only MEN MT287 MT771 RG65 RW SL999 SV3500 WS353 WS353R WS353S CR MPQA MR SICKE SICKR SST2 SST5 STS16 STSB SUBJ TREC MEN MT287 MT771 RG65 RW",
      "STS16 STSB SUBJ TREC Word Only MEN MT287 MT771 RG65 RW SL999 SV3500 WS353 WS353R WS353S CR MPQA MR SICKE SICKR SST2 SST5 STS16 STSB SUBJ TREC MEN MT287 MT771 RG65 RW SL999 SV3500 WS353 WS353R WS353S CR MPQA MR SICKE SICKR SST2 SST5 STS16 STSB SUBJ TREC Char Only MEN MT287 MT771 RG65 RW SL999 SV3500 WS353 WS353R WS353S CR MPQA MR SICKE SICKR SST2 SST5 STS16 STSB SUBJ TREC MEN MT287 MT771 RG65 RW SL999 SV3500 WS353 WS353R WS353S CR MPQA MR SICKE SICKR SST2 SST5 STS16 STSB SUBJ TREC Concat MEN MT287 MT771 RG65 RW SL999 SV3500 WS353 WS353R WS353S CR MPQA MR SICKE SICKR SST2 SST5 STS16 STSB SUBJ TREC MEN MT287 MT771 RG65 RW SL999 SV3500 WS353 WS353R WS353S CR",
      "MT771 RG65 RW SL999 SV3500 WS353 WS353R WS353S CR MPQA MR SICKE SICKR SST2 SST5 STS16 STSB SUBJ TREC MEN MT287 MT771 RG65 RW SL999 SV3500 WS353 WS353R WS353S CR MPQA MR SICKE SICKR SST2 SST5 STS16 STSB SUBJ TREC Scalar Gate MEN MT287 MT771 RG65 RW SL999 SV3500 WS353 WS353R WS353S CR MPQA MR SICKE SICKR SST2 SST5 STS16 STSB SUBJ TREC MEN MT287 MT771 RG65 RW SL999 SV3500 WS353 WS353R WS353S CR MPQA MR SICKE SICKR SST2 SST5 STS16 STSB SUBJ TREC Word-level Sentence-level Vector Gate 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 (b) Models trained in MultiNLI. Figure 4: Spearman correlation between performances in word and sentence level evaluation tasks.",
      "00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 (b) Models trained in MultiNLI. Figure 4: Spearman correlation between performances in word and sentence level evaluation tasks. resentations in rare words, and to word-level rep- resentations in common ones, reaf\ufb01rming the pre- vious \ufb01ndings that subword structures in gen- eral, and characters in particular, are bene\ufb01cial for modeling uncommon words. 7.2 Sentence Representation Learning The problem of representing sentences as \ufb01xed- length vectors has been widely studied. Zhao et al. (2015) suggested a self-adaptive hi- erarchical model that gradually composes words into intermediate phrase representations, and adaptively selects speci\ufb01c hierarchical levels for speci\ufb01c tasks. Kiros et al. (2015) proposed an encoder-decoder model trained by attempting to reconstruct the surrounding sentences of an en- coded passage, in a fashion similar to Skip-gram (Mikolov et al., 2013). Hill et al.",
      "Kiros et al. (2015) proposed an encoder-decoder model trained by attempting to reconstruct the surrounding sentences of an en- coded passage, in a fashion similar to Skip-gram (Mikolov et al., 2013). Hill et al. (2016) over- came the previous model\u2019s need for ordered train- ing sentences by using autoencoders for creating the sentence representations. Jernite et al. (2017) implemented a model simpler and faster to train than the previous two, while having competitive performance. Similar to Kiros et al. (2015), Gan et al. (2017) suggested predicting future sentences with a hierarchical CNN-LSTM encoder. Conneau et al. (2017) trained several sentence encoding architectures on a combination of the SNLI and MultiNLI datasets, and showed that a BiLSTM with max-pooling was the best at pro- ducing highly transferable sentence representa- tions. More recently, Subramanian et al.",
      "(2017) trained several sentence encoding architectures on a combination of the SNLI and MultiNLI datasets, and showed that a BiLSTM with max-pooling was the best at pro- ducing highly transferable sentence representa- tions. More recently, Subramanian et al. (2018) empirically showed that sentence representations created in a multi-task setting (Collobert and We- ston, 2008), performed increasingly better the more tasks they were trained in. Zhang et al. (2018) proposed using an autoencoder that relies on multi-head self-attention over the concatena- tion of the max and mean pooled encoder outputs for producing sentence representations. Finally, Wieting and Kiela (2019) show that modern sen- tence embedding methods are not vastly superior to random methods. The works mentioned so far usually evaluate the quality of the produced sentence representa- tions in sentence-level downstream tasks. Com- mon benchmarks grouping these kind of tasks in- clude SentEval (Conneau and Kiela, 2018), and GLUE (Wang et al., 2019).",
      "The works mentioned so far usually evaluate the quality of the produced sentence representa- tions in sentence-level downstream tasks. Com- mon benchmarks grouping these kind of tasks in- clude SentEval (Conneau and Kiela, 2018), and GLUE (Wang et al., 2019). Another trend, how- ever, is to probe sentence representations to un- derstand what linguistic phenomena they encode (Linzen et al., 2016; Adi et al., 2017; Conneau et al., 2018; Perone et al., 2018; Zhu et al., 2018). 7.3 General Feature-wise Transformations Dumoulin et al. (2018) provide a review on feature-wise transformation methods, of which the mechanisms presented in this paper form a part of. In a few words, the g parameter, in both scalar gate and vector gate mechanisms, can be understood as a scaling parameter limited to the (0, 1) range and conditioned on word representa- tions, whereas adding the scaled v(c) i and v(w) i rep- resentations can be seen as biasing word represen- tations conditioned on character representations.",
      "The previous review extends the work by Perez et al. (2018), which describes the Feature-wise Linear Modulation (FiLM) framework as a gener- alization of Conditional Normalization methods, and apply it in visual reasoning tasks. Some of the reported \ufb01ndings are that, in general, scaling has greater impact than biasing, and that in a set- ting similar to the scalar gate, limiting the scaling parameter to (0, 1) hurt performance. Fu- ture decisions involving the design of mechanisms for combining character and word-level represen- tations should be informed by these insights.",
      "8 Conclusions We presented an empirical study showing the ef- fect that different ways of combining character and word representations has in word-level and sentence-level evaluation tasks. We showed that a vector gate performed consis- tently better across a variety of word similarity and relatedness tasks. Additionally, despite showing inconsistent results in sentence evaluation tasks, it performed signi\ufb01cantly better than the other meth- ods in semantic similarity tasks. We further showed through this mechanism, that learning character-level representations is al- ways bene\ufb01cial, and becomes increasingly so with less common words. In the future it would be interesting to study how the choice of mechanism for combining subword and word representations affects the more recent language-model-based pretraining methods such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018, 2019) and BERT (Devlin et al., 2018). Acknowledgements Thanks to Edison Marrese-Taylor and Pablo Loy- ola for their feedback on early versions of this manuscript. We also gratefully acknowledge the support of the NVIDIA Corporation with the do- nation of one of the GPUs used for this research.",
      "Acknowledgements Thanks to Edison Marrese-Taylor and Pablo Loy- ola for their feedback on early versions of this manuscript. We also gratefully acknowledge the support of the NVIDIA Corporation with the do- nation of one of the GPUs used for this research. Jorge A. Balazs is partially supported by the Japanese Government MEXT Scholarship. References Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. 2017. Fine-grained Anal- ysis of Sentence Embeddings Using Auxiliary Pre- diction Tasks. In Proceedings of the 5th Inter- national Conference on Learning Representations (ICLR), Toulon, France. Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pasca, and Aitor Soroa. 2009. A Study on Similarity and Relatedness Using Distribu- tional and WordNet-based Approaches.",
      "Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pasca, and Aitor Soroa. 2009. A Study on Similarity and Relatedness Using Distribu- tional and WordNet-based Approaches. In Proceed- ings of Human Language Technologies: The 2009 Annual Conference of the North American Chap- ter of the Association for Computational Linguis- tics, pages 19\u201327, Boulder, Colorado. Association for Computational Linguistics. Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2016. SemEval-2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation. In Proceedings of the 10th International Workshop on Semantic Eval- uation (SemEval-2016), pages 497\u2013511, San Diego, California. Association for Computational Linguis- tics. Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.",
      "Association for Computational Linguis- tics. Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017. A Simple but Tough-to-Beat Baseline for Sentence Embeddings. In International Conference on Learn- ing Representations. Oded Avraham and Yoav Goldberg. 2017. The Inter- play of Semantics and Morphology in Word Embed- dings. arXiv preprint arXiv:1704.01938. Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching Word Vectors with Subword Information. Transactions of the Associa- tion for Computational Linguistics, 5:135\u2013146. Jan Botha and Phil Blunsom. 2014. Compositional Morphology for Word Representations and Lan- guage Modelling. In Proceedings of the 31st In- ternational Conference on Machine Learning, vol- ume 32 of Proceedings of Machine Learning Re- search, pages 1899\u20131907, Bejing, China. PMLR.",
      "In Proceedings of the 31st In- ternational Conference on Machine Learning, vol- ume 32 of Proceedings of Machine Learning Re- search, pages 1899\u20131907, Bejing, China. PMLR. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A Large An- notated Corpus for Learning Natural Language In- ference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process- ing, pages 632\u2013642, Lisbon, Portugal. Association for Computational Linguistics. Elia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014. Multimodal Distributional Semantics. Journal of Arti\ufb01cial Intelligence Research, 49:1\u201347. Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez- Gazpio, and Lucia Specia. 2017. SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation.",
      "Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez- Gazpio, and Lucia Specia. 2017. SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation. In Proceed- ings of the 11th International Workshop on Seman- tic Evaluation (SemEval-2017), pages 1\u201314, Van- couver, Canada. Association for Computational Lin- guistics. Billy Chiu, Anna Korhonen, and Sampo Pyysalo. 2016. Intrinsic Evaluation of Word Vectors Fails to Predict Extrinsic Performance. In Proceedings of the 1st Workshop on Evaluating Vector-Space Representa- tions for NLP, pages 1\u20136, Berlin, Germany. Associ- ation for Computational Linguistics. Ronan Collobert and Jason Weston. 2008. A Uni- \ufb01ed Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. In Proceedings of the 25th Annual International Con- ference on Machine Learning (ICML 2008), pages 160\u2013167, Helsinki, Finland.",
      "Alexis Conneau and Douwe Kiela. 2018. SentEval: An Evaluation Toolkit for Universal Sentence Represen- tations. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC-2018), Miyazaki, Japan. European Language Resource Association. Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c Barrault, and Antoine Bordes. 2017. Supervised Learning of Universal Sentence Representations from Natural Language Inference Data. In Proceed- ings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 670\u2013680, Copenhagen, Denmark. Association for Computa- tional Linguistics. Alexis Conneau, Germ\u00b4an Kruszewski, Guillaume Lample, Lo\u00a8\u0131c Barrault, and Marco Baroni. 2018. What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic proper- ties. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 2126\u20132136, Melbourne, Australia.",
      "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 2126\u20132136, Melbourne, Australia. Association for Computational Linguis- tics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Un- derstanding. CoRR, abs/1810.04805. Vincent Dumoulin, Ethan Perez, Nathan Schucher, Flo- rian Strub, Harm de Vries, Aaron Courville, and Yoshua Bengio. 2018. Feature-wise transforma- tions. Distill. Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi, and Chris Dyer. 2016. Problems With Evaluation of Word Embeddings Using Word Similarity Tasks. In Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP, pages 30\u2013 35, Berlin, Germany. Association for Computational Linguistics. Christiane Fellbaum, editor. 1998.",
      "In Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP, pages 30\u2013 35, Berlin, Germany. Association for Computational Linguistics. Christiane Fellbaum, editor. 1998. WordNet: an Elec- tronic Lexical Database. MIT Press. Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey- tan Ruppin. 2002. Placing Search in Context: The Concept Revisited. ACM Transactions on Informa- tion Systems, 20(1):116\u2013131. Zhe Gan, Yunchen Pu, Ricardo Henao, Chunyuan Li, Xiaodong He, and Lawrence Carin. 2017. Learning Generic Sentence Representations Using Convolu- tional Neural Networks. In Proceedings of the 2017 Conference on Empirical Methods in Natural Lan- guage Processing, pages 2390\u20132400, Copenhagen, Denmark. Association for Computational Linguis- tics.",
      "2017. Learning Generic Sentence Representations Using Convolu- tional Neural Networks. In Proceedings of the 2017 Conference on Empirical Methods in Natural Lan- guage Processing, pages 2390\u20132400, Copenhagen, Denmark. Association for Computational Linguis- tics. Daniela Gerz, Ivan Vuli\u00b4c, Felix Hill, Roi Reichart, and Anna Korhonen. 2016. SimVerb-3500: A Large- Scale Evaluation Set of Verb Similarity. In Proceed- ings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2173\u20132182, Austin, Texas. Association for Computational Lin- guistics. Anna Gladkova and Aleksandr Drozd. 2016. Intrinsic Evaluations of Word Embeddings: What Can We Do Better? In Proceedings of the 1st Workshop on Eval- uating Vector-Space Representations for NLP, pages 36\u201342, Berlin, Germany. Association for Computa- tional Linguistics. Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. 2013.",
      "In Proceedings of the 1st Workshop on Eval- uating Vector-Space Representations for NLP, pages 36\u201342, Berlin, Germany. Association for Computa- tional Linguistics. Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. 2013. Speech Recognition with Deep Re- current Neural Networks. In Proceedings of the 2013 International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6645\u20136649, Vancouver, Canada. IEEE. Alex Graves and J\u00a8urgen Schmidhuber. 2005. Frame- wise Phoneme Classi\ufb01cation with Bidirectional LSTM and Other Neural Network Architectures. Neural Networks, 18(5-6):602\u2013610. Guy Halawi, Gideon Dror, Evgeniy Gabrilovich, and Yehuda Koren. 2012. Large-scale Learning of Word Relatedness with Constraints. In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201912, pages 1406\u20131414, Beijing, China. ACM.",
      "2012. Large-scale Learning of Word Relatedness with Constraints. In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201912, pages 1406\u20131414, Beijing, China. ACM. Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning Distributed Representations of Sen- tences from Unlabelled Data. In Proceedings of the 2016 Conference of the North American Chap- ter of the Association for Computational Linguis- tics: Human Language Technologies, pages 1367\u2013 1377, San Diego, California. Association for Com- putational Linguistics. Felix Hill, Roi Reichart, and Anna Korhonen. 2015. SimLex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation. Computational Linguistics, 41(4):665\u2013695. Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation, 9(8):1735\u20131780.",
      "Computational Linguistics, 41(4):665\u2013695. Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation, 9(8):1735\u20131780. Minqing Hu and Bing Liu. 2004. Mining and Sum- marizing Customer Reviews. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201904, pages 168\u2013177, Seattle, Washington. ACM. Stanis\u0142aw Jastrzebski, Damian Le\u00b4sniak, and Woj- ciech Marian Czarnecki. 2017. How to evaluate word embeddings? on importance of data ef\ufb01- ciency and simple supervised tasks. arXiv preprint arXiv:1702.02170.",
      "Yacine Jernite, Samuel R. Bowman, and David Sontag. 2017. Discourse-Based Objectives for Fast Unsu- pervised Sentence Representation Learning. CoRR, abs/1705.00557. John D. Hunter. 2007. Matplotlib: A 2D Graphics En- vironment. Computing in Science & Engineering, 9(3):90\u201395. Eric Jones, Travis Oliphant, Pearu Peterson, et al. 2001\u2013. SciPy: Open source scienti\ufb01c tools for Python. Yoon Kim, Yacine Jernite, David Sontag, and Alexan- der Rush. 2016. Character-Aware Neural Language Models. In Proceedings of the 30th AAAI Con- ference on Arti\ufb01cial Intelligence, pages 2741\u20132749, Phoenix, Arizona. Karin Kipper, Anna Korhonen, Neville Ryant, and Martha Palmer. 2008. A large-scale classi\ufb01cation of English verbs. Language Resources and Evaluation, 42(1):21\u201340.",
      "Karin Kipper, Anna Korhonen, Neville Ryant, and Martha Palmer. 2008. A large-scale classi\ufb01cation of English verbs. Language Resources and Evaluation, 42(1):21\u201340. Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-Thought Vectors. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Infor- mation Processing Systems 28, pages 3294\u20133302. Curran Associates, Inc. Guillaume Lample, Miguel Ballesteros, Sandeep Sub- ramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural Architectures for Named Entity Recognition. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 260\u2013270, San Diego, California.",
      "2016. Neural Architectures for Named Entity Recognition. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 260\u2013270, San Diego, California. Association for Computational Linguistics. Xin Li and Dan Roth. 2002. Learning Question Clas- si\ufb01ers. In COLING 2002: The 19th International Conference on Computational Linguistics. Wang Ling, Chris Dyer, Alan W Black, Isabel Tran- coso, Ramon Fermandez, Silvio Amir, Luis Marujo, and Tiago Luis. 2015. Finding Function in Form: Compositional Character Models for Open Vocab- ulary Word Representation. In Proceedings of the 2015 Conference on Empirical Methods in Natu- ral Language Processing, pages 1520\u20131530, Lis- bon, Portugal. Association for Computational Lin- guistics. Tal Linzen, Emmanuel Dupoux, and Yoav Gold- berg. 2016. Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies.",
      "Association for Computational Lin- guistics. Tal Linzen, Emmanuel Dupoux, and Yoav Gold- berg. 2016. Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies. Transac- tions of the Association for Computational Linguis- tics, 4(1):521\u2013535. Minh-Thang Luong and Christopher D. Manning. 2016. Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models. In Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), pages 1054\u20131063, Berlin, Germany. Association for Computational Linguistics. Thang Luong, Richard Socher, and Christopher D. Manning. 2013. Better Word Representations with Recursive Neural Networks for Morphology. In Proceedings of the Seventeenth Conference on Com- putational Natural Language Learning, pages 104\u2013 113, So\ufb01a, Bulgaria. Association for Computational Linguistics.",
      "2013. Better Word Representations with Recursive Neural Networks for Morphology. In Proceedings of the Seventeenth Conference on Com- putational Natural Language Learning, pages 104\u2013 113, So\ufb01a, Bulgaria. Association for Computational Linguistics. Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella bernardi, and Roberto Zampar- elli. 2014. A SICK cure for the evaluation of com- positional distributional semantic models. In Pro- ceedings of the Ninth International Conference on Language Resources and Evaluation (LREC-2014), Reykjavik, Iceland. European Language Resources Association (ELRA). Wes McKinney. 2010. Data Structures for Statistical Computing in Python. In Proceedings of the 9th Python in Science Conference, pages 51 \u2013 56. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed Representa- tions of Words and Phrases and their Composition- ality.",
      "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed Representa- tions of Words and Phrases and their Composition- ality. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Ad- vances in Neural Information Processing Systems 26, pages 3111\u20133119. Curran Associates, Inc. Yasumasa Miyamoto and Kyunghyun Cho. 2016. Gated Word-Character Recurrent Language Model. In Proceedings of the 2016 Conference on Empiri- cal Methods in Natural Language Processing, pages 1992\u20131997, Austin, Texas. Association for Compu- tational Linguistics. Douglas L. Nelson, Cathy L. McEvoy, and Thomas A. Schreiber. 2004. The University of South Florida free association, rhyme, and word fragment norms. Behavior Research Methods, Instruments, & Com- puters, 36(3):402\u2013407.",
      "Douglas L. Nelson, Cathy L. McEvoy, and Thomas A. Schreiber. 2004. The University of South Florida free association, rhyme, and word fragment norms. Behavior Research Methods, Instruments, & Com- puters, 36(3):402\u2013407. Travis E. Oliphant. 2015. Guide to NumPy, 2nd edi- tion. CreateSpace Independent Publishing Platform, USA. Bo Pang and Lillian Lee. 2004. A Sentimental Educa- tion: Sentiment Analysis Using Subjectivity Sum- marization Based on Minimum Cuts. In Proceed- ings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04). Bo Pang and Lillian Lee. 2005. Seeing Stars: Ex- ploiting Class Relationships for Sentiment Catego- rization with Respect to Rating Scales. In Proceed- ings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905), pages 115\u2013 124, Ann Arbor, Michigan. Association for Compu- tational Linguistics.",
      "Adam Paszke, Sam Gross, Soumith Chintala, Gre- gory Chanan, Edward Yang, Zachary DeVito, Zem- ing Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in PyTorch. In NeurIPS Autodiff Workshop, Long Beach, Cali- fornia. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global Vectors for Word Representation. In Proceedings of the 2014 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, Doha, Qatar. Association for Computational Linguistics. Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron Courville. 2018. FiLM: Vi- sual Reasoning with a General Conditioning Layer. In AAAI Conference on Arti\ufb01cial Intelligence, New Orleans, Louisiana. Christian S. Perone, Roberto Silveira, and Thomas S. Paula. 2018. Evaluation of sentence embeddings in downstream and linguistic probing tasks.",
      "In AAAI Conference on Arti\ufb01cial Intelligence, New Orleans, Louisiana. Christian S. Perone, Roberto Silveira, and Thomas S. Paula. 2018. Evaluation of sentence embeddings in downstream and linguistic probing tasks. CoRR, abs/1806.06259. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. In Proceedings of the 2018 Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long Papers), pages 2227\u20132237, New Orleans, Louisiana. Association for Computational Linguistics. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving Language Under- standing by Generative Pre-Training. Technical re- port, OpenAI.",
      "Association for Computational Linguistics. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving Language Under- standing by Generative Pre-Training. Technical re- port, OpenAI. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners. Tech- nical report, OpenAI. Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich, and Shaul Markovitch. 2011. A Word at a Time: Computing Word Relatedness Using Temporal Semantic Analysis. In Proceedings of the 20th International Conference on World Wide Web, WWW \u201911, pages 337\u2013346, Hyderabad, India. Cyrus Rashtchian, Peter Young, Micah Hodosh, and Julia Hockenmaier. 2010. Collecting Image Anno- tations Using Amazon\u2019s Mechanical Turk.",
      "Cyrus Rashtchian, Peter Young, Micah Hodosh, and Julia Hockenmaier. 2010. Collecting Image Anno- tations Using Amazon\u2019s Mechanical Turk. In Pro- ceedings of the NAACL HLT 2010 Workshop on Cre- ating Speech and Language Data with Amazon\u2019s Mechanical Turk, pages 139\u2013147, Los Angeles, Cal- ifornia. Association for Computational Linguistics. Guido van Rossum. 1995. Python Tutorial. Techni- cal Report CS-R9526, Department of Computer Sci- ence, CWI, Amsterdam, The Netherlands. Herbert Rubenstein and John B. Goodenough. 1965. Contextual Correlates of Synonymy. Communica- tions of the ACM, 8(10):627\u2013633. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive Deep Models for Semantic Compositionality Over a Sentiment Tree- bank.",
      "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive Deep Models for Semantic Compositionality Over a Sentiment Tree- bank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Process- ing, pages 1631\u20131642, Seattle, Washington. Asso- ciation for Computational Linguistics. Sandeep Subramanian, Adam Trischler, Yoshua Ben- gio, and Christopher J Pal. 2018. Learning Gen- eral Purpose Distributed Sentence Representations via Large Scale Multi-task Learning. In Interna- tional Conference on Learning Representations. Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Guil- laume Lample, and Chris Dyer. 2015. Evaluation of Word Vector Representations by Subspace Align- ment. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process- ing, pages 2049\u20132054, Lisbon, Portugal. Associa- tion for Computational Linguistics.",
      "2015. Evaluation of Word Vector Representations by Subspace Align- ment. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process- ing, pages 2049\u20132054, Lisbon, Portugal. Associa- tion for Computational Linguistics. Alex Wang, Amanpreet Singh, Julian Michael, Fe- lix Hill, Omer Levy, and Samuel R. Bowman. 2019. GLUE: A Multi-Task Benchmark and Anal- ysis Platform for Natural Language Understanding. In Proceedings of the 7th International Conference on Learning Representations (ICLR), New Orleans, Louisiana. Sida Wang and Christopher Manning. 2012. Baselines and Bigrams: Simple, Good Sentiment and Topic Classi\ufb01cation. In Proceedings of the 50th Annual Meeting of the Association for Computational Lin- guistics (Volume 2: Short Papers), pages 90\u201394, Jeju Island, Korea. Association for Computational Lin- guistics.",
      "In Proceedings of the 50th Annual Meeting of the Association for Computational Lin- guistics (Volume 2: Short Papers), pages 90\u201394, Jeju Island, Korea. Association for Computational Lin- guistics. Michael Waskom, Olga Botvinnik, Drew O\u2019Kane, Paul Hobson, Joel Ostblom, Saulius Lukauskas, David C Gemperline, Tom Augspurger, Yaroslav Halchenko, John B. Cole, Jordi Warmenhoven, Ju- lian de Ruiter, Cameron Pye, Stephan Hoyer, Jake Vanderplas, Santi Villalba, Gero Kunter, Eric Quin- tero, Pete Bachant, Marcel Martin, Kyle Meyer, Alistair Miles, Yoav Ram, Thomas Brunner, Tal Yarkoni, Mike Lee Williams, Constantine Evans, Clark Fitzgerald, Brian, and Adel Qalieh. 2018. mwaskom/seaborn: v0.9.0 (july 2018). Bernard Lewis Welch. 1947. The Generalization of \u201cStudent\u2019s\u201d Problem When Several Different Pop- ulation Variances are Involved.",
      "2018. mwaskom/seaborn: v0.9.0 (july 2018). Bernard Lewis Welch. 1947. The Generalization of \u201cStudent\u2019s\u201d Problem When Several Different Pop- ulation Variances are Involved. Biometrika, 34(1- 2):28\u201335. Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating Expressions of Opinions and Emotions in Language. Language Resources and Evaluation, 39(2):165\u2013210. John Wieting and Douwe Kiela. 2019. No Training Required: Exploring Random Encoders for Sen- tence Classi\ufb01cation. In Proceedings of the 7th Inter- national Conference on Learning Representations (ICLR), New Orleans, Louisiana.",
      "Adina Williams, Nikita Nangia, and Samuel Bow- man. 2018. A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122, New Orleans, Louisiana. Association for Computational Linguistics. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016.",
      "2016. Google\u2019s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. arXiv preprint arXiv:1609.08144. Zhilin Yang, Bhuwan Dhingra, Ye Yuan, Junjie Hu, William W. Cohen, and Ruslan Salakhutdinov. 2017. Words or Characters? Fine-grained Gating for Read- ing Comprehension. In Proceedings of the 5th Inter- national Conference on Learning Representations (ICLR), Toulon, France. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to vi- sual denotations. Transactions of the Association for Computational Linguistics, 2:67\u201378. Minghua Zhang, Yunfang Wu, Weikang Li, and Wei Li. 2018. Learning Universal Sentence Representations with Mean-Max Attention Autoencoder. In Pro- ceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing, pages 4514\u2013 4523, Brussels, Belgium.",
      "2018. Learning Universal Sentence Representations with Mean-Max Attention Autoencoder. In Pro- ceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing, pages 4514\u2013 4523, Brussels, Belgium. Association for Computa- tional Linguistics. Han Zhao, Zhengdong Lu, and Pascal Poupart. 2015. Self-Adaptive Hierarchical Sentence Model. In Pro- ceedings of the 24th International Joint Conference on Arti\ufb01cial Intelligence, pages 4069\u20134076, Buenos Aires, Argentina. AAAI Press. Xunjie Zhu, Tingfeng Li, and Gerard de Melo. 2018. Exploring Semantic Properties of Sentence Embed- dings. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Vol- ume 2: Short Papers), pages 632\u2013637, Melbourne, Australia. Association for Computational Linguis- tics. A Hyperparameters We only considered words that appear at least twice, for each dataset. Those that appeared only once were considered UNK.",
      "Association for Computational Linguis- tics. A Hyperparameters We only considered words that appear at least twice, for each dataset. Those that appeared only once were considered UNK. We used the Treebank Word Tokenizer as implemented in NLTK12 for tokenizing the training and develop- ment datasets. In the same fashion as Conneau et al. (2017), we used a batch size of 64, an SGD optmizer with an initial learning rate of 0.1, and at each epoch divided the learning rate by 5 if the validation ac- curacy decreased. We also used gradient clipping when gradients where > 5. We de\ufb01ned character vector representations as 50-dimensional vectors randomly initialized by sampling from the uniform distribution in the (\u22120.05; 0.05) range. The output dimension of the character-level BiLSTM was 300 per direction, and remained of such size after combining forward and backward representations as depicted in eq. 1.",
      "The output dimension of the character-level BiLSTM was 300 per direction, and remained of such size after combining forward and backward representations as depicted in eq. 1. Word vector representations where initialized from the 300-dimensional GloVe vectors (Pen- nington et al., 2014), trained in 840B tokens from the Common Crawl13, and \ufb01netuned during train- ing. Words not present in the GloVe vocabulary where randomly initialized by sampling from the uniform distribution in the (\u22120.05; 0.05) range. The input size of the word-level LSTM was 300 for every method except concat in which it was 600, and its output was always 2048 per direc- tion, resulting in a 4096-dimensional sentence rep- resentation. B Datasets B.1 Word Similarity Table B.1 lists the word-similarity datasets and their corresponding reference. As mentioned in section 3.2, all the word-similarity datasets con- tain pairs of words annotated with similarity or re- latedness scores, although this difference is not al- ways explicit. Below we provide some details for each.",
      "As mentioned in section 3.2, all the word-similarity datasets con- tain pairs of words annotated with similarity or re- latedness scores, although this difference is not al- ways explicit. Below we provide some details for each. MEN contains 3000 annotated word pairs with integer scores ranging from 0 to 50. Words cor- respond to image labels appearing in the ESP- Game14 and MIRFLICKR-1M15 image datasets. MTurk287 contains 287 annotated pairs with scores ranging from 1.0 to 5.0. It was created from words appearing in both DBpedia and in news ar- ticles from The New York Times. 12https://www.nltk.org/ 13https://nlp.stanford.edu/projects/glove/ 14http://www.cs.cmu.edu/\u02dcbiglou/resources/ 15http://press.liacs.nl/mirflickr/",
      "Dataset Reference URL MEN Bruni et al. (2014) https://staff.fnwi.uva.nl/e.bruni/MEN MTurk287 Radinsky et al. (2011) https://git.io/fhQA8 (Unof\ufb01cial) MTurk771 Halawi et al. (2012) http://www2.mta.ac.il/\u02dcgideon/mturk771.html RG Rubenstein and Goodenough (1965) https://git.io/fhQAB (Unof\ufb01cial) RareWords (RW) Luong et al. (2013) https://nlp.stanford.edu/\u02dclmthang/morphoNLM/ SimLex999 Hill et al. (2015) https://fh295.github.io/simlex.html SimVerb3500 Gerz et al. (2016) http://people.ds.cam.ac.uk/dsg40/simverb.html WS353 Finkelstein et al. (2002) http://www.cs.technion.ac.il/\u02dcgabr/resources/data/wordsim353/ WS353R Agirre et al.",
      "(2016) http://people.ds.cam.ac.uk/dsg40/simverb.html WS353 Finkelstein et al. (2002) http://www.cs.technion.ac.il/\u02dcgabr/resources/data/wordsim353/ WS353R Agirre et al. (2009) http://alfonseca.org/eng/research/wordsim353.html WS353S Agirre et al. (2009) http://alfonseca.org/eng/research/wordsim353.html Table B.1: Word similarity and relatedness datasets. MTurk771 contains 771 annotated pairs with scores ranging from 1.0 to 5.0, with words having synonymy, holonymy or meronymy relationships sampled from WordNet (Fellbaum, 1998). RG contains 65 annotated pairs with scores ranging from 0.0 to 4.0 representing \u201csimilarity of meaning\u201d. RW contains 2034 pairs of words annotated with similarity scores in a scale from 0 to 10.",
      "RG contains 65 annotated pairs with scores ranging from 0.0 to 4.0 representing \u201csimilarity of meaning\u201d. RW contains 2034 pairs of words annotated with similarity scores in a scale from 0 to 10. The words included in this dataset were ob- tained from Wikipedia based on their frequency, and later \ufb01ltered depending on their WordNet synsets, including synonymy, hyperonymy, hy- ponymy, holonymy and meronymy. This dataset was created with the purpose of testing how well models can represent rare and complex words. SimLex999 contains 999 word pairs annotated with similarity scores ranging from 0 to 10. In this case the authors explicitly considered similarity and not relatedness, addressing the shortcomings of datasets that do not, such as MEN and WS353. Words include nouns, adjectives and verbs. SimVerb3500 contains 3500 verb pairs anno- tated with similarity scores ranging from 0 to 10. Verbs were obtained from the USF free association database (Nelson et al., 2004), and VerbNet (Kip- per et al., 2008).",
      "SimVerb3500 contains 3500 verb pairs anno- tated with similarity scores ranging from 0 to 10. Verbs were obtained from the USF free association database (Nelson et al., 2004), and VerbNet (Kip- per et al., 2008). This dataset was created to address the lack of representativity of verbs in SimLex999, and the fact that, at the time of cre- ation, the best performing models had already sur- passed inter-annotator agreement in verb similar- ity evaluation resources. Like SimLex999, this dataset also explicitly considers similarity as op- posed to relatedness. WS353 contains 353 word pairs annotated with similarity scores from 0 to 10. WS353R is a subset of WS353 containing 252 word pairs annotated with relatedness scores. This dataset was created by asking humans to classify each WS353 word pair into one of the following classes: synonyms, antonyms, identical, hyperonym-hyponym, hyponym-hyperonym, holonym-meronym, meronym-holonym, and none-of-the-above.",
      "This dataset was created by asking humans to classify each WS353 word pair into one of the following classes: synonyms, antonyms, identical, hyperonym-hyponym, hyponym-hyperonym, holonym-meronym, meronym-holonym, and none-of-the-above. These annotations were later used to group the pairs into: similar pairs (synonyms, antonyms, identical, hyperonym- hyponym, and hyponym-hyperonym), related pairs (holonym-meronym, meronym-holonym, and none-of-the-above with a human similarity score greater than 5), and unrelated pairs (classi- \ufb01ed as none-of-the-above with a similarity score less than or equal to 5). This dataset is composed by the union of related and unrelated pairs. WS353S is another subset of WS353 containing 203 word pairs annotated with similarity scores. This dataset is composed by the union of similar and unrelated pairs, as described previously. B.2 Sentence Evaluation Datasets Table B.2 lists the sentence-level evaluation datasets used in this paper.",
      "WS353S is another subset of WS353 containing 203 word pairs annotated with similarity scores. This dataset is composed by the union of similar and unrelated pairs, as described previously. B.2 Sentence Evaluation Datasets Table B.2 lists the sentence-level evaluation datasets used in this paper. The provided URLs correspond to the original sources, and not neces- sarily to the URLs where SentEval16 got the data from17. The version of the CR, MPQA, MR, and SUBJ datasets used in this paper were the ones prepro- cessed by Wang and Manning (2012)18. Both SST2 and SST5 correspond to preprocessed ver- sions of the Stanford Sentiment Treebank (SST) dataset by Socher et al. (2013)19. SST2 corre- sponds to a subset of SST used by Arora et al. (2017) containing \ufb02at representations of sentences annotated with binary sentiment labels, and SST5 to another subset annotated with more \ufb01ne-grained sentiment labels (very negative, negative, neutral, positive, very positive).",
      "(2017) containing \ufb02at representations of sentences annotated with binary sentiment labels, and SST5 to another subset annotated with more \ufb01ne-grained sentiment labels (very negative, negative, neutral, positive, very positive). 16https://github.com/facebookresearch/SentEval/tree/906b34a 17A list of the data used by SentEval can be found in its data setup script: https://git.io/fhQpq 18https://nlp.stanford.edu/\u02dcsidaw/home/projects:nbsvm 19https://nlp.stanford.edu/sentiment/",
      "Dataset Reference URL CR Hu and Liu (2004) https://www.cs.uic.edu/\u02dcliub/FBS/sentiment-analysis.html#datasets MPQA Wiebe et al. (2005) https://mpqa.cs.pitt.edu/corpora/mpqa_corpus/ MR Pang and Lee (2005) http://www.cs.cornell.edu/people/pabo/movie-review-data/ SST2 Arora et al. (2017) https://github.com/PrincetonML/SIF/tree/master/data SST5 See caption. https://git.io/fhQAV SUBJ Pang and Lee (2004) http://www.cs.cornell.edu/people/pabo/movie-review-data/ TREC Li and Roth (2002) http://cogcomp.org/Data/QA/QC/ SICKE Marelli et al. (2014) http://clic.cimec.unitn.it/composes/sick.html SICKR Marelli et al. (2014) http://clic.cimec.unitn.it/composes/sick.html STS16 Agirre et al.",
      "(2014) http://clic.cimec.unitn.it/composes/sick.html SICKR Marelli et al. (2014) http://clic.cimec.unitn.it/composes/sick.html STS16 Agirre et al. (2016) http://ixa2.si.ehu.es/stswiki/index.php/Main_Page STSB Cer et al. (2017) http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark Table B.2: Sentence representation evaluation datasets. SST5 was obtained from a GitHub repository with no associated peer-reviewed work."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1904.05584.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":2048,
  "num_embeddings":16845,
  "avg_doclen":177.3157894737,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1904.05584.pdf"
    }
  }
}