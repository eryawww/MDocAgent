[
  "WORD EMBEDDING BASED NEW CORPUS FOR LOW-RESOURCED LANGUAGE: SINDHI A PREPRINT Wazir Ali, Jay Kumar, Junyu Lu, Zenglin Xu School of Computer Science and Engineering University of Electronic Science and Technology of China January 20, 2022 ABSTRACT Representing words and phrases into dense vectors of real numbers which encode semantic and syntactic properties is a vital constituent in natural language processing (NLP). The success of neural network (NN) models in NLP largely rely on such dense word representations learned on the large unlabeled corpus. Sindhi is one of the rich morphological language, spoken by large population in Pakistan and India lacks corpora which plays an essential role of a test-bed for generating word embeddings and developing language independent NLP systems. In this paper, a large corpus of more than 61 million words is developed for low-resourced Sindhi language for training neural word embeddings. The corpus is acquired from multiple web-resources using web-scrappy. Due to the unavailability of open source preprocessing tools for Sindhi, the prepossessing of such large corpus becomes a challenging problem specially cleaning of noisy data extracted from web resources.",
  "The corpus is acquired from multiple web-resources using web-scrappy. Due to the unavailability of open source preprocessing tools for Sindhi, the prepossessing of such large corpus becomes a challenging problem specially cleaning of noisy data extracted from web resources. Therefore, a preprocessing pipeline is employed for the \ufb01ltration of noisy text. Afterwards, the cleaned vocabulary is utilized for training Sindhi word embeddings with state-of-the-art GloVe, Skip-Gram (SG), and Continuous Bag of Words (CBoW) word2vec algorithms. The intrinsic evaluation approach of cosine similarity matrix and WordSim-353 are employed for the evaluation of generated Sindhi word embeddings. Moreover, we compare the proposed word embeddings with recently revealed Sindhi fastText (SdfastText) word representations. Our intrinsic evaluation results demonstrate the high quality of our generated Sindhi word embeddings using SG, CBoW, and GloVe as compare to SdfastText word representations. Keywords Corpus acquisition \u00b7 Sindhi language \u00b7 Neural networks \u00b7 Word embeddings \u00b7 Continuous Bag of Words \u00b7 Skip gram \u00b7 GloVe 1 Introduction Sindhi is a rich morphological, mutltiscript, and multidilectal language.",
  "Keywords Corpus acquisition \u00b7 Sindhi language \u00b7 Neural networks \u00b7 Word embeddings \u00b7 Continuous Bag of Words \u00b7 Skip gram \u00b7 GloVe 1 Introduction Sindhi is a rich morphological, mutltiscript, and multidilectal language. It belongs to the Indo-Aryan language family [1], with signi\ufb01cant cultural and historical background. Presently, it is recognized as is an of\ufb01cial language [2] in Sindh province of Pakistan, also being taught as a compulsory subject in Schools and colleges. Sindhi is also recognized as one of the national languages in India. Ulhasnagar, Rajasthan, Gujarat, and Maharashtra are the largest Indian regions of Sindhi native speakers. It is also spoken in other countries except for Pakistan and India, where native Sindhi speakers have migrated, such as America, Canada, Hong Kong, British, Singapore, Tanzania, Philippines, Kenya, Uganda, and South, and East Africa. Sindhi has rich morphological structure [3] due to a large number of homogeneous words. Historically, it was written in multiple writing systems, which differ from each other in terms of orthography and morphology.",
  "Sindhi has rich morphological structure [3] due to a large number of homogeneous words. Historically, it was written in multiple writing systems, which differ from each other in terms of orthography and morphology. The Persian-Arabic is the standard script of Sindhi, which was of\ufb01cially accepted in 1852 by the British government1. However, the Sindhi-Devanagari is also a popular writing system in India being written in left to right direction like the Hindi language. Formerly, Khudabadi, Gujrati, Landa, Khojki, and Gurumukhi were also adopted as its writing systems. Even though, Sindhi has great historical and literal background, presently spoken by nearly 1https://www.britannica.com/topic/Sindhi-language arXiv:1911.12579v3  [cs.CL]  30 Dec 2020",
  "A PREPRINT - JANUARY 20, 2022 75 million people [2]. The research on SNLP was coined in 20022, however, IT grabbed research attention after the development of its Unicode system [4]. But still, Sindhi stands among the low-resourced languages due to the scarcity of core language processing resources of the raw and annotated corpus, which can be utilized for training robust word embeddings or the use of machine learning algorithms. Since the development of annotated datasets requires time and human resources. The Language Resources (LRs) are fundamental elements for the development of high quality NLP systems based on automatic or NN based approaches. The LRs include written or spoken corpora, lexicons, and annotated corpora for speci\ufb01c computational purposes. The development of such resources has received great research interest for the digitization of human languages [5]. Many world languages are rich in such language processing resources integrated in their software tools including English [6] [7], Chinese [8] and other languages [9] [10].",
  "The development of such resources has received great research interest for the digitization of human languages [5]. Many world languages are rich in such language processing resources integrated in their software tools including English [6] [7], Chinese [8] and other languages [9] [10]. The Sindhi language lacks the basic computational resources [11] of a large text corpus, which can be utilized for training robust word embeddings and developing language independent NLP applications including semantic analysis, sentiment analysis, parts of the speech tagging, named entity recognition, machine translation [12], multitasking [13], [14]. Presently Sindhi Persian-Arabic is frequently used for online communication, newspapers, public institutions in Pakistan, and India [2]. But little work has been carried out for the development of LRs such as raw corpus [15], [16], annotated corpus [17], [18], [2], [19].",
  "But little work has been carried out for the development of LRs such as raw corpus [15], [16], annotated corpus [17], [18], [2], [19]. In the best of our knowledge, Sindhi lacks the large unlabelled corpus which can be utilized for generating and evaluating word embeddings for Statistical Sindhi Language Processing (SSLP) One way to to break out this loop is to learn word embeddings from unlabelled corpora, which can be utilized to bootstrap other downstream NLP tasks. The word embedding is a new term of semantic vector space [20], distributed representations [21], and distributed semantic models. It is a language modeling approach [22] used for the mapping of words and phrases into n-dimensional dense vectors of real numbers that effectively capture the semantic and syntactic relationship with neighboring words in a geometric way [23] [24]. Such as \u201cEinstein\u201d and \u201cScientist\u201d would have greater similarity compared with \u201cEinstein\u201d and \u201cdoctor.\u201d In this way, word embeddings accomplish the important linguistic concept of \u201ca word is characterized by the company it keeps\".",
  "Such as \u201cEinstein\u201d and \u201cScientist\u201d would have greater similarity compared with \u201cEinstein\u201d and \u201cdoctor.\u201d In this way, word embeddings accomplish the important linguistic concept of \u201ca word is characterized by the company it keeps\". More recently NN based models yield state-of-the-art performance in multiple NLP tasks [25] [26] with the word embeddings. One of the advantages of such techniques is they use unsupervised approaches for learning representations and do not require annotated corpus which is rare for low-resourced Sindhi language. Such representions can be trained on large unannotated corpora, and then generated representations can be used in the NLP tasks which uses a small amount of labelled data. In this paper, we address the problems of corpus construction by collecting a large corpus of more than 61 million words from multiple web resources using the web-scrappy framework. After the collection of the corpus, we carefully preprocessed for the \ufb01ltration of noisy text, e.g., the HTML tags and vocabulary of the English language. The statistical analysis is also presented for the letter, word frequencies and identi\ufb01cation of stop-words.",
  "After the collection of the corpus, we carefully preprocessed for the \ufb01ltration of noisy text, e.g., the HTML tags and vocabulary of the English language. The statistical analysis is also presented for the letter, word frequencies and identi\ufb01cation of stop-words. Finally, the corpus is utilized to generate Sindhi word embeddings using state-of-the-art GloVe [27] SG and CBoW [28] [21] [25] algorithms. The popular intrinsic evaluation method [21] [29] [30] of calculating cosine similarity between word vectors and WordSim353 [31] are employed to measure the performance of the learned Sindhi word embeddings. We translated English WordSim3533 word pairs into Sindhi using bilingual English to Sindhi dictionary. The intrinsic approach typically involves a pre-selected set of query terms [24] and semantically related target words, which we refer to as query words. Furthermore, we also compare the proposed word embeddings with recently revealed Sindhi fastText (SdfastText)4 [26] word representations.",
  "The intrinsic approach typically involves a pre-selected set of query terms [24] and semantically related target words, which we refer to as query words. Furthermore, we also compare the proposed word embeddings with recently revealed Sindhi fastText (SdfastText)4 [26] word representations. To the best of our knowledge, this is the \ufb01rst comprehensive work on the development of large corpus and generating word embeddings along with systematic evaluation for low-resourced Sindhi Persian-Arabic. The synopsis of our novel contributions is listed as follows: \u2022 We present a large corpus of more than 61 million words obtained from multiple web resources and reveal a list of Sindhi stop words. \u2022 We develop a text cleaning pipeline for the preprocessing of the raw corpus. \u2022 Generate word embeddings using GloVe, CBoW, and SG Word2Vec algorithms also evaluate and compare them using the intrinsic evaluation approaches of cosine similarity matrix and WordSim353. \u2022 We are the \ufb01rst to evaluate SdfastText word representations and compare them with our proposed Sindhi word embeddings. The remaining sections of the paper are organized as, Section 2 presents the literature survey regarding computational resources, Sindhi corpus construction, and word embedding models.",
  "\u2022 We are the \ufb01rst to evaluate SdfastText word representations and compare them with our proposed Sindhi word embeddings. The remaining sections of the paper are organized as, Section 2 presents the literature survey regarding computational resources, Sindhi corpus construction, and word embedding models. Afterwards, Section 3 presents the employed 2\"Sindhia lai Kampyutar jo Istemalu\" (Use of computer for Sindhi), an article published in Sindhu yearly, Ulhasnagar. 2002 3Available online at https://rdrr.io/cran/wordspace/man/WordSim353.html 4We denote Sindhi word representations as (SdfastText) recently revealed by fastText, available at (https://fasttext.cc/docs/en/crawl- vectors.html) trained on Common Crawl and Wikipedia corpus of Sindhi Persian-Arabic. 2",
  "A PREPRINT - JANUARY 20, 2022 methodology, Section 4 consist of statistical analysis of the developed corpus. Section 5 present the experimental setup. The intrinsic evaluation results along with comparison are given in Section 6. The discussion and future work are given in Section 7, and lastly, Section 8 presents the conclusion. 2 Related work The natural language resources refer to a set of language data and descriptions [32] in machine readable form, used for building, improving, and evaluating NLP algorithms or softwares. Such resources include written or spoken corpora, lexicons, and annotated corpora for speci\ufb01c computational purposes. Many world languages are rich in such language processing resources integrated in the software tools including NLTK for English [6], Stanford CoreNLP [7], LTP for Chinese [8], TectoMT for German, Russian, Arabic [9] and multilingual toolkit [10]. But Sindhi language is at an early stage for the development of such resources and software tools. The corpus construction for NLP mainly involves important steps of acquisition, preprocessing, and tokenization.",
  "But Sindhi language is at an early stage for the development of such resources and software tools. The corpus construction for NLP mainly involves important steps of acquisition, preprocessing, and tokenization. Initially, [15] discussed the morphological structure and challenges concerned with the corpus development along with orthographical and morphological features in the Persian-Arabic script. The raw and annotated corpus [2] for Sindhi Persian-Arabic is a good supplement towards the development of resources, including raw and annotated datasets for parts of speech tagging, morphological analysis, transliteration between Sindhi Persian-Arabic and Sindhi- Devanagari, and machine translation system. But the corpus is acquired only form Wikipedia-dumps. A survey-based study [5] provides all the progress made in the Sindhi Natural Language Processing (SNLP) with the complete gist of adopted techniques, developed tools and available resources which show that work on resource development on Sindhi needs more sophisticated efforts. The raw corpus is utilized for Sindhi word segmentation [33]. More recently, an initiative towards the development of resources is taken [17] by open sourcing annotated dataset of Sindhi Persian-Arabic obtained from news and social blogs.",
  "The raw corpus is utilized for Sindhi word segmentation [33]. More recently, an initiative towards the development of resources is taken [17] by open sourcing annotated dataset of Sindhi Persian-Arabic obtained from news and social blogs. The existing and proposed work is presented in Table 1 on the corpus development, word segmentation, and word embeddings, respectively. The power of word embeddings in NLP was empirically estimated by proposing a neural language model [22] and multitask learning [13], but recently usage of word embeddings in deep neural algorithms has become integral element [34] for performance acceleration in deep NLP applications. The CBoW and SG [28] [21] popular word2vec neural architectures yielded high quality vector representations in lower computational cost with integration of character- level learning on large corpora in terms of semantic and syntactic word similarity later extended [34] [25]. Both approaches produce state-of-the-art accuracy with fast training performance, better representations of less frequent words and ef\ufb01cient representation of phrases as well. [35] proposed NN based approach for generating morphemic-level word embeddings, which surpassed all the existing embedding models in intrinsic evaluation.",
  "Both approaches produce state-of-the-art accuracy with fast training performance, better representations of less frequent words and ef\ufb01cient representation of phrases as well. [35] proposed NN based approach for generating morphemic-level word embeddings, which surpassed all the existing embedding models in intrinsic evaluation. A count-based GloVe model [27] also yielded state-of-the-art results in an intrinsic evaluation and downstream NLP tasks. The performance of Word embeddings is evaluated using intrinsic [24] [30] and extrinsic evaluation [29] methods. The performance of word embeddings can be measured with intrinsic and extrinsic evaluation approaches. The intrinsic approach is used to measure the internal quality of word embeddings such as querying nearest neighboring words and calculating the semantic or syntactic similarity between similar word pairs. A method of direct comparison for intrinsic evaluation of word embeddings measures the neighborhood of a query word in vector space. The key advantage of that method is to reduce bias and create insight to \ufb01nd data-driven relevance judgment. An extrinsic evaluation approach is used to evaluate the performance in downstream NLP tasks, such as parts-of-speech tagging or named-entity recognition [24], but the Sindhi language lacks annotated corpus for such type of evaluation.",
  "An extrinsic evaluation approach is used to evaluate the performance in downstream NLP tasks, such as parts-of-speech tagging or named-entity recognition [24], but the Sindhi language lacks annotated corpus for such type of evaluation. Moreover, extrinsic evaluation is time consuming and dif\ufb01cult to interpret. Therefore, we opt intrinsic evaluation method [29] to get a quick insight into the quality of proposed Sindhi word embeddings by measuring the cosine distance between similar words and using WordSim353 dataset. A study reveals that the choice of optimized hyper-parameters [36] has a great impact on the quality of pretrained word embeddings as compare to desing a novel algorithm. Therefore, we optimized the hyperparameters for generating robust Sindhi word embeddings using CBoW, SG and GloVe models. The embedding visualization is also useful to visualize the similarity of word clusters. Therefore, we use t-SNE [37] dimensionality reduction algorithm for compressing high dimensional embedding into 2-dimensional x,y coordinate pairs with PCA [38]. The PCA is useful to combine input features by dropping the least important features while retaining the most valuable features.",
  "Therefore, we use t-SNE [37] dimensionality reduction algorithm for compressing high dimensional embedding into 2-dimensional x,y coordinate pairs with PCA [38]. The PCA is useful to combine input features by dropping the least important features while retaining the most valuable features. 3 Methodology This section presents the employed methodology in detail for corpus acquisition, preprocessing, statistical analysis, and generating Sindhi word embeddings. 3",
  "A PREPRINT - JANUARY 20, 2022 Paper Related works Resource [26] Word embedding Wiki-dumps (2016) [15] Text Corpus 4.1M tokens [2] Corpus development Wiki-dumps (2016) [17] Labelled corpus 6.8K [18] Sentiment analysis 31.5K tokens [33] Text Segmentation 1575K Proposed work Raw Corpus 61.39 M tokens Word embeddings 61.39M tokens Table 1: Comparison of existing and proposed work on Sindhi corpus construction and word embeddings 3.1 Task description We initiate this work from scratch by collecting large corpus from multiple web resources. After preprocessing and statistical analysis of the corpus, we generate Sindhi word embeddings with state-of-the-art CBoW, SG, and GloVe algorithms. The generated word embeddings are evaluated using the intrinsic evaluation approaches of cosine similarity between nearest neighbors, word pairs, and WordSim-353 for distributional semantic similarity. Moreover, we use t-SNE with PCA for the comparison of the distance between similar words via visualization.",
  "The generated word embeddings are evaluated using the intrinsic evaluation approaches of cosine similarity between nearest neighbors, word pairs, and WordSim-353 for distributional semantic similarity. Moreover, we use t-SNE with PCA for the comparison of the distance between similar words via visualization. 3.2 Corpus acquisition The corpus is a collection of human language text [32] built with a speci\ufb01c purpose. However, the statistical analysis of the corpus provides quantitative, reusable data, and an opportunity to examine intuitions and ideas about language. Therefore, the corpus has great importance for the study of written language to examine the text. In fact, realizing the necessity of large text corpus for Sindhi, we started this research by collecting raw corpus from multiple web resource using web-scrappy framwork5 for extraction of news columns of daily Kawish6 and Awami Awaz7 Sindhi newspapers, Wikipedia dumps8, short stories and sports news from Wichaar9 social blog, news from Focus Word press blog10, historical writings, novels, stories, books from Sindh Salamat11 literary websites, novels, history and religious books from Sindhi Adabi Board 12 and tweets regarding news and sports are collected from twitter13.",
  "3.3 Preprocessing The preprocessing of text corpus obtained from multiple web resources is a challenging task specially it becomes more complicated when working on low-resourced language like Sindhi due to the lack of open-source preprocessing tools such as NLTK [6] for English. Therefore, we design a preprocessing pipeline depicted in Figure 1 for the \ufb01ltration of unwanted data and vocabulary of other languages such as English to prepare input for word embeddings. Whereas, the involved preprocessing steps are described in detail below the Figure 1. Moreover, we reveal the list of Sindhi stop words [39] which is labor intensive and requires human judgment as well. Hence, the most frequent and least important words are classi\ufb01ed as stop words with the help of a Sindhi linguistic expert. The partial list of Sindhi stop words is given in 4. We use python programming language for designing the preprocessing pipeline using regex and string functions. \u2022 Input: The collected text documents were concatenated for the input in UTF-8 format.",
  "The partial list of Sindhi stop words is given in 4. We use python programming language for designing the preprocessing pipeline using regex and string functions. \u2022 Input: The collected text documents were concatenated for the input in UTF-8 format. \u2022 Replacement symbols: The punctuation marks of a full stop, hyphen, apostrophe, comma, quotation, and exclamation marks replaced with white space for authentic tokenization because without replacing these symbols with white space the words were found joined with their next or previous corresponding words. \u2022 Filtration of noisy data: The text acquisition from web resources contain a huge amount of noisy data. Therefore, we \ufb01ltered out unimportant data such as the rest of the punctuation marks, special characters, HTML tags, all types of numeric entities, email, and web addresses.",
  "\u2022 Filtration of noisy data: The text acquisition from web resources contain a huge amount of noisy data. Therefore, we \ufb01ltered out unimportant data such as the rest of the punctuation marks, special characters, HTML tags, all types of numeric entities, email, and web addresses. 5https://github.com/scrapy/scrapy 6http://kawish.asia/Articles1/index.htm 7http://www.awamiawaz.com/articles/294/ 8https://dumps.wikimedia.org/sdwiki/20180620/ 9http://wichaar.com/news/134/, accessed in Dec-2018 10https://thefocus.wordpress.com/ accessed in Dec-2018 11http://sindhsalamat.com/, accessed in Jan-2019 12http://www.sindhiadabiboard.org/catalogue/History/Main_History.HTML 13https://twitter.com/dailysindhtimes 4",
  "A PREPRINT - JANUARY 20, 2022 Replace punctuation  marks and Special  Characters with white  space Filtration of noisy  data HTML tags Web addresses Non-alphabetic  entities Normalization Tokenization Convert to lower case Remove  1.Multiple spaces 2. English words 3. Duplicate words Cleaned  vocabulary Input  UTF-8 Figure 1: Employed preprocessing pipeline for text cleaning \u2022 Normalization: In this step, We tokenize the corpus then normalize to lower-case for the \ufb01ltration of multiple white spaces, English vocabulary, and duplicate words. The stop words were only \ufb01ltered out for preparing input for GloVe. However, the sub-sampling approach in CBoW and SG can discard most frequent or stop words automatically. 3.4 Word embedding models The NN based approaches have produced state-of-the-art performance in NLP with the usage of robust word embedings generated from the large unlabelled corpus. Therefore, word embeddings have become the main component for setting up new benchmarks in NLP using deep learning approaches.",
  "3.4 Word embedding models The NN based approaches have produced state-of-the-art performance in NLP with the usage of robust word embedings generated from the large unlabelled corpus. Therefore, word embeddings have become the main component for setting up new benchmarks in NLP using deep learning approaches. Most recently, the use cases of word embeddings are not only limited to boost statistical NLP applications but can also be used to develop language resources such as automatic construction of WordNet [40] using the unsupervised approach. The word embedding can be precisely de\ufb01ned as the encoding of vocabulary V into N and the word w from V to vector \u2212\u2192 w into N-dimensional embedding space. They can be broadly categorized into predictive and count based methods, being generated by employing co-occurrence statistics, NN algorithms, and probabilistic models. The GloVe [27] algorithm treats each word as a single entity in the corpus and generates a vector of each word. However, CBoW and SG [28] [21], later extended [34] [25], well-known as word2vec rely on simple two layered NN architecture which uses linear activation function in hidden layer and softmax in the output layer.",
  "However, CBoW and SG [28] [21], later extended [34] [25], well-known as word2vec rely on simple two layered NN architecture which uses linear activation function in hidden layer and softmax in the output layer. The work2vec model treats each word as a bag-of-character n-gram. 3.5 GloVe The GloVe is a log-bilinear regression model [27] which combines two methods of local context window and global matrix factorization for training word embeddings of a given vocabulary in an unsupervised way. It weights the contexts using the harmonic function, for example, a context word four tokens away from an occurrence will be counted as 1 4. The Glove\u2019s implementation represents word w \u2208Vw and context c \u2208Vc in D-dimensional vectors \u2212\u2192 w and \u2212\u2192c in a following way, \u2212\u2192 w \u00b7 \u2212\u2192c + bw + bc = log(#(w, c))\u2200(w, c) \u2208D (1) Where bw and bc represent word and context biases also to be learned as parameters of \u2212\u2192 w and \u2212\u2192c .",
  "The objective of GloVe algorithm is to learn word embeddings by taking the log-count matrix [36] shifted by the bias terms of entire vocabulary as, M log(#(w,c)) \u2248W \u00b7 CT + b \u2212 \u2192 w + b \u2212 \u2192 c (2) where, b \u2212 \u2192 w is row vector |Vw| and b \u2212 \u2192 c is |Vc| is column vector. 3.6 Continuous bag-of-words The standard CBoW is the inverse of SG [28] model, which predicts input word on behalf of the context. The length of input in the CBoW model depends on the setting of context window size which determines the distance to the left and right of the target word. Hence the context is a window that contain neighboring words such as by giving w = {w1, w2, . . . . . . wt} a sequence of words T, the objective of the CBoW is to maximize the probability of given neighboring words such as, T X t=0 log p (wt|ct) (3) Where ct is context of tth word for example with window wt\u2212c, . . .",
  "wt} a sequence of words T, the objective of the CBoW is to maximize the probability of given neighboring words such as, T X t=0 log p (wt|ct) (3) Where ct is context of tth word for example with window wt\u2212c, . . . ..wt\u22121, wt+1 . . . wt+c of size 2c. 5",
  "A PREPRINT - JANUARY 20, 2022 3.7 Skip gram The SG model predicts surrounding words by giving input word [21] with training objective of learning good word embeddings that ef\ufb01ciently predict the neighboring words. The goal of skip-gram is to maximize average log-probability of words w = {w1, w2, . . . . . . wt} across the entire training corpus, J(\u03b8) 1 T T X t=0 \uf8eb \uf8ed X \u2212c\u2264j\u2264c,j log p (wt+j|ct) \uf8f6 \uf8f8 (4) Where ct denotes the context of words indices set of nearby wt words in the training corpus. 3.8 Hyperparameters 3.8.1 Sub-sampling Th sub-sampling [21] approach is useful to dilute most frequent or stop words, also accelerates learning rate, and increases accuracy for learning rare word vectors. Numerous words in English, e.g., \u2018the\u2019, \u2018you\u2019, \u2019that\u2019 do not have more importance, but these words appear very frequently in the text.",
  "Numerous words in English, e.g., \u2018the\u2019, \u2018you\u2019, \u2019that\u2019 do not have more importance, but these words appear very frequently in the text. However, considering all the words equally would also lead to over-\ufb01tting problem of model parameters [25] on the frequent word embeddings and under-\ufb01tting on the rest. Therefore, it is useful to count the imbalance between rare and repeated words. The sub-sampling technique randomly removes most frequent words with some threshold t and probability p of words and frequency f of words in the corpus. P (wi) = 1 \u2212 s t f (wi) (5) Where each wordwi is discarded with computed probability in training phase, f(wi) is frequency of word wi and t > 0 are parameters. 3.8.2 Dynamic context window The traditional word embedding models usually use a \ufb01xed size of a context window. For instance, if the window size ws=6, then the target word apart from 6 tokens will be treated similarity as the next word. The scheme is used to assign more weight to closer words, as closer words are generally considered to be more important to the meaning of the target word.",
  "For instance, if the window size ws=6, then the target word apart from 6 tokens will be treated similarity as the next word. The scheme is used to assign more weight to closer words, as closer words are generally considered to be more important to the meaning of the target word. The CBoW, SG and GloVe models employ this weighting scheme. The GloVe model weights the contexts using a harmonic function, for example, a context word four tokens away from an occurrence will be counted as 1 4. However, CBoW and SG implementation equally consider the contexts by dividing the ws with the distance from target word, e.g. ws=6 will weigh its context by 6 6 5 6 4 6 3 6 2 6 1 6 3.8.3 Sub-word model The sub-word model [25] can learn the internal structure of words by sharing the character representations across words. In that way, the vector for each word is made of the sum of those character n \u2212gram.",
  "In that way, the vector for each word is made of the sum of those character n \u2212gram. Such as, a vector of a word \u201ctable\u201d is a sum of n \u2212gram vectors by setting the letter n \u2212gram size min = 3 to max = 6 as, < ta, tab, tabl, table, table >, abl, able, able >, ble, ble >, le >, we can get all sub-words of \"table\" with minimum length of minn = 3 and maximum length of maxn = 6. The < and > symbols are used to separate pre\ufb01x and suf\ufb01x words from other character sequences. In this way, the sub-word model utilizes the principles of morphology, which improves the quality of infrequent word representations. In addition to character n \u2212grams, the input word w is also included in the set of character n \u2212gram, to learn the representation of each word. We obtain scoring function using a input dictionary of n \u2212grams with size K by giving word w , where Kw \u2282{1, . . . , K}.",
  "We obtain scoring function using a input dictionary of n \u2212grams with size K by giving word w , where Kw \u2282{1, . . . , K}. A word representation Zk is associated to each n \u2212gram Z. Hence, each word is represented by the sum of character n \u2212gram representations, where, s is the scoring function in the following equation, s(w, c) = X k\u2208Kj zT k vc (6) 3.8.4 Position-dependent weights The position-dependent weighting approach [41] is used to avoid direct encoding of representations for words and their positions which can lead to over-\ufb01tting problem. The approach learns positional representations in contextual word representations and used to reweight word embedding. Thus, it captures good contextual representations at lower computational cost, vCV = X p\u2208P dp \u2299ut+p (7) Where p is individual position in context window associated with dp vector. Afterwards the context vector reweighted by their positional vectors is average of context words. The relative positional set is P in context window and vC is context vector of wt respectively. 6",
  "A PREPRINT - JANUARY 20, 2022 3.8.5 Shifted point-wise mutual information The use sparse Shifted Positive Point-wise Mutual Information (SPPMI) [42] word-context matrix in learning word representations improves results on two word similarity tasks. The CBoW and SG have k (number of negatives) [28] [21] hyperparameter, which affects the value that both models try to optimize for each (w, c) : PMI(w, c)\u2212log k. Parameter k has two functions of better estimation of negative examples, and it performs as before observing the probability of positive examples (actual occurrence of w, c). 3.8.6 Deleting rare words Before creating a context window, the automatic deletion of rare words also leads to performance gain in CBoW, SG and GloVe models, which further increases the actual size of context windows. 3.9 Evaluation methods The intrinsic evaluation is based on semantic similarity [24] in word embeddings. The word similarity measure approach states [36] that the words are similar if they appear in the similar context. We measure word similarity of proposed Sindhi word embeddings using dot product method and WordSim353.",
  "3.9 Evaluation methods The intrinsic evaluation is based on semantic similarity [24] in word embeddings. The word similarity measure approach states [36] that the words are similar if they appear in the similar context. We measure word similarity of proposed Sindhi word embeddings using dot product method and WordSim353. 3.9.1 Cosine similarity The cosine similarity between two non-zero vectors is a popular measure that calculates the cosine of the angle between them which can be derived by using the Euclidean dot product method. The dot product is a multiplication of each component from both vectors added together. The result of a dot product between two vectors isn\u2019t another vector but a single value or a scalar. The dot product for two vectors can be de\ufb01ned as: \u2212\u2192a = (a1, a2, a3, . . . , an) and \u2212\u2192b = (b1, b2, b3, . . . , bn) where an and bn are the components of the vector and n is dimension of vectors such as, \u2212\u2192a \u00b7 \u2212\u2192b = n X i=1 aibi=a1b1 + a2b2 + . . .",
  ". . , bn) where an and bn are the components of the vector and n is dimension of vectors such as, \u2212\u2192a \u00b7 \u2212\u2192b = n X i=1 aibi=a1b1 + a2b2 + . . . . . . . . . . . . anbn (8) However, the cosine of two non-zero vectors can be derived by using the Euclidean dot product formula, \u2212\u2192a \u00b7 \u2212\u2192b = \u2225\u2212\u2192a \u2225\u2225\u2212\u2192b \u2225cos(\u03b8) (9) Given ai two vectors of attributes a and b, the cosine similarity, cos(\u03b8), is represented using a dot product and magnitude as, similarity = cos(\u03b8) = \u00afa \u00b7\u20d7b \u2225\u20d7a\u2225\u2225\u20d7b\u2225 Pn i=1 aibi pPn i=1 a2 1 pPn i=1 b2 1 (10) where ai and bi are components of vector \u2212\u2192a and \u2212\u2192b , respectively.",
  "3.9.2 WordSim353 The WordSim353 [43] is popular for the evaluation of lexical similarity and relatedness. The similarity score is assigned with 13 to 16 human subjects with semantic relations [31] for 353 English noun pairs. Due to the lack of annotated datasets in the Sindhi language, we translated WordSim353 using English to Sindhi bilingual dictionary14 for the evaluation of our proposed Sindhi word embeddings and SdfastText. We use the Spearman correlation coef\ufb01cient for the semantic and syntactic similarity comparison which is used to used to discover the strength of linear or nonlinear relationships if there are no repeated data values. A perfect Spearman\u2019s correlation of +1 or \u22121 discovers the strength of a link between two sets of data (word-pairs) when observations are monotonically increasing or decreasing functions of each other in a following way: rs = 1 \u22126 Pn i d2 i n (n2 \u22121) (11) where rs is the rank correlation coef\ufb01cient, n denote the number of observations, and di is the rank difference between ith observations. 4 Statistical analysis of corpus The large corpus acquired from multiple resources is rich in vocabulary.",
  "4 Statistical analysis of corpus The large corpus acquired from multiple resources is rich in vocabulary. We present the complete statistics of collected corpus (see Table 2 with number of sentences, words and unique tokens. 14http://dic.sindhila.edu.pk/index.php?txtsrch= 7",
  "A PREPRINT - JANUARY 20, 2022 Source Category Sentences Vocabulary Unique words Kawish News columns 473,225 13,733,379 109,366 Awami awaz News columns 107,326 7,487,319 65,632 Wikipedia Miscellaneous 844,221 8,229,541 245,621 Social Blogs Stories, sports 7,018 254,327 10,615 History, News 3,260 110,718 7,779 Focus word press Short Stories 63,251 968,639 28,341 Novels 36,859 998,690 18,607 Safarnama 138,119 2,837,595 53,193 Sindh Salamat History 145,845 3,493,020 61,993 Religion 96,837 2,187,563 39,525 Columns 85,995 1,877,813 33,127 Miscellaneous 719,956 9,304,006 168,009 Sindhi Adabi Board History books 478,424 9,757,844 57,854 Twitter News tweets 10,752 159,130 9,",
  "995 1,877,813 33,127 Miscellaneous 719,956 9,304,006 168,009 Sindhi Adabi Board History books 478,424 9,757,844 57,854 Twitter News tweets 10,752 159,130 9,794 Total 3,211,088 61,399,584 908,456 Table 2: Complete statistics of collected corpus from multiple resources n-grams Frequency % in corpus Uni-gram 936,301 1.52889 Bi-gram 19,187,314 31.3311 Tri-gram 11,924,760 19.472 4-gram 14,334,444 23.4068 5-gram 9,459,657 15.4467 6-gram 3,347,907 5.4668 7-gram 1,481,810 2.4196 8-gram 373,417 0.6097 9-gram 163,301 0.2666 10-gram 21,287 0.0347 11-gram 5,892 0.0096 12-gram 3,",
  "4196 8-gram 373,417 0.6097 9-gram 163,301 0.2666 10-gram 21,287 0.0347 11-gram 5,892 0.0096 12-gram 3,033 0.0049 13-gram 1,036 0.0016 14-gram 295 0.0004 Total 61,240,454 100 Table 3: Length of letter n-grams in words, distinct words, frequency and percentage in corpus 4.1 Letter occurrences The frequency of letter occurrences in human language is not arbitrarily organized but follow some speci\ufb01c rules which enable us to describe some linguistic regularities. The Zipf\u2019s law [44] suggests that if the frequency of letter or word occurrence ranked in descending order such as, Fr = a rb (12) Where, Fr is the letter frequency of rth rank, a and b are parameters of input text. The comparative letter frequency in the corpus is the total number of occurrences of a letter divided by the total number of letters present in the corpus.",
  "The comparative letter frequency in the corpus is the total number of occurrences of a letter divided by the total number of letters present in the corpus. The letter frequencies in our developed corpus are depicted in Figure 2; however, the corpus contains 187,620,276 total number of the character set. Sindhi Persian-Arabic alphabet consists of 52 letters but in the vocabulary 59 letters are detected, additional seven letters are modi\ufb01ed uni-grams and standalone honori\ufb01c symbols. 4.2 Letter n-grams frequency We denote the combination of letter occurrences in a word as n-grams, where each letter is a gram in a word. The letter n-gram frequency is carefully analyzed in order to \ufb01nd the length of words which is essential to develop NLP systems, including learning of word embeddings such as choosing the minimum or maximum length of sub-word for character-level representation learning [25]. We calculate the letter n-grams in words along with their percentage in the developed corpus (see Table 3). The bi-gram words are most frequent, mostly consists of stop words and secondly, 4-gram words have a higher frequency. 8",
  "A PREPRINT - JANUARY 20, 2022 24617802 3855640 11835331 14724137 2078026 17356455 6908642 5956196 44434 713782 299393 1604757 2235213 575170 1493021 78658 1435662 263358 1796312 331418 579985 329372 900724 1592050 6021002 1086456 1123830 10755814 256069 164355 633661 1471761 875192 4574119 1058372 1571628 441196 1241299 268225 1231785 249851 6823655 3222686 204454 483007 1474498 1522529 5817630 468353 653865 3792297 21132193 \u064a \u0626 \u06be \u0648 \u06bb \u0646 \u0645 \u0644 \u06b1 \u06b3 \u06af \u06be \u06af \u06a9 \u06aa \u0642 \u06a6 \u0641 \u063a \u0639 \u0638 \u0637 \u0636 \u0635 \u0634 \u0633 \u0632 \u0699 \u0631 \u0630",
  "3792297 21132193 \u064a \u0626 \u06be \u0648 \u06bb \u0646 \u0645 \u0644 \u06b1 \u06b3 \u06af \u06be \u06af \u06a9 \u06aa \u0642 \u06a6 \u0641 \u063a \u0639 \u0638 \u0637 \u0636 \u0635 \u0634 \u0633 \u0632 \u0699 \u0631 \u0630 \u068d \u068a \u068f \u068c \u062f \u062e \u062d \u0687 \u0686 \u0683 \u062c \u06be \u0684 \u062c \u067e \u062b \u067a \u067d \u067f \u062a \u0680 \u067b \u0628 \u0627 SINDHI LETTERS IN ALPHABETICAL ORDER Figure 2: Frequency distribution of letter occurrences 9",
  "A PREPRINT - JANUARY 20, 2022 Figure 3: Most frequent words after \ufb01ltration of stop words. 4.3 Word Frequencies The word frequency count is an observation of word occurrences in the text. The commonly used words are considered to be with higher frequency, such as the word \u201cthe\" in English. Similarly, the frequency of rarely used words to be lower. Such frequencies can be calculated at character or word-level. We calculate word frequencies by counting a word w occurrence in the corpus c, such as, freq(w) = k X k=0 wk \u2208c (13) Where the frequency of w is the sum of every occurrence k of w in c. 4.4 Stop words The most frequent and least important words in NLP are often classi\ufb01ed as stop words. The removal of such words can boost the performance of the NLP model [39], such as sentiment analysis and text classi\ufb01cation. But the construction of such words list is time consuming and requires user decisions. Firstly, we determined Sindhi stop words by counting their term frequencies using Eq.",
  "The removal of such words can boost the performance of the NLP model [39], such as sentiment analysis and text classi\ufb01cation. But the construction of such words list is time consuming and requires user decisions. Firstly, we determined Sindhi stop words by counting their term frequencies using Eq. 13, and secondly, by analysing their grammatical status with the help of Sindhi linguistic expert because all the frequent words are not stop words (see Figure 3). After determining the importance of such words with the help of human judgment, we placed them in the list of stop words. The total number of detected stop words is 340 in our developed corpus. The partial list of most frequent Sindhi stop words is depicted in Table 4 along with their frequency. The \ufb01ltration of stop words is an essential preprocessing step for learning GloVe [27] word embeddings; therefore, we \ufb01ltered out stop words for preparing input for the GloVe model. However, the sub-sampling approach [34] [25] is used to discard such most frequent words in CBoW and SG models.",
  "However, the sub-sampling approach [34] [25] is used to discard such most frequent words in CBoW and SG models. 5 Experiments and results Hyperparameter optimization [24]is more important than designing a novel algorithm. We carefully choose to optimize the dictionary and algorithm-based parameters of CBoW, SG and GloVe algorithms. Hence, we conducted a large number of experiments for training and evaluation until the optimization of most suitable hyperparameters depicted in Table 5 and discussed in Section 5.1. The choice of optimized hyperparameters is based on The high cosine similarity 10",
  "A PREPRINT - JANUARY 20, 2022 Word Freq: Word Freq: Word Freq: Word Freq: Word Freq: \u062c\u064a2068791\u0633\u0646\u062f\u064650219\u062f\u0648\u0631\u0627\u064621351\u0627\u0644\u064110070\u062a\u0646\u0647\u0646\u062c\u06484838 \u062c\u0648980764\u06fd49856\u0648\u064a\u0646\u062f\u062720829\u0628\u0627\u06289962\u06224820 \u062a\u0647895430\u0648\u064a\u0646\u062f\u064849060\u0648\u063a\u064a\u0631\u064720214\u0628\u0646\u06279834\u0627\u0648\u06314741 \u062a\u064a528745\u0631\u0647\u064a\u062746609\u067e\u0648\u0650\u062120132\u0631\u0647\u06469593\u0647\u064a\u06334680 \u0628\u0647538301\u0647\u0648\u0646\u062f\u064845822\u062c\u0627\u0631\u064a192667\u0627\u064a\u0646\u062f\u064a9154\u0628\u0646\u062c\u064a4533 \u0633\u0627\u0646409252\u0645\u0646\u0647\u0646\u062c\u064a43204\u0647\u0644\u064a19200\u0627\u062a\u0627\u06469052\u0647\u0644\u064a\u06274516 \u0627\u0646397751\u0647\u062742837\u0648\u0631\u062a\u064818996\u0628\u06468947\u0627\u0648\u06444493 \u0646\u0647397393\u0628\u064a42508\u0644\u0641\u063818026\u0647\u0644\u064a\u06488890\u0647\u0644\u0646\u062f\u06484092",
  "\u0627\u0646397751\u0647\u062742837\u0648\u0631\u062a\u064818996\u0628\u06468947\u0627\u0648\u06444493 \u0646\u0647397393\u0628\u064a42508\u0644\u0641\u063818026\u0647\u0644\u064a\u06488890\u0647\u0644\u0646\u062f\u06484092 \u0647\u0648393941\u0647\u0646\u064638121\u06fe17784\u062a\u0631\u062c\u0645\u06488787\u0645\u0644\u064a\u06274064 \u062c\u0627291307\u062a\u0648\u0647\u0627\u064637991\u0645\u0637\u0644\u062816773\u0648\u06468222\u0647\u064a\u06444516 \u0647\u0646283280\u0628\u0639\u062f37032\u0647\u062c\u064616602\u0631\u0647\u0646\u062f\u06278101\u0639\u0646\u0648\u0627\u06464820 \u0645\u0627\u0646231711\u0647\u0648\u0646\u062f\u064a36961\u0627\u0633\u064a\u064616308\u0646\u0647\u0627\u064a\u062a7966\u0627\u0648\u06314741 \u0627\u0647\u0648192158\u0646\u0627\u0644\u064835774\u0645\u062b\u0627\u064415405\u0647\u0648\u06337838\u0647\u064a\u06334680 \u062c\u0646\u0647\u0646183937\u0635\u0641\u062d\u064835711\u0648\u064a\u0648\u064615278\u0648\u0631\u062a\u064a7617\u0628\u0646\u062c\u064a4533 \u0648\u064a\u0648165138\u0627\u062a\u064a34695\u0627\u064a\u0646\u062f\u064814814\u0646\u0627\u0627\u06447556\u0622\u064f\u06214518",
  "\u062c\u0646\u0647\u0646183937\u0635\u0641\u062d\u064835711\u0648\u064a\u0648\u064615278\u0648\u0631\u062a\u064a7617\u0628\u0646\u062c\u064a4533 \u0648\u064a\u0648165138\u0627\u062a\u064a34695\u0627\u064a\u0646\u062f\u064814814\u0646\u0627\u0627\u06447556\u0622\u064f\u06214518 \u0627\u0646\u0647\u0646156051\u062a\u0645\u0627\u064533882\u062e\u0648\u062f14702\u0628\u0647\u0631\u062d\u0627\u06447546\u062a\u0627\u0626\u064a\u06464496 \u0627\u0633\u0627\u0646151600\u0648\u0627\u0631\u064633303\u0627\u064a\u062a\u0631\u064813837\u0639\u0627\u0644\u0648\u06477523\u0648\u06834426 \u064a\u0627150984\u0646\u0627\u0644\u064a32736\u062c\u064a\u0627\u064613797\u0646\u0627\u0647\u06467310\u062a\u06464419 \u0633\u0646\u062f\u0633129866\u0647\u062a\u064a32449\u0622\u0647\u064613514\u0622\u064a\u06487218\u06aa\u062c\u06464407 \u0648\u0627\u0631\u064a120907\u062a\u0646\u0647\u064632343\u0628\u063312823\u0647\u064a\u06277206\u0627\u0647\u064a4389 \u0645\u0648\u0646108985\u062a\u0627\u064631957\u0628\u0627\u0648\u062c\u0648\u062f12656\u0647\u062a\u0627\u06467398\u0646\u064a\u0646\u062f4378 \u0627\u0647\u0627101799\u0633\u064831562\u062b\u0627\u0628\u062a12620\u0631\u0647\u0646\u062f\u064a6550\u0622\u064a\u06274357",
  "\u0645\u0648\u0646108985\u062a\u0627\u064631957\u0628\u0627\u0648\u062c\u0648\u062f12656\u0647\u062a\u0627\u06467398\u0646\u064a\u0646\u062f4378 \u0627\u0647\u0627101799\u0633\u064831562\u062b\u0627\u0628\u062a12620\u0631\u0647\u0646\u062f\u064a6550\u0622\u064a\u06274357 \u0622\u0647\u064a100484\u0628\u0627\u0628\u062a30835\u062a\u064812601\u0648\u062c\u0647\u064a6526\u0647\u0648\u0646\u062f4342 \u0647\u064a87537\u064a\u0639\u0646\u064a30146\u0687\u064812592\u062c\u064a\u06aa\u06316493\u0645\u0644\u06bb4331 \u0647\u0626\u064a82867\u0648\u064a\u0646\u062f\u064a28525\u0645\u0639\u0646\u064a12408\u0647\u0648\u0646\u062f\u064a\u0648\u06466453\u0647\u0626\u06334318 \u062c\u064680650\u0627\u0648\u0647\u0627\u064627497\u0631\u0647\u064a\u0648\u064610941\u0627\u062d\u0648\u0627\u06446346\u0633\u0648\u0627\u0650\u06214302 \u0631\u0647\u064a\u064877724\u0648\u0627\u0631\u062727030\u0639\u0646\u0648\u0627\u064610889\u0650\u0621\u0627\u06446029\u0622\u0646\u062f4206 \u0647\u063174937\u0647\u064a\u0648\u064626425\u0628\u0627\u0631\u064a10875\u0648\u0627\u0631\u064a\u0648\u06465457\u062a\u0646\u0647\u06464193 \u0648\u0631\u064a62225\u0635\u0631\u064125442\u0645\u0644\u064a\u064810844\u0645\u0644\u06465321\u06aa\u0631\u064a4165",
  "\u0647\u063174937\u0647\u064a\u0648\u064626425\u0628\u0627\u0631\u064a10875\u0648\u0627\u0631\u064a\u0648\u06465457\u062a\u0646\u0647\u06464193 \u0648\u0631\u064a62225\u0635\u0631\u064125442\u0645\u0644\u064a\u064810844\u0645\u0644\u06465321\u06aa\u0631\u064a4165 \u0631\u0647\u064a60474\u0633\u064a25271\u062c\u0644\u062f10791\u0645\u0644\u0646\u062f\u06485320\u0622\u064a\u06334138 \u062a\u064759968\u062a\u0648\u064624903\u062a\u0646\u0647\u0646\u062c\u064a10790\u062c\u062a\u0627\u06465318\u062c\u0648\u06444123 \u0648\u064a\u062758308\u0647\u064a\u064824871\u0648\u064a10339\u0648\u0627\u0631\u064a\u06465292\u0627\u06bb\u0647\u0648\u0646\u062f4058 \u0647\u062c\u064a51691\u0627\u064a24656\u0648\u064a\u0647\u064a10337\u062d\u0648\u0627\u0644\u06485182\u0622\u064a\u06444037 \u0648\u0627\u0631\u064851501\u0641\u0642\u063722149\u0627\u064a\u0646\u062f\u062710167\u0647\u064a\u06aa\u06315039\u0631\u0647\u06474013 Table 4: Partial list of most frequent Sindhi stop words along with frequency in the developed corpus. 11",
  "A PREPRINT - JANUARY 20, 2022 Parameter CBoW, SG GloVe Epoch 40 40 lr 0.25 0.25 D 300 300 minn char 02 \u2013 maxn char 07 \u2013 ws 7 7 NS 20 \u2013 minw 4 4 Table 5: Optimized parameters for CBoW, SG and GloVe models score in retrieving nearest neighboring words, the semantic, syntactic similarity between word pairs, WordSim353, and visualization of the distance between twenty nearest neighbours using t-SNE respectively. All the experiments are conducted on GTX 1080-TITAN GPU. 5.1 Hyperparameter optimization The state-of-the-art SG, CBoW [28] [34] [21] [25] and Glove [27] word embedding algorithms are evaluated by parameter tuning for development of Sindhi word embeddings. These parameters can be categories into dictionary and algorithm based, respectively. The integration of character n-gram in learning word representations is an ideal method especially for rich morphological languages because this approach has the ability to compute rare and misspelled words. Sindhi is also a rich morphological language.",
  "These parameters can be categories into dictionary and algorithm based, respectively. The integration of character n-gram in learning word representations is an ideal method especially for rich morphological languages because this approach has the ability to compute rare and misspelled words. Sindhi is also a rich morphological language. Therefore more robust embeddings became possible to train with the hyperparameter optimization of SG, CBoW and GloVe algorithms. We tuned and evaluated the hyperparameters of three algorithms individually which are discussed as follows: \u2022 Number of Epochs: Generally, more epochs on the corpus often produce better results but more epochs take long training time. Therefore, we evaluate 10, 20, 30 and 40 epochs for each word embedding model, and 40 epochs constantly produce good results. \u2022 Learning rate (lr): We tried lr of 0.05, 0.1, and 0.25, the optimal lr (0.25) gives the better results for training all the embedding models.",
  "\u2022 Learning rate (lr): We tried lr of 0.05, 0.1, and 0.25, the optimal lr (0.25) gives the better results for training all the embedding models. \u2022 Dimensions (D): We evaluate and compare the quality of 100\u2212D, 200\u2212D, and 300\u2212D using WordSim353 on different ws, and the optimal 300 \u2212D are evaluated with cosine similarity matrix for querying nearest neighboring words and calculating the similarity between word pairs. The embedding dimensions have little affect on the quality of the intrinsic evaluation process. However, the selection of embedding dimensions might have more impact on the accuracy in certain downstream NLP applications. The lower embedding dimensions are faster to train and evaluate. \u2022 Character n-grams: The selection of minimum (minn) and the maximum (maxn) length of character n \u2212grams is an important parameter for learning character-level representations of words in CBoW and SG models. Therefore, the n-grams from 3 \u22129 were tested to analyse the impact on the accuracy of embedding.",
  "Therefore, the n-grams from 3 \u22129 were tested to analyse the impact on the accuracy of embedding. We optimized the length of character n-grams from minn = 2 and maxn = 7 by keeping in view the word frequencies depicted in Table 3. \u2022 Window size (ws): The large ws means considering more context words and similarly less ws means to limit the size of context words. By changing the size of the dynamic context window, we tried the ws of 3, 5, 7 the optimal ws=7 yield consistently better performance. \u2022 Negative Sampling (NS): : The more negative examples yield better results, but more negatives take long training time. We tried 10, 20, and 30 negative examples for CBoW and SG. The best negative examples of 20 for CBoW and SG signi\ufb01cantly yield better performance in average training time. \u2022 Minimum word count (minw): We evaluated the range of minimum word counts from 1 to 8 and analyzed that the size of input vocabulary is decreasing at a large scale by ignoring more words similarly the vocabulary size was increasing by considering rare words.",
  "\u2022 Minimum word count (minw): We evaluated the range of minimum word counts from 1 to 8 and analyzed that the size of input vocabulary is decreasing at a large scale by ignoring more words similarly the vocabulary size was increasing by considering rare words. Therefore, by ignoring words with a frequency of less than 4 in CBoW, SG, and GloVe consistently yields better results with the vocabulary of 200,000 words. \u2022 Loss function (ls): we use hierarchical softmax (hs) for CBoW, negative sampling (ns) for SG and default loss function for GloVe [27]. \u2022 The recommended verbosity level, number of buckets, sampling threshold, number of threads are used for training CBoW, SG [25], and GloVe [27]. 12",
  "A PREPRINT - JANUARY 20, 2022 6 Word similarity comparison of Word Embeddings 6.1 Nearest neighboring words The cosine similarity matrix [36] is a popular approach to compute the relationship between all embedding dimensions of their distinct relevance to query word. The words with similar context get high cosine similarity and geometrical relatedness to Euclidean distance, which is a common and primary method to measure the distance between a set of words and nearest neighbors. Each word contains the most similar top eight nearest neighboring words determined by the highest cosine similarity score using Eq. 10. We present the English translation of both query and retrieved words also discuss with their English meaning for ease of relevance judgment between the query and retrieved words.To take a closer look at the semantic and syntactic relationship captured in the proposed word embeddings, Table 6 shows the top eight nearest neighboring words of \ufb01ve different query words Friday, Spring, Cricket, Red, Scientist taken from the vocabulary. As the \ufb01rst query word Friday returns the names of days Saturday, Sunday, Monday, Tuesday, Wednesday, Thursday in an unordered sequence. The SdfastText returns \ufb01ve names of days Sunday, Thursday, Monday, Tuesday and Wednesday respectively.",
  "As the \ufb01rst query word Friday returns the names of days Saturday, Sunday, Monday, Tuesday, Wednesday, Thursday in an unordered sequence. The SdfastText returns \ufb01ve names of days Sunday, Thursday, Monday, Tuesday and Wednesday respectively. The GloVe model also returns \ufb01ve names of days. However, CBoW and SG gave six names of days except Wednesday along with different writing forms of query word Friday being written in the Sindhi language which shows that CBoW and SG return more relevant words as compare to SdfastText and GloVe. The CBoW returned Add and GloVe returns Honorary words which are little similar to the querry word but SdfastText resulted two irrelevant words Kameeso (N) which is a name (N) of person in Sindhi and Phrase is a combination of three Sindhi words which are not tokenized properly.",
  "Similarly, nearest neighbors of second query word Spring are retrieved accurately as names and seasons and semantically related to query word Spring by CBoW, SG and Glove but SdfastText returned four irrelevant words of Dilbahar (N), Pharase, Ashbahar (N) and Farzana (N) out of eight. The third query word is Cricket, the name of a popular game. The \ufb01rst retrieved word in CBoW is Kabadi (N) that is a popular national game in Pakistan. Including Kabadi (N) all the returned words by CBoW, SG and GloVe are related to Cricket game or names of other games. But the \ufb01rst word in SdfastText contains a punctuation mark in retrieved word Gone.Cricket that are two words joined with a punctuation mark (.), which shows the tokenization error in preprocessing step, sixth retrieved word Misspelled is a combination of three words not related to query word, and Played, Being played are also irrelevant and stop words. Moreover, fourth query word Red gave results that contain names of closely related to query word and different forms of query word written in the Sindhi language.",
  "Moreover, fourth query word Red gave results that contain names of closely related to query word and different forms of query word written in the Sindhi language. The last returned word Unknown by SdfastText is irrelevant and not found in the Sindhi dictionary for translation. The last query word Scientist also contains semantically related words by CBoW, SG, and GloVe, but the \ufb01rst Urdu word given by SdfasText belongs to the Urdu language which means that the vocabulary may also contain words of other languages. Another unknown word returned by SdfastText does not have any meaning in the Sindhi dictionary. More interesting observations in the presented results are the diacritized words retrieved from our proposed word embeddings and The authentic tokenization in the preprocessing step presented in Figure 1. However, SdfastText has returned tri-gram words of Phrase in query words Friday, Spring, a Misspelled word in Cricket and Scientist query words. Hence, the overall performance of our proposed SG, CBoW, and GloVe demonstrate high semantic relatedness in retrieving the top eight nearest neighbor words. 6.2 Word pair relationship Generally, closer words are considered more important to a word\u2019s meaning.",
  "Hence, the overall performance of our proposed SG, CBoW, and GloVe demonstrate high semantic relatedness in retrieving the top eight nearest neighbor words. 6.2 Word pair relationship Generally, closer words are considered more important to a word\u2019s meaning. The word embeddings models have the ability to capture the lexical relations between words. Identifying such relationship that connects words is important in NLP applications. We measure that semantic relationship by calculating the dot product of two vectors using Eq. 10. The high cosine similarity score denotes the closer words in the embedding matrix, while less cosine similarity score means the higher distance between word pairs. We present the cosine similarity score of different semantically or syntactically related word pairs taken from the vocabulary in Table 7 along with English translation, which shows the average similarity of 0.632, 0.650, 0.591 yields by CBoW, SG and GloVe respectively. The SG model achieved a high average similarity score of 0.650 followed by CBoW with a 0.632 average similarity score. The GloVe also achieved a considerable average score of 0.591 respectively.",
  "The SG model achieved a high average similarity score of 0.650 followed by CBoW with a 0.632 average similarity score. The GloVe also achieved a considerable average score of 0.591 respectively. However, the average similarity score of SdfastText is 0.388 and the word pair Microsoft-Bill Gates is not available in the vocabulary of SdfastText. This shows that along with performance, the vocabulary in SdfastText is also limited as compared to our proposed word embeddings. Moreover, the average semantic relatedness similarity score between countries and their capitals is shown in Table 8 with English translation, where SG also yields the best average score of 0.663 followed by CBoW with 0.611 similarity score. The GloVe also yields better semantic relatedness of 0.576 and the SdfastText yield an average score of 0.391. The \ufb01rst query word China-Beijing is not available the vocabulary of SdfastText.",
  "The GloVe also yields better semantic relatedness of 0.576 and the SdfastText yield an average score of 0.391. The \ufb01rst query word China-Beijing is not available the vocabulary of SdfastText. However, the similarity score between Afghanistan-Kabul is lower in our proposed CBoW, SG, GloVe models because the word Kabul is the name of the capital of Afghanistan as well as it frequently appears as an adjective in Sindhi text which means able. 13",
  "A PREPRINT - JANUARY 20, 2022 Query SdfastText Eng. Trans. CBoW Eng. Trans. SG Eng. Trans. GloVe Eng. Trans \u062c\u0645\u0639\u0648 \u0622\u0686\u0631Sunday\u0633\u0648\u0645\u0631Monday\u062c\u0645\u0639\u0648\u0646Friday\u062c\u0645\u0639\u0631\u0627\u062aThursday Friday  \u0628\u0631\u0648\u0632On the day  \u062c\u0645\u0639\u0648\u0646Friday\u062c\u0645\u0639\u064aFriday\u0627\u0631\u0628\u0639Wednesday  \u062e\u0645\u064a\u0633\u0648Kameeso (N)  \u0622\u0686\u0631Sunday\u062c\u0645\u0639\u0627Fridays\u0687\u0646\u0687\u0631Saturday  \u062e\u0645\u064a\u0633Thursday  \u0627\u06b1\u0627\u0631\u0648Tuesday\u062e\u0645\u064a\u0633Thursday\u0628\u0631\u0648\u0632On the day  \u0633\u0648\u0645\u0631Monday\u062c\u0645\u0639\u0627\u06bb\u064aOn Friday\u0633\u0648\u0645\u0631Monday\u0645\u0627\u0646\u0627\u0626\u062a\u064aHonorary  \u0627\u06b1\u0627\u0631\u0648Tuesday\u062e\u0645\u064a\u0633Thursday\u0687\u0646\u0687\u0631Saturday\u062e\u0645\u064a\u0633Thursday \u0627\u0631\u0628\u0639Wednesday  \u0687\u0646\u0687\u0631Saturday  \u0622\u0686\u0631Sunday\u062c\u0645\u0639\u064aFriday \u0627\u0628\u0699\u0648\u0622\u0647\u064a.",
  "\u0647\u064aPhrase\u0622\u0686\u0631Sunday\u0627\u06b1\u0627\u0631\u0648Tuesday\u062c\u0645\u0639\u0648\u0646Friday \u0628\u0647\u0627\u0631 \u0628\u0647\u0627\u0631\u062c\u0648of spring    \u0628\u0647\u0627\u0631Springs\u0628\u0647\u0627\u0631\u0627\u0646Springs\u0628 \u0647\u0627\u0631\u064aComfort Spring   \u0628\u0647\u0627\u0631spring  \u062e\u0632\u0627\u0646Autumn   \u0628\u0647\u0627\u0631Springs\u0633\u062f\u0627Ever \u062f\u0644\u0628\u0647\u0627\u0631Dilbahar (N)  \u067e\u064f\u0631\u0628\u0647\u0627\u0631Mid-autumn\u062e\u0632\u0627\u0646Autumn\u062e\u0648\u0628\u0635\u0648\u0631\u062a\u064a\u0621Beauty \u0628\u0647\u0627\u0631\u0646Springs  \u0633\u064f\u0631\u0647\u0627\u06bbFragrance \u067d\u0699\u0646\u062f\u0699Bloom   \u0628\u0647\u0627\u0631Spring  \u062c\u0648\u0645\u0634\u0647\u0648\u0631\u06af\u0644\u0648Phrase  \u0628\u0647\u0627\u0631\u0646On Springs\u067e\u064f\u0631\u0628\u0647\u0627\u0631mid spring\u0628\u0647\u0627\u0631\u0646Springs  \u0627\u0634\u0628\u0647\u0627\u0631Ashbahar (N)\u0633\u062f\u0627\u0628\u0647\u0627\u0631Ever spring\u0633\u064a\u0627\u0631\u0648Winter\u062e\u0648\u0634\u0628\u0648Fragrance  \u0628\u0648\u062f\u0644\u0648Bodlo (N)\u0633\u064a\u0627\u0631\u0648winter\u0627\u0648\u0646\u0647\u0627\u0631\u0648summer\u062e\u0632\u0627\u0646Autumn  \u0641\u0631\u0632\u0627\u0646\u0647Farzana (N)\u0627\u0648\u0646\u0647\u0627\u0631\u0648summer",
  "spring\u0633\u064a\u0627\u0631\u0648Winter\u062e\u0648\u0634\u0628\u0648Fragrance  \u0628\u0648\u062f\u0644\u0648Bodlo (N)\u0633\u064a\u0627\u0631\u0648winter\u0627\u0648\u0646\u0647\u0627\u0631\u0648summer\u062e\u0632\u0627\u0646Autumn  \u0641\u0631\u0632\u0627\u0646\u0647Farzana (N)\u0627\u0648\u0646\u0647\u0627\u0631\u0648summer   \u0628\u0647\u0627\u0631spring\u067e\u064f\u0631\u0628\u0647\u0627\u0631Mid spring \u06aa\u0631\u06aa\u064a\u067d \u0648\u064a\u0648. \u06aa\u0631\u06aa\u064a\u067dGone.cricket\u06aa\u067b\u068a\u064aKabadi (N)\u06aa\u0631\u06aa\u064a\u067d\u0631\u0646Cricketers\u06aa\u0631\u06aa\u064a\u067d\u0631Cricketer Cricket\u06aa\u0631\u06aa\u064a\u067d\u0631\u0632Cricketers\u067d\u0648\u0631\u0646\u0627\u0645\u064a\u0646\u067dTournament\u06aa\u0631\u06aa\u064a\u067d\u0631Cricketers  \u067d\u0648\u0626\u0646\u067d\u064a\u0626Twenty \u06aa\u0631\u06aa\u064a\u067d\u0631\u0646Cricketers\u06aa\u0631\u06aa\u064a\u067d\u0631\u0646Cricketers  \u0647\u0627\u06aa\u064aHockey\u06af\u0631\u0627\u0626\u0648\u0646\u068a\u0646Grounds \u06aa\u0631\u06aa\u064a\u067d\u0631Cricketer  \u0645\u0626\u0686Match",
  "\u0647\u0627\u06aa\u064aHockey\u06af\u0631\u0627\u0626\u0648\u0646\u068a\u0646Grounds \u06aa\u0631\u06aa\u064a\u067d\u0631Cricketer  \u0645\u0626\u0686Match \u067d\u0648\u0626\u0646\u067d\u064a\u0626Twenty\u06aa\u0631\u06aa\u064a\u067d\u0631\u0646Cricketers \u067d\u0648\u0626\u0646\u067d\u064a20 T-Twenty   \u0631\u0627\u0646\u062f\u064a\u06af\u0631Players  \u0631\u0627\u0646\u062fGame\u067d\u064a\u0633\u067dTest \u0639\u06aa\u0633\u0644\u0648\u0646\u06aa\u0631\u06aaMisspelled\u0631\u0627\u0646\u062fGame  \u0645\u0626\u0686Match  \u0645\u0626\u0686Match \u06a9\u064a\u068f\u064a\u0648Played\u0628\u0626\u067dBat\u0641\u06aa\u0633\u0646\u06afFixing\u067d\u0648\u0626\u0646\u067d\u064aTwenty \u06a9\u064a\u068f\u064a\u0644Being played\u0647\u0627\u06aa\u064aHockey\u0628\u0627\u0644Bat\u0647\u0627\u0631\u0627\u0626\u064aLost \u06b3\u0627\u0699\u0647\u0648 \u06b3\u0627\u0699\u0647\u0648\u064aReddish\u06b3\u0627\u0699\u0647\u064aRed \u06b3\u0627\u0699\u0647\u0648\u064aReddish  \u0627\u0644\u0644\u067d\u064a\u0646Red lamp Red  \u06b3\u0627\u0699\u0647\u06c1Red",
  "\u06b3\u0627\u0699\u0647\u0648 \u06b3\u0627\u0699\u0647\u0648\u064aReddish\u06b3\u0627\u0699\u0647\u064aRed \u06b3\u0627\u0699\u0647\u0648\u064aReddish  \u0627\u0644\u0644\u067d\u064a\u0646Red lamp Red  \u06b3\u0627\u0699\u0647\u06c1Red \u06b3\u0627\u0699\u0647\u0648\u064aReddish  \u06b3\u0627\u0699\u0647\u064aRed  \u06b3\u0627\u0699\u0647\u0627\u0626\u06bbLight red  \u06b3\u0627\u0699\u0647\u0647Red\u0627\u0687\u0648White  \u06b3\u0627\u0699\u0647\u0647Red  \u0647\u064a\u068a\u0648Yellowish  \u06b3\u0627\u0699\u0647\u0633\u0631\u0648Reddish  \u067e\u064a\u0644\u0648Yellowish\u06b3\u0627\u0699\u0647\u064a\u0631\u0699\u0648Reddish  \u06b3\u0627\u0699\u0647\u0648\u064aReddish  \u06b3\u0627\u0699\u0647\u0627\u0626\u06bbLight red  \u0647\u064a\u068a\u0648Yellowish\u06a6\u06aa\u0648Yellow  \u06b3\u0627\u0699\u0647\u0633\u0631\u0648Reddish \u06b3\u0627\u0699\u0647\u064a\u0646Red\u2019s\u06b3\u0627\u0699\u0647\u0647Red\u06b3\u0627\u0699\u0647\u0633\u0631\u0648Reddish\u06b3\u0627\u0699\u0647\u0647Red",
  "\u0647\u064a\u068a\u0648Yellowish\u06a6\u06aa\u0648Yellow  \u06b3\u0627\u0699\u0647\u0633\u0631\u0648Reddish \u06b3\u0627\u0699\u0647\u064a\u0646Red\u2019s\u06b3\u0627\u0699\u0647\u0647Red\u06b3\u0627\u0699\u0647\u0633\u0631\u0648Reddish\u06b3\u0627\u0699\u0647\u0647Red  \u06b3\u0627\u0699\u0647\u0627\u06bbLight red\u06b3\u0627\u0699\u0647\u0633\u0631\u064aReddish  \u06b3\u0627\u0699\u0647\u0627\u06bbLight red\u06b3\u0627\u0699\u0647\u0627\u0626\u06bbLight red \u0647\u0627\u0699\u0647\u0648Unknown\u06b3\u0627\u0699\u0647\u0633\u0631\u0648Reddish   \u06b3\u0627\u0699\u0647\u0627\u0626\u06bbLight red\u06b3\u0627\u0699\u0647\u064aRed \u0633\u0627\u0626\u0646\u0633\u062f\u0627\u0646 \u0633\u0627\u0626\u0646\u0633\u062f\u0627\u0646\u0648 \u06baUrdu word\u0633\u0627\u0626\u0646\u0633\u062f\u0627\u0646\u0646Scientists\u06aa\u064a\u0645\u064a\u0627\u062f\u0627\u0646Chemist \u0633\u0648\u0634\u0644Social Scientist\u0633\u0627\u0626\u0646\u0633\u062f\u0627\u0646\u0646Scientists  \u0645\u0641\u06aa\u0631Thinker\u0633\u0627\u0626\u0646\u0633\u062f\u0627\u0646\u0646Scientists \u0633\u0627\u0626\u0646\u0633\u062f\u0627\u0646Scientist \u0633\u0627\u0626\u0646\u0633\u0646Sciences \u0641\u0627\u0644\u0633\u0627\u0641\u0631philosopher",
  "\u0633\u0648\u0634\u0644Social Scientist\u0633\u0627\u0626\u0646\u0633\u062f\u0627\u0646\u0646Scientists  \u0645\u0641\u06aa\u0631Thinker\u0633\u0627\u0626\u0646\u0633\u062f\u0627\u0646\u0646Scientists \u0633\u0627\u0626\u0646\u0633\u062f\u0627\u0646Scientist \u0633\u0627\u0626\u0646\u0633\u0646Sciences \u0641\u0627\u0644\u0633\u0627\u0641\u0631philosopher  \u0622\u0626\u0646\u0633\u067d\u0627\u0626\u0646Einstein  \u0633\u0626\u06aa\u0627\u0627\u0644\u062c\u0633\u067dPsychologist \u0633\u0627\u0626\u0646\u0633\u0646\u062f\u0627\u0646\u0646Misspelled\u0627\u064a\u0646\u06af\u0632\u064a\u0645\u064a\u0646\u068a\u0631Anaximander  \u0633\u0627\u0626\u0646\u067d\u0633\u067dScientist\u06aa\u0627\u0626\u0631\u0647Kaira (N) \u0633\u0627\u0626\u0646\u0633\u0646\u062f\u0627\u0646Misspelled\u06aa\u064a\u0645\u064a\u0627\u062f\u0627\u0646Chemist  \u0627\u0633\u067d\u0627\u0626\u0646Stein\u06af\u064a\u0644\u0648\u0627\u0646\u064aGailwani \u0646\u064a\u06aa\u0648\u067dUnknown\u0645\u0627\u0647\u0631Expert\u0633\u0627\u0626\u0646\u0633\u062f\u0627\u0646Scientist\u0641\u0627\u0644\u0633\u0627\u0641\u0631Philosopher \u0633\u0627\u0626\u0646\u0633 \u064aScientific\u0633\u0627\u0626\u0646\u0633Science\u0622\u0626\u0646\u0633\u067d\u0627\u0626\u064a\u0646Einstein  \u0645\u0641\u06aa\u0631Thinker Table 6: Eight nearest neighboring words of each query word with English translation 14",
  "\u0633\u0627\u0626\u0646\u0633 \u064aScientific\u0633\u0627\u0626\u0646\u0633Science\u0622\u0626\u0646\u0633\u067d\u0627\u0626\u064a\u0646Einstein  \u0645\u0641\u06aa\u0631Thinker Table 6: Eight nearest neighboring words of each query word with English translation 14",
  "A PREPRINT - JANUARY 20, 2022 Word pair English Translation SdfastText CBoW SG GloVe \u0627\u0633\u062a\u0627\u062f-\u0634\u0627\u06af\u0631\u062fTeacher-Student 0.306 0.635 0.558 0.633 \u0622\u0626\u0646\u0633\u067d\u0627\u0626\u0646- \u0633\u0627\u0626\u0646\u0633\u062f\u0627\u0646Einstein-Scientist 0.432 0.610 0.673 0.621 \u067d\u064a\u0628\u0644-\u06aa\u0631\u0633\u064aTable-chair 0.284 0.520 0.539 0.492 \u06af\u0644-\u06af\u0627\u0644\u0628Flower-Rose 0.347 0.796 0.638 0.588 \u0639\u0648\u0631\u062a- \u0687\u0648\u06aa\u0631\u064aWoman-Girl 0.264 0.601 0.573 0.543 \u068f\u0627\u068f\u0648-\u068f\u0627\u068f\u064aGrandfather-Grandmother 0.486 0.787 0.800 0.691 \u0645\u0631\u062f- \u0687\u0648\u06aa\u0631\u0648Man-boy 0.223 0.451 0.511 0.",
  "543 \u068f\u0627\u068f\u0648-\u068f\u0627\u068f\u064aGrandfather-Grandmother 0.486 0.787 0.800 0.691 \u0645\u0631\u062f- \u0687\u0648\u06aa\u0631\u0648Man-boy 0.223 0.451 0.511 0.472 \u0633\u0646\u068c- \u06aa\u0631\u0627\u0686\u064aSindh-Karachi 0.472 0.567 0.669 0.647 \u067e\u0646\u062c\u0627\u0628-\u0627\u0644\u0647\u0648\u0631Panjab-Lahore 0.386 0.528 0.569 0.513 \u0686\u064a\u0646-\u0686\u064a\u0646\u064aChina-Chinese 0.508 0.654 0.746 0.614 \u0622\u0645\u0631\u064a\u06aa\u0627-\u0622\u0645\u0631\u064a\u06aa\u064aAmerica-American 0.566 0.804 0.875 0.687 \u0645\u0627\u0626\u06aa\u0631\u0648\u0633\u0627\u0641\u067d-\u0628\u0644\u06af\u064a\u067d\u0633Microsoft-Bill Gates NA 0.527 0.502 0.483 Average 0.388 0.632 0.650 0.",
  "566 0.804 0.875 0.687 \u0645\u0627\u0626\u06aa\u0631\u0648\u0633\u0627\u0641\u067d-\u0628\u0644\u06af\u064a\u067d\u0633Microsoft-Bill Gates NA 0.527 0.502 0.483 Average 0.388 0.632 0.650 0.591 Table 7: Word pair relationship using cosine similarity (higher is better) Word pair English Translation SdfastText CBoWs SG GloVe \u0686\u0627\u0626\u0646\u0647-\u0628\u064a\u062c\u0646\u06afChina-Beijing N.A 0.594 0.743 0.542 \u0622\u0645\u0631\u064a\u06aa\u0627-\u0646\u064a\u0648\u064a\u0627\u0631\u06aaAmerica-New York 0.371 0.635 0.689 0.518 \u062c\u0627\u067e\u0627\u0646-\u067d\u0648\u06aa\u064a\u0648Japan-Tokyo 0.451 0.610 0.643 0.806 \u0627\u0646\u068a\u064a\u0627-\u0645\u0645\u0628\u0626\u064aIndia-Mumbai 0.266 0.651 0.759 0.628 \u0628\u0646\u06af\u0627\u0644\u062f\u064a\u0634-\u068d\u0627\u06aa\u0627Bangladesh-Dhaka 0.428 0.629 0.",
  "806 \u0627\u0646\u068a\u064a\u0627-\u0645\u0645\u0628\u0626\u064aIndia-Mumbai 0.266 0.651 0.759 0.628 \u0628\u0646\u06af\u0627\u0644\u062f\u064a\u0634-\u068d\u0627\u06aa\u0627Bangladesh-Dhaka 0.428 0.629 0.633 0.593 \u0627\u064a\u0631\u0627\u0646-\u062a\u0647\u0631\u0627\u0646Iran-Tehran 0.431 0.673 0.769 0.561 \u0627\u0641\u063a\u0627\u0646\u0633\u062a\u0627\u0646-\u0642\u0627\u0628\u0644Afghanistan-Kabul 0.103 0.267 0.283 0.215 \u0639\u0631\u0627\u0642-\u0628\u063a\u062f\u0627\u062fIraq-Baghdad 0.450 0.695 0.712 0.542 \u0633\u0639\u0648\u062f\u064a-\u0631\u064a\u0627\u0636Saudi-Riyadh 0.454 0.576 0.686 0.616 \u0645\u0627\u0644\u0626\u064a\u0634\u064a\u0627-\u06aa\u0648\u0627\u0627\u0644\u0644\u0645\u067e\u0648\u0631Malaysia-Kuala Lumpur 0.573 0.786 0.721 0.712 Average 0.391 0.611 0.663 0.",
  "576 0.686 0.616 \u0645\u0627\u0644\u0626\u064a\u0634\u064a\u0627-\u06aa\u0648\u0627\u0627\u0644\u0644\u0645\u067e\u0648\u0631Malaysia-Kuala Lumpur 0.573 0.786 0.721 0.712 Average 0.391 0.611 0.663 0.576 Table 8: Cosine similarity score between country and capital 6.3 Comparison with WordSim353 We evaluate the performance of our proposed word embeddings using the WordSim353 dataset by translation English word pairs to Sindhi. Due to vocabulary differences between English and Sindhi, we were unable to \ufb01nd the authentic meaning of six terms, so we left these terms untranslated. So our \ufb01nal Sindhi WordSim353 consists of 347 word pairs. Table 9 shows the Spearman correlation results using Eq. 11 on different dimensional embeddings on the translated WordSim353. The Table 9 presents complete results with the different ws for CBoW, SG and GloVe in which the ws=7 subsequently yield better performance than ws of 3 and 5, respectively.",
  "11 on different dimensional embeddings on the translated WordSim353. The Table 9 presents complete results with the different ws for CBoW, SG and GloVe in which the ws=7 subsequently yield better performance than ws of 3 and 5, respectively. The SG model outperforms CBoW and GloVe in semantic and syntactic similarity by achieving the performance of 0.629 with ws=7. In comparison with English [28] achieved the average semantic and syntactic similarity of 0.637, 0.656 with CBoW and SG, respectively. Therefore, despite the challenges in translation from English to Sindhi, our proposed Sindhi word embeddings have ef\ufb01ciently captured the semantic and syntactic relationship. 15",
  "A PREPRINT - JANUARY 20, 2022 Model ws Accuracy CBoW 3 0.568 5 0.582 7 0.596 Skip gram 3 0.617 5 0.621 7 0.629 GloVe 3 0.542 5 0.563 7 0.568 SdfastText 0.374 Table 9: Comparison of semantic and syntactic accuracy of proposed word embeddings using WordSim-353 dataset on 300 \u2212D embedding choosing various window size (ws). Figure 4: Visualization of Sindhi CBoW word embeddings 6.4 Visualization We use t-Distributed Stochastic Neighboring (t-SNE) dimensionality [37] reduction algorithm with PCA [38] for exploratory embeddings analysis in 2-dimensional map. The t-SNE is a non-linear dimensionality reduction algorithm for visualization of high dimensional datasets. It starts the probability calculation of similar word clusters in high- dimensional space and calculates the probability of similar points in the corresponding low-dimensional space.",
  "The t-SNE is a non-linear dimensionality reduction algorithm for visualization of high dimensional datasets. It starts the probability calculation of similar word clusters in high- dimensional space and calculates the probability of similar points in the corresponding low-dimensional space. The purpose of t-SNE for visualization of word embeddings is to keep similar words close together in 2-dimensional x, y coordinate pairs while maximizing the distance between dissimilar words. The t-SNE has a perplexity (PPL) tunable parameter used to balance the data points at both the local and global levels. We visualize the embeddings using PPL=20 on 5000-iterations of 300-D models. We use the same query words (see Table 6) by retrieving the top 20 nearest neighboring word clusters for a better understanding of the distance between similar words. Every query word has a distinct color for the clear visualization of a similar group of words. The closer word clusters show the high similarity between the query and retrieved word clusters. The word clusters in SG (see Fig. 5) are closer to their group of semantically related words. Secondly, the CBoW model depicted in Fig. 4 and GloVe Fig.",
  "The closer word clusters show the high similarity between the query and retrieved word clusters. The word clusters in SG (see Fig. 5) are closer to their group of semantically related words. Secondly, the CBoW model depicted in Fig. 4 and GloVe Fig. 6 also show the better cluster formation of words than SdfastText Fig. 7, respectively. 7 Discussion and future work In this era of the information age, the existence of LRs plays a vital role in the digital survival of natural languages because the NLP tools are used to process a \ufb02ow of un-structured data from disparate sources. It is imperative to mention that presently, Sindhi Persian-Arabic is frequently used in online communication, newspapers, public institutions in Pakistan and India. Due to the growing use of Sindhi on web platforms, the need for its LRs is also increasing for the development of language technology tools. But little work has been carried out for the development of resources which is not suf\ufb01cient to design a language independent or machine learning algorithms. The present work is a \ufb01rst comprehensive initiative on resource development along with their evaluation for statistical Sindhi language processing.",
  "But little work has been carried out for the development of resources which is not suf\ufb01cient to design a language independent or machine learning algorithms. The present work is a \ufb01rst comprehensive initiative on resource development along with their evaluation for statistical Sindhi language processing. More recently, the NN based approaches have produced a state-of-the-art performance in NLP by exploiting unsupervised word embeddings learned from the large unlabelled corpus. Such word embeddings have also motivated 16",
  "A PREPRINT - JANUARY 20, 2022 Figure 5: Visualization of Sindhi SG word embeddings Figure 6: visualization of Sindhi GloVe word embeddings Figure 7: Visualization of SdfastText word embeddings 17",
  "A PREPRINT - JANUARY 20, 2022 the work on low-resourced languages. Our work mainly consists of novel contributions of resource development along with comprehensive evaluation for the utilization of NN based approaches in SNLP applications. The large corpus obtained from multiple web resources is utilized for the training of word embeddings using SG, CBoW and Glove models. The intrinsic evaluation along with comparative results demonstrates that the proposed Sindhi word embeddings have accurately captured the semantic information as compare to recently revealed SdfastText word vectors. The SG yield best results in nearest neighbors, word pair relationship and semantic similarity. The performance of CBoW is also close to SG in all the evaluation matrices. The GloVe also yields better word representations; however SG and CBoW models surpass the GloVe model in all evaluation matrices. Hyperparameter optimization is as important as designing a new algorithm. The choice of optimal parameters is a key aspect of performance gain in learning robust word embeddings. Moreover, We analysed that the size of the corpus and careful preprocessing steps have a large impact on the quality of word embeddings.",
  "Hyperparameter optimization is as important as designing a new algorithm. The choice of optimal parameters is a key aspect of performance gain in learning robust word embeddings. Moreover, We analysed that the size of the corpus and careful preprocessing steps have a large impact on the quality of word embeddings. However, in algorithmic perspective, the character-level learning approach in SG and CBoW improves the quality of representation learning, and overall window size, learning rate, number of epochs are the core parameters that largely in\ufb02uence the performance of word embeddings models. Ultimately, the new corpus of low-resourced Sindhi language, list of stop words and pretrained word embeddings along with empirical evaluation, will be a good supplement for future research in SSLP applications. In the future, we aim to use the corpus for annotation projects such as parts-of-speech tagging, named entity recognition. The proposed word embeddings will be re\ufb01ned further by creating custom benchmarks and the extrinsic evaluation approach will be employed for the performance analysis of proposed word embeddings. Moreover, we will also utilize the corpus using Bi-directional Encoder Representation Transformer [14] for learning deep contextualized Sindhi word representations.",
  "The proposed word embeddings will be re\ufb01ned further by creating custom benchmarks and the extrinsic evaluation approach will be employed for the performance analysis of proposed word embeddings. Moreover, we will also utilize the corpus using Bi-directional Encoder Representation Transformer [14] for learning deep contextualized Sindhi word representations. Furthermore, the generated word embeddings will be utilized for the automatic construction of Sindhi WordNet. 8 Conclusion In this paper, we mainly present three novel contributions of large corpus development contains large vocabulary of more than 61 million tokens, 908,456 unique words. Secondly, the list of Sindhi stop words is constructed by \ufb01nding their high frequency and least importance with the help of Sindhi linguistic expert. Thirdly, the unsupervised Sindhi word embeddings are generated using state-of-the-art CBoW, SG and GloVe algorithms and evaluated using popular intrinsic evaluation approaches of cosine similarity matrix and WordSim353 for the \ufb01rst time in Sindhi language processing. We translate English WordSim353 using the English-Sindhi bilingual dictionary, which will also be a good resource for the evaluation of Sindhi word embeddings.",
  "We translate English WordSim353 using the English-Sindhi bilingual dictionary, which will also be a good resource for the evaluation of Sindhi word embeddings. Moreover, the proposed word embeddings are also compared with recently revealed SdfastText word representations. Our empirical results demonstrate that our proposed Sindhi word embeddings have captured high semantic relat- edness in nearest neighboring words, word pair relationship, country, and capital and WordSim353. The SG yields the best performance than CBoW and GloVe models subsequently. However, the performance of GloVe is low on the same vocabulary because of character-level learning of word representations and sub-sampling approaches in SG and CBoW. Our proposed Sindhi word embeddings have surpassed SdfastText in the intrinsic evaluation matrix. Also, the vocabulary of SdfastText is limited because they are trained on a small Wikipedia corpus of Sindhi Persian-Arabic. We will further investigate the extrinsic performance of proposed word embeddings on the Sindhi text classi\ufb01cation task in the future. The proposed resources along with systematic evaluation will be a sophisticated addition to the computational resources for statistical Sindhi language processing. References [1] Jennifer Cole. Sindhi.",
  "The proposed resources along with systematic evaluation will be a sophisticated addition to the computational resources for statistical Sindhi language processing. References [1] Jennifer Cole. Sindhi. encyclopedia of language & linguistics volume8, 2006. [2] Raveesh Motlani. Developing language technology tools and resources for a resource-poor language: Sindhi. In Proceedings of the NAACL Student Research Workshop, pages 51\u201358, 2016. [3] Hidayatullah Shaikh, Javed Ahmed Mahar, and Mumtaz Hussain Mahar. Instant diacritics restoration system for sindhi accent prediction using n-gram and memory-based learning approaches. INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS, 8(4):149\u2013157, 2017. [4] Abdul-Majid Bhurgri. Enabling pakistani languages through unicode. Microsoft Corporation white paper at http://download. microsoft. com/download/1/4/2/142aef9f-1a74-4a24-b1f4-782d48d41a6d/PakLang.",
  "Enabling pakistani languages through unicode. Microsoft Corporation white paper at http://download. microsoft. com/download/1/4/2/142aef9f-1a74-4a24-b1f4-782d48d41a6d/PakLang. pdf, 2006. [5] Wazir Ali Jamro. Sindhi language processing: A survey. In 2017 International Conference on Innovations in Electrical Engineering and Computational Technologies (ICIEECT), pages 1\u20138. IEEE, 2017. 18",
  "A PREPRINT - JANUARY 20, 2022 [6] Edward Loper and Steven Bird. Nltk: the natural language toolkit. In Proceedings of the ACL-02 Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics-Volume 1, pages 63\u201370. Association for Computational Linguistics, 2002. [7] Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, and David McClosky. The stanford corenlp natural language processing toolkit. In Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations, pages 55\u201360, 2014. [8] Wanxiang Che, Zhenghua Li, and Ting Liu. Ltp: A chinese language technology platform. In Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations, pages 13\u201316. Association for Computational Linguistics, 2010. [9] Martin Popel and Zden\u02c7ek \u017dabokrtsk`y. Tectomt: modular nlp framework.",
  "Association for Computational Linguistics, 2010. [9] Martin Popel and Zden\u02c7ek \u017dabokrtsk`y. Tectomt: modular nlp framework. In International Conference on Natural Language Processing, pages 293\u2013304. Springer, 2010. [10] Llu\u00eds Padr\u00f3, Miquel Collado, Samuel Reese, Marina Lloberes, and Irene Castell\u00f3n. Freeling 2.1: Five years of open-source language processing tools. In 7th International Conference on Language Resources and Evaluation, 2010. [11] Waqar Ali Narejo and Javed Ahmed Mahar. Morphology: Sindhi morphological analysis for natural language processing applications. In 2016 International Conference on Computing, Electronic and Electrical Engineering (ICE Cube), 2016. [12] Yang Li and Tao Yang. Word embedding for understanding natural language: a survey. In Guide to Big Data Applications, pages 83\u2013104. Springer, 2018. [13] Ronan Collobert and Jason Weston.",
  "[12] Yang Li and Tao Yang. Word embedding for understanding natural language: a survey. In Guide to Big Data Applications, pages 83\u2013104. Springer, 2018. [13] Ronan Collobert and Jason Weston. A uni\ufb01ed architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM, 2008. [14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, 2019. [15] Mutee U Rahman. Towards sindhi corpus construction. In Conference on Language and Technology, Lahore, Pakistan, 2010. [16] Fida Hussain Khoso, Mashooque Ahmed Memon, Haque Nawaz, and Sayed Hyder Abbas Musavi.",
  "Towards sindhi corpus construction. In Conference on Language and Technology, Lahore, Pakistan, 2010. [16] Fida Hussain Khoso, Mashooque Ahmed Memon, Haque Nawaz, and Sayed Hyder Abbas Musavi. To build corpus of sindhi. 2019. [17] Mazhar Ali Dootio and Asim Imdad Wagan. Unicode-8 based linguistics data set of annotated sindhi text. Data in brief, 19:1504\u20131514, 2018. [18] Mazhar Ali Dootio and Asim Imdad Wagan. Development of sindhi text corpus. Journal of King Saud University- Computer and Information Sciences, 2019. [19] Mazhar Ali and Asim Imdad Wagan. Sentiment summerization and analysis of sindhi text. Int. J. Adv. Comput. Sci. Appl, 8(10):296\u2013300, 2017. [20] Kevin Lund and Curt Burgess. Producing high-dimensional semantic spaces from lexical co-occurrence.",
  "Int. J. Adv. Comput. Sci. Appl, 8(10):296\u2013300, 2017. [20] Kevin Lund and Curt Burgess. Producing high-dimensional semantic spaces from lexical co-occurrence. Behavior research methods, instruments, & computers, 28(2):203\u2013208, 1996. [21] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111\u20133119, 2013. [22] Yoshua Bengio, R\u00e9jean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model. Journal of machine learning research, 3(Feb):1137\u20131155, 2003. [23] Jacob Andreas and Dan Klein. How much do word embeddings encode about syntax? In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 822\u2013827, 2014.",
  "[23] Jacob Andreas and Dan Klein. How much do word embeddings encode about syntax? In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 822\u2013827, 2014. [24] Tobias Schnabel, Igor Labutov, David Mimno, and Thorsten Joachims. Evaluation methods for unsupervised word embeddings. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 298\u2013307, 2015. 19",
  "A PREPRINT - JANUARY 20, 2022 [25] Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin. Advances in pre- training distributed word representations. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC-2018), 2018. [26] Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov. Learning word vectors for 157 languages. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC-2018), 2018. [27] Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543, 2014. [28] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef\ufb01cient estimation of word representations in vector space.",
  "[28] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef\ufb01cient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. [29] Neha Nayak, Gabor Angeli, and Christopher D Manning. Evaluating word embeddings using a representative suite of practical tasks. In Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP, pages 19\u201323, 2016. [30] B\u00e9n\u00e9dicte Pierrejean and Ludovic Tanguy. Towards qualitative word embeddings evaluation: measuring neighbors variation. 2018. [31] Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pa\u00b8sca, and Aitor Soroa. A study on similarity and relatedness using distributional and wordnet-based approaches. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 19\u201327. Association for Computational Linguistics, 2009.",
  "A study on similarity and relatedness using distributional and wordnet-based approaches. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 19\u201327. Association for Computational Linguistics, 2009. [32] Roland Sch\u00e4fer and Felix Bildhauer. Web corpus construction. Synthesis Lectures on Human Language Technolo- gies, 6(4):1\u2013145, 2013. [33] Zeeshan Bhatti, Imdad Ali Ismaili, Waseem Javaid Soomro, and Dil Nawaz Hakro. Word segmentation model for sindhi text. American Journal of Computing Research Repository, 2(1):1\u20137, 2014. [34] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5:135\u2013146, 2017.",
  "[34] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5:135\u2013146, 2017. [35] Siyu Qiu, Qing Cui, Jiang Bian, Bin Gao, and Tie-Yan Liu. Co-learning of word representations and morpheme representations. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 141\u2013150, 2014. [36] Omer Levy, Yoav Goldberg, and Ido Dagan. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics, 3:211\u2013225, 2015. [37] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579\u20132605, 2008. [38] R\u00e9mi Lebret and Ronan Collobert. Word emdeddings through hellinger pca.",
  "Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579\u20132605, 2008. [38] R\u00e9mi Lebret and Ronan Collobert. Word emdeddings through hellinger pca. arXiv preprint arXiv:1312.5542, 2013. [39] Amaresh Kumar Pandey and Tanvver J Siddiqui. Evaluating effect of stemming and stop-word removal on hindi text retrieval. In Proceedings of the First International Conference on Intelligent Human Computer Interaction, pages 316\u2013326. Springer, 2009. [40] Mikhail Khodak, Andrej Risteski, Christiane Fellbaum, and Sanjeev Arora. Automated wordnet construction using word embeddings. In Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications, pages 12\u201323, 2017. [41] Andriy Mnih and Koray Kavukcuoglu. Learning word embeddings ef\ufb01ciently with noise-contrastive estimation.",
  "[41] Andriy Mnih and Koray Kavukcuoglu. Learning word embeddings ef\ufb01ciently with noise-contrastive estimation. In Advances in neural information processing systems, pages 2265\u20132273, 2013. [42] Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Advances in neural information processing systems, pages 2177\u20132185, 2014. [43] Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. Placing search in context: The concept revisited. ACM Transactions on information systems, 20(1):116\u2013131, 2002. 20",
  "A PREPRINT - JANUARY 20, 2022 [44] Alvaro Corral, Gemma Boleda, and Ramon Ferrer-i Cancho. Zipf\u2019s law for word frequencies: Word forms versus lemmas in long texts. PloS one, 10(7):e0129031, 2015. 21"
]