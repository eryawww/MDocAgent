{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "This is the unrefereed Author\u2019s Original Version (or pre-print Version) of the article. The present version is not the Accepted Manuscript. Query-oriented text summarization based on hypergraph transversals Hadrien Van Lierde and Tommy W. S. Chow Department of Electronic Engineering, City University of Hong Kong 83 Tat Chee Av., Kowloon Tong, Hong Kong, China hadrien.vanlierde@hotmail.com, eetchow@cityu.edu.hk Abstract Existing graph- and hypergraph-based algorithms for document summarization represent the sentences of a corpus as the nodes of a graph or a hypergraph in which the edges represent relationships of lexical similarities between sentences. Each sentence of the corpus is then scored individually, using popular node ranking algorithms, and a summary is produced by extracting highly scored sentences. This approach fails to select a subset of jointly relevant sentences and it may produce redundant summaries that are missing important top- ics of the corpus. To alleviate this issue, a new hypergraph-based summarizer is proposed in this paper, in which each node is a sen- tence and each hyperedge is a theme, namely a group of sentences sharing a topic.",
      "To alleviate this issue, a new hypergraph-based summarizer is proposed in this paper, in which each node is a sen- tence and each hyperedge is a theme, namely a group of sentences sharing a topic. Themes are weighted in terms of their prominence in the corpus and their relevance to a user-de\ufb01ned query. It is further shown that the problem of identifying a subset of sentences covering the relevant themes of the corpus is equivalent to that of \ufb01nding a hypergraph transversal in our theme-based hypergraph. Two exten- sions of the notion of hypergraph transversal are proposed for the purpose of summarization, and polynomial time algorithms building on the theory of submodular functions are proposed for solving the associated discrete optimization problems. The worst-case time com- plexity of the proposed algorithms is squared in the number of terms, which makes it cheaper than the existing hypergraph-based methods.",
      "The worst-case time com- plexity of the proposed algorithms is squared in the number of terms, which makes it cheaper than the existing hypergraph-based methods. A thorough comparative analysis with related models on DUC bench- mark datasets demonstrates the e\ufb00ectiveness of our approach, which outperforms existing graph- or hypergraph-based methods by at least 1 arXiv:1902.00672v1  [cs.CL]  2 Feb 2019",
      "6% of ROUGE-SU4 score. keywords: Query-Oriented Text Summarization, Hypergraph The- ory, Hypergraph Transversal, Sentence Clustering, Submodular Set Functions 1 Introduction The development of automatic tools for the summarization of large corpora of documents has attracted a widespread interest in recent years. With \ufb01elds of application ranging from medical sciences to \ufb01nance and legal science, these summarization systems considerably reduce the time required for knowledge acquisition and decision making, by identifying and formatting the relevant information from a collection of documents. Since most applications involve large corpora rather than single documents, summarization systems developed recently are meant to produce summaries of multiple documents. Similarly, the interest has shifted from generic towards query-oriented summarization, in which a query expresses the user\u2019s needs. Moreover, existing summarizers are generally extractive, namely they produce summaries by extracting relevant sentences from the original corpus. Among the existing extractive approaches for text summarization, graph-based meth- ods are considered very e\ufb00ective due to their ability to capture the global patterns of connection between the sentences of the corpus.",
      "Among the existing extractive approaches for text summarization, graph-based meth- ods are considered very e\ufb00ective due to their ability to capture the global patterns of connection between the sentences of the corpus. These systems generally de\ufb01ne a graph in which the nodes are the sentences and the edges denote relationships of lexical similari- ties between the sentences. The sentences are then scored using graph ranking algorithms such as the PageRank [1] or HITS [2] algorithms, which can also be adapted for the pur- pose of query-oriented summarization [3]. A key step of graph-based summarizers is the way the graph is constructed, since it has a strong impact on the sentence scores. As pointed out in [4], a critical issue of traditional graph-based summarizers is their inability to capture group relationships among sentences since each edge of a graph only connects a pair of nodes. Following the idea that each topic of a corpus connects a group of multiple sen- tences covering that topic, hypergraph models were proposed in [4] and [5], in which the hyperedges represent similarity relationships among groups of sentences.",
      "Following the idea that each topic of a corpus connects a group of multiple sen- tences covering that topic, hypergraph models were proposed in [4] and [5], in which the hyperedges represent similarity relationships among groups of sentences. These group relationships are formed by detecting clusters of lexically similar sentences we refer to as themes or theme-based hyperedges. Each theme is believed to cover a speci\ufb01c topic of the corpus. However, since the models of [4] and [5] de\ufb01ne the themes as groups of lexically similar sentences, the underlying topics are not explicitly discovered. Moreover, their themes do not overlap which contradicts the fact that each sentence carries multiple information and may thus belong to multiple themes, as can be seen from the following example of sentence. \u201dOnce John \ufb01nished studying for his school test the next day, he caught up with his friend at the sport centre and they played soccer together.\u201d Two topics are covered by the sentence above: the topics of studies and leisure. Hence, the sentence should belong to multiple themes simultaneously, which is not allowed in existing hypergraph models of [4] and [5]. 2",
      "The hypergraph model proposed in this paper alleviates these issues by \ufb01rst extracting topics, i.e. groups of semantically related terms, using a new topic model referred to as SEMCOT. Then, a theme is associated to each topic, such that each theme is de\ufb01ned a the group of sentences covering the associated topic. Finally, a hypergraph is formed with sentences as nodes, themes as hyperedges and hyperedge weights re\ufb02ecting the prominence of each theme and its relevance to the query. In such a way, our model alleviates the weaknesses of existing hypergraph models since each theme-based hyperedge is associated to a speci\ufb01c topic and each sentence may belong to multiple themes. Furthermore, a common drawback of existing graph- and hypergraph-based summa- rizers is that they select sentences based on the computation of an individual relevance score for each sentence. This approach fails to capture the information jointly carried by the sentences which results in redundant summaries missing important topics of the corpus. To alleviate this issue, we propose a new approach of sentence selection using our theme-based hypergraph.",
      "This approach fails to capture the information jointly carried by the sentences which results in redundant summaries missing important topics of the corpus. To alleviate this issue, we propose a new approach of sentence selection using our theme-based hypergraph. A minimal hypergraph transversal is the smallest subset of nodes covering all hyperedges of a hypergraph [6]. The concept of hypergraph transver- sal is used in computational biology [7] and data mining [6] for identifying a subset of relevant agents in a hypergraph. In the context of our theme-based hypergraph, a hyper- graph transversal can be viewed as the smallest subset of sentences covering all themes of the corpus. We extend the notion of transversal to take the theme weights into account and we propose two extensions called minimal soft hypergraph transversal and maximal budgeted hypergraph transversal. The former corresponds to \ufb01nding a subset of sentences of minimal aggregated length and achieving a target coverage of the topics of the corpus (in a sense that will be clari\ufb01ed). The latter seeks a subset of sentences maximizing the total weight of covered hyperedges while not exceeding a target summary length.",
      "The latter seeks a subset of sentences maximizing the total weight of covered hyperedges while not exceeding a target summary length. As the associated discrete optimization problems are NP-hard, we propose two approximation al- gorithms building on the theory of submodular functions. Our transversal-based approach for sentence selection alleviates the drawback of methods of individual sentence scoring, since it selects a set of sentences that are jointly covering a maximal number of relevant themes and produces informative and non-redundant summaries. As demonstrated in the paper, the time complexity of the method is equivalent to that of early graph-based summarization systems such as LexRank [1], which makes it more e\ufb03cient than existing hypergraph-based summarizers [4,5]. The scalability of summarization algorithms is es- sential, especially in applications involving large corpora such as the summarization of news reports [8] or the summarization of legal texts [9]. The method of [10] proposes to select sentences by using a maximum coverage ap- proach, which shares some similarities with our model. However, they attempt to select a subset of sentences maximizing the number of relevant terms covered by the sentences.",
      "The method of [10] proposes to select sentences by using a maximum coverage ap- proach, which shares some similarities with our model. However, they attempt to select a subset of sentences maximizing the number of relevant terms covered by the sentences. Hence, they fail to capture the topical relationships among sentences, which are, in con- trast, included in our theme-based hypergraph. A thorough comparative analysis with state-of-the-art summarization systems is in- cluded in the paper. Our model is shown to outperform other models on a benchmark dataset produced by the Document Understanding Conference. The main contributions of this paper are (1) a new topic model extracting groups of semantically related terms based on patterns of term co-occurrences, (2) a natural hypergraph model representing nodes as sentences and each hyperedge as a theme, namely a group of sentences sharing a topic, and (3) a new sentence selection approach based on hypergraph transversals for the extraction of a subset of jointly relevant sentences. 3",
      "The structure of the paper is as follows. In section 2, we present work related to our method. In section 3, we present an overview of our system which is described in further details in section 4. Then, in section 5, we present experimental results. Finally, section 6 presents a discussion and concluding remarks. 2 Background and related work While early models focused on the task of single document summarization, recent systems generally produce summaries of corpora of documents [11]. Similarly, the focus has shifted from generic summarization to the more realistic task of query-oriented summarization, in which a summary is produced with the essential information contained in a corpus that is also relevant to a user-de\ufb01ned query [12]. Summarization systems are further divided into two classes, namely abstractive and extractive models. Extractive summarizers identify relevant sentences in the original corpus and produce summaries by aggregating these sentences [11]. In contrast, an ab- stractive summarizer identi\ufb01es conceptual information in the corpus and reformulates a summary from scratch [12].",
      "Extractive summarizers identify relevant sentences in the original corpus and produce summaries by aggregating these sentences [11]. In contrast, an ab- stractive summarizer identi\ufb01es conceptual information in the corpus and reformulates a summary from scratch [12]. Since abstractive approaches require advanced natural lan- guage processing, the majority of existing summarization systems consist of extractive models. Extractive summarizers di\ufb00er in the method used to identify relevant sentences, which leads to a classi\ufb01cation of models as either feature-based or graph-based approaches. Feature-based methods represent the sentences with a set of prede\ufb01ned features such as the sentence position, the sentence length or the presence of cue phrases [13]. Then, they train a model to compute relevance scores for the sentences based on their features. Since feature-based approaches generally require datasets with labelled sentences which are hard to produce [12], unsupervised graph-based methods have attracted growing interest in recent years. Graph-based summarizers represent the sentences of a corpus as the nodes of a graph with the edges modelling relationships of similarity between the sentences [1]. Then, graph-based algorithms are applied to identify relevant sentences.",
      "Graph-based summarizers represent the sentences of a corpus as the nodes of a graph with the edges modelling relationships of similarity between the sentences [1]. Then, graph-based algorithms are applied to identify relevant sentences. The models generally di\ufb00er in the type of relationship captured by the graph or in the sentence selection ap- proach. Most graph-based models de\ufb01ne the edges connecting sentences based on the co-occurrence of terms in pairs of sentences [1,3,4]. Then, important sentences are iden- ti\ufb01ed either based on node ranking algorithms, or using a global optimization approach. Methods based on node ranking compute individual relevance scores for the sentences and build summaries with highly scored sentences. The earliest such summarizer, LexRank [1], applies the PageRank algorithm to compute sentence scores. Introducing a query bias in the node ranking algorithm, this method can be adapted for query-oriented summa- rization as in [3]. A di\ufb00erent graph model was proposed in [14], where sentences and key phrases form the two classes of nodes of a bipartite graph. The sentences and the key phrases are then scored simultaneously by applying a mutual reinforcement algorithm.",
      "A di\ufb00erent graph model was proposed in [14], where sentences and key phrases form the two classes of nodes of a bipartite graph. The sentences and the key phrases are then scored simultaneously by applying a mutual reinforcement algorithm. An extended bipartite graph ranking algorithm is also proposed in [2] in which the sentences represent one class of nodes and clusters of similar sentences represent the other class. The hubs and authorities algorithm is then applied to compute sentence scores. Adding terms as a third class of nodes, [15] propose to score terms, sentences and sentence clusters simultaneously, based on a mutual reinforcement algorithm which propagates the scores 4",
      "across the three node classes. A common drawback of the approaches based on node ranking is that they compute individual relevance scores for the sentences and they fail to model the information jointly carried by the sentences, which may result in redundant summaries. Hence, global optimization approaches were proposed to select a set of jointly relevant and non-redundant sentences as in [16] and [17]. For instance, [18] propose a greedy algorithm to \ufb01nd a dominating set of nodes in the sentence graph. A summary is then formed with the corresponding set of sentences. Similarly, [16] extract a set of sentences with a maximal similarity with the entire corpus and a minimal pairwise lexical similarity, which is modelled as a multi-objective optimization problem. In contrast, [10] propose a coverage approach in which a set of sentences maximizing the number of distinct relevant terms is selected. Finally, [17] propose a two step approach in which individual sentence relevance scores are computed \ufb01rst. Then a set of sentences with a maximal total relevance and a minimal joint redundancy is selected. All three methods attempt to solve NP-hard problems. Hence, they propose approximation algorithms based on the theory of submodular functions.",
      "Then a set of sentences with a maximal total relevance and a minimal joint redundancy is selected. All three methods attempt to solve NP-hard problems. Hence, they propose approximation algorithms based on the theory of submodular functions. Going beyond pairwise lexical similarities between sentences and relations based on the co-occurrence of terms, hypergraph models were proposed, in which nodes are sen- tences and hyperedges model group relationships between sentences [4]. The hyperedges of the hypergraph capture topical relationships among groups of sentences. Existing hypergraph-based systems [4,5] combine pairwise lexical similarities and clusters of lex- ically similar sentences to form the hyperedges of the hypergraph. Hypergraph ranking algorithms are then applied to identify important and query-relevant sentences. However, they do not provide any interpretation for the clusters of sentences discovered by their method. Moreover, these clusters do not overlap, which is incoherent with the fact that each sentence carries multiple information and hence belongs to multiple semantic groups of sentences. In contrast, each hyperedge in our proposed hypergraph connects sentences covering the same topic, and these hyperedges do overlap.",
      "Moreover, these clusters do not overlap, which is incoherent with the fact that each sentence carries multiple information and hence belongs to multiple semantic groups of sentences. In contrast, each hyperedge in our proposed hypergraph connects sentences covering the same topic, and these hyperedges do overlap. A minimal hypergraph transversal is a subset of the nodes of hypergraph of minimum cardinality and such that each hyperedge of the hypergraph is incident to at least one node in the subset [6]. Theoretically equivalent to the minimum hitting set problem, the problem of \ufb01nding a minimum hypergraph transversal can be viewed as \ufb01nding a subset of representative nodes covering the essential information carried by each hyperedge. Hence, hypergraph transversals \ufb01nd applications in various areas such as computational biology, boolean algebra and data mining [19]. Extensions of hypergraph transversals to include hyperedge and node weights were also proposed in [20]. Since the associated optimization problems are generally NP-hard, various approximation algorithms were proposed, including greedy algorithms [21] and LP relaxations [22].",
      "Extensions of hypergraph transversals to include hyperedge and node weights were also proposed in [20]. Since the associated optimization problems are generally NP-hard, various approximation algorithms were proposed, including greedy algorithms [21] and LP relaxations [22]. The problem of \ufb01nding a hypergraph transversal is conceptually similar to that of \ufb01nding a summarizing subset of a set of objects modelled as a hypergraph. However, to the best of our knowledge, there was no attempt to use hypergraph transversals for text summarization in the past. Since it seeks a set of jointly relevant sentences, our method shares some similarities with existing graph-based models that apply global optimization strategies for sentence selection [10,16,17]. However, our hypergraph better captures topical relationships among sentences than the simple graphs based on lexical similarities between sentences. 5",
      "3 Problem statement and system overview Given a corpus of Nd documents and a user-de\ufb01ned query q, we intend to produce a summary of the documents with the information that is considered both central in the corpus and relevant to the query. Since we limit ourselves to the production of extracts, our task is to extract a set S of relevant sentences from the corpus and to aggregate them to build a summary. Let Ns be the total number of sentences in the corpus. We further split the task into two subtasks: \u2022 target summary length: the summary must cover the largest amount of relevant information while not exceeding a target length L, namely P i\u2208S Li \u2264L, where {Li, 1 \u2264i \u2264Ns} represent the lengths of the sentences, \u2022 target coverage: the summary must have a minimum length while achieving a target coverage of the information expressed by a parameter \u03b3 \u2208[0, 1] expressing the fraction of the information present in the corpus that must be covered by the summary (in a sense that will be clari\ufb01ed). The sentences in the set S are then aggregated to form the \ufb01nal summary. Figure 1 summarizes the steps of our proposed method.",
      "The sentences in the set S are then aggregated to form the \ufb01nal summary. Figure 1 summarizes the steps of our proposed method. After some preprocessing steps, the themes are detected based on a topic detection algorithm which tags each sentence with multiple topics. A theme-based hypergraph is then built with the weight of each theme re\ufb02ecting both its importance in the corpus and its similarity with the query. Finally, depending on the task at hand, one of two types of hypergraph transversal is generated. If the summary must not exceed a target summary length, then a maximal budgeted hypergraph transversal is generated. If the summary must achieve a target coverage, then a minimal soft hypergraph transversal is generated. Finally the sentences corresponding to the generated transversal are selected for the summary. Figure 1: Algorithm Chart. 6",
      "4 Summarization based on hypergraph transver- sals In this section, we present the key steps of our algorithm: after some standard pre- processing steps, topics of semantically related terms are detected from which themes grouping topically similar sentences are extracted. A hypergraph is then formed based on the sentence themes and sentences are selected based on the detection of a hypergraph transversal. 4.1 Preprocessing and similarity computation As the majority of extractive summarization approaches, our model is based on the rep- resentation of sentences as vectors. To reduce the size of the vocabulary, we remove stopwords that do not contribute to the meaning of sentences such as \u201dthe\u201d or \u201da\u201d, using a publicly available list of 667 stopwords 1. The words are also stemmed using Porter Stemmer [23]. Let Nt be the resulting number of distinct terms after these two prepro- cessing steps are performed.",
      "The words are also stemmed using Porter Stemmer [23]. Let Nt be the resulting number of distinct terms after these two prepro- cessing steps are performed. We de\ufb01ne the inverse sentence frequency isf(t) [24] as isf(t) = log \u0012Ns N ts \u0013 (1) where N t s is the number of sentences containing term t. This weighting scheme yields higher weights for rare terms which are assumed to contribute more to the semantics of sentences [24].",
      "Sentence i is then represented by a vector si = [t\ufb01sf(i, 1), ..., t\ufb01sf(i, Nt)] where t\ufb01sf(i, t) = tf(i, t)isf(t) (2) and tf(i, t) is the frequency of term t in sentence i. Finally, to denote the similarity between two text fragments a and b (which can be sentences, groups of sentences or the query), we use the cosine similarity between the t\ufb01sf representations of a and b, as suggested in [3]: sim(a, b) = P t t\ufb01sf(a, t)t\ufb01sf(b, t) pP t t\ufb01sf(a, t)2pP t t\ufb01sf(b, t)2 (3) where t\ufb01sf(a, t) is also de\ufb01ned as the frequency of term t in fragment a multiplied by isf(t). This similarity measure will be used in section 4.3 to compute the similarity with the query q. 4.2 Sentence theme detection based on topic tagging As mentioned in section 1, our hypergraph model is based on the detection of themes.",
      "This similarity measure will be used in section 4.3 to compute the similarity with the query q. 4.2 Sentence theme detection based on topic tagging As mentioned in section 1, our hypergraph model is based on the detection of themes. A theme is de\ufb01ned as a group of sentences covering the same topic. Hence, our theme detection algorithm is based on a 3-step approach: the extraction of topics, the process of tagging each sentence with multiple topics and the detection of themes based on topic tags. 1Stopword Lists by Ranks NL Webmaster Tools, https://www.ranks.nl/stopwords, accessed on 15 November 2017 7",
      "A topic is viewed as a set of semantically similar terms, namely terms that refer to the same subject or the same piece of information. In the context of a speci\ufb01c corpus of related documents, a topic can be de\ufb01ned as a set of terms that are likely to occur close to each other in a document [25]. In order to extract topics, we make use of a clustering approach based on the de\ufb01nition of a semantic dissimilarity between terms. For terms u and v, we \ufb01rst de\ufb01ne the joint isf weight isf(u, v) as isf(u, v) = log \u0012 Ns N uv s \u0013 (4) where N uv s is the number of sentences in which both terms u and v occur together.",
      "Then, the semantic dissimilarity dsem(u, v) between the two terms is de\ufb01ned as dsem(u, v) = isf(u, v) \u2212min(isf(u), isf(v)) max(isf(u), isf(v)) (5) which can be viewed as a special case of the so-called google distance which was already successfully applied to learn semantic similarities between terms on webpages [26]. Us- ing concepts from information theory, isf(u) represents the number of bits required to express the occurrence of term u in a sentence using an optimally e\ufb03cient code. Then, isf(u, v)\u2212isf(u) can be viewed as the number of bits of information in v relative to u. As- suming isf(v) \u2265isf(u), dsem(u, v) can be viewed as the improvement obtained when com- pressing v using a previously compressed code for u and compressing v from scratch [27]. More details can be found in [26].",
      "More details can be found in [26]. In practice, two terms u and v with a low value of dsem(u, v) are expected to consistently occur together in the same context, and they are thus considered to be semantically related in the context of the corpus. Based on the semantic dissimilarity measure between terms, we de\ufb01ne a topic as a group of terms with a high semantic density, namely a group of terms such that each term of the group is semantically related to a su\ufb03ciently high number of terms in the group. The DBSCAN algorithm is a method of density-based clustering that achieves this result by iteratively growing cohesive groups of agents, with the condition that each member of a group should contain a su\ufb03cient number of other members in an \u03f5-neighborhood around it [28]. Using the semantic dissimilarity as a distance measure, DBSCAN extracts groups of semantically related terms which are considered as topics. The advantages o\ufb00ered by DBSCAN over other clustering algorithms are threefold. First, DBSCAN is capable of detecting the number of clusters automatically.",
      "Using the semantic dissimilarity as a distance measure, DBSCAN extracts groups of semantically related terms which are considered as topics. The advantages o\ufb00ered by DBSCAN over other clustering algorithms are threefold. First, DBSCAN is capable of detecting the number of clusters automatically. Second, although the semantic dissimilarity is symmetric and nonnegative, it does not satisfy the triangle inequality. This prevents the use of various clustering algorithms such as the agglomerative clustering with complete linkage [29]. However, DBSCAN does not explicitly require the triangle inequality to be satis\ufb01ed. Finally, it is able to detect noisy samples in low density region, that do not belong to any other cluster. Given a set of pairwise dissimilarity measures, a density threshold \u03f5 and a mini- mum neighborhood size m, DBSCAN returns a number K of clusters and a set of labels {c(i) \u2208{\u22121, 1, ..., K} : 1 \u2264i \u2264Nt} such that c(i) = \u22121 if term i is considered a noisy term. While it is easy to determine a natural value for m, choosing a value for \u03f5 is not straightforward.",
      "While it is easy to determine a natural value for m, choosing a value for \u03f5 is not straightforward. Hence, we adapt DBSCAN algorithm to build our topic model referred to as Semantic Clustering Of Terms (SEMCOT) algorithm. It iteratively applies DBSCAN and decreases the parameter \u03f5 until the size of each cluster does not exceed a prede\ufb01ned 8",
      "value. Algorithm 4.1 summarizes the process. Apart from m, the algorithm also takes parameters \u03f50 (the initial value of \u03f5), M (the maximum number of points allowed in a cluster) and \u03b2 \u22641 (a factor close to 1 by which \u03f5 is multiplied until all clusters have sizes lower than M). Experiments on real-world data suggest empirical values of m = 3, \u03f50 = 0.9, M = 0.1Nt and \u03b2 = 0.95. Additionally, we observe that, among the terms considered as noisy by DBSCAN, some could be highly infrequent terms with a high isf value but yet having a strong impact on the meaning of sentences. Hence, we include them as topics consisting of single terms if their isf value exceeds a threshold \u00b5 whose value is determined by cross-validation, as explained in section 5.",
      "Hence, we include them as topics consisting of single terms if their isf value exceeds a threshold \u00b5 whose value is determined by cross-validation, as explained in section 5. Algorithm 4.1: SEMCOT INPUT: Semantic Dissimilarities {dsem(u, v) : 1 \u2264u, v \u2264Nt}, PARAMETERS: \u03f50, M, m, \u03b2 \u22641, \u00b5 OUTPUT: Number K of topics, topic tags {c(i) : 1 \u2264i \u2264Nt} \u03f5 \u2190\u03f50, minTerms \u2190m, proceed \u2190True while proceed: [c, K] \u2190DBSCAN(dsem, \u03f5, minTerms) if max 1\u2264k\u2264K(|{i : c(i) = k}|) < M: proceed \u2190False else: \u03f5 \u2190\u03b2\u03f5 for each t s.t. c(t) = \u22121 (noisy terms): if isf(t) \u2265\u00b5: c(t) \u2190K + 1, K \u2190K + 1 Once the topics are obtained based on algorithm 4.1, a theme is associated to each topic, namely a group of sentences covering the same topic.",
      "The sentences are \ufb01rst tagged with multiple topics based on a scoring function. The score of the l-th topic in the i-th sentence is given by \u03c3il = X t:c(t)=l t\ufb01sf(i, t) (6) and the sentence is tagged with topic l whenever \u03c3il \u2265\u03b4, in which \u03b4 is a parameter whose value is tuned as explained in section 5 (ensuring that each sentence is tagged with at least one topic). The scores are intentionally not normalized to avoid tagging short sentences with an excessive number of topics. The l-th theme is then de\ufb01ned as the set of sentences Tl = {i : \u03c3il \u2265\u03b4, 1 \u2264i \u2264Ns}. (7) While there exist other summarization models based on the detection of clusters or groups of similar sentence, the novelty of our theme model is twofold. First, each theme is easily interpretable as the set of sentences associated to a speci\ufb01c topic. As such, our themes can be considered as groups of semantically related sentences. Second, it is clear that the themes discovered by our approach do overlap since a single sentence may be tagged with multiple topics.",
      "As such, our themes can be considered as groups of semantically related sentences. Second, it is clear that the themes discovered by our approach do overlap since a single sentence may be tagged with multiple topics. To the best of our knowledge, none of the previous cluster-based summarizers involved overlapping groups of sentences. Our model is thus more realistic since it better captures the multiplicity of the information covered by each sentence. 9",
      "4.3 Sentence hypergraph construction A hypergraph is a generalization of a graph in which the hyperedges may contain any number of nodes, as expressed in de\ufb01nition 1 [4]. Our hypergraph model moreover includes both hyperedge and node weights. De\ufb01nition 1 (Hypergraph). A node- and hyperedge-weighted hypergraph is de\ufb01ned as a quadruplet H = (V, E, \u03c6, w) in which V is a set of nodes, E \u22862V is a set of hyperedges, \u03c6 \u2208R|V | + is a vector of positive node weights and w \u2208R|E| + is a vector of positive hyperedge weights. For convenience, we will refer to a hypergraph by its weight vectors \u03c6 and w, its hyperedges represented by a set E \u22862V and its incidence lists inc(i) = {e \u2208E : i \u2208e} for each i \u2208V . As mentioned in section 1, our system relies on the de\ufb01nition of a theme-based hy- pergraph which models groups of semantically related sentences as hyperedges.",
      "As mentioned in section 1, our system relies on the de\ufb01nition of a theme-based hy- pergraph which models groups of semantically related sentences as hyperedges. Hence, compared to traditional graph-based summarizers, the hypergraph is able to capture more complex group relationships between sentences instead of being restricted to pairwise re- lationships. In our sentence-based hypergraph, the sentences are the nodes and each theme de\ufb01nes a hyperedge connecting the associated sentences. The weight \u03c6i of node i is the length of the i-th sentence, namely: V = {1, ..., Ns} and \u03c6i = Li, 1 \u2264i \u2264Ns E = {e1, ..., eK} \u22862V el = Tl i.e. el \u2208inc(i) \u2194i \u2208Tl (8) Finally, the weights of the hyperedges are computed based on the centrality of the associated theme and its similarity with the query: wl = (1 \u2212\u03bb)sim(Tl, D) + \u03bbsim(Tl, q) (9) where \u03bb \u2208[0, 1] is a parameter and D represents the entire corpus.",
      "sim(Tl, D) denotes the similarity of the set of sentences in theme Tl with the entire corpus (using the t\ufb01sf- based similarity of equation 3) which measures the centrality of the theme in the corpus. sim(Tl, q) refers to the similarity of the theme with the user-de\ufb01ned query q. 4.4 Detection of hypergraph transversals for text sum- marization The sentences to be included in the query-oriented summary should contain the essential information in the corpus, they should be relevant to the query and, whenever required, they should either not exceed a target length or jointly achieve a target coverage (as mentioned in section 3). Existing systems of graph-based summarization generally solve the problem by ranking sentences in terms of their individual relevance [1, 3, 4]. Then, they extract a set of sentences with a maximal total relevance and pairwise similarities not exceeding a prede\ufb01ned threshold. However, we argue that the joint relevance of a group of sentences is not re\ufb02ected by the individual relevance of each sentence. And 10",
      "limiting the redundancy of selected sentences as done in [4] does not guarantee that the sentences jointly cover the relevant themes of the corpus. Considering each topic as a distinct piece of information in the corpus, an alternative approach is to select the smallest subset of sentences covering each of the topics. The latter condition can be reformulated as ensuring that each theme has at least one of its sentences appearing in the summary. Using our sentence hypergraph representation, this corresponds to the detection of a minimal hypergraph transversal as de\ufb01ned below [6]. De\ufb01nition 2. Given an unweighted hypergraph H = (V, E), a minimal hypergraph transversal is a subset S\u2217\u2286V of nodes satisfying S\u2217 = argmin S\u2286V |S| s.t. S i\u2208S inc(i) = E (10) where inc(i) = {e : i \u2208e} denotes the set of hyperedges incident to node i. Figure 2 shows an example of hypergraph and a minimal hypergraph transversal of it (star-shaped nodes).",
      "S i\u2208S inc(i) = E (10) where inc(i) = {e : i \u2208e} denotes the set of hyperedges incident to node i. Figure 2 shows an example of hypergraph and a minimal hypergraph transversal of it (star-shaped nodes). In this case, since the nodes and the hyperedges are unweighted, the minimal transversal is not unique. Figure 2: Example of hypergraph and minimal hypergraph transversal. The problem of \ufb01nding a minimal transversal in a hypergraph is NP-hard [30]. How- ever, greedy algorithms or LP relaxations provide good approximate solutions in prac- tice [22]. As intended, the de\ufb01nition of transversal includes the notion of joint coverage of the themes by the sentences. However, it neglects node and hyperedge weights and it is unable to identify query-relevant themes. Since both the sentence lengths and the rel- evance of themes should be taken into account in the summary generation, we introduce two extensions of transversal, namely the minimal soft hypergraph transversal and the 11",
      "maximal budgeted hypergraph transversal. A minimal soft transversal of a hypergraph is obtained by minimizing the total weights of selected nodes while ensuring that the total weight of covered hyperedges exceeds a given threshold. De\ufb01nition 3 (minimal soft hypergraph transversal). Given a node and hyperedge weighted hypergraph H = (V, E, \u03c6, w) and a parameter \u03b3 \u2208[0, 1], a minimal soft hypergraph transversal is a subset S\u2217\u2286V of nodes satisfying S\u2217 = argmin S\u2286V P i\u2208S \u03c6i s.t. P e\u2208inc(S) we \u2265\u03b3W (11) in which inc(S) = S i\u2208S inc(i) and W = P e we. The extraction of a minimal soft hypergraph transversal of the sentence hypergraph produces a summary of minimal length achieving a target coverage expressed by parameter \u03b3 \u2208[0, 1].",
      "The extraction of a minimal soft hypergraph transversal of the sentence hypergraph produces a summary of minimal length achieving a target coverage expressed by parameter \u03b3 \u2208[0, 1]. As mentioned in section 3, applications of text summarization may also involve a hard constraint on the total summary length L. For that purpose, we introduce the notion of maximal budgeted hypergraph transversal which maximizes the volume of covered hyperedges while not exceeding the target length. De\ufb01nition 4 (maximal budgeted hypergraph transversal). Given a node and hyperedge weighted hypergraph H = (V, E, \u03c6, w) and a parameter L > 0, a maximal budgeted hyper- graph transversal is a subset S\u2217\u2286V of nodes satisfying S\u2217 = argmax S\u2286V P e\u2208inc(S) we s.t. P i\u2208S \u03c6i \u2264L. (12) We refer to the function P e\u2208inc(S) we as the hyperedge coverage of set S. We observe that both weighted transversals de\ufb01ned above include the notion of joint coverage of the hyperedges by the selected nodes.",
      "(12) We refer to the function P e\u2208inc(S) we as the hyperedge coverage of set S. We observe that both weighted transversals de\ufb01ned above include the notion of joint coverage of the hyperedges by the selected nodes. As a result and from the de\ufb01nition of hyperedge weights (equation 9), the resulting summary covers themes that are both central in the corpus and relevant to the query. This approach also implies that the resulting summary does not contain redundant sentences covering the exact same themes. As a result selected sentences are expected to cover di\ufb00erent themes and to be semantically diverse. Both the problems of \ufb01nding a minimal soft transversal or \ufb01nding a maximal budgeted transversal are NP-hard as proved by theorem 1. Theorem 1 (NP-hardness). The problems of \ufb01nding a minimal soft hypergraph transver- sal or a maximal budgeted hypergraph transversal in a weighted hypergraph are NP-hard. Proof.",
      "Theorem 1 (NP-hardness). The problems of \ufb01nding a minimal soft hypergraph transver- sal or a maximal budgeted hypergraph transversal in a weighted hypergraph are NP-hard. Proof. Regarding the minimal soft hypergraph transversal problem, with parameter \u03b3 = 1 and unit node weights, the problem is equivalent to the classical set cover problem (de\ufb01nition 2) which is NP-complete [30]. The maximal budgeted hypergraph transversal problem can be shown to be equivalent to the maximum coverage problem with knapsack constraint which was shown to be NP-complete in [30]. 12",
      "Since both problems are NP-hard, we formulate polynomial time algorithms to \ufb01nd approximate solutions to them and we provide the associated approximation factors. The algorithms build on the submodularity and the non-decreasing properties of the hyperedge coverage function, which are de\ufb01ned below. De\ufb01nition 5 (Submodular and non-decreasing set functions). Given a \ufb01nite set A, a function f : 2A \u2192R is monotonically non-decreasing if \u2200S \u2282A and \u2200u \u2208A \\ S, f(S \u222a{u}) \u2265f(S) (13) and it is submodular if \u2200S, T with S \u2286T \u2282A, and \u2200u \u2208A \\ T, f(T \u222a{u}) \u2212f(T) \u2264f(S \u222a{u}) \u2212f(S). (14) Based on de\ufb01nition 5, we prove in theorem 2 that the hyperedge coverage function is submodular and monotonically non-decreasing, which provides the basis of our algo- rithms. Theorem 2.",
      "(14) Based on de\ufb01nition 5, we prove in theorem 2 that the hyperedge coverage function is submodular and monotonically non-decreasing, which provides the basis of our algo- rithms. Theorem 2. Given a hypergraph H = (V, E, \u03c6, w), the hyperedge coverage function f : 2V \u2192R de\ufb01ned by f(S) = X e\u2208inc(S) we (15) is submodular and monotonically non-decreasing. Proof.",
      "Given a hypergraph H = (V, E, \u03c6, w), the hyperedge coverage function f : 2V \u2192R de\ufb01ned by f(S) = X e\u2208inc(S) we (15) is submodular and monotonically non-decreasing. Proof. The hyperege coverage function f is clearly monotonically non-decreasing and it is submodular since \u2200S \u2286T \u2282V , and s \u2208V \\ T, (f(S \u222a{s}) \u2212f(S)) \u2212(f(T \u222a{s}) \u2212f(T)) = \" P e\u2208inc(S\u222a{s}) we \u2212 P e\u2208inc(S) we # \u2212 \" P e\u2208inc(T \u222a{s}) we \u2212 P e\u2208inc(T ) we # = \" P e\u2208inc({s})\\inc(S) we # \u2212 \" P e\u2208inc({s})\\inc(T ) we # = P e\u2208(inc(T )\u2229inc({s}))\\inc(S) we \u22650 (16) where inc(R) = {e : e\u2229S \u0338= \u2205} for R \u2286V .",
      "The last equality follows from inc(S) \u2286inc(T) and inc({s}) \\ inc(T) \u2286inc({s}) \\ inc(S). Various classes of NP-hard problems involving a submodular and non-decreasing func- tion can be solved approximately by polynomial time algorithms with provable approx- imation factors. Algorithms 4.2 and 4.3 are our core methods for the detection of ap- proximations of maximal budgeted hypergraph transversals and minimal soft hypergraph transversals, respectively. In each case, a transversal is found and the summary is formed by extracting and aggregating the associated sentences. Algorithm 4.2 is based on an adaptation of an algorithm presented in [31] for the maximization of submodular func- tions under a Knaspack constraint. It is our primary transversal-based summarization model, and we refer to it as the method of Transversal Summarization with Target Length (TL-TranSum algorithm). Algorithm 4.3 is an application of the algorithm presented 13",
      "in [21] for solving the submodular set covering problem. We refer to it as Transversal Summarization with Target Coverage (TC-TranSum algorithm). Both algorithms pro- duce transversals by iteratively appending the node inducing the largest increase in the total weight of the covered hyperedges relative to the node weight. While long sentences are expected to cover more themes and induce a larger increase in the total weight of covered hyperedges, the division by the node weights (i.e. the sentence lengths) balances this tendency and allows the inclusion of short sentences as well. In contrast, the methods of sentence selection based on a maximal relevance and a minimal redundancy such as, for instance, the maximal marginal relevance approach of [32], tend to favor the selec- tion of long sentences only.",
      "the sentence lengths) balances this tendency and allows the inclusion of short sentences as well. In contrast, the methods of sentence selection based on a maximal relevance and a minimal redundancy such as, for instance, the maximal marginal relevance approach of [32], tend to favor the selec- tion of long sentences only. The main di\ufb00erence between algorithms 4.2 and 4.3 is the stopping criterion: in algorithm 4.3, the approximate minimal soft transversal is obtained whenever the targeted hyperedge coverage is reached while algorithm 4.2 appends a given sentence to the approximate maximal budgeted transversal only if its addition does not make the summary length exceed the target length L. Algorithm 4.2: Transversal Summarization with Target Length (TL- TranSum) INPUT: Sentence Hypergraph H = (V, E, \u03c6, w), target length L. OUTPUT: Set S of sentences to be included in the summary.",
      "for each i \u2208{1, ..., Ns}: ri \u2190 1 \u03c6i P e\u2208inc(i) we R \u2190\u2205, Q \u2190V , f \u21900 while Q \u0338= \u2205: s\u2217\u2190argmax i\u2208Q ri, Q \u2190Q \\ {s\u2217} if \u03c6s\u2217+ f \u2264L: R \u2190R \u222a{s\u2217}, f \u2190f + l\u2217 for each i \u2208{1, ..., Ns}: ri \u2190ri \u2212 P e\u2208inc(s\u2217)\u2229inc(i) we \u03c6i Let G \u2190{{i} : i \u2208V, \u03c6i \u2264L} S \u2190argmax S\u2208{Q}\u222aG P e\u2208inc(S) we return S 14",
      "Algorithm 4.3: Transversal Summarization with Target Coverage (TC-TranSum) INPUT: Sentence Hypergraph H = (V, E, \u03c6, w), parameter \u03b3 \u2208[0, 1]. OUTPUT: Set S of sentences to be included in the summary. for each i \u2208{1, ..., Ns}: ri \u2190 1 \u03c6i P e\u2208inc(i) we S \u2190\u2205, Q \u2190V , \u02dcW \u21900, W \u2190P e we while Q \u0338= \u2205and \u02dcW < \u03b3W: s\u2217\u2190argmax i\u2208Q ri S \u2190S \u222a{s\u2217}, \u02dcW \u2190\u02dcW + \u03c6s\u2217rs\u2217 for each i \u2208{1, ..., Ns}: ri \u2190ri \u2212 P e\u2208inc(s\u2217)\u2229inc(i) we \u03c6i return S We next provide theoretical guarantees that support the formulation of algorithms 4.2 and 4.3 as approximation algorithms for our hypergraph transversals.",
      "Theorem 3 provides a constant approximation factor for the output of algorithm 4.2 for the detection of minimal soft hypergraph transversals. It builds on the submodularity and the non- decreasing property of the hyperedge coverage function. Theorem 3. Let SL be the summary produced by our TL-TranSum algorithm 4.2, and S\u2217be a maximal budgeted transversal associated to the sentence hypergraph, then X e\u2208inc(SL) we \u22651 2 \u0012 1 \u22121 e \u0013 X e\u2208inc(S\u2217) we. (17) Proof. Since the hyperedge coverage function is submodular and monotonically non- decreasing, the extraction of a maximal budgeted transversal is a problem of maximization of a submodular and monotonically non-decreasing function under a Knapsack constraint, namely max S\u2286V f(S) s.t. X i\u2208S \u03c6i \u2264L (18) where f(S) = P e\u2208inc(S) we.",
      "X i\u2208S \u03c6i \u2264L (18) where f(S) = P e\u2208inc(S) we. Hence, by theorem 2 in [31], the algorithm forming a transversal SF by iteratively growing a set St of sentences according to St+1 = St \u222a ( argmax s\u2208V \\St ( f(S \u222a{s}) \u2212f(S) \u03c6s , \u03c6s + X i\u2208St \u03c6i \u2264L )) (19) produces a \ufb01nal summary SF satisfying f(SF ) \u2265f(S\u2217)1 2 \u0012 1 \u22121 e \u0013 . (20) As algorithm 4.2 implements the iterations expressed by equation 19, it achieves a con- stant approximation factor of 1 2 \u00001 \u22121 e \u0001 . 15",
      "Similarly, theorem 4 provides a data-dependent approximation factor for the output of algorithm 4.3 for the detection of maximal budgeted hypergraph transversals. It also builds on the submodularity and the non-decreasing property of the hyperedge coverage function. Theorem 4. Let SP be the summary produced by our TC-TranSum algorithm 4.3 and let S\u2217be a minimal soft hypergraph transversal, then X i\u2208SP \u03c6i \u2264 X i\u2208S\u2217 \u03c6i \uf8eb \uf8ec \uf8ed1 + log \uf8eb \uf8ec \uf8ed \u03b3W \u03b3W \u2212 P e\u2208inc(ST \u22121) we \uf8f6 \uf8f7 \uf8f8 \uf8f6 \uf8f7 \uf8f8 (21) where S1, ..., ST represent the consecutive sets of sentences produced by algorithm 4.3. Proof. Consider the function g(S) = min(\u03b3W, P e\u2208inc(S) we).",
      "Proof. Consider the function g(S) = min(\u03b3W, P e\u2208inc(S) we). Then the problem of \ufb01nding a minimal soft hypergraph transversal can be reformulated as S\u2217= argmin S\u2286V X s\u2208S \u03c6s s.t. g(S) \u2265g(V ) (22) As g is submodular and monotonically non-decreasing, theorem 1 in [21] shows that the summary SG produced by iteratively growing a set St of sentences such that St+1 = St \u222a ( argmax s\u2208V \\St \u001af(S \u222a{s}) \u2212f(S) \u03c6s \u001b) (23) produces a summary SG satisfying X i\u2208SG \u03c6i \u2264 X i\u2208S\u2217 \u03c6i \u0012 1 + log \u0012 g(V ) g(V ) \u2212g(ST \u22121) \u0013\u0013 .",
      "(24) which can be rewritten as X i\u2208SG \u03c6i \u2264 X i\u2208S\u2217 \u03c6i \uf8eb \uf8ec \uf8ed1 + log \uf8eb \uf8ec \uf8ed \u03b3W \u03b3W \u2212 P e\u2208inc(ST \u22121) we \uf8f6 \uf8f7 \uf8f8 \uf8f6 \uf8f7 \uf8f8. (25) As algorithm 4.3 implements the iterations expressed by equation 23, the summary SS produced by our algorithm 4.3 satis\ufb01es the same inequality. In practice, the result of theorem 4 suggests that the quality of the output depends on the relative increase in the hyperedge coverage induced by the last sentence to be appended to the summary. In particular, if each sentence that is appended to the summary in the interations of algorithm 4.3 covers a su\ufb03cient number of new themes that are not covered already by the summary, the approximation factor is low. 16",
      "4.5 Complexity analysis We analyse the worst case time complexity of each step of our method. The time complex- ity of DBSCAN algorithm [28] is O(Nt log(Nt)). Hence, the theme detection algorithm 4.1 takes O(NcNt log(Nt)) steps, where Nc is the number of iterations of algorithm 4.1 which is generally low compared to the number of terms. The time complexity for the hypergraph construction is O(K(Ns + Nt)) where K is the number of topics, or O(N 2 t ) if Nt \u2265Ns. The time complexity of the sentence selection algorithms 4.2 and 4.3 are bounded by O(NsKCmaxLmax) where Cmax is the number of sentences in the largest theme and Lmax is the length of the longest sentences. Assuming Nt is larger than Ns, the overall time complexity of the method is of O(N 2 t ) steps in the worst case.",
      "Assuming Nt is larger than Ns, the overall time complexity of the method is of O(N 2 t ) steps in the worst case. Hence the method is essentially equivalent to early graph-based models for text summarization in terms of computational burden, such as the LexRank-based systems [1, 3] or greedy approaches based on global optimization [16\u201318]. However, it is computationnally more e\ufb03cient than traditional hypergraph-based summarizers such as the one in [5] which in- volves a Markov Chain Monte Carlo inference for its topic model or the one in [4] which is based on an iterative computation of scores involving costly matrix multiplications at each step. 5 Experiments and evaluation We present experimental results obtained with a Python implementation of algorithms 4.2 and 4.3 on a standard computer with a 2.5GHz processor and a 8GB memory.",
      "5 Experiments and evaluation We present experimental results obtained with a Python implementation of algorithms 4.2 and 4.3 on a standard computer with a 2.5GHz processor and a 8GB memory. 5.1 Dataset and metrics for evaluation We test our algorithms on DUC2005 [33], DUC2006 [34] and DUC2007 [35] datasets which were produced by the Document Understanding Conference (DUC) and are widely used as benchmark datasets for the evaluation of query-oriented summarizers. The datasets consist respectively of 50, 50 and 45 corpora, each consisting of 25 documents of approx- imately 1000 words, on average. A query is associated to each corpus. For evaluation purposes, each corpus is associated with a set of query-relevant summaries written by humans called reference summaries. In each of our experiments, a candidate summary is produced for each corpus by one of our algorithms and it is compared with the reference summaries using the metrics described below. Moreover, in experiments involving algo- rithm 4.2, the target summary length is set to 250 words as required in DUC evalutions.",
      "Moreover, in experiments involving algo- rithm 4.2, the target summary length is set to 250 words as required in DUC evalutions. In order to evaluate the similarity of a candidate summary with a set of reference sum- maries, we make use of the ROUGE toolkit of [36], and more speci\ufb01cally of ROUGE-2 and ROUGE-SU4 metrics, which were adopted by DUC for summary evaluation. ROUGE-2 measures the number of bigrams found both in the candidate summary and the set of reference summaries. ROUGE-SU4 extends this approach by counting the number of unigrams and the number of 4-skip-bigrams appearing in the candidate and the reference summaries, where a 4-skip-bigram is a pair of words that are separated by no more than 4 words in a text. We refer to ROUGE toolkit [36] for more details about the evaluation metrics. ROUGE-2 and ROUGE-SU4 metrics are computed following the same setting as 17",
      "in DUC evaluations, namely with word stemming and jackknife resampling but without stopword removal. 5.2 Parameter tuning Besides the parameters of SEMCOT algorithm for which empirical values were given in section 4.2, there are three parameters of our system that need to be tuned: parameters \u00b5 (threshold on isf value to include a noisy term as a single topic in SEMCOT), \u03b4 (threshold on the topic score for tagging a sentence with a given topic) and \u03bb (balance between the query relevance and the centrality in hyperedge weights). The values of all three param- eters are determined by an alternating maximization strategy of ROUGE-SU4 score in which the values of two parameters are \ufb01xed and the value of the third parameter is tuned to maximize the ROUGE-SU4 score produced by algorithm 4.2 with a target summary length of 250 words, in an iterative fashion. The ROUGE-SU4 scores are evaluated by cross-validation using a leave-one-out process on a validation dataset consisting of 70% of DUC2007 dataset, which yields \u00b5 = 1.98, \u03b4 = 0.85 and \u03bb = 0.4.",
      "The ROUGE-SU4 scores are evaluated by cross-validation using a leave-one-out process on a validation dataset consisting of 70% of DUC2007 dataset, which yields \u00b5 = 1.98, \u03b4 = 0.85 and \u03bb = 0.4. Additionally, we display the evolution of ROUGE-SU4 and ROUGE-2 scores as a function of \u03b4 and \u03bb. For parameter \u03b4, we observe in graphs 3(a) and 3(b) that the quality of the summary is low for \u03b4 close to 0 since it encourages our theme detection algorithm to tag the sentences with irrelevant topics with low associated t\ufb01sf values. In contrast, when \u03b4 exceeds 0.9, some relevant topics are overlooked and the quality of the summaries drops severely. Regarding parameter \u03bb, we observe in graphs 4(a) and 4(b) that \u03bb = 0.4 yields the highest score since it combines both the relevance of themes to the query and their centrality within the corpus for the computation of hyperedge weights. In contrast, with \u03bb = 1, the algorithm focuses on the lexical similarity of themes with the query but it neglects the prominence of each theme.",
      "In contrast, with \u03bb = 1, the algorithm focuses on the lexical similarity of themes with the query but it neglects the prominence of each theme. 0 0.2 0.4 0.6 0.8 1 1.2 0.095 0.1 0.105 0.11 0.115 0.12 0.125 0.13 0 0.2 0.4 0.6 0.8 1 1.2 0.14 0.15 0.16 0.17 0.18 Figure 3: ROUGE-2 and ROUGE-SU4 as a function of \u03b4 for \u03bb = 0.4 and \u00b5 = 1.98. 18",
      "0 0.2 0.4 0.6 0.8 1 0.105 0.11 0.115 0.12 0.125 0.13 0 0.2 0.4 0.6 0.8 1 0.155 0.16 0.165 0.17 0.175 0.18 Figure 4: ROUGE-2 and ROUGE-SU4 as a function of \u03bb for \u03b4 = 0.85 and \u00b5 = 1.98. 5.3 Testing the TC-TranSum algorithm In order to test our soft transversal-based summarizer, we display the evolution of the summary length and the ROUGE-SU4 score as a function of parameter \u03b3 of algorithm 4.3.",
      "5.3 Testing the TC-TranSum algorithm In order to test our soft transversal-based summarizer, we display the evolution of the summary length and the ROUGE-SU4 score as a function of parameter \u03b3 of algorithm 4.3. In \ufb01gure 5(b), we observe that the summary length grows linearly with the value of parameter \u03b3 which con\ufb01rms that our system does not favor longer sentences for low values of \u03b3. The ROUGE-SU4 curve of \ufb01gure 5(a) has a concave shape, with a low score when \u03b3 is close to 0 (due to a poor recall) or when \u03b3 is close to 1 (due to a poor precision). The overall concave shape of the ROUGE-SU4 curve also demonstrates the e\ufb03ciency of our TC-TranSum algorithm: based on our hyperedge weighting scheme and our hyperedge coverage function, highly relevant sentences inducing a signi\ufb01cant increase in the ROUGE-SU4 score are identi\ufb01ed and included \ufb01rst in the summary.",
      "In the subsequent experiments, we focus on TL-TranSum algorithm 4.2 which includes a target summary length and can thus be compared with other summarization systems which generally include a length constraint. 5.4 Testing the hypergraph structure To justify our theme-based hypergraph de\ufb01nition, we test other hypergraph models. We only change the hyperedge model which determines the kind of relationship between sentences that is captured by the hypergraph. The sentence selection is performed by applying algorithm 4.2 to the resulting hypergraph. We test three alternative hyperedge models. First a model based on agglomerative clustering instead of SEMCOT: the same de\ufb01nition of semantic dissimilarity (equation 5) is used, then topics are detected as clus- ters of terms obtained by agglomerative clustering with single linkage with the semantic dissimilarity as a distance measure. The themes are detected and the hypergraph is con- structed in the same way as in our model.",
      "The themes are detected and the hypergraph is con- structed in the same way as in our model. Second, Overlap model de\ufb01nes hyperedges as overlapping clusters of sentences obtained by applying an algorithm of overlapping cluster detection [37] and using the cosine distance between t\ufb01sf representations of sentences as a distance metric. Finally, we test a hypergraph model already proposed in HyperSum system by [4] which combines pairwise hyperedges joining any two sentences having terms 19",
      "0 0.2 0.4 0.6 0.8 1 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 0.2 0.4 0.6 0.8 1 0 100 200 300 400 500 Figure 5: Evolution of the ROUGE-SU4 score (left) and the summary length (right) as a function of the coverage parameter \u03b3 of TC-TranSum algorithm 4.3. in common and hyperedges formed by non-overlapping clusters of sentences obtained by DBSCAN algorithm. Table 1 displays the ROUGE-2 and ROUGE-SU4 scores and the corresponding 95% con\ufb01dence intervals for each model. We observe that our model out- performs both HyperSum and Overlap models by at least 4% and 15% of ROUGE-SU4 score, respectively, which con\ufb01rms that a two-step process extracting consistent topics \ufb01rst and then de\ufb01ning theme-based hyperedges from topic tags outperforms approaches based on sentence clustering, even when these clusters do overlap.",
      "Our model also out- performs the Agglomerative model by 10% of ROUGE-SU4 score, due to its ability to identify noisy terms and to detect the number of topics automatically. System ROUGE-2 ROUGE-SU4 TL-TranSum 0.12997(0.12548 \u22120.13446) 0.17995(0.17612 \u22120.18377) Agglomerative 0.12334(0.11673 \u22120.12994) 0.16292(0.15302 \u22120.17282) Overlap 0.11831(0.11334 \u22120.12328) 0.15640(0.14762 \u22120.16518) HyperSum 0.12317(0.11743 \u22120.12892) 0.17231(0.16561 \u22120.17900) Table 1: ROUGE-2 and ROUGE-SU4 scores for our TL-TranSum system compared to three other hypergraph models. 5.5 Comparison with related systems We compare the performance of our TL-TranSum algorithm 4.2 with that of \ufb01ve related summarization systems.",
      "5.5 Comparison with related systems We compare the performance of our TL-TranSum algorithm 4.2 with that of \ufb01ve related summarization systems. Topic-sensitive LexRank of [3] (TS-LexRank) and HITS algo- rithms of [2] are early graph-based summarizers. TS-LexRank builds a sentence graph based on term co-occurrences in sentences, and it applies a query-biased PageRank algo- rithm for sentence scoring. HITS method additionally extracts clusters of sentences and it applies the hubs and authorities algorithm for sentence scoring, with the sentences as authorities and the clusters as hubs. As suggested in [4], in order to extract query relevant 20",
      "sentences, only the top 5% of sentences that are most relevant to the query are considered. HyperSum extends early graph-based summarizers by de\ufb01ning a cluster-based hypergraph with the sentences as nodes and hyperedges as sentence clusters, as described in section 5.4. The sentences are then scored using an iterative label propagation algorithm over the hypergraph, starting with the lexical similarity of each sentence with the query as initial labels. In all three methods, the sentences with highest scores and pairwise lexical similarity not exceeding a threshold are included in the summary. Finally, we test two methods that also build on the theory of submodular functions. First, the MaxCover approach [10] seeks a summary by maximizing the number of distinct relevant terms ap- pearing in the summary while not exceeding the target summary length (using equation 9 to compute the term relevance scores). While the objective function of the method is similar to that of the problem of \ufb01nding a maximal budgeted hypergraph transversal (equation 12) of [17], they overlook the semantic similarities between terms which are captured by our SEMCOT algorithm and our hypergraph model.",
      "While the objective function of the method is similar to that of the problem of \ufb01nding a maximal budgeted hypergraph transversal (equation 12) of [17], they overlook the semantic similarities between terms which are captured by our SEMCOT algorithm and our hypergraph model. Similarly, the Maximal Relevance Minimal Redundancy (MRMR) \ufb01rst computes relevance scores of sentences as in equation 9, then it seeks a summary with a maximal total relevance score and a mini- mal redundancy while not exceeding the target summary length. The problem is solved by an iterative algorithm building on the submodularity and non-decreasing property of the objective function. Table 2 displays the ROUGE-2 and ROUGE-SU4 scores with the corresponding 95% con\ufb01dence intervals for all six systems, including our TL-TranSum method.",
      "Table 2 displays the ROUGE-2 and ROUGE-SU4 scores with the corresponding 95% con\ufb01dence intervals for all six systems, including our TL-TranSum method. We observe that our system outperforms other graph and hypergraph-based summarizers involving the computation of individual sentence scores: LexRank by 6%, HITS by 13% and Hyper- Sum by 6% of ROUGE-SU4 score; which con\ufb01rms both the relevance of our theme-based hypergraph model and the capacity of our transversal-based summarizer to identify jointly relevant sentences as opposed to methods based on the computation of individual sentence scores. Moreover, our TL-TranSum method also outperforms other approaches such as MaxCover (5%) and MRMR (7%). These methods are also based on a submodular and non-decreasing function expressing the information coverage of the summary, but they are limited to lexical similarities between sentences and fail to detect topics and themes to measure the information coverage of the summary.",
      "These methods are also based on a submodular and non-decreasing function expressing the information coverage of the summary, but they are limited to lexical similarities between sentences and fail to detect topics and themes to measure the information coverage of the summary. System ROUGE-2 ROUGE-SU4 TL-TranSum 0.12997(0.12548 \u22120.13446) 0.17995(0.17612 \u22120.18377) TS-LexRank 0.11037(0.10263 \u22120.11811) 0.16939(0.16233 \u22120.17645) HITS 0.10972(0.10155 \u22120.11789) 0.15927(0.15251 \u22120.16603) HyperSum 0.11994(0.11298 \u22120.12690) 0.16993(0.16189 \u22120.17797) MaxCover 0.11985(0.11028 \u22120.12943) 0.17072(0.16155 \u22120.17988) MRMR 0.11840(0.10999 \u22120.12681) 0.16857(0.16046 \u22120.17668) Table 2: Comparison with related graph- and hypergraph-based summariza- tion systems.",
      "21",
      "5.6 Comparison with DUC systems As a \ufb01nal experiment, we compare our TL-TranSum approach to other summarizers pre- sented at DUC contests. Table 3 displays the ROUGE-2 and ROUGE-SU4 scores for the worst summary produced by a human, for the top four systems submitted for the con- tests, for the baseline proposed by NIST (a summary consisting of the leading sentences of randomly selected documents) and the average score of all methods submitted, respec- tively for DUC2005, DUC2006 and DUC2007 contests. Regarding DUC2007, our method outperforms the best system by 2% and the average ROUGE-SU4 score by 21%. It also performs signi\ufb01cantly better than the baseline of NIST. However, it is outperformed by the human summarizer since our systems produces extracts, while humans naturally re- formulate the original sentences to compress their content and produce more informative summaries.",
      "It also performs signi\ufb01cantly better than the baseline of NIST. However, it is outperformed by the human summarizer since our systems produces extracts, while humans naturally re- formulate the original sentences to compress their content and produce more informative summaries. Tests on DUC2006 dataset lead to similar conclusions, with our TL-TranSum algorithm outperforming the best other system and the average ROUGE-SU4 score by 2% and 22%, respectively. On DUC2005 dataset however, our TL-TranSum method is outperformed by the beset system which is due to the use of advanced NLP techniques (such as sentence trimming [38]) which tend to increase the ROUGE-SU4 score. Nev- ertheless, the ROUGE-SU4 score produced by our TL-TranSum algorithm is still 15% higher than the average score for DUC2005 contest.",
      "Nev- ertheless, the ROUGE-SU4 score produced by our TL-TranSum algorithm is still 15% higher than the average score for DUC2005 contest. DUC2005 DUC2006 DUC2007 Method ROUGE-2 ROUGE-SU4 ROUGE-2 ROUGE-SU4 ROUGE-2 ROUGE-SU4 Hum 0.0897 0.151 0.13260 0.18385 0.17528 0.21892 TL-TranSum 0.077392 0.12869 0.10779 0.15854 0.12997 0.17995 1st 0.07251 0.13163 0.09558 0.15529 0.12448 0.17711 2nd 0.07174 0.12972 0.09097 0.14733 0.12028 0.17074 3rd 0.06984 0.12525 0.08987 0.14755 0.11887 0.16999 4th 0.06963 0.12795 0.08954 0.14607 0.11793 0.17593 Syst.",
      "Av. 0.05842 0.11205 0.07463 0.13021 0.09597 0.14884 Basel. 0.04026 0.08716 0.04947 0.09788 0.06039 0.10507 Table 3: Comparison with DUC2005, DUC2006 and DUC2007 systems 6 Conclusion In this paper, a new hypergraph-based summarization model was proposed, in which the nodes are the sentences of the corpus and the hyperedges are themes grouping sen- tences covering the same topics. Going beyond existing methods based on simple graphs and pairwise lexical similarities, our hypergraph model captures groups of semantically related sentences. Moreover, two new method of sentence selection based on the detec- tion of hypergraph transversals were proposed: one to generate summaries of minimal length and achieving a target coverage, and the other to generate a summary achieving a maximal coverage of relevant themes while not exceeding a target length. The approach generates informative summaries by extracting a subset of sentences jointly covering the relevant themes of the corpus. Experiments on a real-world dataset demonstrate the e\ufb00ectiveness of the approach.",
      "The approach generates informative summaries by extracting a subset of sentences jointly covering the relevant themes of the corpus. Experiments on a real-world dataset demonstrate the e\ufb00ectiveness of the approach. The hypergraph model itself is shown to produce more accurate summaries than other models based on term or sentence clustering. The overall 22",
      "system also outperforms related graph- or hypergraph-based approaches by at least 10% of ROUGE-SU4 score. As a future research direction, we may analyse the performance of other algorithms for the detection of hypergraph transversals, such as methods based on LP relaxations. We may also further extend our topic model to take the polysemy of terms into acount: since each term may carry multiple meanings, a given term could refer to di\ufb00erent topics depending on its context. Finally, we intend to adapt our model for solving related problems, such as commmunity question answering. References References [1] G. Erkan and D. R. Radev, \u201cLexrank: Graph-based lexical centrality as salience in text summarization,\u201d Journal of Arti\ufb01cial Intelligence Research, vol. 22, pp. 457\u2013479, 2004. [2] X. Wan and J. Yang, \u201cMulti-document summarization using cluster-based link anal- ysis,\u201d in Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pp. 299\u2013306, ACM, 2008.",
      "[2] X. Wan and J. Yang, \u201cMulti-document summarization using cluster-based link anal- ysis,\u201d in Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pp. 299\u2013306, ACM, 2008. [3] J. Otterbacher, G. Erkan, and D. R. Radev, \u201cUsing random walks for question- focused sentence retrieval,\u201d in Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pp. 915\u2013922, ACL, 2005. [4] W. Wang, S. Li, J. Li, W. Li, and F. Wei, \u201cExploring hypergraph-based semi-supervised ranking for query-oriented summarization,\u201d Information Sciences, vol. 237, pp. 271\u2013286, 2013. [5] S. Xiong and D. Ji, \u201cQuery-focused multi-document summarization using hypergraph-based ranking,\u201d Information Processing & Management, vol. 52, no. 4, pp. 670\u2013681, 2016.",
      "271\u2013286, 2013. [5] S. Xiong and D. Ji, \u201cQuery-focused multi-document summarization using hypergraph-based ranking,\u201d Information Processing & Management, vol. 52, no. 4, pp. 670\u2013681, 2016. [6] D. Gunopulos, H. Mannila, R. Khardon, and H. Toivonen, \u201cData mining, hyper- graph transversals, and machine learning,\u201d in Proceedings of the sixteenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems, pp. 209\u2013 216, ACM, 1997. [7] S. Klamt, U. U. Haus, and F. Theis, \u201cHypergraphs and cellular networks,\u201d PLoS computational biology, vol. 5, no. 5, p. e1000385, 2009.",
      "[7] S. Klamt, U. U. Haus, and F. Theis, \u201cHypergraphs and cellular networks,\u201d PLoS computational biology, vol. 5, no. 5, p. e1000385, 2009. [8] K. Hong, J. M. Conroy, B. Favre, A. Kulesza, H. Lin, and A. Nenkova, \u201cA repository of state of the art and competitive baseline summaries for generic news summariza- tion.,\u201d in LREC, pp. 1608\u20131616, 2014. [9] A. Kanapala, S. Pal, and R. Pamula, \u201cText summarization from legal documents: a survey,\u201d Arti\ufb01cial Intelligence Review, pp. 1\u201332, 2017. 23",
      "[10] H. Takamura and M. Okumura, \u201cText summarization model based on maximum coverage problem and its variant,\u201d in Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pp. 781\u2013789, Association for Computational Linguistics, 2009. [11] A. Nenkova, K. McKeown, et al., \u201cAutomatic summarization,\u201d Foundations and Trends R\u20ddin Information Retrieval, vol. 5, no. 2\u20133, pp. 103\u2013233, 2011. [12] A. Nenkova and K. McKeown, \u201cA survey of text summarization techniques,\u201d in Mining text data (C. C. Aggarwal and C. Zhai, eds.), ch. 3, pp. 43\u201376, Springer Science & Business Media, 2012. [13] M. A. Fattah, \u201cA hybrid machine learning model for multi-document summariza- tion,\u201d Applied intelligence, vol. 40, no. 4, pp. 592\u2013600, 2014.",
      "[13] M. A. Fattah, \u201cA hybrid machine learning model for multi-document summariza- tion,\u201d Applied intelligence, vol. 40, no. 4, pp. 592\u2013600, 2014. [14] H. Zha, \u201cGeneric summarization and keyphrase extraction using mutual reinforce- ment principle and sentence clustering,\u201d in Proceedings of the 25th annual interna- tional ACM SIGIR conference on Research and development in information retrieval, pp. 113\u2013120, ACM, 2002. [15] Z. Zhang, S. S. Ge, and H. He, \u201cMutual-reinforcement document summarization using embedded graph based sentence clustering for storytelling,\u201d Information Pro- cessing & Management, vol. 48, no. 4, pp. 767\u2013778, 2012. [16] H. Lin and J. Bilmes, \u201cMulti-document summarization via budgeted maximization of submodular functions,\u201d in Human Language Technologies: The 2010 Annual Con- ference of the North American Chapter of the Association for Computational Lin- guistics, pp. 912\u2013920, Association for Computational Linguistics, 2010.",
      "912\u2013920, Association for Computational Linguistics, 2010. [17] W. Yin and Y. Pei, \u201cOptimizing sentence modeling and selection for document sum- marization,\u201d in Proceedings of the Twenty-Fourth International Joint Conference on Arti\ufb01cial Intelligence, pp. 1383\u20131389, 2015. [18] C. Shen and T. Li, \u201cMulti-document summarization via the minimum dominating set,\u201d in Proceedings of the 23rd International Conference on Computational Linguis- tics, pp. 984\u2013992, Association for Computational Linguistics, 2010. [19] A. Gainer-Dewar and P. Vera-Licona, \u201cThe minimal hitting set generation problem: algorithms and computation,\u201d SIAM Journal on Discrete Mathematics, vol. 31, no. 1, pp. 63\u2013100, 2017. [20] E. Boros, V. Gurvich, L. Khachiyan, and K. Makino, \u201cDual-bounded generating problems: weighted transversals of a hypergraph,\u201d Discrete Applied Mathematics, vol. 142, no. 1, pp.",
      "[20] E. Boros, V. Gurvich, L. Khachiyan, and K. Makino, \u201cDual-bounded generating problems: weighted transversals of a hypergraph,\u201d Discrete Applied Mathematics, vol. 142, no. 1, pp. 1\u201315, 2004. [21] L. A. Wolsey, \u201cAn analysis of the greedy algorithm for the submodular set covering problem,\u201d Combinatorica, vol. 2, no. 4, pp. 385\u2013393, 1982. [22] A. Gainer-Dewar and P. Vera-Licona, \u201cThe minimal hitting set generation problem: algorithms and computation,\u201d SIAM Journal on Discrete Mathematics, vol. 31, no. 1, pp. 63\u2013100, 2017. [23] M. F. Porter, \u201cSnowball: A language for stemming algorithms.\u201d http://www. snowball.tartarus.org/texts/introduction.html, 2001. Accessed 15 Novem- ber 2017. 24",
      "[24] G. Salton and C. Buckley, \u201cTerm-weighting approaches in automatic text retrieval,\u201d Information processing & management, vol. 24, no. 5, pp. 513\u2013523, 1988. [25] S. Arora, R. Ge, and A. Moitra, \u201cLearning topic models\u2013going beyond svd,\u201d in Foundations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on, pp. 1\u201310, IEEE, 2012. [26] R. L. Cilibrasi and P. M. Vitanyi, \u201cThe google similarity distance,\u201d IEEE Transac- tions on knowledge and data engineering, vol. 19, no. 3, 2007. [27] R. Cilibrasi and P. M. Vit\u00b4anyi, \u201cClustering by compression,\u201d IEEE Transactions on Information theory, vol. 51, no. 4, pp. 1523\u20131545, 2005. [28] M. Ester, H.-P.",
      "51, no. 4, pp. 1523\u20131545, 2005. [28] M. Ester, H.-P. Kriegel, J. Sander, X. Xu, et al., \u201cA density-based algorithm for discovering clusters in large spatial databases with noise,\u201d in KDD\u201996 Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, vol. 96, pp. 226\u2013231, 1996. [29] L. Rokach and O. Maimon, \u201cClustering methods,\u201d in Data mining and knowledge discovery handbook, pp. 321\u2013352, Springer, 2005. [30] R. M. Karp, \u201cReducibility among combinatorial problems,\u201d in Complexity of com- puter computations (R. Miller, ed.), pp. 85\u2013103, Springer, 1972.",
      "321\u2013352, Springer, 2005. [30] R. M. Karp, \u201cReducibility among combinatorial problems,\u201d in Complexity of com- puter computations (R. Miller, ed.), pp. 85\u2013103, Springer, 1972. [31] J. Leskovec, A. Krause, C. Guestrin, C. Faloutsos, J. VanBriesen, and N. Glance, \u201cCost-e\ufb00ective outbreak detection in networks,\u201d in Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 420\u2013 429, ACM, 2007. [32] J. Carbonell and J. Goldstein, \u201cThe use of mmr, diversity-based reranking for re- ordering documents and producing summaries,\u201d in Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pp. 335\u2013336, ACM, 1998. [33] H. T. Dang, \u201cOverview of duc 2005,\u201d in Proceedings of the document understanding conference, 2005.",
      "335\u2013336, ACM, 1998. [33] H. T. Dang, \u201cOverview of duc 2005,\u201d in Proceedings of the document understanding conference, 2005. [34] T. D. Hoa, \u201cOverview of duc 2006,\u201d in Proceedings of the document understanding conference, 2006. [35] H. T. Dang, \u201cOverview of the duc 2007 summarization task,\u201d in Proceedings of the document understanding conference, 2007. [36] C.-Y. Lin and E. Hovy, \u201cAutomatic evaluation of summaries using n-gram co- occurrence statistics,\u201d in Proceedings of the 2003 Conference of the North Ameri- can Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pp. 71\u201378, Association for Computational Linguistics, 2003. [37] A. Lancichinetti, S. Fortunato, and J. Kert\u00b4esz, \u201cDetecting the overlapping and hier- archical community structure in complex networks,\u201d New Journal of Physics, vol. 11, no. 3, p. 033015, 2009.",
      "11, no. 3, p. 033015, 2009. [38] D. Zajic, B. Dorr, R. Schwartz, C. Monz, and J. Lin, \u201cA sentence-trimming approach to multi-document summarization,\u201d in Proceedings of HLT/EMNLP 2005 Workshop on Text Summarization (HLT/EMNLP 05), pp. 151\u2013158, 2005. 25"
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1902.00672.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":15618,
  "avg_doclen":169.7608695652,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1902.00672.pdf"
    }
  }
}