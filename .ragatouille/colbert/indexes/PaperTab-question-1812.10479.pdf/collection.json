[
  "Multimodal deep learning for short-term stock volatility prediction Marcelo Sardelicha,\u2217, Suresh Manandhara aDepartment of Computer Science Deramore Lane, University of York, Heslington, York, YO10 5GH, UK Abstract Stock market volatility forecasting is a task relevant to assessing market risk. We investigate the interaction between news and prices for the one-day-ahead volatility prediction using state-of-the-art deep learning approaches. The pro- posed models are trained either end-to-end or using sentence encoders transfered from other tasks. We evaluate a broad range of stock market sectors, namely Consumer Staples, Energy, Utilities, Heathcare, and Financials. Our exper- imental results show that adding news improves the volatility forecasting as compared to the mainstream models that rely only on price data. In particular, our model outperforms the widely-recognized GARCH(1,1) model for all sectors in terms of coe\ufb03cient of determination R2, MSE and MAE, achieving the best performance when training from both news and price data.",
  "In particular, our model outperforms the widely-recognized GARCH(1,1) model for all sectors in terms of coe\ufb03cient of determination R2, MSE and MAE, achieving the best performance when training from both news and price data. Keywords: deep learning, sequence learning, transfer learning, \ufb01nancial forecasting, volatility prediction, textual analysis, natural language preprocessing PACS: 05.10.-a, 05.40.-a 2010 MSC: 62-07, 62H99 1. Introduction Natural Language Processing (NLP) has increasingly attracted the atten- tion of the \ufb01nancial community. This trend can be explained by at least three major factors. The \ufb01rst factor refers to the business perspective. It is the eco- nomics of gaining competitive advantage using alternative sources of data and going beyond historical stock prices, thus, trading by analyzing market news automatically. The second factor is the major advancements in the technologies to collect, store, and query massive amounts of user-generated data almost in real-time. The third factor refers to the progress made by the NLP community in understanding unstructured text.",
  "The second factor is the major advancements in the technologies to collect, store, and query massive amounts of user-generated data almost in real-time. The third factor refers to the progress made by the NLP community in understanding unstructured text. \u2217Corresponding author Email addresses: marcelo.sardelich@york.ac.uk (Marcelo Sardelich), suresh@cs.york.ac.uk (Suresh Manandhar) Preprint submitted to Neurocomputing February 8, 2022 arXiv:1812.10479v1  [q-fin.ST]  25 Dec 2018",
  "Over the last decades the number of studies using NLP for \ufb01nancial forecast- ing has experienced exponential growth. According to [1], until 2008, less than \ufb01ve research articles were published per year mentioning both \u201cstock market\u201d and \u201ctext mining\u201d or \u201csentiment analysis\u201d keywords. In 2012, this number in- creased to slightly more than ten articles per year. The last numbers available for 2016 indicates this has increased to sixty articles per year. The ability to mechanically harvest the sentiment from texts using NLP has shed light on con\ufb02icting theories of \ufb01nancial economics. Historically, there has been two di\ufb00ering views on whether disagreement among market participants induces more trades. The \u201cnon-trade theorem\u201d [2] states that assuming all market participants have common knowledge about a market event, the level of disagreement among the participants does not increase the number of trades but only leads to a revision of the market quotes. In contrast, the theoret- ically framework proposed in [3] advocates that disagreement among market participants increases trading volume.",
  "In contrast, the theoret- ically framework proposed in [3] advocates that disagreement among market participants increases trading volume. Using textual data from Yahoo and Rag- ingBull.com message boards to measure the dispersion of opinions (positive or negative) among traders, it was shown in [4] that disagreement among users\u2019 messages helps to predict subsequent trading volume and volatility. Similar relation between disagreement and increased trading volume was found in [5] using Twitter posts. Additionally, textual analysis is adding to the theories of medium-term/long- term momentum/reversal in stock markets [6]. The uni\ufb01ed Hong and Stein model1 [7] on stock\u2019s momentum/reversal proposes that investors underreact to news, causing slow price drifts, and overreact to price shocks not accompa- nied by news, hence inducing reversals. This theoretical predicated behaviour between price and news was systematically estimated and supported in [8, 9] us- ing \ufb01nancial media headlines and in [10] using the Consumer Con\ufb01dence Index R \u20dd published by The Conference Board [11].",
  "This theoretical predicated behaviour between price and news was systematically estimated and supported in [8, 9] us- ing \ufb01nancial media headlines and in [10] using the Consumer Con\ufb01dence Index R \u20dd published by The Conference Board [11]. Similarly, [12] uses the Harvard IV-4 sentiment lexicon to count the occurrence of words with positive and negative connotation of the Wall Street Journal showing that negative sentiment is a good predictor of price returns and trading volumes. Accurate models for forecasting both price returns and volatility are equally important in the \ufb01nancial domain. Volatility measures how wildly the asset is expected to oscillate in a given time period and is related to the second moment of the price return distribution. In general terms, forecasting price returns is relevant to take speculative positions. The volatility, on the other hand, measures the risk of these positions. On a daily basis, \ufb01nancial institutions 1The gradual information di\ufb00usion model of Hong and Stein considers two types of eco- nomic agents, namely \u201cNewswatchers\u201d and \u201cMomentum traders\u201d.",
  "On a daily basis, \ufb01nancial institutions 1The gradual information di\ufb00usion model of Hong and Stein considers two types of eco- nomic agents, namely \u201cNewswatchers\u201d and \u201cMomentum traders\u201d. The model consider three assumptions: 1) \u201cNewswatchers\u201d realize part of the public information and privately adjust their models, which are only based on macroeconomic and company speci\ufb01c forecasts. 2) \u201cMomentum traders\u201d only trade on past price performance. 3) Private, rather than public, information di\ufb00uses gradually, since each agent has a di\ufb00erent time frame to adjust their mod- els. These assumptions about market agents are enough to model the relationship between news and long-term trends or short-term reversals. 2",
  "need to assess the short-term risk2 of their portfolios. Measuring the risk is essential in many aspects. It is imperative for regulatory capital disclosures required by banking supervision bodies. Moreover, it is useful to dynamically adjust position sizing accordingly to market conditions, thus, maintaining the risk within reasonable levels. Although, it is crucial to predict the short-term volatility from the \ufb01nancial markets application perspective, much of the current NLP research on volatility forecasting focus on the volatility prediction for very long-term horizons (see [13, 14, 15, 16, 17]). Predominately, these works are built on extensions of the bag-of- words representation that has the main drawback of not capturing word order. Financial forecasting, however, requires the ability to capture semantics that is dependent upon word order. For example, the headline \u201cQualcomm sues Apple for contract breach\u201d and \u201cApple sues Qualcomm for contract breach\u201d trigger di\ufb00erent responses for each stock and for the market aggregated index, however, they share the same bag-of-words representation. Additionally, these works use features from a pretrained sentiment analyis model to train the \ufb01nancial forecasting model.",
  "Additionally, these works use features from a pretrained sentiment analyis model to train the \ufb01nancial forecasting model. A key limitation of this process is that it requires a labelled sentiment dataset. Additionally, the error propagation is not end-to-end. In this work, we \ufb01ll in the gaps of volatility prediction research in the fol- lowing manner: 1. To move away from long-horizon volatility3 to short-term daily volatility prediction, we introduce a corpus of Reuters \ufb01nancial news. We compiled this corpus at individual stock level comprising the news titles (headlines) of 50 stocks in 5 diversi\ufb01ed sectors with a total of 146,783 samples (2007\u20132017). We also collected daily stock prices from Yahoo Finance website for the 50 stocks. 2. We propose an end-to-end multimodal model that jointly learns from daily stock price and company news. 3. We investigate if the textual mode is complementary or redundant for the short-term volatility prediction problem. Our results indicate that textual mode is complementary and improves the forecasting accuracy. 4.",
  "2. We propose an end-to-end multimodal model that jointly learns from daily stock price and company news. 3. We investigate if the textual mode is complementary or redundant for the short-term volatility prediction problem. Our results indicate that textual mode is complementary and improves the forecasting accuracy. 4. We contribute to the Universal Sentence Representation works in [18, 19, 20] by comparing how transferable are the representations learnt in two di\ufb00erent NLP tasks to the speci\ufb01c problem of volatility forecasting. 5. We propose a hierarchical news relevance attention mechanism that can ef- fectively select the most relevant headline news from the large amount of news released in a given day. 2Usually, this risk is the conditional volatility for the next trading day 3The long-term forecast characteristic of the works described above is explained by the fact that the 10-K reports are only released annually. 3",
  "2. Related work Previous work in [13] incorporates sections of the \u201cForm 10-K\u201d4 to predict the volatility twelve months after the report is released. They train a Support Vector Regression model on top of sparse representation (bag-of-words) with standard term weighting (e.g. Term-Frequency). This work was extended in [14, 15, 16, 17] by employing the Loughran-McDonald Sentiment Word Lists [21], which contain three lists where words are grouped by their sentiments (positive, negative and neutral). In all these works, the textual representation is engineered using the following steps: 1) For each sentiment group, the list is expanded by retrieving 20 most similar words for each word using Word2Vec word embeddings [22]. 2) Finally, each 10-K document is represented using the expanded lists of words. The weight of each word in this sparse representation is de\ufb01ned using Information Retrieval (IR) methods such as term-frequency (tf) and term-frequency with inverted document frequency (t\ufb01df).",
  "2) Finally, each 10-K document is represented using the expanded lists of words. The weight of each word in this sparse representation is de\ufb01ned using Information Retrieval (IR) methods such as term-frequency (tf) and term-frequency with inverted document frequency (t\ufb01df). Particularly, [17] shows that results can be improved using enhanced IR methods and projecting each sparse feature into a dense space using Principal Component Analysis (PCA). The works described above ([14, 15, 16, 17]) target long-horizon volatility predictions (one year or quarterly [17]). In particular, [17] and [16] uses market data (price) features along with the textual representation of the 10-K reports. These existing works that employ multi-modal learning [23] are based on a late fusion5 approach. For example, stacking ensembles to take into account the price and text predictions [17]. In contrast, our end-to-end trained model can learn the joint distribution of both price and text. Predicting the price direction rather than the volatility was the focus in [24].",
  "For example, stacking ensembles to take into account the price and text predictions [17]. In contrast, our end-to-end trained model can learn the joint distribution of both price and text. Predicting the price direction rather than the volatility was the focus in [24]. They extracted sentiment words from Twitter posts to build a time series of col- lective Pro\ufb01le of Mood States (POMS). Their results show that collective mood accurately predicts the direction of Down Jones stock index (86.7% accuracy). In [25] handcrafted text representations including term count, noun-phrase tags and extracted named entities are employed for predicting stock market direc- tion using Support Vector Machine (SVM). An extension of Latent Dirichlet Allocation (LDA) is proposed in [26] to learn a joint latent space of topics and sentiments. Our deep learning models bear a close resemblance to works focused on directional price forecasting [27, 28]. In [27], headline news are processed using Stanford OpenIE to generate triples that are fed into a Neural Tensor Network to create the \ufb01nal headline representation. In [28], a character-level embedding is pre-trained in an unsupervised manner.",
  "In [27], headline news are processed using Stanford OpenIE to generate triples that are fed into a Neural Tensor Network to create the \ufb01nal headline representation. In [28], a character-level embedding is pre-trained in an unsupervised manner. The character embedding is used as 4Companies with listed stocks are enforced by the U.S. Securities and Exchange Commis- sion (SEC) to \ufb01le \u201cForm 10-K\u201d reports on an annual/quarterly basis. These forms provide an overview of the company\u2019s business and \ufb01nancial health. A 10-K form example can be found here 5In the late fusion setup, text and price features are trained independently and a meta model is used in a later stage to discriminate how to weight the contribution of each mode. 4",
  "input to a sequence model to learn the headline representation. Particularly, both works average all headline representations in a given day, rather than attempting to weight the most relevant ones. In this work, we propose a neural attention mechanism to capture the News Relevance and provide experimental evidence that it is a key component of the end-to-end learning process. Our attention extends the previous deep learning methods from [27, 28]. Despite the fact that end-to-end deep learning models have attained state- of-the-art performance, the large number of parameters make them prone to over\ufb01tting. Additionally, end-to-end models are trained from scratch requiring large datasets and computational resources. Transfer learning (TL) alleviates this problem by adapting representations learnt from a di\ufb00erent and poten- tially weakly related source domain to the new target domain. For example, in computer vision tasks the convolutional features learnt from ImageNet [29] dataset (source domain) have been successfully transferred to multiple domain target tasks with much smaller datasets such as object classi\ufb01cation and scene recognition [30]. In this work, we consider TL in our experiments for two main reasons.",
  "In this work, we consider TL in our experiments for two main reasons. First, it address the question whether our proposed dataset is suitable for end-to-end training since the performance of the transferred representations can be com- pared with end-to-end learning. Second, it is still to be investigated which dataset transfers better to the forecasting problem. Recently, the NLP com- munity has focused on universal representations of sentences [18, 20], which are dense representations that carry the meaning of a full sentence. [18] found that transferring the sentence representation trained on the Stanford Natural Language Inference (SNLI) [31] dataset achieves state-of-the-art sentence rep- resentations to multiple NLP tasks (e.g. sentiment analysis, question-type and opinion polarity). Following [18], in this work, we investigate the suitability of SNLI and Reuters RCV1 [32] datasets to transfer learning to the volatility forecasting task. To the best of our knowledge, the hierarchical attention mechanism at head- line level, proposed in our work, has not being applied to volatility prediction so far; neither has been investigated the ability to transfer sentence encoders from source datasets to the target forecasting problem (Transfer Learning). 3.",
  "To the best of our knowledge, the hierarchical attention mechanism at head- line level, proposed in our work, has not being applied to volatility prediction so far; neither has been investigated the ability to transfer sentence encoders from source datasets to the target forecasting problem (Transfer Learning). 3. Our dataset Our corpus covers a broad range of news including news around earnings dates and complements the 10-K reports content. As an illustration, the head- lines \u201cWalmart warns that strong U.S. dollar will cost $15B in sales\u201d and \u201cProc- ter & Gamble Co raises FY organic sales growth forecast after sales beat\u201d de- scribe the company \ufb01nancial conditions and performance from the management point of view \u2013 these are also typical content present in Section 76 of the 10-K 6The section is called \u201cManagement\u2019s Discussion and Analysis of \ufb01nancial conditions and results of operations\u201d (MD&A), which is the management\u2019s forward-looking section. 5",
  "reports. In this section, we describe the steps involved in compiling our dataset of \ufb01nancial news at stock level, which comprises a broad range of business sectors. 3.1. Sectors and stocks The \ufb01rst step in compiling our corpus was to choose the constituents stocks. Our goal was to consider stocks in a broad range of sectors, aiming a diversi\ufb01ed \ufb01nancial domain corpus. We found that Exchange Traded Funds (ETF) provide a mechanical way to aggregate the most relevant stocks in a given industry/- sector. An ETF is a fund that owns assets, e.g. stock shares or currencies, but, unlike mutual funds are traded in stock exchanges. These ETFs are extremely liquid and track di\ufb00erent investment themes. We decided to use SPDR Setcor Funds constituents stocks in our work since the company is the largest provider of sector funds in the United States. We included in our analysis the top 5 (\ufb01ve) sector ETFs by \ufb01nancial trading volume (as in Jan/2018). Among the most traded sectors we also \ufb01ltered out the sectors that were similar to each other.",
  "We included in our analysis the top 5 (\ufb01ve) sector ETFs by \ufb01nancial trading volume (as in Jan/2018). Among the most traded sectors we also \ufb01ltered out the sectors that were similar to each other. For example, the Consumer Staples and Consumer Discretionary sectors are both part of the parent Consumer category. For each of the top 5 sectors we selected the top 10 holdings, which are deemed the most relevant stocks. Table 1, details our dataset sectors and its respective stocks. 3.2. Stock speci\ufb01c data We assume that an individual stock news as the one that explicitly mention the stock name or any of its surface forms in the headline. As an illustration, in order to collect all news for the stock code PG, Procter & Gamble company name, we search all the headlines with any of these words: Procter&Gamble OR Procter and Gamble OR P&G. In this example, the \ufb01rst word is just the company name and the remaining words are the company surface forms.",
  "In this example, the \ufb01rst word is just the company name and the remaining words are the company surface forms. We automatically derived the surface forms for each stock by starting with a seed of surface forms extracted from the DBpedia Knowledge Base (KB). We then applied the following procedure: \u2022 Relate each company name with the KB entity unique identi\ufb01er. \u2022 Retrieve all values of the wikiPageRedirects property. The property holds the names of di\ufb00erent pages that points to the same entity/company name. This step sets the initial seed of surface forms. \u2022 Manually, \ufb01lter out some noisy property values. For instance, from the Procter & Glamble entity page we were able to automatically extract dbr:Procter and gamble and dbr:P & G, but had to manually exclude the noisy associations dbr:Female pads and dbr:California Natural. The result of the steps above is a dictionary of surface forms wdsc. 6",
  "3.3. Stock headlines Our corpus is built at stock code level by collecting headlines from the Reuters Archive. This archive groups the headlines by date, starting from 1 January 2007. Each headline is a html link (<a href> tag) to the full body of the news, where the anchor text is the headline content followed by the release time. For example, the page dated 16 Dec 2016 has the headline \u201cProcter & Gamble appoints Nelson Peltz to board 5:26PM UTC\u201d. For each of the 50 stocks (5 sectors times 10 stocks per sector) selected using the criteria described in subsection 3.1, we retrieved all the headlines from the Reuters Archive raging from 01/01/2007 to 30/12/2017. This process takes the following steps: \u2022 For a given stock code (sc) retrieve all surface forms wdsc. \u2022 For each day, store only the headlines content matching any word in wdsc. For each stored headline we also store the time and timezone. \u2022 Convert the news date and time to Eastern Daylight Time (EDT)7. \u2022 Categorize the news release time.",
  "\u2022 For each day, store only the headlines content matching any word in wdsc. For each stored headline we also store the time and timezone. \u2022 Convert the news date and time to Eastern Daylight Time (EDT)7. \u2022 Categorize the news release time. We consider the following category set: {before market, during market , after market, holidays, weekends}. during market contains news between 9:30AM and 4:00PM. before market before 9:30AM and after market after 4:00PM. The time categories prevents any misalignment between text and stock price data8. Moreover, it prevents data leakage and, consequently, unrealistic pre- dictive model performance. In general, news released after 4:00PM EDT can drastically change market expectations and the returns calculated using close to close prices as in the GARCH(1,1) model (see Equation 1). Following [4], to deal with news misalignment, news issued after 4:00PM (after market) are grouped with the pre-market (before market) on the following trading day. Table 2 shows the distribution of news per sector for each time category.",
  "Following [4], to deal with news misalignment, news issued after 4:00PM (after market) are grouped with the pre-market (before market) on the following trading day. Table 2 shows the distribution of news per sector for each time category. We can see a high concentration of news released before the market opens (55% on average). In contrast, using a corpus compiled from message boards, a large occurrence of news during market hours was found [4]. This behaviour indicating day traders\u2019 activity. Our corpus comprise \ufb01nancial news agency headlines, a content more focused on corporate events (e.g. lawsuits, merges & acquisitions, research & development) and on economic news (see Table 3 for a sample of our dataset). These headlines are mostly factual. On the other hand, user- generated content such as Twitter and message boards (as in [4, 5]) tends to be more subjective. U.S. macroeconomic indicators such as Retail Sales, Jobless Claims and GDP are mostly released around 8:30AM (one hour before the market opens).",
  "U.S. macroeconomic indicators such as Retail Sales, Jobless Claims and GDP are mostly released around 8:30AM (one hour before the market opens). These numbers are key drivers of market activity and, as such, have a high media 7The timezone of the New York Stock exchange 8Note that changing the timezone can change the original news date. 7",
  "coverage. Speci\ufb01c sections of these economic reports impact several stocks and sectors. Another factor that contribute to the high activity of news outside regular trading hours are company earnings reports. These are rarely released during trading hours. Finally, before the market opens news agencies provide a summary of the international markets developments, e.g. the key facts during the Asian and Australian trading hours. All these factors contribute to the high concentration of pre-market news. 4. Background We start this section by reviewing the GARCH(1,1) model, which is a strong benchmark used to evaluate our neural model. We then review the source datasets proposed in the literature that were trained independently and trans- fered to our volatility prediction model. Finally, we review the general architec- tures of sequence modelling and attention mechanisms. 4.1. GARCH model Financial institutions use the concept of \u201cValue at risk\u201d to measure the expected volatility of their portfolios. The widespread econometric model for volatility forecasting is the Generalized Autoregressive Conditional Heteroskedas- ticity (GARCH) [33, 34].",
  "4.1. GARCH model Financial institutions use the concept of \u201cValue at risk\u201d to measure the expected volatility of their portfolios. The widespread econometric model for volatility forecasting is the Generalized Autoregressive Conditional Heteroskedas- ticity (GARCH) [33, 34]. Previous research shows that the GARCH(1,1)9 model is hard to beat. For example, [35] compared GARCH(1,1) with 330 di\ufb00erent econometric volatility models showing that they are not signi\ufb01cantly better than GARCH(1,1). Let pt be the price of an stock at the end of a trading period with closing returns rt given by rt = pt pt\u22121 \u22121 (1) The GARCH process explicitly models the time-varying volatility of asset re- turns.",
  "Let pt be the price of an stock at the end of a trading period with closing returns rt given by rt = pt pt\u22121 \u22121 (1) The GARCH process explicitly models the time-varying volatility of asset re- turns. In the GARCH(1,1) speci\ufb01cation the returns series rt follow the process: rt = \u00b5 + \u03f5t (2) \u03f5t = \u03c3tzt (3) \u03c32 t = a0 + a1\u03f52 t\u22121 + b1\u03c32 t\u22121 (4) where \u00b5 is a constant (return drift) and zt is a sequence of i.i.d. random variables with mean zero and unit variance. It is worth noting that although the conditional mean return described in Equation 2 has a constant value, the conditional volatility \u03c3t is time-dependent and modeled by Equation 31. 9The GARCH(p,q) model is speci\ufb01ed in terms of the number of lagged terms p and q. The GARCH(1,1) speci\ufb01cation considers only one lagged volatility (p = 1) and shock (q = 1) terms. 8",
  "4.1.1. Forecasting The one-step ahead expected volatility forecast can be computed directly from Equation 4 and is given by ET [\u03c32 T +1] = a0 + a1ET [\u03f52] + b1ET [\u03c32 T ] (5) In general, the t\u2032-steps ahead expected volatility ET [\u03c32 T +t\u2032] can be easily ex- pressed in terms of the previous step expected volatility. It is easy to prove by induction that the forecast for any horizon can be represented in terms of the one-step ahead forecast and is given by ET [\u03c32 T +t\u2032] \u2212\u03c32 u = (a1 + b1)(t\u2032\u22121) \u0000ET [\u03c32 T +1] \u2212\u03c32 u \u0001 (6) where \u03c3u is the unconditional volatility: \u03c3u = p a0/(1 \u2212a1 \u2212b1) (7) From the equation above we can see that for long horizons, i.e. t\u2032 \u2192\u221e, the volatility forecast in Equation 6 converges to the unconditional volatility in Equation 7.",
  "t\u2032 \u2192\u221e, the volatility forecast in Equation 6 converges to the unconditional volatility in Equation 7. All the works reviewed in section 1 ([13, 14, 15, 16, 17]) consider GARCH(1,1) benchmark. However, given the long horizon of their predictions (e.g. quarterly or annual), the models are evaluated using the unconditional volatility \u03c3u in Equation 7. In this work, we focus on the short-term volatility prediction and use the GARCH(1,1) one-day ahead conditional volatility prediction in Equa- tion 5 to evaluate our models. 4.1.2.",
  "In this work, we focus on the short-term volatility prediction and use the GARCH(1,1) one-day ahead conditional volatility prediction in Equa- tion 5 to evaluate our models. 4.1.2. Evaluation Let \u03c3t+1 denote the ex-post \u201ctrue\u201d daily volatility at a given time t. The performance on a set with N daily samples can be evaluated using the standard Mean Squared Error (MSE) and Mean Absolute Error (MAE) MSE = 1 N N X t=1 (Et[\u03c3t+1] \u2212\u03c3t+1)2 (8) MAE = 1 N N X t=1 |Et[\u03c3t+1] \u2212\u03c3t+1| (9) Additionally, following [36], the models are also evaluated using the coe\ufb03- cient of determination R2 of the regression \u03c3t+1 = a + bEt[\u03c3t+1] + et (10) where R2 = 1 \u2212 PN t=1 e2 t PN t=1 \u0010 Et[\u03c3t+1] \u22121 N PN t=1 Et[\u03c3t+1] \u00112 (11) 9",
  "One of the challenges in evaluating GARCH models is the fact that the ex-post volatility \u03c3t+1 is not directly observed. Apparently, the squared daily returns r2 t+1 in Equation 1 could stand as a good proxy for the ex-post volatility. However, the squared returns yield very noisy measurements. This is a direct consequence of the term zt that connects the squared return to the latent volatil- ity factor in Equation 3. The use of intraday prices to estimate the ex-post daily volayility was \ufb01rst proposed in [36]. They argue that volatility estimators using intraday prices is the proper way to evaluate the GARCH(1,1) model, as op- posed to squared daily returns. For example, considering the Deutsche Mark the GARCH(1,1) model R2 improves from 0.047 (squared returns) to 0.33 (intraday returns)10 [36]. 4.1.3. Range measures to daily volatility proxy It is clear from the previous section that any volatility model evaluation using the noisy squared returns as the ex-post volatility proxy will lead to very poor performance.",
  "4.1.3. Range measures to daily volatility proxy It is clear from the previous section that any volatility model evaluation using the noisy squared returns as the ex-post volatility proxy will lead to very poor performance. Therefore, high-frequency intraday data is fundamental to short-term volatility performance evaluation. However, intraday data is di\ufb03cult to acquire and costly. Fortunately, there are statistically e\ufb03cient daily volatility estimators that only depend on the open, high, low and close prices. These price \u201cranges\u201d are widely available. In this section, we discuss these estimators. Let Ot, Ht, Lt, Ct be the open, high, low and close prices of an asset in a given day t. Assuming that the daily price follows a geometric Brownian motion with zero drift and constant daily volatility \u03c3, Parkinson (1980) derived the \ufb01rst daily volatility estimator \\ \u03c32 P K,t = ln \u0010 Ht Lt \u00112 4 ln(2) (12) which represents the daily volatility in terms of its price range. Hence, it con- tains information about the price path.",
  "Hence, it con- tains information about the price path. Given this property, it is expected that \u03c3P K is less noisy than the volatility calculated using squared returns. The Parkinson\u2019s volatility estimator was extended by Garman-Klass (1980) which incorporates additional information about the opening (Ot) and closing (Ct) prices and is de\ufb01ned as \\ \u03c32 GK,t = 1 2 ln \u0012Ht Lt \u00132 \u2212(2 ln(2) \u22121) ln \u0012Ct Ot \u00132 (13) The relative noisy of di\ufb00erent estimators \u02c6\u03c3 can be measured in terms of its relative e\ufb03ciency to the daily volatility \u03c3 and is de\ufb01ned as e \u0010 c \u03c32, \u03c32\u0011 \u2261V ar[\u03c32] V ar[c \u03c32] (14) 10The intraday estimator is calculated using squared returns of price data sampled every 5 minutess. 10",
  "where V ar[\u00b7] is the variance operator. It follows directly from Equation 3 that the squared return has e\ufb03ciency 1 and therefore, very noisy. [37] reports Parkin- son (\\ \u03c32 P K,t) volatility estimator has 4.9 relative e\ufb03ciency and Garman-Klass (\\ \u03c32 GK,t) 7.4. Additionally, all the described estimators are unbiased. Many alternative estimators to daily volatility have been proposed in the literature. However, experiments in [37] rate the Garman-Klass volatility esti- mator as the best volatility estimator based only on open, high, low and close prices. In this work, we train our models to predict the state-of-the-art Garman- Klass estimator. Moreover, we evaluate our models and GARCH(1,1) using the metrics described in subsubsection 4.1.2, but with the appropriate volatility proxies, i.e. Parkinson and Garman-Klass estimators. 4.2.",
  "Moreover, we evaluate our models and GARCH(1,1) using the metrics described in subsubsection 4.1.2, but with the appropriate volatility proxies, i.e. Parkinson and Garman-Klass estimators. 4.2. Transfer Learning from other source domains Vector representations of words, also known as Word embeddings [22, 38], that represent a word as a dense vector has become the standard building blocks of almost all NLP tasks. These embeddings are trained on large unlabeled corpus and are able to capture context and similarity among words. Some attempts have been made to learn vector representations of a full sen- tence, rather than only a single word, using unsupervised approaches similar in nature to word embeddings. Recently, [18] showed state-of-the-art performance when a sentence encoder is trained end-to-end on a supervised source task and transferred to other target tasks. Inspired by this work, we investigate the per- formance of sentence encoders trained on the Text categorization and Natural Language Inference (NLI) tasks and use these encoders in our main short-term volatility prediction task.",
  "Inspired by this work, we investigate the per- formance of sentence encoders trained on the Text categorization and Natural Language Inference (NLI) tasks and use these encoders in our main short-term volatility prediction task. A generic sentence encoder Se receives the sentence words as input and returns a vector representing the sentence. This can be expressed as a mapping Se : RT S\u00d7dw \u2192RdS (15) from a variable size sequence of words to a sentence vector S of \ufb01xed-size dS, where T S is the sentence number of words and dw is the pre-trained word embedding dimension. In the following sections, we describe the datasets and architectures to train the sentence encoders of the auxiliary transfer learning tasks. 4.2.1. Reuters RCV1 The Reuters Corpus Volume I (RCV1) is corpus containing 806,791 news articles in the English language collected from 20/08/1996 to 19/08/1997 [32]. The topic of each news was human-annotated using a hierarchical structure.",
  "Reuters RCV1 The Reuters Corpus Volume I (RCV1) is corpus containing 806,791 news articles in the English language collected from 20/08/1996 to 19/08/1997 [32]. The topic of each news was human-annotated using a hierarchical structure. At the top of the hierarchy, lies the coarse-grained categories: CCAT (Cor- porate), ECAT (Economics), GCAT (Government), and MCAT (Markets). A news article can be assigned to more than one category meaning that the text categorization task is mutilabel. Each news is stored in a separate XML \ufb01le. Listing 1 shows the typical structure of an article. 11",
  "<?xml version=\u201d 1.0 \u201d encoding=\u201d iso \u22128859\u22121\u201d ?> <newsitem itemid=\u201d6159\u201d id=\u201d root \u201d date=\u201d1996\u221208\u221221\u201d xml:lang=\u201den\u201d> <headline>Colombia r a i s e s i n t e r n a l c o f f e e p r i c e .</ headline> <d a t e l i n e>BOGOTA 1996\u221208\u221221</ d a t e l i n e> <copyright>( c ) Reuters Limited 1996</ copyright> <metadata> <codes c l a s s=\u201d b i p : t o p i c s : 1 . 0 \u201d> <code code=\u201dC13\u201d> <e d i t d e t a i l a t t r i b u t i o n=\u201d Reuters BIP Coding Group\u201d action=\u201d confirmed \u201d date=\u201d1996\u221208\u221221\u201d/> </code> <code code=\u201dC31\u201d> <e d i t d e t a i l a t t r i b u t i o n=\u201d Reuters BIP Coding Group\u201d action=\u201d confirmed \u201d",
  "Reuters BIP Coding Group\u201d action=\u201d confirmed \u201d date=\u201d1996\u221208\u221221\u201d/> </code> <code code=\u201dC31\u201d> <e d i t d e t a i l a t t r i b u t i o n=\u201d Reuters BIP Coding Group\u201d action=\u201d confirmed \u201d date=\u201d1996\u221208\u221221\u201d/> </code> <code code=\u201dCCAT\u201d> <e d i t d e t a i l a t t r i b u t i o n=\u201d Reuters BIP Coding Group\u201d action=\u201d confirmed \u201d date=\u201d1996\u221208\u221221\u201d/> </code> <code code=\u201dM14\u201d> <e d i t d e t a i l a t t r i b u t i o n=\u201d Reuters BIP Coding Group\u201d action=\u201d confirmed \u201d date=\u201d1996\u221208\u221221\u201d/> </code> <code code=\u201dM141\u201d> <e d i t d e t a i l a t t r i b u t i o n=\u201d Reuters BIP Coding Group\u201d action=\u201d confirmed \u201d date=\u201d1996\u221208\u221221\u201d/> </code> <code code=\u201dMCAT\u201d> <e d i t d e t a",
  "<e d i t d e t a i l a t t r i b u t i o n=\u201d Reuters BIP Coding Group\u201d action=\u201d confirmed \u201d date=\u201d1996\u221208\u221221\u201d/> </code> <code code=\u201dMCAT\u201d> <e d i t d e t a i l a t t r i b u t i o n=\u201d Reuters BIP Coding Group\u201d action=\u201d confirmed \u201d date=\u201d1996\u221208\u221221\u201d/> </code> </ codes> </metadata> </newsitem> Listing 1: RCV1 dataset article example. For brevity\u2019s sake, we only show the markup consumed in our models. This headline has root categories CCAT (Corporate/Industrial) and MCAT (Markets) with direct children categories C13 (REGULATION/POLICY), C31 (MARKETS/MARKETING) and M14 (COMMODITY MARKETS). The last category M141 (SOFT COMMODITIES) is a children of M14 and describes the commodity market type. The RCV1 dataset is not released with a standard train, validation, test split.",
  "The last category M141 (SOFT COMMODITIES) is a children of M14 and describes the commodity market type. The RCV1 dataset is not released with a standard train, validation, test split. In this work, we separated 15% of samples as a test set for evaluation purposes. The remaining samples were further split leaving 70% and 15% for training and validation, respectively. Regarding the categories distribution, we found that, from the original 126 categories, 23 categories were never assigned to any news; therefore, were dis- regarded. From the 103 classes left we found a high imbalance among the labels with a large number of underrepresented categories having less than 12 samples. The very low number of samples for these minority classes brings a great challenge to discriminate the very \ufb01ne-grained categories. Aiming to alleviate this problem, we grouped into a same class all categories below the second hierarchical level. For example, given the root node CCAT (Corporate) we grouped C151 (ACCOUNTS/EARNINGS), C1511 (ANNUAL RESULTS) 12",
  "and C152 (COMMENT/FORECASTS) into the direct child node C15 (PER- FORMANCE). Using this procedure the original 103 categories where reduced to 55. One of the bene\ufb01ts of this procedure was that the less represented classes end up having around thousand samples compared with only 12 samples in the original dataset. Figure 1, shows the architecture for the end-to-end text categorization task. On the bottom of the architecture Se receives word embeddings and outputs a sentence vector S. The S vector pass through a fully connected (FC) layer with sigmoid activation function that outputs a vector \u02c6y \u2208R55 with each element \u02c6yj \u2208[0, 1]. Figure 1: RCV1 text categorization architecture. The sentence encoder Se maps word emebddings wi to a sentence vector S and the last FC layer has a sigmoid activation function. The architecture described above is trained under the assumption that each category is independent but not mutually exclusive since a sample can have more than one category assigned (multilabel classi\ufb01cation).",
  "The architecture described above is trained under the assumption that each category is independent but not mutually exclusive since a sample can have more than one category assigned (multilabel classi\ufb01cation). The loss per sample is the average log loss across all labels: L(\u02c6y, y) = \u2212 55 X i=1 (yi log(\u02c6yi) + (1 \u2212yi) log(1 \u2212\u02c6yi)) (16) where the index i runs over the elements of the predicted and true vectors. Given the high categories imbalance, during the training we monitor the F1 metric of the validation set and choose the model with the highest value. 4.2.2. SNLI dataset Stanford Natural Language Inference (SNLI) dataset [31] consist of 570,000 pairs of sentences. Each pair has a premise and a hypothesis, manually labeled with one of the three labels: entailment, contradiction, or neutral. The SNLI has many desired properties. The labels are equally balanced, as opposed to the 13",
  "RCV1 dataset. Additionally, language inference is a complex task that requires a deeper understanding of the sentence meaning making this dataset suitable for learning supervised sentence encoders that generalize well to other tasks [18]. Table 4, shows examples of SNLI dataset sentence pairs and its respective labels. In order to learn sentence encoders that can be transfered to other tasks unambiguously, we consider a neural network architecture for the sentence en- coder with shared parameters between the premise and hypothesis pairs as in [18]. Figure 2, describes the neural network architecture. After each premise and hypothesis is encoded into Sp and Sh, respectively, we have a fusion layer. This layer has no trainable weights and just concatenate each sentence embedding. Following [18], we add two more matching methods: the absolute di\ufb00erence |Sp \u2212Sh| and the element-wise Sp \u2299Sh. Finally, in order to learn the pair representation, Sph is feed into and FC layer with recti\ufb01ed linear unit (ReLU) activation function, which is expressed as f(x) = log(1 + ex). The last softmax layer outputs the probability of each class.",
  "Finally, in order to learn the pair representation, Sph is feed into and FC layer with recti\ufb01ed linear unit (ReLU) activation function, which is expressed as f(x) = log(1 + ex). The last softmax layer outputs the probability of each class. Figure 2: Natural Language Inference task architecture. Note that the sentence encoder Se is shared between the premise and hypothesis pair. The FC layer learns the representation of the sentence pair and the \ufb01nal Softmax layer asserts the output of the 3 possible labels, i.e. [entailment, contradiction, neutral], sums to one. Finally, the NLI classi\ufb01er weights are optimized in order to minimize the categorical log loss per sample L(\u02c6y, y) = \u2212 3 X j=1 yi log(\u02c6yi) (17) During the training, we monitor the validation set accuracy and choose the model with the highest metric value. 14",
  "4.3. Sequence Models We start this section by reviewing the Recurrent Neural Network (RNN) architecture and its application to encode a sequence of words. RNN\u2019s are capable of handling variable-length sequences, this being a di- rect consequence of its recurrent cell, which shares the same parameters across all sequence elements. In this work, we adopt the Long Short-Term Memory (LSTM) cell [39] with forget gates ft [40]. The LSTM cell is endowed with a memory state that can learn representations that depend on the order of the words in a sentence. This makes LSTM more \ufb01t to \ufb01nd relations that could not be captured using standard bag-of-words representations. Let x1, x2, \u00b7 \u00b7 \u00b7 , xT be a series of observations of length T, where xt \u2208Rdw. In general terms, the LSTM cell receives a previous hidden state ht\u22121 that is combined with the current observation xt and a memory state Ct to output a new hidden state ht. This internal memory state Ct is updated depending on its previous state and three modulating gates: input, forget, and output.",
  "In general terms, the LSTM cell receives a previous hidden state ht\u22121 that is combined with the current observation xt and a memory state Ct to output a new hidden state ht. This internal memory state Ct is updated depending on its previous state and three modulating gates: input, forget, and output. Formally, for each step t the updating process goes as follows (see Figure 3 for a high level schematic view): First, we calculate the input it, forget ft, and output ot gates: it = \u03c3s (Wixt + Uiht\u22121 + bi) (18) ft = \u03c3s (Wfxt + Ufht\u22121 + bf) (19) ot = \u03c3s (Woxt + Uoht\u22121 + bo) (20) where \u03c3s is the sigmoid activation. Second, a candidate memory state eCt is generated: eCt = tanh (Wcxt + Ucht\u22121 + bc) (21) Now we are in a position to set the \ufb01nal memory state Ct.",
  "Second, a candidate memory state eCt is generated: eCt = tanh (Wcxt + Ucht\u22121 + bc) (21) Now we are in a position to set the \ufb01nal memory state Ct. Its value is modulated based on the input and forget gates of Equation 20 and is given by: Ct = it \u2299eCt + ft \u2299Ct\u22121 (22) Finally, based on the memory state and output gate of Equation 20, we have the output hidden state ht = ot \u2299tanh (Ct) (23) Regarding the trainable weights, let n be the LSTM cell number of units. It follows that W\u2019s and U\u2019s matrices of the a\ufb03ne transformations have n \u00d7 dw and n \u00d7 n dimensions, respectively. Its bias terms b\u2019s are vectors of size n. Consequently, the total number of parameters is 4(ndw + n2 + n) and does not depend on the sequence number of time steps T. We see that the LSTM networks are able to capture temporal dependencies in sequences of arbitrary length.",
  "Its bias terms b\u2019s are vectors of size n. Consequently, the total number of parameters is 4(ndw + n2 + n) and does not depend on the sequence number of time steps T. We see that the LSTM networks are able to capture temporal dependencies in sequences of arbitrary length. One straightforward application is to model the Sentence encoder discussed in subsection 4.2, which outputs a sentence vector representation using its words as input. Given a sequence of words {wt}T t=1 we aim to learn the words hidden state {ht}T t=1 in a way that each word captures the in\ufb02uence of its past and future words. The Bidirectional LSTM (BiLSTM) proposed in [41] is an LSTM that 15",
  "Figure 3: Schematic view of a LSTM cell. The observed state xt is combined with previous memory and hidden states to output a hidden state ht. The memory state Ct is an internal state; therefore, not part of the output representation. An LSTM network is trained by looping its shared cell across all sequence length. \u201creads\u201d a sentence, or any sequence in general, from the beginning to the end (forward) and the other way around (backward). The new state ht is the con- catenation ht = [\u2212\u2192 ht, \u2190\u2212 ht] (24) where \u2212\u2192 ht = LSTM (w1, \u00b7 \u00b7 \u00b7 , wT ) (25) \u2190\u2212 ht = LSTM (wT , \u00b7 \u00b7 \u00b7 , w1) (26) (27) Because sentences have di\ufb00erent lengths, we need to convert the T concate- nated hidden states of the BiLSTM into a \ufb01xed-length sentence representation. One straightforward operation is to apply any form of pooling. Attention mecha- nism is an alternative approach where the sentence is represented as an weighted average of hidden states where the weights are learnt end-to-end.",
  "One straightforward operation is to apply any form of pooling. Attention mecha- nism is an alternative approach where the sentence is represented as an weighted average of hidden states where the weights are learnt end-to-end. In the next sections we describe the sentence encoders using pooling and attention layers. 4.3.1. BiLSTM max-pooling The max-pooling layer aims to extract the most salient word features all over the sentence. Formally, it outputs a sentence vector representation SMP \u2208R2n such that SMP = T max t=1 ht (28) where ht is de\ufb01ned in Equation 24 and the max operator is applied over the time steps dimension. Figure 4 illustrates the BiLSTM max-pooling (MP) sentence encoder. The e\ufb03cacy of the max-pooling layer was assessed in many NLP studies. [42] employed a max-pooling layer on top of word representations and argues that it performs better than mean pooling. Experimental results in [18] show that 16",
  "among three types of pooling (max, mean and last11) the max-pooling provides the most universal sentence representations in terms of transferring performance to other tasks. Grounded on these studies, in this work, we choose the BiLSTM max-pooling as our pooling layer of choice. Figure 4: BiLSTM max-pooling. The network performs a polling operation on top of each word hidden state. 4.3.2. BiLSTM attention Attention mechanisms were introduced in the deep learning literature to overcome some simpli\ufb01cations imposed by pooling operators. When we humans read a sentence, we are able to spot its most relevant parts in a given context and disregard information that is redundant or misleading. The attention model aims to mimic this behaviour. Attention layers were proposed for di\ufb00erent NLP tasks. For example, NLI, with cross-attention between premise and hypothesis, Question & Answering and Machine Translation (MT). Speci\ufb01cally in the Machine Translation task, each word in the target sentence learns to attend the relevant words of the source sentence in order to generate the sentence translation.",
  "For example, NLI, with cross-attention between premise and hypothesis, Question & Answering and Machine Translation (MT). Speci\ufb01cally in the Machine Translation task, each word in the target sentence learns to attend the relevant words of the source sentence in order to generate the sentence translation. A sentence encoder with attention (or self-attentive) [43, 44, 45] assigns di\ufb00erent weights to the own words of the sentence; therefore, converting the hidden states into a single sentence vector representation. Considering the word hidden vectors set {h1, \u00b7 \u00b7 \u00b7 , hT } where ht \u2208Rn, the 11The \u201clast\u201d polling is a simple operator that takes only the last element of the T hidden states to represent a sentence. 17",
  "attention mechanism is de\ufb01ned by the equations: \u02dcht = \u03c3 (Wht + b) (29) \u03b1t = exp(v\u22ba\u00b7 \u02dcht) P t exp(v \u00b7 \u02dcht) (30) SAw = X t \u03b1tht (31) where W \u2208Rda\u00d7n, b \u2208Rda\u00d71, and v \u2208Rda\u00d71 are trainable parameters. We can see that the sentence representation SAw is a weighted average of the hidden states. Figure 5 provides a schematic view of the BiLSTM attention, where we can account the attention described in Equation 31 as a two layer model with a dense layer (da units) followed by another dense that predicts \u03b1t (single unit). Figure 5: BiLSTM attention. The speci\ufb01c example encodes a headline from our corpus. 5. Methodology In this section, we \ufb01rst introduce our problem in a deep multimodal learning framework. We then present our neural architecture, which is able to address the problems of news relevance and novelty. Finally, we review the methods applied to learn commonalities between stocks (global features).",
  "Methodology In this section, we \ufb01rst introduce our problem in a deep multimodal learning framework. We then present our neural architecture, which is able to address the problems of news relevance and novelty. Finally, we review the methods applied to learn commonalities between stocks (global features). 5.1. Problem statement Our problem is to predict the daily stock volatility. As discussed in subsub- section 4.1.3, the Gaman-Klass estimator \\ \u03c3GK,t in Equation 13 is a very e\ufb03cient short-term volatility proxy, thus, it is adopted as our target variable. 18",
  "Our goal is to learn a mapping between the next day volatility \u03c3t+1 and historical multimodal data available up to day t. To this aim, we use a sliding window approach with window size T. That is, for each stock sc a sample on day t is expressed as a sequence of historical prices P sc t and corpus headlines N sc t . The price sequence is a vector of Daily Prices (DP) and expressed as P sc t = \u0002 DP sc t\u2212T , DP sc t\u2212T +1, \u00b7 \u00b7 \u00b7 , DP sc t \u0003 (32) where DP sc t\u2032 is a vector of price features.",
  "The price sequence is a vector of Daily Prices (DP) and expressed as P sc t = \u0002 DP sc t\u2212T , DP sc t\u2212T +1, \u00b7 \u00b7 \u00b7 , DP sc t \u0003 (32) where DP sc t\u2032 is a vector of price features. In order to avoid task-speci\ufb01c feature engineering, the daily price features are expressed as the simple returns: DP sc t = \u0014 Osc t Csc t\u22121 \u22121, Hsc t Csc t\u22121 \u22121, Lsc t Csc t\u22121 \u22121, Csc t Csc t\u22121 \u22121 \u0015 (33) The sequence of historical corpus headlines N sc t is expressed as N sc t = \u0002 nsc t\u2212T , nsc t\u2212T +1, \u00b7 \u00b7 \u00b7 , nsc t \u0003 (34) where nsc t\u2032 is a set containing all headlines that in\ufb02uence the market on a given day t\u2032. Aiming to align prices and news modes, we consider the explicit alignment method discussed in subsection 3.3.",
  "Aiming to align prices and news modes, we consider the explicit alignment method discussed in subsection 3.3. That is, nsc t\u2032 contains all stock headlines before the market opens (before markett), during the trading hours (during markett), and previous day after-markets (after markett\u22121). As a text preprocessing step, we tokenize the headlines and convert each word to an integer that refers to its respective pre-trained word embedding. This process is described as follows: First, for all stocks of our corpus we tokenize each headline and extract the corpus vocabulary set V . We then build the embedding matrix Ew \u2208R|V |\u00d7dw, where each row is a word embedding vector dw dimensions. Words that do not have a corresponding embedding, i.e. out of vocabulary words, are skipped. Finally, the input sample of the text mode is a tensor of integers with T \u00d7 ln \u00d7 ls dimensions, where ln is the maximum number of news occurring in a given day and ls is the maximum length of a corpus sentence. Regarding the price mode, we have a T \u00d7 4 tensor of \ufb02oating numbers. 5.2.",
  "Regarding the price mode, we have a T \u00d7 4 tensor of \ufb02oating numbers. 5.2. Global features and stock embedding Given the price and news histories for each stock sc we could directly learn one model per stock. However, this approach su\ufb00ers from two main drawbacks. First, the market activity of one speci\ufb01c stock is expected to impact other stocks, which is a widely accepted pattern named \u201cspillover e\ufb00ect\u201d. Second, since our price data is sampled on a daily basis, we would train the stock model relying on a small number of samples. One possible solution to model the commonality among stocks would be feature enrichment. For example, when modeling a given stock X we would enrich its news and price features by concatenating features 19",
  "from stock Y and Z. Although the feature enrichment is able to model the e\ufb00ect of other stocks, it still would consider only one sample per day. In this work, we propose a method that learns an global model. The global model is implemented using the following methods: \u2022 Multi-Stock batch samples: Since our models are trained using Stochas- tic Gradient Descent, we propose at each mini-batch iteration to sample from a batch set containing any stock of our stocks universe. As a conse- quence, the mapping between volatility and multimodal data is now able to learn common explanatory factors among stocks. Moreover, adopting this approach increases the total number of training samples, which is now the sum of the number of samples per stock. \u2022 Stock Embedding: Utilizing the Multi-Stock batch samples above, we tackle the problem of modeling commonality among stocks. However, it is reasonable to assume that stocks have part of its dynamic driven by idiosyncratic factors. Nevertheless, we could aggregate stocks per sector or rely on any measure of similarity among stocks.",
  "However, it is reasonable to assume that stocks have part of its dynamic driven by idiosyncratic factors. Nevertheless, we could aggregate stocks per sector or rely on any measure of similarity among stocks. In order to incorporate information speci\ufb01c to each stock, we propose to equip our model with a \u201cstock embedding\u201d mode that is learnt jointly with price and news modes. That is to say, we leave the task of distinguishing the speci\ufb01c dynamic of each stock to be learnt by the neural network. Speci\ufb01cally, this stock embedding is modeled using a discrete encoding as input, i.e. Isc t is a vector with size equal to the number of stocks of the stocks universe and has element 1 for the i-th coordinate and 0 elsewhere, thus, indicating the stock of each sample.",
  "Isc t is a vector with size equal to the number of stocks of the stocks universe and has element 1 for the i-th coordinate and 0 elsewhere, thus, indicating the stock of each sample. Formally, we can express the one model per stock approach as the mapping \u03c3sc t+1 = f sc(DN sc t\u2212T , DN sc t\u2212T +1, \u00b7 \u00b7 \u00b7 , DN sc t ; DP sc t\u2212T , DP sc t\u2212T +1, \u00b7 \u00b7 \u00b7 , DP sc t ) (35) where DN sc t\u2032 is a \ufb01xed-vector representing all news released on a given day for the stock sc12 and DP sc t\u2032 is de\ufb01ned in Equation 33. The global model attempts to learn a single mapping f that at each mini- batch iteration randomly aggregates samples across all the universe of stocks, rather than one mapping f sc per stock.",
  "The global model attempts to learn a single mapping f that at each mini- batch iteration randomly aggregates samples across all the universe of stocks, rather than one mapping f sc per stock. The global model is expressed as \u03c3sc t+1 = f(DN sc t\u2212T , DN sc t\u2212T +1, \u00b7 \u00b7 \u00b7 , DN sc t ; DP sc t\u2212T , DP sc t\u2212T +1, \u00b7 \u00b7 \u00b7 , DP sc t ; Isc t ) (36) In the next section, we describe our hierarchical neural model and how the news, price and stock embedding are fused into a joint representation. 12It will become clear in the next section how this news representation is modelled. 20",
  "5.3. Our multimodal hierarchical network In broad terms, our hierarchical neural architecture is described as follows. First, each headline released on a given day t is encoded into a \ufb01xed-size vector St using a sentence encoder13. We then apply our daily New Relevance Attention (NRA) mechanism that attends each news based on its content and converts a variable size of news released on a given day into a single vector denoted by Daily News (DN). We note that this representation take account of the overall e\ufb00ect of all news released on a given day. This process is illustrated in Figure 6. We now are in a position to consider the temporal e\ufb00ect of the past T days of market news and price features. Figure 7 illustrates the neural network architecture from the temporal sequence to the \ufb01nal volatility prediction.",
  "This process is illustrated in Figure 6. We now are in a position to consider the temporal e\ufb00ect of the past T days of market news and price features. Figure 7 illustrates the neural network architecture from the temporal sequence to the \ufb01nal volatility prediction. For each stock code sc the temporal encoding for news is denoted by Market News MN sc t and for the price by Market Price MP sc t and are a function of the past T Daily News representations {DN sc t\u2212T , \u00b7 \u00b7 \u00b7 , DN sc t } (Text mode) and Daily Prices features {DP sc t\u2212T , \u00b7 \u00b7 \u00b7 , DP sc t } (Price mode), where each Daily Price DP sc t\u2032 feature is given by Equation 33 and the DN sc t\u2032 representation is calculated using Daily New Relevance Attention. After the temporal e\ufb00ects of T past days of market activity were already encoded into the Market News MN sc t and Market Price MP sc t , we concatenate feature-wise MN sc t , MPt and the Stock embedding Esc.",
  "After the temporal e\ufb00ects of T past days of market activity were already encoded into the Market News MN sc t and Market Price MP sc t , we concatenate feature-wise MN sc t , MPt and the Stock embedding Esc. The stock embedding Esc represents the stock code of the sample on a given day t. Finally, we have a Fully Connected (FC) layer that learns the Joint Representation of all modes. This \ufb01xed-sized joint representation is fed into a FC layer with linear activation that predicts the next day volatility \u02c6\u03c3t+1. Below, we detail, for each mode separately, the layers of our hierarchical model. \u2013 Text mode 1. Word Embedding Retrieval Standard embedding layer with no trainable parameters. It receives a vector of word indices as input and returns a matrix of word embeddings. 2. News Encoder This layer encodes all news on a given day and outputs a set news embed- dings {S1 t , \u00b7 \u00b7 \u00b7 , Sln t }. Each encoded sentence has dimension dS, which is a hyperparameter of our model.",
  "2. News Encoder This layer encodes all news on a given day and outputs a set news embed- dings {S1 t , \u00b7 \u00b7 \u00b7 , Sln t }. Each encoded sentence has dimension dS, which is a hyperparameter of our model. This layer constitutes a key component of our neural architectures and, as such, we evaluate our models considering sentence encoders trained end-to-end, using the BiLSTM attention (subsub- section 4.3.2) and BiLSTM max-pooling (subsubsection 4.3.1) architectures, and also transferred from the RCV1 and SNLI as \ufb01xed features. 3. Daily news relevance attention Our proposed news relevance attention mechanism for all news released on a given day. The attention mechanism is introduced to tackle information 13The headline encoding St is learnt end-to-end from the headline word embeddings or transfered from the TL tasks as \ufb01xed features. 21",
  "Figure 6: Daily news relevance attention. The \ufb01gure illustrates a day where three news were released for the Walmart company. After the headlines are encoded into a \ufb01xed-size representation S, the daily news relevance attention AR converts all sentences into single vector representation of all Daily News DN by attending each headline based on its content. overload. It was designed to \u201c\ufb01lter out\u201d redundant or misleading news and focus on the relevant ones based solely on the news content. Formally, the layer outputs a Daily News (DN) embedding DN sc t = Pln i=1 \u03b2iSsci t , which is a linear combination of all encoded news on a given day t. This news- level attention uses the same equations as in Equation 31, but with trainable weights {WR, bR, vR}, i.e. the weights are segregated from the sentence en- coder. Figure 6, illustrates our relevance attention. Note that this layer was deliberately developed to be invariant to headlines permutation, as is the case with the linear combination formula above.",
  "the weights are segregated from the sentence en- coder. Figure 6, illustrates our relevance attention. Note that this layer was deliberately developed to be invariant to headlines permutation, as is the case with the linear combination formula above. The reason is that our price data is sampled daily and, as a consequence, we are not able to discriminate the market reaction for each intraday news. 4. News Temporal Context Sequence layer with daily news embeddings DN sc t as time steps. This layer aims to learn the temporal context of news, i.e. the relationship between the news at day t and the T past days. It receives as input a chronologically ordered sequence of T past Daily News embeddings {DN sc t\u2212T , \u00b7 \u00b7 \u00b7 , DN sc t } and outputs the news mode encoding Market News MN sc t \u2208dMN. The sequence with T time steps is encoded using a BiLSTM attention. The layer was designed to capture the temporal order that news are released and the current news novelty. i.e. news that were repeated in the past can be \u201cforgotten\u201d based on the modulating gates of the LSTM network. \u2013 Price mode 5.",
  "The layer was designed to capture the temporal order that news are released and the current news novelty. i.e. news that were repeated in the past can be \u201cforgotten\u201d based on the modulating gates of the LSTM network. \u2013 Price mode 5. Price Encoder Sequence layer analogous to News Temporal Context, but for the price mode. The input is the ordered sequence Daily Prices {DP sc t\u2212T , \u00b7 \u00b7 \u00b7 , DP sc t } of 22",
  "Figure 7: Hierarchical Neural Network architecture. size T, where each element the price feature de\ufb01ned in Equation 33. Particu- larly, the architecture consists of two stacked LSTM\u2019s. The \ufb01rst one outputs for each price feature time step a hidden vector that takes the temporal con- text into account. Then these hidden vectors are again passed to a second independent LSTM. The layer outputs the price mode encoding Market Price MP sc t \u2208dMP . This encoding is the last hidden vector of the second LSTM Market. \u2013 Stock embedding 6. Stock Encoder Stock dense representation. The layer receives the discrete encoding Isc t indicating the sample stock code pass through a FC layer and outputs a stock embedding Esc. \u2013 Joint Representation 7. Merging Feature-wise News, Price, and Stock modes concatenation. No trainable parameters. 7. Joint Representation Encoder FC layer of size dJR. 5.4. Multimodal learning with missing modes During the training we feed into our neural model the price, news, and stock indicator data. The price and stock indicator modes data occur in all days.",
  "No trainable parameters. 7. Joint Representation Encoder FC layer of size dJR. 5.4. Multimodal learning with missing modes During the training we feed into our neural model the price, news, and stock indicator data. The price and stock indicator modes data occur in all days. However, at the individual stock level we can have days that the company is not covered by the media. This feature imposes challenges to our multimodal training since neural networks are not able to handle missing modes without special intervention. A straightforward solution would be to consider only days with news released, disregarding the remaining samples. However, this approach 23",
  "has two main drawbacks. First, the \u201cmissing news\u201d do not happen at random, or are attributed to measurement failure as is, for example, the case of multimodal tasks using mechanical sensors data. Conversely, as highlighted in [8, 9] the same price behaviour results in distinct market reactions when accompanied or not by news14. In other words, speci\ufb01cally to \ufb01nancial forecasting problems the absence or existence of news are highly informative. Some methods were proposed in the multimodal literature to e\ufb00ectively treat informative missing modes or \u201cinformative missingness\u201d, which is a character- istic refereed in the literature as learning with missing modalities [23]. In this work, we directly model the news missingness as a feature of our text model temporal sequence by using the method initially proposed in [46, 47] for clinical data with missing measurements and applied in the context of \ufb01nancial fore- casting in [48]. Speci\ufb01cally, we implement the Zeros & Imputation (ZI) method [47] in order to jointly learn the price mode and news relationship across all days of market activity.",
  "Speci\ufb01cally, we implement the Zeros & Imputation (ZI) method [47] in order to jointly learn the price mode and news relationship across all days of market activity. The ZI implementation is described as follows: Before the daily news se- quence is processed by the text temporal layer (described in item 4) we input a 0 vector for all time steps with missing news and leave the news encoding unchanged otherwise. This step is called zero imputation. In addition, we con- catenate feature-wise an indicator vector with value 1 for all vectors with zero imputation and 0 for the days with news. As described in [48], the ZI method endow a temporal sequence model with the ability to learn di\ufb00erent representations depending on the news history and its relative time position. Moreover, it allows our model to predict the volatility for all days of our time series and, at the same time, take into account the current and past news informative missingness. Furthermore, the learnt positional news encoding works di\ufb00erently than a typical \u201cmasking\u201d, where days without news are not passed through the LSTM cell.",
  "Furthermore, the learnt positional news encoding works di\ufb00erently than a typical \u201cmasking\u201d, where days without news are not passed through the LSTM cell. Masking the time steps would be losing information about the presence or absence of news concomitant with prices. 6. Experimental results and discussions We aim to evaluate our hierarchical neural model in the light of three main aspects. First, we asses the importance of the di\ufb00erent sentence encoders to our end-to-end models and how it compares to transferring the sentence en- coder from our two auxiliary TL tasks. Second, we ablate our proposed news relevance attention (NRA) component to evaluate its importance. Finally, we consider a model that takes into consideration only the price mode (unimodal), i.e. ignoring any architecture related to the text mode. Before we de\ufb01ne the baselines to asses the three aspects described above, we review in the next section the scores of the trained TL tasks. 14Experimental results [8, 9] demonstrate that large price dislocations in the absence of news tend revert and continue the movement (momentum) when driven by news. 24",
  "6.1. Auxiliary transfer learning tasks This section reports the performance of the auxiliary TL tasks considered in this work. Our ultimate goal is to indicate that our scores are in line with previous works All the architectures presented in subsection 4.2 are trained for a maximum of 50 epochs using mini-batch SGD with Adam optimizer [49]. Moreover, at the end of each epoch, we evaluate the validation scores, which are accuracy (Stanfor SNLI dataset) and F1 (RCV1 dataset), and save the weights with the best values. Aiming to seeped up training, we implement early stopping with patience set to 8 epochs. That is, if the validation scores do not improve for more than 10 epochs we halt the training. Finally, we use Glove pre-trained word embeddings [38] as \ufb01xed features. Table 5 compares our test scores with state-of-the-art (SOTA) results re- ported in previous works. We can see that our scores for the SNLI task are very close to state-of-the-art15.",
  "Table 5 compares our test scores with state-of-the-art (SOTA) results re- ported in previous works. We can see that our scores for the SNLI task are very close to state-of-the-art15. Regarding the RCV1 dataset, our results consider only the headline content for training, while the refereed works consider both the news headline and mes- sage body. The reason for training using only the headlines is that both tasks are learnt with the sole purpose of transferring the sentence encoders to our main volatility prediction task, whose textual input is restricted to headlines. 6.2. Training setup During the training of our hierarchical neural model described in subsec- tion 5.3 we took special care to guard against over\ufb01tting. To this aim, we completely separate 2016 and 2017 as the test set and report our results on this \u201cunseen\u201d set. The remaining data is further split into training (2007 to 2013) and validation (2014 to 2015). The model convergence during training is monitored in the validation set.",
  "The remaining data is further split into training (2007 to 2013) and validation (2014 to 2015). The model convergence during training is monitored in the validation set. We monitor the validation score of our model at the end of each epoch and store the network weights if the validation scores improves between two consecutive epochs. Additionally, we use mini-batch SGD with Adam optimizer and early stopping with patience set to eight epochs. The hyperparameter tunning is performed using grid search. All training is performed using the proposed global model approach de- scribed in subsection 5.2, which learns a model that takes into account the features of all the 40 stocks of our corpus. Using this approach our training set has a total of 97,903 samples. Moreover, during the SGD mini-batch sampling the past T days of price and news history tensors and each stock sample stock indicator are randomly selected from the set of all 40 stocks. 6.3.",
  "Using this approach our training set has a total of 97,903 samples. Moreover, during the SGD mini-batch sampling the past T days of price and news history tensors and each stock sample stock indicator are randomly selected from the set of all 40 stocks. 6.3. Stocks universe result In order to evaluate the contributions of each component of our neural model described in subsection 5.3 and the e\ufb00ect of using textual data to predict the 15Models were trained using a concatenation layer and Bidirectional LSTM with 512 and 1024 units, respectively 25",
  "volatility, we report our results using the following baselines16: 1. - News (unimodal price only): This baseline completely ablates (i.e. removes) any architecture related to the news mode, considering only the price encoding and the stock embedding components. Using this ablation we aim to evaluate the in\ufb02uence of news to the volatility prediction problem. 2. + News (End-to-end Sentence Encoders) - NRA: This baseline ab- lates our proposed new relevance attention (NRA) component, and instead, makes use of the same Daily Averaging method in [27, 28], where all \ufb01xed- sized headline representations on a given day are averaged without taking into account the relevance of each news. We evaluate this baseline for both BiLSTM attention (Att) and BiLSTM max-pooling (MP) sentence encoders. Here, our goal is to asses the true contribution of our NRA component in the case SOTA sentence encoders are taken into account. 3.",
  "We evaluate this baseline for both BiLSTM attention (Att) and BiLSTM max-pooling (MP) sentence encoders. Here, our goal is to asses the true contribution of our NRA component in the case SOTA sentence encoders are taken into account. 3. + News (End-to-End W-L Att Sentence Encoder) + NRA: The Word-Level Attention (W-L Att) sentence encoder implements an attention mechanism directly on top of word embeddings, and, as such, does not con- sider the order of words in a sentence. This baseline complements the previ- ous one, i.e. it evaluates the in\ufb02uence of the sentence encoder when our full speci\ufb01cation is considered. 4. + News (TL Sentence Encoders) + NRA: Makes use of sentence en- coders of our two auxiliary TL tasks as \ufb01xed features. This baseline aims to address the following questions, namely: What dataset and models are more suitable to transfer to our speci\ufb01c volatility forecasting problem; How End-to-End models, which are trained on top of word embeddings, perform compared to sentence encoders transferred from other tasks.",
  "This baseline aims to address the following questions, namely: What dataset and models are more suitable to transfer to our speci\ufb01c volatility forecasting problem; How End-to-End models, which are trained on top of word embeddings, perform compared to sentence encoders transferred from other tasks. Table 6 summarizes the test scores for the ablations discussed above. Our best model is the + News (BiLSTM Att) + NRA, which is trained end-to-end and uses our full architecture. The second best model, i.e. + News (BiLSTM MP) + NRA, ranks slightly lower and only di\ufb00ers form the best model in terms of the sentence encoder. The former sentence encoder uses an attention layer (subsubsection 4.3.2) and the the last a max-pooling layer (subsubsection 4.3.1), where both layers are placed on top of the LSTM hidden states of each word.",
  "The former sentence encoder uses an attention layer (subsubsection 4.3.2) and the the last a max-pooling layer (subsubsection 4.3.1), where both layers are placed on top of the LSTM hidden states of each word. Importantly, our experiments show that using news and price (multimodal) to predict the volatility improves the scores by 11% (MSE) and 9% (MAE) when compared with the News (price only unimodal) model that considers only price features as explanatory variables. When comparing the performance of End-to-End models and the TL auxil- iary tasks the following can be observed: The end-to-end models trained with the two SOTA sentence encoders perform better than transferring sentence en- coder from both auxiliary tasks. However, our experiments show that the same 16Minus sign means to remove (ablate) the neural network component while plus means to include the component. 26",
  "does not hold for models trained end-to-end relying on the simpler WL-Att sen- tence encoder, which ignores the order of words in a sentence. In other words, considering the appropriate TL task, it is preferable to transfer a SOTA sentence encoder trained on a larger dataset than learning a less robust sentence encoder in an end-to-end fashion. Moreover, initially, we thought that being the RCV1 a \ufb01nancial domain corpus it would demonstrate a superior performance when compared to the SNLI dataset. Still, the SNLI transfers better than RCV1. We hypothesize that the text categorization task (RCV1 dataset) is not able to capture complex sentence structures at the same level required to perform natural language inference. Particularly to the volatility forecasting problem, our TL results corroborates the same \ufb01ndings in [18], where it was shown that SNLI dataset attains the best sentence encoding for a broad range of pure NLP tasks, including, among other, text categorization and sentiment analysis.",
  "Particularly to the volatility forecasting problem, our TL results corroborates the same \ufb01ndings in [18], where it was shown that SNLI dataset attains the best sentence encoding for a broad range of pure NLP tasks, including, among other, text categorization and sentiment analysis. Signi\ufb01cantly, experimental results in Table 6 clearly demonstrate that our proposed news relevance attention (NRA) outperforms the News Averaging method proposed in previous studies [27, 28]. Even when evaluating our NRA component in conjunction with the more elementary W-L Att sentence encoder it surpass the results of sophisticated sentence encoder using a News Averag- ing approach. In other words, our results strongly points to the advantage of discriminating noisy from impacting news and the e\ufb00ectiveness of learning to attend the most relevant news. Having analyzed our best model, we now turn to its comparative perfor- mance with respect to the widely regarded GARCH(1,1) model described in subsection 4.1.",
  "Having analyzed our best model, we now turn to its comparative perfor- mance with respect to the widely regarded GARCH(1,1) model described in subsection 4.1. We asses our model performance relative to GARCH(1,1) using standard loss metrics (MSE and MAE) and the regression-based accuracy speci\ufb01ed in Equation 10 and measured in terms of the coe\ufb03cient of determination R2. In addition, we evaluate our model across two di\ufb00erent volatility proxies: Garman- Klass ( d \u03c3GK) (Equation 13) and Parkinson ( d \u03c3P K) (Equation 12). We note that, as reviewed in subsubsection 4.1.2, these two volatility proxies are statically e\ufb03cient and proper estimators of the next day volatility. Table 7 reports the comparative performance among our best Price + News model (+ News BiLSTM (MP) + NRA), our Price only (unimodal) model and GARCH(1,1). The results clearly demonstrate the superiority of our model, being more accurate than GRACH for both volatility proxies.",
  "The results clearly demonstrate the superiority of our model, being more accurate than GRACH for both volatility proxies. We note that evaluating the GARCH(1,1) model relying on standard MSE and MAE error metrics should be taken with a grain of salt. [36] provides the background theory and arguments supporting R2 as the metric of choice to evaluate the predictive power of a volatility model. In any case, the outperformance or our model with respect to GARCH(1,1) permeates all three metrics, name R2, MSE and MAE. 6.4. Sector-level results Company sectors are expected to have di\ufb00erent risk levels, in the sense that each sector is driven by di\ufb00erent types of news and economic cycles. Moreover, by performing a sector-level analysis we were initially interested in understand- ing if the outperformance of our model with respect to GARCH(1,1) was the 27",
  "result of a learning bias to a given sector or if, as turned out to be the case, the superior performance of our model spreads across a diversi\ufb01ed portfolio of sectors. In order to evaluate the performance per sector, we \ufb01rst separate the con- stituents stocks for each sector in Table 1. Then, we calculate the same metrics discussed in the previous section for each sector individually. Table 8 reports our experimental results segregated by sector. We observe that the GRACH model accuracy, measured using the R2 score, has a high degree of variability among sectors. For example, the accuracy ranges from 0.15 to 0.44 for the HealthCare and Energy sector, respectively. This high degree of variability is in agreement with previous results reported in [17], but in the context of long-term (quarterly) volatility predictions. Although the GARCH(1,1) accuracy is sector-dependent, without any exception, our model using price and news as input clearly outperforms GRACH sector-wise. This fact allow us to draw the following conclusions: \u2022 Our model outperformance is persistent across sectors, i.e.",
  "Although the GARCH(1,1) accuracy is sector-dependent, without any exception, our model using price and news as input clearly outperforms GRACH sector-wise. This fact allow us to draw the following conclusions: \u2022 Our model outperformance is persistent across sectors, i.e. the charac- teristics of the results reported in Table 7 permeates all sectors, rather than being composed of a mix of outperforming and underperforming sec- tor contributions. This fact provides a strong evidence that our model is more accurate than GARCH(1,1). \u2022 The proposed Global model approach discussed in subsection 5.2 is able to generalize well, i.e. the patterns learnt are not biased to a given sector or stock. One of the limitations of our work is to rely on proxies for the volatility estimation. Although these proxies are handy if only open, high, low and close daily price data is available, having high frequency price data we could estimate the daily volatility using the sum of squared intraday returns to measure the true daily latent volatility.",
  "Although these proxies are handy if only open, high, low and close daily price data is available, having high frequency price data we could estimate the daily volatility using the sum of squared intraday returns to measure the true daily latent volatility. For example, in evaluating the performance for the one-day-ahead GARCH(1,1) Yen/Dollar exchange rate [36] reports R2 values of 0.237 and 0.392 using hourly and \ufb01ve minutes sampled intraday returns, respectively. However, we believe that utilizing intraday data would further improve our model performance. Since our experimental results demonstrate the key aspect of the news rel- evance attention to model architecture we observe that intraday data would arguably ameliorate the learning process. Having intraday data would allow us to pair each individual news release with the instantaneous market price reac- tion. Using daily data we are losing part of this information by only measuring the aggregate e\ufb00ect of all news to the one-day-ahead prediction. 7. Conclusion We study the joint e\ufb00ect of stock news and prices on the daily volatility forecasting problem.",
  "Using daily data we are losing part of this information by only measuring the aggregate e\ufb00ect of all news to the one-day-ahead prediction. 7. Conclusion We study the joint e\ufb00ect of stock news and prices on the daily volatility forecasting problem. To the best of our knowledge, this work is one of the \ufb01rst studies aiming to predict short-term (daily) rather than long-term (quarterly 28",
  "or yearly) volatility taking news and price as explanatory variables and using a comprehensive dataset of news headlines at the individual stock level. Our hierarchical end-to-end model bene\ufb01ts from state-of-the-art approaches to encode text information and to deal with two main challenges in correlating news with market reaction: news relevance and novelty. That is, to address the problem of how to attend the most important news based purely on its content (news relevance attention) and to take into account the temporal information of past news (temporal context). Additionally, we propose a multi-stock mini- batch + stock embedding method suitable to model commonality among stocks. The experimental results show that our multimodal approach outperforms the GARCH(1,1) volatility model, which is the most prevalent econometric model for daily volatility predictions. The outperformance being sector-wise and demonstrates the e\ufb00ectiveness of combining price and news for short-term volatility forecasting. The fact that we outperform GARCH(1,1) for all analyzed sectors con\ufb01rms the robustness of our proposed architecture and evidences that our global model approach generalizes well. We ablated (i.e.",
  "The fact that we outperform GARCH(1,1) for all analyzed sectors con\ufb01rms the robustness of our proposed architecture and evidences that our global model approach generalizes well. We ablated (i.e. removed) di\ufb00erent components of our neural architecture to assess its most relevant parts. To this aim, we replaced our proposed news relevance attention layer, which aims to attend the most important news on a given day, with a simpler architecture proposed in the literature, which averages the daily news. We found that our attention layer improves the results. Ad- ditionally, we ablated all the architecture related to the news mode and found that news enhances the forecasting accuracy. Finally, we evaluated di\ufb00erent sentence encoders, including those transfered from other NLP tasks, and concluded that they achieve better performance as compared to a plain Word-level attention sentence encoder trained end-to-end. However, they do not beat state-of-the-art sentence encoders trained end-to-end. In order to contribute to the literature of Universal Sentence Encoders, we evaluated the performance of transferring sentence encoders from two di\ufb00er- ent tasks to the volatility prediction problem.",
  "However, they do not beat state-of-the-art sentence encoders trained end-to-end. In order to contribute to the literature of Universal Sentence Encoders, we evaluated the performance of transferring sentence encoders from two di\ufb00er- ent tasks to the volatility prediction problem. We showed that models trained on the Natural Language Inference (NLI) task are more suitable to forecasting problems than a \ufb01nancial domain dataset (Reuters RCV1). By analyzing dif- ferent architectures, we showed that a BiLSTM with max-pooling for the SNLI dataset provides the best sentence encoder. In the future, we plan to make use of intraday prices to better assess the predictive power of our proposed models. Additionally, we would further extend our analysis to other stock market sectors. References [1] F. Z. Xing, E. Cambria, R. E. Welsch, Natural language based \ufb01nancial forecasting: a survey, Arti\ufb01cial Intelligence Review 50 (1) (2018) 49\u201373. doi:10.1007/s10462-017-9588-9.",
  "doi:10.1007/s10462-017-9588-9. URL http://link.springer.com/10.1007/s10462-017-9588-9 29",
  "[2] P. Milgrom, N. Stokey, Information, trade and common knowledge, Journal of Economic Theory. URL http://www.sciencedirect.com/science/article/pii/ 0022053182900461 [3] M. Harris, A. Raviv, Di\ufb00erences of Opinion Make a Horse Race, Review of Financial Studies 6 (3) (1993) 473\u2013506. doi:10.1093/rfs/5.3.473. URL http://rfs.oxfordjournals.org/content/6/3/473.abstract [4] W. Antweiler, M. Z. Frank, Is All That Talk Just Noise? The Information Content of Internet Stock Message Boards, The Journal of Finance 59 (3) (2004) 1259\u20131294. URL http://www.jstor.org/stable/info/3694736 [5] T. O. Sprenger, P. G. Sandner, A. Tumasjan, I. M. Welpe, News or Noise?",
  "URL http://www.jstor.org/stable/info/3694736 [5] T. O. Sprenger, P. G. Sandner, A. Tumasjan, I. M. Welpe, News or Noise? Using Twitter to Identify and Understand Company-speci\ufb01c News Flow, Journal of Business Finance & Accounting 41 (7-8) (2014) 791\u2013830. doi: 10.1111/jbfa.12086. URL http://doi.wiley.com/10.1111/jbfa.12086 [6] D. Vayanos, P. Woolley, An Institutional Theory of Momentum and Rever- sal, Review of Financial Studies 26 (5) (2013) 1087\u20131145. doi:10.1093/ rfs/hht014.",
  "doi:10.1093/ rfs/hht014. URL https://academic.oup.com/rfs/article-lookup/doi/10.1093/ rfs/hht014 [7] H. Hong, J. C. Stein, A Uni\ufb01ed Theory of Underreaction, Momentum Trad- ing, and Overreaction in Asset Markets, The Journal of Finance 54 (6) (1999) 2143\u20132184. doi:10.1111/0022-1082.00184. URL http://doi.wiley.com/10.1111/0022-1082.00184 [8] W. S. Chan, Stock price reaction to news and no-news: drift and reversal after headlines, Journal of Financial Economics 70 (2) (2003) 223\u2013260. doi:10.1016/S0304-405X(03)00146-6. URL http://www.sciencedirect.com/science/article/pii/ S0304405X03001466 [9] J. Boudoukh, R. Feldman, S. Kogan, M. Richardson, Which News Moves Stock Prices?",
  "URL http://www.sciencedirect.com/science/article/pii/ S0304405X03001466 [9] J. Boudoukh, R. Feldman, S. Kogan, M. Richardson, Which News Moves Stock Prices? A Textual Analysis, NBER Working Paper. URL http://www.nber.org/papers/w18725 [10] C. Antoniou, J. A. Doukas, A. Subrahmanyam, Cognitive Dissonance, Sentiment, and Momentum, Journal of Financial and Quantitative Analy- sis 48 (01) (2013) 245\u2013275. doi:10.1017/S0022109012000592. URL http://www.journals.cambridge.org/ abstract{_}S0022109012000592 [11] Consumer Con\ufb01dence Survey \u2013 technical note, Tech. rep. (2011). URL https://www.conference-board.org/pdf{_}free/press/ TechnicalPDF{_}4134{_}1298367128.pdf 30",
  "[12] P. C. Tetlock, Giving Content to Investor Sentiment: The Role of Media in the Stock Market, The Journal of Finance 62 (3) (2007) 1139\u20131168. doi:10.1111/j.1540-6261.2007.01232.x. URL http://doi.wiley.com/10.1111/j.1540-6261.2007.01232.x [13] S. Kogan, D. Levin, B. R. Routledge, J. S. Sagi, N. A. Smith, Predicting Risk from Financial Reports with Regression, in: Annual Conference of the North American Chapter of the Association for Computational Linguistics, 2009, pp. 272\u2013280. URL http://www.aclweb.org/anthology/N09-1031 [14] C.-J. Wang, M.-F. Tsai, T. Liu, C.-T. Chang, Financial Sentiment Anal- ysis for Risk Prediction, in: International Joint Conference on Natural Language Processing, 2013, pp. 802\u2013808.",
  "Wang, M.-F. Tsai, T. Liu, C.-T. Chang, Financial Sentiment Anal- ysis for Risk Prediction, in: International Joint Conference on Natural Language Processing, 2013, pp. 802\u2013808. URL http://www.aclweb.org/anthology/I13-1097 [15] M.-F. Tsai, C.-J. Wang, Financial Keyword Expansion via Continuous Word Vector Representations, in: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics, Stroudsburg, PA, USA, 2014, pp. 1453\u2013 1458. doi:10.3115/v1/D14-1152. URL http://aclweb.org/anthology/D14-1152 [16] C. Nopp, T. Wien, A. Hanbury, Detecting Risks in the Banking System by Sentiment Analysis, in: Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,, 2015, pp. 591\u2013600.",
  "591\u2013600. URL http://www.emnlp2015.org/proceedings/EMNLP/pdf/EMNLP071. pdf [17] N. Rekabsaz, M. Lupu, A. Baklanov, A. Hanbury, A. Ur, L. Ander- son, T. Wien, Volatility Prediction using Financial Disclosures Senti- ments with Word Embedding-based IR Models, in: 55th Annual Meet- ing of the Association for Computational Linguistics, 2017, pp. 1712\u20131721. doi:10.18653/v1/P17-1157. URL https://doi.org/10.18653/v1/P17-1157 [18] A. Conneau, D. Kiela, H. Schwenk, L. Barrault, A. Bordes, Supervised Learning of Universal Sentence Representations from Natural Language Inference DataarXiv:1705.02364, doi:10.1.1.156.2685.",
  "URL http://arxiv.org/abs/1705.02364 [19] L. Mou, Z. Meng, R. Yan, G. Li, Y. Xu, L. Zhang, Z. Jin, How Transferable are Neural Networks in NLP Applications?arXiv:1603.06111. URL http://arxiv.org/abs/1603.06111 [20] J. Howard, S. Ruder, Universal Language Model Fine-tuning for Text Clas- si\ufb01cationarXiv:1801.06146. URL http://arxiv.org/abs/1801.06146 31",
  "[21] T. Loughran, B. Mcdonald, When is a Liability not a Liability? Textual Analysis , Dictionaries , and 10-Ks, The Journal of Finance 66 (1) (2011) 35\u201365. URL http://bit.ly/15GhT7K [22] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, J. Dean, Distributed Rep- resentations of Words and Phrases and their Compositionality, in: C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, K. Q. Weinberger (Eds.), Advances in Neural Information Processing Systems 26, Curran Associates, Inc., 2013, pp. 3111\u20133119. URL https://dl.acm.org/citation.cfm?id=2999959 [23] T. Baltru\u02c7saitis, C. Ahuja, L.-P. Morency, Multimodal Machine Learning: A Survey and TaxonomyarXiv:1705.09406.",
  "URL https://dl.acm.org/citation.cfm?id=2999959 [23] T. Baltru\u02c7saitis, C. Ahuja, L.-P. Morency, Multimodal Machine Learning: A Survey and TaxonomyarXiv:1705.09406. URL http://arxiv.org/abs/1705.09406 [24] J. Bollen, H. Mao, X.-J. Zeng, Twitter Mood Predicts the Stock Market, Journal of Computational Science 2 (1) (2011) 1\u20138. arXiv:arXiv:1010.3003v1. URL http://www.sciencedirect.com/science/article/pii/ S187775031100007X [25] R. P. Schumaker, H. Chen, Textual Analysis of Stock Market Prediction Using Breaking Financial News: The AZFin Text System, ACM Trans. Inf. Syst. 27 (2) (2009) 12:1\u2014-12:19. doi:10.1145/1462198.1462204.",
  "Inf. Syst. 27 (2) (2009) 12:1\u2014-12:19. doi:10.1145/1462198.1462204. URL http://doi.acm.org/10.1145/1462198.1462204 [26] T. H. Nguyen, K. Shirai, Topic Modeling based Sentiment Analysis on Social Media for Stock Market Prediction, in: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics, 2015, pp. 1354\u20131364. URL http://www.aclweb.org/anthology/P15-1131 [27] X. Ding, Y. Zhang, T. Liu, J. Duan, Deep learning for event-driven stock prediction, in: Proceedings of the 24th International Joint Conference on Arti\ufb01cial Intelligence (ICJAI 15), 2015, pp. 2327\u20132333.",
  "2327\u20132333. URL https://www.ijcai.org/Proceedings/15/Papers/329.pdf [28] L. d. S. Pinheiro, M. Dras, Stock Market Prediction with Deep Learning: A Character-based Neural Language Model for Event-based Trading, in: Proceedings of the Australasian Language Technology Association Workshop 2017, 2017, pp. 6\u201315. URL https://aclanthology.coli.uni-saarland.de/papers/ U17-1001/u17-1001 [29] J. Deng, W. Dong, R. Socher, L.-J. Li, Kai Li, Li Fei-Fei, ImageNet: A large-scale hierarchical image database, in: 2009 IEEE Conference on Computer Vision and Pattern Recognition, IEEE, 2009, pp. 248\u2013255. 32",
  "doi:10.1109/CVPR.2009.5206848. URL http://ieeexplore.ieee.org/document/5206848/ [30] A. S. Razavian, H. Azizpour, J. Sullivan, S. Carlsson, CNN Features O\ufb00-the-Shelf: An Astounding Baseline for Recognition, in: 2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops, IEEE, 2014, pp. 512\u2013519. doi:10.1109/CVPRW.2014.131. URL http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm? arnumber=6910029 [31] S. R. Bowman, G. Angeli, C. Potts, C. D. Manning, A large annotated corpus for learning natural language inference, in: Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Asso- ciation for Computational Linguistics, Stroudsburg, PA, USA, 2015, pp. 632\u2013642. doi:10.18653/v1/D15-1075.",
  "632\u2013642. doi:10.18653/v1/D15-1075. URL http://aclweb.org/anthology/D15-1075 [32] D. D. Lewis, Y. Yang, T. G. Rose, F. Li, RCV1: A New Benchmark Col- lection for Text Categorization Research, The Journal of Machine Learning Research 5 (2004) 361\u2013397. URL http://dl.acm.org/citation.cfm?id=1005332.1005345 [33] R. F. Engle, Autoregressive Conditional Heteroscedasticity with Estimates of the Variance of United Kingdom In\ufb02ation, Econometrica 50 (4) (1982) 987. doi:10.2307/1912773. URL https://www.jstor.org/stable/1912773?origin=crossref [34] T. Bollerslev, Generalized autoregressive conditional heteroskedas- ticity, Journal of Econometrics 31 (3) (1986) 307\u2013327.",
  "URL https://www.jstor.org/stable/1912773?origin=crossref [34] T. Bollerslev, Generalized autoregressive conditional heteroskedas- ticity, Journal of Econometrics 31 (3) (1986) 307\u2013327. doi: 10.1016/0304-4076(86)90063-1. URL https://www.sciencedirect.com/science/article/pii/ 0304407686900631 [35] P. R. Hansen, A. Lunde, A forecast comparison of volatility models: does anything beat a GARCH(1,1)?, Journal of Applied Econometrics 20 (7) (2005) 873\u2013889. doi:10.1002/jae.800. URL http://doi.wiley.com/10.1002/jae.800 [36] T. G. Andersen, T. Bollerslev, Answering the Skeptics: Yes, Standard Volatility Models do Provide Accurate Forecasts, International Economic Review 39 (4) (1998) 885. doi:10.2307/2527343.",
  "URL https://www.jstor.org/stable/2527343?origin=crossref [37] P. Molnar, Properties of range-based volatility estimators, Inter- national Review of Financial Analysis 23 (2012) 20\u201329. doi: 10.1016/J.IRFA.2011.06.012. URL https://www.sciencedirect.com/science/article/pii/ S1057521911000731 33",
  "[38] J. Pennington, R. Socher, C. D. Manning, GloVe: Global Vectors for Word Representation. URL https://nlp.stanford.edu/pubs/glove.pdf [39] S. Hochreiter, J. Schmidhuber, Long Short-Term Memory, Neural Compu- tation 9 (8) (1997) 1735\u20131780. doi:10.1162/neco.1997.9.8.1735. URL http://www.mitpressjournals.org/doi/10.1162/neco.1997.9. 8.1735 [40] F. A. Gers, J. Schmidhuber, F. Cummins, Learning to Forget: Continual Prediction with LSTM, Neural Computation 12 (10) (2000) 2451\u20132471. doi:10.1162/089976600300015015.",
  "doi:10.1162/089976600300015015. URL http://www.mitpressjournals.org/doi/10.1162/ 089976600300015015 [41] M. Schuster, K. Paliwal, Bidirectional recurrent neural networks, IEEE Transactions on Signal Processing 45 (11) (1997) 2673\u20132681. doi:10.1109/ 78.650093. URL http://ieeexplore.ieee.org/document/650093/ [42] S. Lai, L. Xu, K. Liu, J. Z. AAAI, U. 2015, Recurrent Convolutional Neural Networks for Text Classi\ufb01cation., in: AAAI, 2015, pp. 2267\u20132273. URL http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/ download/9745/9552 [43] P. Li, W. Li, Z. He, X. Wang, Y. Cao, J. Zhou, W. Xu, Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering.",
  "URL https://arxiv.org/pdf/1607.06275.pdf [44] Y. Liu, C. Sun, L. Lin, X. Wang, Learning Natural Language Inference using Bidirectional LSTM model and Inner-AttentionarXiv:arXiv:1605. 09090v1. URL https://arxiv.org/pdf/1605.09090.pdf [45] Z. Lin, M. Feng, C. Nogueira, D. Santos, M. Yu, B. Xiang, B. Zhou, Y. Ben- gio, A Structured Self-Attentive Sentence Embedding, in: ICLR, 2017. URL https://arxiv.org/pdf/1703.03130.pdf [46] Z. C. Lipton, D. C. Kale, C. Elkan, R. Wetzel, Learning to Diagnose with LSTM Recurrent Neural Networks, in: ICLR, 2016. arXiv:1511.03677.",
  "URL http://arxiv.org/abs/1511.03677 [47] Z. C. Lipton, D. Kale, R. Wetzel, Directly Modeling Missing Data in Se- quences with RNNs: Improved Classi\ufb01cation of Clinical Time Series, in: Proceedings of the 1st Machine Learning for Healthcare Conferenc, 2016, pp. 253\u2013270. URL http://proceedings.mlr.press/v56/Lipton16.html 34",
  "[48] J. Alberg, Z. C. Lipton, Improving Factor-Based Quantitative Investing by Forecasting Company Fundamentals, in: 31st Conference on Neural Information Processing Systems (NIPS), 2017. arXiv:1711.04837. URL http://arxiv.org/abs/1711.04837 [49] D. P. Kingma, J. Lei Ba, Adam: A method for stochastic optimization, in: ICLR, 2015. arXiv:arXiv:1412.6980v9. URL https://arxiv.org/pdf/1412.6980.pdf [50] R. Johnson, T. Zhang, E\ufb00ective Use of Word Order for Text Categorization with Convolutional Neural Networks, in: NAACL, 2015, pp. 103\u2013112. URL http://www.anthology.aclweb.org/N/N15/N15-1011.pdf 35",
  "Sector ETF Constituent Stocks Consumer Staples (XLP) Procter & Gamble (PG), Coca-Cola Company (KO), PepsiCo (PEP), Walmart (WMT), Costco Wholesale Corporation (COST), CVS Health Corporation (CVS), Altria Group (MO), Wal- greens Boots Alliance (WBA), Mondelez Inter- national (MDLZ), Colgate-Palmolive (CL), Energy (XLE) Exxon-Mobil (XOM), Chevron (CVX), Cono- coPhillips (COP), EOG Resources (EOG), Oc- cidental Petroleum Corporation (OXY), Valero Energy Corporation (VLO), Halliburton Com- pany (HAL), Schlumberger Limited (SLB), Pioneer Natural Resources (PXD), Anadarko Petroleum Corporation (APC) Utilities (XLU) NextEra Energy (NEE), Duke Energy (DUK), The Southern Company (SO), Dominion En- ergy (D), Exelon Corporation (EXC), American Electric Power Company (AEP), Sempra Energy (SRE), Public Service Enterprise Group (PEG), Consolidated Edison (ED),",
  "Duke Energy (DUK), The Southern Company (SO), Dominion En- ergy (D), Exelon Corporation (EXC), American Electric Power Company (AEP), Sempra Energy (SRE), Public Service Enterprise Group (PEG), Consolidated Edison (ED), Xcel Energy (XEL) Healthcare (XLV) Johnson & Johnson (JNJ), UnitedHealth Group (UNH), P\ufb01zer (PFE), Merck & Co. (MRK), Medtronic (MDT), Amgen (AMGN), Abbott Laboratories (ABT), Gilead Sciences (GILD), Eli Lilly (LLY), Bristol-Myers Squibb (BMY) Financials (XLF) Berkshire Hathaway (BRK-A), JPMorgan Chase (JPM), Bank of America Corporation (BAC), Wells Fargo (WFC), CitiBank (C), Goldman Sachs Group (GS), U.S. Bancorp (USB), Morgan Stanley (MS), American Express (AXP), PNC Financial Services Group (PNC) Table 1: Corpus sectors and respective constituent stocks. For each sector we selected the top 10 stock holdings (as in January 2018).",
  "Bancorp (USB), Morgan Stanley (MS), American Express (AXP), PNC Financial Services Group (PNC) Table 1: Corpus sectors and respective constituent stocks. For each sector we selected the top 10 stock holdings (as in January 2018). Stock codes in parentheses. 36",
  "Sector ETF before during after market market market Consumer Staples 54% 31% 15% Energy 44% 36% 20% Utilities 58% 31% 11% Healthcare 55% 28% 17% Financials 63% 24% 13% total 84,556 40,996 21231 Table 2: Distribution of headlines per sector according to market hours. The majority of the 146,783 headlines are released before 9:30AM (before market). The category after market includes news released after 4:00PM EDT. We count the categories holiday and weekend as before market since they impact the following working day.",
  "The majority of the 146,783 headlines are released before 9:30AM (before market). The category after market includes news released after 4:00PM EDT. We count the categories holiday and weekend as before market since they impact the following working day. Date and time Headline 2011-12-13 00:18:39 EDT Valero reports power outage at Port Arthur re\ufb01nery 2007-04-17 08:54:27 EDT Wells Fargo pro\ufb01t rises 11 pct on com- mercial loans 2017-12-14 14:40:31 EDT Perrigo lines up bid for Merck\u2019s con- sumer health unit 2007-01-03 10:27:42 EDT UPDATE 1-Bear Stearns ups Merck to outperform 2010-02-23 13:35:11 EDT Exxon Mobil says remains bullish on Nigeria 2016-09-22 15:32:13 EDT Texas regulators express \u201cdeep concern\u201d over NextEra deal 2008-10-14 08:30:00 EDT Smart For LifeTM Now Available on Costco.com Table 3: Random samples from our dataset.",
  "Note the factual/objective char- acteristic of our corpus, where typical news do not carry any sentiment connotation. 37",
  "Premise Hypothesis Label Children smiling and waving at camera. There are children present. e Two blond women are hug- ging one another. Some women are hugging on vacation. n A farmer fertilizing his gar- den with manure with a horse and wagon. The man is fertilizering his garden. e The furry brown dog is swim- ming in the ocean. A dog is running around the yard. c A dog drops a red disc on a beach. a dog catch the ball on a beach. c Several armed forces o\ufb03- cers and civilians are stand- ing around a children\u2019s play- ground. Civilians and armed forces o\ufb03cers trade insults at a playground. n Table 4: Stanford NLI (SNLI) dataset examples. Natural language sentence pairs are labelled with entailment (e), contradiction (c), or neutral (n).",
  "Civilians and armed forces o\ufb03cers trade insults at a playground. n Table 4: Stanford NLI (SNLI) dataset examples. Natural language sentence pairs are labelled with entailment (e), contradiction (c), or neutral (n). Dataset Sentence Encoder Score SNLI LSTM original paper ([31]) 0.806 BiLSTM over Mean Pooling ([44]) 0.833 BiLSTM attention (Att) with multiple views and factored fusion layer ([45]) 0.844 BiLSTM max-pooling (MP) with sentence embedding size 4096 ([18]) 0.845 Our BiLSTM Att with sentence embedding size 2048 0.838 Our BiLSTM MP with sentence embedding size 2048 0.841 RCV1 k-NN\u2020 ([32]) 0.765 Best Support Vector Machine (SVM)\u2020 ([32]) 0.816 bow-CNN\u2020 ([50]) 0.840 Our BiLSTM Att with sentence embedding size 2048 (headlines only) 0.809 Our BiLSTM MP with sentence embedding size 2048 (headlines only) 0.811 Table 5: TL auxiliary tasks \u2013 Sentence Encoders comparison.",
  "Test scores are accuracy and F1 scores for the SNLI subsubsection 4.2.2 and RCV1 subsubsection 4.2.1 datasets, respectively. \u2020 indicates model trained with both headlines and body content and using the original 103 classes of the RCV1 dataset, rather than our models that are trained using headlines only and a total of 55 classes (see subsubsection 4.2.1 for a complete description). As a consequence, the reported benchmarks for the RCV1 dataset are not directly comparable and where reported for the sake of a better benchmark. 38",
  "Model MSE MAE All stocks - News (price only unimodal)\u2020 2.140E-05 3.093E-03 + News (BiLSTM Att) - news relevance attention (NRA) 2.078E-03 3.037E-03 + News (BiLSTM MP) - NRA 2.077E-03 3.031E-03 + News (TL Reuters RCV1 BiLSTM MP) + NRA 2.037E-03 3.020E-03 + News (TL Reuters RCV1 BiLSTM Att) + NRA 2.023E-03 3.011E-03 + News (W-L Att)\u2020\u2020 + NRA 2.006E-03 2.947E-03 + News (TL SNLI BiLSTM Att) + NRA 1.986E-03 2.926E-03 + News (TL SNLI BiLSTM MP) + NRA 1.974E-03 2.918E-03 + News (BiLSTM MP) + NRA 1.904E-03 2.",
  "986E-03 2.926E-03 + News (TL SNLI BiLSTM MP) + NRA 1.974E-03 2.918E-03 + News (BiLSTM MP) + NRA 1.904E-03 2.851E-03 + News (BiLSTM Att) + NRA 1.898E-03 2.823E-03 Table 6: Model architecture ablations and sentence encoders comparisons. The minus sign means that the component of our network architecture described in subsection 5.3 was ablated (i.e. removed) and the plus sign that it is added. The second and third row report results replacing the news relevance attention (NRA) with a News Averaging component as in [27, 28]. \u2020 indicates our model was trained using only the price mode. \u2020\u2020 highlights that the sentence encoder Word-Level Attention (W-L Attention) does not take into consideration the headline words order. Best result in bold.",
  "\u2020 indicates our model was trained using only the price mode. \u2020\u2020 highlights that the sentence encoder Word-Level Attention (W-L Attention) does not take into consideration the headline words order. Best result in bold. Model Vol R2 MSE MAE Estimator All Stocks GARCH(1,1) d \u03c3GK 0.357 2.46E-05 3.16E-03 d \u03c3P K 0.329 2.57E-05 3.20E-03 Our Model: Price (Unimodal) d \u03c3GK 0.384 2.14E-05 3.09E-03 d \u03c3P K 0.350 2.36E-05 3.29E-03 Our Model: Price + News d \u03c3GK 0.455 1.90E-05 2.82E-03 d \u03c3P K 0.410 2.09E-05 2.98E-03 Table 7: Our volatility model performance compared with GARCH(1,1). Best performance in bold.",
  "Best performance in bold. Our model has superior performance across the three evaluation metrics and taking into consideration the state-of-the-art volatility proxies, namely Garman-Klass ( [ \u03c3P K) and Parkinson ( [ \u03c3P K). 39",
  "Model Vol R2 MSE MAE Estimator Consumer Staples GARCH(1,1) d \u03c3GK 0.173 2.01E-05 2.63E-03 d \u03c3P K 0.155 2.08E-05 2.70E-03 Our Model: Price (Unimodal) d \u03c3GK 0.194 1.93E-05 2.67E-03 d \u03c3P K 0.176 2.04E-05 2.82E-03 Our Model: Price + News d \u03c3GK 0.224 1.80E-05 2.48E-03 d \u03c3P K 0.201 1.90E-05 2.61E-03 HealthCare GARCH(1,1) d \u03c3GK 0.150 2.20E-05 3.05E-03 d \u03c3P K 0.138 2.33E-05 3.09E-03 Our Model: Price (Unimodal) d \u03c3GK 0.186 2.01E-05 3.01E-03 d \u03c3P K 0.",
  "05E-03 d \u03c3P K 0.138 2.33E-05 3.09E-03 Our Model: Price (Unimodal) d \u03c3GK 0.186 2.01E-05 3.01E-03 d \u03c3P K 0.164 2.24E-05 3.21E-03 Our Model: Price + News d \u03c3GK 0.258 1.76E-05 2.74E-03 d \u03c3P K 0.225 1.96E-05 2.90E-03 Financials GARCH(1,1) d \u03c3GK 0.274 2.02E-05 3.14E-03 d \u03c3P K 0.250 2.17E-05 3.18E-03 Our Model: Price (Unimodal) d \u03c3GK 0.326 1.77E-05 3.10E-03 d \u03c3P K 0.290 2.03E-05 3.32E-03 Our Model: Price + News d \u03c3GK 0.373 1.",
  "326 1.77E-05 3.10E-03 d \u03c3P K 0.290 2.03E-05 3.32E-03 Our Model: Price + News d \u03c3GK 0.373 1.65E-05 2.84E-03 d \u03c3P K 0.332 1.86E-05 3.00E-03 Energy GARCH(1,1) d \u03c3GK 0.443 4.38E-05 4.24E-03 d \u03c3P K 0.412 4.52E-05 4.27E-03 Our Model: Price (Unimodal) d \u03c3GK 0.440 3.60E-05 4.13E-03 d \u03c3P K 0.406 3.98E-05 4.34E-03 Our Model: Price + News d \u03c3GK 0.538 3.04E-05 3.72E-03 d \u03c3P K 0.495 3.38E-05 3.88E-03 Utilities GARCH(1,",
  "34E-03 Our Model: Price + News d \u03c3GK 0.538 3.04E-05 3.72E-03 d \u03c3P K 0.495 3.38E-05 3.88E-03 Utilities GARCH(1,1) d \u03c3GK 0.167 1.71E-05 2.75E-03 d \u03c3P K 0.154 1.75E-05 2.77E-03 Our Model: Price (Unimodal) d \u03c3GK 0.145 1.40E-05 2.56E-03 d \u03c3P K 0.128 1.51E-05 2.75E-03 Our Model: Price + News d \u03c3GK 0.225 1.24E-05 2.34E-03 d \u03c3P K 0.193 1.34E-05 2.51E-03 Table 8: Sector-level performance comparison. 40"
]