{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "c\u20dd2020 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting\/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. SMALL-FOOTPRINT KEYWORD SPOTTING ON RAW AUDIO DATA WITH SINC-CONVOLUTIONS Simon Mittermaier\u22c6\u2020 Ludwig K\u00a8urzinger\u22c6 Bernd Waschneck\u2020 Gerhard Rigoll\u22c6 \u22c6Technische Universit\u00a8at M\u00a8unchen, Munich, Germany \u2020 In\ufb01neon Technologies AG, Munich & Dresden, Germany ABSTRACT Keyword Spotting (KWS) enables speech-based user interac- tion on smart devices. Always-on and battery-powered ap- plication scenarios for smart devices put constraints on hard- ware resources and power consumption, while also demand- ing high accuracy as well as real-time capability. Previous architectures \ufb01rst extracted acoustic features and then applied a neural network to classify keyword probabilities, optimizing towards memory footprint and execution time.",
            "Previous architectures \ufb01rst extracted acoustic features and then applied a neural network to classify keyword probabilities, optimizing towards memory footprint and execution time. Compared to previous publications, we took additional steps to reduce power and memory consumption without reducing classi\ufb01cation accuracy. Power-consuming audio preprocessing and data transfer steps are eliminated by di- rectly classifying from raw audio. For this, our end-to-end architecture extracts spectral features using parametrized Sinc-convolutions. Its memory footprint is further reduced by grouping depthwise separable convolutions. Our network achieves the competitive accuracy of 96.4% on Google\u2019s Speech Commands test set with only 62k parameters. Index Terms\u2014 Keyword Spotting, Wake-Word Detec- tion, Speech Commands, Sinc-Convolutions, Raw Audio 1. INTRODUCTION Speech processing enables natural communication with smart phones or smart home assistants, e.g., Amazon Echo, Google Home. However, continuously performing speech recog- nition is not energy-ef\ufb01cient and would drain batteries of smart devices.",
            "INTRODUCTION Speech processing enables natural communication with smart phones or smart home assistants, e.g., Amazon Echo, Google Home. However, continuously performing speech recog- nition is not energy-ef\ufb01cient and would drain batteries of smart devices. Instead, most speech recognition systems pas- sively listen for utterances of certain wake words such as \u201cOk Google\u201d, \u201cHey Siri\u201d, \u201cAlexa\u201d, etc. to trigger the continuous speech recognition system on demand. This task is referred to as keyword spotting (KWS) or wake-word detection. There are also uses of KWS where a view simple speech commands (e.g. \u201con\u201d, \u201coff\u201d) are enough to interact with a smart device. Conventional hybrid approaches to KWS \ufb01rst divide their audio signal in time frames to extract features, e.g., Mel Fre- quency Cepstral Coef\ufb01cients (MFCC). A neural net then es- timates phoneme or state posteriors of the keyword Hidden Markov Model in order to calculate the keyword probability using a Viterbi search. The wake-word is then recognized when the keyword probability reaches a prede\ufb01ned threshold.",
            "A neural net then es- timates phoneme or state posteriors of the keyword Hidden Markov Model in order to calculate the keyword probability using a Viterbi search. The wake-word is then recognized when the keyword probability reaches a prede\ufb01ned threshold. In recent years, end-to-end architectures gained traction that directly classify keyword posterior probabilities based on the previously extracted features, e.g., [1, 2, 3, 4, 5]. Typical application scenarios imply that the device is powered by a battery, and possesses restricted hardware re- sources to reduce costs. Therefore previous works optimized towards memory footprint and operations per second. In contrast to this, we tune our neural network towards energy conservation in microcontrollers motivated by obervations on power consumption, as detailed in Sec. 3.1. To extract meaningful and representative features from raw audio, our architecture uses parametrized Sinc-convolutions (SincConv) from SincNet [6]. We use Depthwise Separable Convolu- tions (DSConv) [7, 8] that preserve time-context information while at the same time compare features in different chan- nels.",
            "We use Depthwise Separable Convolu- tions (DSConv) [7, 8] that preserve time-context information while at the same time compare features in different chan- nels. To further reduce the number of network parameters, which is key for energy ef\ufb01ciency, we group DSConv-layers, a technique we refer to as Grouped DSConv (GDSConv). Our key contributions are: \u2022 We propose a neural network architecture tuned to- wards energy ef\ufb01ciency in microcontrollers grounded on the observation that memory access is costly, while computation is cheap [9]. \u2022 Our keyword-spotting network classi\ufb01es on raw audio employing SincConvs while at the same time reducing the number of parameters using (G)DSConvs. \u2022 Our base model with 122k parameters performs with the state-of-the-art accuracy of 96.6% on the test set of Googles Speech Commands dataset, on par with TC- ResNet [4] that has 305k parameters and requires sepa- rate preprocessing. Our low-parameter model achieves 96.4% with only 62k parameters. 2.",
            "Our low-parameter model achieves 96.4% with only 62k parameters. 2. RELATED WORK Recently, CNNs have been successfully applied to KWS [2, 3, 4]. Zhang et al. evaluated different neural network architec- tures (such as CNNs, LSTMs, GRUs) in terms of accuracy, computational operations and memory footprint as well as arXiv:1911.02086v2  [eess.AS]  3 May 2020",
            "their deployment on embedded hardware [2]. They achieved their best results using a CNN with DSConvs. Tang et al. ex- plored the use of Deep Residual Networks with dilated convo- lutions to achieve a high accuracy of 95.8% [3], while keeping the number of parameters comparable to [2]. Choi et al. build on this work as they also use a ResNet-inspired architecture. Instead of using 2D convolution over a time-frequency repre- sentation of the data they convolve along the time dimension and treat the frequency dimension as channels [4]. This bears similarities with our approach as we are using 1D convolution along the time dimension as well. However, all the approaches mentioned classify from MFCCs or sim- ilar preprocessed features. Our architecture works directly on raw audio signals. Previous work has been done with using neural networks for directly modeling raw audio for wake word detection in a hybrid approach [10]. There is, however, a recent trend in the speech domain towards using CNNs on raw audio data directly in an end-to-end approach [6, 11, 12, 13].",
            "There is, however, a recent trend in the speech domain towards using CNNs on raw audio data directly in an end-to-end approach [6, 11, 12, 13]. Ravanelli et al. present an effective method of processing raw audio with CNNs, called SincNet. Ker- nels of the \ufb01rst convolutional layer are restricted to only learn shapes of parametrized sinc functions. This method was \ufb01rst introduced for Speaker Recognition [6] and later also used for Phoneme Recognition [11]. To the best of our knowledge, we are the \ufb01rst to apply this method to the task of KWS. The \ufb01rst convolutional layer of our model is inspired by SincNet and we combine it with DSConv that has \ufb01rst been introduced in the domain of Image Processing [8, 14] and applied to KWS in [2] and neural machine translation [7]. Kaiser et al. [7] also introduce the \u201csuper-separable\u201d convo- lution, a DSConv that further reduces the number of parame- ters using grouping.",
            "Kaiser et al. [7] also introduce the \u201csuper-separable\u201d convo- lution, a DSConv that further reduces the number of parame- ters using grouping. The idea of Grouped Convolutions was \ufb01rst used in AlexNet [15] to reduce parameters and operations and to enable distributed computing of the model over multi- ple GPUs. We denominate the combination of grouping and DSonv as GDSConv in our work and use it for our smallest model. 3. MODEL 3.1. Keyword-Spotting on Battery-Powered Devices Typical application scenarios for smart devices imply that the device is powered by a battery, and possesses restricted hard- ware resources. The requirements for a KWS system in these scenarios are (1) very low power consumption to maximize battery life, (2) real-time or near real-time capability, (3) low memory footprint and (4) high accuracy to avoid random ac- tivations and to ensure responsiveness. Regarding real-time capability, our model is designed to operate on a single-core microcontroller capable of 50 MOps per second [2].",
            "Regarding real-time capability, our model is designed to operate on a single-core microcontroller capable of 50 MOps per second [2]. We assume that in microcontrollers the memory consumption of a KWS neural network is asso- ciated with its power consumption: Reading memory values contributes most to power consumption which makes re-use of weights favorable. While in general large memory mod- ules leak more power than small memory modules, one read operation from RAM costs far more energy than the corre- sponding multiply-and-accumulate computation [16, 9]. In addition to the parameter-reducing approach in this work, further steps may be employed to reduce power consump- tion such as quantization, model compression or optimization strategies regarding data\ufb02ows that depend on the utilized hardware platform [16, 9, 17, 18]. 3.2. Feature Extraction using SincConvs SincNet [6] classi\ufb01es on raw audio by restricting the \ufb01l- ters of the \ufb01rst convolutional layer of a CNN to only learn parametrized sinc functions, i.e., sinc (x) = sin(x)\/x.",
            "Feature Extraction using SincConvs SincNet [6] classi\ufb01es on raw audio by restricting the \ufb01l- ters of the \ufb01rst convolutional layer of a CNN to only learn parametrized sinc functions, i.e., sinc (x) = sin(x)\/x. One sinc function in the time domain represents a rectangular function in the spectral domain, therefore two sinc functions can be combined to an ideal band-pass \ufb01lter: g[n, f1, f2] = 2f2 sinc (2\u03c0f2n) \u22122f1 sinc (2\u03c0f1n) (1) G[f, f1, f2] = rect ( f 2f2 ) \u2212rect ( f 2f1 ) (2) Performing convolution with such a \ufb01lter extracts the parts of the input signal that lie within a certain frequency range. SincNet combines Sinc-convolutions with CNNs; as we only use the feature extraction layer of this architecture, we label this layer as SincConv to establish a distinction to SincNet.",
            "SincNet combines Sinc-convolutions with CNNs; as we only use the feature extraction layer of this architecture, we label this layer as SincConv to establish a distinction to SincNet. Compared to one \ufb01lter of a regular CNN, were the num- ber of parameters is derived from its kernel width, e.g., k = 100 [6], Sinc-convolutions only require two parameters to derive each \ufb01lter, the lower and upper cut-off frequencies (f1, f2), resulting in a small memory footprint. SincConv \ufb01lters are initialized with the cutoff frequencies of the mel- scale \ufb01lter bank and then further adjusted during training. Fig. 1 visualizes this adjustment from initialization to after training. SincConv \ufb01lter banks can be easily interpreted, as the two learned parameter correspond to a speci\ufb01c frequency band. Fig. 2 visualizes how a SincConv layer with 7 \ufb01lters processes an audio sample containing the word \u201cyes\u201d. 3.3.",
            "Fig. 2 visualizes how a SincConv layer with 7 \ufb01lters processes an audio sample containing the word \u201cyes\u201d. 3.3. Low-Parameter GDSConv Layers DSConv have been successfully applied to the domain of computer vision [8, 14], neural translation [7] and KWS [2]. Fig. 3 provides an overview of the steps from a regular convo- lution to the GDSConv. A mathematical description is given in [7]. The number of parameters of one DSConv layer amounts to NDSConv = k \u00b7 cin + cin \u00b7 cout with the kernel size k and the number of input and output channels cin and cout re- spectively; the \ufb01rst summand is determined by the depthwise convolution, the second summand by the pointwise convolu- tion [7]. In our model con\ufb01guration, the depthwise convolu- tion only accounts for roughly 5% of parameters in this layer,",
            "0 2000 4000 6000 8000 Frequency 0 1 Amplitude (a) mel-scale filter bank 0 2000 4000 6000 8000 Frequency 0 1 Amplitude (b) learned filter bank Fig. 1. SincConv \ufb01lter bank with 7 \ufb01lters, (a) after initializa- tion with mel-scale weights and (b) after training on [19]. 0 0.5 1 Time -1 0 1 Amplitude 0 0.5 1 Time 1 2 3 4 5 6 7 Channels Fig. 2. An audio sample of the keyword \u201cyes\u201d is convolved with the 7-\ufb01lter SincConv layer from Fig. 1 to extract mean- ingful features. the pointwise for 95%. We therefore reduced the parameters of the pointwise convolution using grouping by a factor g to NGDSConv = k \u00b7 cin + cin\u00b7cout g , rather than the parameters in the depthwise convolution.",
            "1 to extract mean- ingful features. the pointwise for 95%. We therefore reduced the parameters of the pointwise convolution using grouping by a factor g to NGDSConv = k \u00b7 cin + cin\u00b7cout g , rather than the parameters in the depthwise convolution. To allow information exchange between groups we alternate the number of groups per layer, namely 2 and 3, as proposed in [7]. 3.4. Two Low-Parameter Architectures The SincConv as the \ufb01rst layer extracts features from the raw input samples, as shown in Fig. 4. As non-linearity after the SincConv we opt to use log-compression, i.e., y = log(abs(x) + 1), instead of a common activation func- tion (e.g., ReLU). This has also shown to be effective in other CNN architectures for raw audio processing [12, 13]. Five (G)DSConv layers are then used to process the features further: The \ufb01rst layer has a larger kernel size and scales the number of channels to 160. The other four layers have each 160 input and output channels.",
            "Five (G)DSConv layers are then used to process the features further: The \ufb01rst layer has a larger kernel size and scales the number of channels to 160. The other four layers have each 160 input and output channels. Each (G)DSConv block contains the (G)DSConv layer, batch normalization [20] and c 1 k 1 k 1 1 1 c c\/g 1 k 1 (a)  Regular Conv (b)  DSConv (c)  GDSConv 1 1 Fig. 3. Steps from a regular convolution to the grouped depth- wise separable convolution. (a) Regular 1D convolutional layers perform convolution along the time axis, across all channels. (b) The DSConv convolves all channels separately along the time dimension (depthwise), and then adds a 1x1 convolution (i.e., a pointwise convolution) to combine infor- mation across channels. (c) The GDSConv is performed by partitioning the channels into g groups and then applying a DSConv on each group. Our base model employs DSConv layers, and our low-parameter model GDSConv layers.",
            "(c) The GDSConv is performed by partitioning the channels into g groups and then applying a DSConv on each group. Our base model employs DSConv layers, and our low-parameter model GDSConv layers. spatial dropout [21] for regularization, as well as average pooling to reduce temporal resolution. After the (G)DSConv blocks, we use global average pooling to receive a 160- element vector that can be transformed to class posteriors using a Softmax layer to classify 12 classes, i.e., 10 keywords as well as a class for unknown and for silence. The low-parameter model is obtained by grouping the DSConv layers with an alternating number of groups be- tween 2 and 3. For the con\ufb01guration shown in Fig. 4, the base model has 122k parameters. After grouping, the number of parameters is reduced to a total of 62k. Hyperparameters have been selected through extensive experimentation based on the best validation accuracy for a given number of parameters comparable to models in [2, 4]. 4. EVALUATION 4.1.",
            "After grouping, the number of parameters is reduced to a total of 62k. Hyperparameters have been selected through extensive experimentation based on the best validation accuracy for a given number of parameters comparable to models in [2, 4]. 4. EVALUATION 4.1. Training on the Speech Commands Dataset We train and evaluate our model using Google\u2019s Speech Com- mands data set [19], an established dataset for benchmarking KWS systems. The \ufb01rst version of the data set consists of 65k one-second long utterances of 30 different keywords spoken by 1881 different speakers. The most common setup consists of a classi\ufb01cation of 12 classes: \u201cyes\u201d, \u201cno\u201d, \u201cup\u201d, \u201cdown\u201d, \u201cleft\u201d, \u201cright\u201d, \u201con\u201d, \u201coff\u201d, \u201cstop\u201d, \u201cgo\u201d, unknown, or silence. The remaining 20 keywords are labeled as unknown, samples of provided background noise \ufb01les as silence. To ensure the benchmark reproducibility, a separate test set was released with a prede\ufb01ned list of samples for the unknown and the si- lence class.",
            "The remaining 20 keywords are labeled as unknown, samples of provided background noise \ufb01les as silence. To ensure the benchmark reproducibility, a separate test set was released with a prede\ufb01ned list of samples for the unknown and the si- lence class. The second version of the dataset contains 105k samples and \ufb01ve additional keywords [19]. However, previ- ous publications on KWS reported only results on the \ufb01rst version, therefore we focused on the \ufb01rst version and addi- tionally report testing results on version 2 of the dataset. Every sample from the training set is used in training, this",
            "SincConvBlock(40,101,8) DSConvBlock(160,25,2) (G)DSConvBlock(160,9,1) (G)DSConvBlock(160,9,1) (G)DSConvBlock(160,9,1) (G)DSConvBlock(160,9,1) GlobalAveragePooling Softmax(12) Input Output SincConv(c,k,s) log(abs(x) + 1) BatchNorm AvgPooling(2) (G)DSConv(c,k,s) ReLU(x) BatchNorm AvgPooling(2) SpatialDropout(0.1) Fig. 4. The model architecture as described in Sec. 3.4. Pa- rameter con\ufb01gurations of convolutions are given as c, k, s that represent the number of output channels, kernel length and stride, respectively. In our low-parameter model, convolu- tions are grouped from the third to the sixth layer. leads to a class imbalance as there are much more samples for unknown. Class weights in the training phase assign a lower weight to samples labeled as unknown such that the impact on the model is proportional to the other classes.",
            "leads to a class imbalance as there are much more samples for unknown. Class weights in the training phase assign a lower weight to samples labeled as unknown such that the impact on the model is proportional to the other classes. This way, the model can see more unknown word samples during training without getting biased. Our model is trained for 60 epochs with the Adam optimizer [22] with an initial learning rate of 0.001 and learning rate decay of 0.5 after 10 epochs; the model with the highest validation accuracy is saved to evalu- ate accuracy on the test set. 4.2. Results and Discussion The base model composed of DSConv layers without group- ing achieves the state-of-the-art accuracy of 96.6% on the Speech Commands test set. The low-parameter model with GDSConv achieves almost the same accuracy of 96.4% with only about half the parameters. This validates the effective- ness of GDSConv for model size reduction. Table 1 lists these results in comparison with related work. Compared to the DSConv network in [2], our network is more ef\ufb01cient in terms of accuracy for a given parameter count.",
            "This validates the effective- ness of GDSConv for model size reduction. Table 1 lists these results in comparison with related work. Compared to the DSConv network in [2], our network is more ef\ufb01cient in terms of accuracy for a given parameter count. Their biggest model has a 1.2% lower accuracy than our base model while having about 4 times the parameters. Choi et al. [4] has the most competitive results while we are still able to improve upon their accuracy for a given number of parame- ters. They are using 1D convolution along the time dimension as well which may be evidence that this yields better perfor- mance for audio processing or at least KWS.",
            "[4] has the most competitive results while we are still able to improve upon their accuracy for a given number of parame- ters. They are using 1D convolution along the time dimension as well which may be evidence that this yields better perfor- mance for audio processing or at least KWS. Model Accuracy Parameters DS-CNN-S [2] 94.1% 39k DS-CNN-M [2] 94.9% 189k DS-CNN-L [2] 95.4% 498k ResNet15 [3] 95.8% 240k TC-ResNet8 [4] 96.1% 66k TC-ResNet14 [4] 96.2% 137k TC-ResNet14-1.5 [4] 96.6% 305k SincConv+DSConv 96.6% 122k SincConv+GDSConv 96.4% 62k Table 1. Comparison of results on the Speech Commands dataset [19].",
            "Comparison of results on the Speech Commands dataset [19]. Model Accuracy Parameters SincConv+DSConv 97.4% 122k SincConv+GDSConv 97.3% 62k Table 2. Results on Speech Commands version 2 [19]. As opposed to previous works, our architecture does not use preprocessing to extract features, but is able to extract features from raw audio samples with the SincConv layer. That makes it possible to execute a full inference as \ufb02oat- ing point operations, without requiring additional hardware modules to process or transfer preprocessed features. Further- more, we deliberately opted to not use residual connections in our network architecture, considering the memory overhead and added dif\ufb01culty for hardware acceleration modules. For future comparability, we also trained and evaluated our model on the newer version 2 of the Speech Commands data set; see Table 2 for results. On a side note, we observed that models trained on version 2 of the Speech Commands dataset tend to perform better on both the test set for version 2 and the test set for version 1 [19]. 5.",
            "On a side note, we observed that models trained on version 2 of the Speech Commands dataset tend to perform better on both the test set for version 2 and the test set for version 1 [19]. 5. CONCLUSION Always-on, battery-powered devices running keyword spot- ting require energy ef\ufb01cient neural networks with high accu- racy. For this, we identi\ufb01ed the parameter count in a neural network as a main contributor to power consumption, as memory accesses contribute far more to power consump- tion than the computation. Based on this observation, we proposed an energy ef\ufb01cient KWS neural network architec- ture by combining feature extraction using SincConvs with GDSConv layers. Starting with the base model composed of DSConvs that have already less parameters than a regular convolution, we achieved state-of-the-art accuracy on Google\u2019s Speech Com- mands dataset. We further reduce the number of parameters by grouping the convolutional channels to GDSConv, result- ing in a low-parameter model with only 62k parameters.",
            "6. REFERENCES [1] G. Chen, C. Parada, and G. Heigold, \u201cSmall-footprint keyword spotting using deep neural networks,\u201d in 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), May 2014, pp. 4087\u2013 4091. [2] Y. Zhang, N. Suda, L. Lai, and V. Chandra, \u201cHello edge: Keyword spotting on microcontrollers,\u201d arXiv:1711.07128, 2017. [3] R. Tang and J. Lin, \u201cDeep residual learning for small- footprint keyword spotting,\u201d in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), April 2018, pp. 5484\u20135488. [4] S. Choi, S. Seo, B. Shin, H. Byun, M. Kersner, B. Kim, D. Kim, and S. Ha, \u201cTemporal Convolution for Real- Time Keyword Spotting on Mobile Devices,\u201d in Proc. Interspeech 2019, 2019, pp.",
            "Interspeech 2019, 2019, pp. 3372\u20133376. [5] S. Fern\u00b4andez, A. Graves, and J. Schmidhuber, \u201cAn ap- plication of recurrent neural networks to discriminative keyword spotting,\u201d in International Conference on Ar- ti\ufb01cial Neural Networks (ICANN). Springer, 2007, pp. 220\u2013229. [6] M. Ravanelli and Y. Bengio, \u201cSpeaker recognition from raw waveform with sincnet,\u201d in 2018 IEEE Spoken Language Technology Workshop (SLT), Dec 2018, pp. 1021\u20131028. [7] L. Kaiser, A. N. Gomez, and F. Chollet, \u201cDepthwise separable convolutions for neural machine translation,\u201d in International Conference on Learning Representa- tions (ICLR), 2018. [8] F. Chollet, \u201cXception: Deep learning with depthwise separable convolutions,\u201d in 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017, pp. 1800\u20131807.",
            "[8] F. Chollet, \u201cXception: Deep learning with depthwise separable convolutions,\u201d in 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017, pp. 1800\u20131807. [9] V. Sze, Y. Chen, T. Yang, and J. S. Emer, \u201cEf\ufb01cient processing of deep neural networks: A tutorial and sur- vey,\u201d Proceedings of the IEEE, vol. 105, no. 12, pp. 2295\u20132329, Dec 2017. [10] K. Kumatani, S. Panchapagesan, M. Wu, M. Kim, N. Strom, G. Tiwari, and A. Mandai, \u201cDirect model- ing of raw audio with dnns for wake word detection,\u201d in 2017 IEEE Automatic Speech Recognition and Under- standing Workshop (ASRU), Dec 2017, pp. 252\u2013257.",
            "252\u2013257. [11] M. Ravanelli and Y. Bengio, \u201cInterpretable convolu- tional \ufb01lters with sincnet,\u201d in NIPS 2018 Interpretability and Robustness for Audio, Speech and Language Work- shop, 2018. [12] N. Zeghidour, N. Usunier, I. Kokkinos, T. Schaiz, G. Synnaeve, and E. Dupoux, \u201cLearning \ufb01lterbanks from raw speech for phone recognition,\u201d in 2018 IEEE International Conference on Acoustics, Speech and Sig- nal Processing (ICASSP), April 2018, pp. 5509\u20135513. [13] N. Zeghidour, N. Usunier, G. Synnaeve, R. Collobert, and E. Dupoux, \u201cEnd-to-end speech recognition from the raw waveform,\u201d in Proc. Interspeech 2018, 2018, pp. 781\u2013785.",
            "Interspeech 2018, 2018, pp. 781\u2013785. [14] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, \u201cMobilenets: Ef\ufb01cient convolutional neural networks for mobile vision applications,\u201d arXiv:1704.04861, 2017. [15] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cIma- genet classi\ufb01cation with deep convolutional neural net- works,\u201d in Advances in Neural Information Processing Systems (NIPS), 2012. [16] M. Horowitz, \u201c1.1 computing\u2019s energy problem (and what we can do about it),\u201d in 2014 IEEE International Solid-State Circuits Conference Digest of Technical Pa- pers (ISSCC), Feb 2014, pp. 10\u201314.",
            "[16] M. Horowitz, \u201c1.1 computing\u2019s energy problem (and what we can do about it),\u201d in 2014 IEEE International Solid-State Circuits Conference Digest of Technical Pa- pers (ISSCC), Feb 2014, pp. 10\u201314. [17] Y. Chen, J. Emer, and V. Sze, \u201cEyeriss: A spatial architecture for energy-ef\ufb01cient data\ufb02ow for convolu- tional neural networks,\u201d in 2016 ACM\/IEEE 43rd An- nual International Symposium on Computer Architec- ture (ISCA), June 2016, pp. 367\u2013379. [18] M. Shahnawaz, E. Plebani, I. Guaneri, D. Pau, and M. Marcon, \u201cStudying the effects of feature extraction settings on the accuracy and memory requirements of neural networks for keyword spotting,\u201d in 2018 IEEE 8th International Conference on Consumer Electronics - Berlin (ICCE-Berlin), Sep. 2018, pp. 1\u20136.",
            "2018, pp. 1\u20136. [19] P. Warden, \u201cSpeech commands: A dataset for limited- vocabulary speech recognition,\u201d arXiv:1804.03209, 2018. [20] S. Ioffe and C. Szegedy, \u201cBatch normalization: Accel- erating deep network training by reducing internal co- variate shift,\u201d in Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, 2015, pp. 448\u2013456. [21] J. Tompson, R. Goroshin, A. Jain, Y. LeCun, and C. Bre- gler, \u201cEf\ufb01cient object localization using convolutional networks,\u201d in 2015 IEEE Conference on Computer Vi- sion and Pattern Recognition (CVPR), June 2015, pp. 648\u2013656. [22] J. Ba D. P. Kingma, \u201cAdam: A method for stochastic op- timization,\u201d in Conference on Learning Representations (ICLR), 2015."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1911.02086.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 5956.0,
    "avg_doclen_est": 186.125
}
