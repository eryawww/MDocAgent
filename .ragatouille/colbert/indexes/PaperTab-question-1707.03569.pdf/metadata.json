{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Multitask Learning for Fine-Grained Twitter Sentiment Analysis Georgios Balikas Univ. Grenoble Alps, CNRS, Grenoble INP - LIG/Coffreo geompalik@hotmail.com Simon Moura Univ. Grenoble Alps, CNRS, Grenoble INP - LIG simon.moura@univ-grenoble-alpes.fr Massih-Reza Amini Univ. Grenoble Alps, CNRS, Grenoble INP - LJK massih-reza.amini@univ-grenoble-alpes.fr Traditional sentiment analysis approaches tackle problems like ternary (3-category) and \ufb01ne- grained (5-category) classi\ufb01cation by learning the tasks separately. We argue that such classi\ufb01- cation tasks are correlated and we propose a multitask approach based on a recurrent neural net- work that bene\ufb01ts by jointly learning them. Our study demonstrates the potential of multitask models on this type of problems and improves the state-of-the-art results in the \ufb01ne-grained sentiment classi\ufb01cation problem.",
      "Our study demonstrates the potential of multitask models on this type of problems and improves the state-of-the-art results in the \ufb01ne-grained sentiment classi\ufb01cation problem. Keywords: Text Mining; Sentiment Analysis; Deep Learning; Multitask Learning, Twitter Analysis; bidirectional LSTM; Text classi\ufb01cation Introduction Automatic classi\ufb01cation of sentiment has mainly fo- cused on categorizing tweets in either two (binary senti- ment analysis) or three (ternary sentiment analysis) cate- gories (Giachanou & Crestani, 2016). In this work we study the problem of \ufb01ne-grained sentiment classi\ufb01cation where tweets are classi\ufb01ed according to a \ufb01ve-point scale ranging from VeryNegative to VeryPositive. To illustrate this, Ta- ble 1 presents examples of tweets associated with each of these categories. Five-point scales are widely adopted in review sites like Amazon and TripAdvisor, where a user\u2019s sentiment is ordered with respect to its intensity. From a sentiment analysis perspective, this de\ufb01nes a classi\ufb01cation problem with \ufb01ve categories. In particular, Sebastiani et al.",
      "Five-point scales are widely adopted in review sites like Amazon and TripAdvisor, where a user\u2019s sentiment is ordered with respect to its intensity. From a sentiment analysis perspective, this de\ufb01nes a classi\ufb01cation problem with \ufb01ve categories. In particular, Sebastiani et al. (Martino, Gao, & Sebastiani, 2016) de\ufb01ned such classi\ufb01ca- tion problems whose categories are explicitly ordered to be ordinal classi\ufb01cation problems. To account for the ordering of the categories, learners are penalized according to how far from the true class their predictions are. Although considering different scales, the various settings of sentiment classi\ufb01cation are related. First, one may use the same feature extraction and engineering approaches to represent the text spans such as word membership in lex- icons, morpho-syntactic statistics like punctuation or elon- gated word counts (Balikas & Amini, 2016; Kiritchenko, Zhu, & Mohammad, 2014). Second, one would expect that knowledge from one task can be transfered to the others and this would bene\ufb01t the performance.",
      "Second, one would expect that knowledge from one task can be transfered to the others and this would bene\ufb01t the performance. Knowing that a tweet is \u201cPositive\u201d in the ternary setting narrows the classi\ufb01cation de- cision between the VeryPositive and Positive categories in the \ufb01ne-grained setting. From a research perspective this raises the question of whether and how one may bene\ufb01t when tack- ling such related tasks and how one can transfer knowledge from one task to another during the training phase. Our focus in this work is to exploit the relation between the sentiment classi\ufb01cation settings and demonstrate the ben- e\ufb01ts stemming from combining them. To this end, we pro- pose to formulate the different classi\ufb01cation problems as a multitask learning problem and jointly learn them.",
      "Our focus in this work is to exploit the relation between the sentiment classi\ufb01cation settings and demonstrate the ben- e\ufb01ts stemming from combining them. To this end, we pro- pose to formulate the different classi\ufb01cation problems as a multitask learning problem and jointly learn them. Multi- task learning (Caruana, 1997) has shown great potential in various domains and its bene\ufb01ts have been empirically vali- dated (Collobert & Weston, 2008; Plank, 2016; Liu, Qiu, & Huang, 2016b, 2016a) using different types of data and learn- ing approaches. An important bene\ufb01t of multitask learning is that it provides an elegant way to access resources devel- oped for similar tasks. By jointly learning correlated tasks, the amount of usable data increases. For instance, while for ternary classi\ufb01cation one can label data using distant supervi- sion with emoticons (Go, Bhayani, & Huang, 2009), there is no straightforward way to do so for the \ufb01ne-grained problem.",
      "For instance, while for ternary classi\ufb01cation one can label data using distant supervi- sion with emoticons (Go, Bhayani, & Huang, 2009), there is no straightforward way to do so for the \ufb01ne-grained problem. However, the latter can bene\ufb01t indirectly, if the ternary and \ufb01ne-grained tasks are learned jointly. The research question that the paper attempts to answer is the following: Can twitter sentiment classi\ufb01cation problems, and \ufb01ne-grained sentiment classi\ufb01cation in particular, bene\ufb01t from multitask learning? To answer the question, the paper brings the following two main contributions: (i) we show how jointly learning the ternary and \ufb01ne-grained sentiment classi\ufb01cation problems in a multitask setting improves the arXiv:1707.03569v1  [cs.IR]  12 Jul 2017",
      "2 GEORGIOS BALIKAS VeryNegative Beyond frustrated with my #Xbox360 right now, and that as of June, @Microsoft doesn\u2019t support it. Gotta \ufb01nd someone else to \ufb01x the drive. Negative @Microsoft Heard you are a software company. Why then is most of your software so bad that it has to be replaced by 3rd party apps? Neutral @ProfessorF @gilwuvsyou @Microsoft @LivioDeLaCruz We already knew the media march in ideological lockstep but it is nice of him to show it. Positive PAX Prime Thursday is overloaded for me with @Microsoft and Nintendo indie events going down. Also, cider!!! :p VeryPositive I traveled to Redmond today. I\u2019m visiting with @Microsoft @SQLServer engineers tomorrow - at their invitation. Feeling excited. Table 1 The example demonstrates the different levels of sentiment a tweet may convey. Also, note the Twitter-speci\ufb01c use of language and symbols.",
      "I\u2019m visiting with @Microsoft @SQLServer engineers tomorrow - at their invitation. Feeling excited. Table 1 The example demonstrates the different levels of sentiment a tweet may convey. Also, note the Twitter-speci\ufb01c use of language and symbols. state-of-the-art performance,1 and (ii) we demonstrate that recurrent neural networks outperform models previously pro- posed without access to huge corpora while being \ufb02exible to incorporate different sources of data. Multitask Learning for Twitter Sentiment Classi\ufb01cation In his work, Caruana (Caruana, 1997) proposed a multi- task approach in which a learner takes advantage of the mul- tiplicity of interdependent tasks while jointly learning them. The intuition is that if the tasks are correlated, the learner can learn a model jointly for them while taking into account the shared information which is expected to improve its general- ization ability. People express their opinions online on var- ious subjects (events, products..), on several languages and in several styles (tweets, paragraph-sized reviews..), and it is exactly this variety that motivates the multitask approaches.",
      "People express their opinions online on var- ious subjects (events, products..), on several languages and in several styles (tweets, paragraph-sized reviews..), and it is exactly this variety that motivates the multitask approaches. Speci\ufb01cally for Twitter for instance, the different settings of classi\ufb01cation like binary, ternary and \ufb01ne-grained are corre- lated since their difference lies in the sentiment granularity of the classes which increases while moving from binary to \ufb01ne-grained problems. There are two main decisions to be made in our approach: the learning algorithm, which learns a decision function, and the data representation. With respect to the former, neu- ral networks are particularly suitable as one can design ar- chitectures with different properties and arbitrary complex- ity. Also, as training neural network usually relies on back- propagation of errors, one can have shared parts of the net- work trained by estimating errors on the joint tasks and oth- ers specialized for particular tasks. Concerning the data rep- resentation, it strongly depends on the data type available.",
      "Also, as training neural network usually relies on back- propagation of errors, one can have shared parts of the net- work trained by estimating errors on the joint tasks and oth- ers specialized for particular tasks. Concerning the data rep- resentation, it strongly depends on the data type available. For the task of sentiment classi\ufb01cation of tweets with neu- ral networks, distributed embeddings of words have shown great potential. Embeddings are de\ufb01ned as low-dimensional, dense representations of words that can be obtained in an unsupervised fashion by training on large quantities of text (Pennington, Socher, & Manning, 2014). Concerning the neural network architecture, we focus on Recurrent Neural Networks (RNNs) that are capable of mod- eling short-range and long-range dependencies like those ex- hibited in sequence data of arbitrary length like text. While in the traditional information retrieval paradigm such depen- dencies are captured using n-grams and skip-grams, RNNs learn to capture them automatically (Dyer, Ballesteros, Ling, Matthews, & Smith, 2015).",
      "While in the traditional information retrieval paradigm such depen- dencies are captured using n-grams and skip-grams, RNNs learn to capture them automatically (Dyer, Ballesteros, Ling, Matthews, & Smith, 2015). To circumvent the problems with capturing long-range dependencies and preventing gra- dients from vanishing, the long short-term memory network (LSTM) was proposed (Hochreiter & Schmidhuber, 1997). In this work, we use an extended version of LSTM called bidirectional LSTM (biLSTM). While standard LSTMs ac- cess information only from the past (previous words), biL- STMs capture both past and future information effectively (Huang, Xu, & Yu, 2015; Dyer et al., 2015). They consist of two LSTM networks, for propagating text forward and back- wards with the goal being to capture the dependencies better.",
      "They consist of two LSTM networks, for propagating text forward and back- wards with the goal being to capture the dependencies better. Indeed, previous work on multitask learning showed the ef- fectiveness of biLSTMs in a variety of problems: (Alonso & Plank, 2016) tackled sequence prediction, while (Plank, 2016) and (Kiperwasser & Goldberg, 2016) used biLSTMs for Named Entity Recognition and dependency parsing re- spectively. Figure 1 presents the architecture we use for multitask learning. In the top-left of the \ufb01gure a biLSTM net- work (enclosed by the dashed line) is fed with embeddings {X1, . . . , XT} that correspond to the T words of a tokenized tweet. Notice, as discussed above, the biLSTM consists of two LSTMs that are fed with the word sequence forward and backwards. On top of the biLSTM network one (or more) hidden layers H1 transform its output. The output of H1 is led to the softmax layers for the prediction step.",
      "On top of the biLSTM network one (or more) hidden layers H1 transform its output. The output of H1 is led to the softmax layers for the prediction step. There are N softmax layers and each is used for one of the N tasks of the multitask setting. In tasks such as sentiment classi\ufb01ca- tion, additional features like membership of words in senti- ment lexicons or counts of elongated/capitalized words can be used to enrich the representation of tweets before the clas- si\ufb01cation step (Kiritchenko et al., 2014). The lower part of the network illustrates how such sources of information can be incorporated to the process. A vector \u201cAdditional Fea- tures\u201d for each tweet is transformed from the hidden layer(s) HA and then is combined by concatenation with the trans- formed biLSTM output in the HM layer. Experimental setup Our goal is to demonstrate how multitask learning can be successfully applied on the task of sentiment classi\ufb01cation 1An open implementation of the system for research pur- poses is available at https://github.com/balikasg/ sigir2017.",
      "INSERT SHORTTITLE COMMAND IN PREAMBLE 3 Multitask outputs biLSTM LSTM LSTM LSTM LSTM LSTM LSTM XT X1 XT\u22121 X2 X1 XT H1 HA HM Additional Features softmax1 softmaxN . . . Figure 1. The neural network architecture for multitask learning. The biLSTM output is transformed by the hidden layers H1, HM and is led to N output layers, one for each of the tasks. The lower part of the network can be used to incorporate additional information. of tweets. The particularities of tweets are to be short and informal text spans. The common use of abbreviations, cre- ative language etc., makes the sentiment classi\ufb01cation prob- lem challenging. To validate our hypothesis, that learning the tasks jointly can bene\ufb01t the performance, we propose an experimental setting where there are data from two different twitter sentiment classi\ufb01cation problems: a \ufb01ne-grained and a ternary. We consider the \ufb01ne-grained task to be our primary task as it is more challenging and obtaining bigger datasets, e.g.",
      "We consider the \ufb01ne-grained task to be our primary task as it is more challenging and obtaining bigger datasets, e.g. by distant supervision, is not straightforward and, hence we report the performance achieved for this task. Ternary and \ufb01ne-grained sentiment classi\ufb01cation were part of the SemEval-2016 \u201cSentiment Analysis in Twit- ter\u201d task (Nakov, Ritter, Rosenthal, Sebastiani, & Stoyanov, 2016). We use the high-quality datasets the challenge or- ganizers released.2 The dataset for \ufb01ne-grained classi\ufb01ca- tion is split in training, development, development_test and test parts. In the rest, we refer to these splits as train, development and test, where train is composed by the training and the development instances. Table 2 presents an overview of the data. As discussed in (Nakov et al., 2016) and illustrated in the Table, the \ufb01ne-grained dataset is highly unbalanced and skewed towards the positive sentiment: only 13.6% of the training examples are labeled with one of the negative classes.",
      "As discussed in (Nakov et al., 2016) and illustrated in the Table, the \ufb01ne-grained dataset is highly unbalanced and skewed towards the positive sentiment: only 13.6% of the training examples are labeled with one of the negative classes. Feature representation We report results using two differ- ent feature sets. The \ufb01rst one, dubbed nbow, is a neu- ral bag-of-words that uses text embeddings to generate low- dimensional, dense representations of the tweets. To con- struct the nbow representation, given the word embeddings dictionary where each word is associated with a vector, we apply the average compositional function that averages the embeddings of the words that compose a tweet. Simple com- positional functions like average were shown to be robust and ef\ufb01cient in previous work (Mitchell & Lapata, 2010). Instead of training embeddings from scratch, we use the pre- trained on tweets GloVe embeddings of (Pennington et al., 2014).3 In terms of resources required, using only nbow is ef\ufb01cient as it does not require any domain knowledge.",
      "Instead of training embeddings from scratch, we use the pre- trained on tweets GloVe embeddings of (Pennington et al., 2014).3 In terms of resources required, using only nbow is ef\ufb01cient as it does not require any domain knowledge. How- ever, previous research on sentiment analysis showed that us- |D| VeryNeg. Neg. Neutr. Pos. VeryPos. Train 7,292 111 884 2,019 3,726 432 Dev. 1,778 29 204 533 887 125 Test 20,632 138 2,201 10,081 7,830 382 Ternary 5,500 - 785 1,887 2,828 - Table 2 Cardinality and class distributions of the datasets. ing extra resources, like sentiment lexicons, can bene\ufb01t sig- ni\ufb01cantly the performance (Kiritchenko et al., 2014; Balikas & Amini, 2016).",
      "ing extra resources, like sentiment lexicons, can bene\ufb01t sig- ni\ufb01cantly the performance (Kiritchenko et al., 2014; Balikas & Amini, 2016). To validate this and examine at which ex- tent neural networks and multitask learning bene\ufb01t from such features we evaluate the models using an augmented version of nbow, dubbed nbow+. The feature space of the latter, is augmented using 1,368 extra features consisting mostly of counts of punctuation symbols (\u2019!?#@\u2019), emoticons, elon- gated words and word membership features in several sen- timent lexicons.",
      "The feature space of the latter, is augmented using 1,368 extra features consisting mostly of counts of punctuation symbols (\u2019!?#@\u2019), emoticons, elon- gated words and word membership features in several sen- timent lexicons. Due to space limitations, for a complete presentation of these features, we refer the interested reader to (Balikas & Amini, 2016), whose open implementation we used to extract them.4 Evaluation measure To reproduce the setting of the Se- mEval challenges (Nakov et al., 2016), we optimize our sys- tems using as primary measure the macro-averaged Mean Absolute Error (MAEM) given by: MAEM = 1 |C| |C| X j=1 1 |Tej| X xi\u2208Tej |h(xi) \u2212yi| where |C| is the number of categories, Te j is the set of in- stances whose true class is cj, yi is the true label of the in- stance xi and h(xi) the predicted label. The measure penal- izes decisions far from the true ones and is macro-averaged to account for the fact that the data are unbalanced.",
      "The measure penal- izes decisions far from the true ones and is macro-averaged to account for the fact that the data are unbalanced. Comple- mentary to MAEM, we report the performance achieved on 2The datasets are those of Subtasks A and C, available at http://alt.qcri.org/semeval2016/task4/. 3urlhttp://nlp.stanford.edu/data/glove.twitter.27B.zip 4https://github.com/balikasg/SemEval2016 -Twitter_Sentiment_Evaluation",
      "4 GEORGIOS BALIKAS the micro-averaged F1 measure, which is a commonly used measure for classi\ufb01cation. The models To evaluate the multitask learning approach, we compared it with several other models. Support Vec- tor Machines (SVMs) are maximum margin classi\ufb01cation al- gorithms that have been shown to achieve competitive per- formance in several text classi\ufb01cation problems (Nakov et al., 2016). SVMovr stands for an SVM with linear kernel and an one-vs-rest approach for the multi-class problem. Also, SVMcs is an SVM with linear kernel that employs the crammer-singer strategy (Crammer & Singer, 2001) for the multi-class problem. Logistic regression (LR) is another type of linear classi\ufb01cation method, with probabilistic motivation. Again, we use two types of Logistic Regression depending on the multi-class strategy: LRovr that uses an one-vs-rest approach and multinomial Logistic Regression also known as the MaxEnt classi\ufb01er that uses a multinomial criterion.",
      "Again, we use two types of Logistic Regression depending on the multi-class strategy: LRovr that uses an one-vs-rest approach and multinomial Logistic Regression also known as the MaxEnt classi\ufb01er that uses a multinomial criterion. Both SVMs and LRs as discussed above treat the prob- lem as a multi-class one, without considering the ordering of the classes. For these four models, we tuned the hyper- parameter C that controls the importance of the L2 regu- larization part in the optimization problem with grid-search over {10\u22124, . . . , 104} using 10-fold cross-validation in the union of the training and development data and then retrained the models with the selected values. Also, to account for the unbalanced classi\ufb01cation problem we used class weights to penalize more the errors made on the rare classes. These weights were inversely proportional to the frequency of each class. For the four models we used the implementations of Scikit-learn (Pedregosa et al., 2011). For multitask learning we use the architecture shown in Figure 1, which we implemented with Keras (Chollet, 2015).",
      "For the four models we used the implementations of Scikit-learn (Pedregosa et al., 2011). For multitask learning we use the architecture shown in Figure 1, which we implemented with Keras (Chollet, 2015). The embeddings are initialized with the 50-dimensional GloVe embeddings while the output of the biLSTM network is set to dimension 50. The activation function of the hidden layers is the hyperbolic tangent. The weights of the layers were initialized from a uniform distribution, scaled as de- scribed in (Glorot & Bengio, 2010). We used the Root Mean Square Propagation optimization method. We used dropout for regularizing the network. We trained the network using batches of 128 examples as follows: before selecting the batch, we perform a Bernoulli trial with probability pM to select the task to train for. With probability pM we pick a batch for the \ufb01ne-grained sentiment classi\ufb01cation problem, while with probability 1 \u2212pM we pick a batch for the ternary problem.",
      "With probability pM we pick a batch for the \ufb01ne-grained sentiment classi\ufb01cation problem, while with probability 1 \u2212pM we pick a batch for the ternary problem. As shown in Figure 1, the error is backpropagated until the embeddings, that we \ufb01ne-tune during the learning process. Notice also that the weights of the network until the layer HM are shared and therefore affected by both tasks. To tune the neural network hyper-parameters we used 5- fold cross validation. We tuned the probability p of dropout after the hidden layers HM, H1, HA and for the biLSTM for p \u2208{0.2, 0.3, 0.4, 0.5}, the size of the hidden layer HM \u2208 {20, 30, 40, 50} and the probability pM of the Bernoulli tri- als from {0.5, 0.6, 0.7, 0.8}.5 During training, we monitor the network\u2019s performance on the development set and apply early stopping if the performance on the validation set does not improve for 5 consecutive epochs.",
      "Experimental results Table 3 illustrates the performance of the models for the different data representations. The upper part of the Table summarizes the performance of the base- lines. The entry \u201cBalikas et al.\u201d stands for the winning sys- tem of the 2016 edition of the challenge (Balikas & Amini, 2016), which to the best of our knowledge holds the state-of- the-art. Due to the stochasticity of training the biLSTM mod- els, we repeat the experiment 10 times and report the average and the standard deviation of the performance achieved. Several observations can be made from the table. First notice that, overall, the best performance is achieved by the neural network architecture that uses multitask learning. This entails that the system makes use of the available resources ef\ufb01ciently and improves the state-of-the-art performance. In conjunction with the fact that we found the optimal prob- ability pM = 0.5, this highlights the bene\ufb01ts of multitask learning over single task learning. Furthermore, as described above, the neural network-based models have only access to the training data as the development are hold for early stop- ping.",
      "Furthermore, as described above, the neural network-based models have only access to the training data as the development are hold for early stop- ping. On the other hand, the baseline systems were retrained on the union of the train and development sets. Hence, even with fewer resources available for training on the \ufb01ne- grained problem, the neural networks outperform the base- lines. We also highlight the positive effect of the additional features that previous research proposed. Adding the features both in the baselines and in the biLSTM-based architectures improves the MAEM scores by several points. Lastly, we compare the performance of the baseline sys- tems with the performance of the state-of-the-art system of (Balikas & Amini, 2016). While (Balikas & Amini, 2016) uses n-grams (and character-grams) with n > 1, the baseline systems (SVMs, LRs) used in this work use the nbow+ rep- resentation, that relies on unigrams. Although they perform on par, the competitive performance of nbow highlights the potential of distributed representations for short-text classi\ufb01- cation.",
      "Although they perform on par, the competitive performance of nbow highlights the potential of distributed representations for short-text classi\ufb01- cation. Further, incorporating structure and distributed rep- resentations leads to the gains of the biLSTM network, in the multitask and single task setting. Similar observations can be drawn from Figure 2 that presents the F1 scores. Again, the biLSTM network with multitask learning achieves the best performance. It is also to be noted that although the two evaluation measures are correlated in the sense that the ranking of the models is the same, small differences in the MAEM have large effect on the scores of the F1 measure. 5Overall, we cross-validated 512 combinations of parameters. The best parameters were: 0.2 for all dropout rates, 20 neurons for HM and pM = 0.5.",
      "INSERT SHORTTITLE COMMAND IN PREAMBLE 5 nbow nbow+ SVMovr 0.840 0.714 SVMcs 0.946 0.723 LRovr 0.836 0.712 MaxEnt 0.842 0.715 (Balikas & Amini, 2016) - 0.719 biLSTM (single task) 0.827\u00b10.017 0.694\u00b10.04 biLSTM+Multitask 0.786\u00b10.025 0.685\u00b10.024 Table 3 The scores on MAEM for the systems. The best (lowest) score is shown in bold and is achieved in the multitask setting with the biLSTM architecture of Figure 1. SVMcs MaxEnt SVMovr LRovr biLSTM biLSTM+ Multitask 0.15 0.30 0.45 0.251 0.359 0.445 0.459 0.469 0.481 F1 Figure 2. F1 scores using the nbow+ representations. The best performance is achieved with the multitask setting.",
      "F1 scores using the nbow+ representations. The best performance is achieved with the multitask setting. Conclusion In this paper, we showed that by jointly learning the tasks of ternary and \ufb01ne-grained classi\ufb01cation with a multitask learning model, one can greatly improve the performance on the second. This opens several avenues for future research. Since sentiment is expressed in different textual types like tweets and paragraph-sized reviews, in different languages (English, German, ..) and in different granularity levels (bi- nary, ternary,..) one can imagine multitask approaches that could bene\ufb01t from combining such resources. Also, while we opted for biLSTM networks here, one could use convolu- tional neural networks or even try to combine different types of networks and tasks to investigate the performance effect of multitask learning. Lastly, while our approach mainly relied on the foundations of (Caruana, 1997), the internal mech- anisms and the theoretical guarantees of multitask learning remain to be better understood. Acknowledgements This work is partially supported by the CIFRE N 28/2015. References Alonso, H. M., & Plank, B.",
      "Acknowledgements This work is partially supported by the CIFRE N 28/2015. References Alonso, H. M., & Plank, B. (2016). Multitask learning for semantic sequence prediction under varying data conditions. arXiv:1612.02251. Balikas, G., & Amini, M. (2016). Twise at semeval-2016 task 4: Twitter sentiment classi\ufb01cation. In Semeval@naacl-hlt 2016 (pp. 85\u201391). Caruana, R. (1997). Multitask learning. Machine Learning, 28(1), 41\u201375. Chollet, F. (2015). Keras. https://github.com/ fchollet/keras. GitHub. Collobert, R., & Weston, J. (2008). A uni\ufb01ed architecture for nat- ural language processing: deep neural networks with multitask learning. In Icml. Crammer, K., & Singer, Y. (2001).",
      "Collobert, R., & Weston, J. (2008). A uni\ufb01ed architecture for nat- ural language processing: deep neural networks with multitask learning. In Icml. Crammer, K., & Singer, Y. (2001). On the algorithmic implementa- tion of multiclass kernel-based vector machines. JMLR, 2, 265\u2013 292. Dyer, C., Ballesteros, M., Ling, W., Matthews, A., & Smith, N. A. (2015). Transition-based dependency parsing with stack long short-term memory. In Acl (pp. 334\u2013343). Giachanou, A., & Crestani, F. (2016). Like it or not: A survey of twitter sentiment analysis methods. ACM Comput. Surv., 49(2), 28:1\u201328:41. Glorot, X., & Bengio, Y. (2010). Understanding the dif\ufb01culty of training deep feedforward neural networks. In Aistats (pp. 249\u2013 256).",
      "Glorot, X., & Bengio, Y. (2010). Understanding the dif\ufb01culty of training deep feedforward neural networks. In Aistats (pp. 249\u2013 256). Go, A., Bhayani, R., & Huang, L. (2009). Twitter sentiment classi- \ufb01cation using distant supervision. CS224N Project Report, Stan- ford, 1, 12. Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735\u20131780. Huang, Z., Xu, W., & Yu, K. (2015). Bidirectional LSTM-CRF models for sequence tagging. CoRR, abs/1508.01991. Kiperwasser, E., & Goldberg, Y. (2016). Simple and accurate de- pendency parsing using bidirectional LSTM feature representa- tions. TACL. Kiritchenko, S., Zhu, X., & Mohammad, S. M. (2014).",
      "Kiperwasser, E., & Goldberg, Y. (2016). Simple and accurate de- pendency parsing using bidirectional LSTM feature representa- tions. TACL. Kiritchenko, S., Zhu, X., & Mohammad, S. M. (2014). Sentiment analysis of short informal texts. JAIR, 723\u2013762. Liu, P., Qiu, X., & Huang, X. (2016a). Deep multi-task learning with shared memory for text classi\ufb01cation. In Emnlp (pp. 118\u2013 127). Liu, P., Qiu, X., & Huang, X. (2016b). Recurrent neural network for text classi\ufb01cation with multi-task learning. In Ijcai (pp. 2873\u2013 2879). Martino, G. D. S., Gao, W., & Sebastiani, F. (2016). Ordinal text quanti\ufb01cation. In Sigir (pp. 937\u2013940). Mitchell, J., & Lapata, M. (2010).",
      "Martino, G. D. S., Gao, W., & Sebastiani, F. (2016). Ordinal text quanti\ufb01cation. In Sigir (pp. 937\u2013940). Mitchell, J., & Lapata, M. (2010). Composition in distributional models of semantics. Cognitive Science, 34(8), 1388\u20131429. Nakov, P., Ritter, A., Rosenthal, S., Sebastiani, F., & Stoyanov, V. (2016). Semeval-2016 task 4: Sentiment analysis in twitter. In Semeval@naacl-hlt (pp. 1\u201318). Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., . . . Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. JMLR, 12, 2825\u20132830.",
      ". . Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. JMLR, 12, 2825\u20132830. Pennington, J., Socher, R., & Manning, C. D. (2014). Glove: Global vectors for word representation. In Emnlp (pp. 1532\u20131543). Plank, B. (2016). Keystroke dynamics as signal for shallow syntac- tic parsing. In Coling (pp. 609\u2013619)."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1707.03569.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":6434,
  "avg_doclen":183.8285714286,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1707.03569.pdf"
    }
  }
}