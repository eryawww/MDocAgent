{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Recurrent Neural Network Encoder with Attention for Community Question Answering Wei-Ning Hsu and Yu Zhang and James Glass Computer Science and Arti\ufb01cial Intelligence Laboratory Massachusetts Institute of Technology Cambridge, MA 02139, USA {wnhsu,yzhang87,jrg}@csail.mit.edu Abstract We apply a general recurrent neural net- work (RNN) encoder framework to commu- nity question answering (cQA) tasks. Our ap- proach does not rely on any linguistic pro- cessing, and can be applied to different lan- guages or domains. Further improvements are observed when we extend the RNN encoders with a neural attention mechanism that en- courages reasoning over entire sequences. To deal with practical issues such as data spar- sity and imbalanced labels, we apply vari- ous techniques such as transfer learning and multitask learning. Our experiments on the SemEval-2016 cQA task show 10% improve- ment on a MAP score compared to an infor- mation retrieval-based approach, and achieve comparable performance to a strong hand- crafted feature-based method.",
            "Our experiments on the SemEval-2016 cQA task show 10% improve- ment on a MAP score compared to an infor- mation retrieval-based approach, and achieve comparable performance to a strong hand- crafted feature-based method. 1 Introduction Community question answering (cQA) is a paradigm that provides forums for users to ask or answer questions on any topic with barely any restrictions. In the past decade, these websites have attracted a great number of users, and have accumulated a large collection of question-comment threads generated by these users. However, the low restriction results in a high variation in answer quality, which makes it time-consuming to search for useful information from the existing content. It would therefore be valuable to automate the pro- cedure of ranking related questions and comments for users with a new question, or when looking for solutions from comments of an existing question. Automation of cQA forums can be divided into three tasks: question-comment relevance (Task A), question-question relevance (Task B), and question- external comment relevance (Task C). One might think that classic retrieval models like language models for information retrieval (Zhai and Lafferty, 2004) could solve these tasks.",
            "One might think that classic retrieval models like language models for information retrieval (Zhai and Lafferty, 2004) could solve these tasks. However, a big challenge for cQA tasks is that users are used to expressing similar meanings with different words, which creates gaps when matching questions based on common words. Other challenges include in- formal usage of language, highly diverse content of comments, and variation in the length of both ques- tions and comments. To overcome these issues, most previous work (e.g. SemEval 2015 (Nakov et al., 2015)) relied heavily on additional features and reasoning capa- bilities. In (Rockt\u00a8aschel et al., 2015), a neural attention-based model was proposed for automati- cally recognizing entailment relations between pairs of natural language sentences. In this study, we \ufb01rst modify this model for all three cQA tasks. We also extend this framework into a jointly trained model when the external resources are available, i.e. select- ing an external comment when we know the ques- tion that the external comment answers (Task C).",
            "In this study, we \ufb01rst modify this model for all three cQA tasks. We also extend this framework into a jointly trained model when the external resources are available, i.e. select- ing an external comment when we know the ques- tion that the external comment answers (Task C). Our ultimate objective is to classify relevant questions and comments without complicated hand- crafted features. By applying RNN-based encoders, we avoid heavily engineered features and learn the representation automatically. In addition, an atten- tion mechanism augments encoders with the ability to attend to past outputs directly. This becomes help- ful when encoding longer sequences, since we no arXiv:1603.07044v1  [cs.CL]  23 Mar 2016",
            "longer need to compress all information into a \ufb01xed- length vector representation. In our view, existing annotated cQA corpora are generally too small to properly train an end-to-end neural network. To address this, we investigate transfer learning by pretraining the recurrent sys- tems on other corpora, and also generating addi- tional instances from existing cQA corpus. 2 Related Work Earlier work of community question answering re- lied heavily on feature engineering, linguistic tools, and external resource. (Jeon et al., 2006) and (Shah and Pomerantz, 2010) utilized rich non-textual fea- tures such as answer\u2019s pro\ufb01le. (Grundstr\u00a8om and Nugues, 2014) syntactically analyzed the question and extracted name entity features. (Harabagiu and Hickl, 2006) demonstrated a textual entailment sys- tem can enhance cQA task by casting question an- swering to logical entailment.",
            "(Harabagiu and Hickl, 2006) demonstrated a textual entailment sys- tem can enhance cQA task by casting question an- swering to logical entailment. More recent work incorporated word vector into their feature extraction system and based on it de- signed different distance metric for question and an- swer (Tran et al., 2015) (Belinkov et al., 2015). While these approaches showed effectiveness, it is dif\ufb01cult to generalize them to common cQA tasks since linguistic tools and external resource may be restrictive in other languages and features are highly customized for each cQA task. Very recent work on answer selection also in- volved the use of neural networks. (Wang and Ny- berg, 2015) used LSTM to construct a joint vector based on both the question and the answer and then converted it into a learning to rank problem. (Feng et al., 2015) proposed several convolutional neural network (CNN) architectures for cQA. Our method differs in that RNN encoder is applied here and by adding attention mechanism we jointly learn which words in question to focus and hence available to conduct qualitative analysis.",
            "(Feng et al., 2015) proposed several convolutional neural network (CNN) architectures for cQA. Our method differs in that RNN encoder is applied here and by adding attention mechanism we jointly learn which words in question to focus and hence available to conduct qualitative analysis. During classi\ufb01cation, we feed the extracted vector into a feed-forward neural network directly instead of using mean\/max pooling on top of each time steps. 3 Method In this section, we \ufb01rst discuss long short-term mem- ory (LSTM) units and an associated attention mech- anism. Next, we explain how we can encode a pair of sentences into a dense vector for predict- ing relationships using an LSTM with an attention mechanism. Finally, we apply these models to pre- dict question-question similarity, question-comment similarity, and question-external comment similar- ity. 3.1 LSTM Models LSTMs have shown great success in many differ- ent \ufb01elds. An LSTM unit contains a memory cell with self-connections, as well as three multiplicative gates to control information \ufb02ow.",
            "3.1 LSTM Models LSTMs have shown great success in many differ- ent \ufb01elds. An LSTM unit contains a memory cell with self-connections, as well as three multiplicative gates to control information \ufb02ow. Given input vector xt, previous hidden outputs ht\u22121, and previous cell state ct\u22121, LSTM units operate as follows: X = \" xt ht\u22121 # (1) it = \u03c3(WiXX + Wicct\u22121 + bi) (2) ft = \u03c3(WfXX + Wfcct\u22121 + bf) (3) ot = \u03c3(WoXX + Wocct\u22121 + bo) (4) ct = ft \u2299ct\u22121 + it \u2299tanh(WcXX + bc) (5) ht = ot \u2299tanh(ct) (6) where it, ft, ot are input, forget, and output gates, respectively. The sigmoid function \u03c3() is a soft gate function controlling the amount of information \ufb02ow. Ws and bs are model parameters to learn.",
            "The sigmoid function \u03c3() is a soft gate function controlling the amount of information \ufb02ow. Ws and bs are model parameters to learn. 3.2 Neural Attention A traditional RNN encoder-decoder ap- proach (Sutskever et al., 2014) \ufb01rst encodes an arbitrary length input sequence into a \ufb01xed-length dense vector that can be used as input to subsequent classi\ufb01cation models, or to initialize the hidden state of a secondary decoder. However, the requirement to compress all necessary information into a single \ufb01xed length vector can be problematic. A neural attention model (Bahdanau et al., 2014) (Cho et al., 2014) has been recently proposed to alleviate this issue by enabling the network to attend to past outputs when decoding. Thus, the encoder no longer needs to represent an entire sequence with one vector; instead, it encodes information into a sequence of vectors, and adaptively chooses a subset of the vectors when decoding.",
            "LSTM1 LSTM2 Relevant Irrelevant Parallel LSTM Encoder LSTM1 LSTM2 Relevant Irrelevant Sequential LSTM Encoder Shared or not? Figure 1: RNN encoder for related question\/comment selection. w1 w2 wi ... q1 ... qi what is ... bank : The best bank in . . . h1 h2 hi Attention [h0 = L X i=1 \u21b5ihi] hN LSTM 1 LSTM 2 hL Augmented features Relevant Irrelevant Figure 2: Neural attention model for related question\/comment selection. 3.3 Predicting Relationships of Object Pairs with an Attention Model In our cQA tasks, the pair of objects are (question, question) or (question, comment), and the relation- ship is relevant\/irrelevant. The left side of Figure 1 shows one intuitive way to predict relationships us- ing RNNs. Parallel LSTMs encode two objects inde- pendently, and then concatenate their outputs as an input to a feed-forward neural network (FNN) with a softmax output layer for classi\ufb01cation. The representations of the two objects are gener- ated independently in this manner.",
            "Parallel LSTMs encode two objects inde- pendently, and then concatenate their outputs as an input to a feed-forward neural network (FNN) with a softmax output layer for classi\ufb01cation. The representations of the two objects are gener- ated independently in this manner. However, we are more interested in the relationship instead of the ob- ject representations themselves. Therefore, we con- sider a serialized LSTM-encoder model in the right side of Figure 1 that is similar to that in (Rockt\u00a8aschel et al., 2015), but also allows an augmented feature input to the FNN classi\ufb01er. Figure 2 illustrates our attention framework in more detail. The \ufb01rst LSTM reads one object, and passes information through hidden units to the sec- ond LSTM. The second LSTM then reads the other object and generates the representation of this pair after the entire sequence is processed. We build an- other FNN that takes this representation as input to classify the relationship of this pair.",
            "The second LSTM then reads the other object and generates the representation of this pair after the entire sequence is processed. We build an- other FNN that takes this representation as input to classify the relationship of this pair. By adding an attention mechanism to the encoder, we allow the second LSTM to attend to the sequence of output vectors from the \ufb01rst LSTM, and hence generate a weighted representation of \ufb01rst object ac- cording to both objects. Let hN be the last output of second LSTM and M = [h1, h2, \u00b7 \u00b7 \u00b7 , hL] be the sequence of output vectors of the \ufb01rst object. The weighted representation of the \ufb01rst object is h\u2032 = L X i=1 \u03b1ihi (7) The weight is computed by \u03b1i = exp(a(hi, hN)) PL j=1 exp(a(hj, hN)) (8) where a() is the importance model that produces a higher score for (hi, hN) if hi is useful to determine",
            "the object pair\u2019s relationship. We parametrize this model using another FNN. Note that in our frame- work, we also allow other augmented features (e.g., the ranking score from the IR system) to enhance the classi\ufb01er. So the \ufb01nal input to the classi\ufb01er will be hN, h\u2032, as well as augmented features. 3.4 Modeling Question-External Comments For task C, in addition to an original question (oriQ) and an external comment (relC), the question which relC commented on is also given (relQ). To incor- porate this extra information, we consider a multi- task learning framework which jointly learns to pre- dict the relationships of the three pairs (oriQ\/relQ, oriQ\/relC, relQ\/relC). Figure 3 shows our framework: the three lower models are separate serialized LSTM-encoders for the three respective object pairs, whereas the upper model is an FNN that takes as input the concatena- tion of the outputs of three encoders, and predicts the relationships for all three pairs.",
            "Figure 3 shows our framework: the three lower models are separate serialized LSTM-encoders for the three respective object pairs, whereas the upper model is an FNN that takes as input the concatena- tion of the outputs of three encoders, and predicts the relationships for all three pairs. More speci\ufb01- cally, the output layer consists of three softmax lay- ers where each one is intended to predict the rela- tionship of one particular pair. For the overall loss function, we combine three separate loss functions using a heuristic weight vec- tor \u03b2 that allocates a higher weight to the main task (oriQ-relC relationship prediction) as follows: L = \u03b21L1 + \u03b22L2 + \u03b23L3 (9) By doing so, we hypothesize that the related tasks can improve the main task by leveraging common- ality among all tasks. 4 Experiments We evaluate our approach on all three cQA tasks. We use the cQA datasets provided by the Semeval 2016 task 1. The cQA data is organized as follows: there are 267 original questions, each question has 10 related question, and each related question has 10 comments.",
            "We use the cQA datasets provided by the Semeval 2016 task 1. The cQA data is organized as follows: there are 267 original questions, each question has 10 related question, and each related question has 10 comments. Therefore, for task A, there are a total number of 26,700 question-comment pairs. For task B, there are 2,670 question-question pairs. For task C, there are 26,700 question-comment pairs. The test dataset includes 50 questions, 500 related ques- tions and 5,000 comments which do not overlap with 1http:\/\/alt.qcri.org\/semeval2016\/task3 the training set. To evaluate the performance, we use mean average precision (MAP) and F1 score. Baseline System: Figure 4 illustrates our base- line systems. The IR-based system is scored by the Google search engine. For each question-comment pair, or question-question pair, we use Google\u2019s rank to calculate the MAP. While there is no training on the target data, we expect that Google used many ex- ternal resources to produce these ranks.",
            "The IR-based system is scored by the Google search engine. For each question-comment pair, or question-question pair, we use Google\u2019s rank to calculate the MAP. While there is no training on the target data, we expect that Google used many ex- ternal resources to produce these ranks. The feature- rich system is that proposed by (Belinkov et al., 2015) in SemEval-2015. In this approach, they com- pute text-based, vector-based, metadata-based and rank-based features from the pre-processed data. The features are used by a linear SVM for com- ment selection. This system includes traditional handcrafted features, and some RNN-based features (word vectors). It also includes the information from the IR system (ranked-based). So we believe it is a strong baseline to compare with our model. RNN encoder: Our system is based on Theano (Bastien et al., 2012; Bergstra et al., 2010). Table 1 gives a list of hyper-parameters we considered. As suggested by (Greff et al., 2015), the hyper-parameters for LSTMs can be tuned independently.",
            "Table 1 gives a list of hyper-parameters we considered. As suggested by (Greff et al., 2015), the hyper-parameters for LSTMs can be tuned independently. We tuned each parameter separately on a development set (split from the training set) and simply picked the best setting. Our experiments show that using word embeddings from Google-News provides modest improvements, but \ufb01xing the embedding degrades performance a lot. Also, using separate parameters for LSTMs is better than sharing. For the optimization method, AdaDelta converged faster, but AdaGrad gives better performance. Note that all the parameters were tuned on Task A, and we simply applied them to Task B and C. This is for saving computation, and also because Task A is more well-de\ufb01ned compared to B and C in terms of dataset size and label balance. 4.1 Preliminary Results Table 2 shows the initial results using the RNN en- coder for different tasks. We observe that the atten- tion model always gets better results than the RNN without attention, especially for task C. However, the RNN model achieves a very low F1 score.",
            "We observe that the atten- tion model always gets better results than the RNN without attention, especially for task C. However, the RNN model achieves a very low F1 score. For task B, it is even worse than the random baseline.",
            "Task A Task B Task C Model Model Model OriQ RelC RelQ RelQ OriQ RelC Figure 3: Joint learning for external comment selection. User Related Q1 Original Q Google Related Q2 Related Q3 Ranked Ranked IR-based System Related C1 Related C2 Related C3 Related C10 Related C1 Related C2 Related C3 Related C10 Vector-based  Features Text-based  features Metadata-Based  Features Rank-Based  Features Feature-Rich based system Feature Extractor SVM words phrases sentences entire Q&A Common String Jaro Second Str Cosine Jaccard Coe \u2026 Task B Task A Task C Figure 4: IR-based system and feature-rich based system.",
            "Task A Task B Task C Model MAP F1 MAP F1 MAP F1 Random 0.4860 0.5004 0.5595 0.4691 0.1383 0.1277 Parallel LSTM 0.6123 0.6091 0.5553 0.4087 0.2413 0.0057 Seq LSTM 0.6175 0.6063 0.5620 0.4299 0.2356 0.0115 w\/ Attention 0.6239 0.6323 0.5723 0.4334 0.2837 0.1449 Table 2: The RNN encoder results for cQA tasks (bold is best).",
            "Embedding init or random, \ufb01x or update Two LSTM shared or not #cells for LSTM 64, 128, 256 # nodes for MLP 128, 256 Optimizer AdaGrad, AdaDelta, SGD learning rate 0.001,0.01,0.1 Regularizer Dropout, L2 regularization Dropout rate 0.0, 0.2, 0.3, 0.4, 0.5 L2 0, 0.001, 0.0001, 0.00001 Table 1: The hyper-parameters we tuned. Terms in bold represent the selected \ufb01nal parameters. We believe the reason is because for task B, there are only 2,670 pairs for training which is very lim- ited training for a reasonable neural network. For task C, we believe the problem is highly imbalanced data. Since the related comments did not directly comment on the original question, more than 90% of the comments are labeled as irrelevant to the original question. The low F1 (with high precision and low recall) means our system tends to label most com- ments as irrelevant.",
            "Since the related comments did not directly comment on the original question, more than 90% of the comments are labeled as irrelevant to the original question. The low F1 (with high precision and low recall) means our system tends to label most com- ments as irrelevant. In the following section, we in- vestigate methods to address these issues. 4.2 Robust Parameter Initialization One way to improve models trained on limited data is to use external data to pretrain the neural network. We therefore considered two different datasets for this task. \u2022 Cross-domain: The Stanford natural language inference (SNLI) corpus (Bowman et al., 2015) has a huge amount of cleaned premise and hy- pothesis pairs. Unfortunately the pairs are for a different task. The relationship between the premise and hypothesis may be similar to the relation between questions and comments, but may also be different. \u2022 In-domain: since task A seems has reason-",
            "able performance, and the network is also well- trained, we could use it directly to initialize task B. To utilize the data, we \ufb01rst trained the model on each auxiliary data (SNLI or Task A) and then removed the softmax layer. After that, we retrain the network using the target data with a softmax layer that was randomly initialized. For task A, the SNLI cannot improve MAP or F1 scores. Actually it slightly hurts the performance. We surmise that it is probably because the domain is different. Further investigation is needed: for exam- ple, we could only use the parameter for embedding layers etc. For task B, the SNLI yields a slight im- provement on MAP (0.2%), and Task A could give (1.2%) on top of that. No improvement was ob- served on F1. For task C, pretraining by task A is also better than using SNLI (task A is 1% better than the baseline, while SNLI is almost the same).",
            "No improvement was ob- served on F1. For task C, pretraining by task A is also better than using SNLI (task A is 1% better than the baseline, while SNLI is almost the same). In summary, the in-domain pretraining seems bet- ter, but overall, the improvement is less than we ex- pected, especially for task B, which only has very limited target data. We will not make a conclusion here since more investigation is needed. 4.3 Multitask Learning As mentioned in Section 3.4, we also explored a multitask learning framework that jointly learns to predict the relationships of all three tasks. We set 0.8 for the main task (task C) and 0.1 for the other auxiliary tasks. The MAP score did not improve, but F1 increases to 0.1617. We believe this is be- cause other tasks have more balanced labels, which improves the shared parameters for task C. 4.4 Augmented data There are many sources of external question-answer pairs that could be used in our tasks.",
            "We believe this is be- cause other tasks have more balanced labels, which improves the shared parameters for task C. 4.4 Augmented data There are many sources of external question-answer pairs that could be used in our tasks. For exam- ple: WebQuestion (was introduced by the authors of SEMPRE system (Berant et al., 2013)) and The SimpleQuestions dataset 2. All of them are positive examples for our task and we can easily create neg- ative examples from it. Initial experiments indicate that it is very easy to over\ufb01t these obvious negative examples. We believe this is because our negative 2http:\/\/fb.ai\/babi. examples are non-informative for our task and just introduce noise. Since the external data seems to hurt the perfor- mance, we try to use the in-domain pairs to enhance task B and task C. For task B, if relative question 1 (rel1) and relative question 2 (rel2) are both rele- vant to the original question, then we add a positive sample (rel1, rel2, 1).",
            "If either rel1 and rel2 is ir- relevant and the other is relevant, we add a negative sample (rel1, rel2, 0). After doing this, the samples of task B increase from 2, 670 to 11, 810. By apply- ing this method, the MAP score increased slightly from 0.5723 to 0.5789 but the F1 score improved from 0.4334 to 0.5860. For task C, we used task A\u2019s data directly. The results are very similar with a slight improvement on MAP, but large improvement on F1 score from 0.1449 to 0.2064. 4.5 Augmented features To further enhance the system, we incorporate a one hot vector of the original IR ranking as an additional feature into the FNN classi\ufb01er. Table 3 shows the results. In comparing the models with and without augmented features, we can see large improvement for task B and C. The F1 score for task A degrades slightly but MAP improves. This might be because task A already had a substantial amount of training data.",
            "Table 3 shows the results. In comparing the models with and without augmented features, we can see large improvement for task B and C. The F1 score for task A degrades slightly but MAP improves. This might be because task A already had a substantial amount of training data. 4.6 Comparison with Other Systems Table 4 gives the \ufb01nal comparison between differ- ent models (we only list the MAP score because it is the of\ufb01cial score for the challenge). Since the two baseline models did not use any additional data, in this table our system was also restricted to the pro- vided training data. For task A, we can see that if there is enough training data our single system al- ready performs better than a very strong feature-rich based system. For task B, since only limited train- ing data is given, both feature-rich based system and our system are worse than the IR system. For task C, our system also got comparable results with the feature-rich based system. If we do a simple system combination (average the rank score) between our system and the IR system, the combined system will",
            "Task A Task B Task C Model MAP F1 MAP F1 MAP F1 w\/ Attention 0.6239 0.6323 0.5723 0.4334 0.2837 0.1449 w\/ Attention + aug features 0.6385 0.6218 0.6585 0.5382 0.3236 0.1963 Table 3: cQA task results with augmented features (bold is best). give large gains on tasks B and C3. This implies that our system is complimentary with the IR system. Task A Task B Task C Model MAP MAP MAP IR 0.538 0.714 0.307 Attention 0.639 0.659 0.324 Feature-Rich & IR 0.632 0.685 0.339 Attention & IR 0.639 0.717 0.394 Table 4: Compared with other systems (bold is best). 5 Analysis of Attention Mechanism In addition to quantitative analysis, it is natural to qualitatively evaluate the performance of the atten- tion mechanism by visualizing the weight distribu- tion of each instance.",
            "5 Analysis of Attention Mechanism In addition to quantitative analysis, it is natural to qualitatively evaluate the performance of the atten- tion mechanism by visualizing the weight distribu- tion of each instance. We randomly picked several instances from the test set in task A, for which the sentence lengths are more moderate for demonstra- tion. These examples are shown in Figure 5, and categorized into short, long, and noisy sentences for discussion. A darker blue patch refers to a larger weight relative to other words in the same sentence. 5.1 Short Sentences Figure 5a illustrates two cQA examples whose ques- tions are relatively short. The comments corre- sponding to these questions are \u201c...snorkeling two days ago off the coast of dukhan...\u201d and \u201cthe doha international airport...\u201d. We can observe that our model successfully learns to focus on the most rep- resentative part of the question pertaining to classi- fying the relationship, which is \u201dplace for snorkel- ing\u201d for the \ufb01rst example and \u201cplace can ... visited in qatar\u201d for the second example.",
            "We can observe that our model successfully learns to focus on the most rep- resentative part of the question pertaining to classi- fying the relationship, which is \u201dplace for snorkel- ing\u201d for the \ufb01rst example and \u201cplace can ... visited in qatar\u201d for the second example. 3The feature-rich based system was already combined with the IR system) 5.2 Long Sentences In Figure 5b, we investigate two examples with longer questions, which both contain 63 words. In- terestingly, the distribution of weights does not be- come more uniform; the model still focuses atten- tion on a small number of hot words, for example, \u201cpuppy dog for ... mall\u201d and \u201chectic driving in doha ... car insurance ... quite costly\u201d. Additionally, some words that appear frequently but carry little in- formation for classi\ufb01cation are assigned very small weights, such as I\/we\/my, is\/am, like, and to. 5.3 Noisy Sentence Due to the open nature of cQA forums, some con- tent is noisy. Figure 5c is an example with excessive usage of question marks.",
            "5.3 Noisy Sentence Due to the open nature of cQA forums, some con- tent is noisy. Figure 5c is an example with excessive usage of question marks. Again, our model exhibits its robustness by allocating very low weights to the noise symbols and therefore excludes the noninfor- mative content. 6 Conclusion In this paper, we demonstrate that a general RNN en- coder framework can be applied to community ques- tion answering tasks. By adding a neural attention mechanism, we showed quantitatively and qualita- tively that attention can improve the RNN encoder framework. To deal with a more realistic scenario, we expanded the framework to incorporate metadata as augmented inputs to a FNN classi\ufb01er, and pre- trained models on larger datasets, increasing both stability and performance. Our model is consistently better than or comparable to a strong feature-rich baseline system, and is superior to an IR-based sys- tem when there is a reasonable amount of training data. Our model is complimentary with an IR-based system that uses vast amounts of external resources but trained for general purposes. By combining the two systems, it exceeds the feature-rich and IR- based system in all three tasks.",
            "(a) short sentences (b) long sentences (c) noisy sentence Figure 5: Visualization of attention mechanism on short, long, and noisy sentences. Moreover, our approach is also language indepen- dent. We have also performed preliminary experi- ments on the Arabic portion of the SemEval-2016 cQA task. The results are competitive with a hand- tuned strong baseline from SemEval-2015. Future work could proceed in two directions: \ufb01rst, we can enrich the existing system by incorporat- ing available metadata and preprocessing data with morphological normalization and out-of-vocabulary mappings; second, we can reinforce our model by carrying out word-by-word and history-aware at- tention mechanisms instead of attending only when reading the last word. References [Bahdanau et al.2014] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.",
            "2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473. [Bastien et al.2012] Fr\u00b4ed\u00b4eric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud Bergeron, Nicolas Bouchard, and Yoshua Ben-",
            "gio. 2012. Theano: new features and speed im- provements. Deep Learning and Unsupervised Fea- ture Learning NIPS 2012 Workshop. [Belinkov et al.2015] Yonatan Belinkov, Mitra Mo- htarami, Scott Cyphers, and James Glass. 2015. Vectorslu: A continuous word vector approach to answer selection in community question answering systems. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval, vol- ume 15. [Berant et al.2013] J. Berant, A. Chou, R. Frostig, and P. Liang. 2013. Semantic parsing on freebase from question-answer pairs. In EMNLP. [Bergstra et al.2010] James Bergstra, Olivier Breuleux, Fr\u00b4ed\u00b4eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde- Farley, and Yoshua Bengio. 2010. Theano: a CPU and GPU math expression compiler.",
            "2010. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scienti\ufb01c Computing Conference (SciPy), June. Oral Presentation. [Bowman et al.2015] S. Bowman, G. Angeli, C. Potts, and C. Manning. 2015. A large annotated corpus for learning natural language inference. In EMNLP. [Cho et al.2014] Kyunghyun Cho, Bart Van Merri\u00a8enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learn- ing phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078. [Feng et al.2015] Minwei Feng, Bing Xiang, Michael R Glass, Lidan Wang, and Bowen Zhou. 2015. Apply- ing deep learning to answer selection: A study and an open task. arXiv preprint arXiv:1508.01585.",
            "2015. Apply- ing deep learning to answer selection: A study and an open task. arXiv preprint arXiv:1508.01585. [Greff et al.2015] Klaus Greff, Rupesh Kumar Srivastava, Jan Koutn\u00b4\u0131k, Bas R. Steunebrink, and J\u00a8urgen Schmid- huber. 2015. LSTM: A search space odyssey. CoRR, abs\/1503.04069. [Grundstr\u00a8om and Nugues2014] Jakob Grundstr\u00a8om and Pierre Nugues. 2014. Using syntactic features in an- swer reranking. In AAAI 2014 Workshop on Cognitive Computing for Augmented Human Intelligence, pages 13\u201319. [Harabagiu and Hickl2006] Sanda Harabagiu and An- drew Hickl. 2006. Methods for using textual entail- ment in open-domain question answering.",
            "[Harabagiu and Hickl2006] Sanda Harabagiu and An- drew Hickl. 2006. Methods for using textual entail- ment in open-domain question answering. In Proceed- ings of the 21st International Conference on Compu- tational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 905\u2013912. Association for Computational Linguistics. [Jeon et al.2006] Jiwoon Jeon, W Bruce Croft, Joon Ho Lee, and Soyeon Park. 2006. A framework to pre- dict the quality of answers with non-textual features. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in in- formation retrieval, pages 228\u2013235. ACM. [Nakov et al.2015] R. Nakov, L. Marquez, W. Magdy, A. Moschitti, and J. Glass. 2015. Semeval-2015 task 3: Answer selection in community question answer- ing. In Proc. SamEval, pages 282\u2013287.",
            "2015. Semeval-2015 task 3: Answer selection in community question answer- ing. In Proc. SamEval, pages 282\u2013287. [Rockt\u00a8aschel et al.2015] Tim Rockt\u00a8aschel, Edward Grefenstette, Karl Moritz Hermann, Tom\u00b4a\u02c7s Ko\u02c7cisk`y, and Phil Blunsom. 2015. Reasoning about en- tailment with neural attention. arXiv preprint arXiv:1509.06664. [Shah and Pomerantz2010] Chirag Shah and Jefferey Pomerantz. 2010. Evaluating and predicting answer quality in community qa. In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, pages 411\u2013418. ACM. [Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks.",
            "ACM. [Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural informa- tion processing systems, pages 3104\u20133112. [Tran et al.2015] Quan Hung Tran, Vu Tran, Tu Vu, Minh Le Nguyen, and Son Bao Pham. 2015. Jaist: Com- bining multiple features for answer selection in com- munity question answering. In Proceedings of the 9th International Workshop on Semantic Evaluation, Se- mEval, volume 15. [Wang and Nyberg2015] Di Wang and Eric Nyberg. 2015. A long short-term memory model for answer sentence selection in question answering. In ACL. [Zhai and Lafferty2004] C. Zhai and J. Lafferty. 2004. A study of smoothing methods for language models ap- plied to information retrieval. In ACM Trans. Inf. Syst."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1603.07044.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 7166.999816894531,
    "avg_doclen_est": 183.76922607421875
}
