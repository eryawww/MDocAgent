{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Cooperative Learning of Disjoint Syntax and Semantics Serhii Havrylov \u2217 ILCC, University of Edinburgh \/ Edinburgh, UK s.havrylov@ed.ac.uk Germ\u00b4an Kruszewski & Armand Joulin Facebook AI Research {germank,ajoulin}@fb.com Abstract There has been considerable attention devoted to models that learn to jointly infer an ex- pression\u2019s syntactic structure and its seman- tics. Yet, Nangia and Bowman (2018) has re- cently shown that the current best systems fail to learn the correct parsing strategy on math- ematical expressions generated from a sim- ple context-free grammar. In this work, we present a recursive model inspired by Choi et al. (2018) that reaches near perfect accu- racy on this task. Our model is composed of two separated modules for syntax and se- mantics. They are cooperatively trained with standard continuous and discrete optimisation schemes. Our model does not require any lin- guistic structure for supervision, and its re- cursive nature allows for out-of-domain gen- eralisation.",
            "They are cooperatively trained with standard continuous and discrete optimisation schemes. Our model does not require any lin- guistic structure for supervision, and its re- cursive nature allows for out-of-domain gen- eralisation. Additionally, our approach per- forms competitively on several natural lan- guage tasks, such as Natural Language Infer- ence and Sentiment Analysis. 1 Introduction Standard linguistic theories propose that natural language is structured as nested constituents or- ganised in the form of a tree (Partee et al., 1990). However, most popular models, such as the Long Sort-Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), process text without im- posing a grammatical structure. To bridge this gap between theory and practice models that process linguistic expressions in a tree-structured manner have been considered in recent work (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015; Bowman et al., 2016). These tree-based models explicitly require access to the syntactic structure for the text, which is not entirely satisfactory.",
            "These tree-based models explicitly require access to the syntactic structure for the text, which is not entirely satisfactory. Indeed, parse tree level supervision requires a signi\ufb01cant amount of annotations from expert lin- \u2217Work done while the author was an intern at Facebook AI Research. guists. These trees have been annotated with dif- ferent goals in mind than the tasks we are using them for. Such discrepancy may result in a de- terioration of the performance of models relying on them. Recently, several attempts were made to learn these models without explicit supervi- sion for the parser (Yogatama et al., 2016; Mail- lard et al., 2017; Choi et al., 2018). However, Williams et al. (2018a) has recently shown that the structures learned by these models cannot be as- cribed to discovering meaningful syntactic struc- ture. These models even fail to learn the simple context-free grammar of nested mathematical op- erations (Nangia and Bowman, 2018). In this work, we present an extension of Choi et al.",
            "These models even fail to learn the simple context-free grammar of nested mathematical op- erations (Nangia and Bowman, 2018). In this work, we present an extension of Choi et al. (2018), that successfully learns these simple grammars while preserving competitive perfor- mance on several standard linguistic tasks. Con- trary to previous work, our model makes a clear distinction between the parser and the composi- tional function. These two modules are trained with different algorithms, cooperating to build a semantic representation that optimises the objec- tive function. The parser\u2019s goal is to generate a tree structure for the sentence. The composi- tional function follows this structure to produce the sentence representation. Our model contains a continuous component, the compositional func- tion, and a discrete one, the parser. The whole system is trained end-to-end with a mix of rein- forcement learning and gradient descent. Droz- dov and Bowman (2017) has noticed the dif\ufb01culty of mixing these two optimisation schemes without one dominating the other.",
            "The whole system is trained end-to-end with a mix of rein- forcement learning and gradient descent. Droz- dov and Bowman (2017) has noticed the dif\ufb01culty of mixing these two optimisation schemes without one dominating the other. This typically leads to the \u201ccoadaptation problem\u201d where the parser sim- ply follows the compositional function and fails to produce meaningful syntactic structures. In this work, we show that this pitfall can be avoided by synchronising the learning paces of the two optimisation schemes. This is achieved by com- arXiv:1902.09393v2  [cs.CL]  29 May 2019",
            "bining several recent advances in reinforcement learning. First, we use input-dependent control variates to reduce the variance of our gradient esti- mates (Ross, 1997). Then, we apply multiple gra- dient steps to the parser\u2019s policy while controlling for its learning pace using the Proximal Policy Op- timization (PPO) of Schulman et al. (2017). The code for our model is publicly available1. 2 Preliminaries In this section, we present existing works on Re- cursive Neural Networks and their training in the absence of supervision on the syntactic structures. 2.1 Recursive Neural Networks A Recursive Neural Network (RvNN) has its architecture de\ufb01ned by a directed acyclic graph (DAG) given alongside with an input se- quence (Goller and Kuchler, 1996). RvNNs are commonly used in NLP to generate sentence rep- resentation that leverages available syntactic infor- mation, such as a constituency or a dependency parse trees (Socher et al., 2011).",
            "RvNNs are commonly used in NLP to generate sentence rep- resentation that leverages available syntactic infor- mation, such as a constituency or a dependency parse trees (Socher et al., 2011). Given an input sequence and its associ- ated DAG, a RvNN processes the sequence by ap- plying a transformation to the representations of the tokens lying on the lowest levels of the DAG. This transformation, or compositional function, merges these representations into representations for the nodes on the next level of the DAG. This process is repeated recursively along the graph structure until the top-level nodes are reached. In this work, we assume that the compositional func- tion is the same for every node in the graph. Tree-LSTM. We focus on a speci\ufb01c type of RvNNs, the tree-based long short-term memory network (Tree-LSTM) of Tai et al. (2015) and Zhu et al. (2015).",
            "Tree-LSTM. We focus on a speci\ufb01c type of RvNNs, the tree-based long short-term memory network (Tree-LSTM) of Tai et al. (2015) and Zhu et al. (2015). Its compositional function general- izes the LSTM cell of Hochreiter and Schmidhu- ber (1997) to tree-structured topologies, i.e., \uf8ee \uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 z i fl fr o \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fa\uf8fb = \uf8ee \uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 tanh \u03c3 \u03c3 \u03c3 \u03c3 \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fa\uf8fb \u0012 R \u0014hl hr \u0015 + b \u0013 , cp = z \u2299i + cl \u2299fl + cr \u2299fr, hp = tanh(cp) \u2299o, 1https:\/\/github.com\/facebookresearch\/ latent-treelstm where \u03c3 and tanh are the sigmoid and hyperbolic tangent functions.",
            "Tree-LSTM cell is differen- tiable with respect to its recursion matrix R, bias b and its input. The gradients of a Tree-LSTM can thus be computed with backpropagation through structure (BPTS) (Goller and Kuchler, 1996). 2.2 Learning with RvNNs A tree-based RvNN is a function f\u03b8 parameter- ized by a d dimensional vector \u03b8 that predicts an output y given an input x and a tree t. Given a dataset D of N triplets (x, t, y), the parameters of the RvNN are learned with the following minimi- sation problem: min \u03b8\u2208Rd 1 N X (x,t,y)\u2208D \u2113(f\u03b8(x, t), y), (1) where \u2113is a logistic regression function. These models need an externally provided parsing tree for each input sentence during both training and evaluation. Alternatives, such as the shift-reduce- based SPINN model of Bowman et al. (2016), learn an internal parser from the given trees. While these solutions do not need external trees during evaluation, they still require tree level annotations for training.",
            "Alternatives, such as the shift-reduce- based SPINN model of Bowman et al. (2016), learn an internal parser from the given trees. While these solutions do not need external trees during evaluation, they still require tree level annotations for training. More recent work has focused on learning a latent parser with no direct supervision. 2.3 Latent tree models Latent tree models aim at jointly learning the com- positional function f\u03b8 and a parser without super- vision on the syntactic structures (Yogatama et al., 2016; Maillard et al., 2017; Choi et al., 2018). The latent parser is de\ufb01ned as a parametric probabil- ity distribution over trees conditioned on the in- put sequence.",
            "The latent parser is de\ufb01ned as a parametric probabil- ity distribution over trees conditioned on the in- put sequence. The parameters of this tree distribu- tion p\u03c6(.|x) are represented by a vector \u03c6. Given a dataset D of pairs of input sequences x and out- puts y, the parameters \u03b8 and \u03c6 are jointly learned by minimising the following objective function: min \u03b8,\u03c6 L(\u03b8, \u03c6) = 1 N X (x,y) \u2113(E\u03c6[f\u03b8(x, t)], y), (2) where E\u03c6 is the expectation with respect to the p\u03c6(.|x) distribution. Directly minimising this objective function is often dif\ufb01cult due to ex- pensive marginalisation of the unobserved trees. Hence, when \u2113is a convex function (e.g. cross entropy of an exponential family) usually an up- per bound of Eq. (2) can be derived by applying",
            "Jensen\u2019s inequality: \u02c6L(\u03b8, \u03c6) = 1 N X (x,y) E\u03c6[\u2113(f\u03b8(x, t), y)]. (3) Learning a distribution over a set of discrete items involves a discrete optimisation scheme. For ex- ample, the RL-SPINN model of Yogatama et al. (2016) uses a mix of gradient descent for \u03b8 and REINFORCE for \u03c6 (Williams et al., 2018a). Drozdov and Bowman (2017) has recently ob- served that this optimisation strategy tends to pro- duce poor parsers, e.g., parsers that only generate left-branching trees. The effect, called the coad- aptation issue, is caused by both bias in the pars- ing strategy and a difference in convergence paces of continuous and discrete optimisers. Typically, the parameters \u03b8 are learned more rapidly than \u03c6. This limits the exploration of the search space to parsing strategies similar to those found at the be- ginning of the training. 2.3.1 Gumbel Tree-LSTM In their Gumbel Tree-LSTM model, Choi et al.",
            "2.3.1 Gumbel Tree-LSTM In their Gumbel Tree-LSTM model, Choi et al. (2018) propose an alternative parsing strategy to avoid the coadaptation issue. Their parser incre- mentally merges a pair of consecutive constituents until a single one remains. This strategy reduces the bias towards certain tree con\ufb01gurations ob- served with RL-SPINN. Each word i of the input sequence is represented by an embedding vector. A leaf transformation maps this vector to pair of vectors r0 i =(h0 i , c0 i ). We considered three types of leaf transforma- tions: af\ufb01ne transformation, LSTM and bidirec- tional LSTM. The resulting representations form the initial states of the Tree-LSTM. In the ab- sence of supervision, the tree is built in a bottom- up fashion by recursively merging consecutive constituents (i, i + 1) based on merge-candidate scores.",
            "The resulting representations form the initial states of the Tree-LSTM. In the ab- sence of supervision, the tree is built in a bottom- up fashion by recursively merging consecutive constituents (i, i + 1) based on merge-candidate scores. On each level k of the bottom-up deriva- tion, the merge-candidate score of the pair (i, i+1) is computed as follow: sk(i) = \u27e8q, Tree-LSTM(rk i , rk i+1)\u27e9, where q is a trainable query vector and rk i is the constituent representation at position i after k mergings. We merge a pair (i\u2217, i\u2217+ 1) sam- pled from the Categorical distribution built on the merge-candidate scores. The representations of the constituents are then updated as follow: rk+1 i = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 rk i , i < i\u2217, Tree-LSTM(rk i , rk i+1) i = i\u2217, rk i+1 i > i\u2217. This procedure is repeated until one constituent re- mains.",
            "This procedure is repeated until one constituent re- mains. Its hidden state is the input sentence rep- resentation. This procedure is non-differentiable. Choi et al. (2018) use an approximation based on the Gumbel-Softmax distribution (Maddison et al., 2016; Jang et al., 2016) and the reparametrization trick (Kingma and Welling, 2013). This relaxation makes the problem differen- tiable at the cost of a bias in the gradient esti- mates (Jang et al., 2016). This difference between the real objective function and their approxima- tion could explain why their method cannot re- cover simple context-free grammars (Nangia and Bowman, 2018). We investigate this question by proposing an alternative optimisation scheme that directly aims for the correct objective function. 3 Our model We consider the problem de\ufb01ned in Eq. (3) to jointly learn a composition function and an in- ternal parser. Our model is composed of the parser of Choi et al. (2018) and the Tree-LSTM for the composition function.",
            "3 Our model We consider the problem de\ufb01ned in Eq. (3) to jointly learn a composition function and an in- ternal parser. Our model is composed of the parser of Choi et al. (2018) and the Tree-LSTM for the composition function. As suggested in past work (Mnih et al., 2016; Schulman et al., 2017), we added an entropy H over the tree distribution to the objective function: min \u03b8, \u03c6 \u02c6L(\u03b8, \u03c6) \u2212\u03bb X x H(t | x), (4) where \u03bb > 0. This regulariser improves explo- ration by preventing early convergence to a subop- timal deterministic parsing strategy. The new ob- jective function is differentiable with respect to \u03b8, but not \u03c6, the parameters of the parser. Learning \u03b8 follows the same procedure with BPTS as if the tree would be externally given. In the rest of this section, we discuss the opti- mization of the parser and a cooperative training strategy to reduce the coadaptation issue.",
            "Learning \u03b8 follows the same procedure with BPTS as if the tree would be externally given. In the rest of this section, we discuss the opti- mization of the parser and a cooperative training strategy to reduce the coadaptation issue. 3.1 Unbiased gradient estimation We cast the training of the parser as a reinforce- ment learning problem. The parser is an agent whose reward function is the negative of the loss function de\ufb01ned in Eq. (3). Its action space is the",
            "space of binary trees. The agent\u2019s policy is a prob- ability distribution over binary trees that decom- poses as a sequence of K merging actions: p\u03c6(t|x) = K Y k=0 \u03c0\u03c6(ai k|rk), (5) where rk = (rk 0, . . . , rk K\u2212k). The loss func- tion is optimised with respect to \u03c6 with REIN- FORCE (Williams, 1992). REINFORCE requires a considerable number of random samples to ob- tain a gradient estimate with a reasonable level of variance. This number is positively correlated with the size of the search space, which is expo- nentially large in the case of binary trees. We con- sider several extensions of REINFORCE to cir- cumvent this problem. Variance reduction. An alternative solution to increasing the number of samples is the control variates method (Ross, 1997). It takes advantage of random variables with known expected values and positive correlation with the quantity whose expectation is tried to be estimated.",
            "Variance reduction. An alternative solution to increasing the number of samples is the control variates method (Ross, 1997). It takes advantage of random variables with known expected values and positive correlation with the quantity whose expectation is tried to be estimated. Given an input-output pair (x, y) and tree t sampled from p\u03c6(t|x) , let\u2019s de\ufb01ne the random variable G as: G(t) = \u2113(f\u03b8(x, t), y)\u2202log p\u03c6(t|x) \u2202\u03c6 . (6) According to REINFORCE, calculating the gra- dient with respect to \u03c6 for the pair (x, y) is then equivalent to determining the unknown mean of the random variable G(t)2. Let\u2019s assume there is a control variate, i.e., a random variable b(t) that positively correlates with G and has known expected value with respect to p\u03c6(.|x). Given N samples of the G(t) and the control variate b(t), the new gradient estimator is: GCV = Ep\u03c6(t|x)[b(t)]+ 1 N \" N X i=1 (G(ti) \u2212b(ti)) # .",
            "Given N samples of the G(t) and the control variate b(t), the new gradient estimator is: GCV = Ep\u03c6(t|x)[b(t)]+ 1 N \" N X i=1 (G(ti) \u2212b(ti)) # . A popular control variate, or baseline, used in REINFORCE is the moving average of recent rewards multiplied by the score function (Ross, 1997): b(t) = c\u2207\u03c6 log p\u03c6(t|x). It has a zero mean under the p\u03c6(.|x) distribution and it positively correlates with G(t). 2Note that while we are computing the gradients using \u2113, we could also directly optimise the parser with respect to downstream accuracy. Surrogate loss. REINFORCE often is imple- mented via a surrogate loss de\ufb01ned as follow: \u02c6Et [r\u03c6(t)\u2113(f\u03b8(x, t), y)] , (7) where \u02c6Et is the empirical average over a \ufb01nite batch of samples and r\u03c6(t) = p\u03c6(t|x) p\u03c6old(t|x) is the prob- ability ratio with \u03c6old standing for the parameters before the update.",
            "Input-dependent baseline. The moving aver- age baseline cannot detect changes in rewards caused by structural differences in the inputs. In our case, a long arithmetic expression is much harder to parse than a short one, systematically leading to their lower rewards. This structural dif- ferences in the rewards aggravate the credit as- signment problem by encouraging REINFORCE to discard actions sampled for longer sequences even though there might be some subsequences of actions that produce correct parsing subtrees. A solution is to make the baseline input- dependent. In particular, we use the self-critical training (SCT) baseline of Rennie et al. (2017), de\ufb01ned as: b(t, x) = c\u03b8,\u03c6(x)\u2207\u03c6 log p\u03c6(t | x), where c\u03b8,\u03c6 is the reward obtained with the policy used at test time, i.e., \u02c6t = arg max p\u03c6(t|x). This control variate has a zero mean under the p\u03c6(t|x) distribution and correlates positively with the gra- dients. Computing the arg max of a policy among all possible binary trees has exponential complex- ity.",
            "This control variate has a zero mean under the p\u03c6(t|x) distribution and correlates positively with the gra- dients. Computing the arg max of a policy among all possible binary trees has exponential complex- ity. We replace it with a simpler greedy decoding, i.e, a tree t is selected by following a sequence of greedy actions \u02c6ak: \u02c6ak = arg max \u03c0\u03c6(ak | \u02c6rk). This approximation is very ef\ufb01cient and comput- ing the baseline requires only one additional for- ward pass. Gradient normalization. We empirically ob- serve signi\ufb01cant \ufb02uctuations in the gradient norms. This creates instability that can not be reduced by additive terms, such as the input- dependent baselines. A solution is to divide the gradients by a coarse approximation of their norm, e.g., a running estimate of the reward standard de- viation (Mnih and Gregor, 2014). This trick en- sures that the rewards remain approximately in the unit ball, making the learning process less sensi- tive to steep changes in the loss.",
            "3.2 Synchronizing syntax and semantics learning with PPO The gradients of the loss function from the Eq. (4) are calculated using two different schemes, BPST for the composition function parameters \u03b8 and RE- INFORCE for the parser parameters \u03c6. Then, both are updated with SGD. The estimate of the gradi- ent with respect to \u03c6 has higher variance compared to the estimate with respect to \u03b8. Hence, using the same learning rate schedule does not necessar- ily correspond to the same real pace of learning. It is \u03c6 parameters that are harder to optimise, so to improve training stability and convergence it is reasonable to aim for such updates that does not change the policy too much or too little. A simple yet effective solution is the Proximal Policy Opti- mization (PPO) of Schulman et al. (2017).",
            "A simple yet effective solution is the Proximal Policy Opti- mization (PPO) of Schulman et al. (2017). It con- siders the next surrogate loss: \u02c6Et \u0002 max \b r\u03c6(t)\u2113(f\u03b8(x, t), y) , rc \u03c6(t)\u2113(f\u03b8(x, t), y) \t\u0003 , Where rc \u03c6(t) = clip (r\u03c6(t), 1 \u2212\u03f5, 1 + \u03f5) and \u03f5 is a real number in (0; 0.5]. The \ufb01rst argument of the max is the surrogate loss for REINFORCE. The clipped ratio in the second argument disincen- tivises the optimiser from performing updates re- sulting in large tree probability changes. With this, the policy parameters can be optimised with re- peated K steps of SGD to ensure a similar \u201cpace\u201d of learning between the parser and the composi- tional function. 4 Related work Besides the works mentioned in Sec. 2 and Sec. 3, there is a vast literature on learning latent parsers.",
            "4 Related work Besides the works mentioned in Sec. 2 and Sec. 3, there is a vast literature on learning latent parsers. Early connectionist work in inferring context-free grammars proposed stack-augmented models and relied on explicit supervision on the strings that belonged to the target language and those that did not (Giles et al., 1989; Sun, 1990; Das et al., 1992; Mozer and Das, 1992). More recently, new stack- augmented models were shown to learn latent grammars from positive evidence alone (Joulin and Mikolov, 2015). In parallel to these, other sta- tistical approaches were proposed to automatically induce grammars from unparsed text (Sampson, 1986; Magerman and Marcus, 1990; Carroll and Charniak, 1992; Brill, 1993; Klein and Manning, 2002). Our work departs from these approaches in that we aim at learning a latent grammar in the context of performing some given task. Socher et al.",
            "Our work departs from these approaches in that we aim at learning a latent grammar in the context of performing some given task. Socher et al. (2011) uses a surrogate auto- encoder objective to search for a constituency structure, merging nodes greedily based on the re- construction loss. Maillard et al. (2017) de\ufb01nes a relaxation of a CYK-like chart parser that is trained for a particular task. A similar idea is in- troduced in Le and Zuidema (2015) where an au- tomatic parser prunes the chart to reduce the over- all complexity of the algorithm. Another strat- egy, similar in nature, has been recently proposed by Corro and Titov (2018), where Gumbel noise is used with differentiable dynamic programming to generate dependency trees. In contrast, Yo- gatama et al. (2016) learns a Shift-Reduce parser using reinforcement learning. Maillard and Clark (2018) further proposes a beam search strategy to overcome learning trivial trees.",
            "In contrast, Yo- gatama et al. (2016) learns a Shift-Reduce parser using reinforcement learning. Maillard and Clark (2018) further proposes a beam search strategy to overcome learning trivial trees. On a different vein, Vlad Niculae (2018) proposes a quadratic penalty term over the posterior distribution of non- projective dependency trees to enforce sparsity of the relaxation. Finally, there is a large body of work in Reinforcement Learning that aims at dis- covering how to combine elementary modules to solve complex tasks (Singh, 1992; Chang et al., 2018; Sahni et al., 2017). Due to the limited space, we will not discuss them in further details. 5 Experiments We conducted experiments on three different tasks: evaluating mathematical expressions on the ListOps dataset (Nangia and Bowman, 2018), sentiment analysis on the SST dataset (Socher et al., 2013) and natural language inference task on the SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018b) datasets. Technical details.",
            "Technical details. For ListOps, we follow the experimental protocol of Nangia and Bowman (2018), i.e., a 128 dimensional model and a ten- way softmax classi\ufb01er. However, we replace their multi-layer perceptron (MLP) by a linear classi- \ufb01er. The validation set is composed of 1k ex- amples randomly selected from the training set. For SST and NLI, we follow the setup of Choi et al. (2018): we initialise the word vectors with GloVe300D (Pennington et al., 2014) and train an MLP classi\ufb01er on the sentence representations. The hyperparameters are selected on the valida- tion set using 5 random seeds for each con\ufb01gura- tion. Our hyperparameters are the learning rate, weight decay, the regularisation parameter \u03bb, the leaf transformations, variance reduction hyperpa-",
            "No baseline Moving average Self critical No PPO PPO No PPO PPO No PPO PPO min 61.7 61.4 61.7 59.4 63.7 98.2 max 70.1 76.6 74.3 96.0 64.1 99.6 mean \u00b1 std 66.2 \u00b13.2 66.5 \u00b15.9 65.5 \u00b1 4.7 67.5 \u00b114.3 64.0 \u00b10.1 99.2 \u00b10.5 Table 1: Accuracy on ListOps test set for our model with three different baselines, with and without PPO. We use K = 15 for PPO. Model Accuracy LSTM* 71.5\u00b11.5 RL-SPINN* 60.7\u00b12.6 Gumbel Tree-LSTM* 57.6\u00b12.9 Ours 99.2\u00b10.5 Table 2: Accuracy on the ListOps dataset. All models have 128 dimensions. Results for models with * are taken from Nangia and Bowman (2018).",
            "All models have 128 dimensions. Results for models with * are taken from Nangia and Bowman (2018). rameters and the number of updates K in PPO. We use an adadelta optimizer (Zeiler, 2012). 5.1 ListOps The ListOps dataset probes the syntax learning ability of latent tree models (Nangia and Bow- man, 2018). It is designed to have a single cor- rect parsing strategy that a model must learn in order to succeed. It is composed of pre\ufb01x arith- metic expressions and the goal is to predict the numerical output associated with the evaluation of the expression. The sequences are made of integers in [0, 9] and 4 operations: MIN, MAX, MED and SUM MOD. The output is an integer in the range [0, 9]. For example, the expression [MIN 2 [MAX 0 1] [MIN 6 3 ] 5 ] is mapped to the output 1. The ListOps task is thus a sequence classi\ufb01cation problem with 10 classes.",
            "For example, the expression [MIN 2 [MAX 0 1] [MIN 6 3 ] 5 ] is mapped to the output 1. The ListOps task is thus a sequence classi\ufb01cation problem with 10 classes. There are 90k training examples and 10k test ex- amples. It is worth mentioning that the underly- ing semantic of operations and symbols is not pro- vided. In other words, a model has to infer from examples that [MIN 0 1] = 0. As shown in Table 2, the current leading la- tent tree models are unable to learn the correct parsing strategy on ListOps (Nangia and Bowman, 2018). They even achieve performance worse than purely sequential recurrent networks. On the other hand, our model achieves near perfect accuracy on this task, suggesting that our model is able to dis- cover the correct parsing strategy. Our model dif- fers in several ways from the Gumbel Tree-LSTM of Choi et al. (2018) that could explain this gap in performance.",
            "Our model dif- fers in several ways from the Gumbel Tree-LSTM of Choi et al. (2018) that could explain this gap in performance. In the rest of this section, we per- form an ablation study on our model to understand the importance of each of these differences. Impact of the baseline and PPO. We report the impact of our design choices on the performance in Table 1. Our model without baseline nor PPO is vanilla REINFORCE. The baselines only im- prove performance when PPO is used. Further- more, these ablated models without PPO perform on-par with the RL-SPINN model (see Table 2). This con\ufb01rms our expectations for models that fail to synchronise syntax and semantics learning. Interestingly, using PPO has a positive impact on both baselines, but accuracy remains low with the moving average baseline. The reduction of variance induced by the SCT baseline leads to a near-perfect recovery of the good parsing strategy in all \ufb01ve experiments. This shows the importance of this baseline for the stability of our approach. Sensitivity to hyperparameters.",
            "The reduction of variance induced by the SCT baseline leads to a near-perfect recovery of the good parsing strategy in all \ufb01ve experiments. This shows the importance of this baseline for the stability of our approach. Sensitivity to hyperparameters. Our model is relatively robust to hyperparameters changes when we use the SCT baseline and PPO. For example, changing the leaf transformation or dimensionality of the model has a minor impact on performance. However, we have observed that the choice of the optimiser has a signi\ufb01cant impact. For example, the average performance drops to 73.0% if we re- place Adadelta by Adam (Kingma and Ba, 2014). Yet, the maximum value out of 5 runs remains rel- atively high, 99.0%. Untied parameters. As opposed to previous work, the parameters of the parser and the compo- sition function are not tied in our model. Without this separation between syntax and semantics, it would be impossible to update one module with-",
            "Figure 1: Blue crosses depict an average accuracy of \ufb01ve models on the test examples that have lengths within certain range. Black circles illustrate individual models. out changing the other. The gradient direction is then dominated by the low variance signal from the semantic component, making it hard to learn the parser. We con\ufb01rmed experimentally that our model with tied parameters fails to \ufb01nd the correct parser and its accuracy drops to 64.7%. Extrapolation and Grammaticality. Recursive models have the potential to generalise to any se- quence length. Our model was trained with se- quences of length up to 130 tokens. We test the ability of the model to generalise to longer se- quences by generating additional expressions of lengths 200 to 1000. As shown in Fig.1, our model has a little loss in accuracy as the length increases to ten times the maximum length seen during training. On the other hand, we notice that \ufb01nal represen- tations produced by the parser are very similar to each other.",
            "As shown in Fig.1, our model has a little loss in accuracy as the length increases to ten times the maximum length seen during training. On the other hand, we notice that \ufb01nal represen- tations produced by the parser are very similar to each other. Indeed, the cosine similarity between these vectors for the test set has a mean value of 0.998 with a standard deviation of 0.002. There are two possible explanations for this observation: either our model assigns similar representations to valid expressions, or it produces a trivial uninfor- mative representation regardless of the expression. To verify which explanation is correct, we gener- ate ungrammatical expressions by removing either one operation token or one closing bracket sym- bol for each sequence in the test set. As shown in Figure 2, in contrast to grammatical expressions, ungrammatical ones tend to be very different from each other: \u201cHappy families are all alike; every unhappy family is unhappy in its own way.\u201d The only exception, marked by a mode near 1, come Figure 2: The distributions of cosine similarity for el- ements from the different sets of mathematical expres- sions.",
            "A logarithmic scale is used for y-axis. from ungrammatical expressions that represent in- complete expressions because of missing a clos- ing bracket at the end. This kind of sequences were seen by the parser during training and they indeed have to be represented by the same vec- tor. These observations show that our model does not produce a trivial representation, but identi\ufb01es the rules and constraints of the grammar. More- over, vectors for grammatical sequences are so dif- ferent from vectors for ungrammatical ones that you can tell them apart with 99.99% accuracy by simply measuring their cosine similarity to a ran- domly chosen grammatical vector from the train- ing set. Interestingly, we have not observed a sim- ilar signal from the vectors generated by the com- position function. Even learning a naive classi\ufb01er between grammatical and ungrammatical expres- sions on top of these representations achieves an accuracy of only 75%. This suggests that most of the syntactic information is captured by the parser, not the composition function.",
            "Even learning a naive classi\ufb01er between grammatical and ungrammatical expres- sions on top of these representations achieves an accuracy of only 75%. This suggests that most of the syntactic information is captured by the parser, not the composition function. 5.2 Natural Language Inference We next evaluate our model on natural language inference using the Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) and MultiNLI (Williams et al., 2018b) datasets. Nat- ural language inference consists in predicting the relationship between two sentences which can be either entailment, contradiction, or neutral. The task can be formulated as a three-way classi\ufb01ca- tion problem. The results are shown in Tables 3 and 4. When training the model on MultiNLI dataset we augment the training data with the SNLI data and use matched versions of the de-",
            "Model Dim. Acc. Yogatama et al. (2016) 100 80.5 Maillard et al. (2017) 100 81.6 Choi et al. (2018) 100 82.6 Ours 100 84.3\u00b10.3 Bowman et al. (2016) 300 83.2 Munkhdalai and Yu (2017) 300 84.6 Choi et al. (2018) 300 85.6 Choi et al. (2018)\u2020 300 83.7 Choi et al. (2018)* 300 84.9\u00b1 0.1 Ours 300 85.1\u00b10.2 Chen et al. (2017) 600 85.5 Choi et al. (2018) 600 86.0 Ours 600 84.6\u00b10.2 Table 3: Results on SNLI. *: publicly available code and hyperparameter optimization was used to obtain re- sults. \u2020: results are taken from Williams et al. (2018a) Model Dim.",
            "*: publicly available code and hyperparameter optimization was used to obtain re- sults. \u2020: results are taken from Williams et al. (2018a) Model Dim. Acc. LSTM\u2020 300 69.1 SPINN\u2020 300 67.5 RL-SPINN\u2020 300 67.4 Gumbel Tree-LSTM\u2020 300 69.5 Ours 300 70.7\u00b10.3 Table 4: Results on MultiNLI. \u2020: results are taken from Williams et al. (2018a). velopment and test sets. Surprisingly, two out of four models for MultiNLI task collapsed to left- branching parsing strategies. This collapse can be explained by the absence of the entropy regularisa- tion and the small number of PPO updates K = 1, which were determined to be optimal via hyper- parameter optimisation. As with ListOps, using an Adadelta optimizer signi\ufb01cantly improves the training of the model.",
            "As with ListOps, using an Adadelta optimizer signi\ufb01cantly improves the training of the model. 5.3 Sentiment Analysis We evaluate our model on a sentiment classi\ufb01ca- tion task using the Stanford Sentiment Treebank (SST) of Socher et al. (2013). All sentences in SST are represented as binary parse trees, and each subtree of a parse tree is annotated with the corre- sponding sentiment score. There are two versions of the dataset, with either binary labels, \u201cnegative\u201d or \u201cpositive\u201d, (SST-2) or \ufb01ve labels, representing \ufb01ne-grained sentiments (SST-5). As shown in Ta- SST-2 SST-5 Sequential sentence representation Radford et al. (2017) 91.8 52.9 McCann et al. (2017) 90.3 53.7 Peters et al. (2018) - 54.7 RvNN based models with external tree Socher et al. (2013) 85.4 45.7 Tai et al.",
            "(2017) 90.3 53.7 Peters et al. (2018) - 54.7 RvNN based models with external tree Socher et al. (2013) 85.4 45.7 Tai et al. (2015) 88.0 51.0 Munkhdalai and Yu (2017) 89.3 53.1 Looks et al. (2017) 89.4 52.3 RvNN based models with latent tree Yogatama et al. (2016) 86.5 - Choi et al. (2018) 90.7 53.7 Choi et al. (2018)\u2217 90.3\u00b10.5 51.6\u00b10.8 Ours 90.2\u00b10.2 51.5\u00b10.4 Table 5: Accuracy results of models on the SST. All the numbers are from Choi et al. (2018) but \u2217where we used their publicly available code and performed hyperparameter optimization.",
            "All the numbers are from Choi et al. (2018) but \u2217where we used their publicly available code and performed hyperparameter optimization. ble 5, our results are in line with previous work, con\ufb01rming the bene\ufb01ts of using latent syntactic parse trees instead of the prede\ufb01ned syntax. We noticed that all models trained on NLI or sentiment analysis tasks have parsing policies with relatively high entropy. This indicates that the al- gorithm does not prefer any speci\ufb01c grammar. In- deed, generated trees are very similar to balanced ones. This result is in line with Shi et al. (2018) where they observe that binary balanced tree en- coder gets the best results on most classi\ufb01cation tasks. We also compare with state-of-the-art sequence-based models. For the most part, these models are pre-trained on larger datasets and \ufb01ne-tuned on these tasks. Nonetheless, they outperform recursive models by a signi\ufb01cant margin. Performance on these datasets is more impacted by pre-training than by learning the syntax.",
            "For the most part, these models are pre-trained on larger datasets and \ufb01ne-tuned on these tasks. Nonetheless, they outperform recursive models by a signi\ufb01cant margin. Performance on these datasets is more impacted by pre-training than by learning the syntax. It would be interesting to see if a similar pre-training would also improve the performance of recursive models with latent tree learning. 6 Conclusion In this paper, we have introduced a novel model for learning latent tree parsers. Our approach re- lies on a separation between syntax and semantics. This allows dedicated optimisation schemes for",
            "each module. In particular, we found that it is im- portant to have an unbiased estimator of the parser gradients and to allow multiple gradient steps with PPO. When tested on a CFG, our learned parser generalises to sequences of any length and dis- tinguishes grammatical from ungrammatical ex- pressions by forming meaningful representations for well-formed expressions. For natural language tasks, instead, the model prefers to fall back to trivial strategies, in line with what was previously observed by Shi et al. (2018). Additionally, our approach performs competitively on several real natural language tasks. In the future, we would like to explore further relaxation-based techniques for learning the parser, such as REBAR (Tucker et al., 2017) or ReLAX (Grathwohl et al., 2017). Finally, we plan to look into applying recursive ap- proaches to language modelling as a pre-training step and measure if it has the same impact on downstream tasks as sequential models. Acknowledgments We would like to thank Alexander Koller, Ivan Titov, Wilker Aziz and anonymous reviewers for their helpful suggestions and comments.",
            "Acknowledgments We would like to thank Alexander Koller, Ivan Titov, Wilker Aziz and anonymous reviewers for their helpful suggestions and comments. References Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large anno- tated corpus for learning natural language inference. In Proceedings EMNLP 2015, pages 632\u2013642. Samuel R. Bowman, Jon Gauthier, Abhinav Ras- togi, Raghav Gupta, Christopher D. Manning, and Christopher Potts. 2016. A fast uni\ufb01ed model for parsing and sentence understanding. In Proceedings of the ACL 2016, Volume 1: Long Papers. Eric Brill. 1993. Automatic grammar induction and parsing free text: A transformation-based approach. In Proceedings of ACL 93, pages 259\u2013265. Glenn Carroll and Eugene Charniak. 1992. Two exper- iments on learning probabilistic dependency gram- mars from corpora. Department of Computer Sci- ence, Univ.",
            "In Proceedings of ACL 93, pages 259\u2013265. Glenn Carroll and Eugene Charniak. 1992. Two exper- iments on learning probabilistic dependency gram- mars from corpora. Department of Computer Sci- ence, Univ. Michael B. Chang, Abhishek Gupta, Sergey Levine, and Thomas L. Grif\ufb01ths. 2018. Automatically com- posing representation transformations as a means for generalization. CoRR, abs\/1807.04640. Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2017. Enhanced LSTM for natural language inference. In Proceedings of ACL 2017, Volume 1: Long Papers, pages 1657\u20131668. Jihun Choi, Kang Min Yoo, and Sang-goo Lee. 2018. Learning to compose task-speci\ufb01c tree structures. In Proceedings of AAAI 2018. Caio Corro and Ivan Titov. 2018. Differentiable perturb-and-parse: Semi-supervised parsing with a structured variational autoencoder.",
            "2018. Learning to compose task-speci\ufb01c tree structures. In Proceedings of AAAI 2018. Caio Corro and Ivan Titov. 2018. Differentiable perturb-and-parse: Semi-supervised parsing with a structured variational autoencoder. CoRR, abs\/1807.09875. Sreerupa Das, C Lee Giles, and Guo-Zheng Sun. 1992. Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory. In Proceedings of CogSci 1992, page 14. Andrew Drozdov and Samuel Bowman. 2017. The coadaptation problem when learning how and what to compose. Proceedings of the 2nd Workshop on Representation Learning for NLP. C. Lee Giles, Guo-Zheng Sun, Hsing-Hen Chen, Yee- Chun Lee, and Dong Chen. 1989. Higher order recurrent networks and grammatical inference. In Proceedings of NIPS 1989, pages 380\u2013387. Christoph Goller and Andreas Kuchler. 1996.",
            "1989. Higher order recurrent networks and grammatical inference. In Proceedings of NIPS 1989, pages 380\u2013387. Christoph Goller and Andreas Kuchler. 1996. Learning task-dependent distributed representations by back- propagation through structure. Neural Networks, 1:347\u2013352. Will Grathwohl, Dami Choi, Yuhuai Wu, Geoffrey Roeder, and David K. Duvenaud. 2017. Back- propagation through the void: Optimizing control variates for black-box gradient estimation. CoRR, abs\/1711.00123. Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735\u20131780. Eric Jang, Shixiang Gu, and Ben Poole. 2016. Cat- egorical reparameterization with gumbel-softmax. CoRR, abs\/1611.01144. Armand Joulin and Tomas Mikolov. 2015. Inferring algorithmic patterns with stack-augmented recurrent nets.",
            "2016. Cat- egorical reparameterization with gumbel-softmax. CoRR, abs\/1611.01144. Armand Joulin and Tomas Mikolov. 2015. Inferring algorithmic patterns with stack-augmented recurrent nets. In Proceedings of NIPS 2015, pages 190\u2013198. Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. CoRR, abs\/1412.6980. Diederik P. Kingma and Max Welling. 2013. Auto- encoding variational bayes. CoRR, abs\/1312.6114. Dan Klein and Christopher D Manning. 2002. A gener- ative constituent-context model for improved gram- mar induction. In Proceedings of ACL 2002, pages 128\u2013135. Phong Le and Willem H. Zuidema. 2015. The for- est convolutional network: Compositional distribu- tional semantics with a neural chart and without bi- narization.",
            "In Proceedings of ACL 2002, pages 128\u2013135. Phong Le and Willem H. Zuidema. 2015. The for- est convolutional network: Compositional distribu- tional semantics with a neural chart and without bi- narization. In Proceedings of EMNLP 2015, pages 1155\u20131164.",
            "Moshe Looks, Marcello Herreshoff, DeLesley Hutchins, and Peter Norvig. 2017. Deep learning with dynamic computation graphs. arXiv preprint arXiv:1702.02181. Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. 2016. The concrete distribution: A continuous relaxation of discrete random variables. CoRR, abs\/1611.00712. David M Magerman and Mitchell P Marcus. 1990. Parsing a natural language using mutual information statistics. In AAAI, volume 90, pages 984\u2013989. Jean Maillard and Stephen Clark. 2018. Latent tree learning with differentiable parsers: Shift-reduce parsing and chart parsing. CoRR, abs\/1806.00840. Jean Maillard, Stephen Clark, and Dani Yogatama. 2017. Jointly learning sentence embeddings and syntax with unsupervised tree-lstms. CoRR, abs\/1705.09189. Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017.",
            "2017. Jointly learning sentence embeddings and syntax with unsupervised tree-lstms. CoRR, abs\/1705.09189. Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Con- textualized word vectors. In Proceedings of NIPS 2017, pages 6294\u20136305. Andriy Mnih and Karol Gregor. 2014. Neural vari- ational inference and learning in belief networks. arXiv preprint arXiv:1402.0030. Volodymyr Mnih, Adri`a Puigdom`enech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asynchronous methods for deep reinforce- ment learning. In Proceedings of ICML 2016, pages 1928\u20131937. Michael Mozer and Sreerupa Das. 1992. A connection- ist symbol manipulator that discovers the structure of context-free languages.",
            "Asynchronous methods for deep reinforce- ment learning. In Proceedings of ICML 2016, pages 1928\u20131937. Michael Mozer and Sreerupa Das. 1992. A connection- ist symbol manipulator that discovers the structure of context-free languages. In Proceedings of NIPS 1992, pages 863\u2013870. Tsendsuren Munkhdalai and Hong Yu. 2017. Neural tree indexers for text understanding. In Proceedings of EACL 2017, volume 1, page 11. NIH Public Ac- cess. Nikita Nangia and Samuel R. Bowman. 2018. Listops: A diagnostic dataset for latent tree learning. In Pro- ceedings of NAACL-HLT 2018, Student Research Workshop, pages 92\u201399. Barbara BH Partee, Alice G ter Meulen, and Robert Wall. 1990. Mathematical methods in linguistics, volume 30. Springer Science & Business Media. Jeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014.",
            "1990. Mathematical methods in linguistics, volume 30. Springer Science & Business Media. Jeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. Glove: Global vectors for word representation. In Proceedings of EMNLP 2014, pages 1532\u20131543. Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. arXiv preprint arXiv:1802.05365. Alec Radford, Rafal Jozefowicz, and Ilya Sutskever. 2017. Learning to generate reviews and discovering sentiment. arXiv preprint arXiv:1704.01444. Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross, and Vaibhava Goel. 2017. Self-critical sequence training for image captioning.",
            "arXiv preprint arXiv:1704.01444. Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross, and Vaibhava Goel. 2017. Self-critical sequence training for image captioning. In Proceed- ings of CVPR 2017, pages 1179\u20131195. Sheldon M. Ross. 1997. Simulation (2. ed.). Statistical modeling and decision science. Academic Press. Himanshu Sahni, Saurabh Kumar, Farhan Tejani, and Charles L. Isbell. 2017. Learning to compose skills. CoRR, abs\/1711.11289. Geoffrey Sampson. 1986. A stochastic approach to parsing. In Proceedings Coling 1986 Volume 1, vol- ume 1. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. CoRR, abs\/1707.06347.",
            "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. CoRR, abs\/1707.06347. Haoyue Shi, Hao Zhou, Jiaze Chen, and Lei Li. 2018. On tree-based neural sentence modeling. CoRR, abs\/1808.09644. Satinder P. Singh. 1992. Transfer of learning by com- posing solutions of elemental sequential tasks. Ma- chine Learning, 8:323\u2013339. Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011. Semi-supervised recursive autoencoders for predict- ing sentiment distributions. In Proceedings of EMNLP 2011, pages 151\u2013161. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep mod- els for semantic compositionality over a sentiment treebank.",
            "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep mod- els for semantic compositionality over a sentiment treebank. In Proceedings of EMNLP 2013, pages 1631\u20131642. G Sun. 1990. Connectionist pushdownautomata that learn context-free gram-mars. In Proc. IJCNN\u201990, volume 1, pages 577\u2013580. Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved semantic representations from tree-structured long short-term memory net- works. In Proceedings of ACL 2015, Volume 1: Long Papers, pages 1556\u20131566. George Tucker, Andriy Mnih, Chris J. Maddison, John Lawson, and Jascha Sohl-Dickstein. 2017. REBAR: low-variance, unbiased gradient estimates for dis- crete latent variable models. In Proceedings of NIPS 2017, pages 2624\u20132633.",
            "2017. REBAR: low-variance, unbiased gradient estimates for dis- crete latent variable models. In Proceedings of NIPS 2017, pages 2624\u20132633. Claire Cardie Vlad Niculae, Andr\u00b4e F. T. Martins. 2018. Towards dynamic computation graphs via sparse la- tent structure. CoRR, abs\/1809.00653.",
            "Adina Williams, Andrew Drozdov, and Samuel R. Bowman. 2018a. Do latent tree learning models identify meaningful structure in sentences? TACL, 6:253\u2013267. Adina Williams, Nikita Nangia, and Samuel Bowman. 2018b. A broad-coverage challenge corpus for sen- tence understanding through inference. In Proceed- ings of NAACL-HLT 2018, Volume 1 (Long Papers), pages 1112\u20131122. Ronald J. Williams. 1992. Simple statistical gradient- following algorithms for connectionist reinforce- ment learning. Machine Learning, 8:229\u2013256. Dani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, and Wang Ling. 2016. Learning to compose words into sentences with reinforcement learning. CoRR, abs\/1611.09100. Matthew D. Zeiler. 2012. ADADELTA: an adaptive learning rate method. CoRR, abs\/1212.5701. Xiao-Dan Zhu, Parinaz Sobhani, and Hongyu Guo.",
            "Matthew D. Zeiler. 2012. ADADELTA: an adaptive learning rate method. CoRR, abs\/1212.5701. Xiao-Dan Zhu, Parinaz Sobhani, and Hongyu Guo. 2015. Long short-term memory over recursive structures. In Proceedings of ICML 2015, pages 1604\u20131612."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1902.09393.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 10443.000183105469,
    "avg_doclen_est": 174.0500030517578
}
