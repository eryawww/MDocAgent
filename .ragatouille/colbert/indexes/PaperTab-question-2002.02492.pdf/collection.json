[
  "Consistency of a Recurrent Language Model With Respect to Incomplete Decoding Sean Welleck1\u2217 Ilia Kulikov1\u2217 Jaedeok Kim2\u2020 Richard Yuanzhe Pang1 Kyunghyun Cho1,3 1 New York University 2 Samsung Research 3 CIFAR Associate Fellow Abstract Despite strong performance on a variety of tasks, neural sequence models trained with maximum likelihood have been shown to ex- hibit issues such as length bias and degener- ate repetition. We study the related issue of receiving in\ufb01nite-length sequences from a re- current language model when using common decoding algorithms. To analyze this issue, we \ufb01rst de\ufb01ne inconsistency of a decoding algo- rithm, meaning that the algorithm can yield an in\ufb01nite-length sequence that has zero probabil- ity under the model. We prove that commonly used incomplete decoding algorithms \u2013 greedy search, beam search, top-k sampling, and nu- cleus sampling \u2013 are inconsistent, despite the fact that recurrent language models are trained to produce sequences of \ufb01nite length.",
  "We prove that commonly used incomplete decoding algorithms \u2013 greedy search, beam search, top-k sampling, and nu- cleus sampling \u2013 are inconsistent, despite the fact that recurrent language models are trained to produce sequences of \ufb01nite length. Based on these insights, we propose two remedies which address inconsistency: consistent vari- ants of top-k and nucleus sampling, and a self- terminating recurrent language model. Empir- ical results show that inconsistency occurs in practice, and that the proposed methods pre- vent inconsistency. 1 Introduction Neural sequence models trained with maximum likelihood estimation (MLE) have become a stan- dard approach to modeling sequences in a variety of natural language applications such as machine translation (Bahdanau et al., 2015), dialogue mod- eling (Vinyals et al., 2015), and language modeling (Radford et al., 2019). Despite this success, MLE- trained neural sequence models have been shown to exhibit issues such as length bias (Sountsov and Sarawagi, 2016; Stahlberg and Byrne, 2019) and degenerate repetition (Holtzman et al., 2019). \u2217Equal contribution.",
  "\u2217Equal contribution. Correspondence to: Sean Welleck wellecks@nyu.edu. \u2020Work done at New York University. These issues are suspected to be related to the max- imum likelihood objective\u2019s local normalization, which results in a discrepancy between the learned model\u2019s distribution and the distribution induced by the decoding algorithm used to generate sequences (Lafferty et al., 2001; Andor et al., 2016). This has prompted the development of alternative decoding methods (Wu et al., 2016; Holtzman et al., 2019) and training objectives (Murray and Chiang, 2018; Welleck et al., 2019). In this paper, we formalize and study this discrepancy between the model and the decoding algorithm. We begin by formally de\ufb01ning recurrent neu- ral language models, a family that encompasses neural models used in practice, such as recurrent neural networks (Elman, 1990; Cho et al., 2014; Hochreiter and Schmidhuber, 1997), and transform- ers (Vaswani et al., 2017).",
  "Next, we formally de\ufb01ne a decoding algorithm \u2013 a function that induces a distribution over sequences given a recurrent lan- guage model and a context distribution \u2013 which is used to obtain probable sequences from a model. In this paper, we show that the distribution induced by a decoding algorithm can contradict this intended use; instead, the decoding algorithm may return improbable, in\ufb01nite-length sequences. Our main \ufb01nding is that a sequence which re- ceives zero probability under a recurrent language model\u2019s distribution can receive nonzero probabil- ity under the distribution induced by a decoding algorithm. This occurs when the recurrent language model always ranks the sequence termination token outside of the set of tokens considered at each de- coding step, yielding an in\ufb01nite-length, zero proba- bility sequence. This holds whenever the decoding algorithm is incomplete, in the sense that the algo- rithm excludes tokens from consideration at each step of decoding, which is the case for common methods such as greedy search, beam search, top-k sampling (Fan et al., 2018), and nucleus sampling arXiv:2002.02492v2  [cs.LG]  2 Oct 2020",
  "(Holtzman et al., 2019). We formalize our main \ufb01nding using the notion of consistency (Chen et al., 2017) \u2013 whether a distribution assigns probability mass only to \ufb01nite sequences \u2013 and prove that a consistent recurrent language model paired with an incomplete decoding algorithm can induce an inconsistent sequence distribution. Based on the insight that inconsistency occurs due to the behavior of the termination token un- der incomplete decoding, we develop two meth- ods for addressing inconsistency. First, we pro- pose consistent sampling methods which guarantee that the termination token is not excluded from se- lection during decoding. Second, we introduce a self-terminating recurrent language model which ensures that the termination token is eventually ranked above all others, guaranteeing consistency under incomplete decoding. To empirically measure inconsistency, we de- code sequences from trained recurrent language models and measure the proportion of sequences with lengths far exceeding the maximum training sequence length.",
  "To empirically measure inconsistency, we de- code sequences from trained recurrent language models and measure the proportion of sequences with lengths far exceeding the maximum training sequence length. Our experiments on the Wikitext2 dataset (Merity et al., 2016) suggest that inconsis- tency occurs in practice when using incomplete decoding methods, while the proposed consistent sampling methods and self-terminating model pa- rameterization prevent inconsistency and maintain language modeling quality. The theoretical analysis reveals defects of ex- isting decoding algorithms, providing a way to develop future models, inference procedures, and learning algorithms. We present methods related to sampling and model parameterization, but there are more directions for future investigation; we close with directions related to sequence-level learning. 2 Background We begin our discussion by establishing back- ground de\ufb01nitions. First, we de\ufb01ne a sequence which is the main object of our investigation. De\ufb01nition 2.1 (Sequence). A sequence Y is an ordered collection of items from a prede\ufb01ned \ufb01nite vocabulary V .",
  "First, we de\ufb01ne a sequence which is the main object of our investigation. De\ufb01nition 2.1 (Sequence). A sequence Y is an ordered collection of items from a prede\ufb01ned \ufb01nite vocabulary V . A sequence of \ufb01nite length always ends with a special token \u27e8eos\u27e9\u2208V that only appears at the end of a sequence. Each model we consider generates a sequence conditioned on context information, such as a pre\ufb01x in sentence completion. To consider this, we de\ufb01ne a context distribution. De\ufb01nition 2.2 (Context distribution). A context dis- tribution p(C) is a probability distribution de\ufb01ned over a set C. An element C \u2208C is called a context. 2.1 Recurrent Language Models A recurrent language model is an autoregressive model of a sequence distribution, where each con- ditional probability is parameterized with a neural network. Importantly, we assume that all tokens in a sequence are dependent on each other under a recurrent language model.",
  "2.1 Recurrent Language Models A recurrent language model is an autoregressive model of a sequence distribution, where each con- ditional probability is parameterized with a neural network. Importantly, we assume that all tokens in a sequence are dependent on each other under a recurrent language model. This allows us to avoid cases in which the model degenerates to a Marko- vian language model, such as an n-gram model with a \ufb01nite n. De\ufb01nition 2.3 (Recurrent language model). A re- current language model p\u03b8 is a neural network that computes the following at each time step: p\u03b8(yt = v | y<t, C) = exp(u\u22a4 v ht + cv) P v\u2032\u2208V exp(u\u22a4 v\u2032ht + cv\u2032), where ht = f\u03b8(yt, ht\u22121) and h0 = g\u03b8(C), and u, c, \u03b8 are parameters. A recurrent language model thereby computes the probability of a sequence Y = (y1, . . .",
  "A recurrent language model thereby computes the probability of a sequence Y = (y1, . . . , yT ) by p\u03b8(Y | C) = T Y t=1 p\u03b8(yt | y<t, C), where y<t = (y1, . . . , yt\u22121). This distribution sat- is\ufb01es yi \u0338\u22a5\u22a5yj | C, \u2200i < j. Practical variants of the recurrent language model differ by the choice of transition function f\u03b8 (Elman, 1990; Hochreiter and Schmidhuber, 1997; Cho et al., 2014; Vaswani et al., 2017). The use of softmax (Bridle, 1990) implies that every unique token in the vocabulary is considered at every loca- tion of a sequence. Remark 2.1. Under the conditional distribution of a recurrent LM, every token v \u2208V is as- signed a positive probability, implying that 0 < p\u03b8(v | y<t, C) < 1.",
  "Remark 2.1. Under the conditional distribution of a recurrent LM, every token v \u2208V is as- signed a positive probability, implying that 0 < p\u03b8(v | y<t, C) < 1. Any \ufb01nite sequence is proba- ble under a recurrent LM under any context, i.e., p\u03b8(Y | C) > 0 for any sequence Y of \ufb01nite length. 2.2 Decoding Algorithms Because it is intractable to decode the most proba- ble sequence, it is necessary in practice to use an approximate decoding algorithm. De\ufb01nition 2.4 (Decoding algorithm). A decoding algorithm F(p\u03b8, C) is a function that generates",
  "a sequence \u02dcY given a recurrent language model p\u03b8 and context C. Let qF denote the distribution induced by the decoding algorithm F. We consider two families of decoding algo- rithms. In our analysis we only consider algorithms that decode in a single pass, forward in time, with- out modifying previously selected tokens. Stochastic decoding. The \ufb01rst family consists of stochastic algorithms. Among them, ancestral sam- pling is asymptotically unbiased and can be used for \ufb01nding the most probable sequence, although with high variance. De\ufb01nition 2.5 (Ancestral sampling). Ancestral sampling Fanc generates a sequence from a re- current language model p\u03b8 given context C by re- cursively sampling from p\u03b8(yt | \u02dcy<t, C) until \u02dcyt = \u27e8eos\u27e9: \u02dcyt \u223cp\u03b8(yt | \u02dcy<t, C). To avoid the high variance, two approximate stochastic decoding algorithms have recently been proposed and tested with recurrent language mod- els.",
  "To avoid the high variance, two approximate stochastic decoding algorithms have recently been proposed and tested with recurrent language mod- els. Top-k sampling considers only a subset of the k most probable tokens from the vocabulary at a time, while nucleus sampling considers only the minimal subset of most probable tokens whose total probability is higher than a prede\ufb01ned threshold. De\ufb01nition 2.6 (Top-k sampling (Fan et al., 2018)). Top-k sampling Ftop-k generates a sequence from a recurrent language model p\u03b8 given context C by recursively sampling from: q(v) \u221d ( p\u03b8(v | y<t, C), if v \u2208Vk, 0, otherwise. where Vk = arg top-k v\u2032 p\u03b8(v\u2032 | y<t, C). De\ufb01nition 2.7 (Nucleus sampling (Holtzman et al., 2019)). Nucleus sampling Fnuc-\u00b5 generates a se- quence from a recurrent language model p\u03b8 given context C by recursively sampling from the fol- lowing proposal distribution. Let v1, . . .",
  "Nucleus sampling Fnuc-\u00b5 generates a se- quence from a recurrent language model p\u03b8 given context C by recursively sampling from the fol- lowing proposal distribution. Let v1, . . . , v|V | denote tokens in V such that p\u03b8(vi | y<t, C) \u2265 p\u03b8(vj | y<t, C) for all i < j, and de\ufb01ne q(v) \u221d ( p\u03b8(v | y<t, C), if v \u2208V\u00b5, 0, otherwise, where V\u00b5 = \b v1, \u00b7 \u00b7 \u00b7 , vk\u00b5 \t with k\u00b5 = min ( k \f\f\f\f\f k X i=1 p\u03b8(vi | y<t, C) > \u00b5 ) . Deterministic decoding. The other family con- sists of deterministic decoding algorithms, where a token is selected deterministically according to a rule at each decoding step. The most naive al- gorithm, called greedy decoding, simply takes the most probable token at each step. De\ufb01nition 2.8 (Greedy decoding).",
  "The most naive al- gorithm, called greedy decoding, simply takes the most probable token at each step. De\ufb01nition 2.8 (Greedy decoding). Greedy decod- ing Fgreedy generates a sequence from a recurrent language model p\u03b8 given context C by recursively selecting the most likely token from p\u03b8(yt|\u02dcy<t, C) until \u02dcyt = \u27e8eos\u27e9: \u02dcyt = arg max v\u2208V log p\u03b8(yt = v | \u02dcy<t, C). In contrast to greedy decoding, beam search with width k, Fbeam-k, operates on the level of partial se- quences or pre\ufb01xes. Starting from a set of empty pre\ufb01xes, at each iteration a new pre\ufb01x set is formed by expanding each pre\ufb01x with each possible token, then choosing the k highest scoring expanded pre- \ufb01xes; refer to Appendix A for a formal de\ufb01nition. Incompleteness.",
  "Incompleteness. Other than ancestral sampling, the decoding algorithms above are incomplete in that they only consider a strict subset of the full vocabulary V at each time step, aside from the trivial case of k = |V |.1 De\ufb01nition 2.9 (Incomplete Decoding). A decoding algorithm F is incomplete when for each context C and pre\ufb01x y<t, there is a strict subset V \u2032 t \u228aV such that X v\u2208V \u2032 t qF(yt = v | y<t, C) = 1. 3 Consistency of a Decoding Algorithm De\ufb01nition of consistency. A recurrent language model p\u03b8 may assign a positive probability to an in\ufb01nitely long sequence, in which case we call the model inconsistent. This notion of consistency was raised and analyzed earlier, for instance by Booth and Thompson (1973) and Chen et al. (2017), in terms of whether the distribution induced by p\u03b8 is concentrated on \ufb01nite sequences. We extend their de\ufb01nition to account for the context C. De\ufb01nition 3.1 (Consistency of a recurrent lan- guage model).",
  "(2017), in terms of whether the distribution induced by p\u03b8 is concentrated on \ufb01nite sequences. We extend their de\ufb01nition to account for the context C. De\ufb01nition 3.1 (Consistency of a recurrent lan- guage model). A recurrent language model is consistent under a context distribution p(C) if p\u03b8(|Y | = \u221e) = 0. Otherwise, the recurrent lan- guage model is said to be inconsistent. 1Nucleus sampling is incomplete when for every context C and pre\ufb01x y<t, minv\u2208V p\u03b8(v|y<t, C) < 1 \u2212\u00b5.",
  "Any sequence decoded from a consistent model for a given context is guaranteed to terminate. Lemma 3.1. If a recurrent LM p\u03b8 is consistent, p\u03b8(|Y | = \u221e| C) = 0 for any probable context C.2 Next, we establish a practical condition under which a recurrent language model is consistent. Lemma 3.2. A recurrent LM p\u03b8 is consistent if \u2225ht\u2225p is uniformly bounded for some p \u22651. Proof sketch. If \u2225ht\u2225p is bounded, then each u\u22a4 v ht is bounded, hence p\u03b8(\u27e8eos\u27e9|y<t, C) > \u03be > 0 for a constant \u03be. Thus p\u03b8(|Y | = \u221e) \u2264limt\u2192\u221e(1 \u2212 \u03be)t = 0, meaning that p\u03b8 is consistent.",
  "Although this condition is practical because layer normalization or bounded activation func- tions (Elman, 1990; Cho et al., 2014; Vaswani et al., 2017) result in bounded ht, we show that even if a recurrent language model is consistent, a decoding algorithm may produce an in\ufb01nite-length sequence. We formalize this discrepancy using the consis- tency of a decoding algorithm. De\ufb01nition 3.2 (Consistency of a decoding algo- rithm). A decoding algorithm F is consistent with respect to a consistent recurrent language model p\u03b8 under a context distribution p(C) if the decoding algorithm F preserves the consistency of the model p\u03b8, that is, qF(|Y | = \u221e) = 0. When a consistent recurrent language model p\u03b8 and a decoding algorithm F induce a consistent distribution qF, we say that p\u03b8 paired with F is consistent. For instance, any consistent recurrent language model paired with ancestral sampling is consistent, because the induced distribution qFanc is the same as the distribution of the original model. We also have an analogue of Lemma 3.1. Lemma 3.3.",
  "For instance, any consistent recurrent language model paired with ancestral sampling is consistent, because the induced distribution qFanc is the same as the distribution of the original model. We also have an analogue of Lemma 3.1. Lemma 3.3. A consistent decoding algorithm with respect to a consistent recurrent LM decodes only probable sequences. That is, if qF(Y | C) > 0, then p\u03b8(Y | C) > 0 for any probable context C. Inconsistency of incomplete decoding. Any in- complete decoding algorithm (De\ufb01nition 2.9) can be inconsistent regardless of the context distribu- tion, because there is a recurrent LM that places \u27e8eos\u27e9outside of V \u2032 t at every step of decoding. To show this, we construct a consistent recurrent lan- guage model whose distribution induced by an in- complete decoding algorithm is inconsistent. 2Proofs of Lemmas 3.1-3.3 are in Appendix B. Figure 1: A depiction of the model\u2019s sequence distribu- tion (light grey, solid border) and the decoder\u2019s induced sequence distribution (dark grey, dotted border).",
  "2Proofs of Lemmas 3.1-3.3 are in Appendix B. Figure 1: A depiction of the model\u2019s sequence distribu- tion (light grey, solid border) and the decoder\u2019s induced sequence distribution (dark grey, dotted border). The white and black rectangles depict the set of all \ufb01nite and in\ufb01nite sequences, respectively. We prove that un- der practical conditions, any incomplete decoding algo- rithm may be inconsistent with respect to a consistent model, as depicted. Theorem 3.4 (Inconsistency of an incomplete de- coding algorithm). There exists a consistent recur- rent LM p\u03b8 from which an incomplete decoding algorithm F, that considers only up to (|V | \u22121)- most likely tokens according to p\u03b8(yt | y<t, C) at each step t, \ufb01nds an in\ufb01nite-length sequence \u02dcY with probability 1, i.e., qF(|Y | = \u221e) = 1. Proof. We prove this theorem by constructing a tanh recurrent network.",
  "Proof. We prove this theorem by constructing a tanh recurrent network. We de\ufb01ne the recurrent function f\u03b8 as ht = f\u03b8(yt, ht\u22121) = tanh \u0012\u0014 Wh 0 0 I \u0015 ht\u22121 + \u0014 0 e(yt) \u0015\u0013 , where e(yt) \u2208R|V | is a one-hot representation of yt, Wh \u2208Rd\u00d7d where every entry is positive, and I is an identity matrix of size |V | \u00d7 |V |. h0 = g\u03b8(C) is constructed to consist of positive values only. Because each element of |ht| is bounded by 1, the constructed recurrent language model p\u03b8 is consistent by Lemma 3.2.",
  "h0 = g\u03b8(C) is constructed to consist of positive values only. Because each element of |ht| is bounded by 1, the constructed recurrent language model p\u03b8 is consistent by Lemma 3.2. We set uv (see De\ufb01nition 2.3) to uv = \u0014 \u00afuv e(v) \u0015 , u\u27e8eos\u27e9= \u0014 \u00afu\u27e8eos\u27e9 e(\u27e8eos\u27e9) \u0015 , where v \u0338= \u27e8eos\u27e9, all elements of \u00afuv are positive, all elements of \u00afu\u27e8eos\u27e9are negative, and e(v) is a one-hot representation of v. cv is set to zero. This de\ufb01nes a valid recurrent language model (De\ufb01nition 2.3), since the conditional distribution at each time t is in\ufb02uenced by all the previous tokens. More speci\ufb01cally, the logit of a token v",
  "depends on Pt t\u2032=1 1(yt\u2032 = v), where 1 is an indi- cator function. This recurrent language model always outputs positive logits for non-\u27e8eos\u27e9tokens, and outputs negative logits for the \u27e8eos\u27e9token. This im- plies p(\u27e8eos\u27e9| y<t, C) < p(v | y<t, C) for all v \u2208V \\ {\u27e8eos\u27e9}. This means that \u27e8eos\u27e9is al- ways ranked last at each time step, so an incom- plete decoding algorithm that considers at most (|V | \u22121) most probable tokens at each time step from p\u03b8(yt | y<t, C) cannot decode \u27e8eos\u27e9and thus always decodes an in\ufb01nitely long sequence \u02c6Y , i.e., qF(|Y | = \u221e| C) = 1 for any context C. It yields qF(|Y | = \u221e) = 1, while p\u03b8(|Y | = \u221e) = 0 due to consistency of the model p\u03b8. Greedy decoding, beam search, top-k sampling, and nucleus sampling are all inconsistent according to this theorem.",
  "Greedy decoding, beam search, top-k sampling, and nucleus sampling are all inconsistent according to this theorem. 4 Fixing the inconsistency In this section, we consider two ways to prevent inconsistency arising from incomplete decoding algorithms. First, we introduce consistent versions of top-k and nucleus sampling. Second, we in- troduce the self-terminating recurrent language model, which is consistent when paired with any of the decoding algorithms considered in this paper. 4.1 Consistent Sampling Algorithms The proof of Theorem 3.4 suggests that the incon- sistency of incomplete decoding algorithms arises from the fact that \u27e8eos\u27e9may be excluded inde\ufb01- nitely from the set of top-ranked tokens. We pro- pose a simple modi\ufb01cation to top-k and nucleus sampling that forces \u27e8eos\u27e9to be included at each step of decoding. First, we give a condition for when a particular model p\u03b8 paired with a decoding algorithm F is consistent. Theorem 4.1. Suppose a recurrent LM p\u03b8 has uni- formly bounded \u2225ht\u2225p for some p \u22651.",
  "First, we give a condition for when a particular model p\u03b8 paired with a decoding algorithm F is consistent. Theorem 4.1. Suppose a recurrent LM p\u03b8 has uni- formly bounded \u2225ht\u2225p for some p \u22651. If a de- coding algorithm F satis\ufb01es qF(\u27e8eos\u27e9| y<t, C) \u2265 p\u03b8(\u27e8eos\u27e9| y<t, C) for every pre\ufb01x y<t and context C, then the decoding algorithm F is consistent with respect to the model p\u03b8.3 We de\ufb01ne consistent variants of top-k and nu- cleus sampling which satisfy this condition. De\ufb01nition 4.1 (Consistent top-k sampling). Con- sistent top-k sampling is top-k sampling with the 3See Appendix C for the proof. Figure 2: The self-terminating recurrent LM uses the layer shown in grey instead of the standard softmax layer. The layer takes logits (u\u22a4 \u00b7 ht), the previous step\u2019s \u27e8eos\u27e9probability (p\u27e8eos\u27e9 t\u22121 ), and a hyper-parameter \u03f5 \u2208(0, 1).",
  "The layer takes logits (u\u22a4 \u00b7 ht), the previous step\u2019s \u27e8eos\u27e9probability (p\u27e8eos\u27e9 t\u22121 ), and a hyper-parameter \u03f5 \u2208(0, 1). The layer computes \u03b1 using De\ufb01nition 4.3, which determines the \u27e8eos\u27e9probability (p\u27e8eos\u27e9 t \u2208(\u03f5, 1)), and guarantees that p\u27e8eos\u27e9 t > p\u27e8eos\u27e9 t\u22121 . The remaining probability mass is allocated to the non-\u27e8eos\u27e9tokens. following modi\ufb01ed proposal distribution: q(v) \u221d ( p\u03b8(v|y<t, C), if v \u2208V \u2032, 0, otherwise, where V \u2032 = {\u27e8eos\u27e9} \u222aarg top-k v\u2032 p\u03b8(v\u2032 | y<t, C). De\ufb01nition 4.2 (Consistent nucleus sampling). Consistent nucleus sampling is nucleus sampling with the following modi\ufb01ed proposal distribution: q(v) \u221d ( p\u03b8(v | y<t, C), if v \u2208V\u00b5 \u222a{\u27e8eos\u27e9}, 0, otherwise.",
  "Consistent nucleus sampling is nucleus sampling with the following modi\ufb01ed proposal distribution: q(v) \u221d ( p\u03b8(v | y<t, C), if v \u2208V\u00b5 \u222a{\u27e8eos\u27e9}, 0, otherwise. The induced probability of \u27e8eos\u27e9under these two algorithms is always equal to or larger than the model\u2019s probability. By Theorem 4.1, these algo- rithms are consistent with respect to any consistent recurrent language model. 4.2 Self-Terminating Recurrent LM Although these consistent sampling algorithms can be used with any recurrent language model, their stochastic nature may not be suitable for \ufb01nding a single, highly probable sequence. To avoid this lim- itation, we propose the self-terminating recurrent language model (STRLM). De\ufb01nition 4.3 (Self-terminating recurrent lan- guage model). A self-terminating recurrent lan- guage model computes the following conditional",
  "probability at each time step: p\u03b8(v | y<t, C) = \uf8f1 \uf8f2 \uf8f3 1 \u2212\u03b1(ht), v = \u27e8eos\u27e9, \u03b1(ht) exp(u\u22a4 v ht+cv) P v\u2032\u2208V \u2032 exp(u\u22a4 v\u2032ht+cv\u2032), \u03b1(h0) = \u03c3(u\u22a4 \u27e8eos\u27e9h0), \u03b1(ht) = \u03c3(u\u22a4 \u27e8eos\u27e9ht) [1 \u2212p\u03b8(\u27e8eos\u27e9|y<t\u22121, C)] , with \u03c3 : R \u2192[0, 1 \u2212\u03b5] and \u03b5 \u2208(0, 1). ht is computed as in the original recurrent LM. The underlying idea is that the probability of \u27e8eos\u27e9increases monotonically, since p\u27e8eos\u27e9 t = 1 \u2212 tY t\u2032=0 \u03c3(u\u22a4 \u27e8eos\u27e9ht\u2032). Consequently, the STRLM is consistent when paired with greedy decoding or beam search; see Appendix C for formal statements and proofs.",
  "Consequently, the STRLM is consistent when paired with greedy decoding or beam search; see Appendix C for formal statements and proofs. 5 Empirical Validation The theoretical results rely on the existence of a model that results in inconsistency; it remains to be shown that inconsistency with respect to incom- plete decoding occurs with recurrent language mod- els encountered in practice. Moreover, while the proposed methods carry theoretical guarantees in terms of consistency, we must check whether they retain language modeling quality. To do so, we perform experiments using a sequence completion task. In each experiment, we use the beginning of a sequence as context, then decode continuations from a trained recurrent LM and measure the pro- portion of non-terminated sequences in order to approximately measure inconsistency. The \ufb01rst ex- periment (\u00a75.1) shows that inconsistency occurs in practice, and the second experiment (\u00a75.2) shows the effectiveness of the proposed approaches. Our third experiment (\u00a75.3) shows that inconsistency also occurs frequently in GPT-2, a large-scale trans- former language model.4 Sequence completion.",
  "Our third experiment (\u00a75.3) shows that inconsistency also occurs frequently in GPT-2, a large-scale trans- former language model.4 Sequence completion. We evaluate recurrent language models on a sequence completion task, which has previously been used to evaluate the effectiveness of sequence models, e.g., Sutskever et al. (2011); Graves (2013); Radford et al. (2019); Holtzman et al. (2019); Welleck et al. (2019). Se- quence completion is a general setting for studying 4Code available at https://github.com/uralik/ consistency-lm. the behavior of language models, encompassing machine translation (Bahdanau et al., 2015), story generation (Fan et al., 2018), and dialogue mod- eling (Vinyals et al., 2015). The task consists of decoding a continuation \u02c6Y \u223cF(p\u03b8, C) given a length-k pre\ufb01x C = (c1, . . . , ck), resulting in a completion (c1, . . . , ck, \u02c6y1 .",
  ". . , ck), resulting in a completion (c1, . . . , ck, \u02c6y1 . . . , \u02c6yT ). Dataset. Our \ufb01rst two experiments use Wikitext2 (Merity et al., 2016), which consists of paragraphs from English Wikipedia, since it has frequently been used to evaluate language models (Grave et al., 2017; Melis et al., 2018; Merity et al., 2018). We consider both word and BPE5 tokenization. We split each paragraph into sentences using Spacy6. We split each sequence, using the \ufb01rst k tokens as a context and the remaining tokens as a con- tinuation. To ensure that each sequence contains a pre\ufb01x, we prepend padding tokens to make it length k. Special \u27e8bos\u27e9and \u27e8eos\u27e9tokens are inserted at the beginning and end of each sequence. We use k = 10. Table 7 contains dataset statistics. Context distribution.",
  "To ensure that each sequence contains a pre\ufb01x, we prepend padding tokens to make it length k. Special \u27e8bos\u27e9and \u27e8eos\u27e9tokens are inserted at the beginning and end of each sequence. We use k = 10. Table 7 contains dataset statistics. Context distribution. We de\ufb01ne empirical con- text distributions with pre\ufb01xes from the train, valid, and test sets: p(C; D) = 1 |D| P|D| n=1 1(C = C(n)), where D = {(C(n), Y (n))}N n=1 is a dataset split. Evaluation metrics. We use \ufb01nite sequences to approximately measure the consistency of a model paired with a decoding algorithm, since decoding an in\ufb01nite-length sequence is impossible.",
  "Evaluation metrics. We use \ufb01nite sequences to approximately measure the consistency of a model paired with a decoding algorithm, since decoding an in\ufb01nite-length sequence is impossible. We use the proportion of decoded continuations that are longer than a prede\ufb01ned limit, rL = 1 |D| |D| X n=1 1(| \u02c6Y (n)| \u2265L), where \u02c6Y (n) \u223cF(p\u03b8, C(n)) for each context C(n) in D. We call rL the non-termination ratio of the decoding algorithm F for an underlying model and context distribution. A value of rL greater than zero means that some sequences did not terminate within L steps. When L is in\ufb01nity, this implies that the model paired with the decoding algorithm is inconsistent. In practice, we use a \ufb01nite L that is substantially larger than the maximum training sequence length, and we interpret a non-zero rL as evidence that the model paired with the decoding algorithm is inconsistent. We use L = 1500, more than 10 times the max training sequence length.",
  "In practice, we use a \ufb01nite L that is substantially larger than the maximum training sequence length, and we interpret a non-zero rL as evidence that the model paired with the decoding algorithm is inconsistent. We use L = 1500, more than 10 times the max training sequence length. 5github.com/huggingface/tokenizers 6https://spacy.io/",
  "In each experiment, we report the mean and stan- dard deviation of metrics across 10 independent ini- tializations. Unless speci\ufb01ed otherwise, we report metrics using the test context distribution, since the train, valid, and randomly generated context distributions had similar results. Training. We train recurrent language models for sequence completion with maximum likelihood, using the loss L(p\u03b8, Y ) = \u2212PT t=1 log p\u03b8(yt | y<t, c1, . . . , ck), where Y = (c1, . . . , ck, y1, . . . , yT ). This amounts to running the full training sequence through a recurrent model and zeroing the loss for the \ufb01rst k tokens, so that the \ufb01rst k steps correspond to learning a g\u03b8 that encodes the context. Models. We consider recurrent neural networks with hyperbolic tangent activations (tanh-RNN; El- man, 1990) and LSTM units (LSTM-RNN; Hochre- iter and Schmidhuber, 1997).",
  "Models. We consider recurrent neural networks with hyperbolic tangent activations (tanh-RNN; El- man, 1990) and LSTM units (LSTM-RNN; Hochre- iter and Schmidhuber, 1997). We perform an ini- tial hyper-parameter sweep and select the best set of hyper-parameters for each of tanh-RNN and LSTM-RNN based on the validation perplexities.7 With this best set of hyperparameters, we train each of these models with 10 different initializations. The choice of tanh and LSTM RNNs implies that all of the recurrent language models that we train are consistent according to Lemma 3.2. Our LSTM models achieve similar test perplexity (91.86\u00b10.4, word tokenization) to those reported in previous work (Merity et al., 2018); see Appendix D. Additionally, we train self-terminating tanh- RNN and LSTM-RNN variants (De\ufb01nition 4.3) at various values of \u03b5, which controls a lower bound on the termination probability at each step. We use \u03c3(x) = (1 \u2212\u03b5) \u00b7 sigmoid(x).",
  "We use \u03c3(x) = (1 \u2212\u03b5) \u00b7 sigmoid(x). We use the hyper- parameters selected in the preceding grid search. Below, we consider BPE tokenization; similar con- clusions held for word tokenization.8 5.1 Inconsistency of Recurrent LMs In this experiment, we demonstrate evidence of inconsistency with incomplete decoding methods. Table 1 shows non-termination ratios for the re- current language models using the decoding algo- rithms considered in this work. Decoding with an- cestral sampling always resulted in sequences that terminated within L steps, since the induced distri- bution is the same as that of the consistent model. 7Refer to Appendix D for the hyper-parameter ranges. 8Refer to Appendix for results with word tokenization. tanh-RNN LSTM-RNN ancestral 0.00 \u00b1 0.0 0.00 \u00b1 0.0 greedy 12.35 \u00b1 5.18 1.53 \u00b1 1.41 beam-2 1.38 \u00b1 0.95 0.07 \u00b1 0.06 beam-4 0.25 \u00b1 0.19 0.00 \u00b1 0.",
  "0 greedy 12.35 \u00b1 5.18 1.53 \u00b1 1.41 beam-2 1.38 \u00b1 0.95 0.07 \u00b1 0.06 beam-4 0.25 \u00b1 0.19 0.00 \u00b1 0.01 topk-2 0.01 \u00b1 0.01 0.01 \u00b1 0.01 topk-4 0.00 \u00b1 0.0 0.00 \u00b1 0.01 nucleus-0.2 0.06 \u00b1 0.02 0.13 \u00b1 0.15 nucleus-0.4 0.04 \u00b1 0.02 0.02 \u00b1 0.01 consistent topk-2 0.00 \u00b1 0.0 0.00 \u00b1 0.01 consistent topk-4 0.00 \u00b1 0.0 0.00 \u00b1 0.0 consistent nucleus-0.2 0.04 \u00b1 0.02 0.01 \u00b1 0.01 consistent nucleus-0.4 0.02 \u00b1 0.02 0.01 \u00b1 0.",
  "00 \u00b1 0.0 0.00 \u00b1 0.0 consistent nucleus-0.2 0.04 \u00b1 0.02 0.01 \u00b1 0.01 consistent nucleus-0.4 0.02 \u00b1 0.02 0.01 \u00b1 0.01 Table 1: Non-termination ratio (rL (%)) of decoded se- quences using ancestral sampling, incomplete, and con- sistent decoding methods. On the other hand, the non-zero non-termination ratios for the incomplete decoding algorithms sug- gest inconsistency with respect to each algorithm, providing evidence for Theorem 3.4. Using greedy decoding, roughly 12% of all contexts resulted in a non-terminating continua- tion with the tanh-RNN, and roughly 1% with the LSTM-RNN. Nucleus sampling also produced non-terminating sequences with the tanh-RNN (0.06%, nuc-0.2) and LSTM-RNN (0.13%, nuc- 0.2). Top-k sampling yielded a small number of non-terminating samples.",
  "Nucleus sampling also produced non-terminating sequences with the tanh-RNN (0.06%, nuc-0.2) and LSTM-RNN (0.13%, nuc- 0.2). Top-k sampling yielded a small number of non-terminating samples. In general, non- termination approaches zero as k and \u00b5 increase, since \u27e8eos\u27e9has a lower chance of being excluded. Beam search produced non-terminating se- quences with both the tanh-RNN and LSTM-RNN models. This means that \u27e8eos\u27e9was outside of the top tokens (determined by the beam width) con- sidered at each step, since in our experiments we terminated the beam search when a single beam pre\ufb01x contained \u27e8eos\u27e9. Larger beam widths reduce non-termination, similar to increasing k or \u00b5. 5.2 Consistency of the Proposed Methods Consistent sampling. Table 1 shows that consis- tent nucleus and top-k sampling (\u00a74.1) resulted in only terminating sequences, except for a few cases that we attribute to the \ufb01nite limit L used to measure the non-termination ratio.",
  "Table 1 shows that consis- tent nucleus and top-k sampling (\u00a74.1) resulted in only terminating sequences, except for a few cases that we attribute to the \ufb01nite limit L used to measure the non-termination ratio. Consistent nucleus paired with tanh-RNN did not reduce rL as much as when it was paired with LSTM-RNN. Example continuations are shown in Table 2. On pre\ufb01xes that led to non-termination with the base- line method, the quality tends to improve with the consistent variant since the continuation now termi-",
  "Pre\ufb01x One Direction delivered a performance of \u201c Kiss You nucleus \u201d , and the album \u2019s second album , \u201c The X @-@ Files \u201d , \u201c The A. \u201d , \u201c The Preder \u201d , \u201c We \u2019ve Have You \u201d , \u201c I \u2019ve You Wanna Stay \u201d , \u201c The Dream \u201d , \u201c The Bide \u201d , \u201c My Achievement \u201d, \u201c The B. B. \u201d , \u201c A Life \u201d . . . c-nucleus \u201d , and \u201c My Boo \u201d was released on September 29 , 2010 . \u27e8eos\u27e9 Pre\ufb01x Boulter starred in two \ufb01lms in 2008 , nucleus and the band \u2019s music , and \u201c The Rise of Monkey \u201d , \u201c The One With the Way \u201d , \u201c The \u201c Always \u201d , \u201d \u201c Always Your \u201d , \u201c The Wift \u201d , \u201c The Baste \u201d , \u201c The Special With \u201d , \u201c The Way \u201d , \u201c The Special With You \u201d . . . c-nucleus and the latter was released in the United States .",
  ". . c-nucleus and the latter was released in the United States . \u27e8eos\u27e9 Pre\ufb01x This period of unhappiness was the making of Baseline the \u201c most important \u201d of the \u201c mad \u201d , and the \u201c \u201c most important \u201d of the \u201d \u201c \u201d , \u201c the most important \u201d , and the \u201c devil \u201d , \u201c The \u201d , \u201c The One \u201d , \u201c The One \u201d , \u201c The One \u201d , \u201c The One \u201d , \u201c The One \u201d , \u201c The One \u201d , \u201c The One \u201d , \u201c The One \u201d , \u201c The One \u201d , \u201c The One \u201d , \u201c The One \u201d , \u201c The One \u201d , \u201c The One \u201d , \u201c The One \u201d . . . STRLM the \ufb01rst commandment of the poem . \u27e8eos\u27e9 Pre\ufb01x Du Fu \u2019s mother died shortly after he was Baseline a member of the Order of the Order of the Order of the Order of the Order of the Order of the Order of the Order of the Order of the Republic of the Republic of the Republic of the Republic of the Republic of . . . STRLM a member of the Order of the British Empire .",
  ". . STRLM a member of the Order of the British Empire . \u27e8eos\u27e9 Table 2: Continuations with consistent nucleus sampling (\u00b5 = 0.2) and self-terminating LSTM (\u03f5 = 10\u22123). nates. Note that since the model\u2019s non-\u27e8eos\u27e9token probabilities at each step are only modi\ufb01ed by a multiplicative constant, the sampling process can still enter a repetitive cycle (e.g., when the constant is close to 1), though it is guaranteed to terminate. Self-terminating RLM. As seen in Table 3, the self-terminating recurrent language models are con- sistent with respect to greedy decoding, at the ex- pense of perplexity compared to the vanilla model. The value of \u03b5 from De\ufb01nition 4.3, which controls a lower-bound on termination probability at each step, in\ufb02uences both rL and perplexity. When \u03b5 is too large (\u03b5 = 10\u22122), perplexity degrades.",
  "The value of \u03b5 from De\ufb01nition 4.3, which controls a lower-bound on termination probability at each step, in\ufb02uences both rL and perplexity. When \u03b5 is too large (\u03b5 = 10\u22122), perplexity degrades. When \u03b5 is too small (\u03b5 = 10\u22124), the lower-bound grows slowly, so \u27e8eos\u27e9is not guaranteed to be top-ranked within L steps, resulting in a positive rL. An \u03b5 of 10\u22123 balanced consistency and language model- ing quality, with a zero non-termination ratio and perplexity within 8 points of the baseline. As shown in Figure 3, the self-terminating model matches the data length distribution better than the baseline. Example decoded sequences are shown in Table 2. For pre\ufb01xes that led to non- termination with the baseline, the self-terminating models yields \ufb01nite sequences with reasonable quality. The examples suggest that some cases of degenerate repetition (Holtzman et al., 2019; Welleck et al., 2019) are attributed to inconsistency.",
  "The examples suggest that some cases of degenerate repetition (Holtzman et al., 2019; Welleck et al., 2019) are attributed to inconsistency. 5.3 Inconsistency of GPT-2 We perform a \ufb01nal experiment with GPT-2 117M, a transformer language model pre-trained with maximum likelihood on WebText, a collection of ST \u03f5 rL (%) perplexity tanh-RNN ! 10\u22122 00.00 \u00b1 0.0 229.09 \u00b1 9.2 ! 10\u22123 00.00 \u00b1 0.0 191.63 \u00b1 1.4 ! 10\u22124 00.02 \u00b1 0.02 188.36 \u00b1 2.2 \u0017 \u2013 12.35 \u00b1 5.2 186.44 \u00b1 1.4 LSTM ! 10\u22122 0.00 \u00b1 0.0 219.71 \u00b1 9.2 ! 10\u22123 0.00 \u00b1 0.0 186.04 \u00b1 1.6 !",
  "10\u22122 0.00 \u00b1 0.0 219.71 \u00b1 9.2 ! 10\u22123 0.00 \u00b1 0.0 186.04 \u00b1 1.6 ! 10\u22124 0.18 \u00b1 0.35 183.57 \u00b1 2.3 \u0017 \u2013 1.48 \u00b1 1.43 178.19 \u00b1 1.3 Table 3: Non-termination ratio (rL (%)) of greedy- decoded sequences and test perplexity for STRLMs. Figure 3: Lengths of generated sequences using greedy decoding from vanilla and self-terminating LSTMs. scraped web pages (see Radford et al. (2019)). GPT-2 has been observed to produce repetitive text with greedy and beam search (Holtzman et al., 2019). Experimental setup. We use the Wikitext-103 dataset (Merity et al., 2016), a large-scale collec-",
  "tion of Wikipedia articles with over 100 million words and 260 thousand unique tokens. We split the dataset into sequences according to the dataset\u2019s newline boundaries, then split each sequence into a context C and continuation Y , resulting in a dataset of (C, Y ) pairs. Each continuation ends in a spe- cial \u27e8eos\u27e9token. We use a context size of k = 10 tokens, and discard sequences that are length k or shorter. The resulting dataset contains 874,556 training, 1,896 validation, and 2,162 test pairs. We \ufb01ne-tune the pre-trained GPT-2 model using maximum likelihood for 400k steps, and select the model state with the lowest validation perplexity (evaluated every 5k steps). Each training batch con- tains a maximum of 1024 total tokens. We use the implementation and default hyper-parameters from the transformers library (Wolf et al., 2019). We \ufb01ne-tune the self-terminating GPT-2 models in a similar manner, starting from the pre-trained GPT- 2 model and using the same hyper-parameters.",
  "We use the implementation and default hyper-parameters from the transformers library (Wolf et al., 2019). We \ufb01ne-tune the self-terminating GPT-2 models in a similar manner, starting from the pre-trained GPT- 2 model and using the same hyper-parameters. Each model is evaluated using greedy decoding with a maximum sequence length of 500, which was selected so that each decoded validation batch could \ufb01t in GPU memory. We de\ufb01ne the non- termination ratio (rL) using L = 500; this limit is more strict than the limit used in the preced- ing experiments (1500), yet still allows us to see large differences in generation behavior between the model and the ground truth (e.g. see Figure 4). Results. Table 4 shows the non-termination ratio and perplexity of the baseline and self-terminating GPT-2 models. The self-terminating variant pre- vents non-termination, at the cost of perplexity. The model here uses \u03f5 = 2.5 \u00d7 10\u22123, which we se- lected after observing that at higher values of \u03f5, e.g.",
  "The self-terminating variant pre- vents non-termination, at the cost of perplexity. The model here uses \u03f5 = 2.5 \u00d7 10\u22123, which we se- lected after observing that at higher values of \u03f5, e.g. 1.0 \u00d7 10\u22123, the self-terminating model generated sequences longer than the limit used to determine termination (500). Figure 4 shows the length dis- tributions of the baseline GPT-2 continuations and those of the self-terminating GPT-2. The GPT- 2 117M model generates many sequences at or near the maximum sequence length (500), unlike the ground-truth data. Introducing self-termination shifts the mass towards shorter sequences, whose lengths are also present in the ground-truth data. 6 Future Directions The methods we proposed in this paper resolve in- consistency by changing the decoding algorithm or model parameterization. Another approach is to address inconsistency in the learning phase.",
  "Introducing self-termination shifts the mass towards shorter sequences, whose lengths are also present in the ground-truth data. 6 Future Directions The methods we proposed in this paper resolve in- consistency by changing the decoding algorithm or model parameterization. Another approach is to address inconsistency in the learning phase. One rL (%) perplexity GPT2-117M 37.91 20.92 GPT2-117M ST 00.00 27.25 Table 4: Non-termination ratio (rL (%)) of greedy- decoded sequences and perplexity for GPT2-117M and the self-terminating variant (ST) on Wikitext-103. interesting direction is to investigate whether the lack of decoding in maximum likelihood learning is a cause of inconsistency. Maximum likelihood learning \ufb01ts the model p\u03b8 using the data distribu- tion, whereas a decoded sequence from the trained model follows the distribution qF induced by a decoding algorithm. Sequence-level learning, how- ever, uses a decoding algorithm during training (e.g., Ranzato et al. (2016)), which we hypothe- size can result in a good sequence generator that is consistent with respect to incomplete decoding.",
  "Sequence-level learning, how- ever, uses a decoding algorithm during training (e.g., Ranzato et al. (2016)), which we hypothe- size can result in a good sequence generator that is consistent with respect to incomplete decoding. 7 Conclusion We extended the notion of consistency of a recur- rent language model put forward by Chen et al. (2017) to incorporate a decoding algorithm, and used it to analyze the discrepancy between a model and the distribution induced by a decoding algo- rithm. We proved that incomplete decoding is in- consistent, and proposed two methods to prevent this: consistent decoding and the self-terminating recurrent language model. Using a sequence com- pletion task, we con\ufb01rmed that empirical incon- sistency occurs in practice, and that each method prevents inconsistency while maintaining the qual- ity of generated sequences. We suspect the absence of decoding in maximum likelihood estimation as a cause behind this inconsistency, and suggest inves- tigating sequence-level learning as an alternative. Acknowledgements We thank Chris Dyer, Noah Smith and Kevin Knight for valuable discussions.",
  "We suspect the absence of decoding in maximum likelihood estimation as a cause behind this inconsistency, and suggest inves- tigating sequence-level learning as an alternative. Acknowledgements We thank Chris Dyer, Noah Smith and Kevin Knight for valuable discussions. This work was supported by NSF Award 1922658 NRT-HDR: FU- TURE Foundations, Translation, and Responsibil- ity for Data Science; Samsung Advanced Institute of Technology (Next Generation Deep Learning: from pattern recognition to AI); and Samsung Re- search (Improving Deep Learning using Latent Structure). KC thanks eBay and NVIDIA for their support.",
  "References Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman Ganchev, Slav Petrov, and Michael Collins. 2016. Globally normal- ized transition-based neural networks. In 54th An- nual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers, volume 4, pages 2442\u20132452. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd Inter- national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. T. L. Booth and R. A. Thompson. 1973. Applying prob- ability measures to abstract languages. IEEE Trans- actions on Computers, C-22(5):442\u2013450. John S Bridle. 1990.",
  "T. L. Booth and R. A. Thompson. 1973. Applying prob- ability measures to abstract languages. IEEE Trans- actions on Computers, C-22(5):442\u2013450. John S Bridle. 1990. Probabilistic interpretation of feedforward classi\ufb01cation network outputs, with re- lationships to statistical pattern recognition. In Neu- rocomputing, pages 227\u2013236. Springer. Yining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan May, and Kevin Knight. 2017. Recurrent neural networks as weighted language recognizers. arXiv preprint arXiv:1711.05408. Kyunghyun Cho, Bart van Merri\u00a8enboer, Dzmitry Bah- danau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder\u2013decoder ap- proaches. In Proceedings of SSST-8, Eighth Work- shop on Syntax, Semantics and Structure in Statisti- cal Translation, pages 103\u2013111, Doha, Qatar. Asso- ciation for Computational Linguistics.",
  "In Proceedings of SSST-8, Eighth Work- shop on Syntax, Semantics and Structure in Statisti- cal Translation, pages 103\u2013111, Doha, Qatar. Asso- ciation for Computational Linguistics. Jeffrey L Elman. 1990. Finding structure in time. Cog- nitive science, 14(2):179\u2013211. Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi- erarchical neural story generation. arXiv preprint arXiv:1805.04833. Edouard Grave, Armand Joulin, and Nicolas Usunier. 2017. Improving neural language models with a con- tinuous cache. In 5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings. Alex Graves. 2013. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850. Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997. Long short-term memory.",
  "Alex Graves. 2013. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850. Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735\u20131780. Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degener- ation. arXiv preprint arXiv:1904.09751. John Lafferty, Andrew McCallum, and Fernando C N Pereira. 2001. Conditional random \ufb01elds: Prob- abilistic models for segmenting and labeling se- quence data. ICML \u201901 Proceedings of the Eigh- teenth International Conference on Machine Learn- ing. G\u00b4abor Melis, Chris Dyer, and Phil Blunsom. 2018. On the state of the art of evaluation in neural language models. In 6th International Conference on Learn- ing Representations, ICLR 2018 - Conference Track Proceedings.",
  "G\u00b4abor Melis, Chris Dyer, and Phil Blunsom. 2018. On the state of the art of evaluation in neural language models. In 6th International Conference on Learn- ing Representations, ICLR 2018 - Conference Track Proceedings. Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2018. Regularizing and optimizing LSTM language models. In 6th International Conference on Learning Representations, ICLR 2018 - Confer- ence Track Proceedings. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture mod- els. ArXiv, abs/1609.07843. Kenton Murray and David Chiang. 2018. Correct- ing length bias in neural machine translation. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 212\u2013223, Brus- sels, Belgium. Association for Computational Lin- guistics. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.",
  "Association for Computational Lin- guistics. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9. Marc\u2019Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2016. Sequence level train- ing with recurrent neural networks. In 4th Inter- national Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings. Pavel Sountsov and Sunita Sarawagi. 2016. Length bias in encoder decoder models and a case for global conditioning. In Proceedings of the 2016 Confer- ence on Empirical Methods in Natural Language Processing, pages 1516\u20131525, Austin, Texas. Asso- ciation for Computational Linguistics. Felix Stahlberg and Bill Byrne. 2019. On NMT search errors and model errors: Cat got your tongue?",
  "Asso- ciation for Computational Linguistics. Felix Stahlberg and Bill Byrne. 2019. On NMT search errors and model errors: Cat got your tongue? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 3354\u2013 3360, Hong Kong, China. Association for Computa- tional Linguistics. Ilya Sutskever, James Martens, and Geoffrey Hinton. 2011. Generating text with recurrent neural net- works. In Proceedings of the 28th International Conference on Machine Learning, ICML 2011. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all",
  "you need. In Advances in Neural Information Pro- cessing Systems. Oriol Vinyals, Google Quoc, and V Le. 2015. A Neu- ral Conversational Model. In ICML Deep Learning Workshop. Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Di- nan, Kyunghyun Cho, and Jason Weston. 2019. Neu- ral text generation with unlikelihood training. arXiv preprint arXiv:1908.04319. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, R\u00b4emi Louf, Morgan Fun- towicz, et al. 2019. Transformers: State-of-the- art natural language processing. arXiv preprint arXiv:1910.03771. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.",
  "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google\u2019s neural machine translation system: Bridging the gap between hu- man and machine translation. arXiv preprint arXiv:1609.08144.",
  "A Additional De\ufb01nitions In contrast to greedy decoding, beam search with width k, Fbeam-k, operates on the level of partial sequences or pre\ufb01xes. De\ufb01nition A.1 (Pre\ufb01x). A pre\ufb01x \u03c1t is an ordered collection of items from V . The score of a pre\ufb01x is s(\u03c1t) = t X \u03c4=1 log p\u03b8(y\u03c4 = \u03c1t[\u03c4] | \u03c1t[< \u03c4], C), where \u03c1t[\u03c4] is a token at time \u03c4 from \u03c1t. Starting from a set of empty pre\ufb01xes, at each iteration a new pre\ufb01x set is formed by expanding each pre\ufb01x, then choosing the k highest scoring expanded pre\ufb01xes. De\ufb01nition A.2 (Beam search). Beam search with width k, Fbeam\u2212k, generates a sequence from a recurrent language model p\u03b8 by maintaining a size- k pre\ufb01x set Ptop t . Starting with P top 0 = \u2205, at each iteration t \u2208{1, 2, . . .}",
  "Starting with P top 0 = \u2205, at each iteration t \u2208{1, 2, . . .} beam search forms a new pre\ufb01x set Ptop t by expanding the current set, Pt = S \u03c1\u2208Ptop t\u22121{\u03c1 \u25e6v | v \u2208V } (where \u03c1 \u25e6v is con- catenation), then choosing the k highest scoring elements: Ptop t = arg top-k \u03c1\u2208Pt s(\u03c1). Any \u03c1 \u2208Ptop t end- ing with \u27e8eos\u27e9is restricted from being expanded further, and is added to a set S. Beam search ends when S contains k sequences, and returns the high- est scoring sequence in S. B Proof of Lemmas in Section 3 Lemma 3.1. If a recurrent language model p\u03b8 is consistent, p\u03b8(|Y | = \u221e| C) = 0 for any probable context C. Proof. Suppose there exists a probable context \u02dcC such that p\u03b8(|Y | = \u221e| \u02dcC) > 0.",
  "Suppose there exists a probable context \u02dcC such that p\u03b8(|Y | = \u221e| \u02dcC) > 0. Then p\u03b8(|Y | = \u221e) = E [p\u03b8(|Y | = \u221e| C)] \u2265p( \u02dcC)p\u03b8(|Y | = \u221e| \u02dcC) > 0, which contradicts the consistency of the model p\u03b8. Lemma 3.2. A recurrent language model p\u03b8 is consistent if \u2225ht\u2225p is uniformly bounded for some p \u22651. Proof. Let B > 0 be an upper bound such that \u2225ht\u2225p < B for all t. Let q be the conjugate of p satisfying 1/p + 1/q = 1. Then we have from H\u00a8older\u2019s inequality, for all v \u2208V and t, u\u22a4 v ht \u2264\u2225u\u22a4 v ht\u22251 \u2264\u2225ht\u2225p\u2225uv\u2225q < Bu+, where u+ = maxv\u2208V \u2225uv\u2225q.",
  "Note that log X v\u2208V eu\u22a4 v ht+cv \u2264log \u0012 max v\u2208V eu\u22a4 v ht+cv \u00d7 |V | \u0013 \u2264max v\u2208V {u\u22a4 v ht + cv} + log |V | < Bu+ + c+ + log |V |, where c+ = maxv\u2208V cv.",
  "For a given y<t and con- text C, log p\u03b8(\u27e8eos\u27e9| y<t, C) =(u\u22a4 \u27e8eos\u27e9ht + c\u27e8eos\u27e9) \u2212log X v\u2208V eu\u22a4 v ht+cv >(\u2212Bu+ + c\u27e8eos\u27e9) \u2212(Bu+ + c+ + log |V |) > \u2212\u221e, and it follows that p\u03b8(\u27e8eos\u27e9| y<t, C) > \u03be > 0 for some strictly positive constant \u03be. Then p\u03b8(|Y | = \u221e) = lim t\u2192\u221ep\u03b8(|Y | > t) = lim t\u2192\u221eE [p\u03b8(|Y | > t | C)] = E h lim t\u2192\u221ep\u03b8(|Y | > t | C) i \u2264E h lim t\u2192\u221e(1 \u2212\u03be)ti = 0, and hence p\u03b8 is consistent. Lemma 3.3. A consistent decoding algorithm with respect to a consistent recurrent language model decodes only probable sequences.",
  "Lemma 3.3. A consistent decoding algorithm with respect to a consistent recurrent language model decodes only probable sequences. That is, if qF(Y | C) > 0, then p\u03b8(Y | C) > 0 for any proba- ble context C. Proof. Suppose there exists a decoded sequence \u02dcY by F and probable context \u02dcC such that qF( \u02dcY | \u02dcC) > 0 but p\u03b8( \u02dcY | \u02dcC) = 0. By Remark 2.1, the sequence \u02dcY is of in\ufb01nite length and thus qF(|Y | = \u221e| \u02dcC) \u2265qF( \u02dcY | \u02dcC) > 0, which contra- dicts the consistency of qF by Lemma 3.1. C Proofs for Section 4 Theorem 4.1. Suppose a recurrent LM p\u03b8 has uni- formly bounded \u2225ht\u2225p for some p \u22651.",
  "C Proofs for Section 4 Theorem 4.1. Suppose a recurrent LM p\u03b8 has uni- formly bounded \u2225ht\u2225p for some p \u22651. If a de- coding algorithm F satis\ufb01es qF(\u27e8eos\u27e9| y<t, C) \u2265 p\u03b8(\u27e8eos\u27e9| y<t, C) for every pre\ufb01x y<t and context C, then the decoding algorithm F is consistent with respect to the model p\u03b8.",
  "Proof. By Lemma 3.2 the model p\u03b8 is con- sistent and p\u03b8(\u27e8eos\u27e9| y<t, C) > \u03be for some positive value \u03be. Thus, qF(\u27e8eos\u27e9| y<t, C) \u2265 p\u03b8(\u27e8eos\u27e9| y<t, C) > \u03be. For t \u22651, qF(|Y | > t | C) = qF(y1 \u0338= \u27e8eos\u27e9, \u00b7 \u00b7 \u00b7 , yt \u0338= \u27e8eos\u27e9| C) \u2264(1 \u2212\u03be)t. Taking the limit t \u2192\u221eand expectation over C, we have qF(|Y | = \u221e) = EC h lim t\u2192\u221eqF(|Y | > t | C) i \u2264lim t\u2192\u221e(1 \u2212\u03be)t = 0, from which the decoding algorithm is consistent. Theorem 4.2. Greedy decoding is consistent with respect to any self-terminating recurrent LM. Proof.",
  "Theorem 4.2. Greedy decoding is consistent with respect to any self-terminating recurrent LM. Proof. Let p\u27e8eos\u27e9 t denote p\u03b8(\u27e8eos\u27e9| y<t, C) and a\u27e8eos\u27e9 t denote u\u22a4 \u27e8eos\u27e9ht + c\u27e8eos\u27e9. By De\ufb01nition 4.3 we have p\u27e8eos\u27e9 t = 1 \u2212\u03c3(a\u27e8eos\u27e9 t )(1 \u2212p\u27e8eos\u27e9 t\u22121 ) = 1 \u2212 tY t\u2032=0 \u03c3(a\u27e8eos\u27e9 t\u2032 ) \u22651 \u2212(1 \u2212\u03f5)t+1. Take B = \u2212log 2/ log(1 \u2212\u03f5). We then have p\u27e8eos\u27e9 t > 1/2 for all t > B, which implies that \u27e8eos\u27e9is always the most probable token after time step B. Hence, the sequence length is less than B with probability 1. Theorem 4.3. Beam search with width k, Fbeam\u2212k, is consistent with respect to any STRLM. Proof.",
  "Theorem 4.3. Beam search with width k, Fbeam\u2212k, is consistent with respect to any STRLM. Proof. Let S(\u03c1) be the size-k set of sequences kept by Fbeam\u2212k that start with a pre\ufb01x \u03c1. Take B = \u2212log 2/ log(1 \u2212\u03f5) as in the proof of Theorem 4.2. Suppose that there exists at least one pre\ufb01x \u02c6\u03c1 \u2208P top B which does not end with \u27e8eos\u27e9. We \ufb01rst want to show that \u02c6\u03c1 induces at most k more steps in beam search with width k, that is, Y \u2208S(\u02c6\u03c1) implies |Y | \u2264B + k. We know from the proof of Theorem 4.2 that an STRLM p\u03b8 satis\ufb01es: for any context C and v \u2208V \\ {\u27e8eos\u27e9}, p\u03b8(\u27e8eos\u27e9| \u02c6\u03c1, C) > p\u03b8(v | \u02c6\u03c1, C). For any subsequence y = (y1, . . .",
  "For any subsequence y = (y1, . . . , yl) with y1 \u0338= \u27e8eos\u27e9, p\u03b8(\u02c6\u03c1 \u25e6y | \u02c6\u03c1, C) = lY i=1 p\u03b8(yi | \u02c6\u03c1 \u25e6y<i, C) \u2264p\u03b8(y1 | \u02c6\u03c1, C) < p\u03b8(\u27e8eos\u27e9| \u02c6\u03c1, C). Thus, \u02c6\u03c1 \u25e6\u27e8eos\u27e9is the most probable sequence among sequences starting with the pre\ufb01x \u02c6\u03c1, and it follows that \u02c6\u03c1 \u25e6\u27e8eos\u27e9\u2208S(\u02c6\u03c1). Thus, in S(\u02c6\u03c1), there are (k \u22121) sequences start- ing with \u02c6\u03c1 \u25e6v for v \u2208V \\ {\u27e8eos\u27e9}. By the same argument, at each step at least one sequence ending with \u27e8eos\u27e9is added to S(\u02c6\u03c1), and therefore at time step (B + k), k sequences ending with \u27e8eos\u27e9are in S(\u02c6\u03c1).",
  "By the same argument, at each step at least one sequence ending with \u27e8eos\u27e9is added to S(\u02c6\u03c1), and therefore at time step (B + k), k sequences ending with \u27e8eos\u27e9are in S(\u02c6\u03c1). Note that the result set S by Fbeam\u2212k (De\ufb01ni- tion 2.11) satis\ufb01es S \u2286 [ \u03c1\u2208P top B S(\u03c1). Since each \u03c1 \u2208P top B induces sequences of length at most B + k, we have p\u03b8(|Y | > B + k | C) = 0. Taking the expectation over C yields the consis- tency of the model p\u03b8.",
  "Parameter Values Hidden Size {256, 512, 1024} Dropout {0.1, 0.3, 0.5} Embedding Weight Tying {True, False} Table 5: Grid search speci\ufb01cation. The values selected for the LSTM-RNN and tanh-RNN models are shown in bold and italics, respectively (word tokenization). D Additional Results and Experiment Details Training. Each model is trained on a single Nvidia P40 GPU for up to 100 epochs, stopping when validation perplexity does not decrease for 10 consecutive epochs. Hyper-parameters. Tables 5,6 show the grid search speci\ufb01cations. All models were 2 layers and were trained with the Adam optimizer. Model perplexities. Tables 10, 11 shows train and test perplexities for the tanh-RNN and LSTM- RNN models using word and BPE tokenization, respectively. Additional example continuations. Table 12 shows additional greedy-decoded continuations us- ing a self-terminating LSTM-RNN and the baseline LSTM-RNN with BPE tokenization. GPT-2 length distributions.",
  "Additional example continuations. Table 12 shows additional greedy-decoded continuations us- ing a self-terminating LSTM-RNN and the baseline LSTM-RNN with BPE tokenization. GPT-2 length distributions. Figure 4 shows the length distributions of ground-truth continuations, continuations from GPT-2 117M, and continuations from the self-terminating GPT-2 117M. Figure 4: Lengths of ground-truth and greedy-decoded continuations from the baseline GPT-2 117M and self- terminating GPT-2 117M models (\u03f5 = 0.0025). Parameter Values Hidden Size {256, 512, 1024} Dropout {0.1, 0.3, 0.5} Embedding Weight Tying {True, False} Table 6: Grid search speci\ufb01cation. The values selected for the LSTM-RNN and tanh-RNN models are shown in bold and italics, respectively (BPE tokenization). Type # Train # Valid # Test |V | Avg.",
  "The values selected for the LSTM-RNN and tanh-RNN models are shown in bold and italics, respectively (BPE tokenization). Type # Train # Valid # Test |V | Avg. len Word 78274 8464 9708 33182 24 BPE 83344 8721 10156 19483 28 Table 7: Wikitext2 statistics.",
  "Type # Train # Valid # Test |V | Avg. len Word 78274 8464 9708 33182 24 BPE 83344 8721 10156 19483 28 Table 7: Wikitext2 statistics. tanh-RNN LSTM-RNN ancestral 0.00 \u00b1 0.0 0.00 \u00b1 0.0 greedy 6.07 \u00b1 5.6 1.03 \u00b1 0.3 beam-2 1.21 \u00b1 0.3 0.07 \u00b1 0.1 beam-4 0.29 \u00b1 0.1 0.00 \u00b1 0.0 topk-2 0.84 \u00b1 0.8 0.00 \u00b1 0.0 topk-4 0.02 \u00b1 0.0 0.00 \u00b1 0.0 nucleus-0.2 2.49 \u00b1 0.2 0.76 \u00b1 0.3 nucleus-0.4 0.32 \u00b1 0.1 0.22 \u00b1 0.1 Table 8: Non-termination ratio (rL (%)) of decoded sequences using ancestral sampling and incomplete de- coding methods (word tokenization).",
  "ST \u03f5 rL (%) perplexity tanh-RNN ! 10\u22122 0.00 \u00b1 0.0 150.07 \u00b1 2.7 ! 10\u22123 0.00 \u00b1 0.0 138.01 \u00b1 0.6 ! 10\u22124 1.04 \u00b1 0.6 138.67 \u00b1 1.8 \u0017 \u2013 6.07 \u00b1 5.6 136.57 \u00b1 1.8 LSTM ! 10\u22122 0.00 \u00b1 0.0 101.24 \u00b1 0.3 ! 10\u22123 0.00 \u00b1 0.0 94.33 \u00b1 0.6 ! 10\u22124 0.94 \u00b1 0.5 94.15 \u00b1 0.8 \u0017 \u2013 1.03 \u00b1 0.3 91.86 \u00b1 0.4 Table 9: Non-termination ratio (rL (%)) of greedy- decoded sequences and test perplexity for self- terminating recurrent models (word tokenization).",
  "model context perplexity tanh-RNN train 91.54 \u00b1 7.9 tanh-RNN test 136.57 \u00b1 1.8 LSTM-RNN train 45.80 \u00b1 2.5 LSTM-RNN test 91.86 \u00b1 0.4 Table 10: Perplexities of trained recurrent language models (word tokenization).",
  "model context perplexity tanh-RNN train 61.20 \u00b1 1.2 tanh-RNN test 186.44 \u00b1 1.4 LSTM-RNN train 72.72 \u00b1 2.4 LSTM-RNN test 178.39 \u00b1 1.2 Table 11: Perplexities of trained recurrent language models (BPE tokenization).",
  "Pre\ufb01x Payne was quoted as saying : \u201c With the album nucleus \u2019s \u201d album , \u201c The Predious \u201d , \u201c The One With the Wind \u201d , \u201c I \u2019ve Ever \u2019t Have You \u2019s My Way \u201d , \u201c I \u2019ve Ever It \u2019s Johnny \u201d , \u201c The Predes \u201d , \u201c \u201c Always \u201d , \u201c The Predatory Was \u201d , \u201c The Dream \u201d , \u201c The Baste \u201d , \u201d \u201c Always Boy \u201d , \u201c My Drum \u201d , \u201c The Simpsons \u201d , \u201c \u201c Always Man \u201d, \u201c The \u201c Sweet Night \u201d , . . .",
  ". . c-nucleus \u2019s \u201d album , \u201c The Predious \u201d , \u201c The One With the Wind \u201d , \u201c I \u2019ve Ever \u2019t Have You \u2019s My Way \u201d \u27e8eos\u27e9 Pre\ufb01x In a 2006 interview , fellow actor Ben Whish nucleus \u2019s father , a young actor , and a romantic relationship with the show , \u201c The One Where the The Simpsons \u201d , \u201c The Pape \u201d , \u201c The Next Generation \u201d , \u201c The Sixth Extinction \u201d , \u201c We \u2019t You Wanna Stay \u201d , \u201c The Dream \u201d , \u201c The Predator \u201d , \u201c The Collection \u201d , \u201c The Big Lear \u201d , \u201c The Predor \u201d , \u201c The Predation \u201d , \u201c My Blue \u201d , \u201c The Simpsons \u201d , \u201c The Sixth Extinction \u201d , \u201c My Love \u201d , \u201c The Rise of the Year \u201d , \u201c The Simpsons \u201d , \u201c The Predator \u201d , \u201c My Dream \u201d , . . . c-nucleus was the \ufb01rst time in the \ufb01lm , and was published in the same episode of the season .",
  ". . c-nucleus was the \ufb01rst time in the \ufb01lm , and was published in the same episode of the season . \u27e8eos\u27e9 Pre\ufb01x Most of what is known of Du Fu \u2019s Baseline \u201c the \u201d , the \u201d \u201c great \u201d , the \u201d \u201c \u201d , \u201c the most important \u201d , \u201c the most important \u201d , \u201c Ode to the Nightingale \u201d , \u201c Ode to the Nightingale \u201d , \u201c Ode to the Nightingale \u201d , \u201c Ode to the Nightingale \u201d , \u201c Ode to the Nightingale \u201d , \u201c Ode to the Nightingale \u201d , \u201c Ode to the Nightingale \u201d , \u201c Ode to the Nightingale \u201d , . . . STRLM Coty , was a \u201c one of the most important \u201d of the American science \ufb01ction .",
  ". . STRLM Coty , was a \u201c one of the most important \u201d of the American science \ufb01ction . \u27e8eos\u27e9 Pre\ufb01x He was relieved by Yan Wu , a friend and Baseline the \ufb01rst wife of the Order of the Order of the Order of the Order of the Order of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic . . . STRLM the wife of the Royal Navy . \u27e8eos\u27e9 Table 12: More continuations with consistent nucleus sampling (\u00b5 = 0.2) and self-terminating LSTM (\u03f5 = 10\u22123) with BPE tokenization."
]