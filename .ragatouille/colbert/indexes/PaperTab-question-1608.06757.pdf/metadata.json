{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Robust Named Entity Recognition in Idiosyncratic Domains Sebastian Arnold Felix A. Gers Torsten Kilias Alexander L\u00a8oser Beuth University of Applied Sciences Luxemburger Stra\u00dfe 10 13353 Berlin, Germany {sarnold,gers,tkilias,aloeser}@beuth-hochschule.de Abstract Named entity recognition often fails in idiosyncratic domains. That causes a problem for depending tasks, such as en- tity linking and relation extraction. We propose a generic and robust approach for high-recall named entity recognition. Our approach is easy to train and offers strong generalization over diverse domain- speci\ufb01c language, such as news docu- ments (e.g. Reuters) or biomedical text (e.g. Medline). Our approach is based on deep contextual sequence learning and utilizes stacked bidirectional LSTM net- works. Our model is trained with only few hundred labeled sentences and does not rely on further external knowledge. We re- port from our results F1 scores in the range of 84\u201394% on standard datasets.",
      "Our approach is based on deep contextual sequence learning and utilizes stacked bidirectional LSTM net- works. Our model is trained with only few hundred labeled sentences and does not rely on further external knowledge. We re- port from our results F1 scores in the range of 84\u201394% on standard datasets. 1 Introduction Information extraction tasks have become very important not only in the Web, but also for in- house enterprise settings. One of the crucial steps towards understanding natural language is named entity recognition (NER), which aims to extract mentions of entity names in text. NER is nec- essary for many higher-level tasks such as en- tity linking, relation extraction, building knowl- edge graphs, question answering and intent based search. In these scenarios, NER recall is critical, as candidates that are never generated can not be recovered later (Hachey et al., 2013). Challenges. Pink et al. (2014) show that NER components can reduce the search space for slot \ufb01lling tasks by 99.8% with a recall loss of 15%.",
      "Challenges. Pink et al. (2014) show that NER components can reduce the search space for slot \ufb01lling tasks by 99.8% with a recall loss of 15%. However, large effort is required to adapt most an- notators to specialized domains, such as biomedi- cal documents. When focusing on recall for these domains, we face three major problems. First, the language used in the documents is often id- iosyncratic and cannot be effectively identi\ufb01ed by standard natural language processing (NLP) tools (Prokofyev et al., 2014). Second, training these domains is dif\ufb01cult: data is sparse, data may con- tain a large number of non-linkable entity men- tions (NILs) and large labeled gold standards are hardly available. Third, applications vary greatly and we cannot standardize annotation guidelines to meet all of their requirements (Ling et al., 2015). For example, NER on news texts might fo- cus on proper named entity annotation (e.g.",
      "Third, applications vary greatly and we cannot standardize annotation guidelines to meet all of their requirements (Ling et al., 2015). For example, NER on news texts might fo- cus on proper named entity annotation (e.g. peo- ple, companies and locations), whereas phrase recognition on medical text might include the an- notation of common concepts (e.g. medical terms and treatments). We therefore focus on a general- ized NER component with high recall, which can be trained ad-hoc with only few labeled examples. Common error analysis. Ling et al. (2015) point out common errors of NER systems, which yield non-recognized mentions (false negatives), invalid detections (false positives), wrong bound- aries (e.g. multi-word mentions, missing deter- miners) and annotation errors from human label- ers (e.g., correct answers are not marked as cor- rect, unclear annotation guidelines). Consider the following example taken from the biomedical GE- NIA corpus (Ohta et al., 2002), with underlined named entity mentions: Example: Engagement of the Lewis X antigen (CD15) results in monocyte activation.",
      "Consider the following example taken from the biomedical GE- NIA corpus (Ohta et al., 2002), with underlined named entity mentions: Example: Engagement of the Lewis X antigen (CD15) results in monocyte activation. Nuclear extracts of anti- CD15 cross-linked cells demonstrated en- hanced levels of the transcriptional factor activator protein-1, minimally changed nuclear factor-kappa B, and did not affect SV40 promoter speci\ufb01c protein-1. arXiv:1608.06757v1  [cs.CL]  24 Aug 2016",
      "We observe that common errors originate from a manifold number of sources, which are frequently: \u2022 non-verbatim mentions (e.g. misspellings, al- ternate writings: monoyctes, Lewis-X) \u2022 part-of-speech (POS) tagging errors (e.g. unidenti\ufb01ed NP tags: monoycte/JJ) \u2022 wrong capitalization (e.g. uppercase head- lines, lowercase proper names) \u2022 unseen or novel words (e.g. idiosyncratic lan- guage: anti-CD15) \u2022 irregular word context (e.g. collapsed lists, semi-structured data, invalid segmentation) Our contribution. We contribute DATEXIS- NER, a generic annotator for robust named entity recognition that can be trained for various domains with low human labeling effort. DATEXIS-NER does not depend on domain-speci\ufb01c rules, dictio- naries, \ufb01ne-tuning, syntactic annotation or exter- nal knowledge bases. Instead, our approach is built from scratch and is based on core character features of text. We train our model for news and biomedical domains with raw text data and few hundred labels.",
      "Instead, our approach is built from scratch and is based on core character features of text. We train our model for news and biomedical domains with raw text data and few hundred labels. From our results, we report equal performance compared to state-of-the-art NER an- notators with high 90% F1 scores for common NER corpora, such as CoNLL2003, KORE50, ACE2004 and MSNBC. We show on the highly domain-speci\ufb01c biomedical GENIA corpus that our approach adapts to various idiosyncratic do- mains. In particular, we observe that bidirectional long short-term memory (LSTM) networks cap- ture useful distributional context for NER appli- cations and generic letter-trigram word encoding with surface forms compensates typing and cap- italization errors. With a combination of these techniques, we achieve better context representa- tion than word2vec models trained with signi\ufb01- cantly larger corpora. The rest of this paper is structured as follows. In Section 2, we discuss related work. We intro- duce our approach of robust named entity recog- nition in Section 3.",
      "The rest of this paper is structured as follows. In Section 2, we discuss related work. We intro- duce our approach of robust named entity recog- nition in Section 3. In Section 4, we evaluate our approach compared to state-of-the-art annotators and discuss the most common errors in the com- ponents of our system. We conclude in Section 5 and propose future work on our approach. 2 Related Work Named entity recognition. The task of NER has been extensively studied with various eval- uation in the last decades: MUC-6, MUC-7, CoNLL2002, CoNLL2003 and ACE. The stan- dard approach to NER is the application of dis- criminative tagging (Collins, 2002) to the task of NER (McCallum and Li, 2003), often with lin- ear chain Conditional Random Field (CRF), Hid- den Markov (HMM) or Maximum Entropy Hid- den Markov Models (MEMM). Later, Bengio et al.",
      "Later, Bengio et al. (2003) used continuous-space language mod- els, where type-to-vector word mappings can be learned using backpropagation. Mikolov et al. (2013) achieved a more effective vector represen- tation using the skip-gram model. The model opti- mizes the likelihood of tokens over a window sur- rounding a given token. This training process pro- duces a linear classi\ufb01er that predicts words condi- tioned on the central token\u2019s vector representation. Recall bounds for idiosyncratic entity linking. Named entity linking is the task to match tex- tual mentions of named entities to a knowledge base (Shen et al., 2015). This task requires a set of candidate mentions from sentences. As a re- sult, the recall from the underlying NER system constitutes an upper bound for entity linking ac- curacy (Hachey et al., 2013). Moreover, Pink et al.",
      "This task requires a set of candidate mentions from sentences. As a re- sult, the recall from the underlying NER system constitutes an upper bound for entity linking ac- curacy (Hachey et al., 2013). Moreover, Pink et al. (2014) show that \u201cstate-of-the-art systems are substantially limited by low recall\u201d and don\u2019t per- form well especially on idiosyncratic data while Prokofyev et al. (2014) highlight that terms with high novelty or high speci\ufb01city cannot ef\ufb01ciently be linked by current systems. State-of-the-art NER implementations. We distinguish between three broad categories for generating candidate entities: Babelfy (Moro et al., 2014), Entityclassi\ufb01er.eu (Dojchinovski and Kliegr, 2013), DBpedia Spotlight (Mendes et al., 2011) or TagMe2 (Ferragina and Scaiella, 2010) spot noun chunks and \ufb01lter them with dictionar- ies, often derived from Wikipedia.",
      "Stanford NER (Manning et al., 2014) or LingPipe1 utilize dis- criminative tagging approaches. FOX (Speck and Ngomo, 2014) or NERD-ML (Van Erp et al., 2013) combine several approaches in an ensem- ble learner for enhancing precision. The GE- NIA tagger2 is a tagger speci\ufb01cally tuned for biomedical text. It is trained on the GENIA-based BioNLP/NLPBA 2004 data set (Kim et al., 2004) that includes named entity recognition for biomed- ical text. The biomedical NER system of Zhou and 1http://alias-i.com/lingpipe/ 2http://www.nactem.ac.uk/tsujii/GENIA/tagger/",
      "Su (2004) is built using HMM and an additional SVM with sigmoid. It uses lexical-level features, e.g. word formation and morphological patterns, and utilizes dictionaries. The system of Finkel et al. (2004) uses a MEMM. Settles (2004) use CRF classi\ufb01ers with syntactical features and synset dic- tionaries. Basically, all these systems bene\ufb01t from our work. 3 Robust Contextual Word Labeling We abstract the task of NER as sequential word la- beling problem. Figure 1 illustrates an example for sequential transformation of a sentence into word labels. We express each sentence in a document as a sequence of words: w = (w0, w1, . . . , wn), e.g. w0 = Aspirin. We de\ufb01ne a mention as the longest possible span of adjacent tokens that refer to a an entity or relevant concept of a real-world ob- ject, such as Aspirin (ASA). We further assume that mentions are non-recursive and non-overlapping.",
      "We de\ufb01ne a mention as the longest possible span of adjacent tokens that refer to a an entity or relevant concept of a real-world ob- ject, such as Aspirin (ASA). We further assume that mentions are non-recursive and non-overlapping. To encode boundaries of the mention span, we adapt the idea of Ramshaw and Marcus (1995), which has been adapted as BIO2 standard in the CoNLL2003 shared task (Tjong Kim Sang and De Meulder, 2003). We assign labels {B, I, O} to each token to mark begin, inside and outside of a mention from left to right. We use the input se- quence w together with a target sequence y of the same length that contains a BIO2 label for each word: y = (y0, y1, . . . , yn), e.g. y0 = B. To pre- dict the most likely label \u02c6yt of a token regarding its context, we utilize recurrent neural networks.",
      ". . , yn), e.g. y0 = B. To pre- dict the most likely label \u02c6yt of a token regarding its context, we utilize recurrent neural networks. 3.1 Robust Word Encoding Methods We have shown that most common errors for recall loss are misspellings, POS errors, capitalization, unseen words and irregular context. Therefore we generalize our model throughout three layers: ro- bust word encoding, in-sentence word context and contextual sequence labeling. Letter-trigram word hashing to overcome spelling errors. Dictionary-based word vector- ization methods suffer from sparse training sets, especially in the case of non-verbatim mentions, rare words, typing and capitalization errors. For example, the word2vec model of Mikolov et al. (2013) generalizes insuf\ufb01ciently for rare words in idiosyncratic domains or for misspelled words, since for these words no vector representation is learned at training time. In the GENIA data set, we notice 27% unseen words (dictionary misses) in Aspirin has  an antiplatelet effect .",
      "In the GENIA data set, we notice 27% unseen words (dictionary misses) in Aspirin has  an antiplatelet effect .  #As spi pir iri rin in# #ha has as# #an an# #an ant nti ... let et# #ef eff ffe fec ect ct# #.# BLSTM B O B O I O LSTM FF Trigram Hashing (TRI) Softmax w0 w1 w2 ... wn x0 x1 x2 ... xn y0 y1 y2 ... yn h0 h1 h2 ... hn Figure 1: Architecture of the LSTM network used for named entity recognition. The charac- ter stream \u201cAspirin has an antiplatelet effect.\u201d is tokenized into words wt and converted into word vectors xt using letter-trigram hashing. These vec- tors are propagated through a recurrent neural net- work with four layers: (1) feed-forward encoding of word vectors, (2+3) bidirectional LSTM layers for context representation using forward and back- ward passes, (4) LSTM decoder layer for context- sensitive label prediction.",
      "These vec- tors are propagated through a recurrent neural net- work with four layers: (1) feed-forward encoding of word vectors, (2+3) bidirectional LSTM layers for context representation using forward and back- ward passes, (4) LSTM decoder layer for context- sensitive label prediction. The output labels yt fol- low the BIO2 standard and represent mention be- gin (B), inside (I) and outside (O) per token. the pretrained word2vec model3. As training data generation is expensive, we investigate a generic approach for the generation of word vectors. We use letter-trigram word hashing as introduced by Huang et al. (2013). This technique goes beyond words and generates word vectors as a compos- ite of discriminative three-letter \u201csyllables\u201d, that might also include misspellings. Therefore, it is robust against dictionary misses and has the ad- vantage (despite its name) to group syntactically similar words in similar vector spaces. We com- 3GoogleNews-vectors-negative300 embeddings (3.6 GB)",
      "pare this approach to word embedding models such as word2vec. Surface form features for word vector robust- ness. The most important features for NER are word shape properties, such as length, initial capi- talization, all-word uppercase, in-word capitaliza- tion and use of numbers or punctuation (Ling and Weld, 2012). Mixed-case word encodings implic- itly include capitalization features. However, this approach impedes generalization, as words appear in various surface forms, e.g. capitalized at the beginning of sentences, uppercase in headlines, lowercase in social media text. The strong coher- ence between uppercase and lowercase characters \u2013 they might have identical semantics \u2013 is not en- coded in the embedding. Therefore, we encode the words using lowercase letter-trigrams. To keep the surface information, we add \ufb02ag bits to the vec- tor that indicate initial capitalization, uppercase, lower case or mixed case. 3.2 Deep Contextual Sequence Learning With sparse training data in the idiosyncratic do- main, we expect input data with high variance.",
      "3.2 Deep Contextual Sequence Learning With sparse training data in the idiosyncratic do- main, we expect input data with high variance. Therefore, we require a strong generalization for the syntactic and semantic representation of lan- guage. To reach into the high 80\u201390% NER F1 performance, long-range context-sensitive in- formation is indispensable. We apply the com- putational model of recurrent neural networks, in particular long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997; Gers et al., 2002) to the problem of sequence label- ing. Like neural feed-forward networks, LSTMs are able to learn complex parameters using gradi- ent descent, but include additional recurrent con- nections between cells to in\ufb02uence weight up- dates over adjacent time steps. With their ability to memorize and forget over time, LSTMs have proven to generalize context-sensitive sequential data well (Graves, 2012; Lipton and Berkowitz, 2015). Figure 1 shows an unfolded representation of the steps through a sentence.",
      "With their ability to memorize and forget over time, LSTMs have proven to generalize context-sensitive sequential data well (Graves, 2012; Lipton and Berkowitz, 2015). Figure 1 shows an unfolded representation of the steps through a sentence. We feed the LSTM with letter-trigram vectors xt as input data, one word at a time. The hidden layer of the LSTM represents context from long range dependencies over the entire sentence from left to right. How- ever, to achieve deeper contextual understanding over the boundaries of multi-word annotations and at the beginning of sentences, we require a back- wards pass through the sentence. We therefore im- plement a bidirectional LSTM and feed the output of both directions into a second LSTM layer for combined label prediction. Bidirectional sequence learning. For the use in the neural network, word encodings xt and labels yt are real-valued vectors. To predict the most likely label \u02c6yt of a token, we utilize a LSTM with input nodes gt, input gates it, forget gate ft, out- put gate ot and internal state st.",
      "For the use in the neural network, word encodings xt and labels yt are real-valued vectors. To predict the most likely label \u02c6yt of a token, we utilize a LSTM with input nodes gt, input gates it, forget gate ft, out- put gate ot and internal state st. For the bidirec- tional case, all gates are duplicated and combined into forward state ht and backward state zt. The network is trained using backpropagation through time (BPTT) by adapting weights W and bias pa- rameters b to \ufb01t the training examples.",
      "For the bidirec- tional case, all gates are duplicated and combined into forward state ht and backward state zt. The network is trained using backpropagation through time (BPTT) by adapting weights W and bias pa- rameters b to \ufb01t the training examples. gt = \u03c6(Wgxxt + Wghht\u22121 + bg) it = \u03c3(Wixxt + Wihht\u22121 + bi) ft = \u03c3(Wfxxt + Wfhht\u22121 + bf) ot = \u03c3(Woxxt + Wohht\u22121 + bo) st = \u03c6(gt \u2299it + st\u22121 \u2299ft) ht = \u20d7st \u2299\u20d7ot / zt = \u20d7 st \u2299 \u20d7 ot yt = softmax(Wyhht + Wyzzt + by) (1) We iterate over labeled sentences in mini-batches and update the weights accordingly. The network is then used to predict label probabilities yt for un- seen word sequences wt. 3.3 Implementation of NER Components To show the impact of our bidirectional LSTM model, we measure annotation performance on three different neural network con\ufb01gurations.",
      "The network is then used to predict label probabilities yt for un- seen word sequences wt. 3.3 Implementation of NER Components To show the impact of our bidirectional LSTM model, we measure annotation performance on three different neural network con\ufb01gurations. We implement all components using the Deeplearn- ing4j framework4. For preprocessing (sen- tence and word tokenization), we use Stanford CoreNLP5 (Manning et al., 2014). We test the se- quence labeler using three input encodings: \u2022 DICT: We build a dictionary over all words in the corpus and generate the input vector using 1-hot encoding for each word \u2022 EMB: We use the GoogleNews word2vec embeddings, which encodes each word as vector of size 300 \u2022 TRI: we implement letter-trigram word hash- ing as described in Section 3.1. 4http://deeplearning4j.org, version 0.4-rc3.9-SNAPSHOT 5version 3.6.0",
      "During training and test, we group all tokens of a sentence as mini-batch. We evaluate three differ- ent neural network types to show the impact of the bidirectional sequence learner. \u2022 FF: As baseline, we train a non-sequential feed-forward model based on a fully con- nected multilayer perceptron network with 3 hidden layers of size 150 with relu activa- tion, feeding into a 3-class softmax classi- \ufb01er. We train the model using backpropa- gation with stochastic gradient descent and a learning rate of 0.005. \u2022 LSTM: We use a con\ufb01guration of a sin- gle feed-forward layer of size 150 with two additional layers of single-direction LSTM with 20 cells and a 3-class softmax classi\ufb01er. We train the model using backpropagation- through-time (BPTT) with stochastic gradi- ent descent and a learning rate of 0.005.",
      "We train the model using backpropagation- through-time (BPTT) with stochastic gradi- ent descent and a learning rate of 0.005. \u2022 BLSTM: Our \ufb01nal con\ufb01guration consists of a single feed-forward layer of size 150 with one bidirectional LSTM layer with 20 cells and an additional single-direction LSTM with 20 cells into a 3-class softmax classi\ufb01er. The BLSTM model is trained the same way as the single-direction LSTM. 4 Evaluation We evaluate nine con\ufb01gurations of our model on \ufb01ve gold standard evaluation data sets. We show that the combination of letter-trigram word hash- ing with bidirectional LSTM yields the best results and outperforms sequence learners based on dic- tionaries or word2vec. To highlight the general- ization of our model to idiosyncratic domains, we run tests on common-typed data sets as well as on specialized medical documents. We compare our system on these data sets with specialized state-of- the-art systems.",
      "To highlight the general- ization of our model to idiosyncratic domains, we run tests on common-typed data sets as well as on specialized medical documents. We compare our system on these data sets with specialized state-of- the-art systems. 4.1 Evaluation Set Up We train two models with identical parameteriza- tion, each with 2000 randomly chosen labeled sen- tences from a standard data set. To show the ef- fectiveness of the components, we evaluate differ- ent con\ufb01gurations of this setting with 2000 ran- dom sentences from the remaining set. The model was trained using Deeplearning4j with nd4j-x86 backend. Training the TRI+BLSTM con\ufb01guration on a commodity Intel i7 notebook with 4 cores at 2.8GHz takes approximately 50 minutes.",
      "The model was trained using Deeplearning4j with nd4j-x86 backend. Training the TRI+BLSTM con\ufb01guration on a commodity Intel i7 notebook with 4 cores at 2.8GHz takes approximately 50 minutes. Training Set CoNLL2003 GENIA topic news biomedical #docs overall 60343 1999 #sentences 2000 2000 #tokens 29471 51531 #mentions 3396 8205 vector size DICT 23558 23996 vector size EMB 300 300 vector size TRI 10831 15616 Table 1: Overview of CoNLL2003 and GENIA training datasets and sizes of word encodings. We use 2000 sentences of each set for training. Evaluation data sets. Table 1 gives an overview of the standard data sets we use for training. The GENIA Corpus (Ohta et al., 2002) contains biomedical abstracts from the PubMed database. We use GENIA technical term annotations 3.02, which cover linguistic expressions to entities of in- terest in molecular biology, e.g. proteins, genes and cells.",
      "The GENIA Corpus (Ohta et al., 2002) contains biomedical abstracts from the PubMed database. We use GENIA technical term annotations 3.02, which cover linguistic expressions to entities of in- terest in molecular biology, e.g. proteins, genes and cells. CoNLL2003 (Kim et al., 2004) is a standard NER dataset based on the Reuters RCV- 1 news corpus. It covers named entities of type person, location, organization and misc. For testing the overall annotation performance, we utilize CoNLL2003-testA and a 50 docu- ment split from GENIA. Additionally, we test on the complete KORE50 (Hoffart et al., 2012), ACE2004 (Mitchell et al., 2005) and MSNBC data sets using the GERBIL evaluation framework (Us- beck et al., 2015). 4.2 Measurements We measure precision, recall and F1 score of our DATEXIS-NER system and state-of-the-art anno- tators introduced in Section 2.",
      "4.2 Measurements We measure precision, recall and F1 score of our DATEXIS-NER system and state-of-the-art anno- tators introduced in Section 2. For the comparison with black box systems, we evaluate annotation re- sults using weak annotation match. For a more de- tailed in-system error analysis, we measure BIO2 labeling performance based on each token. Measuring annotation performance using NER-style F1. We measure the overall perfor- mance of mention annotation using the evaluation measures de\ufb01ned by Cornolti et al. (2013), which are also used by Ling et al. (2015). Let D be a set of documents with gold standard mention annotations G = {Gd | d \u2208D} with a total of N = |G| examples. Each mention gi \u2208G is de\ufb01ned by start position b and end position",
      "Common Test Sets CoNLL2003 KORE50 ACE2004 MSNBC corpus RCV-1 RCV-1 newswire MSNBC news topic news (en) news (en) news (en) news (en) annotation guideline named entities named entities all mentions wikif cation Annotator Method Prec Rec F1 Prec Rec F1 Prec Rec F1 Prec Rec F1 Babelfy POS+DICT 53.8 70.4 61.0 72.1 73.6 72.9 12.1 42.2 18.8 43.3 77.8 55.6 DBpedia Spotlight DICT 74.7 66.4 70.3 n/a n/a n/a 13.0 74.8 22.2 56.2 49.0 52.4 Entityclassif er.eu Noun phrase 81.2 83.0 82.1 93.8 94.4 94.1 13.3 90.5 23.2 76.2 93.9 84.1 FOX Ensemble 99.1 75.2 85.5 94.",
      "2 83.0 82.1 93.8 94.4 94.1 13.3 90.5 23.2 76.2 93.9 84.1 FOX Ensemble 99.1 75.2 85.5 94.6 73.6 82.8 12.4 59.5 20.6 38.3 31.3 34.4 LingPipe MUC-7 LM+HMM 91.5 66.6 77.1 93.9 86.1 89.9 16.3 88.9 27.5 73.3 78.7 75.9 NERD-ML  Ensemble 59.9 72.0 65.4 70.4 82.6 76.0 19.6 47.4 27.7 69.7 57.2 62.8 Stanford NER CRF+Dist 99.5 76.1 86.2 94.8 76.4 84.6 18.2 90.9 30.3 95.",
      "4 27.7 69.7 57.2 62.8 Stanford NER CRF+Dist 99.5 76.1 86.2 94.8 76.4 84.6 18.2 90.9 30.3 95.2 84.1 89.3 TagMe 2  DICT 68.3 47.7 56.2 66.5 88.2 75.8 23.2 71.6 35.0 55.6 38.0 45.2 BLSTM 87.8 94.6 91.0 92.6 95.1 93.8 13.3 98.0 23.4 73.3 98.2 83.9 DATEXIS-NER Table 2: Comparison of annotators trained for common English news texts (micro-averaged scores on match per annotation span). The table shows micro-precision, recall and NER-style F1 for CoNLL2003, KORE50, ACE2004 and MSNBC datasets. e in the source document d.",
      "The table shows micro-precision, recall and NER-style F1 for CoNLL2003, KORE50, ACE2004 and MSNBC datasets. e in the source document d. To quantify the performance of the system, we compare G to the set of predicted annotations P = {Pd | d \u2208D} with mentions pi \u2208P: tpd = |{p \u2208Pd | \u2203g \u2208Gd : m(p, g)}| fpd = |{p \u2208Pd | \u2204g \u2208Gd : m(p, g)}| tnd = |{p /\u2208Pd | \u2204g \u2208Gd : m(p, g)}| fnd = |{g \u2208Gd | \u2204p \u2208Pd : m(g, p)}| (2) We compare using a weak annotation match: m : (p, g) 7\u2192(bp \u2264bg \u2264ep)\u2228 (bp \u2264eg \u2264ep)\u2228 (bg \u2264bp \u2264eg)\u2228 (bq \u2264ep \u2264eg) (3) We measure micro-averaged precision (Prec),",
      "g) 7\u2192(bp \u2264bg \u2264ep)\u2228 (bp \u2264eg \u2264ep)\u2228 (bg \u2264bp \u2264eg)\u2228 (bq \u2264ep \u2264eg) (3) We measure micro-averaged precision (Prec), re- call (Rec) and NER-style (F1) score: Prec = P d\u2208D tpd P d\u2208D (tpd + fpd) Rec = P d\u2208D tpd P d\u2208D (tpd + fnd) F1 = 2 \u00b7 Prec \u00b7 Rec Prec + Rec (4) Measuring BIO2 labeling performance. Tun- ing the model con\ufb01guration with annotation match measurement is not always feasible. We there- fore measure tpc, fpc, tnc, fnc separately for each label class c \u2208{B, I, O} in our classi\ufb01cation model and calculate binary classi\ufb01cation precision Precc, recall Recc and F1 c scores.",
      "We there- fore measure tpc, fpc, tnc, fnc separately for each label class c \u2208{B, I, O} in our classi\ufb01cation model and calculate binary classi\ufb01cation precision Precc, recall Recc and F1 c scores. To avoid skewed results from the expectedly large O class, we use macro-averaging over the three classes: PrecBIO = 1 3 X c\u2208{B,I,O} tpc tpc + fpc RecBIO = 1 3 X c\u2208{B,I,O} tpc tpc + fnc (5) 4.3 Evaluation Results We now discuss the evaluation of our DATEXIS- NER system on common and idiosyncratic data. Overall model performance on common types. Table 2 shows the comparison of DATEXIS-NER with eight state-of-the-art annotators on four com- mon news data sets. Both common and medical models are con\ufb01gured identically and trained on only 2000 labeled sentences, without any exter- nal prior knowledge.",
      "Table 2 shows the comparison of DATEXIS-NER with eight state-of-the-art annotators on four com- mon news data sets. Both common and medical models are con\ufb01gured identically and trained on only 2000 labeled sentences, without any exter- nal prior knowledge. We observe that DATEXIS- NER achieves the highest recall scores of all tested annotators, with 95%\u201398% on all measured data sets. Moreover, DATEXIS-NER precision scores are equal or better than median. Overall, we achieve high micro-F1 scores of 84%\u201394% on news entity recognition, which is slightly better than the ontology-based Entityclassi\ufb01er.eu NER and reveals a better generalization than the 3-type Stanford NER with distributional semantics. We",
      "Medical Test Set GENIA corpus GENIA 3.02 topic biomedical (en) annotation guideline medical terms Annotator Method Prec Rec F1 Zhou and Su (2004) HMM+SVM 76.0 69.4 72.6 Finkel et al. (2004) MEMM 71.6 68.6 70.1 Settles et al. (2004) CRF 70.3 69.3 69.8 GENIA tagger CDN 75.8 67.5 71.4 LingPipe GENIA DICT 95.5 97.5 96.5 BLSTM 83.5 85.4 84.4 DATEXIS-NER Table 3: Comparison of annotators trained for biomedical text. The table shows NER annotation results for 50 documents from the GENIA dataset. notice that systems specialized on word-sense dis- ambiguation (Babelfy, DBpedia Spotlight) don\u2019t perform well on \u201craw\u201d untyped entity recognition tasks. The highest precision scores are reached by Stanford NER.",
      "notice that systems specialized on word-sense dis- ambiguation (Babelfy, DBpedia Spotlight) don\u2019t perform well on \u201craw\u201d untyped entity recognition tasks. The highest precision scores are reached by Stanford NER. We also notice a low precision of all annotators on the ACE2004 dataset and high variance in MSNBC performance, which are prob- ably caused by differing annotation standards. Biomedical recognition performance. Table 3 shows the results of biomedical entity recognition compared to the participants of the JNLPBA 2004 bio-entity recognition task (Kim et al., 2004). We notice that for these well-written Medline ab- stracts, there is not such a strong skew between precision and recall. Our DATEXIS-NER system outperforms the HMM, MEMM, CRF and CDN based models with a micro-F1 score of 84%. How- ever, the highly specialized GENIA chunker for LingPipe achieves higher scores. This chunker is a very simple generative model predictor that is based on a sliding window of two tokens, word shape and dictionaries.",
      "How- ever, the highly specialized GENIA chunker for LingPipe achieves higher scores. This chunker is a very simple generative model predictor that is based on a sliding window of two tokens, word shape and dictionaries. We interpret this score as strong over\ufb01tting using a dictionary of the well- de\ufb01ned GENIA terms. Therefore, this model will generalize hardly considering the simple model. We can con\ufb01rm this presumption in the common data sets, where the MUC-6 trained HMM Ling- Pipe chunker performs on average on unseen data. Evaluation of system components. We evalu- ate different con\ufb01gurations of the components that we describe in Section 3.3. Table 4 shows the results of experiments on both CoNLL2003 and GENIA data sets. We report the highest macro- F1 scores for BIO2 labeling for the con\ufb01guration Conf guration CoNLL2003 GENIA Layers Prec Rec F1 Prec Rec F1 DICT+FF 55.8 51.1 53.3 75.6 65.8 70.4 EMB+FF 80.9 76.",
      "8 51.1 53.3 75.6 65.8 70.4 EMB+FF 80.9 76.8 78.8 71.0 64.5 67.6 TRI+FF 74.7 74.3 74.5 75.3 74.2 74.8 DICT+LSTM 59.8 84.8 70.2 82.8 87.4 85.1 EMB+LSTM 94.2 89.8 91.9 81.2 78.0 79.6 TRI+LSTM 91.0 92.3 91.6 87.8 84.7 86.2 DICT+BLSTM 81.9 77.9 79.9 84.0 88.9 86.4 EMB+BLSTM 95.4 88.5 91.8 76.4 84.1 80.1 TRI+BLSTM 93.7 92.4 93.0 88.6 89.3 89.",
      "9 86.4 EMB+BLSTM 95.4 88.5 91.8 76.4 84.1 80.1 TRI+BLSTM 93.7 92.4 93.0 88.6 89.3 89.0 Table 4: Comparison of nine con\ufb01gurations from our implementation (macro-averaged scores on BIO2 classi\ufb01cation per token). of letter-trigram word vectors and bidirectional LSTM. We notice that dictionary-based word en- codings (DICT) work well for idiosyncratic med- ical domains, whereas they suffer from high word ambiguity in the news texts. Pretrained word2vec embeddings (EMB) perform well on news data, but cannot adapt to the medical domain without retraining, because of a large number of unseen words. Therefore, word2vec generally achieves a high precision on news texts, but low recall on medical text. The letter-trigram approach (TRI) combines both word vector generalization and ro- bustness towards idiosyncratic language.",
      "Therefore, word2vec generally achieves a high precision on news texts, but low recall on medical text. The letter-trigram approach (TRI) combines both word vector generalization and ro- bustness towards idiosyncratic language. We observe that the contextual LSTM model achieves scores throughout in the 85%\u201394% range and signi\ufb01cantly outperforms the feed-forward (FF) baseline that shows a maximum of 75%. Bidirectional LSTMs can further improve label classi\ufb01cation in both precision and recall. 4.4 Discussion and Error Analysis We investigate different aspects of the DATEXIS- NER components by manual inspection of clas- si\ufb01cation errors in the context of the document. For the error classes described in the introduction (false negative detections, false positives and in- valid boundaries), we observe following causes: Unseen words and misspellings. In dictionary based con\ufb01gurations (e.g. 1-hot word vector en- coding DICT), we observe false negative predic- tions caused by dictionary misses for words that do not exist in the training data. The cause can be rare unseen or novel words (e.g.",
      "In dictionary based con\ufb01gurations (e.g. 1-hot word vector en- coding DICT), we observe false negative predic- tions caused by dictionary misses for words that do not exist in the training data. The cause can be rare unseen or novel words (e.g. T-prolymphocytic cells) or misspellings (e.g. strengthnend). These words",
      "yield a null vector result from the encoder and can therefore not be distinguished by the LSTM. The error increases when using word2vec, be- cause these models are trained with stop words \ufb01l- tered out. This implicates that e.g. mentions sur- rounded by or containing a determiner (e.g. The Sunday Telegraph quoted Majorie Orr) are highly error prone towards the detection of their bound- aries. We resolve this error by the letter-trigram approach. Unseen trigrams (e.g. thh) may still be missing in the word vector, but only affect single dimensions as opposed to the vector as a whole. Misleading surface form features. Surface forms encode important features for NER (e.g. capitalization of \u201cnew\u201d in Alan Shearer was named as the new England captain / as New York beat the Angels). However, case-sensitive word vectoriza- tion methods yield a large amount of false positive predictions caused by incorrect capitalization in the input data. An uppercase headline (e.g. TEN- NIS - U.S.",
      "However, case-sensitive word vectoriza- tion methods yield a large amount of false positive predictions caused by incorrect capitalization in the input data. An uppercase headline (e.g. TEN- NIS - U.S. TEAM ON THE ROAD FOR 1997 FED CUP) is encoded completely different than a low- ercase one (e.g. U.S. team on the road for Fed Cup). Because of that, we achieve best results with low- ercase word vectors and additional surface form feature \ufb02ags, as described in Section 3.1. Syntagmatic word relations. We observe men- tions that are composed of co-occurring words with high ambiguity (e.g. degradation of IkB alpha in T cell lines). These groups encode strong syn- tagmatic word relations (Sahlgren, 2008) that can be leveraged to resolve word sense and homonyms from sentence context. Therefore, correct bound- aries in these groups can effectively be identi\ufb01ed only with contextual models such as LSTMs. Paradigmatic word relations.",
      "Therefore, correct bound- aries in these groups can effectively be identi\ufb01ed only with contextual models such as LSTMs. Paradigmatic word relations. Orthogonal to the previous problem, different words in a paradig- matic relation (Sahlgren, 2008) can occur in the same context (e.g. cyclosporin A-treated cells / HU treated cells). These groups are ef\ufb01ciently repre- sented in word2vec. However, letter-trigram vec- tors cannot encode paradigmatic groups and there- fore require a larger training sample to capture these relations. Context boundaries. Often, synonyms can only be resolved regarding a larger document context than the local sentence context known by the LSTM. In these cases, word sense is rede\ufb01ned by a topic model local to the paragraph (e.g. sports: Tiger was lost in the woods after divorce.). This problem does not heavily affect NER recall, but is crucial for named entity disambiguation and coref- erence resolution. Limitations. The proposed DATEXIS-NER model is restricted to recognize boundaries of generic mentions in text.",
      "This problem does not heavily affect NER recall, but is crucial for named entity disambiguation and coref- erence resolution. Limitations. The proposed DATEXIS-NER model is restricted to recognize boundaries of generic mentions in text. We evaluate the model on annotations of isolated types (e.g. persons, organizations, locations) for comparison purposes only, but we do not approach NER-style typing. Contrary, we approach to detect mentions without type information. The detection of speci\ufb01c types can be realized by training multiple independent models on a selection of labels per type and nest- ing the resulting annotations using a longest-span semantic type heuristic (Kholghi et al., 2015). 5 Summary Ling et al. (2015) show that the task of NER is not clearly de\ufb01ned and rather depends on a speci\ufb01c problem context. Contrary, most NER approaches are speci\ufb01cally trained on \ufb01xed datasets in a batch mode. Worse, they often suffer from poor recall (Pink et al., 2014).",
      "Contrary, most NER approaches are speci\ufb01cally trained on \ufb01xed datasets in a batch mode. Worse, they often suffer from poor recall (Pink et al., 2014). Ideally, one could personalize the task of recognizing named entities, concepts or phrases according to the speci\ufb01c problem. \u201cPer- sonalizing\u201d and adapting such annotators should happen with very limited human labeling effort, in particular for idiosyncratic domains with sparse training data. Our work follows this line. From our results we report F1 scores between 84\u201394% when using bidirectional multi-layered LSTMs, letter-trigram word hashing and surface form features on only few hundred training examples. This work is only a preliminary step towards the vision of personalizing annotation guidelines for NER (Ling et al., 2015). In our future work, we will focus on additional important idiosyncratic domains, such as health, life science, fashion, en- gineering or automotive.",
      "In our future work, we will focus on additional important idiosyncratic domains, such as health, life science, fashion, en- gineering or automotive. For these domains, we will consider the process of detecting mentions and linking them to an ontology as a joint task and we will investigate simple and interactive work- \ufb02ows for creating robust personalized named en- tity linking systems. Acknowledgements Our work is funded by the Federal Ministry of Economic Affairs and Energy (BMWi) under grant agreement 01MD15010B (Project: Smart Data Web).",
      "References [Bengio et al.2003] Yoshua Bengio, R\u00b4ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A Neu- ral Probabilistic Language Model. Journal of Ma- chine Learning Research, 3:1137\u20131155. [Collins2002] Michael Collins. 2002. Discriminative Training Methods for Hidden Markov Models: The- ory and Experiments with Perceptron Algorithms. In EMNLP\u201902, pages 1\u20138, Stroudsburg, PA, USA. ACL. [Cornolti et al.2013] Marco Cornolti, Paolo Ferragina, and Massimiliano Ciaramita. 2013. A Framework for Benchmarking Entity-Annotation Systems. In WWW\u201913, pages 249\u2013260. ACM. [Dojchinovski and Kliegr2013] Milan Dojchinovski and Tom\u00b4a\u02c7s Kliegr. 2013. Entityclassi\ufb01er. eu: Real-Time Classi\ufb01cation of Entities in Text with Wikipedia.",
      "ACM. [Dojchinovski and Kliegr2013] Milan Dojchinovski and Tom\u00b4a\u02c7s Kliegr. 2013. Entityclassi\ufb01er. eu: Real-Time Classi\ufb01cation of Entities in Text with Wikipedia. In Machine Learning and Knowledge Discovery in Databases, pages 654\u2013658. Springer. [Ferragina and Scaiella2010] Paolo Ferragina and Ugo Scaiella. 2010. TAGME: On-the-\ufb02y Annotation of Short Text Fragments (by Wikipedia Entities). In CIKM\u201910, pages 1625\u20131628, New York, NY, USA. ACM. [Finkel et al.2004] Jenny Finkel, Shipra Dingare, Huy Nguyen, Malvina Nissim, Christopher Manning, and Gail Sinclair. 2004. Exploiting Context for Biomedical Entity Recognition: from Syntax to the Web. In JNLPBA\u201904, pages 88\u201391. ACL.",
      "2004. Exploiting Context for Biomedical Entity Recognition: from Syntax to the Web. In JNLPBA\u201904, pages 88\u201391. ACL. [Gers et al.2002] Felix Gers, Juan Antonio Perez-Ortiz, Douglas Eck, and J\u00a8urgen Schmidhuber. 2002. Learning Context Sensitive Languages with LSTM Trained with Kalman Filters. [Graves2012] Alex Graves. 2012. Supervised Se- quence Labelling with Recurrent Neural Networks, volume 385. Springer, Berlin Heidelberg. [Hachey et al.2013] Ben Hachey, Will Radford, Joel Nothman, Matthew Honnibal, and James R. Curran. 2013. Evaluating Entity Linking with Wikipedia. Arti\ufb01cial intelligence, 194:130\u2013150. [Hochreiter and Schmidhuber1997] Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation, 9(8):1735\u20131780.",
      "[Hochreiter and Schmidhuber1997] Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation, 9(8):1735\u20131780. [Hoffart et al.2012] Johannes Hoffart, Stephan Seufert, Dat Ba Nguyen, Martin Theobald, and Gerhard Weikum. 2012. KORE: Keyphrase Overlap Re- latedness for Entity Disambiguation. In CIKM\u201912, pages 545\u2013554. ACM. [Huang et al.2013] Po-Sen Huang, Xiaodong He, Jian- feng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning Deep Structured Semantic Mod- els for Web Search using Clickthrough Data. In CIKM\u201913, pages 2333\u20132338. ACM. [Kholghi et al.2015] Mahnoosh Kholghi, Laurianne Sitbon, Guido Zuccon, and Anthony Nguyen. 2015.",
      "In CIKM\u201913, pages 2333\u20132338. ACM. [Kholghi et al.2015] Mahnoosh Kholghi, Laurianne Sitbon, Guido Zuccon, and Anthony Nguyen. 2015. External Knowledge and Query Strategies in Active Learning: a Study in Clinical Information Extrac- tion. In CIKM\u201915, pages 143\u2013152. ACM. [Kim et al.2004] Jin-Dong Kim, Tomoko Ohta, Yoshi- masa Tsuruoka, Yuka Tateisi, and Nigel Collier. 2004. Introduction to the Bio-Entity Recognition Task at JNLPBA. In JNLPBA\u201904, pages 70\u201375. ACL. [Ling and Weld2012] Xiao Ling and Daniel S. Weld. 2012. Fine-Grained Entity Recognition. In AAAI\u201912. [Ling et al.2015] Xiao Ling, Sameer Singh, and Daniel S Weld. 2015. Design Challenges for Entity Linking. ACL\u201915, 3:315\u2013328.",
      "2012. Fine-Grained Entity Recognition. In AAAI\u201912. [Ling et al.2015] Xiao Ling, Sameer Singh, and Daniel S Weld. 2015. Design Challenges for Entity Linking. ACL\u201915, 3:315\u2013328. [Lipton and Berkowitz2015] Zachary C. Lipton and John Berkowitz. 2015. A Critical Review of Re- current Neural Networks for Sequence Learning. arXiv:1506.00019 [cs.LG]. [Manning et al.2014] Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In ACL System Demonstrations, pages 55\u201360. [McCallum and Li2003] Andrew McCallum and Wei Li. 2003. Early Results for Named Entity Recog- nition with Conditional Random Fields, Feature In- duction and Web-enhanced Lexicons.",
      "In ACL System Demonstrations, pages 55\u201360. [McCallum and Li2003] Andrew McCallum and Wei Li. 2003. Early Results for Named Entity Recog- nition with Conditional Random Fields, Feature In- duction and Web-enhanced Lexicons. In CONLL \u201903, pages 188\u2013191, Stroudsburg, PA, USA. ACL. [Mendes et al.2011] Pablo N Mendes, Max Jakob, Andr\u00b4es Garcia-Silva, and Christian Bizer. 2011. DBpedia Spotlight: Shedding Light on the Web of Documents. In I-Semantics 2011, pages 1\u20138. ACM. [Mikolov et al.2013] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Ef\ufb01cient Es- timation of Word Representations in Vector Space. arXiv:1301.3781 [cs.CL]. [Mitchell et al.2005] Alexis Mitchell, Stephanie Strassel, Shudong Huang, and Ramez Zakhary. 2005.",
      "arXiv:1301.3781 [cs.CL]. [Mitchell et al.2005] Alexis Mitchell, Stephanie Strassel, Shudong Huang, and Ramez Zakhary. 2005. ACE 2004 Multilingual Training Corpus. LDC, Philadelphia, 1:1\u20131. [Moro et al.2014] Andrea Moro, Alessandro Raganato, and Roberto Navigli. 2014. Entity Linking meets Word Sense Disambiguation: a Uni\ufb01ed Approach. ACL\u201914, 2:231\u2013244. [Ohta et al.2002] Tomoko Ohta, Yuka Tateisi, and Jin- Dong Kim. 2002. The GENIA Corpus: An Anno- tated Research Abstract Corpus in Molecular Biol- ogy Domain. In International Conference on Hu- man Language Technology Research 2002, pages 82\u201386. Morgan Kaufmann Publishers Inc.",
      "[Pink et al.2014] Glen Pink, Joel Nothman, and James R. Curran. 2014. Analysing Recall Loss in Named Entity Slot Filling. In EMNLP\u201914, pages 820\u2013830, Doha, Qatar. ACL. [Prokofyev et al.2014] Roman Prokofyev, Gianluca De- martini, and Philippe Cudr\u00b4e-Mauroux. 2014. Ef- fective Named Entity Recognition for Idiosyncratic Web Collections. In WWW\u201914, pages 397\u2013408, Geneva, Switzerland. IW3C2. [Ramshaw and Marcus1995] Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text chunking using transformation-based learning. In WVLC\u201995. ACL. [Sahlgren2008] Magnus Sahlgren. 2008. The Distri- butional Hypothesis. Italian Journal of Linguistics, 20(1):33\u201354. [Settles2004] Burr Settles. 2004.",
      "ACL. [Sahlgren2008] Magnus Sahlgren. 2008. The Distri- butional Hypothesis. Italian Journal of Linguistics, 20(1):33\u201354. [Settles2004] Burr Settles. 2004. Biomedical Named Entity Recognition Using Conditional Random Fields and Rich Feature Sets. In JNLPBA\u201904, pages 104\u2013107. ACL. [Shen et al.2015] Wei Shen, Jianyong Wang, and Ji- awei Han. 2015. Entity Linking with a Knowl- edge Base: Issues, Techniques, and Solutions. IEEE Transactions on Knowledge and Data Engineering, 27(2):443\u2013460, February. [Speck and Ngomo2014] Ren\u00b4e Speck and Axel- Cyrille Ngonga Ngomo. 2014. Ensemble Learning for Named Entity Recognition. In ISWC\u201914, pages 519\u2013534. Springer. [Tjong Kim Sang and De Meulder2003] Erik F. Tjong Kim Sang and Fien De Meulder. 2003.",
      "2014. Ensemble Learning for Named Entity Recognition. In ISWC\u201914, pages 519\u2013534. Springer. [Tjong Kim Sang and De Meulder2003] Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In CoNLL\u201903, pages 142\u2013147. ACL. [Usbeck et al.2015] Ricardo Usbeck, Michael R\u00a8oder, Axel-Cyrille Ngonga Ngomo, Ciro Baron, An- dreas Both, Martin Br\u00a8ummer, Diego Ceccarelli, Marco Cornolti, Didier Cherix, Bernd Eickmann, Paolo Ferragina, Christiane Lemke, Andrea Moro, Roberto Navigli, Francesco Piccinno, Giuseppe Rizzo, Harald Sack, Ren\u00b4e Speck, Rapha\u00a8el Troncy, J\u00a8org Waitelonis, and Lars Wesemann. 2015. GERBIL: General Entity Annotator Benchmark- ing Framework.",
      "2015. GERBIL: General Entity Annotator Benchmark- ing Framework. In WWW\u201915, pages 1133\u20131143, Geneva, Switzerland. IW3C2. [Van Erp et al.2013] Marieke Van Erp, Giuseppe Rizzo, and Rapha\u00a8el Troncy. 2013. Learning with the Web: Spotting Named Entities on the Intersection of NERD and Machine Learning. In #MSM\u201913, pages 27\u201330, Rio de Janeiro, Brazil. ACM. [Zhou and Su2004] GuoDong Zhou and Jian Su. 2004. Exploring Deep Knowledge Resources in Biomed- ical Name Recognition. In JNLPBA\u201904, pages 96\u2013 99. ACL."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1608.06757.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":10614,
  "avg_doclen":176.9,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1608.06757.pdf"
    }
  }
}