[
  "SEMI-SUPERVISED METHODS FOR OUT-OF-DOMAIN DEPENDENCY PARSING by JUNTAO YU A thesis submitted to The University of Birmingham for the degree of DOCTOR OF PHILOSOPHY School of Computer Science College of Engineering and Physical Sciences The University of Birmingham November 2017 arXiv:1810.02100v1  [cs.CL]  4 Oct 2018",
  "",
  "Abstract Dependency parsing is one of the important natural language processing tasks that assigns syntactic trees to texts. Due to the wider availability of dependency corpora and improved parsing and machine learning techniques, parsing accuracies of supervised learning-based systems have been signi\ufb01cantly improved. However, due to the nature of supervised learning, those parsing systems highly rely on the manually annotated training corpora. They work reasonably good on the in-domain data but the performance drops signi\ufb01cantly when tested on out-of-domain texts. To bridge the performance gap between in-domain and out-of-domain, this thesis investigates three semi-supervised techniques for out-of- domain dependency parsing, namely co-training, self-training and dependency language models. Our approaches use easily obtainable unlabelled data to improve out-of-domain parsing accuracies without the need of expensive corpora annotation. The evaluations on several English domains and multi-lingual data show quite good improvements on parsing accuracy. Overall this work conducted a survey of semi-supervised methods for out-of- domain dependency parsing, where I extended and compared a number of important semi- supervised methods in a uni\ufb01ed framework.",
  "The evaluations on several English domains and multi-lingual data show quite good improvements on parsing accuracy. Overall this work conducted a survey of semi-supervised methods for out-of- domain dependency parsing, where I extended and compared a number of important semi- supervised methods in a uni\ufb01ed framework. The comparison between those techniques shows that self-training works equally well as co-training on out-of-domain parsing, while dependency language models can improve both in- and out-of-domain accuracies.",
  "To my wonderful wife Mingyu Du.",
  "ACKNOWLEDGEMENTS Now nearly four years, since I \ufb01rst come to Birmingham, I and my wife had a great time here. I would take this opportunity to thank all the friends who supported and took care of us during our time in Birmingham. First of all, I would like to thank my primary supervisor Bernd Bohnet, who is not only a great supervisor but also a good friend. Four years ago, when Bernd \ufb01rst got me into his group, I have very limited knowledge about the natural language processing and research in general. During those years, through our meetings (majorly in the Costa and recently on the train :) ), he equipped me with all I need for my PhD study. From how to use Mate, to writing my \ufb01rst paper, preparing my \ufb01rst conference talk, applying for travel funding, applying for jobs and writing this thesis, whenever I needed help, Bernd is always there to support me. Because of Bernd, I had a great time in Birmingham! I would like also to thank my co-supervisor Mark and John for their supervision and took care of me within the department. For their feedbacks on my research, papers and this thesis.",
  "Because of Bernd, I had a great time in Birmingham! I would like also to thank my co-supervisor Mark and John for their supervision and took care of me within the department. For their feedbacks on my research, papers and this thesis. It would be less joy if we don\u2019t have all the friends here in the UK, I would like to thank all the lovely friends for the wonderful time we spend together! Finally, I would like to thank my family for their support and encouragement. Without their help, I would not even start my degree. I would especially thank my wonderful wife for make every important decision with me and took care of me all the time. For cooking me the delicious food, growing me the beautiful garden, they are huge contributors to the happiness of life and of course the vanilla lattes. For the time you spent to listen the talks start with \u201dmy name is\u201d which made you an expert of \u201dself-training\u201d! For introducing",
  "me the work-life balance, for introducing the bolt journal, for an endless list of things you did for me, it is hard to imagine a life without you.",
  "CONTENTS 1 Introduction 1 1.1 Research Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2 Thesis Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.3 Published Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.4 Chapter Summary . . . . . . . . . . . . . .",
  ". . . . . . . . . . . . . . 5 1.4 Chapter Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2 Background and Experiment Set-up 7 2.1 Dependency parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.1.1 Graph-based Systems . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.1.2 Transition-based Systems . . . . .",
  ". . . . . . . . . . . . . . . . . . . . . . 8 2.1.2 Transition-based Systems . . . . . . . . . . . . . . . . . . . . . . . . 11 2.1.3 Neural Network-based Systems . . . . . . . . . . . . . . . . . . . . 13 2.1.4 The Mate Parser . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.2 Out-of-domain Parsing . . . . . . . . . . . . . .",
  ". . . . . . . . . . . . . 15 2.2 Out-of-domain Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 2.2.1 Approaches to Out-of-Domain Parsing . . . . . . . . . . . . . . . . 22 2.2.2 Semi-Supervised Approaches . . . . . . . . . . . . . . . . . . . . . . 24 2.3 Corpora . . . . . . . . . . . . . . . . . . . . . . . . . .",
  ". . 24 2.3 Corpora . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 2.3.1 The Main Evaluation Corpora . . . . . . . . . . . . . . . . . . . . . 33 2.4 Evaluation Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 2.5 Analysis Techniques . . . . . . . . . . . . . . . . . . . . .",
  ". . . . . . . 34 2.5 Analysis Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 2.6 Chapter Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 3 Co-training 39",
  "3.1 Agreement Based Co-training . . . . . . . . . . . . . . . . . . . . . . . . . 40 3.2 Experiment Set-up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 3.3 Empirical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 3.4 Analysis . . . . . . . . . . . . . . . . . . . . . . . . .",
  ". . . . 43 3.4 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 3.4.1 Token Level Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . 47 3.4.2 Sentence Level Analysis . . . . . . . . . . . . . . . . . . . . . . . . 50 3.5 Chapter Summary . . . . . . . . . . . . . . . . . . . .",
  ". . . . . . . . 50 3.5 Chapter Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 4 Self-training 57 4.1 Con\ufb01dence-based Self-training . . . . . . . . . . . . . . . . . . . . . . . . . 58 4.2 Experiment Set-up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 4.3 Empirical Results . . . . . . . . . . . .",
  ". . . . . . . . . . . . . . . . 63 4.3 Empirical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 4.4 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 4.4.1 Token Level Analysis . . . . . . . . . . . . . . . . . . . . . . . . . .",
  ". 68 4.4.1 Token Level Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . 69 4.4.2 Sentence Level Analysis . . . . . . . . . . . . . . . . . . . . . . . . 72 4.5 Chapter Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 5 Multi-lingual Self-training 80 5.1 Multi-lingual Con\ufb01dence-based Self-training . . . . . . . . . . . . . . . . .",
  ". 75 5 Multi-lingual Self-training 80 5.1 Multi-lingual Con\ufb01dence-based Self-training . . . . . . . . . . . . . . . . . 81 5.2 Experiment Set-up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 5.3 Empirical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 5.4 Analysis . . . . . . . . . . . . . . . . . . . . . .",
  ". . . . . . . 85 5.4 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87 5.4.1 Positive E\ufb00ects Analysis . . . . . . . . . . . . . . . . . . . . . . . . 88 5.4.2 Negative E\ufb00ects Analysis . . . . . . . . . . . . . . . . . . . . . . . . 92 5.5 Chapter Summary . . . . . . . . . . . . . .",
  ". . . . . . . . . . . . . . 92 5.5 Chapter Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 6 Dependency Language Models 95 6.1 Dependency Language Models for Transition-based System . . . . . . . . . 96 6.2 Experiment Set-up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98 6.3 Empirical Results . . . . . . . . . . . . . . . . . . . . . .",
  ". . . . . . 98 6.3 Empirical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100",
  "6.4 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 6.4.1 English Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106 6.4.2 Analysis for Chinese . . . . . . . . . . . . . . . . . . . . . . . . . . 114 6.5 Chapter Summary . . . . . . . . . . . . . . . . . . . . . .",
  ". . . . . . 114 6.5 Chapter Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117 7 Conclusions 120 7.1 Conclusions on Co-training . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 7.1.1 Could the o\ufb00-the-shelf dependency parsers be successfully used in co-training for domain adaptation? . . . . . . . . . . . . . . . . . . 121 7.1.2 Would tri-training be more e\ufb00ective for out-of-domain parsing when o\ufb00-the-shelf dependency parsers are used? . .",
  ". . . . . . . . . . . . 121 7.1.2 Would tri-training be more e\ufb00ective for out-of-domain parsing when o\ufb00-the-shelf dependency parsers are used? . . . . . . . . . . . . . . 122 7.2 Conclusions on Self-training . . . . . . . . . . . . . . . . . . . . . . . . . . 122 7.2.1 How could self-training be e\ufb00ectively used in out-of-domain depen- dency parsing? . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
  ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122 7.2.2 If self-training works for English dependency parsing, can it be adapted to other languages? . . . . . . . . . . . . . . . . . . . . . . 123 7.3 Conclusions on Dependency Language Models . . . . . . . . . . . . . . . . 123 7.3.1 Can dependency language models be adapted to strong transition- based parsers? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
  ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 7.3.2 Can dependency language models be used for out-of-domain parsing?124 7.3.3 Quality or quantity of the auto-parsed data, which one is more important to the successful use of dependency language models? . . 124 7.4 Chapter Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125",
  "LIST OF FIGURES 2.1 The dependency relations of the sentence (Tom played football with his classmate .) parsed by Mate parser. . . . . . . . . . . . . . . . . . . . . . . 8 2.2 Parsing the sentence (Tom plays football) with a graph-based dependency parser. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.3 Parsing the sentence (Tom plays football) with an arc-eager transition- based dependency parser. . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
  ". . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.4 Neural Network architecture of Chen and Manning (2014) system . . . . . 14 2.5 Parsing the sentence (A hearing is scheduled on the issue) with the Mate transition-based dependency parser. . . . . . . . . . . . . . . . . . . . . . . 18 2.6 The bar chart used to visualise our analysis on individual labels. . . . . . . 36 2.7 An example of our sentence level analysis on di\ufb00erent number of unknown words per sentence. . . . . . . . . . . . . . . . . . . . . . .",
  ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 3.1 The results of our normal agreement-based co-training with three di\ufb00erent parser pairs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 3.2 The e\ufb00ect of omitting short sentences from additional training data. . . . 45 3.3 The results of our tri-training compared with normal co-training. . . . . . 46 3.4 The performance comparison between the tri-training approach and the baseline on major labels. . . . . . . . . . . . .",
  ". . . . . 46 3.4 The performance comparison between the tri-training approach and the baseline on major labels. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 3.5 The comparison between the tri-training approach and the baseline on di\ufb00erent number of tokens per sentence. . . . . . . . . . . . . . . . . . . . 51",
  "3.6 The comparison between the tri-training approach and the baseline on di\ufb00erent number of unknown words per sentence. . . . . . . . . . . . . . . 52 3.7 The comparison between the tri-training approach and the baseline on di\ufb00erent number of prepositions per sentence. . . . . . . . . . . . . . . . . 53 3.8 The comparison between the tri-training approach and the baseline on di\ufb00erent number of conjunctions per sentence. . . . . . . . . . . . . . . . 55 4.1 The accuracies when inspecting 10-100% sentences of the Weblogs devel- opment set ranked by the con\ufb01dence-based methods. . . . . . . . . . . . .",
  ". 55 4.1 The accuracies when inspecting 10-100% sentences of the Weblogs devel- opment set ranked by the con\ufb01dence-based methods. . . . . . . . . . . . . 60 4.2 The accuracies, sentence lengths and the parse scores of individual sen- tences in Weblogs development set. . . . . . . . . . . . . . . . . . . . . . 61 4.3 The root mean square-error (fr) of Weblogs development set after ranked by adjusted parse scores with di\ufb00erent values of d. . . . . . . . . . . . . . . 63 4.4 The e\ufb00ect of our self-training approaches on the Weblogs development set. . . . . . . . . . . . . . . . . .",
  ". . . 63 4.4 The e\ufb00ect of our self-training approaches on the Weblogs development set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 4.5 The identical rate between the adjusted parse score-based and the Delta- based methods, when top ranked n percent is concerned. . . . . . . . . . . 69 4.6 The performance comparison between the self-training approach and the baseline on major labels. . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
  ". . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 4.7 The comparison between the self-training approach and the baseline on di\ufb00erent number of tokens per sentence. . . . . . . . . . . . . . . . . . . . 72 4.8 The comparison between the self-training approach and the baseline on di\ufb00erent number of unknown words per sentence. . . . . . . . . . . . . . . 73 4.9 The comparison between the self-training approach and the baseline on di\ufb00erent number of prepositions per sentence. . . . . . . . . . . . . . . . .",
  ". 73 4.9 The comparison between the self-training approach and the baseline on di\ufb00erent number of prepositions per sentence. . . . . . . . . . . . . . . . . 74 4.10 The comparison between the self-training approach and the baseline on di\ufb00erent number of conjunctions per sentence. . . . . . . . . . . . . . . . 75",
  "5.1 Accuracies of sentences which have a position number within the top 50% after ranking the auto-parsed sentences of German development set by the adjusted parse scores with di\ufb00erent values of d. . . . . . . . . . . . . . 82 5.2 The accuracies when inspecting 10-100% sentences of the German devel- opment set ranked by the con\ufb01dence-based methods. . . . . . . . . . . . . 83 5.3 The performance comparison between the multi-lingual self-training ap- proach and the baseline on major labels. . . . . . . . . . . . . . . . . . . . 90 5.4 The comparison between the multi-lingual self-training approach and the baseline on di\ufb00erent number of tokens per sentence. . . . . . . . . . . . .",
  ". . . 90 5.4 The comparison between the multi-lingual self-training approach and the baseline on di\ufb00erent number of tokens per sentence. . . . . . . . . . . . . 91 5.5 The comparison between the multi-lingual self-training approach and the baseline on di\ufb00erent number of unknown words per sentence. . . . . . . . 92 5.6 The accuracies when inspecting 10-100% sentences of the French test set ranked by the con\ufb01dence-based methods. . . . . . . . . . . . . . . . . . . . 93 6.1 E\ufb00ects (LAS) of di\ufb00erent number of DLMs on English and Chinese devel- opment sets. . . . . . . . . . . . . . . . . . .",
  ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 6.2 E\ufb00ects (LAS) of DLMs extracted from di\ufb00erent size (in million sentences) of corpus on English and Chinese development sets. . . . . . . . . . . . . . 102 6.3 The English performance comparison between the DLM approach and the baseline on major labels. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106 6.4 The English comparison between the DLM approach and the baseline on di\ufb00erent number of tokens per sentence. . . . . .",
  ". . . . . . . . . . . . 106 6.4 The English comparison between the DLM approach and the baseline on di\ufb00erent number of tokens per sentence. . . . . . . . . . . . . . . . . . . . 109 6.5 The English comparison between the DLM approach and the baseline on di\ufb00erent number of unknown words per sentence. . . . . . . . . . . . . . . 110 6.6 The English comparison between the DLM approach and the baseline on di\ufb00erent number of prepositions per sentence. . . . . . . . . . . . . . . . . 111 6.7 The English comparison between the DLM approach and the baseline on di\ufb00erent number of conjunctions per sentence. . . . . .",
  ". . . . . . . . . . . 111 6.7 The English comparison between the DLM approach and the baseline on di\ufb00erent number of conjunctions per sentence. . . . . . . . . . . . . . . . 111",
  "6.8 The Chinese performance comparison between the DLM approach and the baseline on major labels. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114 6.9 The Chinese comparison between the DLM approach and the baseline on di\ufb00erent number of tokens per sentence. . . . . . . . . . . . . . . . . . . . 116 6.10 The Chinese comparison between the DLM approach and the baseline on di\ufb00erent number of prepositions per sentence. . . . . . . . . . . . . . . . . 117 6.11 The Chinese comparison between the DLM approach and the baseline on di\ufb00erent number of conjunctions per sentence. . . . . . . . . . . . . . . . . 118",
  "LIST OF TABELS 2.1 Transitions for arc-eager parsing. . . . . . . . . . . . . . . . . . . . . . . . 13 2.2 Transitions for joint tagging and parsing. . . . . . . . . . . . . . . . . . . . 16 2.3 Labelled attachment scores achieved by the MST, Malt, and Mate parsers trained on the Conll training set and tested on di\ufb00erent domains. . . . . 22 2.4 The size of the source domain (Conll) training and test sets for our main evaluation corpora. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
  ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 2.5 The size of the target domain test datasets for our main evaluation corpora. 32 2.6 The size of unlabelled datasets for our main evaluation corpora. . . . . . . 33 3.1 The analysis of identical annotations on Weblogs development set. . . . . 41 3.2 The quantity and quality (LAS) of identical (Mate-Malt) development set sentences when omitting the short sentences. . . . . . . . . . . . . . . . . . 44 3.3 The quantity and quality (LAS) of identical development set sentences agreed by di\ufb00erent parser pairs. . . . . . . . . . . .",
  ". . . . . . 44 3.3 The quantity and quality (LAS) of identical development set sentences agreed by di\ufb00erent parser pairs. . . . . . . . . . . . . . . . . . . . . . . . . 46 3.4 The e\ufb00ect of applying the best con\ufb01guration (tri-training) to our test do- mains. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 3.5 The confusion matrix of dependency labels, compared between the tri- training approach and the baseline. . . . . . . . . . . . . . . . . .",
  ". . . 47 3.5 The confusion matrix of dependency labels, compared between the tri- training approach and the baseline. . . . . . . . . . . . . . . . . . . . . . . 48 3.6 The accuracy comparison between the tri-training approach and the base- line on unknown words. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 3.7 The example sentences that have been improved by the tri-training ap- proach when compared to the baseline. . . . . . . . . . . . . . . . . . . . . 54",
  "4.1 The size of datasets for Chemical domain evaluation. . . . . . . . . . . . 64 4.2 The e\ufb00ect of the adjusted parse score-based and the Delta-based self- training approaches on our main test sets. . . . . . . . . . . . . . . . . . . 67 4.3 The results of the adjusted parse score-based and the Delta-based self- training approaches on the Chemical test set compared with previous work. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 4.4 The confusion matrix of dependency labels, compared between the self- training approaches and the baseline. . . . . . . . . .",
  ". . . . . . . . . . . 67 4.4 The confusion matrix of dependency labels, compared between the self- training approaches and the baseline. . . . . . . . . . . . . . . . . . . . . . 70 4.5 The accuracy comparison between the self-training approach and the base- line on unknown words. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 4.6 The example sentences that have been improved by the parse score-based self-training approach when compared to the baseline. . . . . . . . . . . . . 76 4.7 The example sentences that have been improved by the Delta-based self- training approach when compared to the baseline. . . . .",
  ". . . . . . . . . . . . 76 4.7 The example sentences that have been improved by the Delta-based self- training approach when compared to the baseline. . . . . . . . . . . . . . . 77 5.1 Statistics about the Spmrl multi-lingual corpora . . . . . . . . . . . . . . 84 5.2 Comparing our self-trained results with the best non-ensemble system in the SPMRL Shared Task (LORIA). . . . . . . . . . . . . . . . . . . . . . . 86 5.3 The confusion matrix of dependency labels, compared between the multi- lingual self-training approach and the baseline. . . . . . . . . . . . . . . .",
  ". . . 86 5.3 The confusion matrix of dependency labels, compared between the multi- lingual self-training approach and the baseline. . . . . . . . . . . . . . . . . 89 5.4 The accuracy comparison between the multi-lingual self-training approach and the baseline on unknown words. . . . . . . . . . . . . . . . . . . . . . 90 5.5 The basic statistic of datasets for French evaluation. . . . . . . . . . . . . 93 6.1 DLM-based feature templates which we used in the parser. . . . . . . . . . 98 6.2 The size of datasets for the Wsj Stanford conversion evaluation. . . . . . .",
  ". . . . . . . . . 98 6.2 The size of datasets for the Wsj Stanford conversion evaluation. . . . . . . 98 6.3 The size of datasets for the Chinese Treebank 5 (Ctb) evaluation. . . . . . 99 6.4 Comparing our DLM enhanced results with top performing parsers on En- glish. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103",
  "6.5 Comparing our DLM enhanced results with top performing parsers on Chi- nese. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 6.6 The results of our DLM approach on English main evaluation corpus. . . . 104 6.7 The confusion matrix of dependency labels, compared between the DLM approach and the baseline on the in-domain test set. . . . . . . . . . . . . 107 6.8 The confusion matrix of dependency labels, compared between the DLM approach and the baseline on the out-of-domain test sets. . . . . . . . . . . 108 6.9 The English accuracy comparison between the DLM approach and the baseline on unknown words. . . .",
  ". . . . . . . . . . 108 6.9 The English accuracy comparison between the DLM approach and the baseline on unknown words. . . . . . . . . . . . . . . . . . . . . . . . . . . 109 6.10 The example sentences that have been improved by the DLM approach when compared to the baseline on in-domain test set. . . . . . . . . . . . . 112 6.11 The example sentences that have been improved by the DLM approach when compared to the baseline on out-of-domain test sets. . . . . . . . . . 113 6.12 The confusion matrix of dependency labels, compared between the DLM approach and the baseline on Chinese test set. . . . . . . . . . . . . . . . .",
  ". . 113 6.12 The confusion matrix of dependency labels, compared between the DLM approach and the baseline on Chinese test set. . . . . . . . . . . . . . . . . 115 6.13 The Chinese accuracy comparison between the DLM approach and the baseline on unknown words. . . . . . . . . . . . . . . . . . . . . . . . . . . 116",
  "CHAPTER 1 INTRODUCTION Syntactic parsing is an important natural language processing (NLP) task that focuses on analysing the syntactic structures of sentences. The syntax of a sentence has been found to be important to many other NLP tasks that require deeper analysis of the sentences, such as semantic parsing (Surdeanu et al., 2008; Haji\u02c7c et al., 2009), anaphora resolu- tion (Pradhan et al., 2011; Pradhan et al., 2012) and machine translation (Tiedemann, 2012). There are two major families of syntactic parsing, the \ufb01rst one is constituency parsing that generates parse trees of sentences according to phrase structure grammars, the other is dependency parsing that assigns head-child relations to the words of a sen- tence.",
  "There are two major families of syntactic parsing, the \ufb01rst one is constituency parsing that generates parse trees of sentences according to phrase structure grammars, the other is dependency parsing that assigns head-child relations to the words of a sen- tence. Initially, the parsing community mainly focused on constituency parsing systems, as a result, a number of high accuracy constituency parsers have been introduced, such as the Collins Parser (Collins, 1999), Stanford PCFG Parser (Klein and D. Manning, 2003), BLLIP reranking parser (Charniak and Johnson, 2005) and Berkeley Parser (Petrov and Klein, 2007). In the past decade, dependency-based systems have gained more and more attention (McDonald and Pereira, 2006; Nivre, 2009; Martins et al., 2010; Bohnet et al., 2013; Martins et al., 2013), as they have a better multi-lingual capacity and are more e\ufb03cient. For a long period, dependency parsing systems were mainly based on carefully selected feature sets, we denote those systems as conventional dependency parsers.",
  "For a long period, dependency parsing systems were mainly based on carefully selected feature sets, we denote those systems as conventional dependency parsers. In the recent years, a number of dependency parsing systems based on neural networks have also been investigated, some of which have achieved better accuracies when compared to conventional dependency parsers. We evaluated our approaches only on conventional de- 1",
  "pendency parsers, as these neural network-based systems were introduced after we \ufb01nished most of the work. However, the techniques evaluated in this thesis have the potential to be adapted to neural network-based parsers as well. Many dependency parsers are based on supervised learning techniques, which could produce high accuracy when trained on a large amount of training data from the same domain (McDonald and Pereira, 2006; Nivre, 2009; Martins et al., 2010; Bohnet et al., 2013; Martins et al., 2013). However, those models trained on the speci\ufb01c training data are vulnerable when dealing with data from domains di\ufb00erent from the training data (Nivre et al., 2007a; Petrov and McDonald, 2012). One e\ufb00ective way to make models less domain speci\ufb01c is to annotate more balanced corpora. However, the annotation work is very time-consuming and expensive. As a result of these di\ufb03culties, only very limited annotations are available to the community.",
  "One e\ufb00ective way to make models less domain speci\ufb01c is to annotate more balanced corpora. However, the annotation work is very time-consuming and expensive. As a result of these di\ufb03culties, only very limited annotations are available to the community. As an alternative to annotating new corpora, domain adaptation techniques have been introduced to train more robust models for out-of-domain parsing. Semi-supervised methods are one family of those techniques that aim to improve the out-of-domain parsing performance by enhancing the in-domain models with a large amount of unlabelled data. Some semi-supervised methods use the unlabelled data as the additional training data, such as co-training (Sarkar, 2001; Sagae and Tsujii, 2007; Zhang et al., 2012) and self-training (McClosky et al., 2006b; Reichart and Rappoport, 2007; Sagae, 2010). Alternatively, other research uses the unlabelled data indirectly.",
  "Alternatively, other research uses the unlabelled data indirectly. Word clusters (Zhou et al., 2011; Pekar et al., 2014) and word embeddings (Chen and Manning, 2014; Weiss et al., 2015) are examples of this direction. 1.1 Research Questions The focus of this thesis is on using semi-supervised techniques to bridge the accuracies be- tween the in-domain and the out-of-domain dependency parsing. More precisely, this the- sis evaluates three important semi-supervised methods, namely co-training, self-training and dependency language models. Two of the methods use unlabelled data directly as ad- 2",
  "ditional training data (i.e. co-/self-training). Co-training is a method that has been used in many domain adaptation tasks, it uses multiple learners to derive additional training data from unlabelled target domain data. The successful use of co-training is conditioned on learners being as di\ufb00erent as possible. Previous work on parsing with co-training is mainly focused on using learners that are carefully designed to be very di\ufb00erent. In this thesis, we use only o\ufb00-the-shelf dependency parsers as our learners to form our co-training approaches. In total, we evaluate two co-training approaches, the normal co-training (uses two parsers) and the tri-training (uses three parsers). For both approaches, the evaluation learner is retrained on the additional training data annotated identically by two source learners. The normal co-training uses two learners, the evaluation learner is used as one of the source learners, while the tri-training uses three learners, two of which are used as source learners, the third one is used as the evaluation learner. Compare to the normal co-training, tri-training approach allows the evaluation learner to learn from the novel annotations that is not predicted by its own.",
  "Compare to the normal co-training, tri-training approach allows the evaluation learner to learn from the novel annotations that is not predicted by its own. For our evaluation on co-training, we trying to answer the following research questions: Q1. Could the o\ufb00-the-shelf dependency parsers be successfully used in co-training for domain adaptation? Q2. Would tri-training be more e\ufb00ective for out-of-domain parsing when o\ufb00-the-shelf dependency parsers are used? In contrast to co-training, which retrains the parser on additional training data anno- tated by multiple learners, self-training retrains the parser on training data enlarged by its own automatically labelled data. Previous research mainly focused on applying self- training to constituency parsers (McClosky et al., 2006b; Reichart and Rappoport, 2007; Sagae, 2010). Attempts to use self-training for dependency parsing either need additional classi\ufb01ers (Kawahara and Uchimoto, 2008) or only use partial parse trees (Chen et al., 2008).",
  "Attempts to use self-training for dependency parsing either need additional classi\ufb01ers (Kawahara and Uchimoto, 2008) or only use partial parse trees (Chen et al., 2008). In this thesis, we aim to \ufb01nd a more e\ufb00ective way to use self-training for depen- dency parsing. We intend to answer the following research questions for our self-training evaluation: 3",
  "Q3. How could self-training be e\ufb00ectively used in out-of-domain dependency parsing? Q4. If self-training works for English dependency parsing, can it be adapted to other languages? To use auto-labelled data as additional training data is e\ufb00ective but comes with conse- quences. First of all, the re-trained models usually have a lower performance on the source domain data. Secondly, those approaches can only use a relatively small unlabelled data, as training parsers on a large corpus might be time-consuming or even intractable on a corpus of millions of sentences. To overcome those limitations we investigate dependency language models which use the unlabelled data indirectly. Dependency language models (DLM) were previously used by Chen et al. (2012) to leverage the performance and the e\ufb03ciency of a weak second-order graph-based parser (McDonald and Pereira, 2006). In this thesis, we adapt this method to a strong transition-based parser (Bohnet et al., 2013) that on its own can produce very promising accuracies. The research questions for this part are as follows: Q5.",
  "In this thesis, we adapt this method to a strong transition-based parser (Bohnet et al., 2013) that on its own can produce very promising accuracies. The research questions for this part are as follows: Q5. Can dependency language models be adapted to strong transition-based parsers? Q6. Can dependency language models be used for out-of-domain parsing? Q7. Quality or quantity of the auto-parsed data, which one is more important to the successful use of dependency language models? 1.2 Thesis Structure After the introduction, in Chapter 2 we begin by discussing the background knowledge and previous work related to this thesis. This mainly covers two topics, dependency parsing and domain adaptation. We then introduce the Mate parser in detail. Mate is a strong transition-based parser which is used in all of our evaluations. After that, we introduce the corpora and the evaluation/analysis methods. In Chapter 3 we introduce our experiments on agreement-based co-training. It \ufb01rst discusses the e\ufb00ect of using di\ufb00erent o\ufb00-the-shelf parsers on a normal agreement-based co- 4",
  "training setting (i.e. only involves two parsers). And then we introduce our experiments on its variant that uses three parsers (tri-training). Chapter 4 and Chapter 5 introduce our con\ufb01dence-based self-training approaches. In Chapter 4, we introduce our evaluations on con\ufb01dence-based self-training for English out- of-domain dependency parsing. In total, two con\ufb01dence-based methods are compared in our experiments. Chapter 5 introduces our experiments on multi-lingual datasets. The con\ufb01dence-based self-training approach is evaluated on nine languages. Chapter 6 discusses our dependency language models method that is able to improve both in-domain and out-of-domain parsing. The evaluations on English include both in- domain and out-of-domain datasets, in addition to that, we also evaluated on the Chinese in-domain data. Chapter 7 provides a summary of the thesis and gives conclusions. 1.3 Published Work In total, there are four publications based on this thesis. Each of the publications is related to one chapter of this thesis, Pekar et al. (2014) is related to our evaluation on co-training (Chapter 3).",
  "1.3 Published Work In total, there are four publications based on this thesis. Each of the publications is related to one chapter of this thesis, Pekar et al. (2014) is related to our evaluation on co-training (Chapter 3). Yu et al. (2015) is made from our English self-training evaluation (Chapter 4). Yu and Bohnet (2015) is associated with our multi-lingual self-training experiments (Chapter 5). Yu and Bohnet (2017) presents our work on dependency language models (Chapter 6). Juntao Yu and Bernd Bohnet. 2017. Dependency language models for transition-based dependency parsing. In Proceeding of the 15th International Conference on Parsing Technologies, pages 11-17, Pisa, Italy. Association for Computational Linguistics. Juntao Yu and Bernd Bohnet. 2015. Exploring con\ufb01dence-based self-training for multi- lingual dependency parsing in an under-resourced language scenario. In Proceeding of the Third International Conference on Dependency Linguistics, pages 350-358, Uppsala, Sweden.",
  "2015. Exploring con\ufb01dence-based self-training for multi- lingual dependency parsing in an under-resourced language scenario. In Proceeding of the Third International Conference on Dependency Linguistics, pages 350-358, Uppsala, Sweden. Uppsala University. 5",
  "Juntao Yu, Mohab Elkaref, and Bernd Bohnet. 2015. Domain adaptation for depen- dency parsing via self-training. In Proceeding of the 14th International Conference on Parsing Technologies, pages 1-10, Bilbao, Spain. Association for Computational Linguistics. Viktor Pekar, Juntao Yu, Mohab Elkaref, and Bernd Bohnet. 2014. Exploring options for fast domain adaptation of dependency parsers. In Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages, pages 54-65, Dublin, Ireland. Dublin City University. 1.4 Chapter Summary In this chapter, we \ufb01rst brie\ufb02y introduced dependency parsing and the problems of out- of-domain parsing that we are trying to address in this thesis. We then discussed the research questions that we intend to answer. The chapter also gave a brief introduction of the thesis structure. Finally, the chapter illustrated the published works based on this thesis. 6",
  "CHAPTER 2 BACKGROUND AND EXPERIMENT SET-UP In this chapter, we \ufb01rst introduce the background and related work of this thesis, which includes a brief introduction of dependency parsing systems, a detailed introduction of the baseline parser (Bohnet et al., 2013) and previous work on out-of-domain parsing (especially those on semi-supervised approaches). We then introduce the corpora that have been used in this thesis. Finally, we introduce the evaluation metric and the analysis methods. 2.1 Dependency parsing Dependency parsing is one important way to analyse the syntactic structures of natural language. It has been widely studied in the past decade. A dependency parsing task takes natural language (usually tokenised sentence) as input and outputs a sequence of head-dependent relations. Figure 2.1 shows the dependency relations of a sentence (Tom played football with his classmate .) parsed by an o\ufb00-the-shelf dependency parser. During the past decade, many dependency parsing systems have been introduced, most of them are graph-based or transition-based systems. The graph-based system solves the parsing problem by searching for maximum spanning trees (MST).",
  "parsed by an o\ufb00-the-shelf dependency parser. During the past decade, many dependency parsing systems have been introduced, most of them are graph-based or transition-based systems. The graph-based system solves the parsing problem by searching for maximum spanning trees (MST). A \ufb01rst-order MST parser \ufb01rst assigns scores to directed edges between tokens of a sentence. It then uses an algorithm to search a valid dependency tree with the highest score. By contrast, the transition-based system solves the parsing task as a sequence of transition decisions, in 7",
  "<root> Tom played football with his classmate . NNP VBD NN IN PRP$ NN . ROOT SBJ OBJ ADV P PMOD NMOD Figure 2.1: The dependency relations of the sentence (Tom played football with his classmate .) parsed by Mate parser. each step the parser deciding the next transition. In Section 2.1.1 and 2.1.2 we brie\ufb02y describe the two major system types. In recent years, deep learning has been playing an important role in the machine learning community. As a result, several neural network- based systems have been introduced, some of them surpassing the state-of-the-art accuracy achieved by the conventional dependency parsers based on perceptions or SVMs. We brie\ufb02y touch on neural network-based systems in Section 2.1.3, although most of them are still transition/graph-based systems. The evaluation of the neural network-based parsers is beyond the scope of this thesis, as they become popular after most of the work of this thesis has been done.",
  "The evaluation of the neural network-based parsers is beyond the scope of this thesis, as they become popular after most of the work of this thesis has been done. We mainly use the Mate parser (Bohnet et al., 2013), a transition-based approach that was state-of-the-art at the beginning of this work and whose performance remained competitive even after the introduction of the parsers based on neural network. Section 2.1.4 introduces the technical details of the Mate parser. 2.1.1 Graph-based Systems The graph-based dependency parser solves the parsing problem by searching for maxi- mum spanning trees (MST). In the following, we consider the \ufb01rst-order MST parser of McDonald et al. (2005). Let x be the input sentence, y be the dependency tree of x, xi is the ith word of x, (i, j) \u2208y is the directed edge between xi (head) and xj (dependent). 8",
  "(1) Build the graph: <root> Tom plays football (2) Score the edges: <root> Tom plays football 5 40 10 30 20 5 10 (3) Select highest scoring tree: <root> Tom plays football 5 40 10 30 20 5 10 (4) The \ufb01nal parser tree: <root> Tom plays football 40 30 20 Figure 2.2: Parsing the sentence (Tom plays football) with a graph-based dependency parser. dt(x) is used to represent the set of possible dependency trees of the input sentence where y \u2208dt(x). The parser considers all valid directed edges between tokens in x and builds the parse trees in a bottom-up fashion by applying a CKY parsing algorithm. It scores a parse tree y by summing up the scores s(i, j) of all the edges (i, j) \u2208y. The s(i, j) is calculated according to a high-dimensional binary feature representation f and a weight vector w learned from training data \u03c4 (\u03c4 = {(xt, yt)}T t=1).",
  "The s(i, j) is calculated according to a high-dimensional binary feature representation f and a weight vector w learned from training data \u03c4 (\u03c4 = {(xt, yt)}T t=1). To be more speci\ufb01c, the score of a parse tree y of an input sentence x is calculated as follows: 9",
  "s(x, y) = X (i,j)\u2208y s(i, j) = X (i,j)\u2208y w \u2217f(i, j) Where f consists of a set of binary feature representations associated with a number of feature templates. For example, an edge (plays, football) with a bi-gram feature template (headword, depword) will give a value of 1 for the following feature representation: f(i, j) = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 1 if headword = \u201cplays\u201d and depword = \u201cfootball\u201d 0 otherwise After scoring the possible parse trees dt(x), the parser outputs the highest-scored dependency tree ybest. Figure 2.2 shows an example of a sentence being parsed with a \ufb01rst-order graph-based parser. In terms of training, the parser uses an online learning algorithm to learn the weight vector w from the training set \u03c4. In each training step, only one training instance (xt, yt) ((xt, yt) \u2208\u03c4) is considered, the w is updated after each step.",
  "In terms of training, the parser uses an online learning algorithm to learn the weight vector w from the training set \u03c4. In each training step, only one training instance (xt, yt) ((xt, yt) \u2208\u03c4) is considered, the w is updated after each step. More precisely, the Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006) is used to create a margin between the score of a correct parse tree s(xt, yt) and the incorrect ones s(xt, y\u2032) (y\u2032 \u2208 dt(xt)). The loss L(yt, y\u2032) of a dependency tree is de\ufb01ned as the number of incorrect edges. Let w(i), w(i+1) be the weight vector before and after the update of the ith training step, w(i+1) is updated subject to keeping the margin at least as large as the L(yt, y\u2032), while at the same time, keeping the norm of the changes to the w as small as possible. A more detailed training algorithm is showed in algorithm 1.",
  "A more detailed training algorithm is showed in algorithm 1. The MST parser is later improved by McDonald and Pereira (2006) to include second- order features, however, the system is still weaker than its successors which also include third-order features (Koo and Collins, 2010). Other mostly used strong graph-based parsers include Mate graph-based parser (Bohnet, 2010) and Turbo Parser (Martins et al., 2013). 10",
  "Data: \u03c4 = {(xt, yt)}T t=1 Result: w 1 w0 = 0; i = 0; 2 for n : 1..N do // N training iterations 3 for t : 1..T do 4 w(i+1) = update w(i) to min ||w(i+1) \u2212w(i)||; 5 s.t. s(xt, yt) \u2212s(xt, y\u2032) \u2265L(yt, y\u2032); 6 \u2200y\u2032 \u2208dt(xt); 7 i = i + 1; 8 end 9 end Algorithm 1: MIRA algorithm for MST parser 2.1.2 Transition-based Systems The transition-based parsers build the dependency trees in a very di\ufb00erent fashion com- pared to graph-based systems. Instead of searching for the maximum spanning trees, transition-based systems parse a sentence with a few pre-de\ufb01ned transitions. The Malt parser (Nivre et al., 2007b) is one of the earliest transition-based parsers which has been later widely used by researchers.",
  "Instead of searching for the maximum spanning trees, transition-based systems parse a sentence with a few pre-de\ufb01ned transitions. The Malt parser (Nivre et al., 2007b) is one of the earliest transition-based parsers which has been later widely used by researchers. The parser is well engineered and can be con\ufb01gured to use di\ufb00erent transition systems. We take the parser\u2019s default transition system (arc-eager) as an example to show how the transition-based parser works. The Malt parser starts with an initial con\ufb01guration and performs one transition at a time in a deterministic fashion until it reaches the \ufb01nal con\ufb01guration. The parser\u2019s con\ufb01gurations are represented by triples c = (\u03a3, B, A), where \u03a3 is the stack that stores partially visited tokens, B is a list of remaining tokens that are unvisited, and A stores the directed arcs between token pairs that have already been parsed.",
  "The parser\u2019s con\ufb01gurations are represented by triples c = (\u03a3, B, A), where \u03a3 is the stack that stores partially visited tokens, B is a list of remaining tokens that are unvisited, and A stores the directed arcs between token pairs that have already been parsed. The parser\u2019s initial con\ufb01guration consists of an empty \u03a3 and an empty A, while all the input tokens are stored in B. The \ufb01nal con\ufb01guration is required to have an empty B. A set of four transitions (Shift, Left-Arc, Right-Arc and Reduce) are de\ufb01ned to build the parse trees. The Shift transition moves the token on the top of B into \u03a3, the Left-Arc transition adds an arc from the top of B to the top of \u03a3 and removes the token on the top of \u03a3, the Right-Arc transition adds an arc from the top of \u03a3 to the top of B and moves the token on the top of B into \u03a3, and the Reduce 11",
  "The initial state: [ ] [ Tom plays football ] Perform Shift transition: [ Tom ] [ plays football ] Perform Left-Arc transition: [ \u0018\u0018\u0018 Tom ] [ plays football ] SBJ Perform Shift transition: [ plays ] [ football ] Perform Right-Arc transition: [ plays football ] [ ] OBJ Perform Reduce transition: [ plays ((((( football ] [ ] The \ufb01nal parse tree: <root> Tom plays football ROOT OBJ SBJ Figure 2.3: Parsing the sentence (Tom plays football) with an arc-eager transition-based dependency parser. The square brackets denote the stack (left) and the bu\ufb00er (right) used by transition-based parser. transition simply removes the token on the top of \u03a3. More precisely, table 2.1 shows the details of the transitions of an arc-eager system. To train the parser, support vector machine classi\ufb01er (SVM) with the one-versus-all strategy is used to solve the transition-based parser as a multi-classi\ufb01cation problem. In a transition-based parsing scenario, the classes are di\ufb00erent transitions.",
  "To train the parser, support vector machine classi\ufb01er (SVM) with the one-versus-all strategy is used to solve the transition-based parser as a multi-classi\ufb01cation problem. In a transition-based parsing scenario, the classes are di\ufb00erent transitions. Each of the SVMs is trained to maximise the margin between the target transition and the other transitions, as in the one-versus-all strategy the classes other than the target class are treated the same as the negative examples. Since the data may not be linearly separable, they use in additional a quadratic kernel (K(xi, xj) = (\u03b3xT i xj + r)2) to map the data 12",
  "Transition Left-Arc ([\u03c3|i], [j|\u03b2], A) \u21d2(\u03c3, [j|\u03b2], A \u222a{(j \u2192i)} Right-Arc ([\u03c3|i], [j|\u03b2], A) \u21d2([\u03c3|i|j], \u03b2, A \u222a{(i \u2192j)}) Shift (\u03c3, [i|\u03b2], A) \u21d2([\u03c3|i], \u03b2, A) Reduce ([\u03c3|i], B, A) \u21d2(\u03c3, B, A) Table 2.1: Transitions for arc-eager parsing. into a higher dimensional space. The SVMs are trained to predict the next transition based on a given parser con\ufb01guration. They used similar binary feature representations as those of the MST parser, in which the features are mapped into a high dimensional vector.",
  "into a higher dimensional space. The SVMs are trained to predict the next transition based on a given parser con\ufb01guration. They used similar binary feature representations as those of the MST parser, in which the features are mapped into a high dimensional vector. The feature templates for the transition-based system are mainly associated with the con\ufb01gurations, for example, a feature between the \u03a3top (the top of the stack) and the Btop (the top of the Bu\ufb00er) is as follows: fci = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 1 if \u03a3top = \u201cplays\u201d and Btop = \u201cfootball\u201d 0 otherwise Figure 2.3 shows an example of parsing the sentence (Tom plays football) with the Malt transition-based parser. Bene\ufb01ting from the deterministic algorithm, the Malt parser is able to parse the non- projective sentences in linear time (Nivre, 2009), which is much faster compared to the second-order MST parser\u2019s cubic-time parsing (McDonald and Pereira, 2006).",
  "Bene\ufb01ting from the deterministic algorithm, the Malt parser is able to parse the non- projective sentences in linear time (Nivre, 2009), which is much faster compared to the second-order MST parser\u2019s cubic-time parsing (McDonald and Pereira, 2006). Although the deterministic parsing is fast, the error made in the previous transitions will largely a\ufb00ect the decisions taken afterwards, which results in a lower accuracy. To overcome this problem beam search has been introduced to the transition-based systems, which leads to signi\ufb01cant accuracy improvements (Bohnet et al., 2013). 2.1.3 Neural Network-based Systems Neural network-based systems have only been recently introduced to the literature. Chen and Manning (2014) were the \ufb01rst to introduce a simple neural network to a deterministic 13",
  "Figure 2.4: Neural Network architecture taking from Chen and Manning (2014) transition-based parser, yielding good results. The parser used an arc-standard transition system. Similar to arc-eager, the arc-standard is another highly used transition-based system. Many dependency parsers are based on or have options to use an arc-standard approach, which include the Malt parser we introduced in the previous section (section 2.1.2) and our main evaluation parser (Mate parser). We will introduce the arc-standard transition system in more detail in section 2.1.4. One of the major di\ufb00erences between the neural network based systems and the con- ventional systems is the use of feature representations. Instead of using the binary feature representations (commonly used by the conventional systems), the neural network based approaches represent the features by embeddings. During training, feature embeddings (e.g. word, part-of-speech embeddings) are capable of capturing the semantic information of the features. Take the part-of-speech tags as an example, adjective tags JJ, JJR, JJS will have similar embeddings.",
  "During training, feature embeddings (e.g. word, part-of-speech embeddings) are capable of capturing the semantic information of the features. Take the part-of-speech tags as an example, adjective tags JJ, JJR, JJS will have similar embeddings. This allows the neural network-based systems to reduce the feature sparsity problem of the conventional parser systems. Conventional parsers usually represent di\ufb00erent tokens or token combinations by independent feature spaces, thus are highly sparse. Another advantage of using the neural network based approach is that the system 14",
  "allows using the pre-trained word embeddings. Word embeddings extracted from large unlabelled data carry the statistical strength of the words, this could be a better bases for the system when compared to the randomly initialised embeddings. The empirical results con\ufb01rmed that large improvements can be achieved by using the pre-trained word embeddings. The idea of using the pre-trained word embeddings goes into the same direction of the semi-supervised approaches that use unlabelled data indirectly, such as dependency language models evaluated in this thesis, or word clusters. In terms of the network architecture, Chen and Manning (2014) used a single hidden layer and a softmax layer to predict the next transition based on the current con\ufb01guration. To map the input layer to the hidden layer they used a cube activation function (h = (W wxw + W txt + W lxl + b)3), in which xw, xt, xl are feature embeddings of the words, part-of-speech tags and arc labels and W w, W t, W l are the relative weights. Figure 2.4 shows the details of their neural network architecture. This \ufb01rst attempt of using the neural network for dependency parsing leads to many subsequent research.",
  "Figure 2.4 shows the details of their neural network architecture. This \ufb01rst attempt of using the neural network for dependency parsing leads to many subsequent research. Chen and Manning (2014)\u2019s system has been later extended by Weiss et al. (2015) who introduced beam search to the system and achieved state-of-the- art accuracy. Since then a number of more complex and powerful neural networks have been evaluated, such as the stack-LSTM (Dyer et al., 2015) and the bi-directional LSTM (Dozat and Manning, 2017). The current state-of-the-art is achieved by the parser of Dozat and Manning (2017) who used the bi-directional LSTM in their system. 2.1.4 The Mate Parser In this thesis, we mainly used the Mate transition-based parser (Bohnet and Kuhn, 2012; Bohnet and Nivre, 2012; Bohnet et al., 2013).",
  "2.1.4 The Mate Parser In this thesis, we mainly used the Mate transition-based parser (Bohnet and Kuhn, 2012; Bohnet and Nivre, 2012; Bohnet et al., 2013). The parser is one of the best performing parsers on the data set of the major shared task (CoNLL 2009) on dependency parsing (Haji\u02c7c et al., 2009) and it is freely available 1. The parser uses the arc-standard transition system, it is also integrated with a number of techniques to maximise the parser\u2019s per- 1https://code.google.com/p/mate-tools/ 15",
  "Transition Condition Left-Arcd ([\u03c3|i, j], B, A, \u03c0, \u03b4) \u21d2([\u03c3|j], B, A \u222a{(j, i)}, \u03c0, \u03b4[(j, i) \u2192d]) i \u0338= 0 Right-Arcd ([\u03c3|i, j], B, A, \u03c0, \u03b4) \u21d2([\u03c3|i], B, A \u222a{(i, j)}, \u03c0, \u03b4[(i, j) \u2192d]) Shiftp (\u03c3, [i|\u03b2], A, \u03c0, \u03b4) \u21d2([\u03c3|i], \u03b2, A, \u03c0[i\u22a4], \u03b4) Swap ([\u03c3|i, j], B, A, \u03c0, \u03b4) \u21d2([\u03c3|j], [i|\u03b2], A, \u03c0, \u03b4) 0 < i < j Table 2.2: Transitions for joint tagging and parsing taking from Bohnet et al. (2013). \u03a3 (the stack) is represented as a list with its head to the right and a tail \u03c3; The bu\ufb00er B as a list with its head to the left and tail \u03b2. formance.",
  "(2013). \u03a3 (the stack) is represented as a list with its head to the right and a tail \u03c3; The bu\ufb00er B as a list with its head to the left and tail \u03b2. formance. Firstly, the parser employs a beam search to go beyond the greedy approach. Secondly, it uses an additional optional graph-based model to rescore the beam entries. In their paper (Bohnet and Kuhn, 2012), they name it completion model as it scores factors of the graph as soon as they are \ufb01nished by the parser. Furthermore, the parser has an option for joint tagging and parsing (Bohnet and Nivre, 2012). Same as the pipeline sys- tem, the tagger model is trained separately from the parser model. However, during the parsing, instead of using only the best-predicted part-of-speech (PoS) tag, they made the n-best (n > 1) PoS tags of a token available to the parser. The joint system is able to gain a higher accuracy for both PoS tagging and parsing compared to a pipeline system.",
  "The joint system is able to gain a higher accuracy for both PoS tagging and parsing compared to a pipeline system. In this thesis, we use the Mate parser as our baseline and make the necessary modi\ufb01cations, where appropriate to comply with the requirements of our approaches. The transition-based part of the parser uses a modi\ufb01ed arc-standard transition system. Comparing to the original arc-standard transition system (has only three transitions: Left- Arc, Right-Arc and Shift) of Nivre (2004), the Mate parser modi\ufb01ed the Shift transition for joint tagging and parsing and included the Swap transition to handling non-projective parsing. More precisely, the parser tags and parses a sentence x = w1, ..., wn using a sequence of transitions listed in Table 2.2. An additional arti\ufb01cial token <root> (w0) is added to the beginning of the sentence to allow the parser assigning a Root to the sentence at the last step of the transitions. The transitions change the initial con\ufb01guration (cs) in steps until reaching a terminal con\ufb01guration (ct). Bohnet et al.",
  "The transitions change the initial con\ufb01guration (cs) in steps until reaching a terminal con\ufb01guration (ct). Bohnet et al. (2013) used the 5-tuples C = (\u03a3, B, A, \u03c0, \u03b4) to represent all con\ufb01gurations, where \u03a3 (the stack) and B (the 16",
  "bu\ufb00er) refers to disjoint sublists of the sentence x, A is a set of arcs, \u03c0 and \u03b4 are functions to assign a part-of-speech tag to each word and a dependency label to each arc. The initial con\ufb01guration (cs) has an empty stack, the bu\ufb00er consists of the full input sentence x, and the arc set A is empty. The terminal con\ufb01guration (ct) is characterised by an empty stack and bu\ufb00er, hence no further transitions can be taken. The arc set A consists of a sequence of arc pairs (i, j), where i is the head and j is the dependent. They use Tree(x, c) to represent the tagged dependency tree de\ufb01ned for x by c = (\u03a3, B, A, \u03c0, \u03b4). As shown in Table 2.2, the Left-Arcd adds an arc from the token (j) at the top of the stack (\u03a3) to the token (i) at the second top of the stack and removes the dependent (i) from the stack.",
  "As shown in Table 2.2, the Left-Arcd adds an arc from the token (j) at the top of the stack (\u03a3) to the token (i) at the second top of the stack and removes the dependent (i) from the stack. At the same time, the \u03b4 function assigns a dependency label (d) to the newly created arc (j, i). The Left-Arcd transition is permissible as long as the token at the second top of the stack is not the <root> (i.e. i \u0338= 0). The Right-Arcd adds a labelled arc from the token (i) at the second top of the stack to the token (j) at the top of the stack and removes the later. The Shiftp transition assigns a PoS tag p to the \ufb01rst node of the bu\ufb00er and moves it to the top of the stack. The Swap transition that is used to handling non-projective tree extracts the token (i) at the second top of the stack and moves it back to the bu\ufb00er.",
  "The Swap transition that is used to handling non-projective tree extracts the token (i) at the second top of the stack and moves it back to the bu\ufb00er. The Swap transition is only permissible when the top two tokens of the stack are in the original word order (i.e. i < j), this prevents the same two tokens from being swapped more than once. In additional, the arti\ufb01cial <root> token is not allowed to be swapped back to the bu\ufb00er (i.e. i > 0). Figure 2.5 shows an example of joint tagging and parsing a sentence by the Mate parser. The graph-based completion model consists of a number of di\ufb00erent second- and third- order feature models to rescore the partial parse tree Treen(x, cn). Some feature models are similar to Carreras (2007) and Koo and Collins (2010). Take one of the models 2a as an example, which consists of the second-order factors of Carreras (2007): 1. The head and the dependent. 2. The head, the dependent and the right/left-most grandchild in between.",
  "Take one of the models 2a as an example, which consists of the second-order factors of Carreras (2007): 1. The head and the dependent. 2. The head, the dependent and the right/left-most grandchild in between. 17",
  "Initial state: [ ] [ <root> A hearing is scheduled on the issue ] Shift x 3: [ <root> A/DT hearing/NN ] [ is scheduled on the issue ] Left-Arc: [ <root> \u0018\u0018\u0018 \u0018 A/DT hearing/NN ] [ is scheduled on the issue ] DET Shift x 3: [ <root> hearing/NN is/VBZ scheduled/VBN on/IN ] [ the issue ] Swap x 2: [ <root> hearing/NN on/IN ] [ is/VBZ scheduled/VBN the issue ] Shift x 3: [ <root> hearing/NN on/IN is/VBZ scheduled/VBN the/DT ] [ issue ] Swap x 2: [ <root> hearing/NN on/IN the/DT ] [ is/VBZ scheduled/VBN issue ] Shift x 3: [ <root> hearing/NN on/IN the/DT is/VBZ scheduled/VBN issue/NN ] [ ] Swap x 2: [ <root> hearing/NN on/IN the/DT issue/NN] [ is/VBZ scheduled/VBN ] Left-Arc: [ <root> hearing/NN",
  "hearing/NN on/IN the/DT is/VBZ scheduled/VBN issue/NN ] [ ] Swap x 2: [ <root> hearing/NN on/IN the/DT issue/NN] [ is/VBZ scheduled/VBN ] Left-Arc: [ <root> hearing/NN on/IN \u0018\u0018\u0018\u0018 the/DT issue/NN] [ is/VBZ scheduled/VBN ] DET Right-Arc x 2: [ <root> hearing/NN \u0018\u0018\u0018 \u0018 on/IN \u0018\u0018\u0018\u0018\u0018 issue/NN] [ is/VBZ scheduled/VBN ] PC NMOD Shift: [ <root> hearing/NN is/VBZ ] [ scheduled/VBN ] Left-Arc: [ <root> (((((( hearing/NN is/VBZ ] [ scheduled/VBN ] SBJ Shift: [ <root> is/VBZ scheduled/VBN ] [ ] Right-Arc x 2: [ <root> \u0018\u0018\u0018\u0018 is/VBZ (((((((( scheduled/VBN ] [ ] ROOT VG Output: <root> A/DT hearing/NN is/VBZ scheduled/VBN on/NN the/DT issue/NN ROOT DET NMOD",
  "[ ] Right-Arc x 2: [ <root> \u0018\u0018\u0018\u0018 is/VBZ (((((((( scheduled/VBN ] [ ] ROOT VG Output: <root> A/DT hearing/NN is/VBZ scheduled/VBN on/NN the/DT issue/NN ROOT DET NMOD SBJ VG PC DET Figure 2.5: Parsing the sentence (A hearing is scheduled on the issue) with the Mate transition-based dependency parser. The square brackets denote the stack (left) and the bu\ufb00er (right) used by transition-based parser. 3. The head, the dependent and the right/left-most grandchild away from the head. 4. The head, the dependent and between those words the right/left-most sibling. 18",
  "Data: (x, w, b) Result: Tree(x, h.c) 1 h0.c \u2190cs(x); 2 h0.s \u21900.0; 3 h0.f \u2190{0.0}dim(w); 4 Beam \u2190[h0]; 5 while \u2203c \u2208Beam : c /\u2208Ct do 6 Tmp \u2190[ ]; 7 for h : Beam do 8 for t \u2208T : Permissible(c, t) do 9 h.f \u2190h.f + f(h.c, t); 10 h.s \u2190h.s + f(h.c, t) \u2217w; 11 h.c \u2190t(h.c); 12 Tmp \u2190Insert(h, Tmp); 13 end 14 end 15 Beam \u2190Prune(Tmp, b); 16 end 17 h \u2190Top(Beam); 18 return Tree(x, h.c); Algorithm 2: Beam search algorithm for the Mate parser Feature models are independent to each other and can be easily turned on/o\ufb00by con\ufb01guration.",
  "The score of a parse tree Tree(x, c) or a partial parse tree Treen(x, cn) is then de\ufb01ned as the sum of the scores from the both parts: Score(x, c) = ScoreT(x, c) + ScoreG(x, c) Where ScoreT(x, c) is the score of the transition-based part of the parser and ScoreG(x, c) is the score from the graph-based completion model. Mate parser uses similar binary feature representations as those of the MST/Malt parser (the features are represented by a high dimensional feature vector (f)). A learned weight vector (w) is used with the feature vector (f) to score the con\ufb01gurations in con- junction with the next transition. In addition, the parser uses the beam search to mitigate error propagation. Comparing with the deterministic parsing algorithm that only keeps the best partial parse tree, the beam search approach keeps the n-best partial parse trees during the inference. By using the beam search, errors made in the early stage can po- 19",
  "tentially be recovered in the late stage, as long as the correct con\ufb01guration has not fallen out of the beam. The beam search algorithm takes a sentence (x), the weight vector (w) and the beam size parameter (b) and returns the best scoring parse tree (Tree(x, h.c)). A parse hypothesis (h) of a sentence consists of a con\ufb01guration (h.c), a score (h.s) and a feature vector (h.f). Initially the Beam only consists of the initial hypothesis (h0), in which h0 contains a initial con\ufb01guration of the sentence (cs(x)), a score of 0.0 and a initial feature vector ({0.0}dim(w)). The transitions (T) change the hypotheses in steps and create new hypotheses by applying di\ufb00erent permissible transitions to them. For each step, the top b scoring hypotheses are kept in the Beam. The beam search terminates when every hypothesis in the Beam contains a terminal con\ufb01guration (h.c \u2208Ct). It then returns the top scoring parse tree (Tree(x, h.c)).",
  "For each step, the top b scoring hypotheses are kept in the Beam. The beam search terminates when every hypothesis in the Beam contains a terminal con\ufb01guration (h.c \u2208Ct). It then returns the top scoring parse tree (Tree(x, h.c)). Algorithm 2 outlines the details of the beam search algorithm used by the Mate parser. In order to learn the weight vector, the parser goes through the training set (\u03c4 = {(xt, yt)}T t=1) for N iterations. The weight vector is updated for every sentence xt when an incorrect parse is returned (i.e. the highest scoring parse y\u2217 t is di\ufb00erent from the gold parse yt). More precisely, the passive-aggressive update of Crammer et al. (2006) is used: w(i+1) = w(i) + f(xt, yt) \u2212f(xt, y\u2217 t ) ||f(xt, yt) \u2212f(xt, y\u2217 t )||2 In this thesis, unless speci\ufb01ed, we used the default settings of the parser: 1. We use all the graph-based features of the completion model.",
  "We use all the graph-based features of the completion model. 2. We use the joint PoS-tagging with two-best tags for each token. 3. We use a beam of 40. 4. We use 25 iterations of training. 5. We do not change the sentence order of the training data during training. 20",
  "2.2 Out-of-domain Parsing The release of the large manually annotated Penn Treebank (PTB) (P. Marcus et al., 1993) and the development of the supervised learning techniques enable researchers to work on the supervised learning based parsing systems. Over the last two decades, the parsing accuracy has been signi\ufb01cantly improved. A number of strong parsing systems for both constituency and dependency families have been developed (Klein and D. Manning, 2003; Petrov and Klein, 2007; Bohnet et al., 2013; Martins et al., 2013; Weiss et al., 2015; Dozat and Manning, 2017). The parsers based on supervised learning techniques capture statistics from labelled corpora to enable the systems to correctly predict parse trees when input the corresponding sentences. Since the PTB corpus contains mainly texts from news domain, the supervised learning based parsers trained on PTB corpus are sensitive to domain shifting. Those systems are able to achieve high accuracies when tested on the PTB test set (i.e. in-domain parsing). However, when applying them on data from di\ufb00erent sources (i.e.",
  "Those systems are able to achieve high accuracies when tested on the PTB test set (i.e. in-domain parsing). However, when applying them on data from di\ufb00erent sources (i.e. out-of-domain parsing), such as web domain (Petrov and McDonald, 2012) and chemical text(Nivre et al., 2007a), the accuracy drops signi\ufb01cantly. Table 2.3 shows a comparison of the in-domain and out-of-domain parsing performance of three parsers that have been frequently used by researchers (i.e. MST (McDonald and Pereira, 2006), Malt (Nivre, 2009), and Mate parser (Bohnet et al., 2013)). Those parsers are trained on the training data from the major shared task on dependency parsing (i.e. CoNLL 2009 (Haji\u02c7c et al., 2009)). The training set contains mainly the news domain data from the Penn Treebank.",
  "Those parsers are trained on the training data from the major shared task on dependency parsing (i.e. CoNLL 2009 (Haji\u02c7c et al., 2009)). The training set contains mainly the news domain data from the Penn Treebank. In our evaluation, we \ufb01rst test them on the CoNLL test set which denotes our in-domain examples; for our out-of-domain examples we test the parsers on a number of di\ufb00erent domains from the OntoNotes v5.01 corpus. As we can see from the results, the accuracies on out-of-domain texts are much lower than that of in-domain texts, with the largest accuracy di\ufb00erence of more than 15% (i.e. Mate parser has an accuracy of 90.1% on in-domain texts and an accuracy of 74.4% on texts from broadcast 1https://catalog.ldc.upenn.edu/LDC2013T19 21",
  "Domain MST Malt Mate Newswire 84.8 81.7 87.1 Pivot Texts 84.9 83.0 86.6 Broadcast News 79.4 78.1 81.2 Magazines 77.1 74.7 79.3 Broadcast Conversation 73.4 70.5 74.4 CoNLL 86.9 84.7 90.1 Table 2.3: Labelled attachment scores achieved by the MST, Malt, and Mate parsers trained on the Conll training set and tested on di\ufb00erent domains. conversations). How can we reduce the accuracy gap between the in-domain and the out- of-domain parsing? The most straightforward way would be annotating more text for the target domain, however, this approach is very expensive and time-consuming. There are only very limited manually annotated corpora available, which con\ufb01rms the high costs of the annotation process. Domain adaptation is a task focused on solving the out-of-domain problems but without the need for manual annotation.",
  "There are only very limited manually annotated corpora available, which con\ufb01rms the high costs of the annotation process. Domain adaptation is a task focused on solving the out-of-domain problems but without the need for manual annotation. There are a number of directions to work on the domain adaptation task, each of them focusing on a di\ufb00erent aspect. These directions include semi-supervised techniques, domain speci\ufb01c training data selection, external lexicon resources and parser ensembles. Each direction has its own advantages and disadvantages, we brie\ufb02y discuss in Section 2.2.1. In this thesis, we mainly focus on one direction that improves the out-of-domain accuracy by using unlabelled data (Semi- supervised approaches). Similar to other domain adaptation approaches, semi-supervised approaches do not require to manually annotate new data, but instead, they use the widely available unlabelled data. Some semi-supervised approaches focus on boosting the training data by unlabelled data that is automatically annotated by the base models, others aid the parsers by incorporating features extracted from the large unlabelled data. In Section 2.2.2 we discuss both approaches in detail.",
  "Some semi-supervised approaches focus on boosting the training data by unlabelled data that is automatically annotated by the base models, others aid the parsers by incorporating features extracted from the large unlabelled data. In Section 2.2.2 we discuss both approaches in detail. 2.2.1 Approaches to Out-of-Domain Parsing As stated above, the domain adaptation techniques are designed to \ufb01ll the accuracy gaps between the source domain and the target domain. Previous work on domain adaptation 22",
  "tasks is mainly focused on four directions: semi-supervised techniques (Sarkar, 2001; McClosky et al., 2006b; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Koo et al., 2008; Sagae, 2010; Zhou et al., 2011; Zhang et al., 2012), target domain training data selection (Plank and van Noord, 2011; S\u00f8gaard and Plank, 2012; Khan et al., 2013), external lexicon resources (Szolovits, 2003; Pyysalo et al., 2006; Pekar et al., 2014) and parser ensembles (Nivre et al., 2007a; Le Roux et al., 2012; Zhang et al., 2012; Petrov and McDonald, 2012). The semi-supervised techniques focus on exploring the largely available unlabelled data. There are two major ways to use the unlabelled data. The \ufb01rst family aims to boost the training data.",
  "The semi-supervised techniques focus on exploring the largely available unlabelled data. There are two major ways to use the unlabelled data. The \ufb01rst family aims to boost the training data. Data that has been automatically annotated by the base models are used directly in re-training as the additional training set, up-training, self-training and co-training are techniques of this family. The other family uses the features extracted from unlabelled data to aid the base model, this type of techniques include word embeddings, word clusters and dependency language models. In this thesis, we use semi-supervised techniques from both families and we will discuss them in detail in Section 2.2.2. Domain speci\ufb01c training data selection is a technique based on the assumption that similarity methods are able to derive a subset of the source domain training data that \ufb01ts an individual test domain. Plank and van Noord (2011) investigated several similarity methods to automatically select sentences from training data for the target domain, which gain signi\ufb01cant improvements when comparing with random selection. Positive impacts are also found by Khan et al.",
  "Plank and van Noord (2011) investigated several similarity methods to automatically select sentences from training data for the target domain, which gain signi\ufb01cant improvements when comparing with random selection. Positive impacts are also found by Khan et al. (2013) when they experimented with training data selection on parsing \ufb01ve sub-genres of web data. The advantage of this technique is that it does not need any extra data, however, it is also restricted to learn only from the source domain training set. Lack of the knowledge of the unknown words is one of the well-known problems faced by domain adaptation tasks, i.e. target domain test sets usually contain more unknown words (vocabularies which did not appear in the training data) than source domain test sets (Nivre et al., 2007a; Petrov and McDonald, 2012) . One way to solve this problem is 23",
  "to use the external lexicon resources created by the linguistics. External lexicons provide additional information for tokens, such as word lemma, part-of-speech tags, morphological information and so on. This information can be used by parsers directly to help making the decision. Previously, lexicons have been used by Szolovits (2003) and Pyysalo et al. (2006) to improve the link grammar parser on the medical domain. Both approaches showed large improvements on parsing accuracy. Recently, Pekar et al. (2014) extracted a lexicon from a crowd-sourced online dictionary (Wiktionary) and applied it to a strong dependency parser. Unfortunately, in their approach, the dictionary achieved a moderate improvement only. The fourth direction of domain adaptation is parser ensembles, it becomes more no- ticeable, due to its good performance in shared tasks.",
  "Unfortunately, in their approach, the dictionary achieved a moderate improvement only. The fourth direction of domain adaptation is parser ensembles, it becomes more no- ticeable, due to its good performance in shared tasks. For example, in the \ufb01rst workshop on syntactic analysis of non-canonical language (SANCL), the ensemble-based systems on average produced much better results than that of single parsers (Le Roux et al., 2012; Zhang et al., 2012; Petrov and McDonald, 2012). However, those ensemble-based systems are not used in real-world tasks, due to the complex architectures and high running time. 2.2.2 Semi-Supervised Approaches Semi-supervised approaches use unlabelled data to bridge the accuracy gap between in- domain and out-of-domain. In recent years, unlabelled data has gained large popularity in syntactic parsing tasks, as it can easily and inexpensively be obtained, cf.",
  "In recent years, unlabelled data has gained large popularity in syntactic parsing tasks, as it can easily and inexpensively be obtained, cf. (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006a; Koo et al., 2008; S\u00f8gaard and Rish\u00f8j, 2010; Petrov and McDonald, 2012; Chen et al., 2013; Weiss et al., 2015). This is in stark contrast to the high costs of manually labelling new data. Some techniques such as self-training (McClosky et al., 2006a) and co-training (Sarkar, 2001) use auto- parsed data as additional training data. This enables the parser to learn from its own or other parsers\u2019 annotations. Other techniques include word clustering (Koo et al., 2008) and word embedding (Bengio et al., 2003) which are generated from a large amount of unlabelled data. The outputs can be used as features or inputs for parsers. Both groups 24",
  "of techniques have been shown e\ufb00ective on syntactic parsing tasks (Zhou and Li, 2005; Reichart and Rappoport, 2007; Sagae, 2010; S\u00f8gaard and Rish\u00f8j, 2010; Yu et al., 2015; Weiss et al., 2015). Boosting the Training Set The \ufb01rst group uses unlabelled data (usually parsed data) directly in the training process as additional training data. The most common approaches in this group are co-training and self-training. Co-training is a technique, that has been frequently used by domain adaptation for parsers (Sarkar, 2001; Sagae and Tsujii, 2007; Zhang et al., 2012; Petrov and McDonald, 2012). The early version of co-training uses two di\ufb00erent \u2019views\u2019 of the classi\ufb01er, each \u2019view\u2019 has a distinct feature set. Two \u2019views\u2019 are used to annotate unlabelled set after trained on the same training set.",
  "The early version of co-training uses two di\ufb00erent \u2019views\u2019 of the classi\ufb01er, each \u2019view\u2019 has a distinct feature set. Two \u2019views\u2019 are used to annotate unlabelled set after trained on the same training set. Then both classi\ufb01ers are retrained on the newly an- notated data and the initial training set (Blum and Mitchell, 1998). Blum and Mitchell (1998) \ufb01rst applied a multi-iteration co-training on classifying web pages. Then it was extended by Collins and Singer (1999) to investigate named entity classi\ufb01cation. At that stage, co-training strongly depended on the splitting of features (Zhu, 2005). One year after, Goldman and Zhou (2000) introduced a new variant of co-training which used two di\ufb00erent learners, but both of them took the whole feature sets. One learner\u2019s high con\ufb01- dence data are used to teach the other learner. After that, Zhou and Li (2005) proposed another variant of co-training (tri-training).",
  "One learner\u2019s high con\ufb01- dence data are used to teach the other learner. After that, Zhou and Li (2005) proposed another variant of co-training (tri-training). Tri-training used three learners, each learner is designed to learn from data on which the other two learners have agreed. In terms of the use of co-training in the syntactic analysis area, Sarkar (2001) \ufb01rst applied the co-training to a phrase structure parser. He used a subset (9695 sentences) of labelled Wall Street Journal data as initial training set and a larger pool of unlabelled data (about 30K sentences). In each iteration of co-training, the most probable n sentences from two views are added to the training set of the next iteration. In their experiments, the parser achieved signi\ufb01cant improvements in both precision and recall (7.79% and 25",
  "10.52% respectively) after 12 iterations of co-training. The work most close to ours was presented by (Sagae and Tsujii, 2007) in the shared task of the conference on computational natural language learning (CoNLL). They used two di\ufb00erent settings of a shift-reduce parser to complete a one iteration co-training, and their approach successfully achieved improvements of approximately 2-3%. Their outputs have also scored the best in the out-of-domain track (Nivre et al., 2007a). The two settings they used in their experiments are distinguished from each other in three ways. Firstly, they parse the sentences in reverse directions (forward vs backward). Secondly, the search strategies are also not the same (best-\ufb01rst vs deterministic). Finally, they use di\ufb00erent learners (maximum entropy classi\ufb01er vs support vector machine).",
  "Firstly, they parse the sentences in reverse directions (forward vs backward). Secondly, the search strategies are also not the same (best-\ufb01rst vs deterministic). Finally, they use di\ufb00erent learners (maximum entropy classi\ufb01er vs support vector machine). The maximum entropy classi\ufb01er learns a conditional model p(y|x) by maximising the conditional entropy (H(p) = \u2212P x,y \u02dcp(x)p(y|x) log p(y|x))1, while the support vector machines (SVMs) are linear classi\ufb01ers trained to maximise the margin between di\ufb00erent classes. In order to enable the multi-class classi\ufb01cation, they used the all-versus-all strategy to train multiple SVMs for predicting the next transition. In addition, a polynomial kernel with degree 2 is used to make the data linearly separable. Sagae and Tsujii (2007) proved their assumptions in their experiments. Firstly, the two settings they used are di\ufb00erent enough to produce distinct results. Secondly, the perfect agreement between two learners is an indication of correctness.",
  "Sagae and Tsujii (2007) proved their assumptions in their experiments. Firstly, the two settings they used are di\ufb00erent enough to produce distinct results. Secondly, the perfect agreement between two learners is an indication of correctness. They reported that the labelled attachment score could be above 90% when the two views agreed. By contrast, the labelled attachment scores of the individual view were only between 78% and 79%. Tri-training is a variant of co-training. A tri-training approach uses three learners, in which the source learner is retrained on the data produced by the other two learners. This allows the source learner to explore additional annotations that are not predicted by its own, thus it has a potential to be more e\ufb00ective than the co-training. Tri-training is used by (Zhang et al., 2012) in the \ufb01rst workshop on syntactic analysis of non-canonical language (SANCL)(Petrov and McDonald, 2012). They add the sentences which the two 1\u02dcp(x) is the empirical distribution of x in the training data. 26",
  "parsers agreed on into the third parser\u2019s training set, then retrain the third parser on the new training set. However, in their experiments, tri-training did not signi\ufb01cantly a\ufb00ect their results. Recently, Weiss et al. (2015) used normal agreement based co-training and tri-training in their evaluation of a state-of-the-art neural network parser. Their evaluation is similar to the Chapter 3 of this thesis, although they used di\ufb00erent parsers. Please note their paper is published after our evaluation on co-training (Pekar et al., 2014). In their work, the annotations agreed by a conventional transition-based parser (zPar) (Zhang and Nivre, 2011) and the Berkeley constituency parser (Petrov and Klein, 2007) have been used as additional training data. They retrained their neural network parser and the zPar parser on the extended training data. The neural network parser gained around 0.3% from the tri-training, and it outperforms the state-of-the-art accuracy by a large 1%.",
  "They retrained their neural network parser and the zPar parser on the extended training data. The neural network parser gained around 0.3% from the tri-training, and it outperforms the state-of-the-art accuracy by a large 1%. By contrast, their co-training evaluation on the zPar parser found only negative e\ufb00ects. Self-training is another semi-supervised technique that only involves one learner. In a typical self-training iteration, a learner is \ufb01rstly trained on the labelled data, and then the trained learner is used to label some unlabelled data. After that, the unlabelled data with the predictions (usually the high con\ufb01dent predictions of the model) are added to the training data to re-train the learner. The self-training iteration can also be repeated to do a multi-iteration self-training. When compared with co-training, self-training has a number of advantages. Firstly unlike the co-training that requires two to three learners, the self-training only requires one learner, thus it is more likely we can use the self-training than co-training in an under resourced scenario.",
  "When compared with co-training, self-training has a number of advantages. Firstly unlike the co-training that requires two to three learners, the self-training only requires one learner, thus it is more likely we can use the self-training than co-training in an under resourced scenario. Secondly, to generate the additional training data, co-training requires the unlabelled data to be double annotated by di\ufb00erent learners, this is more time-consuming than self-training\u2019s single annotation requirement. In term of the previous work on parsing via self-training, Charniak (1997) \ufb01rst applied self-training to a PCFG parser, but this \ufb01rst attempt of using self-training for parsing failed. Steedman et al. (2003) implemented self-training and evaluated it using several settings. They used a 500 sentences training data and parsed only 30 sentences in each 27",
  "self-training iteration. After multiple self-training iterations, it only achieved moderate improvements. This is caused probably by the small number of additional sentences used for self-training. McClosky et al. (2006a) reported strong self-training results with an improvement of 1.1% f-score by using the Charniak-parser, cf. (Charniak and Johnson, 2005). The Charniak-parser is a two stage parser that contains a lexicalized context-free parser and a discriminative reranker. They evaluated on two di\ufb00erent settings. In the \ufb01rst setting, they add the data annotated by both stages and retrain the \ufb01rst stage parser on the new training set, this results in a large improvement of 1.1%. In the second setting, they retrain the \ufb01rst stage parser on its own annotations, the result shows no improvement. Their \ufb01rst setting is similar to the co-training as the \ufb01rst stage parser is retrained on the annotation co-selected by the second stage reranker, in which the additional training data is more accurate than the predictions of \ufb01rst stage parser.",
  "Their \ufb01rst setting is similar to the co-training as the \ufb01rst stage parser is retrained on the annotation co-selected by the second stage reranker, in which the additional training data is more accurate than the predictions of \ufb01rst stage parser. McClosky et al. (2006b) applied the same method later on out-of-domain texts which show good accuracy gains too. Reichart and Rappoport (2007) showed that self-training can improve the performance of a constituency parser without a reranker for the in-domain parsing. However, their approach used only a rather small training set when compared to that of McClosky et al. (2006a). Sagae (2010) investigated the contribution of the reranker for a constituency parser in a domain adaptation setting. Their results suggest that constituency parsers without a reranker can achieve statistically signi\ufb01cant improvements in the out-of-domain parsing, but the improvement is still larger when the reranker is used. In the workshop on syntactic analysis of non-canonical language (SANCL) 2012 shared task, self-training was used by most of the constituency-based systems, cf.",
  "In the workshop on syntactic analysis of non-canonical language (SANCL) 2012 shared task, self-training was used by most of the constituency-based systems, cf. (Petrov and McDonald, 2012). The top ranked system is also enhanced by self-training, this indicates that self-training is probably an established technique to improve the accuracy of con- stituency parsing on out-of-domain data, cf. (Le Roux et al., 2012). However, none of 28",
  "the dependency-based systems used self-training in the SANCL 2012 shared task. One of the few successful approaches to self-training for dependency parsing was in- troduced by Chen et al. (2008). They improved the unlabelled attachment score by about one percentage point for Chinese.Chen et al. (2008) added parsed sentences that have a high ratio of dependency edges that span only a short distance, i.e. the head and de- pendent are close together. The rationale for this procedure is the observation that short dependency edges show a higher accuracy than longer edges. Kawahara and Uchimoto (2008) used a separately trained binary classi\ufb01er to select reliable sentences as additional training data. Their approach improved the unlabelled accuracy of texts from a chemical domain by about 0.5%. Goutam and Ambati (2011) applied a multi-iteration self-training approach on Hindi to improve parsing accuracy within the training domain. In each iteration, they add a small number (1,000) of additional sentences to a small initial training set of 2,972 sentences, the additional sentences were selected due to their parse scores.",
  "In each iteration, they add a small number (1,000) of additional sentences to a small initial training set of 2,972 sentences, the additional sentences were selected due to their parse scores. They improved upon the baseline by up to 0.7% and 0.4% for labelled and unlabelled attachment scores after 23 self-training iterations. While many other evaluations on self-training for dependency parsing are found un- helpful or even have negative e\ufb00ects on results. Plank (2011) applied self-training with single and multiple iterations for parsing of Dutch using the Alpino parser (Malouf and Noord, 2004), which was modi\ufb01ed to produce dependency trees. She found self-training produces only a slight improvement in some cases but worsened when more unlabelled data is added. Plank and S\u00f8gaard (2013) used self-training in conjunction with dependency triplets statistics and the similarity-based sentence selection for Italian out-of-domain parsing. They found the e\ufb00ects of self-training are unstable and does not lead to an improvement. Cerisara (2014) and Bj\u00a8orkelund et al.",
  "They found the e\ufb00ects of self-training are unstable and does not lead to an improvement. Cerisara (2014) and Bj\u00a8orkelund et al. (2014) applied self-training to dependency pars- ing on nine languages. Cerisara (2014) could only report negative results in their self- training evaluations for dependency parsing. Similarly, Bj\u00a8orkelund et al. (2014) could 29",
  "observe only on Swedish a positive e\ufb00ect. Integrating with Features Learned from Unlabelled Data The second group uses the unlabelled data indirectly. Instead of using the unlabelled data as training data, they incorporate the information extracted from large unlabelled data as features to the parser. Word clusters (Koo et al., 2008; Cerisara, 2014) and word embeddings (Chen and Manning, 2014; Weiss et al., 2015) are most well-known approaches of this family. However, other attempts have also been evaluated, such as dependency language models (DLM) (Chen et al., 2012). Word Clustering is an unsupervised algorithm that is able to group the similar words into the same classes by analysing the co-occurrence of the words in a large unlabelled corpus. The popular clustering algorithm includes Brown (Brown et al., 1992; Liang, 2005) and the Latent dirichlet allocation (LDA) (Chrupala, 2011) clusters. Koo et al.",
  "The popular clustering algorithm includes Brown (Brown et al., 1992; Liang, 2005) and the Latent dirichlet allocation (LDA) (Chrupala, 2011) clusters. Koo et al. (2008) \ufb01rst employed a set of features based on brown clusters to a second- order graph-based dependency parser. They evaluated on two languages (English and Czech) and yield about one percentage improvements for both languages. The similar features have been adapted to a transition-based parser of Bohnet and Nivre (2012). The LDA clusters have been used by Cerisara (2014) in the workshop on statistical parsing of morphologically rich languages (SPMRL) 2014 shared tasks (Seddah et al., 2014) on parsing nine di\ufb00erent languages, their system achieved the best average results across all non-ensemble parsers. Word embeddings is another approach that relies on the co-occurrence of the words.",
  "Word embeddings is another approach that relies on the co-occurrence of the words. Instead of assigning the words into clusters, word embedding represent words as a low dimensional vector (such as 50 or 300 dimensional vector), popular word embedding algo- rithms include word2vec (Mikolov et al., 2013) and global vectors for word representation (GloVe) (Pennington et al., 2014). Due to the nature of the neural networks, word embed- dings can be e\ufb00ectively used in the parsers based on neural networks. By using pre-trained word embeddings the neural network-based parsers can usually achieve a higher accuracy 30",
  "compared with those who used randomly initialised embeddings (Chen and Manning, 2014; Weiss et al., 2015; Dozat and Manning, 2017). Other Approaches that use di\ufb00erent ways to extract features from unlabelled data have also been reported. Mirroshandel et al. (2012) used lexical a\ufb03nities to rescore the n-best parses. They extract the lexical a\ufb03nities from parsed French corpora by calculating the relative fre- quencies of head-dependent pairs for nine manually selected patterns. Their approach gained a labelled improvement of 0.8% over the baseline. Chen et al. (2012) applied high-order DLMs to a second-order graph-based parser. This approach is most close to the Chapter 6 of this thesis. The DLMs allow the new parser to explore higher-order features without increasing the time complexity. The DLMs are extracted from a 43 million words English corpus (Charniak, 2000) and a 311 million words corpus of Chinese (Huang et al., 2009) parsed by the baseline parser.",
  "The DLMs are extracted from a 43 million words English corpus (Charniak, 2000) and a 311 million words corpus of Chinese (Huang et al., 2009) parsed by the baseline parser. Features based on the DLMs are used in the parser. They gained 0.66% UAS for English and an impressive 2.93% for Chinese. Chen et al. (2013) combined the basic \ufb01rst- and second-order features with meta fea- tures based on frequencies. The meta features are extracted from auto-parsed annotations by counting the frequencies of basic feature representations in a large corpus. With the help of meta features, the parser achieved the state-of-the-art accuracy on Chinese. 2.3 Corpora As mentioned previously, one contribution of this thesis is evaluating major semi-supervised techniques in a uni\ufb01ed framework. For our main evaluation, we used English data from the conference on computational natural language learning (Conll) 2009 shared task (Haji\u02c7c et al., 2009) as our source of in-domain evaluation.",
  "For our main evaluation, we used English data from the conference on computational natural language learning (Conll) 2009 shared task (Haji\u02c7c et al., 2009) as our source of in-domain evaluation. For out-of-domain evaluation, we used weblogs portion of OntoNotes v5.01 corpus (Weblogs) and the \ufb01rst workshop on syntac- tic analysis of non-canonical language shared task data (Newsgroups,Reviews,Answers) 1https://catalog.ldc.upenn.edu/LDC2013T19 31",
  "train test Conll Conll Sentences 39,279 2,399 Tokens 958,167 57,676 Avg. Length 24.39 24.04 Table 2.4: The size of the source domain (Conll) training and test sets for our main evaluation corpora. dev test Weblogs Weblogs Newsgroups Reviews Answers Source OntoNotes OntoNotes SANCL SANCL SANCL Sentences 2,150 2,141 1,195 1,906 1,744 Tokens 42,144 40,733 20,651 28,086 28,823 Avg. Length 19.6 19.03 17.28 14.74 16.53 Table 2.5: The size of the target domain test datasets for our main evaluation corpora. (Petrov and McDonald, 2012). Section 2.3.1 introduces our main evaluation corpora in detail. For comparison and multi-lingual evaluation, we also evaluated some of our ap- proaches in various additional corpora.",
  "(Petrov and McDonald, 2012). Section 2.3.1 introduces our main evaluation corpora in detail. For comparison and multi-lingual evaluation, we also evaluated some of our ap- proaches in various additional corpora. Our self-training approach has been evaluated on chemical domain data (Chemical) from the conference on computational natural lan- guage learning 2007 shared task (Nivre et al., 2007a) and nine languages datasets from the workshop on statistical parsing of morphologically rich languages (Spmrl) 2014 shared task(Seddah et al., 2014). Our dependency language models approach has been evaluated in addition on Wall Street Journal portion of Penn English Treebank 3 (Wsj) (P. Marcus et al., 1993) and Chinese Treebank 5 (Ctb) (Xue et al., 2005). As both treebanks do not contain unlabelled data, we used the data of Chelba et al. (2013) and the Xinhua portion of Chinese Gigaword Version 5.0 1 for our English and Chinese tests respectively.",
  "As both treebanks do not contain unlabelled data, we used the data of Chelba et al. (2013) and the Xinhua portion of Chinese Gigaword Version 5.0 1 for our English and Chinese tests respectively. We introduce those corpora in the experiment set-up section of the relevant chapters. 32",
  "unlabelled Weblogs Newsgroups Reviews Answers Sentences 513,687 512,000 512,000 27,274 Tokens 9,882,352 9,373,212 7,622,891 424,299 Avg. Length 19.24 18.31 14.89 15.55 Table 2.6: The size of unlabelled datasets for our main evaluation corpora. 2.3.1 The Main Evaluation Corpora In this section, we introduce our main evaluation corpora that have been used in all of the semi-supervised approaches evaluated in this thesis. The Conll English corpus built on the Penn English Treebank 3 (P. Marcus et al., 1993) which contains mainly Wall Street Journals but also included a small portion of Brown corpus(Francis and Kucera, 1979). The training set contains only Wall Street Journals, the small subset of the Brown corpus has been included in the test set. The constituency trees from Penn English Treebank are converted to dependency representa- tion by the LTH constituent-to-dependency conversion tool, cf. (Johansson and Nugues, 2007).",
  "The constituency trees from Penn English Treebank are converted to dependency representa- tion by the LTH constituent-to-dependency conversion tool, cf. (Johansson and Nugues, 2007). A basic statistic of the corpus can be found in Table 2.4. For our Weblogs domain test we used the Ontonotes v5.01 corpus. The Ontonotes corpus contains various domains of text such as weblogs, broadcasts, talk shows and pivot texts. We used the last 20% of the weblogs portion of the Ontonotes v5.0 corpus as our target domain development set and the main test set. The selected subset allows us to build su\ufb03cient sized datasets similar to the source domain test set. More precisely, the \ufb01rst half of the selected corpus is used as a test set while the second half is used as the development set. Table 2.5 shows some basic statistic of those datasets. Newsgroups, Reviews and Answers domain data are used as additional test sets for our evaluation.",
  "More precisely, the \ufb01rst half of the selected corpus is used as a test set while the second half is used as the development set. Table 2.5 shows some basic statistic of those datasets. Newsgroups, Reviews and Answers domain data are used as additional test sets for our evaluation. Those additional test domains are provided by the \ufb01rst workshop on syntactic analysis of non-canonical language (SANCL) shared task (Petrov and McDonald, 2012). The shared task is focused on the parsing English web text, in total, they prepared 1https://catalog.ldc.upenn.edu/LDC2011T13 1https://catalog.ldc.upenn.edu/LDC2013T19 33",
  "\ufb01ve web domain datasets, two of them are development datasets (Email, Weblogs) and the other three (Newsgroups, Reviews and Answers) are used as test sets. For each of the domains, a small labelled set and a large unlabelled set are provided. In this thesis, we used all three test datasets (both labelled and unlabelled data). In addition, one of the unlabelled texts (Weblogs) from the development portion of the shared task is also used. We used for each domain a similar sized unlabelled dataset to make the evaluation more uni\ufb01ed. The only exception is the answers domain, as its unlabelled dataset is much smaller than the other three domains, thus we used all of the data provided. A basic statistic of the labelled test sets and unlabelled data can be found in Table 2.5 and 2.6 respectively. In term of the dependency representation, we used the LTH conversion for our main evaluation corpora.",
  "A basic statistic of the labelled test sets and unlabelled data can be found in Table 2.5 and 2.6 respectively. In term of the dependency representation, we used the LTH conversion for our main evaluation corpora. Same as the CoNLL 2009 shared task we converted all the labelled data from constituent trees to dependency representation by the LTH constituent-to- dependency conversion tool (Johansson and Nugues, 2007) when needed. 2.4 Evaluation Methods To measure the parser\u2019s performance, we report labelled attachment scores (LAS) and unlabelled attachment scores (UAS). For our evaluation on the main corpora, we use the o\ufb03cial evaluation script of the CoNLL 2009 shared task, in which all punctuation marks are included in the evaluation. The LAS and UAS are the standard ways to evaluate the accuracy of a dependency parser. Due to the single-head property of the dependency trees, the dependency parsing can be seen as a tagging task, thus the single accuracy metric is well suited for the evaluation. Both LAS and UAS measure the accuracy by calculating the percentage of the dependency edges that have been correctly attached.",
  "Due to the single-head property of the dependency trees, the dependency parsing can be seen as a tagging task, thus the single accuracy metric is well suited for the evaluation. Both LAS and UAS measure the accuracy by calculating the percentage of the dependency edges that have been correctly attached. The UAS considers an edge is correct if the attachment is correct, it does not take the label into account, while the LAS counts only the edges that are both correctly attached and the correct label also assigned. The LAS is more strict than UAS thus we mainly focus on 34",
  "LAS in our evaluation. Let Ca be the number of edges that are correctly attached, Ca+l be the number of edges that are both correctly attached and have the correct label, Ct be the total number of edges, we compute: Unlabelled attachment score (UAS) = Ca / Ct (2.1) Labelled attachment score (LAS) = Ca+l / Ct (2.2) For signi\ufb01cance testing, we use the randomised parsing evaluation comparator from a major shared task on dependency parsing (Nivre et al., 2007a) . The script takes predictions annotated by two di\ufb00erent models of the same dataset. Let the \ufb01rst input be the one which has a higher overall accuracy. The null hypothesis of the script is that the accuracy di\ufb00erence between the \ufb01rst input and the second input is not statistically signi\ufb01cant. And the p-values represent the probability that the null hypothesis is correct.",
  "The null hypothesis of the script is that the accuracy di\ufb00erence between the \ufb01rst input and the second input is not statistically signi\ufb01cant. And the p-values represent the probability that the null hypothesis is correct. We use the script\u2019s default setting of 10,000 iterations (itotal), for each iteration, the comparator randomly selects one sentence from the dataset and compares the accuracies of the sentence predicted in the two di\ufb00erent inputs. Let iless be the number of randomly selected instances that are predicted less accurately in the \ufb01rst input when compared to the predictions in the second input. The p-value is calculated by: p = iless itotal We mark the signi\ufb01cance levels based on their p-values, * for p < 0.05, ** for p < 0.01. 2.5 Analysis Techniques To understand the behaviour of our methods, we assess our results on a number of tests. We analyse the results on both token level and sentences level. For token level, we focus on the accuracies of individual syntactic labels and the known/unknown words accuracies.",
  "2.5 Analysis Techniques To understand the behaviour of our methods, we assess our results on a number of tests. We analyse the results on both token level and sentences level. For token level, we focus on the accuracies of individual syntactic labels and the known/unknown words accuracies. For sentence level, we used the methods from McClosky et al. (2006a) to evaluate sentences 35",
  "NMOD P PMOD SBJ OBJ ROOT ADV NAME VC COORD TMP DEP CONJ LOC \u22120.5 0 0.5 1 1.5 Accuracy Change (%) Recall Precision F-score Figure 2.6: The bar chart used to visualise our analysis on individual labels. in four factors. We used all four factors from their analysis, i.e. sentence length, the number of unknown words, the number of prepositions and number of conjunctions. Token Level Analysis. Our token level analysis consists of two tests, the \ufb01rst test assesses the accuracy changes for individual labels. The goal of this test is to \ufb01nd out the e\ufb00ects of our semi-supervised methods on di\ufb00erent labels. For an individual label, we calculate the recall, precision and the f-score. Let PL be the number of the label L predicted by the parser, GL be the count of label L presented in the gold data and PGL be the number of the label predicted correctly.",
  "For an individual label, we calculate the recall, precision and the f-score. Let PL be the number of the label L predicted by the parser, GL be the count of label L presented in the gold data and PGL be the number of the label predicted correctly. The precision (PreL), recall (RecL) and the f-score (FL) are calculated as follows: PreL = PGL / PL (2.3) RecL = PGL / GL (2.4) FL = 2 \u2217PreL \u2217RecL PreL + RecL (2.5) We compute for each label, the score di\ufb00erences between our enhanced model and 36",
  "0 1 2 3 4 0 20 40 60 80 100 Number of Unknown Words Percentage (%) Better Worse No Change 0 1 2 3 40 1,000 2,000 3,000 Number of Unknown Words Number of Sentences Better Worse No Change No. of Sents Figure 2.7: An example of our sentence level analysis on di\ufb00erent number of unknown words per sentence. the base model. The results for the most frequent labels are visualised by the bar chart. Figure 2.6 is an example of the bar chart we used, the x-axis shows the names of the relevant label, the y-axis shows the accuracy changes in percentage. For each of the labels, we report the accuracy changes of all three scores (recall, precision and f-score), the left (blue) bar represents the recall, the middle (red) bar represents the precision and the right (brown) bar is for f-score. The second test assesses the overall accuracy of known words and unknown words. The unknown words are de\ufb01ned as the words that are not presented in the initial training set.",
  "The second test assesses the overall accuracy of known words and unknown words. The unknown words are de\ufb01ned as the words that are not presented in the initial training set. The initial training set is the one we used to train the base model. To compute the accuracy for known and unknown words, we \ufb01rst assign all the tokens in the dataset into two groups (known and unknown) and then we calculate the labelled and unlabelled accuracies for each of the groups separately. We compare the improvements achieved by our enhanced model on known and unknown words to understand the ability of our model on handling unknown words. Sentence Level Analysis. For our sentence level analysis, we evaluate on four factors (sentence length, the number of unknown words, the number of prepositions and 37",
  "the number of conjunctions) that are known to be problematic in parsing. We use a method similar to McClosky et al. (2006a) in our analysis. For each of the factors, we assign sentences to di\ufb00erent classes according to their property, sentences that have the same property are assigned to the same class. Take unknown words factor as an example, sentences which contain the same number of unknown words are grouped together. For each group, we calculate the percentage of sentences that are improved, worsened or unchanged in accuracy by our enhanced model. The reason for using the percentage instead of the number of sentences that were used by McClosky et al. (2006a) is mainly because the absolute numbers vary greatly both within the factor and between factors, thus is not suitable for comparison. The percentage, on the other hand, can be easily compared. In addition to the above values, we also report the number of the sentences in each class. Figure 2.7 shows an example of our sentence level analysis on the di\ufb00erent number of unknown words per sentence. The x-axis shows the conditions of the classes.",
  "In addition to the above values, we also report the number of the sentences in each class. Figure 2.7 shows an example of our sentence level analysis on the di\ufb00erent number of unknown words per sentence. The x-axis shows the conditions of the classes. In this example, it represents the di\ufb00erent number of unknown words in a single sentence. The y-axis to the left is the percentage and the y-axis to the right is the number of sentences. The blue dashed line represents the percentage of the sentences that are parsed better by our enhanced model, the red dotted line represent the portion that is parsed less accurate, the black dash-dotted line shows the portion of sentences whose accuracy are unchanged. The black solid line is the number of sentences in the individual classes. 2.6 Chapter Summary This chapter introduced the background and the experiment set-up. The \ufb01rst part focused on dependency parsers, it introduced three major types of dependency parsers and gave a detailed introduction of the base parser used in this thesis. The second part discussed the problem caused by parsing out-of-domain text and the techniques that have been used by previous work to solve the problem.",
  "The \ufb01rst part focused on dependency parsers, it introduced three major types of dependency parsers and gave a detailed introduction of the base parser used in this thesis. The second part discussed the problem caused by parsing out-of-domain text and the techniques that have been used by previous work to solve the problem. The third part introduced the corpora we used. The last two parts showed our evaluation methods and analysis techniques. 38",
  "CHAPTER 3 CO-TRAINING In this chapter, we introduce our co-training approach. Co-training is one of the popular semi-supervised techniques that has been applied to many natural language processing tasks, such as named entity recognition (Collins and Singer, 1999), constituency parsing (Sarkar, 2001) and dependency parsing (Sagae and Tsujii, 2007; Petrov and McDonald, 2012). Although co-training approaches are popular, they do not always bring positive e\ufb00ects (Zhang et al., 2012; Weiss et al., 2015). Improvements on results are usually reported by learners that are carefully designed to be as di\ufb00erent as possible. Such as in Sagae and Tsujii (2007)\u2019s approach, they form the co-training with parsers consisting of di\ufb00erent learning algorithms and search strategies. However, o\ufb00-the-shelf parsers use many similar features, the output of these parsers are more likely to agree with each other. Thus it is unclear whether the o\ufb00-the-shelf parsers are suitable for co-training.",
  "However, o\ufb00-the-shelf parsers use many similar features, the output of these parsers are more likely to agree with each other. Thus it is unclear whether the o\ufb00-the-shelf parsers are suitable for co-training. In this work we evaluate co-training with a number of o\ufb00-the-shelf parsers that are freely available to the research community, namely Malt parser (Nivre, 2009), MST parser (McDonald and Pereira, 2006), Mate parser (Bohnet et al., 2013), and Turbo parser (Mar- tins et al., 2010). We evaluate those parsers on agreement based co-training algorithms. The evaluation learner is retrained on the training set that is boosted by automatically annotated sentences agreed by two source learners. We investigate both normal agree- ment based co-training and a variant called tri-training. In a normal co-training setting the evaluation learner is used as one of the source learners, and in a tri-training scenario, the source learners are di\ufb00erent from the evaluation learner. 39",
  "In the following sections we introduce our approaches in Section 3.1. We then introduce our experiment settings and results in Section 3.2 and Section 3.3 respectively. After that, in Section 3.4 we analyse the results and trying to understand how co-training helps. In the last section (Section 3.5), we summarise our \ufb01nding. 3.1 Agreement Based Co-training In this work, we apply an agreement based co-training to out-of-domain dependency parsing. Our agreement based co-training is inspired by the observation from Sagae and Tsujii (2007) in which the two parsers agreeing on an annotation is an indication of a higher accuracy. We proposed two types of agreement based approaches: one uses parser pairs (normal co-training), the other uses three parsers which is also known as tri-training. Two approaches use a similar algorithm, which involves two source learners and one evaluation learner. Two source learners are used to produce additional training data for retraining the evaluation learner. More precisely, our algorithm is as follows: 1. Two source learners are trained separately on the source domain training set to generate two base models. 2.",
  "Two source learners are used to produce additional training data for retraining the evaluation learner. More precisely, our algorithm is as follows: 1. Two source learners are trained separately on the source domain training set to generate two base models. 2. Both models are used to parse a large number of target domain unlabelled data. 3. After that, we compare two automatically labelled predictions for each of the sen- tences, the \ufb01rst N (such as 10k, 20k) predictions that both models agreed are added to the end of the source domain training set. 4. Finally, we retrain the evaluation learner on the boosted training set generated in step 3. Although both approaches share the similar algorithm, the major di\ufb00erences between them are: both parsers involved by normal co-training are used as the source learners, in which one of them is also used as the evaluation learner; by contrast, tri-training uses 40",
  "Malt MST Turbo Mate LAS (Single) 72.63 75.35 74.85 77.54 LAS (Identical) 89.32 89.08 90.48 - Identical rate 19.81 20.32 22.28 - Avg. Length 8.92 9.03 8.96 - Table 3.1: The analysis of identical annotations on Weblogs development set. three parsers in total, in which two of them are used as the source learners and the third one is used as the evaluation learner. In terms of parsers selection, we selected four public available dependency parsers, which include two benchmark parsers (Malt parser (Nivre, 2009) and MST parser (Mc- Donald and Pereira, 2006)), one transition-based Mate parser (Bohnet et al., 2013), and one graph-based Turbo parser (Martins et al., 2010). These parsers have been widely used by researchers. A more detailed discussion of the dependency parser can be found in section 2.1. The agreement based co-training depends on the assumption that identical annota- tions between two learners indicate the correctness.",
  "These parsers have been widely used by researchers. A more detailed discussion of the dependency parser can be found in section 2.1. The agreement based co-training depends on the assumption that identical annota- tions between two learners indicate the correctness. To con\ufb01rm the suitability of selected parsers, in the preliminary evaluation we assessed the accuracy of identical analysis gener- ated by parser pairs. Because we intend to use the Mate parser as our evaluation parser, we paired each of the other three parsers with Mate parser to create three co-training pairs. We assess our assumption by annotating our Weblogs development set, the de- velopment set is parsed by all four parsers. We then extract the identical annotations (whole sentence) from parser pairs. We show the accuracy of individual parsers and the accuracy of identical annotations in Table 3.1. The second row shows the labelled accu- racy of each parser on the Weblogs development set. The third row shows the labelled accuracy of the identical annotations between the named parser and Mate parser. The fourth row shows the agreement rate of the parser pairs. The last row shows the average sentence length of the identical annotations.",
  "The third row shows the labelled accuracy of the identical annotations between the named parser and Mate parser. The fourth row shows the agreement rate of the parser pairs. The last row shows the average sentence length of the identical annotations. As we can see from the table, our assump- tion is correct on all the parser pairs. Actually, when they agreed on the annotations, the accuracies can be 16% higher than that of individual parsers. However, we also noticed 41",
  "that the average sentence length of the identical annotations is in stark contrast with that of the entire development set (19.6 tokens/sentence). We will discuss this potential con\ufb02ict in the later section. 3.2 Experiment Set-up In our evaluation on co-training we use our main evaluation corpora that consists of a source domain training set (Conll), a Weblogs domain development set, a in-domain test set (Conll) and four out-of-domain test sets (Weblogs, Newsgroups, Reviews and Answers). For each target domains, we used in addition a large unlabelled dataset to supply the additional training set. We evaluate various di\ufb00erent settings on the devel- opment set to tune the best con\ufb01guration, after that, we apply the best setting to all the test domains. As mentioned before, we used four parsers in our experiments. cf. the Malt parser (Nivre, 2009), MST parser (McDonald and Pereira, 2006), Mate parser (Bohnet et al., 2013), and the Turbo parser (Martins et al., 2010).",
  "cf. the Malt parser (Nivre, 2009), MST parser (McDonald and Pereira, 2006), Mate parser (Bohnet et al., 2013), and the Turbo parser (Martins et al., 2010). We use the default settings for all the parsers. The part-of-speech tags is annotated by Mate parser\u2019s internal tagger. To create the additional training corpus, the unlabelled datasets are annotated by all the parsers which are trained on the Conll source domain training set. The Mate parser is used as our evaluation learner, the baseline for all the domains are generated by Mate parser trained on the same Conll training set and applied directly to target domains. We mainly report the labelled attachment scores (LAS), but also include the unlabelled attachment scores (UAS) for our evaluations on test sets. We mark the signi\ufb01cance levels according to the p-values, * indicates signi\ufb01cance at the p < 0.05 level, ** for the p < 0.01 level. 42",
  "3.3 Empirical Results Agreement based co-training. We \ufb01rst evaluate the parser pairs on the normal agree- ment based co-training. Each of the other three parsers is paired with Mate parser to be the source learners of our co-training. For each pairwise parser combinations, the unlabelled Weblogs text is double parsed by the parser pairs. The sentences that are annotated identically by both parsers are used as candidates for the additional training set. We take di\ufb00erent amount of additional training sentences from the candidates pool to retrain the Mate parser. Figure 3.1 shows the co-training results of adding 10k to 30k additional training data for all three parser pairs. As we can see from the \ufb01gure, all the co-training results achieved improvements when compared with the Mate baseline. The largest improvement of one percentage point is achieved by Mate-Malt parser pair when adding 20k or 30k additional training data. We also notice a negative correlation between the improvement and the identical rate mentioned previously in Table 3.1.",
  "The largest improvement of one percentage point is achieved by Mate-Malt parser pair when adding 20k or 30k additional training data. We also notice a negative correlation between the improvement and the identical rate mentioned previously in Table 3.1. The Turbo parser has the highest identical rate, in which it annotated 479 out of 2150 sentences (22.28%) exactly the same as Mate parser when evaluated on the development set. This is 2% higher than that of MST parser and 2.5% higher than the Malt parser. However, the improvements achieved by the pairs are shown to be negatively correlated, i.e. the Mate-Malt pair gains the largest improvement, the Mate-Turbo pair achieved the lowest gain. This \ufb01nding is in-line with the fundamental of co-training that requires the learners to be as di\ufb00erent as possible. Removing short sentences from identical data. The identical annotations be- tween the parsers are like a double-edged sword, they consist of a higher accuracy but in the same time shorter in average sentence length.",
  "Removing short sentences from identical data. The identical annotations be- tween the parsers are like a double-edged sword, they consist of a higher accuracy but in the same time shorter in average sentence length. Take our Mate-Malt pair as an example, the average sentence length of the identical annotations is only 8 tokens, this is much lower than the development set\u2019s 19.6 tokens/sentence and the Conll training set\u2019s 24.4 tokens/sentence. To make the additional training data more similar to the manually annotated data, we exclude the extremely short sentences from the pool. More precisely 43",
  "10 20 30 77 78 79 80 Additional Training Data (K) Labeled Attachment Score (%) Mate-Malt Mate-MST Mate-Turbo Mate Baseline Figure 3.1: The results of our normal agreement-based co-training with three di\ufb00erent parser pairs. LAS (Identical) Avg. Length Identical Sentences >6 tokens 89.44 13.1 248 >5 tokens 89.29 12.67 278 >4 tokens 89.19 11.94 311 All sentences 89.32 8.35 426 Table 3.2: The quantity and quality (LAS) of identical (Mate-Malt) development set sentences when omitting the short sentences. we set three minimal sentence length thresholds (4, 5 and 6 tokens), sentences shorter than the thresholds are removed from the pool. We then take 30k sentences from the remaining pool as the additional training data. By taking out the short sentences the av- erage sentence length of the selected sentences is closer to that of the development set. As shown in Table 3.2, the average sentence length reached 13 tokens/sentence.",
  "By taking out the short sentences the av- erage sentence length of the selected sentences is closer to that of the development set. As shown in Table 3.2, the average sentence length reached 13 tokens/sentence. One of the major concerns when we exclude the short sentences from the pool is that the accuracy of the remaining pool might drop. The short sentences are easier to parse, thus they usually have a higher accuracy. However, an evaluation on the development set shows that there is almost no e\ufb00ect on the accuracies (see Table 3.2). In term of the results, we gained a 0.27% additional improvement when discarding short sentences (Figure 3.2). 44",
  "4 5 6 78 78.5 79 79.5 80 Mininum Sentence Length Labeled Attachment Score (%) Exclude Short Sentences All Sentences Figure 3.2: The e\ufb00ect of omitting short sentences from additional training data. Three learners co-training. In the normal co-training setting, the Mate parser is used as one of the source learners to provide additional training data for retraining itself. Based on this setting the Mate parser can learn only from the annotations it has already known. The tri-training algorithm is on the other hand designed to allow the evaluation learner to learn from sources other than itself. This gives the Mate parser the potential to explore novel examples from other parsers. In our tri-training experiments, we used the Malt parser and the MST parser as our source learners. The sentences that are annotated identically by these parsers are added to the pool for retraining the Mate parser. To assess the quality of the identical annotations between Malt and MST parsers we apply them to our development set.",
  "The sentences that are annotated identically by these parsers are added to the pool for retraining the Mate parser. To assess the quality of the identical annotations between Malt and MST parsers we apply them to our development set. We also assessed the sentences that are annotated identically by Malt and MST parsers but di\ufb00erent to Mate parser\u2019s annotation, this allows us to know the scale of the novel examples. As shown in Table 3.3, the accuracy of the sentences agreed by Malt and MST parsers is even slightly higher than that of Mate and Malt parsers, this is surprising as MST parser is less accurate than Mate parser. The analysis also showed that half of the identical annotations from Malt and MST parsers are actually novel to Mate parser. We compared our tri-training and co-training results in Figure 3.3, the tri-training results constantly outperform the normal co-training. The best result of 45",
  "LAS (Identical) Identical Sentences Mate-Malt 89.44 248 Malt-MST 90.20 300 Malt-MST excl. Mate 89.28 147 Table 3.3: The quantity and quality (LAS) of identical development set sentences agreed by di\ufb00erent parser pairs. 10 20 30 40 77 78 79 80 Additional Training Data (K) Labeled Attachment Score (%) Tri-training Normal Co-training Mate Baseline Figure 3.3: The results of our tri-training compared with normal co-training. 79.12% is achieved by retraining the Mate parser with 20k additional training data agreed by Malt-MST parsers (tri-training). The best tri-training result is 0.24% higher than that of co-training and nearly 1.6% higher than the Mate baseline. Evaluating on test domains. We then evaluated our best con\ufb01guration (tri- training) on our four test domains.",
  "The best tri-training result is 0.24% higher than that of co-training and nearly 1.6% higher than the Mate baseline. Evaluating on test domains. We then evaluated our best con\ufb01guration (tri- training) on our four test domains. Under the tri-training setting, the unlabelled datasets of each domain are double parsed by Malt-MST pairs, the \ufb01rst 20k identical annotations are used as additional training data to retrain the Mate parser. The only exception is for answers domain. Due to the lack of unlabelled data the additional training data is much smaller, we used all 3k identical sentences for retraining. Table 3.4 shows our tri-training results accompanied by the baselines. The tri-training setting achieved large labelled im- provements up to 1.8 percentage points. For unlabelled attachment scores, the models gained up to 0.59% absolute improvements. We also tested the retrained Weblogs do- 46",
  "Tri-training Baseline LAS UAS LAS UAS Weblogs 80.59** 85.61** 78.99 85.1 Newsgroups 76.44** 83.13 75.3 82.88 Reviews 76.87** 83.27** 75.07 82.68 Answers 74.59** 81.58 73.08 81.15 Conll 90.16 92.47 90.07 92.4 Table 3.4: The e\ufb00ect of applying the best con\ufb01guration (tri-training) to our test domains. main model on the in-domain test set. The results show the tri-trained model does not a\ufb00ect the in-domain accuracy. 3.4 Analysis From the above experiments, we demonstrated the e\ufb00ect of co-/tri-training on parsing out-of-domain text with the o\ufb00-the-shelf parsers. It remains unclear how the additional training data helps the target domain parsing. To understand where the improvements come from, in this section we give a detailed study on the results.",
  "It remains unclear how the additional training data helps the target domain parsing. To understand where the improvements come from, in this section we give a detailed study on the results. We compare the annotations produced by our tri-training approach and the baseline and evaluate the changes on both token level and sentence level. For our analysis, we treat all the target domain as the same, the Weblogs, Newsgroups, Reviews and Answers domain test sets are used as a single set. 3.4.1 Token Level Analysis Individual Label Accuracy. We \ufb01rst compared the individual label accuracies of the tri-trained model and the baseline. For each of the label we calculate recalls, precisions and f-scores, we then compute the score di\ufb00erences between the tri-trained model and the baseline model. Table 3.4 shows the score changes of the most frequent labels. All the f-scores of our tri-trained model outperform the baseline, the only exception is the P (punctuations) which drops slightly by 0.1%.",
  "Table 3.4 shows the score changes of the most frequent labels. All the f-scores of our tri-trained model outperform the baseline, the only exception is the P (punctuations) which drops slightly by 0.1%. Eight labels achieved around 0.5% improve- ments which include ROOT (root of the sentence), SBJ (subject), COORD (coordination), 47",
  "Confusion Baseline tri-training NMOD \u2192ADV 235 229 NMOD \u2192LOC 162 164 NMOD \u2192HYPH 198 196 NMOD \u2192NAME 569 583 NMOD \u2192PMOD 187 179 NMOD \u2192HMOD 217 213 NMOD \u2192ROOT,OBJ,SBJ,DEP 491 442 P \u2192HYPH 162 173 P \u2192NAME,NMOD 233 245 SBJ \u2192NMOD 169 157 SBJ \u2192OBJ 132 72 OBJ \u2192NMOD 218 202 OBJ \u2192SBJ 117 89 PMOD \u2192NMOD 290 275 PMOD \u2192OBJ 122 92 ROOT \u2192NMOD 235 235 ROOT \u2192OBJ,",
  "SBJ 256 216 ADV \u2192MNR 150 129 ADV \u2192AMOD 152 134 ADV \u2192LOC 227 214 ADV \u2192NMOD 382 373 ADV \u2192TMP 182 195 ADV \u2192DIR 118 113 COORD \u2192NMOD 164 140 COORD \u2192ROOT 102 94 VC \u2192OPRD 114 23 CONJ \u2192NMOD 132 130 DEP \u2192ROOT 190 199 DEP \u2192OBJ 267 241 DEP \u2192SBJ 403 420 DEP \u2192NMOD 382 396 DEP \u2192TMP 176 165 DEP \u2192ADV 142 133 AMOD \u2192ADV 169 157 AMOD \u2192NMOD 265 273 AMOD \u2192HYPH 104 105 TMP \u2192ADV 280 268 TMP \u2192NMOD 133 128 PRD \u2192OBJ 854 97 PRD \u2192ADV,VC 255 184 Table 3.5: The confusion matrix of dependency labels, compared between the tri-training approach and the baseline. 48",
  "NMOD P SBJ PMOD ROOT ADV COORD VC CONJ DEP AMOD TMP OBJ PRD 0 1 2 3 4 Accuracy Change (%) Recall Precision F-score 0 10 20 30 40 Accuracy Change (%) Recall Precision F-score Figure 3.4: The performance comparison between the tri-training approach and the baseline on major labels. The x-axis shows the labels, the y-axis to the left shows the accuracy changes for labels from start to TMP, the y-axis to the right-hand side is for label OBJ and PRD only. CONJ (conjunct), modi\ufb01ers (NMOD (modi\ufb01er of nominal), PMOD (modi\ufb01er of prepo- sition), AMOD (modi\ufb01er of adjective or adverbial)) and DEP (unclassi\ufb01ed relations). ADV (adverbial), VC (verb chain) and TMP (temporal adverbial or nominal modi\ufb01er) are labels that have improvements between 1% and 2%. The accuracy changes are much larger for label OBJ and PRD, thus we used a secondary y-axis for them.",
  "The accuracy changes are much larger for label OBJ and PRD, thus we used a secondary y-axis for them. More precisely, an improvement of 5.9% is found on OBJ (object), a much better precision of 10% sug- gests this improvement is mainly contributed by the reduced false positive. The largest improvement of 15% comes from label PRD (predicative complement), the improvement is as a result of signi\ufb01cant recall change. The baseline parser can only recall 43% of the label, it has been improved signi\ufb01cantly (34%) by the tri-trained model. Table 3.5 shows the confusion matrix of dependency labels. As we can see from the table, the PRD has been frequently labeled as OBJ by the baseline, but this has been largely corrected by our tri-training model. 49",
  "Tri-training Baseline Tokens LAS UAS LAS UAS Known 101616 78.7 84.5 77.1 84.1 Unknown 6055 63.2 72.6 61.4 71.9 All 107671 77.8 83.8 76.3 83.4 Table 3.6: The accuracy comparison between the tri-training approach and the baseline on unknown words. Unknown Words Accuracy. We then evaluate unknown words at the token level, by comparing the labelled and unlabelled accuracy scores between words that presented in the source domain training data (Known) and words that are unseen from training sets (Unknown). We present the accuracy comparison of known/unknown words together with that of all tokens in Table 3.6. The tri-trained model achieved better gains on unknown words for both labelled and unlabelled accuracies. The labelled gains of the tri-trained model on unknown words are 1.8%, which is 0.2% higher than that of known words (1.6%). The unlabelled improvements on unknown words (0.7%) is 0.3% higher than known words (0.4%).",
  "The unlabelled improvements on unknown words (0.7%) is 0.3% higher than known words (0.4%). Although the absolute gains for unknown words are larger, the performance of known words is still better in terms of the error reduction rate. For known words, tri-trained model reduced 7% errors on labelled accuracy and this is 2.4% better than that of unknown words. The error reduction for unlabelled accuracy is the same (2.5%) for both unknown and known words. 3.4.2 Sentence Level Analysis We then carry out our sentence level analysis, the sentence level analysis use sentences as a whole, all the tokens in the same sentences are always put into the same class. In total, we analysis four di\ufb00erent sentences factors, our goal is to have a more clear picture about the improvements of di\ufb00erent type of sentences. Sentence Length. Figure 3.5 shows the performance changes for sentences of di\ufb00er- ent length, the results of the tri-trained model is compared with the baseline.",
  "Sentence Length. Figure 3.5 shows the performance changes for sentences of di\ufb00er- ent length, the results of the tri-trained model is compared with the baseline. As we can see from the \ufb01gure, the percentage of sentences that remain the same accuracies contin- 50",
  "0 10 20 30 40 0 20 40 60 80 100 Number of Tokens Percentage (%) Better Worse No Change 0 10 20 30 400 100 200 300 Number of Tokens Number of Sentences Better Worse No Change No. of Sents Figure 3.5: The comparison between the tri-training approach and the baseline on di\ufb00erent number of tokens per sentence. uously decrease when the sentence length increases. We suggest this is mainly because longer sentences are harder to parse, thus are less likely to have the same accuracy. The rate of sentences parsed better is constantly larger than that of parsed worse. The gaps widened when the sentence length increases until reached the widest point at a length of 30, after that the gap narrowed and become very close at 40 tokens. However, there are only less than 200 sentences in the classes which have a sentence length of more than 35, thus the results of those classes become less reliable. Overall, the analysis suggests the major improvements are contributed by sentences that have a length between 15 and 30 tokens. Unknown Words.",
  "However, there are only less than 200 sentences in the classes which have a sentence length of more than 35, thus the results of those classes become less reliable. Overall, the analysis suggests the major improvements are contributed by sentences that have a length between 15 and 30 tokens. Unknown Words. Unknown words are hard to parse as the model trained on training data do not have su\ufb03cient information to annotate those words. Thus a large number of unknown words in a sentence usually results in a poor accuracy. We group sentences that have the same number of unknown words and then apply our analysis method to each class. We noted that 50% of the sentences do not contain unknown words, 30% of them contain one unseen word, 12% of which contain 2 such words, the rest 8% contain 3 or 4 unknown words. For the sentences that do not contain unknown words, about 60% of 51",
  "0 1 2 3 4 0 20 40 60 80 100 Number of Unknown Words Percentage (%) Better Worse No Change 0 1 2 3 40 1,000 2,000 3,000 Number of Unknown Words Number of Sentences Better Worse No Change No. of Sents Figure 3.6: The comparison between the tri-training approach and the baseline on di\ufb00erent number of unknown words per sentence. them remain the same accuracy, 25% of them have a higher accuracy and 15% of them are pared worse. This gap widened slowly until 3 unknown words per sentence, after that the gap narrowed for sentences have 4 unknown words. Overall, the gains on sentences with unknown words are slightly better than that of sentences contain only known words. This is in line with our \ufb01nding in the token level analysis. Prepositions. The attachment of prepositions is one of the complex problems that are di\ufb03cult for parsing. It can be found even harder when going out-of-domain, as their behaviour might change. To address those changes we looked at the labels assigned to the prepositions.",
  "Prepositions. The attachment of prepositions is one of the complex problems that are di\ufb03cult for parsing. It can be found even harder when going out-of-domain, as their behaviour might change. To address those changes we looked at the labels assigned to the prepositions. For both source and target domain we \ufb01nd NMOD (Modi\ufb01er of nominal), ADV (General adverbial), LOC (Locative adverbial or nominal modi\ufb01er) and TMP (Temporal adverbial or nominal modi\ufb01er) are the most frequently assigned labels, those labels covering 80% of the total prepositions. However, the percentages for the source domain and the target domain are very di\ufb00erent. In the source domain 35% of the prepositions are labelled as NMOD and 19% of them are labelled as ADV, while, in the target domain, the rate for NMOD and ADV are very close, both labels contribute around 28%. In terms of our sentence level analysis on the number of prepositions, 52",
  "0 1 2 3 4 5 6 0 20 40 60 80 100 Number of Prepositions Percentage (%) Better Worse No Change 0 1 2 3 4 5 60 500 1,000 1,500 2,000 Number of Prepositions Number of Sentences Better Worse No Change No. of Sents Figure 3.7: The comparison between the tri-training approach and the baseline on di\ufb00erent number of prepositions per sentence. Figure 3.7 illustrates the performance changes when the number of prepositions increases in sentences. The percentages of sentences parsed better and worse increased smoothly when the number of preposition increases, the tri-training gains at least 10% for all the cases. Generally speaking, tri-training works better for sentences that have prepositions, the average gain for sentences that have prepositions is 15% and this is 5% more than that of sentences that do not have a proposition. Conjunctions. The annotation of conjunctions is another well-known problem for parsing. More conjunction usually results in a longer sentence and are more complex as well.",
  "Conjunctions. The annotation of conjunctions is another well-known problem for parsing. More conjunction usually results in a longer sentence and are more complex as well. Figure 3.8 shows the analysis on conjunctions. The \ufb01gure is similar to that of prepositions, the tri-training model gained more than 11% for all the classes and have higher gains for sentences containing conjunctions. Example Sentences. Table 3.7 shows some example sentences that have been im- proved largely by our tri-training approach. 53",
  "If 1 p 11adv you2 3sbj come3 1sub upon 4 p 3adv something5 4pmod important6 5appo , 7 11p by 8 p 11adv all9 10nmod means 10 8pmod make 11 0root a12 13nmod note13 11obj of 14 p 13nmod it15 14pmod , 16 11p and 17 c 11coord so18 11adv on19 18amod .",
  "20 11p But 1 c 31dep creating 2 31sbj a3 5nmod balanced4 5nmod community5 2obj with 6 p 5nmod a7 8nmod mix8 6pmod of 9 p 8nmod housing10 9pmod ,11 10p o\ufb03ces12 10coord ,13 12p shopping14 12coord and 15 c 14coord other16 17nmod amenities17 15conj \u201318 5p allowing19 5appo people20 19obj to21 19oprd live22 21im close23 22loc to 24 p 23amod where 25 27loc they26 27sbj work27 24pmod and 28 c 27coord play29 28conj \u201330 5p is 31 0root an32 36nmod even 33 35amod more 34 35amod worthy 35 36nmod goal 36 31prd . 37 31p In 1 p 5adv some2 3nmod respects3 1pmod ,",
  "37 31p In 1 p 5adv some2 3nmod respects3 1pmod ,4 5p is5 0root n\u2019t6 5adv that7 5sbj essentially8 5adv what9 14obj No 10 11name Va11 12nmod jursidictions 12 u 13sbj are 13 5prd doing 14 13vc -15 14p favoring 16 14adv non-residential17 18nmod development 18 16obj and 19 c 16coord letting20 19conj other21 22nmod jursidictions 22 u 20obj handle 23 20oprd the24 25nmod residential25 23obj ?26 5p Her 1 4nmod \u201c2 4p Rubble3 4name Division 4 6sbj \u201d5 4p mixes 6 0root such7 9nmod disparate8 9nmod materials 9 6obj as 10 p 9nmod ink11 13nmod -12 13nmod jet13 14nmod prints 14 10pmod pasted 15 u 14appo on 16 p 15loc board17 16pmod ,",
  "18 14p foam19 20nmod rubber20 14coord ,21 20p galvanized22 23nmod steel23 20coord ,24 23p concrete25 23coord ,26 25p steel27 28nmod rebar 28 u 25coord and 29 c 28coord bungee 30 u 31nmod cords 31 u 29conj . 32 6p they1 2sbj were2 0root convinced 3 2prd that 4 p 3amod if 5 p 20adv only6 8adv they7 8sbj could 8 5sub speak9 8vc to 10 p 9adv an11 12nmod American12 10pmod , 13 20p Abather 14 u 19nmod \u2019s15 14suffix charred16 19nmod and 17 c 16coord mangled 18 u 17conj \ufb02esh19 20sbj would 20 4sub make21 20vc their22 23nmod case23 21obj ,",
  "24 2p but 25 c 2coord they26 27sbj had27 25conj never28 27tmp gotten29 27vc past 30 p 29adv the31 34nmod Jordanian 32 u 34nmod security33 34nmod guards34 30pmod .35 2p and 1 c 3dep i2 3sbj promise 3 0root to 4 3oprd fess 5 u 4im up6 5prt eventually7 5tmp and 8 c 5coord tell9 8conj of 10 p 9adv at 11 p 13dep least12 11amod one 13 15nmod such 14 15nmod epic 15 10pmod i 16 17sbj survived 17 15nmod \u2013 18 3p - 1 3p Dr.2 3title Seuss 3 u 0root , 4 3p \u201c5 3p One6 7nmod Fish 7 3coord ,8 7p Two 9 10nmod Fish 10 7coord ,",
  "2 3title Seuss 3 u 0root , 4 3p \u201c5 3p One6 7nmod Fish 7 3coord ,8 7p Two 9 10nmod Fish 10 7coord ,11 10p Red12 13nmod Fish 13 10coord , 14 13p Blue15 16nmod Fish 16 13coord \u201d 17 3p or 1 c 19dep , 2 19p as 3 p 19adv warren 4 5name harding 5 u 7sbj once 6 7tmp said 7 3sub :8 19p \u201c9 19p At 10 p 19loc either11 12nmod end12 10pmod of 13 p 12nmod the14 16nmod social15 16nmod spectrum16 13pmod ,17 19p there18 19loc lies 19 0root a20 22nmod leisure21 22nmod class22 19sbj .",
  "17 19p there18 19loc lies 19 0root a20 22nmod leisure21 22nmod class22 19sbj . 23 19p \u201d 24 19p when 1 12tmp the2 3nmod guy 3 12sbj ( 4 9p the5 6nmod owner 6 9dep ,7 9p it8 9sbj turned 9 3prn out10 9prt )11 9p arrived 12 20tmp to 13 12prp open14 13im the15 17nmod gas16 17nmod station17 14obj , 18 20p he19 20sbj took 20 0root one21 22nmod look22 20obj at 23 p 20adv our24 26nmod cow 25 u 26nmod pie26 23pmod with 27 p 26nmod wheels28 27pmod and 29 c 20coord said30 29conj \u201c31 30p what 32 34nmod the33 34nmod fook 34 u 30obj ?",
  "35 20p \u201d 36 20p SO 1 14adv , 2 14p IF 3 p 14adv YOU4 5sbj WANT 5 3sub A6 7nmod BURGER7 5obj AND 8 c 7coord FRIES 9 u 8conj , 10 14p WELL 11 14dep , 12 14p IT 13 14sbj IS 14 0root OK 15 14prd . 16 14p Table 3.7: The example sentences that have been improved by the tri-training approach when compared to the baseline. In which the dependency head/relation of a token are marked as the subscript, while the superscript is the index of token. The unknown words, prepositions and conjunctions are highlighted with u , p and c respectively. We highlight the di\ufb00erent levels of the improvements achieved by our tri-training model on the dependency edges by di\ufb00erent colours. In which the blue colour means both head and label are corrected, the yellow colour means only the head is corrected and the green colour means only the label is corrected. 54",
  "0 1 2 3 0 20 40 60 80 100 Number of Conjunctions Percentage (%) Better Worse No Change 0 1 2 30 1,000 2,000 3,000 4,000 Number of Conjunctions Number of Sentences Better Worse No Change No. of Sents Figure 3.8: The comparison between the tri-training approach and the baseline on di\ufb00erent number of conjunctions per sentence. 3.5 Chapter Summary In this chapter we present our evaluations on two co-training approaches (co-training and tri-training). The main contribution of our evaluation on co-training is to assess the suit- ability of using the o\ufb00-the-shelf parsers to form co-training. We \ufb01rst evaluated on the normal agreement based co-training with four o\ufb00-the-shelf parsers. Three of them are paired with the Mate parser to generate additional training data for retraining the Mate parser. We evaluated the parser pairs by adding di\ufb00erent number of sentences into the training data. We also evaluated the pairs with additional training data that excluded the short annotations.",
  "Three of them are paired with the Mate parser to generate additional training data for retraining the Mate parser. We evaluated the parser pairs by adding di\ufb00erent number of sentences into the training data. We also evaluated the pairs with additional training data that excluded the short annotations. The results show co-training is able to improve largely on target domain and additional gains are achieved when excluding the short sentences. We then evaluated the second approach (tri-training) that retrains the Mate parser on additional training data annotated identically by MST-Malt parsers. Bene\ufb01t from the novel annota- tions that not predicted by the Mate parser, tri-training outperforms our best co-training setting. The further evaluation on tri-training shows large improvements on all four test domains. The method achieved the largest improvement of 1.8% and 0.6% for labelled 55",
  "and unlabelled accuracies. We then applied both token level and sentence level analysis to \ufb01nd out where the improvement comes from. The analysis suggests tri-training gained particularly large improvement on label OBJ (objects) and PRD (predicative comple- ment). The analysis of unknown words on both token level and sentence level shows only a slightly larger improvement on unknown words when compared with known words. The analysis on sentence length suggests tri-training helped mainly on sentences with a length between 15 and 30 tokens. The analysis on prepositions and conjunctions shows larger gains are achieved on sentences containing prepositions or conjunctions. Overall we demonstrated that co-/tri-training are powerful techniques for out-of-domain parsing when the o\ufb00-the-shelf parsers are used. 56",
  "CHAPTER 4 SELF-TRAINING In this chapter, we introduce our self-training approach for English out-of-domain text. Self-training is one of the semi-supervised techniques that improves the learner\u2019s perfor- mance by its own annotations. Taking parsing as an example, a basic self-training iteration usually consists of three steps: \ufb01rstly a base model is trained on the original manually annotated training data, then the base model is used to annotate unlabelled sentences (usually much larger than the original training set), \ufb01nally the parser is retrained on the new training set, which consists of both manually and automatically annotated data. The self-training iteration can also be repeated to conduct a multi-iteration approach. Self-training has been adapted \ufb01rst to constituency parsers and achieved reasonably good gains for both in- and out-of-domain parsing (McClosky et al., 2006b; McClosky et al., 2006a; Reichart and Rappoport, 2007; Sagae, 2010; Petrov and McDonald, 2012).",
  "While self-training approaches for dependency parsing are less successful, the evaluations usu- ally found no impact or even negative e\ufb00ects on accuracy (Plank and van Noord, 2011; Plank and S\u00f8gaard, 2013; Cerisara, 2014; Bj\u00a8orkelund et al., 2014). There are only a few successful self-training approaches reported on the dependency parsing, but those approaches are usually more complex than the basic self-training iterations. Kawahara and Uchimoto (2008)\u2019s approach needs a separately trained classi\ufb01er to select additional training data, Chen et al. (2008) used only partial parse trees and Goutam and Ambati (2011)\u2019s approach conditions on a small initial training set. In this work, we introduce a novel con\ufb01dence-based self-training approach to out- 57",
  "of-domain dependency parsing. Our approach uses con\ufb01dence-based methods to select training sentences for self-training. The con\ufb01dence scores are generated during the parsing thus we do not need to train a separate classi\ufb01er. Our self-training approach employs a single basic self-training iteration, except for the second step we add only sentences that have higher con\ufb01dence scores to the training set. Overall, we present a simple but e\ufb00ective con\ufb01dence-based self-training approach for English out-of-domain dependency parsing. We compare two con\ufb01dence-based methods to select training data for our self-training. We evaluate our approaches on the main evaluation corpora as well as the Chemical domain text from the domain adaptation track of CoNLL 2007 shared task. The remaining parts of this chapter are organised as follows. Section 4.1 shows the detail of our self-training approaches. Section 4.2 introduces the experiment set-up of our evaluation. We then discuss and analyse the results in Section 4.3 and 4.4 respectively. The last section (Section 4.5) summarises the chapter.",
  "Section 4.2 introduces the experiment set-up of our evaluation. We then discuss and analyse the results in Section 4.3 and 4.4 respectively. The last section (Section 4.5) summarises the chapter. 4.1 Con\ufb01dence-based Self-training The con\ufb01dence-based self-training approach is inspired by the successful use of the high- quality dependency trees in our agreement based co-training and the correlation between the prediction quality and the con\ufb01dence-based methods (Dredze et al., 2008; Crammer et al., 2009; Mejer and Crammer, 2012). The con\ufb01dence-besed methods were previously used by Mejer and Crammer (2012) to assess the parsing quality of a graph-based parser, but they haven\u2019t been used in self-training or transition-based parser before this work. Based on our experience on co-training and the results of the previous work on self-training, we believe the selection of high-quality dependency trees is a crucial precondition for the successful application of self-training to dependency parsing.",
  "Based on our experience on co-training and the results of the previous work on self-training, we believe the selection of high-quality dependency trees is a crucial precondition for the successful application of self-training to dependency parsing. Therefore, we explore two con\ufb01dence-based methods to select such dependency trees from newly parsed sentences. More precisely, our self-training approach consists of the following steps: 1. A parser is trained on the source domain training set in order to generate a base 58",
  "model. 2. A large number of unlabelled sentences from a target domain is annotated by the base model. 3. The newly parsed sentences which have a high con\ufb01dence score are added to the source domain training set as additional training data. 4. The parser is then retrained on the new training set in order to produce a self-trained model. 5. Finally, we evaluate the target domain data by the self-trained model. We test two methods to gain con\ufb01dence scores for a dependency tree. The \ufb01rst method uses the parse scores, which is based on the observation that a higher parse score is correlated with a higher parsing quality. The second method uses the method of Mejer and Crammer (2012) to compute the Delta score. Mejer and Crammer (2012) compute a con\ufb01dence score for each edge. The algorithm attaches each edge to an alternative head. The Delta is the score di\ufb00erence between the original dependency tree and the tree with the changed edge. This method provides a per-edge con\ufb01dence score.",
  "The algorithm attaches each edge to an alternative head. The Delta is the score di\ufb00erence between the original dependency tree and the tree with the changed edge. This method provides a per-edge con\ufb01dence score. Note that the scores are real numbers and might be greater than 1. We changed the Delta-approach in two aspects from that of Mejer and Crammer (2012). We request that the new parse tree contains a node that has either a di\ufb00erent head or might have a di\ufb00erent edge label or both, since we use labelled dependency trees in contrast to Mejer and Crammer (2012). To obtain a single score for a tree, we use the averaged score of scores computed for the individual edge by the Delta function. We use our main evaluation parser (Mate parser (Bohnet et al., 2013)) to implement our self-training approach. Mate is an arc-standard transition-based parser which employs beam search and a graph-based rescoring model. This parser computes a score for each dependency tree by summing up the scores for each transition and dividing the score by the total number of transitions.",
  "Mate is an arc-standard transition-based parser which employs beam search and a graph-based rescoring model. This parser computes a score for each dependency tree by summing up the scores for each transition and dividing the score by the total number of transitions. Due to the swap-operation (used for non-projective parsing), the number of transitions can vary, cf. (Kahane et al., 1998; Nivre, 2007). 59",
  "0 20 40 60 80 100 75 80 85 90 95 Percentage of Sentences Labeled Attachment Score (%) Parse Score Adjusted Parse Score Delta Average Accuracy of Entire Set Figure 4.1: The accuracies when inspecting 10-100% sentences of the Weblogs devel- opment set ranked by the con\ufb01dence-based methods. Our second con\ufb01dence-based method requires the computation of the score di\ufb00erences between the best tree and alternative trees. To compute the smallest di\ufb00erence (Delta), we modi\ufb01ed the parser to derive the highest scoring alternative parse tree that replaces a given edge with an alternative one. This means either that the dependent is attached to another node or the edge label is changed, or both the dependent is attached to another node and the edge is relabelled. More precisely, during the parsing for alternative trees, beam candidates that contain the speci\ufb01ed labelled edge will be removed from the beam at the end of each transition.",
  "More precisely, during the parsing for alternative trees, beam candidates that contain the speci\ufb01ed labelled edge will be removed from the beam at the end of each transition. Let Scorebest be the score of the best tree, Scorei be the score of the alternative tree for the ith labelled edge and L be the length of the sentence, the Delta (ScoreDelta) for a parse tree is then calculated as follows: ScoreDelta = LP i=1 |Scorebest \u2212Scorei| L (4.1) To obtain high-accuracy dependency trees is crucial for our self-training approaches, thus we \ufb01rst assess the performance of the con\ufb01dence-based methods on the development set for selecting high-quality dependency trees. We rank the parsed sentences by their 60",
  "20 40 1 2 3 60 80 100 Sentence Length Parse Score Labelled Attachment Score(%) Figure 4.2: The accuracies, sentence lengths and the parse scores of individual sentences in Weblogs development set. con\ufb01dence scores in a descending order. Figure 4.1 shows the accuracy scores when selecting 10-100% of sentences with an increment of 10%. The Delta method shows the best performance for detecting high-quality parse trees. We observed that when inspecting 10% of sentences, the accuracy score di\ufb00erence between the Delta method and the average score of the entire set is nearly 14%. The method using the parse score does not show such a high accuracy di\ufb00erence. The accuracy of the 10% top ranked sentences are lower. We observed that despite that the parse score is the averaged value of the transitions, long sentences generally exhibit a higher score. Thus, short sentences tend to be ranked at the bottom, regardless of the accuracy. To give a more clear view, we plot the relations between the sentence lengths, parse scores and the accuracies in \ufb01gure 4.2.",
  "Thus, short sentences tend to be ranked at the bottom, regardless of the accuracy. To give a more clear view, we plot the relations between the sentence lengths, parse scores and the accuracies in \ufb01gure 4.2. The sentences of the Weblogs development set are represented by dots in the \ufb01gure based on their properties. To soften the sentences proportional to their length, we penalise the original parser score according to the sentence length, i.e. longer sentences are penalised more. The penalisation is done assuming a subtractive relationship between the original score and the length of the sentences (L) weighted by a constant (d) which we \ufb01t on the development set. The new parse scores are calculated as follows: 61",
  "Scoreadjusted = Scoreoriginal \u2212L \u00d7 d (4.2) To obtain the constant d, we apply the de\ufb01ned equation to all sentences of the de- velopment set and rank the sentences according to their adjusted scores in a descending order. The value of d is selected to minimise the root mean square-error (fr) of the ranked sentences. Following Mejer and Crammer (2012) we compute the fr by: fr = sX i ni(ci \u2212ai)2/( X i ni) (4.3) We use 100 bins to divide the accuracy into ranges of one percent. As the parse scores computed by the parser are generally in the range of [0,3], the parse scores in the range of [ (i\u22121)\u00d73 100 , i\u00d73 100] are assigned to the ith bin. Let ni be the number of sentences in ith bin, ci be the estimated accuracy of the bin calculated by i\u22120.5 100 and ai be the actual accuracy of the bin. We calculate fr by iterating stepwise over d from 0 to 0.05 with an increment of 0.005.",
  "We calculate fr by iterating stepwise over d from 0 to 0.05 with an increment of 0.005. Figure 4.3 shows the fr for the adjusted parse scores with di\ufb00erent values of d. The lowest fr is achieved when d = 0.015, this reduces the fr from 0.15 to 0.06 when compared to the parse score method without adjustment (d = 0). In contrast to the fr = 0.06 calculated when d is set to 0.015, the unranked sentences have a fr of 0.38, which is six times larger than that of the adjusted one. The reduction on fr achieved by our adjustment indicates that the adjusted parse scores have a higher correlation to the accuracy when compared to the ones without the adjustment. Figure 4.1 shows the performance of the adjusted parse scores for \ufb01nding high accuracy parse trees in relation to the original parse score and the Delta-based method. The adjusted parse score-based method performs signi\ufb01cantly better than that of the original score with a performance similar to the Delta method.",
  "The adjusted parse score-based method performs signi\ufb01cantly better than that of the original score with a performance similar to the Delta method. The method based on the parse scores is faster as we do not need to apply the parser to \ufb01nd alternatives for each edge of a dependency tree. 62",
  "0 1 2 3 4 5 \u00b710\u22122 0 0.1 0.2 0.3 0.4 0.5 Value of d Root Mean Square-Error (fr) Adjusted Parse Score Unranked Figure 4.3: The root mean square-error (fr) of Weblogs development set after ranked by adjusted parse scores with di\ufb00erent values of d. 4.2 Experiment Set-up For our evaluation on self-training, we used our main evaluation corpora and the Chem- ical domain text from the domain adaptation track of CoNLL 2007 shared task. We mainly evaluated on our main evaluation corpora and the best setting is also tuned on the development set of the main evaluation corpora. The Chemical domain evalua- tion is only used for comparison with previous work, we do not optimise our approaches speci\ufb01cally for this domain. For the main evaluation corpora, we used the Conll source domain training set, the Weblogs domain development set, the Conll source domain test set and Weblogs, Newsgroups, Reviews domain test sets.",
  "For the main evaluation corpora, we used the Conll source domain training set, the Weblogs domain development set, the Conll source domain test set and Weblogs, Newsgroups, Reviews domain test sets. We do not evaluate our approach on the Answers domain as the unlabelled data for this domain is not large enough for our self-training. The evaluation corpus for Chemical domain is taken from the domain adaptation track of the CoNLL 2007 shared task (Nivre et al., 2007a). The shared task is the second year running for the dependency parsing task. Besides the multi-lingual parsing 63",
  "train test unlabelled Sentences 18,577 195 256,000 Tokens 446,573 5,001 6,848,072 Avg. Length 24.04 25.65 26.75 Table 4.1: The size of datasets for Chemical domain evaluation. track introduced from the previous year, the 2007 shared task also included a track on domain adaptation task. The domain adaptation track provided mainly two domains (Biomedical and Chemical), in which the biomedical domain is used as development set and the chemical domain is used as evaluation set. The source domain training set consists of sections 2-11 of the Wall Street Journal section of the Penn Treebank (P. Marcus et al., 1993). A su\ufb03cient size of unlabelled data are also provided by the organiser, we used the \ufb01rst 256k sentences in our work. The labelled data are converted to dependency relations by the LTH constituent-to-dependency conversion tool (Johansson and Nugues, 2007). Table 4.1 shows the basic statistics of the training, development and the test set.",
  "The labelled data are converted to dependency relations by the LTH constituent-to-dependency conversion tool (Johansson and Nugues, 2007). Table 4.1 shows the basic statistics of the training, development and the test set. For the Chemical domain test we used only the data from the CoNLL 2007 shared task to make a fair comparison with Kawahara and Uchimoto (2008)\u2019s results. We use the Mate transition-based parser in our experiments. The parser is modi\ufb01ed to output the con\ufb01dence scores, other than that we used its default settings. For part- of-speech tagging, we use predicted tags from Mate\u2019s internal tagger for all the evaluated domains. For Chemical domain we evaluated additionally on gold tags as they are used by previous work. The baselines are trained only on the respective source domain training data. For the evaluation of the parser\u2019s accuracy, we report both labelled (LAS) and unla- belled (UAS) attachment scores, but mainly focus on the labelled version. We included all punctuation marks in the evaluation.",
  "The baselines are trained only on the respective source domain training data. For the evaluation of the parser\u2019s accuracy, we report both labelled (LAS) and unla- belled (UAS) attachment scores, but mainly focus on the labelled version. We included all punctuation marks in the evaluation. The signi\ufb01cance levels are marked according to the p-values, * and ** are used to represent the p-value of 0.05 and 0.01 levels respectively. 64",
  "50 100 150 200 250 300 77 77.5 78 78.5 79 Additional Training Data (K) Labeled Attachment Score (%) Random Adjusted Parse Score Delta Baseline Figure 4.4: The e\ufb00ect of our self-training approaches on the Weblogs development set. 4.3 Empirical Results Random Selection-based Self-training. To have an idea of the performance of basic self-training, we \ufb01rst evaluated with randomly selected additional training data. The triangle marked curve in Figure 4.4 shows the accuracy of the random selection-based self- training. We used from 50k to 200k randomly selected additional training data to retrain the Mate parser. The retrained models obtain some small improvements when compared with the baseline. The improvements achieved by the di\ufb00erent number of additional training data are very similar: they all around 0.2%. Those small improvements obtained by the basic self-training are not statistically signi\ufb01cant. This \ufb01nding is in line with previous work of applying non-con\ufb01dence-based self-training approaches to dependency parsing, cf.",
  "Those small improvements obtained by the basic self-training are not statistically signi\ufb01cant. This \ufb01nding is in line with previous work of applying non-con\ufb01dence-based self-training approaches to dependency parsing, cf. (Cerisara, 2014; Bj\u00a8orkelund et al., 2014). Parse Score-based Self-training. We then evaluate with our \ufb01rst con\ufb01dence-based method, that uses parse scores. As proposed the automatically annotated sentences are ranked in descending order by the adjusted parse scores before they are used as additional training data. As shown in Figure 4.4, we add between 50k to 300k top ranked sentences from the Weblogs auto-annotated dataset. The method achieved 0.52% improvement 65",
  "when we use 50k additional training data and the improvement increased to 0.66% when 250k sentences are used. After that, the improvement decreased. We use an auto-labelled dataset of 500k sentences. After we rank the sentences by our con\ufb01dence-based methods, the \ufb01rst half is expected to have an accuracy higher than the average, and the second half is expected to have one lower than average. Thus we should avoid using sentences from the second half of the ranked dataset. Delta-based self-training. For our Delta-based approach, we select additional train- ing data with the Delta method. We train the parser by adding between 50k to 300k sentences from the target domain. Same as the parse score-based method, we gain the largest improvement when 250k sentences are used, which improves the baseline by 0.73% (cf. Figure 4.4). Although this improvement is slightly higher than that of the parse score-based method, the accuracies are lower than the baseline when we use 50k and 100k ranked sentences from Delta based method.",
  "Figure 4.4). Although this improvement is slightly higher than that of the parse score-based method, the accuracies are lower than the baseline when we use 50k and 100k ranked sentences from Delta based method. Our error analysis shows that these parse trees are mainly short sentences consisting of only three words. These sentences contribute probably no additional information that the parser can exploit. Evaluating on test domains. We adapt our best settings of 250k additional sen- tences for both approaches and apply them to three test sets (Weblogs, Newsgroups and Reviews). As illustrated in Table 4.2, nearly all the results produced by both ap- proaches are statistically signi\ufb01cant improvements when compared to the baselines. The only exception is the unlabelled improvement of the parse score approach on Reviews domain which has a p-value of 0.08. Both approaches achieved the largest improvements on Weblogs domain. The largest labelled improvement of 0.81% is achieved by the parse score-based method, while the largest unlabelled improvement of 0.77% is achieved by the Delta method.",
  "Both approaches achieved the largest improvements on Weblogs domain. The largest labelled improvement of 0.81% is achieved by the parse score-based method, while the largest unlabelled improvement of 0.77% is achieved by the Delta method. For Newsgroups domain both approaches gained the similar labelled and unlabelled improvements of 0.6%. For Reviews domain the Delta method achieved 0.4 - 0.5% improvements on labelled and unlabelled accuracies. The parse score-based ap- proach achieved lower improvements of 0.3%. In terms of the in-domain evaluation, the accuracies of both approaches are lower than the baseline. 66",
  "Parse Score Delta Baseline LAS UAS LAS UAS LAS UAS Weblogs 79.8** 85.82** 79.68** 85.88** 78.99 85.1 Newsgroups 75.88** 83.41* 75.87* 83.49** 75.3 82.88 Reviews 75.43* 82.99 75.6** 83.09* 75.07 82.68 Conll 89.4 91.88 89.67 92.13 90.07 92.4 Table 4.2: The e\ufb00ect of the adjusted parse score-based and the Delta-based self-training approaches on our main test sets.",
  "PPOS GPOS LAS UAS LAS UAS Parse Score 80.8* 83.62* 83.44** 85.74** Delta 81.1* 83.71* 83.58** 85.8** Baseline 79.68 82.5 81.96 84.28 Kawahara (Self-trained) - - - 84.12 Kawahara (Baseline) - - - 83.58 Sagae (Co-training) - - 81.06 83.42 Table 4.3: The results of the adjusted parse score-based and the Delta-based self-training approaches on the Chemical test set compared with the best-reported self-training gain (Kawahara and Uchimoto, 2008) and the best results of CoNLL 2007 shared task, cf. Sagae and Tsujii (2007). (PPOS: results based on predicted tags, GPOS: results based on gold tags, Self-trained: results of self-training experiments, Co-trained: results of co- training experiments.) We further evaluate our best settings on Chemical texts provided by the CoNLL 2007 shared task.",
  "(PPOS: results based on predicted tags, GPOS: results based on gold tags, Self-trained: results of self-training experiments, Co-trained: results of co- training experiments.) We further evaluate our best settings on Chemical texts provided by the CoNLL 2007 shared task. We adapt the best settings of the main evaluation corpora and apply both con\ufb01dence-based approaches to the Chemical domain. For the constant d, we use 0.015 and we use 125k additional training data out of the 256k from the unlabelled data of the Chemical domain. We evaluate our con\ufb01dence-based methods on both predicted and gold part-of-speech tags. After retraining, both con\ufb01dence-based methods achieve signi\ufb01cant improvements in all experiments. Table 4.3 shows the results for the Chemical domain. When we use predicted part-of-speech tags, the Delta-based method gains a labelled improvement of 1.42%, while the parse score-based approach gains 1.12%.",
  "Table 4.3 shows the results for the Chemical domain. When we use predicted part-of-speech tags, the Delta-based method gains a labelled improvement of 1.42%, while the parse score-based approach gains 1.12%. For the experiments based on gold tags, we achieved larger labelled improvements of 1.62% for the Delta-based and 1.48% for the parse score-based methods. For all experiments, the unlabelled improvements are similar to that of labelled ones. 67",
  "Table 4.3 compares our results with that of Kawahara and Uchimoto (2008). We added also the results of Sagae and Tsujii (2007) but those are not directly comparable since they were gained with co-training. Sagae and Tsujii (2007) gained additional training data by parsing the unlabelled data with two parsers and then they select those sentences where the parsers agree on. Kawahara and Uchimoto (2008) reported positive results for self-training. They used a separately trained binary classi\ufb01er to select additional training data and are evaluated only on gold tags. Our baseline is higher than Kawahara and Uchimoto (2008)\u2019s self- training result. Starting from this strong baseline, we could improve by 1.62% LAS and 1.52% UAS which is an error reduction of 9.6% on the UAS (cf. Table 4.3). The largest improvement of 1.52% compared to that of Kawahara and Uchimoto (2008) (0.54% UAS) is substantially larger.",
  "Table 4.3). The largest improvement of 1.52% compared to that of Kawahara and Uchimoto (2008) (0.54% UAS) is substantially larger. We obtained the result by a simple method, and we do not need a separately trained classi\ufb01er. 4.4 Analysis Our self-training approaches demonstrated their merit in the above experiments, two con\ufb01dence-based methods work equally well on most of the domains. This suggests self- training can be used for out-of-domain dependency parsing when there is a reasonably good con\ufb01dence-based method available. As two con\ufb01dence-based methods showed similar performances on our tested domains, the \ufb01rst guess would be they might consist of a large portion of identical additional training data. We assess our assumption on the development set. We \ufb01rst rank the dataset by di\ufb00erent methods. Let Deltan and PSn be the top ranked n% sentences of the development set by their Delta and adjusted parse scores. The identical rate is de\ufb01ned as the percentage of sentences that are presented in both Deltan and PSn.",
  "Let Deltan and PSn be the top ranked n% sentences of the development set by their Delta and adjusted parse scores. The identical rate is de\ufb01ned as the percentage of sentences that are presented in both Deltan and PSn. Figure 4.5 shows the identical rate of our methods. The identical rates are lower than we expected, for top ranked 10% sentences only 5% of them are identical, and the identical rate is 56% for the \ufb01rst half of the ranked list. As the 68",
  "0 20 40 60 80 100 0 20 40 60 80 100 Percentage of Sentences Identical Rate (%) Figure 4.5: The identical rate between the adjusted parse score-based and the Delta- based methods, when top ranked n percent is concerned. additional training data from Delta and adjusted parse scores can consist of more than 40 percent di\ufb00erent sentences, we suspect there might be some behaviour di\ufb00erence between two methods. In order to have a more clear picture about the behaviours of our con\ufb01dence- based methods, we applied both token level and sentence level analysis to those methods. This allows us to have an in-depth comparison between our con\ufb01dence-based methods. In the same way as we did in our analysis for co-training, we plot the accuracy changes of major syntactic labels and compute improvements di\ufb00erent on unknown/known words in our token level analysis. For sentence level analysis, we evaluate all four factors on both con\ufb01dence-based methods, cf. sentence length, the number of unknown words, the number of prepositions and the number of conjunctions.",
  "For sentence level analysis, we evaluate all four factors on both con\ufb01dence-based methods, cf. sentence length, the number of unknown words, the number of prepositions and the number of conjunctions. For our analysis, three target domain test sets are used as a single set. 4.4.1 Token Level Analysis Individual Label Accuracy. Figure 4.6 shows the comparison of accuracy changes between our adjusted parse score-based approach and the Delta-based approach. Two approaches show similar patterns on the individual labels, both of them show no e\ufb00ect on 69",
  "Confusion Baseline Parse Score Delta NMOD \u2192ADV 200 192 226 NMOD \u2192HYPH 190 190 193 NMOD \u2192NAME 530 565 510 NMOD \u2192PMOD 151 128 128 NMOD \u2192HMOD 206 211 212 NMOD \u2192OBJ,SBJ,LOC 361 333 337 P \u2192HYPH,NMOD 245 263 254 SBJ \u2192NMOD,OBJ 238 221 224 OBJ \u2192NMOD 174 150 170 PMOD \u2192NMOD 228 225 215 PMOD \u2192OBJ 104 88 91 ROOT \u2192NMOD 201 192 198 ROOT \u2192OBJ 105 88 101 ADV \u2192LOC 195 180 186 ADV \u2192NMOD 305 285 300 ADV \u2192MNR,AMOD,DIR,",
  "AMOD,DIR,TMP 457 416 411 COORD \u2192NMOD 125 109 109 VC \u2192OPRD 95 90 75 CONJ \u2192NMOD 101 99 100 DEP \u2192ROOT 152 153 155 DEP \u2192OBJ 173 161 166 DEP \u2192SBJ 305 303 311 DEP \u2192NMOD 294 322 302 DEP \u2192ADV,TMP 229 241 234 AMOD \u2192NMOD 208 223 227 AMOD \u2192ADV,HYPH 236 242 235 TMP \u2192ADV 225 250 259 TMP \u2192NMOD 112 117 115 PRD \u2192OBJ 700 724 722 PRD \u2192ADV,VC 218 220 213 Table 4.4: The confusion matrix of dependency labels, compared between the self-training approaches and the baseline. 70",
  "NMOD P PMOD SBJ ROOT OBJ ADV COORD VC CONJ PRD DEP AMOD TMP 0 2 4 Accuracy Change (%) a) Parse Score Method NMOD P PMOD SBJ ROOT OBJ ADV COORD VC CONJ PRD DEP AMOD TMP 0 2 4 Accuracy Change (%) b) Delta Based Method Recall Precision F-score Figure 4.6: The performance comparison between the self-training approach and the baseline on major labels. labels such as P (punctuations), CONJ (conjunct) and PRD (predicative complement). They both gained more than 0.5% f-score on ROOT (root of the sentence), COORD (coordination), some modi\ufb01ers (PMOD, AMOD) and unclassi\ufb01ed relations (DEP). In addition to the common improvements between two methods, the Delta method also gains a 0.9% improvement on VC (Verb chain), and the parse score method achieved 0.5% improvement on SBJ (subject). Figure 4.4 shows the confusion matrix of your self-training methods compared with the baseline. Unknown Words Accuracy. For unlabelled improvements, both methods showed 71",
  "Parse Score Delta Baseline Tokens LAS UAS LAS UAS LAS UAS Known 84421 78.4 85.0 78.4 85.1 77.8 84.5 Unknown 5049 62.4 73.5 62.5 73.8 61.6 72.5 All 89470 77.5 84.4 77.5 84.5 76.9 83.8 Table 4.5: The accuracy comparison between the self-training approach and the baseline on unknown words. 0 10 20 30 40 0 20 40 60 80 100 Number of Tokens Percentage (%) a) Parse Score Method Better Worse No Change 0 10 20 30 400 100 200 300 Number of Tokens Number of Sentences a) Parse Score Method Better Worse No Change No.",
  "of Sents 0 10 20 30 40 0 20 40 60 80 100 Number of Tokens Percentage (%) b) Delta Based Method Better Worse No Change 0 10 20 30 400 100 200 300 Number of Tokens Number of Sentences b) Delta Based Method Better Worse No Change No. of Sents Figure 4.7: The comparison between the self-training approach and the baseline on di\ufb00erent number of tokens per sentence. a large gap between known words and unknown words. Improvements on unknown words are at least doubled in value when compared to that of known words. The improvement di\ufb00erences are smaller on the labelled accuracies. The value for unknown words is only 0.2% higher than that of known words. This is an indication that self-training is able to improve unknown words attachment but still does not have su\ufb03cient information to make label decisions. The improvements of the entire set are same as that of known words and are not a\ufb00ected largely by the unknown words. This is due to the unknown words only occupying 5% of the dataset.",
  "The improvements of the entire set are same as that of known words and are not a\ufb00ected largely by the unknown words. This is due to the unknown words only occupying 5% of the dataset. 4.4.2 Sentence Level Analysis Sentence Length. For the sentence level analysis we \ufb01rst evaluate the performance of our self-training approaches on the di\ufb00erent sentence lengths. The sentences that have the 72",
  "0 1 2 3 4 0 20 40 60 80 100 Number of Unknown Words Percentage (%) a) Parse Score Method (LAS) Better Worse No Change 0 1 2 3 40 1,000 2,000 3,000 Number of Unknown Words Number of Sentences a) Parse Score Method (LAS) Better Worse No Change No. of Sents 0 1 2 3 4 0 20 40 60 80 100 Number of Unknown Words Percentage (%) b) Delta Based Method (LAS) Better Worse No Change 0 1 2 3 40 1,000 2,000 3,000 Number of Unknown Words Number of Sentences b) Delta Based Method (LAS) Better Worse No Change No.",
  "of Sents 0 1 2 3 4 0 20 40 60 80 100 Number of Unknown Words Percentage (%) c) Parse Score Method (UAS) Better Worse No Change 0 1 2 3 40 1,000 2,000 3,000 Number of Unknown Words Number of Sentences c) Parse Score Method (UAS) Better Worse No Change No. of Sents 0 1 2 3 4 0 20 40 60 80 100 Number of Unknown Words Percentage (%) d) Delta Based Method (UAS) Better Worse No Change 0 1 2 3 40 1,000 2,000 3,000 Number of Unknown Words Number of Sentences d) Delta Based Method (UAS) Better Worse No Change No. of Sents Figure 4.8: The comparison between the self-training approach and the baseline on di\ufb00erent number of unknown words per sentence. same length are grouped into classes.",
  "of Sents Figure 4.8: The comparison between the self-training approach and the baseline on di\ufb00erent number of unknown words per sentence. same length are grouped into classes. For each class, the sentences are further classi\ufb01ed into three subclasses (better, worse and no change) according to their accuracies when compared with the baseline. We plot them together with the number of sentences in individual classes in Figure 4.7. The left-hand side is the \ufb01gure for the parse score-based method, while the right-hand side is that of the Delta-based method. At a \ufb01rst glance, both methods show similar behaviours, they both do not help the very short sentences. The percentages for sentences longer than 30 tokens are varied. More precisely, the parse score-based method helps most on the sentences containing between 10 and 35 tokens, and the Delta-based method is most productive on sentences which have a length between 15 and 30 tokens. Unknown Words. For the sentence level analysis of unknown words, we evaluate 73",
  "0 2 4 0 20 40 60 80 100 Number of Prepositions Percentage (%) a) Parse Score Method Better Worse No Change 0 2 4 0 500 1,000 1,500 2,000 Number of Prepositions Number of Sentences a) Parse Score Method Better Worse No Change No. of Sents 0 2 4 0 20 40 60 80 100 Number of Prepositions Percentage (%) b) Delta Based Method Better Worse No Change 0 2 4 0 500 1,000 1,500 2,000 Number of Prepositions Number of Sentences b) Delta Based Method Better Worse No Change No. of Sents Figure 4.9: The comparison between the self-training approach and the baseline on di\ufb00erent number of prepositions per sentence. on both labelled and unlabelled accuracy scores. This is mainly because according to our token level analysis our self-training gained much larger unlabelled improvements on the unknown words than that of known words.",
  "on both labelled and unlabelled accuracy scores. This is mainly because according to our token level analysis our self-training gained much larger unlabelled improvements on the unknown words than that of known words. Figure 4.8 shows our analysis of unknown words, the upper \ufb01gures are the analysis of labelled accuracies and the lower two are that of unlabelled accuracies. As we can see from the above two \ufb01gures, the gap between sentences that have a better labelled accuracy and sentences worsened in accuracy are not a\ufb00ected by the increasing number of unknown words in sentences. The gap on unlabelled accuracies shows a clear increasement when more than two unknown words are found in the sentence. This is in line with our \ufb01nding in the token level analysis that self-training could improve more on unknown words attachment. Prepositions. The preposition analysis of our con\ufb01dence-based self-training is shown in Figure 4.9. Both methods show very similar curves, they gain small improvements around 1% on sentences that have up to one preposition, but they achieved larger improve- ments on sentences that have at least 2 prepositions.",
  "Both methods show very similar curves, they gain small improvements around 1% on sentences that have up to one preposition, but they achieved larger improve- ments on sentences that have at least 2 prepositions. Although the di\ufb00erences between sentences that are parsed better and those parsed worse varies for the di\ufb00erent number of prepositions, most of the gains are larger than 6% and the largest gain is around 14%. Overall, the con\ufb01dence-based self-training methods show clear better performances on sentences that have multiple prepositions. 74",
  "0 1 2 3 0 20 40 60 80 100 Number of Conjunctions Percentage (%) a) Parse Score Method Better Worse No Change 0 1 2 30 1,000 2,000 3,000 Number of Conjunctions Number of Sentences a) Parse Score Method Better Worse No Change No. of Sents 0 1 2 3 0 20 40 60 80 100 Number of Conjunctions Percentage (%) b) Delta Based Method Better Worse No Change 0 1 2 30 1,000 2,000 3,000 Number of Conjunctions Number of Sentences b) Delta Based Method Better Worse No Change No. of Sents Figure 4.10: The comparison between the self-training approach and the baseline on di\ufb00erent number of conjunctions per sentence. Conjunctions. In terms of conjunctions, both methods show similar \ufb01gures, cf. Figure 4.10. They both show gains for most of the cases, except that the parse score- based method shows no e\ufb00ect on sentences that have 3 conjunctions.",
  "In terms of conjunctions, both methods show similar \ufb01gures, cf. Figure 4.10. They both show gains for most of the cases, except that the parse score- based method shows no e\ufb00ect on sentences that have 3 conjunctions. They both start with a small gain of 2-3% when there is no conjunction in the sentence and the improvement widened to 7-10% for sentences have more conjunctions. There are only 100 sentences in the class of 3 conjunctions, thus the numbers of this class are less reliable. Generally speaking, the self-training approaches work slightly better on the sentences that have more conjunctions. Example Sentences. Table 4.6 and table 4.7 present example sentences that have been improved by the parse score-based and the Delta-based self-training approaches respectively. We choose four sentences (the \ufb01rst four sentences) that have been largely improved by both approaches, as we can see from table the improvements achieved by both models are very similar, some are even identical.",
  "We choose four sentences (the \ufb01rst four sentences) that have been largely improved by both approaches, as we can see from table the improvements achieved by both models are very similar, some are even identical. 4.5 Chapter Summary In this chapter, we introduced two novel con\ufb01dence-based self-training approaches to do- main adaptation for dependency parsing. We compared a self-training approach that 75",
  "But 1 c 31dep creating 2 31sbj a3 5nmod balanced4 5nmod community5 2obj with 6 p 5nmod a7 8nmod mix8 6pmod of 9 p 8nmod housing10 9pmod ,11 10p o\ufb03ces12 10coord ,13 12p shopping14 12coord and 15 c 14coord other16 17nmod amenities17 15conj \u201318 5p allowing19 5appo people20 19obj to21 19oprd live22 21im close23 22loc to 24 p 23amod where 25 27loc they26 27sbj work27 24pmod and 28 c 27coord play29 28conj \u201330 5p is 31 0root an32 36nmod even 33 35amod more 34 35amod worthy 35 36nmod goal 36 31prd .",
  "37 31p Her 1 4nmod \u201c2 4p Rubble3 4name Division 4 6sbj \u201d5 4p mixes 6 0root such7 9nmod disparate8 9nmod materials 9 6obj as 10 p 9nmod ink11 13nmod -12 13nmod jet13 14nmod prints 14 10pmod pasted 15 u 14appo on 16 p 15loc board17 16pmod ,18 14p foam19 20nmod rubber20 14coord ,21 20p galvanized22 23nmod steel23 20coord ,24 23p concrete25 23coord ,26 25p steel27 28nmod rebar 28 u 25coord and 29 c 28coord bungee 30 u 31nmod cords 31 u 29conj .",
  "32 6p Go1 0root look2 1oprd at 3 p 2adv DHRM 4 u 3pmod and 5 c 4coord the6 9nmod state7 9nmod courts8 9nmod system9 5conj (10 9p separate 11 13nmod HR 12 u 13nmod dept 13 u 9appo )14 9p and 15 c 2coord see16 15conj what17 21obj the 18 20nmod state 19 20nmod folks 20 21sbj do 21 16obj , 22 21p and 23 c 21coord who24 30obj all25 26nmod you26 27sbj \u2019re27 23conj talking28 27vc about 29 p 28adv furloughing 30 u 29pmod .",
  "22 21p and 23 c 21coord who24 30obj all25 26nmod you26 27sbj \u2019re27 23conj talking28 27vc about 29 p 28adv furloughing 30 u 29pmod .31 1p and 1 c 3dep i 2 3sbj promise 3 0root to 4 3oprd fess 5 u 4im up6 5prt eventually7 5tmp and 8 c 5coord tell9 8conj of 10 p 9adv at 11 p 13dep least12 11amod one 13 15nmod such 14 15nmod epic 15 10pmod i 16 17sbj survived 17 15nmod \u2013 18 3p If 1 p 11adv you2 3sbj come3 1sub upon 4 p 3adv something5 4pmod important 6 5appo ,",
  "7 11p by 8 p 11adv all9 10nmod means 10 8pmod make 11 0root a12 13nmod note13 11obj of 14 p 13nmod it15 14pmod , 16 11p and 17 c 11coord so18 11adv on19 18amod . 20 11p [jingzhe19] 1 u 12dep However 2 12adv ,",
  "16 11p and 17 c 11coord so18 11adv on19 18amod . 20 11p [jingzhe19] 1 u 12dep However 2 12adv , 3 12p the4 5nmod post 5 12sbj of 6 p 5nmod driving7 6pmod on 8 p 7tmp a9 11nmod snowy 10 u 11nmod day11 8pmod reminds12 0root me13 12obj again14 12adv of 15 p 12adv the16 17nmod story17 15pmod of 18 p 17nmod Hua 19 u 20name Xin 20 u 18pmod and 21 c 20coord Wang22 23name Lang23 21conj in 24 p 17loc the 25 27nmod New 26 27name Anecdotes 27 u 24pmod of 28 p 27nmod Social29 30name Talk 30 28pmod .",
  "31 12p But 1 c 32dep to 2 32sbj stand3 2im , 4 3p day5 3tmp after 6 p 5nmod day7 6pmod ,8 2p and 9 c 2coord to10 9conj make11 10im such12 14nmod preposterous13 14nmod statements14 11obj , 15 14p known 16 14appo to 17 p 16adv everybody18 17pmod to 19 16oprd be20 19im lies 21 20prd ,22 2p without 23 p 2mnr even24 25adv being25 23pmod ridiculed26 25vc in 27 p 26loc your28 30nmod own29 30nmod milieu 30 u 27pmod ,31 32p can 32 0root only33 32adv happen34 32vc in 35 p 34loc this36 37nmod region37 35pmod .",
  "31 32p can 32 0root only33 32adv happen34 32vc in 35 p 34loc this36 37nmod region37 35pmod . 38 32p The1 3nmod only2 3nmod thing 3 7sbj that4 5dep was5 3nmod edible 6 5prd was 7 0root the8 10nmod steamed 9 u 10nmod rice10 7prd and 11 c 7coord the12 15nmod vegetable 13 15nmod lo 14 u 15nmod mein 15 u 16sbj was 16 11conj barely17 18amod tolerable18 16prd .",
  "19 7p The1 2nmod asparagus 2 u 10sbj , 3 2p seared 4 u 5nmod tuna 5 2coord , 6 5p and 7 c 5coord lobster8 9nmod tail 9 7conj were 10 0root the11 12nmod best12 10prd we13 15sbj ever14 15tmp had15 12nmod . 16 10p Table 4.6: The example sentences that have been improved by the parse score-based self- training approach when compared to the baseline. In which the dependency head/relation of a token are marked as the subscript, while the superscript is the index of token. The unknown words, prepositions and conjunctions are highlighted with u , p and c re- spectively. We highlight the di\ufb00erent levels of the improvements achieved by our parse score-based self-training model on the dependency edges by di\ufb00erent colours. In which the blue colour means both head and label are corrected, the yellow colour means only the head is corrected and the green colour means only the label is corrected. 76",
  "But 1 c 31dep creating 2 31sbj a3 5nmod balanced4 5nmod community5 2obj with 6 p 5nmod a7 8nmod mix8 6pmod of 9 p 8nmod housing10 9pmod ,11 10p o\ufb03ces12 10coord ,13 12p shopping14 12coord and 15 c 14coord other16 17nmod amenities17 15conj \u201318 5p allowing19 5appo people20 19obj to21 19oprd live22 21im close23 22loc to 24 p 23amod where 25 27loc they26 27sbj work27 24pmod and 28 c 27coord play29 28conj \u201330 5p is 31 0root an32 36nmod even 33 35amod more 34 35amod worthy 35 36nmod goal36 31prd .",
  "37 31p Her 1 4nmod \u201c2 4p Rubble3 4name Division 4 6sbj \u201d5 4p mixes 6 0root such7 9nmod disparate8 9nmod materials 9 6obj as 10 p 9nmod ink11 13nmod -12 13nmod jet13 14nmod prints 14 10pmod pasted 15 u 14appo on 16 p 15loc board17 16pmod ,18 14p foam19 20nmod rubber20 14coord ,21 20p galvanized22 23nmod steel23 20coord ,24 23p concrete25 23coord ,26 25p steel27 28nmod rebar 28 u 25coord and 29 c 28coord bungee 30 u 31nmod cords 31 u 29conj .",
  "32 6p Go1 0root look2 1oprd at 3 p 2adv DHRM 4 u 3pmod and 5 c 4coord the6 9nmod state7 9nmod courts8 9nmod system9 5conj (10 9p separate 11 13nmod HR 12 u 13nmod dept 13 u 9appo )14 9p and 15 c 2coord see16 15conj what17 21obj the 18 20nmod state 19 20nmod folks 20 21sbj do 21 16obj , 22 21p and 23 c 21coord who24 30obj all25 26nmod you26 27sbj \u2019re27 23conj talking28 27vc about 29 p 28adv furloughing 30 u 29pmod .",
  "22 21p and 23 c 21coord who24 30obj all25 26nmod you26 27sbj \u2019re27 23conj talking28 27vc about 29 p 28adv furloughing 30 u 29pmod .31 1p and 1 c 3dep i 2 3sbj promise 3 0root to 4 3oprd fess 5 u 4im up6 5prt eventually7 5tmp and 8 c 5coord tell9 8conj of 10 p 9adv at 11 p 13dep least12 11amod one 13 15nmod such 14 15nmod epic 15 10pmod i 16 17sbj survived 17 15nmod \u2013 18 3p In 1 p 11adv fact2 1pmod ,",
  "3 11p the4 6nmod MINI 5 u 6name COOPER 6 11sbj she 7 8sbj was 8 6nmod riding 9 8vc in 10 p 9adv is 11 0root not12 11adv what13 16obj the14 15nmod reports15 16sbj said16 11prd . 17 11p But 1 c 3dep everyone2 3sbj needs3 0root to4 3oprd recognize5 4im that 6 p 5obj Arlington7 9nmod \u2019s8 7suffix decision 9 16sbj not10 11adv to11 9nmod pursue12 11im a 13 15nmod balanced 14 15nmod community 15 12obj means 16 6sub that 17 p 16obj housing18 19sbj will 19 17sub end20 19vc up21 20prt somewhere22 20loc else23 22amod ,",
  "24 22p presumably25 26pmod in 26 p 22amod outlying 27 28nmod counties 28 26pmod .29 3p Jim1 0root , 2 1p on 3 p 1adv behalf4 3pmod of 5 p 4nmod the6 11nmod Virginia7 11name Recycling8 9name Markets9 10nmod Development10 11nmod Council11 5pmod and 12 c 11coord the 13 15nmod Mid-Atlantic 14 u 15name Consortium 15 12conj of 16 p 15nmod Recycling17 21nmod and 18 c 17coord Economic19 20name Development20 18conj O\ufb03cials 21 16pmod (22 15p MACREDO 23 u 15appo )24 15p , 25 1p thanks26 1dep for 27 p 26nmod plugging 28 27pmod e-cycling 29 u 28obj .",
  "25 1p thanks26 1dep for 27 p 26nmod plugging 28 27pmod e-cycling 29 u 28obj .30 1p when 1 12tmp the2 3nmod guy 3 12sbj ( 4 9p the5 6nmod owner 6 9dep ,7 9p it8 9sbj turned 9 3prn out10 9prt )11 9p arrived 12 20tmp to 13 12prp open14 13im the15 17nmod gas16 17nmod station17 14obj , 18 20p he19 20sbj took 20 0root one21 22nmod look22 20obj at 23 p 20adv our24 26nmod cow 25 u 26nmod pie26 23pmod with 27 p 26nmod wheels28 27pmod and 29 c 20coord said30 29conj \u201c31 30p what32 34nmod the33 34nmod fook 34 u 30obj ?",
  "35 20p \u201d 36 20p In 1 p 26loc Pakistan2 4nmod national3 4nmod chart4 1pmod besides 5 p 26adv the6 8nmod transit7 8nmod a\ufb04iction 8 5pmod to 9 p 8nmod transit10 11nmod Venus11 9pmod in 12 p 8loc the13 15nmod fourth14 15nmod house15 12pmod by 16 p 8nmod FMs 17 u 18nmod Rahu 18 u 16pmod and 19 c 18coord Mercury20 19conj , 21 26p natal 22 u 23nmod Saturn 23 26sbj and 24 c 23coord Venus25 24conj are 26 0root also27 26adv under 28 p 26prd the29 31nmod close30 31nmod a\ufb04iction31 28pmod of 32 p 31nmod transit 33 34nmod Rahu 34 u 32pmod .",
  "35 26p Table 4.7: The example sentences that have been improved by the Delta-based self- training approach when compared to the baseline. In which the dependency head/relation of a token are marked as the subscript, while the superscript is the index of token. The unknown words, prepositions and conjunctions are highlighted with u , p and c respec- tively. We highlight the di\ufb00erent levels of the improvements achieved by our Delta-based self-training model on the dependency edges by di\ufb00erent colours. In which the blue colour means both head and label are corrected, the yellow colour means only the head is corrected and the green colour means only the label is corrected. 77",
  "uses random selection and two con\ufb01dence-based approaches. The random selection-based self-training method did not improve the accuracy which is in line with previously pub- lished negative results, both con\ufb01dence-based methods achieved statistically signi\ufb01cant improvements and showed relatively high accuracy gains. We tested both con\ufb01dence-based approaches on three web related domains of our main evaluation corpora (Weblogs, Newsgroups, Reviews) and the Chemical domain. Our con\ufb01dence-based approaches achieved statistically signi\ufb01cant improvements in all tested domains. For web domains, we gained up to 0.8 percentage points for both labelled and unlabelled accuracies. On average the Delta-based approach improved the accuracy by 0.6% for both labelled and unlabelled accuracies. Similarly, the parse score-based method improved labelled accuracy scores by 0.6% and unlabelled accuracy scores by 0.5%. In terms of the Chemical domain, the Delta-based and the parse score-based approaches gained 1.42% and 1.12% labelled accuracies respectively when using predicted PoS tags.",
  "In terms of the Chemical domain, the Delta-based and the parse score-based approaches gained 1.42% and 1.12% labelled accuracies respectively when using predicted PoS tags. When we used gold PoS tags, a larger labelled improvement of 1.62% is achieved by the Delta method and 1.48% is gained by the parse score method. The unlabelled improvements for both methods are similar to their labelled improvements for all the experiments. In total, our approaches achieved signi\ufb01cantly better accuracy for all four domains. We conclude from the experiments that self-training based on con\ufb01dence is worth ap- plying in a domain adaptation scenario and that a con\ufb01dence-based self-training approach seems to be crucial for the successful application of self-training in dependency parsing. Our evaluation underlines the \ufb01nding that the pre-selection of parse trees is probably a precondition that self-training becomes e\ufb00ective in the case of dependency parsing and to reach a signi\ufb01cant accuracy gain. The further analysis compared the behaviour of two approaches and gave a clearer picture of in which part self-training helps most.",
  "The further analysis compared the behaviour of two approaches and gave a clearer picture of in which part self-training helps most. As a preliminary analysis, we assessed the overlap between the top ranked sentences of two methods. When we compared the top ranked 50% of the development set by di\ufb00erent methods, 56% of them are identical. 78",
  "As there are more than 40% sentences which are selected di\ufb00erently by di\ufb00erent methods, we expect some clear di\ufb00erences in our in-depth analysis on token and sentence level. Sur- prisingly, the further analysis suggested that both methods played similar roles on most of the analysis, the behaviour di\ufb00erences are rather small. In our token level analysis, both methods gained large improvements on the root, coordination, modi\ufb01ers and unclassi\ufb01ed relations. We also found much larger unlabelled improvements for unknown words. For sentence level analysis, we noticed that our approaches helped most the medium length sentences (10-30 tokens/sentence). Generally speaking, they also have a better perfor- mance on sentences that have certain levels of complexity, such as sentences that have more than 2 unknown words or at least 2 prepositions. This might also because of the simpler sentences have already a reasonably good accuracy when baseline model is used, thus are harder to improve. 79",
  "CHAPTER 5 MULTI-LINGUAL SELF-TRAINING Self-training approaches have previously been used mainly for English parsing (McClosky et al., 2006a; McClosky et al., 2006b; Reichart and Rappoport, 2007; Kawahara and Uchi- moto, 2008; Sagae, 2010; Petrov and McDonald, 2012). The few successful attempts of using self-training for languages other than English were limited only to a single language (Chen et al., 2008; Goutam and Ambati, 2011). The evaluations of using self-training for multiple languages are still found no improvements on accuracies (Cerisara, 2014; Bj\u00a8orkelund et al., 2014). In the previous chapter we demonstrated the power of the con\ufb01dence-based self- training on English out-of-domain parsing, the evaluation on four di\ufb00erent domains showed large gains. We wonder if the self-training methods could be adapted to other languages. The \ufb01rst problem with going beyond English is the lack of resources.",
  "We wonder if the self-training methods could be adapted to other languages. The \ufb01rst problem with going beyond English is the lack of resources. To the best of our knowledge, there is no out-of-domain corpus available for languages other than English. In fact, even for English, the out-of-domain dataset is very limited. Thus, we are not able to evaluate on the same domain adaptation scenario as we did for English. In English evaluation, we do not use any target domain manually annotated data for training, which is a typical domain adaptation scenario that assume no target domain training data is annotated. The other common domain adaptation scenario assumes that there is a small number of target domain training data available. In this chapter, we use a small training set (5,000 sentences) to simulate the latter scenario. The same domain unlabelled set is annotated by the base model to enlarge the training data. Strictly speaking, this is an 80",
  "under-resourced in-domain parsing setting as in the 2014 shared task at the workshop on statistical parsing of morphologically rich language (SPMRL) (Seddah et al., 2014). More precisely, in this chapter, we evaluate with the adjusted parse score-based method, as both methods have very similar performances and the adjusted parse scores are fast to compute. We evaluate this method on nine languages (Arabic, Basque, French, German, Hebrew, Hungarian, Korean, Polish, Swedish) corpora of the SPMRL shared task (Seddah et al., 2014). The rest of the chapter are organized as follows: We introduce our approach and experiment settings in Section 5.1 and 5.2 respectively. Section 5.3 and 5.4 discusses and analyses the results. We summarise the chapter in Section 5.5. 5.1 Multi-lingual Con\ufb01dence-based Self-training Our goal for the multi-lingual experiments is to evaluate the performance of our con\ufb01dence- based method on more languages.",
  "We summarise the chapter in Section 5.5. 5.1 Multi-lingual Con\ufb01dence-based Self-training Our goal for the multi-lingual experiments is to evaluate the performance of our con\ufb01dence- based method on more languages. Our previous evaluations on multiple web domains and the Chemical domain showed that our con\ufb01guration is robust and can be directly used across domains. Thus, in our multi-lingual evaluation we again directly adapt our best con\ufb01guration from our English evaluation, in which the \ufb01rst half of the ranked auto- annotated dataset is used as additional training data for all the languages. We also do not tune di\ufb00erent con\ufb01gurations for individual language, as we want to evaluate the con\ufb01dence-based self-training in a uni\ufb01ed framework. More precisely, our multi-lingual self-training approach consists of a single iteration with the following steps: 1. A parser is trained on a (small) initial training set to generate a base model. 2. We analyse a large number of unlabelled sentences with the base model. 3.",
  "A parser is trained on a (small) initial training set to generate a base model. 2. We analyse a large number of unlabelled sentences with the base model. 3. We build a new training set consisting of the initial training set and 50% newly analysed sentences parsed with a high con\ufb01dence. 4. We retrain the parser on the new training set to produce a self-trained model. 81",
  "0 1 2 3 4 5 \u00b710\u22122 89.5 90 90.5 91 91.5 Value of d Labeled Attachment Score (%) Adjusted Parse Score Figure 5.1: Accuracies of sentences which have a position number within the top 50% after ranking the auto-parsed sentences of German development set by the adjusted parse scores with di\ufb00erent values of d. 5. Finally, the self-trained model is used to annotate the test set. Here we give a recap of our adjusted parse score method and con\ufb01rm the correlation between accuracy and the adjusted parse scores on the multi-lingual development set. The adjusted parse score method which we proposed in the previous chapter is mainly based on the observation that the parse scores of sentences are correlated with their accuracies. However, the original parse scores are sensitive to sentence length, in which longer sentences usually have higher scores. To tackle this problem, we introduce a simple but e\ufb00ective adjustment on the scores.",
  "However, the original parse scores are sensitive to sentence length, in which longer sentences usually have higher scores. To tackle this problem, we introduce a simple but e\ufb00ective adjustment on the scores. The original parse score of an auto-parsed sentence (Scoreoriginal) is subtracted by its sentence length (L) multiplied by a \ufb01xed number d. More precisely, the adjusted parse scores are calculated as follows: Scoreadjusted = Scoreoriginal \u2212L \u00d7 d (5.1) To obtain the constant d, we apply the de\ufb01ned equation with di\ufb00erent values of d to all sentences of the development set and rank the sentences by their adjusted scores in a descending order. Let No(i) be the position number of the ith sentence after ranking them 82",
  "0 20 40 60 80 100 86 88 90 92 94 Percentage of Sentences (%) Labeled Attachment Score (%) Original Parse Score Adjusted Parse Score Average Accuracy Figure 5.2: The accuracies when inspecting 10-100% sentences of the German develop- ment set ranked by the con\ufb01dence-based methods. by the adjusted scores. The value of d is selected to maximize the accuracy of sentences that have a No(i) within the top 50%. We evaluate stepwise di\ufb00erent values of d from 0 to 0.05 with an increment of 0.005. The highest accuracy of the top ranked sentences is achieved when d = 0.015 (see Figure 5.1), thus d is set to 0.015 in our experiments. The d value used in our English evaluations is the same 0.015, this shows a stability of our equation. Figure 5.2 shows the accuracies when inspecting 10 -100% of sentences ranked by adjusted and original parse scores. We found that adjusted parse scores lead to a higher correlation with accuracies compared to original parse scores.",
  "Figure 5.2 shows the accuracies when inspecting 10 -100% of sentences ranked by adjusted and original parse scores. We found that adjusted parse scores lead to a higher correlation with accuracies compared to original parse scores. This is in line with our \ufb01nding in previous evaluation on English out-of-domain data. 5.2 Experiment Set-up We evaluate our adjusted parse score-based self-training approach with the Spmrl multi- lingual corpora. The Spmrl multi-lingual corpora consist of nine languages (Arabic, Basque, French, German, Hebrew, Hungarian, Korean, Polish, Swedish) in- domain datasets available from 2014 Shared Task at the workshop on statistical parsing of morphologically rich languages (SPMRL), cf. (Seddah et al., 2014). We have chosen the 83",
  "Arabic Basque French German Hebrew train: Sentences 5,000 5,000 5,000 5,000 5,000 Tokens 224,907 61,905 150,984 87,841 128,046 Avg. Length 44.98 12.38 30.19 17.56 25.60 test: Sentences 1,959 946 2,541 5,000 716 Tokens 73,878 11,457 75,216 92,004 16,998 Avg. Length 37.71 12.11 29.60 18.40 23.74 unlabelled: Sentences 100,000 100,000 100,000 100,000 100,000 Tokens 4,340,695 1,785,474 1,618,324 1,962,248 2,776,500 Avg.",
  "Length 43.41 17.85 16.18 19.62 27.77 Hungarian Korean Polish Swedish train: Sentences 5,000 5,000 5,000 5,000 Tokens 109,987 68,336 52,123 76,357 Avg. Length 21.99 13.66 10.42 15.27 test: Sentences 1,009 2,287 822 666 Tokens 19,908 33,766 8,545 10,690 Avg. Length 19.73 14.76 10.39 16.05 unlabelled: Sentences 100,000 100,000 100,000 100,000 Tokens 1,913,154 2,147,605 2,024,323 1,575,868 Avg. Length 19.13 21.48 20.24 15.76 Table 5.1: Statistics about the Spmrl multi-lingual corpora 84",
  "datasets as there are no multi-lingual out-of-domain corpora available. Actually, even the in-domain corpora for many languages are rather small. We used the 5k smaller training set from the shared task, to make the scenario similar to the domain adaptation task that assumes a small number of target domain data is available. This setting is also a good basis for exploration for improving parsing accuracy of under-resourced languages. For each language, the shared task also provided a su\ufb03cient unlabelled data which is required by our evaluation. We evaluate nine languages in a uni\ufb01ed setting, in which the 5k training set and a 100k unlabelled dataset are used for all the languages. For additional training set, we parse all 100k sentences for each of the languages and use 50k of them as the additional training set. For tuning the d value of our adjusted parse score-based method, we used only the German development set, as we intend to use a uni\ufb01ed setting for all languages and the German development set is the largest in size. Table 5.1 shows statistics about the corpora that we used in our experiments.",
  "Table 5.1 shows statistics about the corpora that we used in our experiments. We evaluate all nine languages on the Mate parser (Bohnet et al., 2013), the default settings are used in all the experiments. To output the con\ufb01dence scores we slightly modi\ufb01ed the parser, however, this does not a\ufb00ect the parser\u2019s accuracy. For part-of- speech tagging, we use the Mate parser\u2019s internal tagger for all the evaluations. The baselines are obtained from models trained only on the 5k initial training data. We report both labelled (LAS) and unlabelled (UAS) attachment scores, and mainly focus on the labelled accuracy. In line with the shared task o\ufb03cial evaluation method, we include all the punctuations in our evaluation. The statistically signi\ufb01cance levels are marked according to their p-values, (*) p-value < 0.05, (**) p-value < 0.01. 5.3 Empirical Results In this section, we report our results of the adjusted parse score-based self-training ap- proach on the test sets of nine languages.",
  "5.3 Empirical Results In this section, we report our results of the adjusted parse score-based self-training ap- proach on the test sets of nine languages. To obtain the increased training data for our self-trained model, the unlabelled data is parsed and ranked by their con\ufb01dence scores. 85",
  "Baseline Self-train LORIA LAS UAS LAS UAS LAS UAS Arabic 82.09 85.17 82.22 85.21 81.65 84.56 Basque 78.35 84.8 79.22** 85.61** 81.39 86.86 French 81.91 86.03 81.48 85.63 81.74 85.89 German 81.54 84.72 81.87** 85.18** 83.35 86.37 Hebrew 78.86 85.08 79.04 85.26 75.55 82.79 Hungarian 83.13 87.48 83.56* 87.65 82.88 87.26 Korean 73.31 77.75 75.45** 79.54** 74.15 78.53 Polish 81.97 87.8 81.35 87.25 79.95 87.98 Swedish 79.67 86.1 80.26 86.78* 80.04 86.3 Average 80.",
  "15 78.53 Polish 81.97 87.8 81.35 87.25 79.95 87.98 Swedish 79.67 86.1 80.26 86.78* 80.04 86.3 Average 80.09 84.99 80.49 85.35 80.08 85.17 Table 5.2: Comparing our self-trained results with the best non-ensemble system in the SPMRL Shared Task (LORIA). The 50% (50k) top ranked sentences are added to the initial training set. We retrain the Mate parser on the new training set. The empirical results on nine languages show that our approach worked for \ufb01ve lan- guages which are Basque, German, Hungarian, Korean and Swedish. Moreover, the self-trained model achieved on average (nine languages) 0.4% gains for both labelled and unlabelled accuracies. These improvements are achieved only by a uni\ufb01ed experiment setting, we do not tune parameters for individual language.",
  "Moreover, the self-trained model achieved on average (nine languages) 0.4% gains for both labelled and unlabelled accuracies. These improvements are achieved only by a uni\ufb01ed experiment setting, we do not tune parameters for individual language. Our self-training approach has the potential to achieve even better performances if we treat each of the languages separately, however, this is beyond the scope of this work. More precisely, our self-training method achieved the largest labelled and unlabelled improvements on Korean with absolute gains of 2.14 and 1.79 percentage points re- spectively. Other than Korean, we also gain statistically signi\ufb01cant improvements on Basque, German, Hungarian and Swedish. For Basque, the method achieved 0.87% gain for labelled accuracy and the improvement for unlabelled accuracy is 0.81%. For German, improvements of 0.33% and 0.46% are gained by our self-trained model for labelled and unlabelled scores respectively. For Hungarian, we achieved a 0.42% gain on labelled accuracy, the unlabelled improvement is smaller (0.17%) thus not statistically signi\ufb01cant.",
  "For Hungarian, we achieved a 0.42% gain on labelled accuracy, the unlabelled improvement is smaller (0.17%) thus not statistically signi\ufb01cant. For Swedish, improvements of 0.59% and 0.68% are achieved for labelled and 86",
  "unlabelled accuracies. The unlabelled gain is statistically signi\ufb01cant, while the labelled gain is not a statistically signi\ufb01cant improvement which has a p-value of 0.067. As the im- provements on Swedish are large but the test set is small (only contains 666 sentences), we decided to enlarge the test set by the Swedish development set. The Swedish de- velopment set contains 494 sentences and is not used for tuning in our experiments. The evaluation on the combined set showed 0.7% and 0.6% statistically signi\ufb01cant (p <0.01) improvements for labelled and unlabelled scores. This con\ufb01rms the e\ufb00ectiveness of our self-training method on Swedish. In terms of the e\ufb00ects of our method on other lan- guages, our method gains moderate improvements on Arabic and Hebrew but these are statistically insigni\ufb01cant accuracy gains. We \ufb01nd negative results for French and Polish. Table 5.2 shows detailed results of our self-training experiments.",
  "We \ufb01nd negative results for French and Polish. Table 5.2 shows detailed results of our self-training experiments. We compare our self-training results with the best non-ensemble parsing system of the SPMRL shared tasks (Seddah et al., 2013; Seddah et al., 2014). The best results of the non-ensemble system are achieved by Cerisara (2014). Their system is also based on the semi-supervised learning, the LDA clusters (Chrupala, 2011) are used to explore the unlabelled data. The average labelled accuracy of our baseline on nine languages is same as the one achieved by Cerisara (2014) and our self-trained results are 0.41% higher than their results. The average unlabelled accuracy of our self-trained model also surpasses that of Cerisara (2014) but with a smaller margin of 0.18%. Overall, our self-trained models perform better in six languages (Arabic, Hebrew, Hungarian, Korean, Polish and Swedish) compared to the best non-ensemble system of Cerisara (2014).",
  "Overall, our self-trained models perform better in six languages (Arabic, Hebrew, Hungarian, Korean, Polish and Swedish) compared to the best non-ensemble system of Cerisara (2014). 5.4 Analysis In this section, we analyse the results achieved by our self-training approach. Our ap- proach achieved improvements on most of the languages, but also showed negative e\ufb00ects on two languages. Thus, we analyse both positive and negative e\ufb00ects introduced by our self-training approach. 87",
  "For the analysis on positive e\ufb00ects, we choose the Korean dataset, as our self-training method achieved the largest improvement on it. The goal for our analysis on Korean is to \ufb01nd out where the improvement comes from. We apply our token and sentence level analysis to Korean. We evaluate for the token level the accuracy changes of individual labels and compare the improvements of unknown and known words. For our sentence level evaluation, we evaluate the performances on di\ufb00erent sentence length and the number of unknown words per sentence. We do not evaluate on the number of subjects, the number of prepositions and number of conjunctions as those factors are language speci\ufb01c, thus they might not suitable for Korean. For the analysis of negative e\ufb00ects, we analyse the French dataset as the French test set is larger than that of Polish. We aim to have an idea why our self-training approach has a negative e\ufb00ect on results.",
  "For the analysis of negative e\ufb00ects, we analyse the French dataset as the French test set is larger than that of Polish. We aim to have an idea why our self-training approach has a negative e\ufb00ect on results. Our analysis focuses on two directions, \ufb01rstly, we check the correlation between the quality of French data and our con\ufb01dence scores, as the correlation is the pre-condition of the successful use of our self-training approach; secondly, we check the similarity between the test set and the unlabelled set to assess the suitability of unlabelled data. 5.4.1 Positive E\ufb00ects Analysis Token Level Analysis Individual Label Accuracy. The Korean syntactic labels set used in the shared task contains 22 labels (Seddah et al., 2014). We listed the 12 most frequently used labels in our analysis. Those labels are presented in the Korean test set for at least 1,000 times. As we can see from the Figure 5.3, the largest f-score improvement of 5.6% is achieved on conjuncts (conj).",
  "Those labels are presented in the Korean test set for at least 1,000 times. As we can see from the Figure 5.3, the largest f-score improvement of 5.6% is achieved on conjuncts (conj). Large gains of more than 0.4% are achieved on nearly all the labels, the only exception is punctuations (p), for punctuations our self-training approach only achieved a moderate improvement of 0.1%. The adverbial modi\ufb01er (adv), topic (tpc), subordination (sub), auxiliary verb (aux) and modi\ufb01er of predicate (vmod) 88",
  "Confusion Baseline Self-training adn \u2192nmod 99 113 adn \u2192sub,root 88 84 adv \u2192adn 52 35 adv \u2192sub,nmod,vmod 126 130 p \u2192conj 55 28 p \u2192nmod 126 136 p \u2192adn 103 91 p \u2192vmod 57 50 nmod \u2192conj 62 39 nmod \u2192adn 209 166 nmod \u2192adv 99 88 nmod \u2192vmod 215 179 nmod \u2192sub 41 38 root \u2192aux 103 116 root \u2192adn 41 15 tpc \u2192adn 107 74 tpc \u2192nmod 30 29 sub \u2192conj 75 69 sub \u2192adn 74 57 sub \u2192adv 40 50 sbj \u2192comp 35 36 aux \u2192root 66 59 aux \u2192sub,adn 68 57 conj \u2192sub 88 86 conj \u2192adn 56 42 conj \u2192nmod 48 54 vmod \u2192nmod 187 195 vmod \u2192adv 77 78 vmod \u2192sub,adn,",
  "adn 68 57 conj \u2192sub 88 86 conj \u2192adn 56 42 conj \u2192nmod 48 54 vmod \u2192nmod 187 195 vmod \u2192adv 77 78 vmod \u2192sub,adn,amod 116 108 Table 5.3: The confusion matrix of dependency labels, compared between the multi- lingual self-training approach and the baseline. have improvements between 0.4% and 0.9%. The other \ufb01ve labels, adnominal modi\ufb01er (adn), modi\ufb01er of nominal (nmod), root of the sentence (root), object (obj), subject (sbj) are improved by more than 1%. Table 5.3 shows the confusion matrix of the dependency labels. Unknown Words Accuracy. Table 5.4 shows our analysis of the unknown words. The unknown words rate for the Korean test is surprisingly higher than expected, more than 45% of the words in the test set are not presented in the training set. This might due 89",
  "adn adv p nmod root tpc obj sub sbj aux conj vmod \u22122 0 2 4 6 Accuracy Change (%) Recall Precision F-score Figure 5.3: The performance comparison between the multi-lingual self-training approach and the baseline on major labels. Self-training Baseline Tokens LAS UAS LAS UAS Known 15567 81.6 84.0 79.7 82.2 Unknown 12799 67.9 74.1 65.5 72.3 All 28366 75.5 79.5 73.3 77.7 Table 5.4: The accuracy comparison between the multi-lingual self-training approach and the baseline on unknown words. to two reasons: \ufb01rstly the training set is very small only contains 5k sentences thus have a less coverage of vocabulary; secondly and the main reason is the Korean tokens used in the shared task are combinations of the word form and the grammatical a\ufb03xes. The latter creates much more unique tokens.",
  "The latter creates much more unique tokens. The vocabulary of the training set is 29,715, but the total number of tokens is only 68,336, which means each token only shows less than 2.3 times on average. Despite the high unknown words rate, our self-training approach showed a better labelled improvement (2.4%) on unknown words than that of known words (1.9%). While the unlabelled improvement (1.8%) is exactly the same for both known and unknown words. 90",
  "0 5 10 15 20 25 0 20 40 60 80 100 Number of Tokens Percentage (%) Better Worse No Change 0 5 10 15 20 250 50 100 150 200 Number of Tokens Number of Sentences Better Worse No Change No. of Sents Figure 5.4: The comparison between the multi-lingual self-training approach and the baseline on di\ufb00erent number of tokens per sentence. Sentence Level Analysis Sentence Length. We then apply the sentence level analysis for Korean test set. We \ufb01rst evaluate on the di\ufb00erent sentence length, sentences that have the same length are assigned into the same group. We then calculate the percentage of sentences that are improved, decreased or unchanged in accuracy for each group. We plot the results along with the number of sentences in each of the groups in Figure 5.4. As we can see from the \ufb01gure, the gap between the improved and decreased sentences are smaller (about 3%) on short sentences that contain less than 10 tokens. The gap signi\ufb01cantly widens when the sentence length grows.",
  "As we can see from the \ufb01gure, the gap between the improved and decreased sentences are smaller (about 3%) on short sentences that contain less than 10 tokens. The gap signi\ufb01cantly widens when the sentence length grows. The gap increased to 30% for sentences containing more than 20 tokens. This is a clear indication that our self-training yielded stronger enhancements on longer sentences. Unknown Words. As we found in the token level analysis, the unknown words rate is very high for Korean test set. In the extreme case, there could be more than 20 unknown words in a single sentence. The curve shows an overall increased gap between the sentences improved by the self-trained model and those worsened when the number of unknown words per sentence increases. However, the gains sometimes drop, the most 91",
  "0 2 4 6 8 10 0 20 40 60 80 100 Number of Unknown Words Percentage (%) Better Worse No Change 0 2 4 6 8 100 100 200 300 Number of Unknown Words Number of Sentences Better Worse No Change No. of Sents Figure 5.5: The comparison between the multi-lingual self-training approach and the baseline on di\ufb00erent number of unknown words per sentence. notable group is the one for sentences containing 7 unknown words. The percentage of worsened sentences are even 0.5% higher than that of improved ones. It is unclear the reason why the behaviour changes, but due to the group size is small (only 200 sentences) we suggest this might caused by chance. 5.4.2 Negative E\ufb00ects Analysis Con\ufb01dence Score Analysis As our con\ufb01dence-based self-training is based on the hypothesis that the con\ufb01dence scores are able to indicate the quality of the annotations.",
  "5.4.2 Negative E\ufb00ects Analysis Con\ufb01dence Score Analysis As our con\ufb01dence-based self-training is based on the hypothesis that the con\ufb01dence scores are able to indicate the quality of the annotations. Thus when our self-training approach showed a negative e\ufb00ect on the accuracy, the \ufb01rst thing comes to our mind is to check the correlation between con\ufb01dence scores and accuracies. We analyse the correlation on the French test set by ranking the sentences in the dataset according to their con\ufb01dence scores. We assess the accuracy of the top ranked n percent sentences. We set n to 10% and increase it by 10% in each step until all the sentences are included. We show the analysis in Figure 5.6. The analysis suggests that there is a reasonably high correlation between the quality of the sentences and our con\ufb01dence-based method. The top ranked 92",
  "0 20 40 60 80 100 82 84 86 88 90 Percentage of Sentences (%) Labeled Attachment Score (%) Adjusted Parse Score Average Accuracy Figure 5.6: The accuracies when inspecting 10-100% sentences of the French test set ranked by the con\ufb01dence-based methods. train test unlabelled Sentences 5,000 2,541 100,000 Tokens 150,984 75,216 1,618,324 Avg. Length 30.19 29.60 16.18 UNK (%) - 5.91 16.82 Similarity (%) - 99.74 95.47 Table 5.5: The basic statistic of datasets for French evaluation. 10% sentences have an accuracy of 89.99% which is 8% higher than the average. The accuracy for top ranked 50% sentences is 86.77% which surpasses the average by 5%. Unlabelled Data Analysis The quality of unlabelled data is another issue that might a\ufb00ect the results.",
  "The accuracy for top ranked 50% sentences is 86.77% which surpasses the average by 5%. Unlabelled Data Analysis The quality of unlabelled data is another issue that might a\ufb00ect the results. We \ufb01rst compute the basic statistics of the training, test and unlabelled dataset to have a surface level comparison. As shown in Table 5.5 the unlabelled data is very di\ufb00erent from the training and test set. More precisely, the average sentence length of the unlabelled data is much shorter. The unknown words rate of the unlabelled dataset (16.82%) is three times higher than that of the test set (5.91%). We further calculate the cosine similarity between the training set and the test/unlabelled dataset. The test set is highly similar to 93",
  "the training set with a similarity of 99.74%. The similarity score of the unlabelled data is more than 4% lower, which suggests the unlabelled data is more di\ufb00erent. 5.5 Chapter Summary In this chapter, we evaluated an e\ufb00ective con\ufb01dence-based self-training approach on nine languages. Due to the lack of out-of-domain resources, we used an under-resourced in- domain setting instead. We used for all languages a uni\ufb01ed setting, the parser is retrained on the new training set boosted by the top 50k ranked parse trees selected from a 100k auto-parsed dataset. Our approach successfully improved accuracies of \ufb01ve languages (Basque, German, Hungarian, Korean and Swedish) without tuning variables for the individual lan- guage. We can report the largest labelled and unlabelled accuracy gain of 2.14% and 1.79% on Korean, on average we improved the baselines of \ufb01ve languages by 0.87% (LAS) and 0.78% (UAS). We further did an in-depth analysis on Korean and French.",
  "We further did an in-depth analysis on Korean and French. For Korean, we did a number of analysis on both token level and sentence level to understand where the improvement comes from. The analysis on the individual label showed that the self- trained model achieved large improvement on all the major labels, and it achieved the largest gain on conjuncts (conj). The analysis of unknown words showed that the self- trained model gained a larger labelled improvement for unknown words. The analysis on sentence length suggested the self-training approach achieved larger improvements on longer sentences. For French, we aim to understand why self-training did not work. The analysis showed the con\ufb01dence scores have a reasonably high correlation with the annotation quality, hence it is less likely be the reason of self-training\u2019s negative e\ufb00ect. While the large di\ufb00erence between unlabelled data and the training/test sets is more likely a major contributor to the accuracy drop. 94",
  "CHAPTER 6 DEPENDENCY LANGUAGE MODELS In this chapter, we introduce our dependency language models (DLM) approach for both in-domain and out-of-domain dependency parsing. The co-training and self-training ap- proaches evaluated in the previous chapters have demonstrated their e\ufb00ectiveness on the out-of-domain parsing, however, neither approaches gained large improvements on the source domain accuracy. In fact, sometimes they even have a negative e\ufb00ect on the in- domain results. Another disadvantage of co-/self-training is that they can use only a relatively small additional training dataset, as training parsers on a large corpus might be time-consuming or even intractable on a corpus of millions of sentences. The goal of our DLM approach is to create a robust model that is able to improve both in-domain and out-of-domain accuracies. Unlike the co-/self-training, the DLM approach does not use the unlabelled data directly for retraining. Instead, a small number of features based on DLMs are integrated into the parser, thus we could explore much larger unlabelled datasets.",
  "Unlike the co-/self-training, the DLM approach does not use the unlabelled data directly for retraining. Instead, a small number of features based on DLMs are integrated into the parser, thus we could explore much larger unlabelled datasets. Other semi-supervised techniques that use the unlabelled data indirectly include word clustering (Brown et al., 1992; Chrupala, 2011) and word embedding (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014). However, both word clustering and word embedding are generated from unannotated data, thus do not consider the syntactic structures. The DLMs used in this work are generated from the automatically annotated dataset, which could bene\ufb01t additionally from the syntactic annotations. Dependency language models are variants of language models based on dependency structures. An N-gram DLM is able to predict the next child when given N-1 immediate 95",
  "previous children and their head. DLMs were \ufb01rst introduced by Shen et al. (2008) and were later adapted to dependency parsing by Chen et al. (2012). Chen et al. (2012) inte- grated DLMs extracted from large auto-parsed corpora into a second-order graph-based parser. DLMs allow the parser to explore higher order features but without increasing the time complexity. We use a similar approach as Chen et al. (2012), but our approach is di\ufb00erent in six important aspects: 1. We apply DLMs to a transition-based dependency parser. 2. We additionally use syntactic labels in the DLM-based features as our parser pro- duces the labelled annotations. 3. The DLM-based features are integrated into a strong parser that is able to achieve competitive baselines. 4. We use not only single DLM but also multiple DLMs in our experiments. 5. We evaluate our approach on both in-domain and out-of-domain parsing. 6.",
  "The DLM-based features are integrated into a strong parser that is able to achieve competitive baselines. 4. We use not only single DLM but also multiple DLMs in our experiments. 5. We evaluate our approach on both in-domain and out-of-domain parsing. 6. Inspired by our co-training approach, we also investigate the parser with DLMs generated from high-quality auto-parsed data. In the rest of this chapter, we introduce our approaches in Section 6.1, we present our experiment set-up in Section 6.2. In Section 6.3 and 6.4 we discuss and analyse the results. In the \ufb01nal section (Section 6.5) we summarise the chapter. 6.1 Dependency Language Models for Transition-based System Dependency language models were introduced by Shen et al. (2008) to capture long dis- tance relations in syntactic structures. An N-gram DLM predicts the next child based on N-1 immediate previous children and their head. We integrate DLMs extracted from a 96",
  "large parsed corpus into the Mate parser (Bohnet et al., 2013). We \ufb01rst train a base model with the manually annotated training set. The base model is then used to annotate a large number of unlabelled sentences. After that, we extract DLMs from the auto-annotated corpus. Finally, we retrain the parser with additional DLM-based features. Further, we experimented with techniques to improve the quality of the syntactic annotations which we use to build the DLMs. We parse the unlabelled data with two di\ufb00erent parsers and then select the annotations on which both parsers agree on. The method is similar to co-training except that we do not train the parser directly on these auto-labelled sentences. We build the DLMs with the method of Chen et al. (2012). For each child xch, we gain the probability distribution Pu(xch|HIS), where HIS refers to N \u22121 immedi- ate previous children and their head xh. The previous children for xch are those who share the same head with xch but are closer to the head word according to the word sequence in the sentence.",
  "The previous children for xch are those who share the same head with xch but are closer to the head word according to the word sequence in the sentence. Consider the left side child xLk in the dependency relations (xLk...xL1, xh, xR1...xRm) as an example; the N-1 immediate previous children for xLk are xLk\u22121..xLk\u2212N+1. In our approach, we estimate Pu(xch|HIS) by the relative frequency: Pu(xch|HIS) = count(xch, HIS) P x\u2032 ch count(x\u2032 ch, HIS) (6.1) By their probabilities, the N-grams are sorted in a descending order. We then used the thresholds of Chen et al. (2012) to replace the probabilities with one of the three classes (PH, PM, PL) according to their position in the sorted list, i.e. the probabilities having an index in the \ufb01rst 10% of the sorted list are replaced with PH, PM refers to probabilities ranked between 10% and 30%, probabilities that are ranked below 30% are replaced with PL.",
  "the probabilities having an index in the \ufb01rst 10% of the sorted list are replaced with PH, PM refers to probabilities ranked between 10% and 30%, probabilities that are ranked below 30% are replaced with PL. During parsing, we use an additional class PO for relations not presented in DLMs. We use the classes instead of the probability is because our baseline parser uses the binary feature representations, classes are required to map the features into the binary feature representations. As a result, the real number features are hard to be integrated into 97",
  "< NODLM, \u03c6(Pu(s0)), \u03c6(Pu(s1)), label > < NODLM, \u03c6(Pu(s0)), \u03c6(Pu(s1)), label, s0 pos > < NODLM, \u03c6(Pu(s0)), \u03c6(Pu(s1)), label, s0 word > < NODLM, \u03c6(Pu(s0)), \u03c6(Pu(s1)), label, s1 pos > < NODLM, \u03c6(Pu(s0)), \u03c6(Pu(s1)), label, s1 word > < NODLM, \u03c6(Pu(s0)), \u03c6(Pu(s1)), label, s0 pos, s1 pos > < NODLM, \u03c6(Pu(s0)), \u03c6(Pu(s1)), label, s0 word, s1 word > Table 6.1: DLM-based feature templates which we used in the parser. train dev test unlabelled Section 2-21 22 23 - Sentences 39,832 1,700 2,416 30,546,808 Tokens 950,028 40,117 56,684 771,306,902 Avg.",
  "train dev test unlabelled Section 2-21 22 23 - Sentences 39,832 1,700 2,416 30,546,808 Tokens 950,028 40,117 56,684 771,306,902 Avg. Length 23.85 23.60 23.46 25.25 Table 6.2: The size of datasets for the Wsj Stanford conversion evaluation. the existing system. In the preliminary experiments, the PH class is mainly \ufb01lled by unusual relations that only appeared a few times in the parsed text. To avoid this we con\ufb01gured the DLMs to only use elements which have a minimum frequency of three, i.e. count(xch, HIS) \u22653.",
  "To avoid this we con\ufb01gured the DLMs to only use elements which have a minimum frequency of three, i.e. count(xch, HIS) \u22653. Table 6.1 shows our feature templates, where NODLM is an index which allows DLMs to be distinguished from each other, s0, s1 are the top and the second top of the stack, \u03c6(Pu(s0/s1)) refers the coarse label of probabilities Pu(xs0/s1|HIS) (one of the PH, PM, PL, PO), s0/s1 pos, s0/s1 word refer to part-of-speech tags, word forms of s0/s1, and label is the dependency label between s0 and s1. 6.2 Experiment Set-up For our experiments on English in-domain text, we used the Wall Street Journal portion (Wsj) of the Penn English Treebank (P. Marcus et al., 1993). The constituency trees are converted to the Stanford style dependency relations.",
  "6.2 Experiment Set-up For our experiments on English in-domain text, we used the Wall Street Journal portion (Wsj) of the Penn English Treebank (P. Marcus et al., 1993). The constituency trees are converted to the Stanford style dependency relations. The Stanford conversion attracts more attention during the recent years, it has been used in the SANCL 2012 shared tasks (Petrov and McDonald, 2012) and many state-of-the-art results were also reported using this conversion (Weiss et al., 2015; Andor et al., 2016; Dozat and Manning, 2017). We 98",
  "train dev test unlabelled Section 001-815, 886-931, 816-885, - 1001-1136 1148-1151 1137-1147 Sentences 16,118 805 1,915 19,806,808 Tokens 437,860 20,454 50,319 467,242,601 Avg. Length 27.16 25.41 26.27 23.59 Table 6.3: The size of datasets for the Chinese Treebank 5 (Ctb) evaluation. follow the standard splits of the corpus, section 2-21 are used for training, section 22 and 23 are used as the development set and the test set respectively. We used the Stanford parser 1 v3.3.0 to convert the constituency trees into Stanford style dependencies (de Marne\ufb00e et al., 2006). For unlabelled data, we used the data of Chelba et al. (2013) which contains around 30 million sentences (800 million words) from the news domain.",
  "For unlabelled data, we used the data of Chelba et al. (2013) which contains around 30 million sentences (800 million words) from the news domain. Table 6.2 shows the basic statistics about the corpus; In addition to the Wsj corpus, we also evaluate our approach on the main evalua- tion corpus of this thesis. Our main evaluation corpus consists of a Conll source do- main training set, a source domain test set and four target domain test sets (Weblogs, Newsgroups, Reviews and Answers). Unlike our Wsj corpus that uses Stanford de- pendencies, the main evaluation corpus is based on the LTH conversion (Johansson and Nugues, 2007). Experimenting on di\ufb00erent conversions and domains allow us to evaluate our method\u2019s robustness. For unlabelled data, we use the same dataset as in our Wsj evaluation. For Chinese, we evaluate our approach only on the in-domain scenario, this is due to the lack of out-of-domain corpus.",
  "For unlabelled data, we use the same dataset as in our Wsj evaluation. For Chinese, we evaluate our approach only on the in-domain scenario, this is due to the lack of out-of-domain corpus. We use Chinese Treebank 5 (CTB5) (Xue et al., 2005) as the source of our gold standard data. The Chinese Treebank 5 corpus mainly consists of articles from Xinhua news agency but also contains some articles from Sinorama magazine and information services department of HKSAR. We follow the splits of Zhang and Nivre (2011), the constituency trees are converted to dependency relations by the Penn2Malt2 tool using head rules of Zhang and Clark (2008). We use the Xinhua portion 1http://nlp.stanford.edu/software/lex-parser.shtml 2http://stp.ling\ufb01l.uu.se/ nivre/research/Penn2Malt.html 99",
  "of Chinese Gigaword Version 5.0 1 as our source for unlabelled data. We noticed that the unlabelled data we used actually contains the Xinhua portion of the CTB5; to avoid potential con\ufb02ict we removed them from the unlabelled data. After the pre-processing, our Chinese unlabelled data consists of 20 million sentences which are roughly 450 million words. We use ZPar2 v0.7.5 as our pre-processing tool. The word segmentor of ZPar is trained on the CTB5 training set. Table 6.3 gives some statistics about the corpus. We use a modi\ufb01ed version of the Mate transition-based parser in our experiments. We enhance the parser with our DLM-based features; other than this we used the parser\u2019s default setting. The part-of-speech tags are supplied by Mate parser\u2019s internal tagger. The baselines are trained only on the initial training set. In most of our experiments, DLMs are extracted from data annotated by the base model of Mate parser.",
  "The part-of-speech tags are supplied by Mate parser\u2019s internal tagger. The baselines are trained only on the initial training set. In most of our experiments, DLMs are extracted from data annotated by the base model of Mate parser. For the evaluation on higher quality DLMs, the unlabelled data is additionally tagged and parsed by Berkeley parser (Petrov and Klein, 2007) and is converted to dependency trees with the same tools as for gold data. We report both labelled (LAS) and unlabelled (UAS) attachment scores for our eval- uation. The punctuation marks are excluded for our English and Chinese in-domain evaluations. For English evaluation on our main evaluation corpus we include the punc- tuations. The signi\ufb01cance levels are marked due to their p-values, we use * and ** to represent the p-value of 0.05 and 0.01 levels respectively. 6.3 Empirical Results Parsing with Single DLM. We \ufb01rst evaluate the e\ufb00ect of the single DLM for both English and Chinese.",
  "6.3 Empirical Results Parsing with Single DLM. We \ufb01rst evaluate the e\ufb00ect of the single DLM for both English and Chinese. We generate the unigram, bigram and trigram DLMs from 5 mil- lion auto-annotated sentences of the individual language. We then retrain the parser by providing di\ufb00erent DLMs to generate new models. The lines marked with triangles in Figure 6.1 shows the results of our new models. Unigram DLM achieved the largest im- 1https://catalog.ldc.upenn.edu/LDC2011T13 2https://github.com/frcchang/zpar 100",
  "1 2 3 91 91.1 91.2 91.3 91.4 91.5 Value of N Labeled Attachment Score (%) a) English Single DLM Multiple DLMs Baseline 1 2 3 4 79 79.5 80 80.5 Value of N Labeled Attachment Score (%) b) Chinese Single DLM Multiple DLMs Baseline Figure 6.1: E\ufb00ects (LAS) of di\ufb00erent number of DLMs on English and Chinese develop- ment sets. provements for both English and Chinese. The unigram model achieved 0.38% labelled improvement for English and the improvement for Chinese is 0.9%. Parsing with Multiple DLMs. We then evaluate the parser with multiple DLMs. We use DLMs up to N-gram to retrain the parser. Take N=2 as an example, we use both unigram and bigram DLMs for retraining. This setting allows the parser to explore multiple DLMs at the same time.",
  "We use DLMs up to N-gram to retrain the parser. Take N=2 as an example, we use both unigram and bigram DLMs for retraining. This setting allows the parser to explore multiple DLMs at the same time. We plot our multi-DLM results by lines marked with the circle in Figure 6.1 a) and b) for English and Chinese respectively. As we can see from the \ufb01gures, the best setting for English remains the same, the parser does not gain additional improvement from the bigram and trigram. For Chinese, the improvement increased when more DLMs are used. We achieved the largest improvement by using unigram, bigram and trigram DLMs at the same time (N=3). By using multiple DLMs we achieved a 1.16% gain on Chinese. Extracting DLMs from Larger datasets. To determine the optimal corpus size to build DLMs we extract DLMs from di\ufb00erent size corpora.",
  "By using multiple DLMs we achieved a 1.16% gain on Chinese. Extracting DLMs from Larger datasets. To determine the optimal corpus size to build DLMs we extract DLMs from di\ufb00erent size corpora. We start with 10 million sentences and increase the size in steps until all the unlabelled data (30 million for En- glish and 20 million for Chinese) are used. We compare our results with the best result achieved by the DLMs extracted from 5 million annotations in Figure 6.2. The results 101",
  "0 5 10 15 20 25 30 91 91.1 91.2 91.3 91.4 91.5 Corpus Size (M) Labeled Attachment Score (%) a) English DLM Baseline 0 5 10 15 20 79 79.5 80 80.5 Corpus Size (M) Labeled Attachment Score (%) b) Chinese DLM Baseline Figure 6.2: E\ufb00ects (LAS) of DLMs extracted from di\ufb00erent size (in million sentences) of corpus on English and Chinese development sets. on English data suggest that the DLMs generated from larger corpora do not gain addi- tional improvement when compared to the one that used 5 million sentences. The Chinese results show a moderate additional gain of 0.04% when compared to the previous best re- sult. The e\ufb00ects indicate that 5 million sentences might already be enough for generating reasonably good DLMs. Extracting DLMs from High Quality Data.",
  "The Chinese results show a moderate additional gain of 0.04% when compared to the previous best re- sult. The e\ufb00ects indicate that 5 million sentences might already be enough for generating reasonably good DLMs. Extracting DLMs from High Quality Data. To evaluate the in\ufb02uence of the quality of the input corpus for building the DLMs, we experiment in addition with DLMs extracted from high-quality corpora. The higher quality corpora are prepared by parsing unlabelled sentences with the Mate parser and the Berkeley parser. We add only the sentences that are parsed identically by both parsers to the high-quality corpus. For Chi- nese, only 1 million sentences that consist of 5 tokens in average have the same syntactic structures assigned by the two parsers. Unfortunately, this amount is not su\ufb03cient for the experiments as their average sentence length is in stark contrast with the training data (27.1 tokens). For English, we obtained 7 million sentences with an average sentence length of 16.9 tokens. To get an impression of the quality, we parse the development set with those parsers.",
  "For English, we obtained 7 million sentences with an average sentence length of 16.9 tokens. To get an impression of the quality, we parse the development set with those parsers. When the parsers agree, the parse trees have an accuracy of 97% (LAS), while the labelled scores of both parsers are around 91%. This indicates 102",
  "System Beam POS LAS UAS Zhang and Nivre (2011) 32 97.44 90.95 93.00 Bohnet and Kuhn (2012) 80 97.44 91.19 93.27 Martins et al. (2013) N/A 97.44 90.55 92.89 Zhang and McDonald (2014) N/A 97.44 91.02 93.22 Chen and Manning (2014) 1 N/A 89.60 91.80 Dyer et al. (2015) 1 97.30 90.90 93.10 Weiss et al. (2015) 8 97.44 92.05 93.99 Andor et al. (2016) 32 97.44 92.79 94.61 Dozat and Manning (2017) N/A N/A 94.6 95.8 Chen et al. (2012) Baseline @ 8 N/A N/A 92.10 Chen et al.",
  "(2012) Baseline @ 8 N/A N/A 92.10 Chen et al. (2012) DLM @ 8 N/A N/A 92.76 Our Baseline @ 40 97.33 92.44 93.38 Our Baseline 40 97.36 90.95 93.08 80 97.34 91.05 93.28 150 97.34 91.05 93.29 Our DLM 40 97.38 91.41** 93.59** 80 97.39 91.47** 93.65** 150 97.42 91.56** 93.74** Table 6.4: Comparing our DLM enhanced results with top performing parsers on English. (@ results on Yamada and Matsumoto (2003) conversion.) that parse trees where both parsers return the same tree have a higher accuracy. The DLMs extracted from 7 million higher quality sentences achieved a labelled accuracy of 91.56% which is 0.13% higher than the best result achieved by DLMs extracted from single parsed sentences.",
  "that parse trees where both parsers return the same tree have a higher accuracy. The DLMs extracted from 7 million higher quality sentences achieved a labelled accuracy of 91.56% which is 0.13% higher than the best result achieved by DLMs extracted from single parsed sentences. In total, the new model outperforms the baseline by 0.51%, with an error reduction rate of 5.7%. Evaluating on Test Sets. We apply the best settings tuned on the development sets to the test sets. The best setting for English is the unigram DLM derived from the double parsed sentences. Table 6.4 presents our results and top performing dependency parsers which were evaluated on the same English dataset. Our approach surpasses our baseline by 0.46/0.51% (LAS/UAS) and is only lower than the three best neural network systems. When using a larger beam of 150, our system achieved a more competitive result. To have an idea of the performance di\ufb00erence between our baseline and that of Chen et al.",
  "When using a larger beam of 150, our system achieved a more competitive result. To have an idea of the performance di\ufb00erence between our baseline and that of Chen et al. (2012), we include the accuracy of Mate parser on the same Yamada and Matsumoto (2003) conversion used by Chen et al. (2012). Our baseline is 0.64% higher 103",
  "System Beam POS LAS UAS Hatori et al. (2011) 64 93.94 N/A 81.33 Li et al. (2012) N/A 94.60 79.01 81.67 Chen et al. (2013) N/A N/A N/A 83.08 Chen et al. (2015) N/A 93.61 N/A 82.94 Our Baseline 40 93.99 78.49 81.52 80 94.02 78.48 81.58 150 93.98 78.96 82.11 Our DLM 40 94.27 79.42** 82.51** 80 94.39 79.79** 82.79** 150 94.40 80.21** 83.28** Table 6.5: Comparing our DLM enhanced results with top performing parsers on Chinese.",
  "DLM Baseline LAS UAS LAS UAS Weblogs 79.77** 85.88** 78.99 85.1 Newsgroups 76.21** 83.7** 75.3 82.88 Reviews 75.47* 83.01 75.07 82.68 Answers 73.49 81.62* 73.08 81.15 Conll 90.43** 92.8** 90.07 92.4 Table 6.6: The results of our DLM approach on English main evaluation corpus. than their enhanced result and is 1.28% higher than their baseline. This con\ufb01rms that our approach is evaluated on a much stronger parser. For Chinese, we extracted the DLMs from 10 million sentences parsed by the Mate parser and using the unigram, bigram and the trigram DLMs together. Table 6.5 shows the results of our approach and a number of the best Chinese parsers. Our system gained a large improvement of 0.93/0.98% for labelled and unlabelled attachment scores.",
  "Table 6.5 shows the results of our approach and a number of the best Chinese parsers. Our system gained a large improvement of 0.93/0.98% for labelled and unlabelled attachment scores. Our scores with the default beam size (40) are competitive and are 0.2% higher than the best reported result (Chen et al., 2013) when increasing the beam size to 150. Moreover, we gained improvements up to 0.42% for part-of-speech tagging on Chinese tests, and our tagging accuracies for English are constantly higher than the baselines. Results on English Main Evaluation Corpus. Finally, we apply our best English setting to our main evaluation corpus. We \ufb01rst extract new DLMs from the double parsed annotations of the LTH conversion, as LTH conversion is used in our main evaluation 104",
  "corpus. We then retain the parser with newly generated DLMs and apply the model to all \ufb01ve test domains (Conll, Weblogs, Newsgroups, Reviews and Answers). Table 6.6 shows the results of our best model and the baselines. Our newly trained model outperforms the baseline in all of the domains for both labelled and unlabelled accuracies. The largest improvements of 0.91% and 0.82% is achieved on Newsgroups domain for labelled and unlabelled accuracy respectively. On average our approach achieved 0.6% labelled and unlabelled improvements for four target domains. The enhanced model also improved the source domain accuracy by 0.36% and 0.4% for labelled and unlabelled scores respectively. 6.4 Analysis In this section, we analyse the improvements achieved by our DLM-enhanced models. We analyse both English and Chinese results. For English, we analyse the results of our main evaluation corpus, as the corpus contains both in-domain and out-of-domain data. This allows us to compare the source domain and target domain results in a uni\ufb01ed framework.",
  "We analyse both English and Chinese results. For English, we analyse the results of our main evaluation corpus, as the corpus contains both in-domain and out-of-domain data. This allows us to compare the source domain and target domain results in a uni\ufb01ed framework. We analyse the Conll in-domain test set and a combined out-of-domain dataset which consists of the Weblogs, Newsgroups, Reviews and Answers domain test sets. For Chinese, we analyse the in-domain test set to \ufb01nd out the sources of the improvements. We apply the token and sentence level analysis for both languages. The token level analysis includes the accuracy assessment of individual labels and the improvements comparison of known and unknown words. The sentence level analysis consists of assessments on four factors: sentence lengths, the number of unknown words, the number of prepositions and the number of conjunctions. For each of the factors, we group the sentences based on their properties assessed by each factor, we then calculate for each group the percentage of sentences that are improved, worsened and unchanged in accuracy. The improvements of each group can then be visualised by the gaps between improved and worsened sentences. 105",
  "NMOD P PMOD SBJ OBJ ROOT ADV NAME VC COORD TMP DEP CONJ LOC \u22120.5 0 0.5 1 1.5 Accuracy Change (%) a) In-domain Test Set NMOD P SBJ PMOD ROOT OBJ ADV COORD VC CONJ DEP PRD AMOD TMP \u22120.5 0 0.5 1 1.5 Accuracy Change (%) b) Out-of-domain Test Set Recall Precision F-score Figure 6.3: The English performance comparison between the DLM approach and the baseline on major labels. 6.4.1 English Analysis Token Level Analysis Individual Label Accuracy. We \ufb01rst analyse accuracy changes of most frequent labels of our in-domain and out-of-domain test sets. As we can see from Figure 6.3 the most frequent labels of in-domain data are slightly di\ufb00erent from that of out-of-domain data. Label NAME (name-internal link) and LOC (locative adverbial) that frequently showed in the in-domain set is less frequent in out-of-domain data. Instead, the out-of-domain data 106",
  "Confusion Baseline DLM NMOD \u2192ADV 105 104 NMOD \u2192OBJ 37 28 NMOD \u2192AMOD 41 33 NMOD \u2192DEP 112 109 NMOD \u2192NAME 61 66 NMOD \u2192APPO 34 34 NMOD \u2192PMOD 83 76 NMOD \u2192SBJ,TMP,CONJ 75 72 SBJ \u2192NMOD 33 29 SBJ \u2192PMOD 28 29 OBJ \u2192NMOD 48 41 PMOD \u2192NMOD 67 56 PMOD \u2192OBJ 34 37 PMOD \u2192APPO,TMP 46 45 ROOT \u2192NMOD 22 19 ADV \u2192LOC 33 28 ADV \u2192NMOD 103 93 ADV \u2192TMP 43 45 ADV \u2192MNR,APPO,AMOD 74 69 CONJ \u2192NMOD 41 35 DEP \u2192NMOD 74 82 NAME \u2192NMOD 55 52 TMP \u2192LOC 33 28 TMP \u2192NMOD 36 37 TMP \u2192ADV 84 80 LOC \u2192NMOD 58 47 LOC \u2192ADV 59 62 Table 6.7: The confusion matrix of dependency labels,",
  "7: The confusion matrix of dependency labels, compared between the DLM approach and the baseline on the in-domain test set. have more PRD (predicative complement) and AMOD (modi\ufb01er of adjective or adverbial) than in-domain data. In term of the improvements of individual labels, they both show improvements on most of the labels. They achieved improvements of at least 0.4% on label OBJ (object), COORD (coordination), CONJ (conjunct). More precisely, the DLM model achieved large improvements of more than 1% for in-domain data on CONJ (conjunct) and LOC (locative adverbial) and gained moderate improvements of more than 0.4% on OBJ (object), COORD (coordination) and ADV (adverbial). While for out-of-domain data, our approach gained more than 1% f-scores on OBJ (object) and PRD (predicative 107",
  "Confusion Baseline DLM NMOD \u2192ADV 235 219 NMOD \u2192LOC 162 156 NMOD \u2192HYPH 198 200 NMOD \u2192NAME 569 559 NMOD \u2192PMOD 187 174 NMOD \u2192HMOD 217 218 NMOD \u2192ROOT,OBJ,SBJ,DEP 491 470 P \u2192HYPH 162 170 P \u2192NAME,NMOD 233 221 SBJ \u2192NMOD 169 156 SBJ \u2192OBJ 132 107 OBJ \u2192NMOD 218 191 OBJ \u2192SBJ 117 106 PMOD \u2192NMOD 290 279 PMOD \u2192OBJ 122 108 ROOT \u2192NMOD 235 240 ROOT \u2192OBJ,",
  "SBJ 256 244 ADV \u2192MNR 150 156 ADV \u2192AMOD 152 134 ADV \u2192LOC 227 210 ADV \u2192NMOD 382 362 ADV \u2192TMP 182 204 ADV \u2192DIR 118 110 COORD \u2192NMOD 164 143 COORD \u2192ROOT 102 96 VC \u2192OPRD 114 83 CONJ \u2192NMOD 132 113 DEP \u2192ROOT 190 186 DEP \u2192OBJ 267 244 DEP \u2192SBJ 403 394 DEP \u2192NMOD 382 392 DEP \u2192TMP 176 183 DEP \u2192ADV 142 133 AMOD \u2192ADV 169 179 AMOD \u2192NMOD 265 270 AMOD \u2192HYPH 104 106 TMP \u2192ADV 280 283 TMP \u2192NMOD 133 122 PRD \u2192OBJ 854 834 PRD \u2192ADV,VC 255 238 Table 6.8: The confusion matrix of dependency labels, compared between the DLM approach and the baseline on the out-of-domain test sets. 108",
  "In-domain Test Set Out-of-domain Test Set DLM Baseline DLM Baseline Tokens LAS UAS LAS UAS Tokens LAS UAS LAS UAS Known 56640 90.4 92.8 90.1 92.4 101616 77.8 84.7 77.1 84.1 Unknown 1036 91.1 93.5 90.1 93.1 6055 62.0 72.3 61.4 71.9 All 57676 90.4 92.8 90.1 92.4 107671 76.9 84.0 76.3 83.4 Table 6.9: The English accuracy comparison between the DLM approach and the baseline on unknown words. 0 10 20 30 40 0 20 40 60 80 100 Number of Tokens Percentage (%) a) In-domain Test Set Better Worse No Change 0 10 20 30 400 20 40 60 80 100 Number of Tokens Number of Sentences a) In-domain Test Set Better Worse No Change No.",
  "of Sents 0 10 20 30 40 0 20 40 60 80 100 Number of Tokens Percentage (%) b) Out-of-domain Test Set Better Worse No Change 0 10 20 30 400 100 200 300 Number of Tokens Number of Sentences b) Out-of-domain Test Set Better Worse No Change No. of Sents Figure 6.4: The English comparison between the DLM approach and the baseline on di\ufb00erent number of tokens per sentence. complement), and improved three major modi\ufb01ers (NMOD, PMOD and AMOD), VC (verb chain), COORD (coordination), CONJ (conjunct) and DEP (unclassi\ufb01ed) for more than 0.4%. Table 6.7 and table 6.8 show the confusion matrices of the dependency labels on in-domain and out-of-domain test sets respectively. Unknown Words Accuracy. The unknown words rate for the in-domain test set is much lower than that of the out-of-domain one.",
  "Table 6.7 and table 6.8 show the confusion matrices of the dependency labels on in-domain and out-of-domain test sets respectively. Unknown Words Accuracy. The unknown words rate for the in-domain test set is much lower than that of the out-of-domain one. For the in-domain test set, only 1,000 tokens are unknown and surprisingly both the DLM model and the base model have a better accuracy on the unknown words. Our DLM model achieved labelled improvement of 1% on the unknown words which is 3 times than the gain for that of known words (0.3%). While the unlabelled improvement for both known and unknown words are exactly the same 0.4%. The larger improvement on out-of-domain data is achieved on the known words, with a 0.1%-0.2% small di\ufb00erence when compared to that of unknown words. A detailed comparison can be found in Table 6.9. 109",
  "0 1 2 3 0 20 40 60 80 100 Number of Unknown Words Percentage (%) a) In-domain Test Set Better Worse No Change 0 1 2 30 500 1,000 1,500 2,000 Number of Unknown Words Number of Sentences a) In-domain Test Set Better Worse No Change No. of Sents 0 1 2 3 4 0 20 40 60 80 100 Number of Unknown Words Percentage (%) b) Out-of-domain Test Set Better Worse No Change 0 1 2 3 40 1,000 2,000 3,000 Number of Unknown Words Number of Sentences b) Out-of-domain Test Set Better Worse No Change No. of Sents Figure 6.5: The English comparison between the DLM approach and the baseline on di\ufb00erent number of unknown words per sentence. Sentence Level Analysis Sentence Length. Figure 6.4 shows our analysis on sentence length. The analysis of in-domain data shows the DLM model mostly helped the sentences consisting of 10-20 tokens.",
  "Sentence Level Analysis Sentence Length. Figure 6.4 shows our analysis on sentence length. The analysis of in-domain data shows the DLM model mostly helped the sentences consisting of 10-20 tokens. For sentences shorter than 10 tokens the DLM model even shows some negative e\ufb00ects. We suggest this might because for in-domain parsing the base model is already able to achieve a high accuracy on short sentences thus they are harder to improve. When sentences are longer than 20 tokens, the rates for both improved and worsened sentences varies, but the overall positive and negative e\ufb00ects are similar. In terms of the analysis on out-of-domain set, positive e\ufb00ects of more than 4.5% can be found in sentences that have a length of 10-35 tokens, but not in sentences shorter than 10 tokens. Unknown Words. As stated before, the in-domain test set contains fewer unknown words. In fact, most of the sentences do not contain unknown words or only have one unknown word. The DLM model achieved 3% gain for the former and 3.9% gain for the latter.",
  "Unknown Words. As stated before, the in-domain test set contains fewer unknown words. In fact, most of the sentences do not contain unknown words or only have one unknown word. The DLM model achieved 3% gain for the former and 3.9% gain for the latter. For analysis of the out-of-domain data, our DLM model showed similar gains of around 5% for all the classes. Figure 6.5 shows our analysis on unknown words. Prepositions. The number of prepositions analysis for in-domain data does not show a clear picture of where the improvement comes from. The rates of sentences parsed better and sentences parsed worse varies, cf. Figure 6.6. While the analysis for out-of-domain 110",
  "0 2 4 6 0 20 40 60 80 100 Number of Prepositions Percentage (%) a) In-domain Test Set Better Worse No Change 0 2 4 60 200 400 600 800 1,000 Number of Prepositions Number of Sentences a) In-domain Test Set Better Worse No Change No. of Sents 0 2 4 6 0 20 40 60 80 100 Number of Prepositions Percentage (%) b) Out-of-domain Test Set Better Worse No Change 0 2 4 60 500 1,000 1,500 2,000 Number of Prepositions Number of Sentences b) Out-of-domain Test Set Better Worse No Change No. of Sents Figure 6.6: The English comparison between the DLM approach and the baseline on di\ufb00erent number of prepositions per sentence.",
  "of Sents Figure 6.6: The English comparison between the DLM approach and the baseline on di\ufb00erent number of prepositions per sentence. 0 1 2 0 20 40 60 80 100 Number of Conjunctions Percentage (%) a) In-domain Test Set Better Worse No Change 0 1 20 500 1,000 1,500 Number of Conjunctions Number of Sentences a) In-domain Test Set Better Worse No Change No. of Sents 0 1 2 3 0 20 40 60 80 100 Number of Conjunctions Percentage (%) b) Out-of-domain Test Set Better Worse No Change 0 1 2 30 1,000 2,000 3,000 4,000 Number of Conjunctions Number of Sentences b) Out-of-domain Test Set Better Worse No Change No. of Sents Figure 6.7: The English comparison between the DLM approach and the baseline on di\ufb00erent number of conjunctions per sentence.",
  "of Sents Figure 6.7: The English comparison between the DLM approach and the baseline on di\ufb00erent number of conjunctions per sentence. showed a clear increased gap between sentences have better accuracies and the sentences have lowered accuracies when the number of prepositions increases. The largest gap of 10% is achieved on sentences that have at least 5 prepositions. Conjunctions. Figure 6.7 shows our analysis of the di\ufb00erent number of conjunctions. For in-domain test set, the DLM model gained 4% for sentences do not have conjunctions and the number decreased when the number of conjunctions increases. For the out- of-domain test set the enhanced model gained around 4% for sentences have up to 2 conjunctions, after that, the gap increased to 13% for sentences have 3 conjunctions. 111",
  "\u201c 1 20p It2 3sbj seems 3 20obj to4 3adv me5 4pmod that 6 p 3prd a7 8nmod story 8 11sbj like 9 p 8nmod this 10 9pmod breaks 11 6sub just12 13pmod before 13 p 11tmp every14 17nmod important15 17nmod Cocom16 17nmod meeting17 13pmod ,18 20p \u201d19 20p said 20 0root a21 23nmod Washington22 23nmod lobbyist23 20sbj for 24 p 23nmod a25 26nmod number26 24pmod of 27 p 26nmod U.S.28 30nmod computer29 30nmod companies30 27pmod .",
  "31 20p The1 2nmod games 2 17sbj Bronx 3 4nmod children4 5sbj played 5 2nmod ( 6 7p holding7 2prn kids8 7obj down9 7prt and 10 c 7coord stripping11 10conj them12 11obj , 13 7p for 14 p 7dep example15 14pmod ) 16 7p seem 17 0root tame18 17prd by 19 p 17adv today20 23nmod \u2019s21 20suffix crack22 23nmod standards23 19pmod ,24 17p but 25 c 17coord Ms.26 27title Cunningham27 28sbj makes28 25conj it29 28obj all30 31dep sound31 28oprd like 32 p 31prd a33 35nmod great34 35nmod adventure35 32pmod .",
  "36 17p This1 3nmod role2 3nmod reversal3 4sbj holds 4 0root true5 4oprd ,6 4p as 7 p 8amod well8 4adv ,9 4p for 10 p 4adv his 11 16nmod three 12 16nmod liberal 13 16nmod and 14 c 13coord moderate 15 14conj allies 16 10pmod , 17 16p Justices 18 20title Thurgood 19 u 20name Marshall 20 16appo ,21 20p Harry22 23name Blackmun 23 u 20coord and 24 c 23coord John25 26name Stevens26 24conj .",
  "27 4p Harvard 1 6name Law 2 6name School 3 6name Professor 4 6name Laurence5 6name Tribe 6 7sbj says7 0root there8 9sbj is9 7obj a10 16nmod \u201c 11 14p generation12 14hmod -13 12hyph skipping 14 16nmod \u201d 15 14p \ufb02avor16 9prd to17 9adv current18 19nmod dissents 19 u 17pmod .20 7p While 1 p 12adv there2 3sbj are3 1sub some 4 9nmod popular 5 9nmod action 6 9nmod and 7 c 6coord drama 8 7conj series 9 3prd ,",
  "20 7p While 1 p 12adv there2 3sbj are3 1sub some 4 9nmod popular 5 9nmod action 6 9nmod and 7 c 6coord drama 8 7conj series 9 3prd , 10 12p few 11 12sbj boast 12 0root the13 15nmod high14 15nmod culture 15 12obj and 16 c 15coord classy 17 19nmod production 18 19nmod values 19 16conj one20 21sbj might 21 15nmod expect22 21vc .",
  "23 12p The1 2nmod question2 3sbj is3 0root ,4 3p if 5 p 33adv group6 7nmod con\ufb02icts7 9sbj still8 9tmp exist9 5sub ( 10 11p as 11 p 9prn undeniably 12 14adv they13 14sbj do14 11sub ) 15 11p , 16 5p and 17 c 5coord if 18 p 17conj Mr.19 20title Mason20 22nmod \u2019s21 20suffix type22 26sbj of 23 p 22nmod ethnic24 25nmod humor25 23pmod is26 18sub passe 27 u 26prd ,28 33p then29 33adv what30 32nmod other31 32nmod means32 35obj do 33 3prd we34 33sbj have35 33vc for 36 p 32nmod letting37 36pmod o\ufb0038 37prt steam39 37obj ?",
  "40 33p The1 3nmod main2 3nmod thing3 4sbj was 4 0root portfolio5 6nmod insurance6 4prd , 7 6p \u201d 8 6p a9 12nmod mechanical10 12nmod trading11 12nmod system 12 6appo intended 13 12appo to14 13oprd protect15 14im an16 17nmod investor17 15obj against 18 p 15adv losses19 18pmod . 20 4p \u201c 21 4p At 1 p 3prd stake2 1pmod is3 0root what4 16obj Mike5 6name Swavely6 16sbj ,7 6p Compaq8 10nmod \u2019s9 8suffix president10 6appo of 11 p 10nmod North12 13name America13 14nmod operations14 11pmod ,",
  "7 6p Compaq8 10nmod \u2019s9 8suffix president10 6appo of 11 p 10nmod North12 13name America13 14nmod operations14 11pmod ,15 6p calls16 28nmod \u201c 17 20p the 18 20nmod Holy 19 20name Grail 20 u 16oprd of 21 p 20nmod the22 24nmod computer23 24nmod industry 24 21pmod \u201d 25 20p \u2013 26 28p the27 28nmod search28 3sbj for 29 p 28nmod \u201c30 29p a31 33nmod real32 33nmod computer33 29pmod in 34 p 33loc a35 36nmod package36 34pmod so37 38amod small 38 36appo you39 40sbj can40 38amod take41 40vc it42 41obj everywhere43 41loc .44 3p \u201d45 3p Table 6.",
  "44 3p \u201d45 3p Table 6.10: The example sentences that have been improved by the DLM approach when compared to the baseline on in-domain test set. In which the dependency head/relation of a token are marked as the subscript, while the superscript is the index of token. The unknown words, prepositions and conjunctions are highlighted with u , p and c respec- tively. We highlight the di\ufb00erent levels of the improvements achieved by our dlm model on the dependency edges by di\ufb00erent colours. In which the blue colour means both head and label are corrected, the yellow colour means only the head is corrected and the green colour means only the label is corrected. 112",
  "2. 1 u 12dep To 2 12prp make3 2im the4 5nmod body5 3obj fully6 7mnr absorb 7 3oprd proteins 8 7obj better 9 7mnr ,10 12p we11 12sbj should12 0root eat13 12vc plenty14 13obj of 15 p 14nmod food 16 15pmod containing 17 16appo vitamin 18 u 19nmod B1 19 u 17obj and 20 c 19coord vitamin 21 u 22nmod C22 20conj .",
  "23 12p But 1 c 3dep everyone2 3sbj needs3 0root to4 3oprd recognize5 4im that 6 p 5obj Arlington7 9nmod \u2019s8 7suffix decision 9 16sbj not10 11adv to11 9nmod pursue12 11im a 13 15nmod balanced 14 15nmod community 15 12obj means 16 6sub that 17 p 16obj housing18 19sbj will 19 17sub end20 19vc up21 20prt somewhere22 20loc else23 22amod ,24 22p presumably 25 26pmod in 26 p 22amod outlying 27 28nmod counties 28 26pmod .29 3p In 1 p 5adv some2 3nmod respects3 1pmod ,",
  "24 22p presumably 25 26pmod in 26 p 22amod outlying 27 28nmod counties 28 26pmod .29 3p In 1 p 5adv some2 3nmod respects3 1pmod ,4 5p is5 0root n\u2019t6 5adv that7 5sbj essentially8 5adv what9 14obj No 10 11name Va11 12nmod jursidictions 12 u 13sbj are13 5prd doing 14 13vc -15 14p favoring 16 14adv non-residential17 18nmod development 18 16obj and 19 c 16coord letting20 19conj other21 22nmod jursidictions 22 u 20obj handle 23 20oprd the24 25nmod residential25 23obj ?26 5p Her 1 4nmod \u201c2 4p Rubble3 4name Division 4 6sbj \u201d5 4p mixes 6 0root such7 9nmod disparate8 9nmod materials 9 6obj as 10 p 9nmod ink11 13nmod -12 13nmod jet13 14nmod prints 14 10pmod pasted 15 u 14appo on 16 p 15loc board17 16pmod ,",
  "18 14p foam19 20nmod rubber20 14coord ,21 20p galvanized22 23nmod steel23 20coord ,24 23p concrete25 23coord ,26 25p steel27 28nmod rebar 28 u 25coord and 29 c 28coord bungee 30 u 31nmod cords 31 u 29conj . 32 6p they1 2sbj were2 0root convinced 3 2prd that 4 p 3amod if 5 p 20adv only6 8adv they7 8sbj could 8 5sub speak9 8vc to 10 p 9adv an11 12nmod American12 10pmod , 13 20p Abather 14 u 19nmod \u2019s15 14suffix charred16 19nmod and 17 c 16coord mangled 18 u 17conj \ufb02esh19 20sbj would 20 4sub make21 20vc their22 23nmod case23 21obj ,",
  "24 2p but 25 c 2coord they26 27sbj had27 25conj never28 27tmp gotten29 27vc past 30 p 29adv the31 34nmod Jordanian 32 u 34nmod security33 34nmod guards34 30pmod .35 2p - 1 3p Dr.2 3title Seuss 3 u 0root , 4 3p \u201c5 3p One6 7nmod Fish 7 3coord ,8 7p Two 9 10nmod Fish 10 7coord ,11 10p Red12 13nmod Fish 13 10coord ,",
  "4 3p \u201c5 3p One6 7nmod Fish 7 3coord ,8 7p Two 9 10nmod Fish 10 7coord ,11 10p Red12 13nmod Fish 13 10coord , 14 13p Blue15 16nmod Fish 16 13coord \u201d17 3p But 1 c 2dep let2 0root s3 2obj hope4 2oprd for 5 p 4prp their6 7nmod sake7 5pmod ( 8 7p and 9 c 7coord the10 11nmod sake 11 9conj of 12 p 11nmod all 13 15nmod space 14 15nmod lovers 15 12pmod out 16 15loc there 17 16amod ) 18 7p that 19 p 4obj they20 21sbj can21 19sub rede\ufb01ne22 21vc their23 24nmod image24 22obj and 25 c 22coord rekindle26 25conj the27 28nmod hope28 26obj of 29 p 28nmod space30 31nmod colonization 31 u 29pmod again32 26adv .",
  "33 2p Flex 1 u 0root your2 3nmod muscles 3 1obj , 4 1p friend 5 6nmod Libra 6 u 1voc , 7 1p and 8 c 1coord prepare9 8conj for 10 p 9adv a11 14nmod relatively12 13amod easy13 14nmod ride14 10pmod . 15 1p Bending 1 5sbj to 2 p 1dir the 3 4nmod right 4 2pmod indicates 5 0root :6 5p \u201d 7 u 5p I8 9sbj know 9 5obj the10 12nmod right11 12nmod way12 9obj to13 12nmod request14 13im You15 14obj .",
  "16 5p \u201d 17 u 5p SO 1 14adv , 2 14p IF 3 p 14adv YOU4 5sbj WANT 5 3sub A6 7nmod BURGER7 5obj AND 8 c 7coord FRIES 9 u 8conj , 10 14p WELL11 14dep ,12 14p IT 13 14sbj IS 14 0root OK 15 14prd . 16 14p Table 6.11: The example sentences that have been improved by the DLM approach when compared to the baseline on out-of-domain test sets. In which the dependency head/relation of a token are marked as the subscript, while the superscript is the index of token. The unknown words, prepositions and conjunctions are highlighted with u , p and c respectively. We highlight the di\ufb00erent levels of the improvements achieved by our dlm model on the dependency edges by di\ufb00erent colours. In which the blue colour means both head and label are corrected, the yellow colour means only the head is corrected and the green colour means only the label is corrected.",
  "In which the blue colour means both head and label are corrected, the yellow colour means only the head is corrected and the green colour means only the label is corrected. 113",
  "VMOD NMOD OBJ SBJ ROOT POBJ M DEG DEC LC \u22121 0 1 2 Accuracy Change (%) Recall Precision F-score Figure 6.8: The Chinese performance comparison between the DLM approach and the baseline on major labels. Example Sentences. Table 6.10 and table 6.11 show some example sentences that have been improved largely by our DLM-based approaches on the English in-domain and out-of-domain test sets respectively. 6.4.2 Analysis for Chinese Token Level Analysis Individual Label Accuracy. The Chinese dataset has a smaller label set than that of English, the 10 most frequent labels already cover 97% of the test set. We illustrate accuracy changes of individual labels in Figure 6.8. Our DLM model improved all major labels, the only exception is the label M (dependent of measure word, such as in words \u201c \u201d (19 years),\u201c \u201d is the dependent of the measure word \u201c \u201d) which showed a 1% decreasement in f-score.",
  "Our DLM model improved all major labels, the only exception is the label M (dependent of measure word, such as in words \u201c \u201d (19 years),\u201c \u201d is the dependent of the measure word \u201c \u201d) which showed a 1% decreasement in f-score. Our model achieved the largest improvement of 1.9% on POBJ (object of preposition), large improvements of more than 1% can be also found for label OBJ (object), DEG (dependent of associative DE), DEC (dependent of DE in a 114",
  "Confusion Baseline DLM VMOD \u2192POBJ 125 127 VMOD \u2192ROOT 305 305 VMOD \u2192NMOD 579 508 VMOD \u2192OBJ 411 384 VMOD \u2192SBJ 307 309 VMOD \u2192DEC,DEG 126 119 NMOD \u2192VMOD 369 374 NMOD \u2192SBJ 153 161 NMOD \u2192POBJ,DEC,M,OBJ 242 208 SBJ \u2192NMOD 218 214 SBJ \u2192VMOD 186 182 SBJ \u2192OBJ 104 93 SBJ \u2192POBJ 47 50 OBJ \u2192VMOD 282 279 OBJ \u2192NMOD 160 140 OBJ \u2192SBJ 107 108 OBJ \u2192POBJ,ROOT,DEG 192 175 ROOT \u2192VMOD 299 285 ROOT \u2192OBJ 74 77 POBJ \u2192NMOD,VMOD,OBJ,SBJ 270 241 M \u2192NMOD 58 83 DEG \u2192DEC,VMOD,OBJ 151 136 DEC \u2192NMOD,VMOD,OBJ,DEG 224 215 LC \u2192VMOD 31 26 Table 6.12: The confusion matrix of dependency labels,",
  "SBJ 270 241 M \u2192NMOD 58 83 DEG \u2192DEC,VMOD,OBJ 151 136 DEC \u2192NMOD,VMOD,OBJ,DEG 224 215 LC \u2192VMOD 31 26 Table 6.12: The confusion matrix of dependency labels, compared between the DLM approach and the baseline on Chinese test set. relative-clause) and LC (Child of localizer). For all other labels, moderate improvements of 0.2%-0.3% are achieved by our method. Table 6.12 shows the confusion matrix of the dependency labels on the Chinese test set. Unknown Words Accuracy. Table 6.13 shows our analysis of the unknown words accuracies. Our DLM model improved mainly the known words, with 1% large gains for both labelled and unlabelled accuracies. While our model did not improve the labelled accuracy of the unknown words, the model only achieved a small 0.2% improvement on the unlabelled score. This is an indication that the Chinese unknown words are very hard to improve without the manually annotated examples. 115",
  "DLM Baseline Tokens LAS UAS LAS UAS Known 39636 80.1 82.9 79.1 81.9 Unknown 3137 71.3 77.7 71.3 77.5 All 42773 79.4 82.5 78.5 81.6 Table 6.13: The Chinese accuracy comparison between the DLM approach and the baseline on unknown words. 0 10 20 30 40 0 20 40 60 80 100 Number of Tokens Percentage (%) Better Worse No Change 0 10 20 30 400 20 40 60 80 100 Number of Tokens Number of Sentences Better Worse No Change No. of Sents Figure 6.9: The Chinese comparison between the DLM approach and the baseline on di\ufb00erent number of tokens per sentence. Sentence Level Analysis Sentence Length. As shown in Figure 6.9, the Chinese sentences are evenly distributed in the classes of di\ufb00erent sentence length. Our model had limited e\ufb00ects on sentences less than 20 tokens but showed large gains on sentences longer than that.",
  "Sentence Level Analysis Sentence Length. As shown in Figure 6.9, the Chinese sentences are evenly distributed in the classes of di\ufb00erent sentence length. Our model had limited e\ufb00ects on sentences less than 20 tokens but showed large gains on sentences longer than that. The enhanced model achieved a gain of 5% on sentences of 20 tokens and the improvement increases until reaching the largest gain (24%) at the class of 35 tokens/sentence. Overall the major improvements of Chinese data were achieved on sentences that have at least 20 tokens. Unknown Words. We skip the unknown words factor for our Chinese sentence level analysis. This is due to the \ufb01nding from our token level analysis, which suggests our model did not improve the accuracy of the unknown words. Thus it is not necessary for 116",
  "0 1 2 3 0 20 40 60 80 100 Number of Prepositions Percentage (%) Better Worse No Change 0 1 2 30 200 400 600 800 1,000 Number of Prepositions Number of Sentences Better Worse No Change No. of Sents Figure 6.10: The Chinese comparison between the DLM approach and the baseline on di\ufb00erent number of prepositions per sentence. us to conduct further evaluation of this factor. Prepositions. As shown in Figure 6.10 most Chinese sentences have no or only single prepositions. The DLM model achieved an improvement of 3.6% for sentences do not contain a preposition. For sentences that contain single preposition, our model achieved 10.4% gain. The gain decreased largely when more prepositions are found in the sentences. Conjunctions. The curves of our analysis on the di\ufb00erent number of conjunctions (Figure 6.11) are nearly identical to that of prepositions.",
  "The gain decreased largely when more prepositions are found in the sentences. Conjunctions. The curves of our analysis on the di\ufb00erent number of conjunctions (Figure 6.11) are nearly identical to that of prepositions. For sentences that do not have conjunction a gain of 5.5% is achieved and the improvement for sentences containing a single conjunction is much larger (9.8%). The improvement dropped for sentences containing 2 conjunctions. 6.5 Chapter Summary In this chapter, we adapted the dependency language models (DLM) approach of Chen et al. (2012) to a strong transition-based parser. We integrated a small number of DLM- 117",
  "0 1 2 0 20 40 60 80 100 Number of Conjunctions Percentage (%) Better Worse No Change 0 1 20 500 1,000 1,500 Number of Conjunctions Number of Sentences Better Worse No Change No. of Sents Figure 6.11: The Chinese comparison between the DLM approach and the baseline on di\ufb00erent number of conjunctions per sentence. based features into the parser to allow the parser to explore DLMs extracted from a large auto-parsed corpus. We evaluated the parser with single and multiple DLMs extracted from corpora of di\ufb00erent size and quality to improve the in-domain accuracy of the English and Chinese texts. The English model enhanced by a unigram DLM extracted from double parsed high-quality sentences achieved statistically signi\ufb01cant improvements of 0.46% and 0.51% for labelled and unlabelled accuracies respectively. Our results outperform most of the latest systems and are close to the state-of-the-art.",
  "Our results outperform most of the latest systems and are close to the state-of-the-art. By using all unigram, bigram and trigram DLMs in our Chinese experiments, we achieved large improvements of 0.93% and 0.98% for both labelled and unlabelled scores. When increasing the beam size to 150, our system outperforms the best reported results by 0.2%. In addition to that, our approach gained an improvement of 0.4% on Chinese part-of-speech tagging. We further evaluate our approach on our main evaluation corpus. The method is tested on both in-domain and out-of-domain parsing. Our DLM-based approach achieved large improvement on all \ufb01ve domains evaluated (Conll, Weblogs, Newsgroups, Reviews, Answers). We achieved the labelled and unlabelled improvements of up to 0.91% and 0.82% on Newsgroups domain. On average we achieved 0.6% gains for both 118",
  "labelled and unlabelled scores on four out-of-domain test sets. We also improved the in-domain accuracy by 0.36% (LAS) and 0.4% (UAS). The analysis on our English main evaluation corpus suggests that the DLM model behaves di\ufb00erently on in-domain and out-of-domain parsing for a number of factors. Firstly, the DLM model achieved the largest improvement on label CONJ (conjunct) and LOC (locative adverbial) for in-domain parsing, while the largest improvement for out-of-domain dataset is contributed by OBJ (object) and PRD (predicative complement). Secondly, the DLM model improved more on unknown words for in-domain data but for out-of-domain text, DLM model delivered larger gains on known words. Thirdly, the analysis on sentence level shows that our model achieved most improvement on sentences of a length between 10 and 20, the range is wider (10-35) for out-of-domain data. We also analysed the Chinese results.",
  "Thirdly, the analysis on sentence level shows that our model achieved most improvement on sentences of a length between 10 and 20, the range is wider (10-35) for out-of-domain data. We also analysed the Chinese results. The analysis shows the improvement on Chinese data is mainly contributed by the objects (OBJ, POBJ), dependent of DE (DEC, DEG) and children of localizer (LC). The DLM model only shows a large improvement on the known words, it nearly does not a\ufb00ect the unknown words accuracy. The DLM model mostly helped the sentences that have at least 20 tokens. 119",
  "CHAPTER 7 CONCLUSIONS In this last chapter, we summarise the work of this thesis. In this thesis, we evaluated three semi-supervised techniques (co-training, self-training and dependency language models) on out-of-domain dependency parsing. The evaluations on various domains and languages demonstrated the e\ufb00ectiveness and robustness of all three techniques. We believe we have achieved the initial goals of this thesis. As introduced in Chapter 1, our goals for this thesis are to answer the following research questions: 1. Could the o\ufb00-the-shelf dependency parsers be successfully used in co-training for domain adaptation? 2. Would tri-training be more e\ufb00ective for out-of-domain parsing when o\ufb00-the-shelf dependency parsers are used? 3. How could self-training be e\ufb00ectively used in out-of-domain dependency parsing? 4. If self-training works for English dependency parsing, can it be adapted to other languages? 5. Can dependency language models be adapted to strong transition-based parsers? 6. Can dependency language models be used for out-of-domain parsing? 7.",
  "4. If self-training works for English dependency parsing, can it be adapted to other languages? 5. Can dependency language models be adapted to strong transition-based parsers? 6. Can dependency language models be used for out-of-domain parsing? 7. Quality or quantity of the auto-parsed data, which one is more important to the successful use of dependency language models? 120",
  "In the following sections, we answer all the questions in turns. Section 7.1 summarises our work on agreement based co-training and tri-training, we answer questions 1 and 2 in this section. In Section 7.2 we conclude our evaluations on English and multi- lingual con\ufb01dence-based self-training; questions 3 and 4 are answered in this section. We discuss our work on dependency language models in Section 7.3 and answer the last three questions. 7.1 Conclusions on Co-training In this section, we discuss our work on agreement based co-training (Chapter 3) and answer two research questions related to our co-training evaluation. 7.1.1 Could the o\ufb00-the-shelf dependency parsers be successfully used in co-training for domain adaptation? To answer this question we evaluated the agreement based co-training approach with four popular o\ufb00-the-shelf parsers (Malt parser (Nivre, 2009), MST parser (McDonald and Pereira, 2006), Mate parser (Bohnet et al., 2013) and Turbo parser (Martins et al., 2010)).",
  "We pair the Mate parser with the rest of three parsers to create three co-training settings. The unlabelled data is double parsed by the parser pairs and the sentences that are annotated the same by both parsers are used as additional training data. New models are created by retraining the Mate parser on training data boosted by di\ufb00erent parser pairs. All the enhanced models achieved large gains when compared to the baselines. The largest improvement of 1.1% is achieved by the Mate and Malt parsers. An additional 0.27% is achieved when we omit the short sentences from the additional training data. Our results demonstrated the e\ufb00ectiveness of the agreement-based co-training on out-of- domain parsing. The o\ufb00-the-shelf parsers have proved their suitability on this task. 121",
  "7.1.2 Would tri-training be more e\ufb00ective for out-of-domain parsing when o\ufb00-the-shelf dependency parsers are used? The tri-training di\ufb00erent from the normal co-training by retraining the evaluation learner on additional training data agreed by other two learners. In total, three learners are required, to form the tri-training we used the Malt, MST parsers as the source learners and the Mate parser is used as the evaluation learner. The tri-trained model outperforms the best normal co-training setting on all the experiments, thus is more e\ufb00ective. A large 1.6% improvement is achieved on the development set when compared to the baseline. We further evaluate the tri-training approach on four test domains. It achieved largest labelled and unlabelled improvements of 1.8% and 0.58% respectively. On average it achieved 1.5% (LAS) and 0.4% (UAS) for all four test domains. Our results not only con\ufb01rmed the tri-training is more e\ufb00ective than normal co-training but also demonstrated the merit of tri-training on multiple tested domains.",
  "Our results not only con\ufb01rmed the tri-training is more e\ufb00ective than normal co-training but also demonstrated the merit of tri-training on multiple tested domains. 7.2 Conclusions on Self-training In this section, we discuss our work on con\ufb01dence-based self-training (Chapter 4 and 5) and answer two relevant questions. 7.2.1 How could self-training be e\ufb00ectively used in out-of-domain dependency parsing? We start with the hypothesis that the selection of high-quality auto-annotated data is the pre-condition of the successful use of self-training on dependency parsing. To obtain the high-quality additional training data we introduced two con\ufb01dence-based methods that are able to detect high accuracy annotations. We compared our con\ufb01dence-based self-training with the random selection-based self-training and the baseline. The random selection-based self-training is not able to gain statistically signi\ufb01cant improvement which 122",
  "is in line with previous work. Both con\ufb01dence-based methods achieved large improvements on all three web domain test sets and the additional Chemical domain evaluation. For web domain, our method achieved up to 0.8% gains for both labelled and unlabelled scores. On average both methods improved the baseline by 0.6% (LAS and UAS). The evaluation on the Chemical domain resulted in larger improvements of up to 1.4% (LAS) and 1.2% (UAS). The evaluation on di\ufb00erent domains con\ufb01rmed our hypothesis. 7.2.2 If self-training works for English dependency parsing, can it be adapted to other languages? We demonstrated the e\ufb00ectiveness of our con\ufb01dence-based self-training for English depen- dency parsing in the last question, cf. Section 7.2.1. To assess the multi-lingual capacity of our con\ufb01dence-based self-training, we evaluated it on nine languages (Arabic, Basque, French, German, Hebrew, Hungarian, Korean, Polish, Swedish) corpora.",
  "Section 7.2.1. To assess the multi-lingual capacity of our con\ufb01dence-based self-training, we evaluated it on nine languages (Arabic, Basque, French, German, Hebrew, Hungarian, Korean, Polish, Swedish) corpora. We evaluated on a uni\ufb01ed setting for all the languages, the results show our method is able to achieve statistically signi\ufb01cant improvements on \ufb01ve languages (Basque, German, Hungarian, Korean and Swedish). Our self-training approach achieved the largest labelled and unlabelled accuracy gain of 2.14% and 1.79% on Korean. The average im- provements achieved by our method on \ufb01ve languages are 0.87% (LAS) and 0.78% (UAS). We further analyse the result of a negative e\ufb00ect (French) introduced by our method to assess the reason why self-training did not work. The analysis suggests the large di\ufb00erence between unlabelled data and the training data is likely to be the main reason disquali\ufb01es the self-training.",
  "The analysis suggests the large di\ufb00erence between unlabelled data and the training data is likely to be the main reason disquali\ufb01es the self-training. Overall, our evaluations show that con\ufb01dence-based self-training can be successfully applied to multi-lingual dependency parsing. 7.3 Conclusions on Dependency Language Models In this section, we discuss our \ufb01ndings on dependency language models (Chapter 6) and answer the last three research questions. 123",
  "7.3.1 Can dependency language models be adapted to strong transition-based parsers? To answer this question, we applied the dependency language models (DLM) to the Mate transition-based parser. We successfully integrated the DLM-based features to the transition-based parser by using a modi\ufb01ed version of Chen et al. (2012)\u2019s original templates for the graph-based parser. The evaluations on English and Chinese in-domain parsing con\ufb01rmed the e\ufb00ectiveness of dependency language models on the Mate parser. We improved a strong English baseline by 0.46% and 0.51% for labelled and unlabelled accuracies respectively. For Chinese, we achieved the state-of-the-art accuracy and gained large improvements of 0.93% (LAS) and 0.98% (UAS). The results show a strong evidence that dependency language models can be adapted successfully to a strong transition-based parser. 7.3.2 Can dependency language models be used for out-of-domain parsing? To address this question, we applied our approach to four web domain texts (Weblogs, Newsgroups, Reviews, Answers).",
  "7.3.2 Can dependency language models be used for out-of-domain parsing? To address this question, we applied our approach to four web domain texts (Weblogs, Newsgroups, Reviews, Answers). We achieved the largest labelled and unlabelled improvements of 0.91% and 0.82% on Newsgroups domain. And on average we achieved 0.6% gains for both labelled and unlabelled scores. The evaluations on multiple domains advised that DLM-based approach is an e\ufb00ective technique for domain adaptation tasks. 7.3.3 Quality or quantity of the auto-parsed data, which one is more important to the successful use of dependency language models? The evaluations on both English and Chinese suggest no large additional gains can be achieved by using DLMs extracted from corpus larger than 5 million sentences. In fact, 124",
  "in most of the cases, the best model is achieved by using DLMs extracted from 5 million sentences. The evaluation of using DLMs extracted from high-quality data, on the other hand, surpasses the best results achieved by normal quality DLMs. Overall, the quality of the auto-labelled data used to generate DLMs is more important than the quantity. 7.4 Chapter Summary In this chapter, we summarised our work of this thesis by answering seven research ques- tions that we introduced in Chapter 1. We successfully answered all the questions using our \ufb01ndings in the previous chapters. 125",
  "LIST OF REFERENCES [Andor et al.2016] Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman Ganchev, Slav Petrov, and Michael Collins. 2016. Globally normal- ized transition-based neural networks. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2442\u20132452, Berlin, Germany. Association for Computational Linguistics. [Bengio et al.2003] Yoshua Bengio, R\u00b4ejean Ducharme, Pascal Vincent, and Christian Jan- vin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137\u20131155. [Bj\u00a8orkelund et al.2014] Anders Bj\u00a8orkelund, \u00a8Ozlem C\u00b8etino\u02d8glu, Agnieszka Fale\u00b4nska, Rich\u00b4ard Farkas, Thomas Mueller, Wolfgang Seeker, and Zsolt Sz\u00b4ant\u00b4o. 2014.",
  "2014. The IMS-Wroc law- Szeged-CIS entry at the SPMRL 2014 shared task: Reranking and morphosyntax meet unlabeled data. In Proceedings of the Shared Task on Statistical Parsing of Morphologically Rich Languages, Dublin, Ireland. Dublin City University. [Blum and Mitchell1998] Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, pages 92\u2013100, Madison, Wisconsin, USA. Association for Computing Machinery. [Bohnet and Kuhn2012] Bernd Bohnet and Jonas Kuhn. 2012. The best of both worlds \u2013 a graph-based completion model for transition-based parsers. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 77\u201387, Avignon, France. Association for Computational Linguistics. [Bohnet and Nivre2012] Bernd Bohnet and Joakim Nivre. 2012.",
  "Association for Computational Linguistics. [Bohnet and Nivre2012] Bernd Bohnet and Joakim Nivre. 2012. A transition-based sys- tem for joint part-of-speech tagging and labeled non-projective dependency parsing. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Lan- guage Processing and Computational Natural Language Learning, pages 1455\u20131465, Jeju Island, Korea. Association for Computational Linguistics. [Bohnet et al.2013] Bernd Bohnet, Joakim Nivre, Igor Boguslavsky, Richard Farkas, Filip Ginter, and Jan Hajic. 2013. Joint morphological and syntactic analysis for richly in\ufb02ected languages. Transactions of the Association of Computational Linguistics, 1:415\u2013428. [Bohnet2010] Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 89\u201397, Beijing, China. Association for Computational Linguistics. 126",
  "[Brown et al.1992] Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4):467\u2013479. [Carreras2007] Xavier Carreras. 2007. Experiments with a higher-order projective depen- dency parser. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 957\u2013961, Prague, Czech Republic. Association for Computational Linguistics. [Cerisara2014] Christophe Cerisara. 2014. Semi-supervised experiments at LORIA for the SPMRL 2014 shared task. In Proceedings of the Shared Task on Statistical Parsing of Morphologically Rich Languages, Dublin, Ireland. Dublin City University. [Charniak and Johnson2005] Eugene Charniak and Mark Johnson. 2005. Coarse-to-\ufb01ne n-best parsing and maxent discriminative reranking.",
  "Dublin City University. [Charniak and Johnson2005] Eugene Charniak and Mark Johnson. 2005. Coarse-to-\ufb01ne n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 173\u2013180, Ann Arbor, Michigan, USA. Association for Computational Linguistics. [Charniak1997] Eugene Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proceedings of the Fourteenth National Conference on Arti\ufb01cial Intelligence and Ninth Conference on Innovative Applications of Arti\ufb01cial Intelligence, pages 598\u2013603, Providence, Rhode Island. Association for the Advancement of Arti\ufb01- cial Intelligence. [Charniak2000] Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Pro- ceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics, pages 132\u2013139, Seattle, Washington, USA. Association for Computational Linguistics.",
  "2000. A maximum-entropy-inspired parser. In Pro- ceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics, pages 132\u2013139, Seattle, Washington, USA. Association for Computational Linguistics. [Chelba et al.2013] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, and Philipp Koehn. 2013. One billion word benchmark for measuring progress in statistical language modeling. Technical Report 41880, Google. [Chen and Manning2014] Danqi Chen and Christopher Manning. 2014. A fast and accu- rate dependency parser using neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 740\u2013750, Doha, Qatar. Association for Computational Linguistics. [Chen et al.2008] Wenliang Chen, Youzheng Wu, and Hitoshi Isahara. 2008. Learning reliable information for dependency parsing adaptation.",
  "Association for Computational Linguistics. [Chen et al.2008] Wenliang Chen, Youzheng Wu, and Hitoshi Isahara. 2008. Learning reliable information for dependency parsing adaptation. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 113\u2013120, Manchester, UK. Association for Computational Linguistics. [Chen et al.2012] Wenliang Chen, Min Zhang, and Haizhou Li. 2012. Utilizing dependency language models for graph-based dependency parsing models. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 213\u2013222, Jeju Island, Korea. Association for Computational Linguistics. 127",
  "[Chen et al.2013] Wenliang Chen, Min Zhang, and Yue Zhang. 2013. Semi-supervised feature transformation for dependency parsing. In Proceedings of the 2013 Confer- ence on Empirical Methods in Natural Language Processing, pages 1303\u20131313, Seattle, Washington, USA. Association for Computational Linguistics. [Chen et al.2015] Wenliang Chen, Min Zhang, and Yue Zhang. 2015. Distributed feature representations for dependency parsing. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 23(3):451\u2013460. [Chrupala2011] Grzegorz Chrupala. 2011. E\ufb03cient induction of probabilistic word classes with LDA. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 363\u2013372, Chiang Mai, Thailand. Asian Federation of Natural Lan- guage Processing. [Collins and Singer1999] Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classi\ufb01cation.",
  "Asian Federation of Natural Lan- guage Processing. [Collins and Singer1999] Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classi\ufb01cation. In Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 100\u2013110, College Park, Maryland, USA. Association for Computational Linguistics. [Collins1999] Micheal Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania. [Crammer et al.2006] Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. J. Mach. Learn. Res., 7:551\u2013585, December. [Crammer et al.2009] Koby Crammer, Alex Kulesza, and Mark Dredze. 2009. Adaptive regularization of weight vectors.",
  "J. Mach. Learn. Res., 7:551\u2013585, December. [Crammer et al.2009] Koby Crammer, Alex Kulesza, and Mark Dredze. 2009. Adaptive regularization of weight vectors. In Advances in Neural Information Processing Sys- tems, volume 22, pages 414\u2013422, Vancouver, Canada. Curran Associates, Inc. [de Marne\ufb00e et al.2006] Marie-Catherine de Marne\ufb00e, Bill Maccartney, and Christopher Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the Fifth International Conference on Language Resources and Eval- uation, Genoa, Italy. European Language Resources Association. [Dozat and Manning2017] Timothy Dozat and Christopher Manning. 2017. Deep bia\ufb03ne attention for neural dependency parsing. In Proceedings of the 5th International Con- ference on Learning Representations, Toulon, France. [Dredze et al.2008] Mark Dredze, Koby Crammer, and Fernando Pereira.",
  "Deep bia\ufb03ne attention for neural dependency parsing. In Proceedings of the 5th International Con- ference on Learning Representations, Toulon, France. [Dredze et al.2008] Mark Dredze, Koby Crammer, and Fernando Pereira. 2008. Con\ufb01dence-weighted linear classi\ufb01cation. In Proceedings of the 25th international conference on Machine learning, pages 264\u2013271, Helsinki, Finland. Association for Computing Machinery. [Dyer et al.2015] Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and A. Noah Smith. 2015. Transition-based dependency parsing with stack long short- term memory. In Proceedings of the 53rd Annual Meeting of the Association for Com- putational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 334\u2013343, Beijing, China. Association for Computational Linguistics. 128",
  "[Francis and Kucera1979] W. N. Francis and H. Kucera. 1979. Brown corpus manual: Manual of information to accompany a standard corpus of present-day edited american english, for use with digital computers. Technical report, Department of Linguistics, Brown University. [Goldman and Zhou2000] Sally A. Goldman and Yan Zhou. 2000. Enhancing supervised learning with unlabeled data. In Proceedings of the Seventeenth International Confer- ence on Machine Learning, pages 327\u2013334, San Francisco, California, USA. Morgan Kaufmann Publishers Inc. [Goutam and Ambati2011] Rahul Goutam and Ram Bharat Ambati. 2011. Exploring self training for Hindi dependency parsing. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 1452\u20131456, Chiang Mai, Thailand. Asian Federation of Natural Language Processing.",
  "2011. Exploring self training for Hindi dependency parsing. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 1452\u20131456, Chiang Mai, Thailand. Asian Federation of Natural Language Processing. [Haji\u02c7c et al.2009] Jan Haji\u02c7c, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawa- hara, Ant`onia Maria Mart\u00b4\u0131, Llu\u00b4\u0131s M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad\u00b4o, Jan \u02c7Step\u00b4anek, Pavel Stra\u02c7n\u00b4ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL-2009 shared task: Syntactic and semantic dependencies in multi- ple languages. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task, pages 1\u201318, Boulder, Colorado. Association for Com- putational Linguistics. [Hatori et al.2011] Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun\u2019ichi Tsujii.",
  "Association for Com- putational Linguistics. [Hatori et al.2011] Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun\u2019ichi Tsujii. 2011. Incremental joint POS tagging and dependency parsing in Chinese. In Pro- ceedings of 5th International Joint Conference on Natural Language Processing, pages 1216\u20131224, Chiang Mai, Thailand. Asian Federation of Natural Language Processing. [Huang et al.2009] Liang Huang, Wenbin Jiang, and Qun Liu. 2009. Bilingually- constrained (monolingual) shift-reduce parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1222\u20131231, Singapore. Association for Computational Linguistics. [Johansson and Nugues2007] Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for English. In Proceedings of the 16th Nordic Conference of Computational Linguistics, pages 105\u2013112, Tartu, Estonia. University of Tartu.",
  "[Johansson and Nugues2007] Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for English. In Proceedings of the 16th Nordic Conference of Computational Linguistics, pages 105\u2013112, Tartu, Estonia. University of Tartu. [Kahane et al.1998] Sylvain Kahane, Alexis Nasr, and Owen Rambow. 1998. Pseudo- projectivity: A polynomially parsable non-projective dependency grammar. In Pro- ceedings of the 17th International Conference on Computational Linguistics, pages 646\u2013652, Montreal, Quebec, Canada. Association for Computational Linguistics. [Kawahara and Uchimoto2008] Daisuke Kawahara and Kiyotaka Uchimoto. 2008. Learn- ing reliability of parses for domain adaptation of dependency parsing. In Proceedings of the Third International Joint Conference on Natural Language Processing, pages 709\u2013714, Hyderabad, India. Association for Computational Linguistics. 129",
  "[Khan et al.2013] Mohammad Khan, Markus Dickinson, and Sandra K\u00a8ubler. 2013. To- wards domain adaptation for parsing web data. In Proceedings of the International Conference Recent Advances in Natural Language Processing, pages 357\u2013364, Hissar, Bulgaria. INCOMA Ltd. [Klein and D. Manning2003] Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 423\u2013430, Sapporo, Japan. Association for Compu- tational Linguistics. [Koo and Collins2010] Terry Koo and Michael Collins. 2010. E\ufb03cient third-order depen- dency parsers. In Proceedings of the 48th Annual Meeting of the Association for Com- putational Linguistics, pages 1\u201311, Uppsala, Sweden. Association for Computational Linguistics. [Koo et al.2008] Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi- supervised dependency parsing.",
  "Association for Computational Linguistics. [Koo et al.2008] Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi- supervised dependency parsing. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 595\u2013 603, Columbus, Ohio, USA. Association for Computational Linguistics. [Le Roux et al.2012] Joseph Le Roux, Jennifer Foster, Joachim Wagner, Rasul Samad Zadeh Kaljahi, and Anton Bryl. 2012. DCU-Paris13 systems for the SANCL 2012 shared task. In Proceedings of the First Workshop on Syntactic Analysis of Non- Canonical Language, Montreal, Quebec, Canada. [Li et al.2012] Zhenghua Li, Min Zhang, Wanxiang Che, and Ting Liu. 2012. A separately passive-aggressive training algorithm for joint POS tagging and dependency parsing. In Proceedings of the 24th International Conference on Computational Linguistics, pages 1681\u20131698, Mumbai, India. Association for Computational Linguistics. [Liang2005] Percy Liang.",
  "2012. A separately passive-aggressive training algorithm for joint POS tagging and dependency parsing. In Proceedings of the 24th International Conference on Computational Linguistics, pages 1681\u20131698, Mumbai, India. Association for Computational Linguistics. [Liang2005] Percy Liang. 2005. Semi-supervised learning for natural language. Master\u2019s thesis, Massachusetts Institute of Technology. [Malouf and Noord2004] Robert Malouf and Gertjan Noord. 2004. Wide coverage parsing with stochastic attribute value grammars. In Proceedings of the IJCNLP-04 Workshop on Beyond Shallow Analyses - Formalisms and statistical modeling for deep analyses, Hainan, China. Asian Federation of Natural Language Processing. [Martins et al.2010] Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar, and Mario Figueiredo. 2010. Turbo parsers: Dependency parsing by approximate variational inference. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 34\u201344, Cambridge, Massachusetts, USA. Association for Computational Linguistics.",
  "2010. Turbo parsers: Dependency parsing by approximate variational inference. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 34\u201344, Cambridge, Massachusetts, USA. Association for Computational Linguistics. [Martins et al.2013] Andre Martins, Miguel Almeida, and A. Noah Smith. 2013. Turning on the Turbo: Fast third-order non-projective Turbo parsers. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 617\u2013622, So\ufb01a, Bulgaria. Association for Computational Linguistics. 130",
  "[McClosky et al.2006a] David McClosky, Eugene Charniak, and Mark Johnson. 2006a. E\ufb00ective self-training for parsing. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Lin- guistics, pages 152\u2013159, New York, USA. Association for Computational Linguistics. [McClosky et al.2006b] David McClosky, Eugene Charniak, and Mark Johnson. 2006b. Reranking and self-training for parser adaptation. In Proceedings of the 21st Inter- national Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 337\u2013344, Sydney, Australia. Associ- ation for Computational Linguistics. [McDonald and Pereira2006] Ryan McDonald and Fernando Pereira. 2006. Online learn- ing of approximate dependency parsing algorithms. In Proceedings of the 11th Confer- ence of the European Chapter of the Association for Computational Linguistics, pages 81\u201388, Trento, Italy. Association for Computational Linguistics.",
  "2006. Online learn- ing of approximate dependency parsing algorithms. In Proceedings of the 11th Confer- ence of the European Chapter of the Association for Computational Linguistics, pages 81\u201388, Trento, Italy. Association for Computational Linguistics. [McDonald et al.2005] Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 91\u201398, Ann Arbor, Michigan, USA. Association for Computational Linguistics. [Mejer and Crammer2012] Avihai Mejer and Koby Crammer. 2012. Are you sure? con\ufb01- dence in prediction of dependency tree edges. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, pages 573\u2013576, Montreal, Quebec, Canada. Association for Computational Linguistics.",
  "con\ufb01- dence in prediction of dependency tree edges. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, pages 573\u2013576, Montreal, Quebec, Canada. Association for Computational Linguistics. [Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Je\ufb00 Dean. 2013. Distributed representations of words and phrases and their composi- tionality. In Advances in neural information processing systems, volume 26, pages 3111\u20133119. Curran Associates, Inc. [Mirroshandel et al.2012] Abolghasem Seyed Mirroshandel, Alexis Nasr, and Joseph Le Roux. 2012. Semi-supervised dependency parsing using lexical a\ufb03nities. In Pro- ceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 777\u2013785, Jeju Island, Korea. Association for Computational Linguistics.",
  "2012. Semi-supervised dependency parsing using lexical a\ufb03nities. In Pro- ceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 777\u2013785, Jeju Island, Korea. Association for Computational Linguistics. [Nivre et al.2007a] Joakim Nivre, Johan Hall, Sandra K\u00a8ubler, Ryan McDonald, Jens Nils- son, Sebastian Riedel, and Deniz Yuret. 2007a. The CoNLL 2007 shared task on dependency parsing. In Proceedings of the 2007 Joint Conference on Empirical Meth- ods in Natural Language Processing and Computational Natural Language Learning, pages 915\u2013932, Prague, Czech Republic. Association for Computational Linguistics. [Nivre et al.2007b] Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G\u00a8ul\u00b8sen Eryi\u02c7git, Sandra K\u00a8ubler, Svetoslav Marinov, and Erwin Marsi. 2007b. Maltparser: A language-independent system for data-driven dependency parsing.",
  "2007b. Maltparser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95\u2013135. 131",
  "[Nivre2004] Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together, pages 50\u201357. Association for Computational Linguistics. [Nivre2007] Joakim Nivre. 2007. Incremental non-projective dependency parsing. In Proceedings of the Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, pages 396\u2013403, Rochester, New York, USA. Association for Computational Linguistics. [Nivre2009] Joakim Nivre. 2009. Non-projective dependency parsing in expected linear time. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing, pages 351\u2013359, Singapore. Association for Computational Linguistics. [P. Marcus et al.1993] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank.",
  "Association for Computational Linguistics. [P. Marcus et al.1993] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313\u2013330. [Pekar et al.2014] Viktor Pekar, Juntao Yu, Mohab Elkaref, and Bernd Bohnet. 2014. Exploring options for fast domain adaptation of dependency parsers. In Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages, pages 54\u201365, Dublin, Ireland. Dublin City University. [Pennington et al.2014] Je\ufb00rey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1532\u20131543, Doha, Qatar. Association for Computational Linguistics.",
  "2014. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1532\u20131543, Doha, Qatar. Association for Computational Linguistics. [Petrov and Klein2007] Slav Petrov and Dan Klein. 2007. Improved inference for un- lexicalized parsing. In Proceedings of the Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Lin- guistics, pages 404\u2013411, Rochester, New York, USA. Association for Computational Linguistics. [Petrov and McDonald2012] Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 shared task on parsing the web. In Proceedings of the First Workshop on Syntactic Analysis of Non-Canonical Language, Montreal, Quebec, Canada. [Plank and S\u00f8gaard2013] Barbara Plank and Anders S\u00f8gaard. 2013. Experiments in newswire-to-law adaptation of graph-based dependency parsers. In Evaluation of Nat- ural Language and Speech Tools for Italian, pages 70\u201376, Berlin, Heidelberg.",
  "2013. Experiments in newswire-to-law adaptation of graph-based dependency parsers. In Evaluation of Nat- ural Language and Speech Tools for Italian, pages 70\u201376, Berlin, Heidelberg. Springer. [Plank and van Noord2011] Barbara Plank and Gertjan van Noord. 2011. E\ufb00ective mea- sures of domain similarity for parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1566\u20131576, Portland, Oregon, USA. Association for Computational Linguistics. [Plank2011] Barbara Plank. 2011. Domain Adaptation for Parsing. Ph.D. thesis, Univer- sity of Groningen. 132",
  "[Pradhan et al.2011] Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, Martha Palmer, Ralph Weischedel, and Nianwen Xue. 2011. CoNLL-2011 shared task: Modeling unrestricted coreference in OntoNotes. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, pages 1\u201327, Portland, Oregon, USA. Association for Computational Linguistics. [Pradhan et al.2012] Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning: Shared Task, pages 1\u201340, Jeju Island, Korea. Association for Computational Linguistics. [Pyysalo et al.2006] Sampo Pyysalo, Tapio Salakoski, Sophie Aubin, and Adeline Nazarenko. 2006.",
  "Association for Computational Linguistics. [Pyysalo et al.2006] Sampo Pyysalo, Tapio Salakoski, Sophie Aubin, and Adeline Nazarenko. 2006. Lexical adaptation of link grammar to the biomedical sublanguage: A comparative evaluation of three approaches. BMC Bioinformatics, 7(Suppl 3). [Reichart and Rappoport2007] Roi Reichart and Ari Rappoport. 2007. Self-training for enhancement and domain adaptation of statistical parsers trained on small datasets. In Proceedings of the 45th Annual Meeting of the Association of Computational Lin- guistics, pages 616\u2013623, Prague, Czech Republic. Association for Computational Lin- guistics. [Sagae and Tsujii2007] Kenji Sagae and Jun\u2019ichi Tsujii. 2007. Dependency parsing and domain adaptation with LR models and parser ensembles. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Compu- tational Natural Language Learning, pages 1044\u20131050, Prague, Czech Republic.",
  "2007. Dependency parsing and domain adaptation with LR models and parser ensembles. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Compu- tational Natural Language Learning, pages 1044\u20131050, Prague, Czech Republic. Asso- ciation for Computational Linguistics. [Sagae2010] Kenji Sagae. 2010. Self-training without reranking for parser domain adapta- tion and its impact on semantic role labeling. In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, pages 37\u201344, Uppsala, Sweden. Association for Computational Linguistics. [Sarkar2001] Anoop Sarkar. 2001. Applying co-training methods to statistical parsing. In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics, pages 1\u20138, Pittsburgh, Pennsylvania, USA. Association for Computational Linguistics.",
  "2001. Applying co-training methods to statistical parsing. In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics, pages 1\u20138, Pittsburgh, Pennsylvania, USA. Association for Computational Linguistics. [Seddah et al.2013] Djam\u00b4e Seddah, Reut Tsarfaty, Sandra K\u00a8ubler, Marie Candito, D. Jinho Choi, Rich\u00b4ard Farkas, Jennifer Foster, Iakes Goenaga, Koldo Gojenola Gal- letebeitia, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann, Wolfgang Maier, Joakim Nivre, Adam Przepi\u00b4orkowski, Ryan Roth, Wolfgang Seeker, Yannick Versley, Veronika Vincze, Marcin Woli\u00b4nski, Alina Wr\u00b4oblewska, and Villemonte Eric de la Clergerie. 2013. Overview of the SPMRL 2013 shared task: A cross-framework evaluation of parsing morphologically rich languages.",
  "2013. Overview of the SPMRL 2013 shared task: A cross-framework evaluation of parsing morphologically rich languages. In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 146\u2013182, Seattle, Washington, USA. Association for Computational Linguistics. 133",
  "[Seddah et al.2014] Djam\u00b4e Seddah, Sandra K\u00a8ubler, and Reut Tsarfaty. 2014. Introducing the SPMRL 2014 shared task on parsing morphologically-rich languages. In Proceed- ings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Lan- guages and Syntactic Analysis of Non-Canonical Languages, pages 103\u2013109, Dublin, Ireland. Dublin City University. [Shen et al.2008] Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to- dependency machine translation algorithm with a target dependency language model. In Proceedings of the 46th Annual Meeting of the Association for Computational Lin- guistics: Human Language Technologies, pages 577\u2013585, Columbus, Ohio, USA. As- sociation for Computational Linguistics. [S\u00f8gaard and Plank2012] Anders S\u00f8gaard and Barbara Plank. 2012. Parsing the web as covariate shift. In Proceedings of the First Workshop on Syntactic Analysis of Non- Canonical Language, Montreal, Quebec, Canada.",
  "[S\u00f8gaard and Plank2012] Anders S\u00f8gaard and Barbara Plank. 2012. Parsing the web as covariate shift. In Proceedings of the First Workshop on Syntactic Analysis of Non- Canonical Language, Montreal, Quebec, Canada. [S\u00f8gaard and Rish\u00f8j2010] Anders S\u00f8gaard and Christian Rish\u00f8j. 2010. Semi-supervised dependency parsing using generalized tri-training. In Proceedings of the 23rd Inter- national Conference on Computational Linguistics, pages 1065\u20131073, Beijing, China. Association for Computational Linguistics. [Steedman et al.2003] Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen Clark, Re- becca Hwa, Julia Hockenmaier, Paul Ruhlen, Steven Baker, and Jeremiah Crim. 2003. Bootstrapping statistical parsers from small datasets. In Proceedings of the 10th Con- ference of the European Chapter of the Association for Computational Linguistics, pages 331\u2013338, Budapest, Hungary. Association for Computational Linguistics.",
  "2003. Bootstrapping statistical parsers from small datasets. In Proceedings of the 10th Con- ference of the European Chapter of the Association for Computational Linguistics, pages 331\u2013338, Budapest, Hungary. Association for Computational Linguistics. [Surdeanu et al.2008] Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu\u00b4\u0131s M`arquez, and Joakim Nivre. 2008. The CoNLL 2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of the Twelfth Conference on Computa- tional Natural Language Learning, pages 159\u2013177, Manchester, England. Association for Computational Linguistics. [Szolovits2003] Peter Szolovits. 2003. Adding a medical lexicon to an English parser. AMIA Annual Symposium Proceedings, pages 639\u2013643. [Tiedemann2012] J\u00a8org Tiedemann. 2012. Parallel data, tools and interfaces in OPUS.",
  "2003. Adding a medical lexicon to an English parser. AMIA Annual Symposium Proceedings, pages 639\u2013643. [Tiedemann2012] J\u00a8org Tiedemann. 2012. Parallel data, tools and interfaces in OPUS. In Proceedings of the Eighth International Conference on Language Resources and Evalu- ation, pages 2214\u20132218, Istanbul, Turkey. European Language Resources Association. [Weiss et al.2015] David Weiss, Chris Alberti, Michael Collins, and Slav Petrov. 2015. Structured training for neural network transition-based parsing. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 323\u2013333, Beijing, China. Association for Computational Linguistics. [Xue et al.2005] Naiwen Xue, Fei Xia, Fu-dong Chiou, and Marta Palmer. 2005. The Penn Chinese TreeBank: Phrase structure annotation of a large corpus. Natural Language Engineering, 11(2):207\u2013238. 134",
  "[Yamada and Matsumoto2003] Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of the 8th Inter- national Workshop on Parsing Technologies, pages 195\u2013206, Nancy, France. Lorraine Laboratory for Research in Information Technology and its Applications. [Yu and Bohnet2015] Juntao Yu and Bernd Bohnet. 2015. Exploring con\ufb01dence-based self-training for multilingual dependency parsing in an under-resourced language sce- nario. In Proceedings of the Third International Conference on Dependency Linguis- tics, pages 350\u2013358, Uppsala, Sweden. Uppsala University. [Yu and Bohnet2017] Juntao Yu and Bernd Bohnet. 2017. Dependency language mod- els for transition-based dependency parsing. In Proceedings of the 15th International Conference on Parsing Technologies, pages 11\u201317, Pisa, Italy. Association for Compu- tational Linguistics.",
  "2017. Dependency language mod- els for transition-based dependency parsing. In Proceedings of the 15th International Conference on Parsing Technologies, pages 11\u201317, Pisa, Italy. Association for Compu- tational Linguistics. [Yu et al.2015] Juntao Yu, Mohab Elkaref, and Bernd Bohnet. 2015. Domain adapta- tion for dependency parsing via self-training. In Proceedings of the 14th International Conference on Parsing Technologies, pages 1\u201310, Bilbao, Spain. Association for Com- putational Linguistics. [Zhang and Clark2008] Yue Zhang and Stephen Clark. 2008. A tale of two parsers: In- vestigating and combining graph-based and transition-based dependency parsing. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Pro- cessing, pages 562\u2013571, Honolulu, Hawaii, USA. Association for Computational Lin- guistics. [Zhang and McDonald2014] Hao Zhang and Ryan McDonald. 2014. Enforcing structural diversity in cube-pruned dependency parsing.",
  "Association for Computational Lin- guistics. [Zhang and McDonald2014] Hao Zhang and Ryan McDonald. 2014. Enforcing structural diversity in cube-pruned dependency parsing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 656\u2013661, Baltimore, Maryland, USA. Association for Computational Linguistics. [Zhang and Nivre2011] Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 188\u2013193, Portland, Oregon, USA. Association for Computational Linguistics. [Zhang et al.2012] Meishan Zhang, Wanxiang Che, Yijia Liu, Zhenghua Li, and Ting Liu. 2012. HIT dependency parsing: Bootstrap aggregating heterogeneous parsers. In Proceedings of the First Workshop on Syntactic Analysis of Non-Canonical Language, Montreal, Quebec, Canada. [Zhou and Li2005] Zhi-Hua Zhou and Ming Li. 2005.",
  "2012. HIT dependency parsing: Bootstrap aggregating heterogeneous parsers. In Proceedings of the First Workshop on Syntactic Analysis of Non-Canonical Language, Montreal, Quebec, Canada. [Zhou and Li2005] Zhi-Hua Zhou and Ming Li. 2005. Tri-training: exploiting unlabeled data using three classi\ufb01ers. IEEE Transactions on Knowledge and Data Engineering, 17(11):1529\u20131541. [Zhou et al.2011] Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011. Exploiting web-derived selectional preference to improve statistical dependency parsing. In Pro- ceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1556\u20131565, Portland, Oregon, USA. Association for Computational Linguistics. 135",
  "[Zhu2005] Xiaojin Zhu. 2005. Semi-supervised learning literature survey. Technical Re- port 1530, Computer Sciences, University of Wisconsin-Madison. 136"
]