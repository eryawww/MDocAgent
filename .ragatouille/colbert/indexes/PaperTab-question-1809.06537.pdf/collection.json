[
  "Automatic Judgment Prediction via Legal Reading Comprehension Shangbang Long1, Cunchao Tu2, Zhiyuan Liu2\u2217, Maosong Sun2 1Peking University 2Department of Computer Science and Technology State Key Lab on Intelligent Technology and Systems Institute for Arti\ufb01cial Intelligence, Tsinghua University, Beijing, China longlongsb@pku.edu.cn, tucunchao@gmail.com, {lzy,sms}@tsinghua.edu.cn Abstract Automatic judgment prediction aims to pre- dict the judicial results based on case mate- rials. It has been studied for several decades mainly by lawyers and judges, considered as a novel and prospective application of arti\ufb01cial intelligence techniques in the legal \ufb01eld. Most existing methods follow the text classi\ufb01cation framework, which fails to model the complex interactions among complementary case ma- terials. To address this issue, we formalize the task as Legal Reading Comprehension ac- cording to the legal scenario. Following the working protocol of human judges, LRC pre- dicts the \ufb01nal judgment results based on three types of information, including fact descrip- tion, plaintiffs\u2019 pleas, and law articles.",
  "Following the working protocol of human judges, LRC pre- dicts the \ufb01nal judgment results based on three types of information, including fact descrip- tion, plaintiffs\u2019 pleas, and law articles. More- over, we propose a novel LRC model, Auto- Judge, which captures the complex semantic interactions among facts, pleas, and laws. In experiments, we construct a real-world civil case dataset for LRC. Experimental results on this dataset demonstrate that our model achieves signi\ufb01cant improvement over state- of-the-art models. We will publish all source codes and datasets of this work on github. com for further research. 1 Introduction Automatic judgment prediction is to train a ma- chine judge to determine whether a certain plea in a given civil case would be supported or rejected. In countries with civil law system, e.g. mainland China, such process should be done with reference to related law articles and the fact description, as is performed by a human judge. The intuition comes from the fact that under civil law system, law articles act as principles for juridical judg- ments. Such techniques would have a wide range of promising applications.",
  "The intuition comes from the fact that under civil law system, law articles act as principles for juridical judg- ments. Such techniques would have a wide range of promising applications. On the one hand, legal consulting systems could provide better access to \u2217Corresponding author. Figure 1: An Example of LRC. high-quality legal resources in a low-cost way to legal outsiders, who suffer from the complicated terminologies. On the other hand, machine judge assistants for professionals would help improve the ef\ufb01ciency of the judicial system. Besides, au- tomated judgment system can help in improving juridical equality and transparency. From another perspective, there are currently 7 times much more civil cases than criminal cases in mainland China, with annual rates of increase of 10.8% and 1.6% respectively, making judgment prediction in civil cases a promising application (Zhuge, 2016).",
  "From another perspective, there are currently 7 times much more civil cases than criminal cases in mainland China, with annual rates of increase of 10.8% and 1.6% respectively, making judgment prediction in civil cases a promising application (Zhuge, 2016). Previous works (Aletras et al., 2016; Katz et al., 2017; Luo et al., 2017; Sulea et al., 2017) formal- ize judgment prediction as the text classi\ufb01cation task, regarding either charge names or binary judg- ments, i.e., support or reject, as the target classes. These works focus on the situation where only one result is expected, e.g., the US Supreme Court\u2019s decisions (Katz et al., 2017), and the charge name prediction for criminal cases (Luo et al., 2017). Despite these recent efforts and their progress, au- tomatic judgment prediction in civil law system is still confronted with two main challenges: arXiv:1809.06537v1  [cs.AI]  18 Sep 2018",
  "One-to-Many Relation between Case and Plea. Every single civil case may contain multiple pleas and the result of each plea is co-determined by related law articles and speci\ufb01c aspects of the involved case. For example, in divorce proceed- ings, judgment of alienation of mutual affection is the key factor for granting divorce but custody of children depends on which side can provide better an environment for children\u2019s growth as well as parents\u2019 \ufb01nancial condition. Here, different pleas are independent. Heterogeneity of Input Triple. Inputs to a judgment prediction system consist of three het- erogeneous yet complementary parts, i.e., fact de- scription, plaintiff\u2019s plea, and related law arti- cles. Concatenating them together and treating them simply as a sequence of words as in previ- ous works (Katz et al., 2017; Aletras et al., 2016) would cause a great loss of information. This is the same in question-answering where the dual in- puts, i.e., query and passage, should be modeled separately.",
  "This is the same in question-answering where the dual in- puts, i.e., query and passage, should be modeled separately. Despite the introduction of the neural networks that can learn better semantic representations of input text, it remains unsolved to incorporate proper mechanisms to integrate the complemen- tary triple of pleas, fact descriptions, and law ar- ticles together. Inspired by recent advances in question an- swering (QA) based reading comprehension (RC) (Wang et al., 2017; Cui et al., 2017; Nguyen et al., 2016; Rajpurkar et al., 2016), we propose the Le- gal Reading Comprehension (LRC) framework for automatic judgment prediction. LRC incorpo- rates the reading mechanism for better modeling of the complementary inputs above-mentioned, as is done by human judges when referring to le- gal materials in search of supporting law articles. Reading mechanism, by simulating how human connects and integrates multiple text, has proven an effective module in RC tasks.",
  "Reading mechanism, by simulating how human connects and integrates multiple text, has proven an effective module in RC tasks. We argue that applying the reading mechanism in a proper way among the triplets can obtain a better understand- ing and more informative representation of the original text, and further improve performance . To instantiate the framework, we propose an end- to-end neural network model named AutoJudge. For experiments, we train and evaluate our mod- els in the civil law system of mainland China. We collect and construct a large-scale real-world data set of 100, 000 case documents that the Supreme People\u2019s Court of People\u2019s Republic of China has made publicly available. Fact description, pleas, and results can be extracted easily from these case documents with regular expressions, since the original documents have special typographi- cal characteristics indicating the discourse struc- ture. We also take into account law articles and their corresponding juridical interpretations. We also implement and evaluate previous methods on our dataset, which prove to be strong baselines. Our experiment results show signi\ufb01cant im- provements over previous methods.",
  "We also take into account law articles and their corresponding juridical interpretations. We also implement and evaluate previous methods on our dataset, which prove to be strong baselines. Our experiment results show signi\ufb01cant im- provements over previous methods. Further exper- iments demonstrate that our model also achieves considerable improvement over other off-the-shelf state-of-the-art models under classi\ufb01cation and question answering framework respectively. Ab- lation tests carried out by taking off some compo- nents of our model further prove its robustness and effectiveness. To sum up, our contributions are as follows: (1) We introduce reading mechanism and re- formalize judgment prediction as Legal Reading Comprehension to better model the complemen- tary inputs. (2) We construct a real-world dataset for exper- iments, and plan to publish it for further research. (3) Besides baselines from previous works, we also carry out comprehensive experiments com- paring different existing deep neural network methods on our dataset. Supported by these ex- periments, improvements achieved by LRC prove to be robust. 2 Related Work 2.1 Judgment Prediction Automatic judgment prediction has been studied for decades.",
  "Supported by these ex- periments, improvements achieved by LRC prove to be robust. 2 Related Work 2.1 Judgment Prediction Automatic judgment prediction has been studied for decades. At the very \ufb01rst stage of judgment prediction studies, researchers focus on mathemat- ical and statistical analysis of existing cases, with- out any conclusions or methodologies on how to predict them (Lauderdale and Clark, 2012; Segal, 1984; Keown, 1980; Ulmer, 1963; Nagel, 1963; Kort, 1957). Recent attempts consider judgment prediction under the text classi\ufb01cation framework. Most of these works extract ef\ufb01cient features from text (e.g., N-grams) (Liu and Chen, 2017; Sulea et al., 2017; Aletras et al., 2016; Lin et al., 2012; Liu and Hsieh, 2006) or case pro\ufb01les (e.g., dates, terms, locations and types) (Katz et al., 2017). All these methods require a large amount of human effort",
  "to design features or annotate cases. Besides, they also suffer from generalization issue when applied to other scenarios. Motivated by the successful application of deep neural networks, Luo et al. (Luo et al., 2017) in- troduce an attention-based neural model to predict charges of criminal cases, and verify the effective- ness of taking law articles into consideration. Nev- ertheless, they still fall into the text classi\ufb01cation framework and lack the ability to handle multiple inputs with more complicated structures. 2.2 Text Classi\ufb01cation As the basis of previous judgment prediction works, typical text classi\ufb01cation task takes a sin- gle text content as input and predicts the category it belongs to. Recent works usually employ neural networks to model the internal structure of a sin- gle input (Kim, 2014; Baharudin et al., 2010; Tang et al., 2015; Yang et al., 2016). There also exists another thread of text classi\ufb01- cation called entailment prediction.",
  "There also exists another thread of text classi\ufb01- cation called entailment prediction. Methods pro- posed in (Hu et al., 2014; Mitra et al., 2017) are intended for complementary inputs, but the mech- anisms can be considered as a simpli\ufb01ed version of reading comprehension. 2.3 Reading Comprehension Reading comprehension is a relevant task to model heterogeneous and complementary inputs, where an answer is predicted given two channels of in- puts, i.e. a textual passage and a query. Consid- erable progress has been made (Cui et al., 2017; Dhingra et al., 2017; Wang et al., 2017). These models employ various attention mechanism to model the interaction between passage and query. Inspired by the advantage of reading comprehen- sion models on modeling multiple inputs, we ap- ply this idea into the legal area and propose legal reading comprehension for judgment prediction.",
  "These models employ various attention mechanism to model the interaction between passage and query. Inspired by the advantage of reading comprehen- sion models on modeling multiple inputs, we ap- ply this idea into the legal area and propose legal reading comprehension for judgment prediction. 3 Legal Reading Comprehension 3.1 Conventional Reading Comprehension Conventional reading comprehension (He et al., 2017; Joshi et al., 2017; Nguyen et al., 2016; Rajpurkar et al., 2016) usually considers reading comprehension as predicting the answer given a passage and a query, where the answer could be a single word, a text span of the original passage, chosen from answer candidates, or generated by human annotators. Generally, an instance in RC is represented as a triple \u27e8p, q, a\u27e9, where p, q and a correspond to passage, query and answer respectively. Given a triple \u27e8p, q, a\u27e9, RC takes the pair \u27e8p, q\u27e9as the in- put and employs attention-based neural models to construct an ef\ufb01cient representation. Afterwards, the representation is fed into the output layer to select or generate an answer.",
  "Given a triple \u27e8p, q, a\u27e9, RC takes the pair \u27e8p, q\u27e9as the in- put and employs attention-based neural models to construct an ef\ufb01cient representation. Afterwards, the representation is fed into the output layer to select or generate an answer. 3.2 Legal Reading Comprehension Existing works usually formalize judgment pre- diction as a text classi\ufb01cation task and focus on extracting well-designed features of speci\ufb01c cases. Such simpli\ufb01cation ignores that the judgment of a case is determined by its fact description and mul- tiple pleas. Moreover, the \ufb01nal judgment should act up to the legal provisions, especially in civil law systems. Therefore, how to integrate the in- formation (i.e., fact descriptions, pleas, and law articles) in a reasonable way is critical for judg- ment prediction. Inspired by the successful application of RC, we propose a framework of Legal Reading Comprehension(LRC) for judgment prediction in the legal area. As illustrated in Fig. 1, for each plea in a given case, the prediction of judgment result is made based the fact description and the potentially relevant law articles.",
  "As illustrated in Fig. 1, for each plea in a given case, the prediction of judgment result is made based the fact description and the potentially relevant law articles. In a nutshell, LRC can be formalized as the fol- lowing quadruplet task: \u27e8f, p, l, r\u27e9, (1) where f is the fact description, p is the plea, l is the law articles and r is the result. Given \u27e8f, p, l\u27e9, LRC aims to predict the judgment result as r = arg max r\u2208{support, reject} P(r|f, p, l). (2) The probability is calculated with respect to the interaction among the triple \u27e8f, p, l\u27e9, which will draw on the experience of the interaction between \u27e8passage, question\u27e9pairs in RC. To summarize, LRC is innovative in the follow- ing aspects: (1) While previous works \ufb01t the problem into text classi\ufb01cation framework, LRC re-formalizes the way to approach such problems. This new framework provides the ability to deal with the heterogeneity of the complementary inputs.",
  "This new framework provides the ability to deal with the heterogeneity of the complementary inputs. (2) Rather than employing conventional RC models to handle pair-wise text information in the legal area, LRC takes the critical law articles into",
  "Pair-Wise Attentive Reader mGRU Text Encoder Bi-GRU Plea I ask for divorce up Fact They gave birth \u2026. since birth uf Law If they \u2026  grant ul memory \u03b1t,1 \u03b1t,2 \u03b1t,3 \u03b1t,4 xp=[up,cf] Output vp vl CNN v*=[vp,vl] xl=[ul,cf] Figure 2: An overview of AutoJudge. consideration and models the facts, pleas, and law articles jointly for judgment prediction, which is more suitable to simulate the human mode of deal- ing with cases. 4 Methods We propose a novel judgment prediction model AutoJudge to instantiate the LRC framework. As shown in Fig. 2, AutoJudge consists of three \ufb02exi- ble modules, including a text encoder, a pair-wise attentive reader, and an output module. In the following parts, we give a detailed intro- duction to these three modules. 4.1 Text Encoder As illustrated in Fig. 2, Text Encoder aims to en- code the word sequences of inputs into continuous representation sequences.",
  "In the following parts, we give a detailed intro- duction to these three modules. 4.1 Text Encoder As illustrated in Fig. 2, Text Encoder aims to en- code the word sequences of inputs into continuous representation sequences. Formally, consider a fact description f = {wf t }m t=1, a plea p = {wp t }n t=1, and the relevant law articles l = {wl t}k t=1, where wt denotes the t- th word in the sequence and m, n, k are the lengths of word sequences f, p, l respectively. First, we convert the words to their respective word embeddings to obtain f = {wf t }m t=1, p = {wp t }n t=1 and l = {wl t}k t=1, where w \u2208Rd.",
  "First, we convert the words to their respective word embeddings to obtain f = {wf t }m t=1, p = {wp t }n t=1 and l = {wl t}k t=1, where w \u2208Rd. After- wards, we employ bi-directional GRU (Cho et al., 2014; Bahdanau et al., 2015; Chung et al., 2014) to produce the encoded representation u of all words as follows: uf t = BiGRUF (uf t\u22121, wf t ), up t = BiGRUP (up t\u22121, wp t ), ul t = BiGRUL(ul t\u22121, wl t). (3) Note that, we adopt different bi-directional GRUs to encode fact descriptions, pleas, and law articles respectively(denoted as BiGRUF , BiGRUP , and BiGRUL). With these text encoders, f, p, and l are converting into uf = {uf t }m t=1, up = {up t }n t=1, and ul = {ul t}k t=1.",
  "With these text encoders, f, p, and l are converting into uf = {uf t }m t=1, up = {up t }n t=1, and ul = {ul t}k t=1. 4.2 Pair-Wise Attentive Reader How to model the interactions among the input text is the most important problem in reading com- prehension. In AutoJudge, we employ a pair-wise attentive reader to process \u27e8uf, up\u27e9and \u27e8uf, ul\u27e9 respectively. More speci\ufb01cally, we propose to use pair-wise mutual attention mechanism to cap- ture the complex semantic interaction between text pairs, as well as increasing the interpretability of AutoJudge. 4.2.1 Pair-Wise Mutual Attention For each input pair \u27e8uf, up\u27e9or \u27e8uf, ul\u27e9, we em- ploy pair-wise mutual attention to select relevant information from fact descriptions uf and produce more informative representation sequences. As a variant of the original attention mecha- nism (Bahdanau et al., 2015), we design the pair- wise mutual attention unit as a GRU with internal memories denoted as mGRU.",
  "As a variant of the original attention mecha- nism (Bahdanau et al., 2015), we design the pair- wise mutual attention unit as a GRU with internal memories denoted as mGRU. Taking the representation sequence pair \u27e8uf, up\u27e9for instance, mGRU stores the fact se- quence uf into its memories. For each timestamp t \u2208[1, n], it selects relevant fact information cf t from the memories as follows, cf t = m X i=1 \u03b1t,iuf i . (4) Here, the weight \u03b1t,i is the softmax value as \u03b1t,i = exp(at,i) Pm j=1 exp(at,j). (5) Note that, at,j represents the relevance between up t and uf j . It is calculated as follows, at,j = VT tanh(Wfuf j + Wpup t + Upvp t\u22121). (6)",
  "Here, vp t\u22121 is the last hidden state in the GRU, which will be introduced in the following part. V is a weight vector, and Wf, Wp, Up are attention metrics of our proposed pair-wise attention mech- anism. 4.2.2 Reading Mechanism With the relevant fact information cf t and up t , we get the t-th input of mGRU as xp t = up t \u2295cf t , (7) where \u2295indicates the concatenation operation. Then, we feed xp t into GRU to get more infor- mative representation sequence vp = {vp t }n t=1 as follows, vp t = GRU(vp t\u22121, xp t ). (8) For the input pair \u27e8uf, ul\u27e9, we can get vl = {vl t}k t=1 in the same way. Therefore, we omit the implementation details Here.",
  "(8) For the input pair \u27e8uf, ul\u27e9, we can get vl = {vl t}k t=1 in the same way. Therefore, we omit the implementation details Here. Similar structures with attention mechanism are also applied in (Wang et al., 2017; Rocktaschel et al., 2016; Wang and Jiang, 2016; Bahdanau et al., 2015) to obtain mutually aware represen- tations in reading comprehension models, which signi\ufb01cantly improve the performance of this task. 4.3 Output Layer Using text encoder and pair-wise attentive reader, the initial input triple \u27e8f, p, l\u27e9has been converted into two sequences, i.e., vp = {vp t }n t=1 and vl = {vl t}k t=1, where vl t is de\ufb01ned similarly to vp t . These sequences reserve complex semantic infor- mation about the pleas and law articles, and \ufb01lter out irrelevant information in fact descriptions. With these two sequences, we concatenate vp and vl along the sequence length dimension to generate the sequence v\u2217= {vt}n+k t=1 .",
  "These sequences reserve complex semantic infor- mation about the pleas and law articles, and \ufb01lter out irrelevant information in fact descriptions. With these two sequences, we concatenate vp and vl along the sequence length dimension to generate the sequence v\u2217= {vt}n+k t=1 . Since we have employed several GRU layers to encode the sequential inputs, another recurrent layer may be redundant. Therefore, we utilize a 1-layer CNN (Kim, 2014) to capture the local structure and generate the representation vector for the \ufb01- nal prediction. Assuming y \u2208[0, 1] is the predicted probabil- ity that the plea in the case sample would be sup- ported and r \u2208{0, 1} is the gold standard, Auto- Judge aims to minimize the cross-entropy as fol- lows, L = \u22121 N N X i=1 [rilnyi + (1 \u2212ri)ln(1 \u2212yi)], (9) where N is the number of training data. As all the calculation in our model is differentiable, we em- ploy Adam (Kingma and Ba, 2015) for optimiza- tion.",
  "As all the calculation in our model is differentiable, we em- ploy Adam (Kingma and Ba, 2015) for optimiza- tion. 5 Experiments To evaluate the proposed LRC framework and the AutoJudge model, we carry out a series of exper- iments on the divorce proceedings, a typical yet complex \ufb01eld of civil cases. Divorce proceedings often come with several kinds of pleas, e.g. seek- ing divorce, custody of children, compensation, and maintenance, which focuses on different as- pects and thus makes it a challenge for judgment prediction. 5.1 Dataset Construction for Evaluation 5.1.1 Data Collection Since none of the datasets from previous works have been published, we decide to build a new one. We randomly collect 100, 000 cases from China Judgments Online1, among which 80, 000 cases are for training, 10, 000 each for validation and testing. Among the original cases, 51% are granted divorce and others not. There are 185, 723 valid pleas in total, with 52% supported and 48% rejected.",
  "Among the original cases, 51% are granted divorce and others not. There are 185, 723 valid pleas in total, with 52% supported and 48% rejected. Note that, if the divorce plea in a case is not granted, the other pleas of this case will not be considered by the judge. Case materials are all natural language sentences, with averagely 100.08 tokens per fact description and 12.88 per plea. There are 62 relevant law articles in total, each with 26.19 tokens averagely. Note that the case documents include special typographical sig- nals, making it easy to extract labeled data with regular expression. 5.1.2 Data Pre-Processing We apply some rules with legal prior to preprocess the dataset according to previous works (Liu et al., 2003, 2004; Bian and Shun-yuan, 2005), which have proved effective in our experiments. Name Replacement2: All names in case doc- uments are replaced with marks indicating their roles, instead of simply anonymizing them, e.g. <Plantiff>, <Defendant>, <Daughter x> and so on.",
  "Name Replacement2: All names in case doc- uments are replaced with marks indicating their roles, instead of simply anonymizing them, e.g. <Plantiff>, <Defendant>, <Daughter x> and so on. Since \u201call are equal before the law\u201d3, 1http://wenshu.court.gov.cn 2We use regular expressions to extract names and roles from the formatted case header. 3Constitution of the People\u2019s Republic of China",
  "names should make no more difference than what role they take. Law Article Filtration : Since most acces- sible divorce proceeding documents do not con- tain ground-truth \ufb01ne-grained articles4, we use an unsupervised method instead. First, we extract all the articles from the law text with regular ex- pression. Afterwards, we select the most relevant 10 articles according to the fact descriptions as follows. We obtain sentence representation with CBOW (Mikolov et al., 2013a,b) weighted by in- verse document frequency, and calculate cosine distance between cases and law articles. Word em- beddings are pre-trained with Chinese Wikipedia pages5. As the \ufb01nal step, we extract top 5 rel- evant articles for each sample respectively from the main marriage law articles and their interpreta- tions, which are equally important. We manually check the extracted articles for 100 cases to en- sure that the extraction quality is fairly good and acceptable.",
  "We manually check the extracted articles for 100 cases to en- sure that the extraction quality is fairly good and acceptable. The \ufb01ltration process is automatic and fully un- supervised since the original documents have no ground-truth labels for \ufb01ne-grained law articles, and coarse-grained law-articles only provide lim- ited information. We also experiment with the ground-truth articles, but only a small fraction of them has \ufb01ne-grained ones, and they are usually not available in real-world scenarios. 5.2 Implementation Details We employ Jieba6 for Chinese word segmentation and keep the top 20, 000 frequent words. The word embedding size is set to 128 and the other low-frequency words are replaced with the mark <UNK>. The hidden size of GRU is set to 128 for each direction in Bi-GRU. In the pair-wise at- tentive reader, the hidden state is set to 256 for mGRu. In the CNN layer, \ufb01lter windows are set to 1, 3, 4, and 5 with each \ufb01lter containing 200 feature maps.",
  "In the pair-wise at- tentive reader, the hidden state is set to 256 for mGRu. In the CNN layer, \ufb01lter windows are set to 1, 3, 4, and 5 with each \ufb01lter containing 200 feature maps. We add a dropout layer (Srivastava et al., 2014) after the CNN layer with a dropout rate of 0.5. We use Adam(Kingma and Ba, 2015) for training and set learning rate to 0.0001, \u03b21 to 0.9 , \u03b22 to 0.999, \u03f5 to 1e \u22128, batch size to 64. We employ precision, recall, F1 and accuracy for evaluation metrics. We repeat all the experiments 4Fine-grained articles are in the Juridical Interpretations, giving detailed explanation, while the Marriage Law only covers some basic principles. 5https://dumps.wikimedia.org/zhwiki/ 6https://github.com/fxsjy/jieba for 10 times, and report the average results.",
  "5https://dumps.wikimedia.org/zhwiki/ 6https://github.com/fxsjy/jieba for 10 times, and report the average results. 5.3 Baselines For comparison, we adopt and re-implement three kinds of baselines as follows: Lexical Features + SVM We implement an SVM with lexical features in accordance with pre- vious works (Lin et al., 2012; Liu and Hsieh, 2006; Aletras et al., 2016; Liu and Chen, 2017; Sulea et al., 2017) and select the best feature set on the development set. Neural Text Classi\ufb01cation Models We im- plement and \ufb01ne-tune a series of neural text classi- \ufb01ers, including attention-based method(Luo et al., 2017) and other methods we deem important. CNN (Kim, 2014) and GRU (Cho et al., 2014; Yang et al., 2016) take as input the concate- nation of fact description and plea.",
  "CNN (Kim, 2014) and GRU (Cho et al., 2014; Yang et al., 2016) take as input the concate- nation of fact description and plea. Similarly, CNN/GRU+law refers to using the concatenation of fact description, plea and law articles as inputs. RC Models We implement and train some off-the-shelf RC models, including r-net(Wang et al., 2017) and AoA(Cui et al., 2017), which are the leading models on SQuAD leaderboard. In our initial experiments, these models take fact descrip- tion as passage and plea as query. Further, Law articles are added to the fact description as a part of the reading materials, which is a simple way to consider them as well. 5.4 Results and Analysis From Table 1, we have the following observations: (1) AutoJudge consistently and signi\ufb01cantly outperforms all the baselines, including RC mod- els and other neural text classi\ufb01cation models, which shows the effectiveness and robustness of our model.",
  "(2) RC models achieve better performance than most text classi\ufb01cation models (excluding GRU+Attention), which indicates that reading mechanism is a better way to integrate informa- tion from heterogeneous yet complementary in- puts. On the contrary, simply adding law articles as a part of the reading materials makes no differ- ence in performance. Note that, GRU+Attention employ similar attention mechanism as RC does and takes additional law articles into considera- tion, thus achieves comparable performance with RC models.",
  "Models P R F1 Acc. MaxFreq 52.2 100 68.6 52.2 SVM* 57.8 53.5 55.6 55.5 CNN 76.1 81.9 79.0 77.6 CNN+law 74.4 79.4 77.0 76.0 GRU 79.2 72.9 76.1 76.6 GRU+law 78.2 68.2 72.8 74.4 GRU+Attention* 79.1 80.7 80.0 79.1 AoA 79.3 78.9 79.2 78.3 AoA+law 79.0 79.2 79.1 78.3 r-net 79.5 78.7 79.2 78.4 r-net+law 79.3 78.8 79.0 78.3 AutoJudge 80.4 86.6 83.4 82.2 Table 1: Experimental results(%).",
  "P/R/F1 are re- ported for positive samples and calculated as the mean score over 10-time experiments. Acc is de\ufb01ned as the proportion of test samples classi- \ufb01ed correctly, equal to micro-precision. MaxFreq refers to always predicting the most frequent label, i.e. support in our dataset. * indicates methods proposed in previous works. (3) Comparing with conventional RC mod- els, AutoJudge achieves signi\ufb01cant improvement with the consideration of additional law arti- cles. It re\ufb02ects the difference between LRC and conventional RC models. We re-formalize LRC in legal area to incorporate law articles via the reading mechanism, which can enhance judgment prediction. Moreover, CNN/GRU+law decrease the performance by simply concate- nating original text with law articles, while GRU+Attention/AutoJudge increase the perfor- mance by integrating law articles with attention mechanism. It shows the importance and ratio- nality of using attention mechanism to capture the interaction between multiple inputs.",
  "It shows the importance and ratio- nality of using attention mechanism to capture the interaction between multiple inputs. The experiments support our hypothesis as pro- posed in the Introduction part that in civil cases, it\u2019s important to model the interactions among case materials. Reading mechanism can well per- form the matching among them. 5.5 Ablation Test AutoJudge is characterized by the incorporation of pair-wise attentive reader, law articles, and a CNN output layer, as well as some pre-processing with legal prior. We design ablation tests respectively to evaluate the effectiveness of these modules. When taken off the attention mechanism, AutoJudge de- grades into a GRU on which a CNN is stacked. Models F1 Acc.",
  "We design ablation tests respectively to evaluate the effectiveness of these modules. When taken off the attention mechanism, AutoJudge de- grades into a GRU on which a CNN is stacked. Models F1 Acc. AutoJudge 83.4 82.2 w/o reading mechanism 78.9(\u21934.5) 78.2(\u21934.0) w/o law articles 79.6(\u21933.8) 78.4(\u21933.8) CNN\u2192LSTM 77.6(\u21935.8) 77.7(\u21934.5) w/o Pre-Processing 81.1(\u21932.3) 80.3(\u21931.9) w/o law article selection 80.6(\u21932.8) 80.5(\u21931.7) with GT law articles 85.1(\u21911.7) 84.1(\u21911.9) Table 2: Experimental results of ablation tests (%). When taken off law articles, the CNN output layer only takes {vP t }LP t=1 as input.",
  "When taken off law articles, the CNN output layer only takes {vP t }LP t=1 as input. Besides, our model is tested respectively without name-replacement or unsupervised selection of law articles (i.e. passing the whole law text). As mentioned above, we sys- tem use law articles extracted with unsupervised method, so we also experiment with ground-truth law articles. Results are shown in Table 2. We can infer that: (1) The performance drops signi\ufb01cantly after removing the attention layer or excluding the law articles, which is consistent with the comparison between AutoJudge and baselines. The result ver- i\ufb01es that both the reading mechanism and incorpo- ration of law articles are important and effective. (2) After replacing CNN with an LSTM layer, performance drops as much as 4.4% in accuracy and 5.7% in F1 score. The reason may be the redundancy of RNNs. AutoJudge has employed several GRU layers to encode text sequences. An- other RNN layer may be useless to capture se- quential dependencies, while CNN can catch the local structure in convolution windows.",
  "The reason may be the redundancy of RNNs. AutoJudge has employed several GRU layers to encode text sequences. An- other RNN layer may be useless to capture se- quential dependencies, while CNN can catch the local structure in convolution windows. (3) Motivated by existing rule-based works, we conduct data pre-processing on cases, including name replacement and law article \ufb01ltration. If we remove the pre-processing operations, the perfor- mance drops considerably. It demonstrates that applying the prior knowledge in legal \ufb01led would bene\ufb01t the understanding of legal cases. Performance Over Law Articles It\u2019s intu- itive that the quality of the retrieved law articles would affect the \ufb01nal performance. As is shown in Table 2, feeding the whole law text without \ufb01ltra- tion results in worse performance. However, when we train and evaluate our model with ground truth articles, the performance is boosted by nearly 2% in both F1 and Acc. The performance improve- ment is quite limited compared to that in previous work (Luo et al., 2017) for the following reasons:",
  "(1) As mentioned above, most case documents only contain coarse-grained articles, and only a small number of them contain \ufb01ne-grained ones, which has limited information in themselves. (2) Unlike in criminal cases where the application of an article indicates the corresponding crime, law articles in civil cases work as reference, and can be applied in both the cases of supports and rejects. As law articles cut both ways for the judgment re- sult, this is one of the characteristics that distin- guishes civil cases from criminal ones. We also need to remember that, the performance of 84.1% in accuracy or 85.1% in F1 score is unattainable in real-world setting for automatic prediction where ground-truth articles are not available. Reading Weighs More Than Correct Law Articles In the area of civil cases, the under- standing of the case materials and how they inter- act is a critical factor. The inclusion of law articles is not enough. As is shown in Table 2, compared to feeding the model with an un-selected set of law articles, taking away the reading mechanism results in greater performance drop7.",
  "The inclusion of law articles is not enough. As is shown in Table 2, compared to feeding the model with an un-selected set of law articles, taking away the reading mechanism results in greater performance drop7. Therefore, the ability to read, understand and select relevant information from the complex multi-sourced case materials is necessary. It\u2019s even more important in real world since we don\u2019t have access to ground- truth law articles to make predictions. 5.6 Case Study Visualization of Positive Samples We visu- alize the heat maps of attention results8. As shown in Fig. 3, deeper background color repre- sents larger attention score. The attention score is calculated with Eq. (5). We take the average of the resulting n \u00d7 m at- tention matrix over the time dimension to obtain attention values for each word. The visualization demonstrates that the atten- tion mechanism can capture relevant patterns and semantics in accordance with different pleas in different cases. Failure Analysis As for the failed sam- ples, the most common reason comes from the anonymity issue, which is also shown in Fig. 3. As mentioned above, we conduct name replace- ment.",
  "Failure Analysis As for the failed sam- ples, the most common reason comes from the anonymity issue, which is also shown in Fig. 3. As mentioned above, we conduct name replace- ment. However, some critical elements are also anonymized by the government, due to the privacy 73.9% vs. 1.7% in Acc, and 4.4% vs. 2.8% in F1. 8Examples given here are all drawn from the test set whose predictions match the real judgment. Figure 3: Visualization of Attention Mechanism. issue. These elements are sometimes important to judgment prediction. For example, determination of the key factor long-time separation is relevant to the explicit dates, which are anonymized. 6 Conclusion In this paper, we explore the task of predicting judgments of civil cases. Comparing with con- ventional text classi\ufb01cation framework, we pro- pose Legal Reading Comprehension framework to handle multiple and complex textual inputs. Moreover, we present a novel neural model, Au- toJudge, to incorporate law articles for judgment prediction.",
  "Comparing with con- ventional text classi\ufb01cation framework, we pro- pose Legal Reading Comprehension framework to handle multiple and complex textual inputs. Moreover, we present a novel neural model, Au- toJudge, to incorporate law articles for judgment prediction. In experiments, we compare our model on divorce proceedings with various state-of-the- art baselines of various frameworks. Experimental results show that our model achieves considerable improvement than all the baselines. Besides, visu- alization results also demonstrate the effectiveness and interpretability of our proposed model. In the future, we can explore the following di- rections: (1) Limited by the datasets, we can only verify our proposed model on divorce proceed- ings. A more general and larger dataset will ben- e\ufb01t the research on judgment prediction. (2) Judi- cial decisions in some civil cases are not always bi- nary, but more diverse and \ufb02exible ones, e.g. com- pensation amount. Thus, it is critical for judgment prediction to manage various judgment forms.",
  "References Nikolaos Aletras, Dimitrios Tsarapatsanis, Daniel Preotiuc-Pietro, and Vasileios Lampos. 2016. Pre- dicting judicial decisions of the european court of human rights: A natural language processing per- spective. PeerJ Computer Science, 2. Baharum Baharudin, Lam Hong Lee, and Khairullah Khan. 2010. A review of machine learning al- gorithms for text-documents classi\ufb01cation. JAIT, 1(1):4\u201320. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate. In Proceedings of ICLR. Guo-wei Bian and Teng Shun-yuan. 2005. Integrating query translation and text classi\ufb01cation in a cross- language patent access system. In Proceedings of NTCIR-7 Workshop Meeting, pages 252\u2013261.",
  "Guo-wei Bian and Teng Shun-yuan. 2005. Integrating query translation and text classi\ufb01cation in a cross- language patent access system. In Proceedings of NTCIR-7 Workshop Meeting, pages 252\u2013261. Kyunghyun Cho, Bart Van Merrienboer, Caglar Gul- cehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. Computer Sci- ence. Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence model- ing. In Proceedings of NIPS. Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang, Ting Liu, and Guoping Hu. 2017. Attention-over- attention neural networks for reading comprehen- sion. In Proceedings of ACL.",
  "In Proceedings of NIPS. Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang, Ting Liu, and Guoping Hu. 2017. Attention-over- attention neural networks for reading comprehen- sion. In Proceedings of ACL. Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. 2017. Gated-attention readers for text comprehen- sion. In Proceedings of ACL. Wei He, Kai Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wang. 2017. Dureader: a chinese machine reading comprehen- sion dataset from real-world applications. arXiv preprint arXiv:1711.05073. Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional neural network architec- tures for matching natural language sentences.",
  "arXiv preprint arXiv:1711.05073. Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional neural network architec- tures for matching natural language sentences. In Proceedings of NIPS, pages 2042\u20132050. Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehen- sion. In Proceedings of ACL. Daniel Martin Katz, Michael J Bommarito II, and Josh Blackman. 2017. A general approach for predict- ing the behavior of the supreme court of the united states. PloS one, 12(4). R Keown. 1980. Mathematical models for legal pre- diction. Computer/LJ, 2:829. Yoon Kim. 2014. Convolutional neural networks for sentence classi\ufb01cation. In Proceedings of EMNLP. Diederik Kingma and Jimmy Ba. 2015.",
  "Computer/LJ, 2:829. Yoon Kim. 2014. Convolutional neural networks for sentence classi\ufb01cation. In Proceedings of EMNLP. Diederik Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceedings of ICLR. Fred Kort. 1957. Predicting supreme court decisions mathematically: A quantitative analysis of the \u201dright to counsel\u201d cases. American Political Science Re- view, 51(1):1\u201312. Benjamin E Lauderdale and Tom S Clark. 2012. The supreme court\u2019s many median justices. American Political Science Review, 106(4):847\u2013866. Wan-Chen Lin, Tsung-Ting Kuo, Tung-Jia Chang, Chueh-An Yen, Chao-Ju Chen, and Shou-de Lin. 2012. Exploiting machine learning models for chi- nese legal documents labeling, case classi\ufb01cation, and sentencing prediction. In Processdings of RO- CLING, page 140.",
  "2012. Exploiting machine learning models for chi- nese legal documents labeling, case classi\ufb01cation, and sentencing prediction. In Processdings of RO- CLING, page 140. Chao-Lin Liu, Cheng-Tsung Chang, and Jim-How Ho. 2004. Case instance generation and re\ufb01nement for case-based criminal summary judgments in chinese. JISE. Chao Lin Liu, Jim How Ho, and Jim How Ho. 2003. Classi\ufb01cation and clustering for case-based criminal summary judgments. In Proceedings of the Interna- tional Conference on Arti\ufb01cial Intelligence and Law, pages 252\u2013261. Chao-Lin Liu and Chwen-Dar Hsieh. 2006. Exploring phrase-based classi\ufb01cation of judicial documents for criminal charges in chinese. In Proceedings of IS- MIS, pages 681\u2013690. Yi Hung Liu and Yen Liang Chen. 2017. A two-phase sentiment analysis approach for judgement predic- tion. Journal of Information Science.",
  "In Proceedings of IS- MIS, pages 681\u2013690. Yi Hung Liu and Yen Liang Chen. 2017. A two-phase sentiment analysis approach for judgement predic- tion. Journal of Information Science. Bingfeng Luo, Yansong Feng, Jianbo Xu, Xiang Zhang, and Dongyan Zhao. 2017. Learning to pre- dict charges for criminal cases with legal basis. In Proceedings of EMNLP. Tomas Mikolov, Kai Chen, Greg Corrado, and Jef- frey Dean. 2013a. Ef\ufb01cient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013b. Distributed representa- tions of words and phrases and their compositional- ity. In Proceedings of NIPS, pages 3111\u20133119. Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017.",
  "2013b. Distributed representa- tions of words and phrases and their compositional- ity. In Proceedings of NIPS, pages 3111\u20133119. Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to match using local and distributed representations of text for web search. In Proceed- ings of WWW, pages 1291\u20131299.",
  "Stuart S Nagel. 1963. Applying correlation analysis to case prediction. Texas Law Review, 42:1006. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. Ms marco: A human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of EMNLP. Tim Rocktaschel, Edward Grefenstette, Karl Moritz Hermann, Tomas Kocisky, and Phil Blunsom. 2016. Reasoning about entailment with neural attention. In Proceedings of ICLR. Jeffrey A Segal. 1984. Predicting supreme court cases probabilistically: The search and seizure cases, 1962-1981. American Political Science Review, 78(4):891\u2013900.",
  "In Proceedings of ICLR. Jeffrey A Segal. 1984. Predicting supreme court cases probabilistically: The search and seizure cases, 1962-1981. American Political Science Review, 78(4):891\u2013900. Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from over\ufb01tting. JMLR, 15(1):1929\u20131958. Octavia Maria Sulea, Marcos Zampieri, Mihaela Vela, and Josef Van Genabith. 2017. Exploring the use of text classi cation in the legal domain. In Proceedings of ASAIL workshop. Duyu Tang, Bing Qin, and Ting Liu. 2015. Document modeling with gated recurrent neural network for sentiment classi\ufb01cation. In Proceedings of EMNLP, pages 1422\u20131432. S Sidney Ulmer. 1963.",
  "Duyu Tang, Bing Qin, and Ting Liu. 2015. Document modeling with gated recurrent neural network for sentiment classi\ufb01cation. In Proceedings of EMNLP, pages 1422\u20131432. S Sidney Ulmer. 1963. Quantitative analysis of judi- cial processes: Some practical and theoretical appli- cations. Law & Contemp. Probs., 28:164. Shuohang Wang and Jing Jiang. 2016. Learning natu- ral language inference with lstm. In Proceedings of NAACL. Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and Ming Zhou. 2017. Gated self-matching net- works for reading comprehension and question an- swering. In Proceedings of ACL, volume 1, pages 189\u2013198. Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alexander J Smola, and Eduard H Hovy. 2016. Hi- erarchical attention networks for document classi- \ufb01cation. In Proceedings of NAACL, pages 1480\u2013 1489.",
  "2016. Hi- erarchical attention networks for document classi- \ufb01cation. In Proceedings of NAACL, pages 1480\u2013 1489. Pingping Zhuge. 2016. Chinese Law Yearbook. The Chinese Law Yearbook Press."
]