{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "arXiv:2004.01694v1  [cs.CL]  3 Apr 2020 Journal of Arti\ufb01cial Intelligence Research 67 (2020) 653\u2013672 Submitted 02\/2019; published 03\/2020 A Set of Recommendations for Assessing Human\u2013Machine Parity in Language Translation Samuel L\u00a8aubli laeubli@cl.uzh.ch Institute of Computational Linguistics, University of Zurich Sheila Castilho sheila.castilho@adaptcentre.ie ADAPT Centre, Dublin City University Graham Neubig gneubig@cs.cmu.edu Language Technologies Institute, Carnegie Mellon University Rico Sennrich sennrich@cl.uzh.ch Institute of Computational Linguistics, University of Zurich Qinlan Shen qinlans@cs.cmu.edu Language Technologies Institute, Carnegie Mellon University Antonio Toral a.toral.ruiz@rug.nl Center for Language and Cognition, University of Groningen Abstract The quality of machine translation has increased remarkably over the past years, to the degree that it was found to be indistinguishable from professional human translation in a number of empirical investigations.",
            "We reassess Hassan et al.\u2019s 2018 investigation into Chinese to English news translation, showing that the \ufb01nding of human\u2013machine parity was owed to weaknesses in the evaluation design\u2014which is currently considered best practice in the \ufb01eld. We show that the professional human translations contained signi\ufb01cantly fewer errors, and that perceived quality in human evaluation depends on the choice of raters, the availability of linguistic context, and the creation of reference translations. Our results call for revisiting current best practices to assess strong machine translation systems in general and human\u2013machine parity in particular, for which we o\ufb00er a set of recommendations based on our empirical \ufb01ndings. 1. Introduction Machine translation (MT) has made astounding progress in recent years thanks to improve- ments in neural modelling (Bahdanau, Cho, & Bengio, 2015; Sutskever, Vinyals, & Le, 2014; Vaswani et al., 2017), and the resulting increase in translation quality is creating new chal- lenges for MT evaluation.",
            "Human evaluation remains the gold standard, but there are many design decisions that potentially a\ufb00ect the validity of such a human evaluation. This paper is a response to two recent human evaluation studies in which some neural ma- chine translation systems reportedly performed at (or above) the level of human translators c\u20dd2020 AI Access Foundation. All rights reserved.",
            "L\u00a8aubli, Castilho, Neubig, Sennrich, Shen, & Toral for news translation from Chinese to English (Hassan et al., 2018) and English to Czech (Bojar et al., 2018; Popel, 2018). Both evaluations were based on current best practices in the \ufb01eld: they used a source-based direct assessment with non-expert annotators, using data sets and the evaluation protocol of the Conference on Machine Translation (WMT). While the results are intriguing, especially because they are based on best practices in MT evaluation, Bojar et al. (2018, p. 293) warn against taking their results as evidence for human\u2013machine parity, and caution that \u201cfor well-resourced language pairs, an update of WMT evaluation style will be needed to keep up with the progress in machine translation.\u201d We concur that these \ufb01ndings have demonstrated the need to critically re-evaluate the design of human MT evaluation. Our paper investigates three aspects of human MT evaluation, with a special focus on assessing human\u2013machine parity: the choice of raters, the use of linguistic context, and the creation of reference translations.",
            "Our paper investigates three aspects of human MT evaluation, with a special focus on assessing human\u2013machine parity: the choice of raters, the use of linguistic context, and the creation of reference translations. We focus on the data shared by Hassan et al. (2018), and empirically test to what extent changes in the evaluation design a\ufb00ect the outcome of the human evaluation.1 We \ufb01nd that for all three aspects, human translations are judged more favourably, and signi\ufb01cantly better than MT, when we make changes that we believe strengthen the evaluation design. Based on our empirical \ufb01ndings, we formulate a set of recommendations for human MT evaluation in general, and assessing human\u2013machine parity in particular. All of our data are made publicly available for external validation and further analysis.2 2. Background We \ufb01rst review current methods to assess the quality of machine translation system outputs, and highlight potential issues in using these methods to compare such outputs to translations produced by professional human translators. 2.1 Human Evaluation of Machine Translation The evaluation of MT quality has been the subject of controversial discussions in research and the language services industry for decades due to its high economic importance.",
            "2.1 Human Evaluation of Machine Translation The evaluation of MT quality has been the subject of controversial discussions in research and the language services industry for decades due to its high economic importance. While automatic evaluation methods are particularly important in system development, there is consensus that a reliable evaluation should\u2014despite high costs\u2014be carried out by hu- mans. Various methods have been proposed for the human evaluation of MT quality (c.f. Castilho, Doherty, Gaspari, & Moorkens, 2018). What they have in common is that the MT output to be rated is paired with a translation hint: the source text or a reference translation. The MT output is then either adapted or scored with reference to the translation hint by human post-editors or raters, respectively. 1. Our results synthesise and extend those reported by L\u00a8aubli, Sennrich, and Volk (2018) and Toral, Castilho, Hu, and Way (2018). 2. https:\/\/github.com\/ZurichNLP\/mt-parity-assessment-data",
            "Assessing Human\u2013Machine Parity in Language Translation As part of the large-scale evaluation campaign at WMT, two primary evaluation methods have been used in recent years: relative ranking and direct assessment (Bojar, Federmann, et al., 2016). In the case of relative ranking, raters are presented with outputs from two or more systems, which they are asked to evaluate relative to each other (e.g., to determine system A is better than system B). Ties (e.g., system A is as good or as bad as system B) are typically allowed. Compared to absolute scores on Likert scales, data obtained through relative ranking show better inter- and intra-annotator agreement (Callison-Burch, Fordyce, Koehn, Monz, & Schroeder, 2007). However, they do not allow conclusions to be drawn about the order of magnitude of the di\ufb00erences, so that it is not possible to determine how much better system A was than system B. This is one of the reasons why direct assessment has prevailed as an evaluation method more recently.",
            "However, they do not allow conclusions to be drawn about the order of magnitude of the di\ufb00erences, so that it is not possible to determine how much better system A was than system B. This is one of the reasons why direct assessment has prevailed as an evaluation method more recently. In contrast to relative ranking, the raters are presented with one MT output at a time, to which they assign a score between 0 and 100. To increase homogeneity, each rater\u2019s ratings are standardised (Graham, Baldwin, Mo\ufb00at, & Zobel, 2013). Reference translations serve as the basis in the context of WMT, and evaluations are carried out by monolingual raters. To avoid reference bias, the evaluation can be based on source texts instead, which presupposes bilingual raters, but leads to more reliable results overall (Bentivogli, Cettolo, Federico, & Federmann, 2018). 2.2 Assessing Human\u2013Machine Parity Hassan et al.",
            "2.2 Assessing Human\u2013Machine Parity Hassan et al. (2018) base their claim of achieving human\u2013machine parity on a source-based direct assessment as described in the previous section, where they found no signi\ufb01cant di\ufb00erence in ratings between the output of their MT system and a professional human translation. Similarly, Bojar et al. (2018) report that the best-performing English to Czech system submitted to WMT 2018 (Popel, 2018) signi\ufb01cantly outperforms the human refer- ence translation. However, the authors caution against interpreting their results as evidence of human\u2013machine parity, highlighting potential limitations of the evaluation. In this study, we address three aspects that we consider to be particularly relevant for human evaluation of MT, with a special focus on testing human\u2013machine parity: the choice of raters, the use of linguistic context, and the construction of reference translations. Choice of Raters The human evaluation of MT output in research scenarios is typically conducted by crowd workers in order to minimise costs.",
            "Choice of Raters The human evaluation of MT output in research scenarios is typically conducted by crowd workers in order to minimise costs. Callison-Burch (2009) shows that aggregated assessments of bilingual crowd workers are \u201cvery similar\u201d to those of MT devel- opers, and Graham, Baldwin, Mo\ufb00at, and Zobel (2017), based on experiments with data from WMT 2012, similarly conclude that with proper quality control, MT systems can be evaluated by crowd workers. Hassan et al. (2018) also use bilingual crowd workers, but the studies supporting the use of crowdsourcing for MT evaluation were performed with older MT systems, and their \ufb01ndings may not carry over to the evaluation of contemporary higher-quality neural machine translation (NMT) systems. In addition, the MT developers to which crowd workers were compared are usually not professional translators. We hy- pothesise that expert translators will provide more nuanced ratings than non-experts, and",
            "L\u00a8aubli, Castilho, Neubig, Sennrich, Shen, & Toral that their ratings will show a higher di\ufb00erence between MT outputs and human transla- tions. Linguistic Context MT has been evaluated almost exclusively at the sentence level, owing to the fact that most MT systems do not yet take context across sentence boundaries into account. However, when machine translations are compared to those of professional translators, the omission of linguistic context\u2014e. g., by random ordering of the sentences to be evaluated\u2014does not do justice to humans who, in contrast to most MT systems, can and do take inter-sentential context into account (Voigt & Jurafsky, 2012; Wang, Tu, Way, & Liu, 2017). We hypothesise that an evaluation of sentences in isolation, as applied by Hassan et al. (2018), precludes raters from detecting translation errors that become apparent only when inter-sentential context is available, and that they will judge MT quality less favourably when evaluating full documents.",
            "We hypothesise that an evaluation of sentences in isolation, as applied by Hassan et al. (2018), precludes raters from detecting translation errors that become apparent only when inter-sentential context is available, and that they will judge MT quality less favourably when evaluating full documents. Reference Translations The human reference translations with which machine transla- tions are compared within the scope of a human\u2013machine parity assessment play an impor- tant role. Hassan et al. (2018) used all source texts of the WMT 2017 Chinese\u2013English test set for their experiments, of which only half were originally written in Chinese; the other half were translated from English into Chinese. Since translated texts are usually simpler than their original counterparts (Laviosa-Braithwaite, 1998), they should be easier to translate for MT systems. Moreover, di\ufb00erent human translations of the same source text sometimes show considerable di\ufb00erences in quality, and a comparison with an MT system only makes sense if the human reference translations are of high quality. Hassan et al.",
            "Moreover, di\ufb00erent human translations of the same source text sometimes show considerable di\ufb00erences in quality, and a comparison with an MT system only makes sense if the human reference translations are of high quality. Hassan et al. (2018), for ex- ample, had the WMT source texts re-translated as they were not convinced of the quality of the human translations in the test set. At WMT 2018, the organisers themselves noted that \u201cthe manual evaluation included several reports of ill-formed reference translations\u201d (Bojar et al., 2018, p. 292). We hypothesise that the quality of the human translations has a signi\ufb01cant e\ufb00ect on \ufb01ndings of human\u2013machine parity, which would indicate that it is necessary to ensure that human translations used to assess parity claims need to be carefully vetted for their quality. We empirically test and discuss the impact of these factors on human evaluation of MT in Sections 3\u20135.",
            "We empirically test and discuss the impact of these factors on human evaluation of MT in Sections 3\u20135. Based on our \ufb01ndings, we then distil a set of recommendations for hu- man evaluation of strong MT systems, with a focus on assessing human\u2013machine parity (Section 6). 2.3 Translations We use English translations of the Chinese source texts in the WMT 2017 English\u2013Chinese test set (Bojar et al., 2017) for all experiments presented in this article: HA The professional human translations in the dataset of Hassan et al. (2018).1 HB Professional human translations that we ordered from a di\ufb00erent translation vendor, which included a post-hoc native English check. We produced these only for the documents that were originally Chinese, as discussed in more detail in Section 5.2.",
            "Assessing Human\u2013Machine Parity in Language Translation MT1 The machine translations produced by Hassan et al.\u2019s (2018) best system (Combo- 6),1 for which the authors found parity with HA. MT2 The machine translations produced by Google\u2019s production system (Google Trans- late) in October 2017, as contained in Hassan et al.\u2019s (2018) dataset.1 Statistical signi\ufb01cance is denoted by * (p \u2264.05), ** (p \u2264.01), and *** (p \u2264.001) throughout this article, unless otherwise stated. 3. Choice of Raters Both professional and amateur evaluators can be involved in human evaluation of MT quality. However, from published work in the \ufb01eld (Doherty, 2017), it is fair to say that there is a tendency to \u201crely on students and amateur evaluators, sometimes with an unde\ufb01ned (or self-rated) pro\ufb01ciency in the languages involved, an unknown expertise with the text type\u201d (Castilho et al., 2018, p. 23).",
            "23). Previous work on evaluation of MT output by professional translators against crowd work- ers by Castilho et al. (2017) showed that for all language pairs (involving 11 languages) evaluated, crowd workers tend to be more accepting of the MT output by giving higher \ufb02uency and adequacy scores and performing very little post-editing. The authors argued that non-expert translators lack knowledge of translation and so might not notice subtle di\ufb00erences that make one translation more suitable than another, and therefore, when con- fronted with a translation that is hard to post-edit, tend to accept the MT rather than try to improve it. 3.1 Evaluation Protocol We test for di\ufb00erence in ratings of MT outputs and human translations between experts and non-experts. We consider professional translators as experts, and both crowd workers and MT researchers as non-experts.3 We conduct a relative ranking experiment using one professional human (HA) and two machine translations (MT1 and MT2), considering the native Chinese part of the WMT 2017 Chinese\u2013English test set (see Section 5.2 for details).",
            "The 299 sentences used in the experiments stem from 41 documents, randomly selected from all the documents in the test set originally written in Chinese, and are shown in their original order. Raters are shown one sentence at a time, and see the original Chinese source alongside the three translations. The previous and next source sentences are also shown, in order to provide the annotator with local inter-sentential context. Five raters\u2014two experts and three non-experts\u2014participated in the assessment. The ex- perts were professional Chinese to English translators: one native in Chinese with a \ufb02uent 3. This terminology is not consistent with other literature, where MT researchers have been referred to as experts and crowd workers as non-experts (e. g., Callison-Burch, 2009).",
            "L\u00a8aubli, Castilho, Neubig, Sennrich, Shen, & Toral Rank Translators All Experts Non-experts n = 3873 n = 1785 n = 2088 1 HA 1.939 * HA 2.247 * HA 1.324 2 MT1 1.199 * MT1 1.197 * MT1 0.940 * 3 MT2 \u22123.144 MT2 \u22123.461 MT2 \u22122.268 Table 1: Ranks and TrueSkill scores (the higher the better) of one human (HA) and two machine translations (MT1, MT2) for evaluations carried out by expert and non-expert translators. An asterisk next to a translation indicates that this translation is signi\ufb01cantly better than the one in the next rank at p \u2264.05. level of English, the other native in English with a \ufb02uent level of Chinese. The non-experts were NLP researchers native in Chinese, working in an English-speaking country. The ratings are elicited with Appraise (Federmann, 2012).",
            "level of English, the other native in English with a \ufb02uent level of Chinese. The non-experts were NLP researchers native in Chinese, working in an English-speaking country. The ratings are elicited with Appraise (Federmann, 2012). We derive an overall score for each translation (HA, MT1, and MT2) based on the rankings. We use the TrueSkill method adapted to MT evaluation (Sakaguchi, Post, & Van Durme, 2014) following its usage at WMT15,4 i. e., we run 1,000 iterations of the rankings recorded with Appraise followed by clustering (signi\ufb01cance level \u03b1 = 0.05). 3.2 Results Table 1 shows the TrueSkill scores for each translation resulting from the evaluations by expert and non-expert translators.",
            "3.2 Results Table 1 shows the TrueSkill scores for each translation resulting from the evaluations by expert and non-expert translators. We \ufb01nd that translation expertise a\ufb00ects the judgement of MT1 and HA, where the rating gap is wider for the expert raters.5 This indicates that non-experts disregard translation nuances in the evaluation, which leads to a more tolerant judgement of MT systems and a lower inter-annotator agreement (\u03ba = 0.13 for non-experts versus \u03ba = 0.254 for experts). It is worth noticing that, regardless of their expertise, the performance of human raters may vary over time. For example, performance may improve or decrease due to learning e\ufb00ects or fatigue, respectively (Gonzalez, Best, Healy, Kole, & Bourne, 2011). It is likely that such longitudinal e\ufb00ects are present in our data. They should be accounted for in future work, e. g., by using trial number as an additional predictor (Toral, Wieling, & Way, 2018). 4. https:\/\/github.com\/mjpost\/wmt15 5.",
            "They should be accounted for in future work, e. g., by using trial number as an additional predictor (Toral, Wieling, & Way, 2018). 4. https:\/\/github.com\/mjpost\/wmt15 5. As mentioned before, relative ranking mostly tells whether a translation is better than another but not by how much. The TrueSkill score is able to measure that di\ufb00erence, but may be di\ufb03cult to interpret.",
            "Assessing Human\u2013Machine Parity in Language Translation 4. Linguistic Context Another concern is the unit of evaluation. Historically, machine translation has primarily operated on the level of sentences, and so has machine translation evaluation. However, it has been remarked that human raters do not necessarily understand the intended meaning of a sentence shown out-of-context (Wu et al., 2016), which limits their ability to spot some mistranslations. Also, a sentence-level evaluation will be blind to errors related to textual cohesion and coherence. While sentence-level evaluation may be good enough when evaluating MT systems of rela- tively low quality, we hypothesise that with additional context, raters will be able to make more nuanced quality assessments, and will also reward translations that show more textual cohesion and coherence. We believe that this aspect should be considered in evaluation, especially when making claims about human\u2013machine parity, since human translators can and do take inter-sentential context into account (Voigt & Jurafsky, 2012; Wang et al., 2017).",
            "We believe that this aspect should be considered in evaluation, especially when making claims about human\u2013machine parity, since human translators can and do take inter-sentential context into account (Voigt & Jurafsky, 2012; Wang et al., 2017). 4.1 Evaluation Protocol We test if the availability of document-level context a\ufb00ects human\u2013machine parity claims in terms of adequacy and \ufb02uency. In a pairwise ranking experiment, we show raters (i) isolated sentences and (ii) entire documents, asking them to choose the better (with ties allowed) from two translation outputs: one produced by a professional translator, the other by a machine translation system. We do not show reference translations as one of the two options is itself a human translation. We use source sentences and documents from the WMT 2017 Chinese\u2013English test set (see Section 2.3): documents are full news articles, and sentences are randomly drawn from these news articles, regardless of their position. We only consider articles from the test set that are native Chinese (see Section 5.2). In order to compare our results to those of Hassan et al.",
            "We only consider articles from the test set that are native Chinese (see Section 5.2). In order to compare our results to those of Hassan et al. (2018), we use both their professional human (HA) and machine translations (MT1). Each rater evaluates both sentences and documents, but never the same text in both condi- tions so as to avoid repetition priming (Francis & S\u00b4aenz, 2007). The order of experimental items as well as the placement of choices (HA, MT1; left, right) are randomised. We use spam items for quality control (Kittur, Chi, & Suh, 2008): In a small fraction of items, we render one of the two options nonsensical by randomly shu\ufb04ing the order of all translated words, except for 10 % at the beginning and end. If a rater marks a spam item as better than or equal to an actual translation, this is a strong indication that they did not read both options carefully.",
            "If a rater marks a spam item as better than or equal to an actual translation, this is a strong indication that they did not read both options carefully. We recruit professional translators (see Section 3) from proz.com, a well-known online mar- ket place for professional freelance translation, considering Chinese to English translators and native English revisers for the adequacy and \ufb02uency conditions, respectively. In each condition, four raters evaluate 50 documents (plus 5 spam items) and 104 sentences (plus",
            "L\u00a8aubli, Castilho, Neubig, Sennrich, Shen, & Toral Context N Adequacy Fluency MT1 Tie HA p MT1 Tie HA p Sentence 208 49.5 % 9.1 % 41.4 % 31.7 % 17.3 % 51.0 % ** Document 200 37.0 % 11.0 % 52.0 % * 22.0 % 28.5 % 49.5 % *** Table 2: Pairwise ranking results for machine (MT1) against professional human translation (HA) as obtained from blind evaluation by professional translators. Preference for MT1 is lower when document-level context is available. 16 spam items). We use two non-overlapping sets of documents and two non-overlapping sets of sentences, and each is evaluated by two raters. 4.2 Results Results are shown in Table 2. We note that sentence ratings from two raters are excluded from our analysis because of unintentional textual overlap with documents, meaning we cannot fully rule out that sentence-level decisions were informed by access to the full docu- ments they originated from.",
            "4.2 Results Results are shown in Table 2. We note that sentence ratings from two raters are excluded from our analysis because of unintentional textual overlap with documents, meaning we cannot fully rule out that sentence-level decisions were informed by access to the full docu- ments they originated from. Moreover, we exclude document ratings from one rater in the \ufb02uency condition because of poor performance on spam items, and recruit an additional rater to re-rate these documents. We analyse our data using two-tailed Sign Tests, the null hypothesis being that raters do not prefer MT1 over HA or vice versa, implying human\u2013machine parity. Following WMT evaluation campaigns that used pairwise ranking (e. g., Bojar et al., 2013), the number of successes x is the number of ratings in favour of HA, and the number of trials n is the number of all ratings except for ties. Adding half of the ties to x and the total number of ties to n (Emerson & Simon, 1979) does not impact the signi\ufb01cance levels reported in this section.",
            "Adding half of the ties to x and the total number of ties to n (Emerson & Simon, 1979) does not impact the signi\ufb01cance levels reported in this section. Adequacy raters show no statistically signi\ufb01cant preference for MT1 or HA when evaluating isolated sentences (x = 86, n = 189, p = .244). This is in accordance with Hassan et al. (2018), who found the same in a source-based direct assessment experiment with crowd workers. With the availability of document-level context, however, preference for MT1 drops from 49.5 to 37.0 % and is signi\ufb01cantly lower than preference for human translation (x = 104, n = 178, p < .05). This evidences that document-level context cues allow raters to get a signal on adequacy. Fluency raters prefer HA over MT1 both on the level of sentences (x = 106, n = 172, p < .01) and documents (x = 99, n = 143, p < .001).",
            "Fluency raters prefer HA over MT1 both on the level of sentences (x = 106, n = 172, p < .01) and documents (x = 99, n = 143, p < .001). This is somewhat surprising given that in- creased \ufb02uency was found to be one of the main strengths of NMT (Bojar, Chatterjee, et al., 2016), as we further discuss in Section 5.1. The availability of document-level con- text decreases \ufb02uency raters\u2019 preference for MT1, which falls from 31.7 to 22.0 %, without increasing their preference for HA (Table 2).",
            "Assessing Human\u2013Machine Parity in Language Translation Source \u4f20\u7edf\u4e60\u4fd7\u5f15\u5165\u65b0\u4eae\u70b9\u201c2016\u76c2 \u76c2 \u76c2\u5170 \u5170 \u5170\u6587 \u6587 \u6587\u5316 \u5316 \u5316\u8282 \u8282 \u8282\u201d\u9999\u6e2f\u7ef4\u56ed\u5f00\u5e55\u6572\u9523\u6253\u9f13\u7684\u97f3\u4e50\u3001\u4f20\u7edf\u7684 \u5c0f\u98df\u3001\u82b1\u4fcf\u7684\u88c5\u9970\u3001\u4eba\u7fa4\u6c79\u6d8c\u7684\u73b0\u573a\u3002\u7531\u9999\u6e2f\u6f6e\u5c5e\u793e\u56e2\u603b\u4f1a\u4e3b\u529e\u7684\u201c2016\u76c2 \u76c2 \u76c2\u5170 \u5170 \u5170 \u6587 \u6587 \u6587\u5316 \u5316 \u5316\u8282 \u8282 \u8282\u201d12\u65e5\u81f314\u65e5\u5728\u7ef4\u591a\u5229\u4e9a\u516c\u56ed\u4e3e\u529e\uff0c\u8fd9\u662f\u9999\u6e2f\u6700\u76db\u5927\u7684\u4e00\u573a\u76c2\u5170\u80dc\u4f1a\u3002 HA Traditional customs with new highlights - 2016 Ullam Cultural Festival un- veiled at Victoria Park in Hong Kong Music with drums and gongs, traditional snacks, fanciful decorations, and a chock-a-block crowd at the scene.",
            "The \u201c2016 Ullam Cultural Festival\u201d organized by the Federation of Hong Kong Chiu Chow Community Organizations will be held at Victoria Park from 12th to the 14th. MT1 Traditional customs introduce new bright spot \u201c2016 Ullambana Cultural Festival\u201d Hong Kong Victoria Park opening Gongs and drums music, traditional snacks, fancy decorations, the crowd surging scene. Organised by the Federation of Teochew Societies in Hong Kong, the \u201c2016 Python Cultural Festival\u201d is held at Victoria Park from 12 to 14 July. Table 3: Two consecutive sentences of a Chinese news article as translated into English by a professional human translator (HA) and a machine translation system (MT1). Emphasis added. 4.3 Discussion Our \ufb01ndings emphasise the importance of linguistic context in human evaluation of MT. In terms of adequacy, raters assessing documents as a whole show a signi\ufb01cant preference for human translation, but when assessing single sentences in random order, they show no signi\ufb01cant preference for human translation.",
            "In terms of adequacy, raters assessing documents as a whole show a signi\ufb01cant preference for human translation, but when assessing single sentences in random order, they show no signi\ufb01cant preference for human translation. Document-level evaluation exposes errors to raters which are hard or impossible to spot in a sentence-level evaluation, such as coherent translation of named entities. The example in Table 3 shows the \ufb01rst two sentences of a Chinese news article as translated by a professional human translator (HA) and Hassan et al.\u2019s (2018) NMT system (MT1). When looking at both sentences (document-level evaluation), it can be seen that MT1 uses two di\ufb00erent translations to refer to a cultural festival, \u201c2016\u76c2\u5170\u6587\u5316\u8282\u201d, whereas the human translation uses only one. When assessing the second sentence out of context (sentence-level evaluation), it is hard to penalise MT1 for producing \u201c2016 Python Cultural Festival\u201d, particularly for \ufb02uency raters without access to the corresponding source text. For further examples, see Section 5.1 and Table 6. 5.",
            "For further examples, see Section 5.1 and Table 6. 5. Reference Translations Yet another relevant element in human evaluation is the reference translation used. This is the focus of this section, where we cover two aspects of reference translations that can have an impact on evaluation: quality and directionality.",
            "L\u00a8aubli, Castilho, Neubig, Sennrich, Shen, & Toral 5.1 Quality Because the translations are created by humans, a number of factors could lead to compro- mises in quality: Errors in Understanding: If the translator is a non-native speaker of the source lan- guage, they may make mistakes in interpreting the original message. This is partic- ularly true if the translator does not normally work in the domain of the text, e. g., when a translator who normally works on translating electronic product manuals is asked to translate news. Errors in Fluency: If the translator is a non-native speaker of the target language, they might not be able to generate completely \ufb02uent text. This similarly applies to domain- speci\ufb01c terminology. Limited Resources: Unlike computers, human translators have limits in time, attention, and motivation, and will generally do a better job when they have su\ufb03cient time to check their work, or are particularly motivated to do a good job, such as when doing a good job is necessary to maintain their reputation as a translator.",
            "E\ufb00ects of Post-editing: In recent years, a large number of human translation jobs are performed by post-editing MT output, which can result in MT artefacts remaining even after manual post-editing (Castilho, Resende, & Mitkov, 2019; Daems, Vande- pitte, Hartsuiker, & Macken, 2017; Toral, 2019). In this section, we examine the e\ufb00ect of the quality of underlying translations on the conclu- sions that can be drawn with regards to human\u2013machine parity. We \ufb01rst do an analysis on (i) how the source of the human translation a\ufb00ects claims of human\u2013machine parity, and (ii) whether signi\ufb01cant di\ufb00erences exist between two varieties of human translation. We follow the same protocol as in Section 4.1, having 4 professional translators per condition, evaluate the translations for adequacy and \ufb02uency on both the sentence and document level.6 The results are shown in Table 4.",
            "We follow the same protocol as in Section 4.1, having 4 professional translators per condition, evaluate the translations for adequacy and \ufb02uency on both the sentence and document level.6 The results are shown in Table 4. From this, we can see that the human translation HB, which was aggressively edited to ensure target \ufb02uency, resulted in lower adequacy (Table 4b). With more \ufb02uent and less accurate translations, raters do not prefer human over machine translation in terms of adequacy (Table 4a), but have a stronger preference for human translation in terms of \ufb02uency (compare Tables 4a and 2). In a direct comparison of the two human translations (Table 4b), we also \ufb01nd that HA is considered signi\ufb01cantly more adequate than HB, while there is no signi\ufb01cant di\ufb00erence in \ufb02uency. To achieve a \ufb01ner-grained understanding of what errors the evaluated translations exhibit, we perform a categorisation of 150 randomly sampled sentences based on the classi\ufb01cation used by Hassan et al.",
            "To achieve a \ufb01ner-grained understanding of what errors the evaluated translations exhibit, we perform a categorisation of 150 randomly sampled sentences based on the classi\ufb01cation used by Hassan et al. (2018).7 We expand the classi\ufb01cation with a Context category, which we use to mark errors that are only apparent in larger context (e. g., regarding poor register choice, or coreference errors), and which do not clearly \ufb01t into one of the other categories. 6. Translators were recruited from proz.com. 7. Hassan et al.\u2019s (2018) classi\ufb01cation is in turn based on, but signi\ufb01cantly di\ufb00erent than that proposed by Vilar, Xu, Luis Fernando, and Ney (2006).",
            "Assessing Human\u2013Machine Parity in Language Translation Context N Adequacy Fluency MT1 Tie HB p MT1 Tie HB p Sentence 416 34.6 % 18.8 % 46.6 % 20.7 % 21.2 % 58.2 % *** Document 200 41.0 % 9.0 % 50.0 % 18.0 % 21.0 % 61.0 % *** (a) Machine translation MT1 against professional human translation HB Context N Adequacy Fluency HA Tie HB p HA Tie HB p Sentence 416 56.7 % 10.6 % 32.7 % *** 40.4 % 24.0 % 35.6 % Document 200 64.0 % 9.0 % 27.0 % *** 34.0 % 22.0 % 44.0 % (b) Professional human translation HA against professional human translation HB Table 4: Pairwise ranking results for one machine (MT1) and two professional human translations (HA, HB) as obtained from blind evaluation by professional translators. Hassan et al.",
            "Hassan et al. (2018) perform this classi\ufb01cation only for the machine-translated outputs, and thus the natural question of whether the mistakes that humans and computers make are qualitatively di\ufb00erent is left unanswered. Our analysis was performed by one of the co-authors who is a bi-lingual native Chinese\/English speaker. Sentences were shown in the context of the document, to make it easier to determine whether the translations were correct based on the context. The analysis was performed on one machine translation (MT1) and two human translation outputs (HA, HB), using the same 150 sentences, but blinding their origin by randomising the order in which the documents were presented. We show the results of this analysis in Table 5. From these results, we can glean a few interesting insights. First, we \ufb01nd signi\ufb01cantly larger numbers of errors of the categories of Incorrect Word and Named Entity in MT1, indicating that the MT system is less e\ufb00ective at choosing correct translations for individual words than the human translators.",
            "First, we \ufb01nd signi\ufb01cantly larger numbers of errors of the categories of Incorrect Word and Named Entity in MT1, indicating that the MT system is less e\ufb00ective at choosing correct translations for individual words than the human translators. An example of this can be found in Table 6a, where we see that the MT system refers to a singular \u201cpoint of view\u201d and translates \u201c\u7ebf\u8def\u201d (channel, route, path) into the semantically similar but inadequate \u201clines\u201d. Interestingly, MT1 has signi\ufb01cantly more Word Order errors, one example of this being shown in Table 6b, with the relative placements of \u201cat the end of last year\u201d (\u53bb\u5e74\u5e74\u5e95) and \u201cstop production\u201d (\u505c\u4ea7). This result is particularly notable given previous reports that NMT systems have led to great increases in reordering accuracy compared to previous statistical MT systems (Bentivogli, Bisazza, Cettolo, & Federico, 2016; Neubig, Morishita, & Nakamura, 2015), demonstrating that the problem of generating correctly ordered output is far from solved even in very strong NMT systems.",
            "Moreover, HB had signi\ufb01cantly more Missing Word (Semantics)",
            "L\u00a8aubli, Castilho, Neubig, Sennrich, Shen, & Toral Error Category Errors Signi\ufb01cance HA HB MT1 HA \u2013 HB HA \u2013 MT1 HB \u2013 MT1 Incorrect Word 51 52 85 *** *** Semantics 33 36 48 Grammaticality 18 16 37 ** ** Missing Word 37 69 56 *** * Semantics 22 62 34 *** *** Grammaticality 15 7 22 ** Named Entity 16 19 30 * Person 1 10 10 * * Location 5 4 6 Organization 4 4 8 Event 1 1 3 Other 5 1 7 Word Order 1 4 17 *** ** Factoid 1 1 6 Word Repetition 2 4 4 Collocation 15 18 27 Unknown Words\/Misspellings 0 1 0 Context (Register, Coreference, etc.)",
            "6 9 12 Any 81 103 118 * *** Total 129 177 237 ** *** ** Table 5: Classi\ufb01cation of errors in machine translation MT1 and two professional human translation outputs HA and HB. Errors represent the number of sentences (out of N = 150) that contain at least one error of the respective type. We also report the number of sentences that contain at least one error of any category (Any), and the total number of error categories present in all sentences (Total). Statistical signi\ufb01cance is assessed with Fisher\u2019s exact test (two-tailed) for each pair of translation outputs.",
            "Assessing Human\u2013Machine Parity in Language Translation Source \u5728\u76ee\u524d\u8f83\u4e3a\u4e3b\u6d41\u7684\u89c2 \u89c2 \u89c2\u70b9 \u70b9 \u70b9\u4e2d\uff0c\u756a\u85af\u7684\u5f15\u8fdb\u4e3b\u8981\u6709\u4e09\u6761\u7ebf \u7ebf \u7ebf\u8def \u8def \u8def\u3002 HA Currently more mainstream perspectives point to three channels for the in- troduction of sweet potatoes. HB There are currently three theories in regards to how the sweet potato was introduced. MT1 In the current more mainstream point of view, the introduction of sweet potato has three main lines. (a) Incorrect Word Source \u8be5\u4f01\u4e1a\u4f4d\u4e8e\u9752\u5c9b\u8001\u57ce\u533a\u7684\u5382\u533a\u53bb \u53bb \u53bb\u5e74 \u5e74 \u5e74\u5e74 \u5e74 \u5e74\u5e95 \u5e95 \u5e95\u5168\u9762\u505c \u505c \u505c\u4ea7 \u4ea7 \u4ea7,\u73af\u4fdd\u642c\u8fc1\u81f3\u5e73\u5ea6\u7684\u65b0\u5382\u533a\u3002 HA This corporation, situated in the factory area of Old Town of Qingdao, stopped its production lines at the end of last year. HB The same company had a plant in Qingdao\u2019s old town but was shut down last year.",
            "HB The same company had a plant in Qingdao\u2019s old town but was shut down last year. MT1 The enterprise is located in the old city of Qingdao plant at the end of last year to stop production. (b) Reordering Source \u636e\u77e5\u60c5\u4eba\u58eb\u900f\u9732\uff0c\u8fd1 \u8fd1 \u8fd1\u671f \u671f \u671f\uff0c\u82cf\u5b81\u9ad8\u5c42\u4e0e\u82f9\u679c\u516c\u53f8\u9891\u9891\u89c1\u9762\uff0c\u76ee\u7684\u5c31\u662f\u4e3a\u4e86\u51c6\u5907\u5145 \u8db3\u7684\u8d27\u6e90\u3002 HA Insider disclosure revealed that the upper management of Suning and Apple has met frequently in recent times for the purpose of preparing enough resources. HB According to informed sources, Suning executives met frequently with Apple in order to prepare enough stock. MT1 According to people familiar with the matter, recently, Suning executives have been meeting with Apple frequently in order to prepare an adequate supply. (c) Missing Word (Semantics) Table 6: Qualitative examples of di\ufb00erences in types of errors found in machine (MT1) and two professional human translations (HA, HB). Emphasis added.",
            "L\u00a8aubli, Castilho, Neubig, Sennrich, Shen, & Toral Source \u5f20\u5f6c\u5f6c\u548c\u5bb6 \u5bb6 \u5bb6\u4eba \u4eba \u4eba\u805a\u5c11\u79bb\u591a... \u7236 \u7236 \u7236\u6bcd \u6bcd \u6bcd\u8bf4... \u5f20 \u5f20 \u5f20\u5f6c \u5f6c \u5f6c\u5f6c \u5f6c \u5f6c\u5f88\u5c11\u8bf4\u81ea \u81ea \u81ea\u5df1 \u5df1 \u5df1\u7684 \u7684 \u7684\u8f9b \u8f9b \u8f9b\u82e6 \u82e6 \u82e6\uff0c\u66f4\u591a\u7684\u662f\u8ddf\u7236 \u7236 \u7236 \u6bcd \u6bcd \u6bcd\u804a\u4e9b\u5f00\u5fc3\u7684\u4e8b\u3002 HA Zhang Binbin spends little time with family... Her parents said... Zhang Binbin seldom said she found things di\ufb03cult. More often, she would chat about happy things with parents. HB Zhang Binbin saw her family less... Her parents said... she would seldom talk about her hardship and would mostly talk about something happy with her parents MT1 Zhang Binbin and his family gathered less... Parents said... Zhang Binbin rarely said their hard work, more with their parents to talk about something happy.",
            "HB Zhang Binbin saw her family less... Her parents said... she would seldom talk about her hardship and would mostly talk about something happy with her parents MT1 Zhang Binbin and his family gathered less... Parents said... Zhang Binbin rarely said their hard work, more with their parents to talk about something happy. (d) Context Table 6: (Continued from previous page.) errors than both HA (p < .001) and MT1 (p < .001), an indication that the proofreading process resulted in drops of content in favour of \ufb02uency. An example of this is shown in Table 6c, where HB dropped the information that the meetings between Suning and Apple were recently (\u8fd1\u671f) held. Finally, while there was not a signi\ufb01cant di\ufb00erence, likely due to the small number of examples overall, it is noticeable that MT1 had a higher percentage of Collocation and Context errors, which indicate that the system has more trouble translating words that are dependent on longer-range context. Similarly, some Named Entity errors are also attributable to translation inconsistencies due to lack of longer-range context.",
            "Similarly, some Named Entity errors are also attributable to translation inconsistencies due to lack of longer-range context. Table 6d shows an example where we see that the MT system was unable to maintain a consistently gendered or correct pronoun for the female Olympic shooter Zhang Binbin (\u5f20\u5f6c\u5f6c). Apart from showing qualitative di\ufb00erences between the three translations, the analysis also supports the \ufb01nding of the pairwise ranking study: HA is both preferred over MT1 in the pairwise ranking study, and exhibits fewer translation errors in our error classi\ufb01cation. HB has a substantially higher number of missing words than the other two translations, which agrees with the lower perceived adequacy in the pairwise ranking. However, the analysis not only supports the \ufb01ndings of the pairwise ranking study, but also adds nuance to it. Even though HB has the highest number of deletions, and does worse than the other two translations in a pairwise adequacy ranking, it is similar to HA, and better than MT1, in terms of most other error categories. 5.2 Directionality Translation quality is also a\ufb00ected by the nature of the source text.",
            "5.2 Directionality Translation quality is also a\ufb00ected by the nature of the source text. In this respect, we note that from the 2,001 sentences in the WMT 2017 Chinese\u2013English test set, half were originally written in Chinese; the remaining half were originally written in English and then manually translated into Chinese. This Chinese reference \ufb01le (half original, half translated)",
            "Assessing Human\u2013Machine Parity in Language Translation Rank Original Language Both Chinese English n = 6675 n = 3873 n = 2802 1 HA 1.587 * HA 1.939 * MT1 1.059 2 MT1 1.231 * MT1 1.199 * HA 0.772 * 3 MT2 -2.819 MT2 -3.144 MT2 -1.832 Table 7: Ranks of the translations given the original language of the source side of the test set shown with their TrueSkill score (the higher the better). An asterisk next to a translation indicates that this translation is signi\ufb01cantly better than the one in the next rank at p \u2264.05. was then manually translated into English by Hassan et al. (2018) to make up the reference for assessing human\u2013machine parity. Therefore, 50 % of the reference comprises direct English translations from the original Chinese, while 50 % are English translations from the human-translated \ufb01le from English into Chinese, i. e., backtranslations of the original English.",
            "Therefore, 50 % of the reference comprises direct English translations from the original Chinese, while 50 % are English translations from the human-translated \ufb01le from English into Chinese, i. e., backtranslations of the original English. According to Laviosa (1998), translated texts di\ufb00er from their originals in that they are sim- pler, more explicit, and more normalised. For example, the synonyms used in an original text may be replaced by a single translation. These di\ufb00erences are referred to as transla- tionese, and have been shown to a\ufb00ect translation quality in the \ufb01eld of machine translation (Castilho et al., 2019; Daems, De Clercq, & Macken, 2017; Kurokawa, Goutte, & Isabelle, 2009; Toral, 2019). We test whether translationese has an e\ufb00ect on assessing parity between translations pro- duced by humans and machines, using relative rankings of translations in the WMT 2017 Chinese\u2013English test set by \ufb01ve raters (see Section 3).",
            "We test whether translationese has an e\ufb00ect on assessing parity between translations pro- duced by humans and machines, using relative rankings of translations in the WMT 2017 Chinese\u2013English test set by \ufb01ve raters (see Section 3). Our hypothesis is that the di\ufb00erence between human and machine translation quality is smaller when source texts are translated English (translationese) rather than original Chinese, because a translationese source text should be simpler and thus easier to translate for an MT system. We con\ufb01rm Laviosa\u2019s ob- servation that \u201ctranslationese\u201d Chinese (that started as English) exhibits less lexical variety than \u201cnatively\u201d Chinese text and demonstrate that translationese source texts are generally easier for MT systems to score well on. Table 7 shows the TrueSkill scores for translations (HA, MT1, and MT2) of the entire test set (Both) versus only the sentences originally written in Chinese or English therein.",
            "Table 7 shows the TrueSkill scores for translations (HA, MT1, and MT2) of the entire test set (Both) versus only the sentences originally written in Chinese or English therein. The human translation HA outperforms the machine translation MT1 signi\ufb01cantly when the original language is Chinese, while the di\ufb00erence between the two is not signi\ufb01cant when the original language is English (i. e., translationese input). We also compare the two subsets of the test set, original and translationese, using type- token ratio (TTR). Our hypothesis is that the TTR will be smaller for the translationese",
            "L\u00a8aubli, Castilho, Neubig, Sennrich, Shen, & Toral subset, thus its simpler nature getting re\ufb02ected in a less varied use of language. While both subsets contain a similar number of sentences (1,001 and 1,000), the Chinese subset contains more tokens (26,468) than its English counterpart (22,279). We thus take a subset of the Chinese (840 sentences) containing a similar amount of words to the English data (22,271 words). We then calculate the TTR for these two subsets using bootstrap resampling. The TTR for Chinese (M = 0.1927, SD = 0.0026, 95 % con\ufb01dence interval [0.1925, 0.1928]) is 13 % higher than that for English (M = 0.1710, SD = 0.0025, 95 % con\ufb01dence interval [0.1708, 0.1711]).",
            "Our results show that using translationese (Chinese translated from English) rather than original source texts results in higher scores for MT systems in human evaluation, and that the lexical variety of translationese is smaller than that of original text. 6. Recommendations Our experiments in Sections 3\u20135 show that machine translation quality has not yet reached the level of professional human translation, and that human evaluation methods which are currently considered best practice fail to reveal errors in the output of strong NMT systems. In this section, we recommend a set of evaluation design changes that we believe are needed for assessing human\u2013machine parity, and will strengthen the human evaluation of MT in general. (R1) Choose professional translators as raters. In our blind experiment (Section 3), non-experts assess parity between human and machine translation where professional trans- lators do not, indicating that the former neglect more subtle di\ufb00erences between di\ufb00erent translation outputs. (R2) Evaluate documents, not sentences. When evaluating sentences in random order, professional translators judge machine translation more favourably as they cannot identify errors related to textual coherence and cohesion, such as di\ufb00erent translations of the same product name.",
            "(R2) Evaluate documents, not sentences. When evaluating sentences in random order, professional translators judge machine translation more favourably as they cannot identify errors related to textual coherence and cohesion, such as di\ufb00erent translations of the same product name. Our experiments show that using whole documents (i. e., full news articles) as unit of evaluation increases the rating gap between human and machine translation (Section 4). (R3) Evaluate \ufb02uency in addition to adequacy. Raters who judge target language \ufb02uency without access to the source texts show a stronger preference for human translation than raters with access to the source texts (Sections 4 and 5.1). In all of our experiments, raters prefer human translation in terms of \ufb02uency while, just as in Hassan et al.\u2019s (2018) evaluation, they \ufb01nd no signi\ufb01cant di\ufb00erence between human and machine translation in sentence-level adequacy (Tables 2 and 4a). Our error analysis in Table 6 also indicates that MT still lags behind human translation in \ufb02uency, speci\ufb01cally in grammaticality.",
            "Our error analysis in Table 6 also indicates that MT still lags behind human translation in \ufb02uency, speci\ufb01cally in grammaticality. (R4) Do not heavily edit reference translations for \ufb02uency. In professional trans- lation work\ufb02ows, texts are typically revised with a focus on target language \ufb02uency after",
            "Assessing Human\u2013Machine Parity in Language Translation an initial translation step. As shown in our experiment in Section 5.1, aggressive revi- sion can make translations more \ufb02uent but less accurate, to the degree that they become indistinguishable from MT in terms of accuracy (Table 4a). (R5) Use original source texts. Raters show a signi\ufb01cant preference for human over machine translations of texts that were originally written in the source language, but not for source texts that are translations themselves (Section 5.2). Our results are further evidence that translated texts tend to be simpler than original texts, and in turn easier to translate with MT. Our work empirically strengthens and extends the recommendations on human MT evalua- tion in previous work (L\u00a8aubli et al., 2018; Toral, Castilho, et al., 2018), some of which have meanwhile been adopted by the large-scale evaluation campaign at WMT 2019 (Barrault et al., 2019): the new evaluation protocol uses original source texts only (R5) and gives raters access to document-level context (R2).",
            "The \ufb01ndings of WMT 2019 provide further evidence in support of our recommendations. In particular, human English to Czech translation was found to be signi\ufb01cantly better than MT (Barrault et al., 2019, p. 28); the comparison includes the same MT system (CUNI-Transformer-T2T-2018) which outperformed human translation according to the previous protocol (Bojar et al., 2018, p. 291). Results also show a larger di\ufb00erence between human translation and MT in document-level evaluation.8 We note that in contrast to WMT, the judgements in our experiments are provided by a small number of human raters: \ufb01ve in the experiments of Sections 3 and 5.2, four per condi- tion (adequacy and \ufb02uency) in Section 4, and one in the \ufb01ne-grained error analysis presented in Section 5.1.",
            "Moreover, the results presented in this article are based on one text domain (news) and one language direction (Chinese to English), and while a large-scale evaluation with another language pair supports our \ufb01ndings (see above), further experiments with more languages, domains, and raters will be required to increase their external validity. 7. Conclusion We compared professional human Chinese to English translations to the output of a strong MT system. In a human evaluation following best practices, Hassan et al. (2018) found no signi\ufb01cant di\ufb00erence between the two, concluding that their NMT system had reached parity with professional human translation. Our blind qualitative analysis, however, showed that the machine translation output contained signi\ufb01cantly more incorrect words, omissions, mistranslated names, and word order errors. Our experiments show that recent \ufb01ndings of human\u2013machine parity in language transla- tion are owed to weaknesses in the design of human evaluation campaigns. We empirically tested alternatives to what is currently considered best practice in the \ufb01eld, and found that the choice of raters, the availability of linguistic context, and the creation of reference 8.",
            "We empirically tested alternatives to what is currently considered best practice in the \ufb01eld, and found that the choice of raters, the availability of linguistic context, and the creation of reference 8. Speci\ufb01cally, the absolute di\ufb00erence between HUMAN and CUNI-Transformer-T2T-2018 in terms of aver- age standardized human scores is 11\u201322% for segment-level evaluation, 24% for segment-level evaluation with document-level context, and 39% for document-level evaluation (Barrault et al., 2019, p. 28).",
            "L\u00a8aubli, Castilho, Neubig, Sennrich, Shen, & Toral translations have a strong impact on perceived translation quality. As for the choice of raters, professional translators showed a signi\ufb01cant preference for human translation, while non-expert raters did not. In terms of linguistic context, raters found human translation signi\ufb01cantly more accurate than machine translation when evaluating full documents, but not when evaluating single sentences out of context. They also found human translation sig- ni\ufb01cantly more \ufb02uent than machine translation, both when evaluating full documents and single sentences. Moreover, we showed that aggressive editing of human reference trans- lations for target language \ufb02uency can decrease adequacy to the point that they become indistinguishable from machine translation, and that raters found human translations sig- ni\ufb01cantly better than machine translations of original source texts, but not of source texts that were translations themselves. Our results strongly suggest that in order to reveal errors in the output of strong MT systems, the design of MT quality assessments with human raters should be revisited.",
            "Our results strongly suggest that in order to reveal errors in the output of strong MT systems, the design of MT quality assessments with human raters should be revisited. To that end, we have o\ufb00ered a set of recommendations, supported by empirical data, which we believe are needed for assessing human\u2013machine parity, and will strengthen the human evaluation of MT in general. Our recommendations have the aim of increasing the validity of MT evaluation, but we are aware of the high cost of having MT evaluation done by professional translators, and on the level of full documents. We welcome future research into alternative evaluation protocols that can demonstrate their validity at a lower cost. References Bahdanau, D., Cho, K., & Bengio, Y. (2015). Neural Machine Translation by Jointly Learning to Align and Translate. In Proceedings of ICLR. San Diego, CA. Barrault, L., Bojar, O., Costa-juss`a, M. R., Federmann, C., Fishel, M., Graham, Y., . . . Zampieri, M. (2019).",
            "San Diego, CA. Barrault, L., Bojar, O., Costa-juss`a, M. R., Federmann, C., Fishel, M., Graham, Y., . . . Zampieri, M. (2019). Findings of the 2019 Conference on Machine Translation (WMT19). In Proceedings of WMT (pp. 1\u201361). Florence, Italy. Bentivogli, L., Bisazza, A., Cettolo, M., & Federico, M. (2016). Neural versus Phrase-Based Machine Translation Quality: a Case Study. In Proceedings of EMNLP (pp. 257\u2013267). Austin, Texas. Bentivogli, L., Cettolo, M., Federico, M., & Federmann, C. (2018). Machine Translation Hu- man Evaluation: an investigation of evaluation based on Post-Editing and its relation with Direct Assessment. In Proceedings of IWSLT (pp. 62\u201369). Bruges, Belgium.",
            "(2018). Machine Translation Hu- man Evaluation: an investigation of evaluation based on Post-Editing and its relation with Direct Assessment. In Proceedings of IWSLT (pp. 62\u201369). Bruges, Belgium. Bojar, O., Buck, C., Callison-Burch, C., Federmann, C., Haddow, B., Koehn, P., . . . Specia, L. (2013). Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of WMT (pp. 1\u201344). So\ufb01a, Bulgaria. Bojar, O., Chatterjee, R., Federmann, C., Graham, Y., Haddow, B., Huang, S., . . . Turchi, M. (2017). Findings of the 2017 Conference on Machine Translation (WMT17). In Proceedings of WMT (pp. 169\u2013214). Copenhagen, Denmark. Bojar, O., Chatterjee, R., Federmann, C., Graham, Y., Haddow, B., Huck, M., . .",
            "In Proceedings of WMT (pp. 169\u2013214). Copenhagen, Denmark. Bojar, O., Chatterjee, R., Federmann, C., Graham, Y., Haddow, B., Huck, M., . . . Zampieri, M. (2016). Findings of the 2016 Conference on Machine Translation. In Proceedings of WMT (pp. 131\u2013198). Berlin, Germany. Bojar, O., Federmann, C., Fishel, M., Graham, Y., Haddow, B., Huck, M., . . . Monz, C. (2018). Findings of the 2018 Conference on Machine Translation (WMT18). In",
            "Assessing Human\u2013Machine Parity in Language Translation Proceedings of WMT (pp. 272\u2013307). Belgium, Brussels. Bojar, O., Federmann, C., Haddow, B., Koehn, P., Post, M., & Specia, L. (2016). Ten Years of WMT Evaluation Campaigns: Lessons Learnt. In Workshop on Translation Evaluation: From Fragmented Tools and Data Sets to an Integrated Ecosystem (pp. 27\u201336). Portoroz, Slovenia. Callison-Burch, C. (2009). Fast, Cheap, and Creative: Evaluating Translation Quality Using Amazon\u2019s Mechanical Turk. In Proceedings of EMNLP (pp. 286\u2013295). Singapore. Callison-Burch, C., Fordyce, C., Koehn, P., Monz, C., & Schroeder, J. (2007). (Meta-) evaluation of machine translation. In Proceedings of WMT (pp. 136\u2013158). Prague, Czech Republic.",
            "(2007). (Meta-) evaluation of machine translation. In Proceedings of WMT (pp. 136\u2013158). Prague, Czech Republic. Castilho, S., Doherty, S., Gaspari, F., & Moorkens, J. (2018). Approaches to Human and Machine Translation Quality Assessment. In Translation Quality Assessment: From Principles to Practice (Vol. 1, pp. 9\u201338). Springer International Publishing. Castilho, S., Moorkens, J., Gaspari, F., Way, A., Georgakopoulou, P., Gialama, M., . . . Sennrich, R. (2017). Crowdsourcing for NMT evaluation: Professional translators versus the crowd. In Proceedings of Translating and the Computer 39. London, UK. Castilho, S., Resende, N., & Mitkov, R. (2019). What In\ufb02uences the Features of Post- editese? A Preliminary Study. In Proceedings of HiT-IT (pp. 19\u201327).",
            "Castilho, S., Resende, N., & Mitkov, R. (2019). What In\ufb02uences the Features of Post- editese? A Preliminary Study. In Proceedings of HiT-IT (pp. 19\u201327). Varna, Bul- garia. Daems, J., De Clercq, O., & Macken, L. (2017). Translationese and Post-editese: How comparable is comparable quality? Linguistica Antverpiensia, New Series \u2013 Themes in Translation Studies, 16, 89\u2013103. Daems, J., Vandepitte, S., Hartsuiker, R., & Macken, L. (2017). Translation methods and experience: A comparative analysis of human translation and post-editing with students and professional translators. Meta, 62(2), 245\u2013270. Doherty, S. (2017). Issues in human and automatic translation quality assessment. In Human issues in translation technology (pp. 131\u2013148). Routledge. Emerson, J. D., & Simon, G. A.",
            "Doherty, S. (2017). Issues in human and automatic translation quality assessment. In Human issues in translation technology (pp. 131\u2013148). Routledge. Emerson, J. D., & Simon, G. A. (1979). Another Look at the Sign Test When Ties Are Present: The Problem of Con\ufb01dence Intervals. The American Statistician, 33(3), 140\u2013142. Federmann, C. (2012). Appraise: An Open-Source Toolkit for Manual Evaluation of Ma- chine Translation Output. The Prague Bulletin of Mathematical Linguistics, 98, 25\u201335. (Code available at https:\/\/github.com\/cfedermann\/Appraise.) Francis, W. S., & S\u00b4aenz, S. P. (2007). Repetition priming endurance in picture naming and translation: Contributions of component processes. Memory & Cognition, 35(3), 481\u2013493. Gonzalez, C., Best, B., Healy, A. F., Kole, J. A., & Bourne, L. E.",
            "Memory & Cognition, 35(3), 481\u2013493. Gonzalez, C., Best, B., Healy, A. F., Kole, J. A., & Bourne, L. E. (2011). A cognitive modeling account of simultaneous learning and fatigue e\ufb00ects. Cognitive Systems Research, 12(1), 19\u201332. Graham, Y., Baldwin, T., Mo\ufb00at, A., & Zobel, J. (2013). Continuous Measurement Scales in Human Evaluation of Machine Translation. In Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse (pp. 33\u201341). So\ufb01a, Bulgaria. Graham, Y., Baldwin, T., Mo\ufb00at, A., & Zobel, J. (2017). Can machine translation systems be evaluated by the crowd alone? Natural Language Engineering, 23(1), 3\u201330. Hassan, H., Aue, A., Chen, C., Chowdhary, V., Clark, J., Federmann, C., . . .",
            "L\u00a8aubli, Castilho, Neubig, Sennrich, Shen, & Toral Zhou, M. (2018). Achieving Human Parity on Automatic Chinese to En- glish News Translation. arXiv preprint 1803.05567. (Data available at http:\/\/aka.ms\/Translator-HumanParityData.) Kittur, A., Chi, E. H., & Suh, B. (2008). Crowdsourcing User Studies with Mechanical Turk. In Proceedings of CHI (pp. 453\u2013456). Florence, Italy. Kurokawa, D., Goutte, C., & Isabelle, P. (2009). Automatic detection of translated text and its impact on machine translation. In Proceedings of MT-Summit XII (pp. 81\u201388). Laviosa, S. (1998). Core Patterns of Lexical Use in a Comparable Corpus of English Narrative Prose. Meta, 43(4), 557\u2013570. Laviosa-Braithwaite, S. (1998). Universals of translation.",
            "Laviosa, S. (1998). Core Patterns of Lexical Use in a Comparable Corpus of English Narrative Prose. Meta, 43(4), 557\u2013570. Laviosa-Braithwaite, S. (1998). Universals of translation. In Routledge Encyclopedia of Translation Studies (pp. 288\u2013291). Routledge. L\u00a8aubli, S., Sennrich, R., & Volk, M. (2018). Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation. In Proceedings of EMNLP (pp. 4791\u2013 4796). Brussels, Belgium. Neubig, G., Morishita, M., & Nakamura, S. (2015). Neural Reranking Improves Subjective Quality of Machine Translation: NAIST at WAT2015. In Proceedings of WAT2015. Kyoto, Japan. Popel, M. (2018). CUNI Transformer Neural MT System for WMT18. In Proceedings of WMT (pp. 486\u2013491).",
            "In Proceedings of WAT2015. Kyoto, Japan. Popel, M. (2018). CUNI Transformer Neural MT System for WMT18. In Proceedings of WMT (pp. 486\u2013491). Brussels, Belgium. Sakaguchi, K., Post, M., & Van Durme, B. (2014). E\ufb03cient Elicitation of Annotations for Human Evaluation of Machine Translation. In Proceedings of WMT (pp. 1\u201311). Baltimore, MD. Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to Sequence Learning with Neural Networks. In Proceedings of NIPS (pp. 3104\u20133112). Montreal, Canada. Toral, A. (2019). Post-editese: an Exacerbated Translationese. In Proceedings of MT Sum- mit (pp. 273\u2013281). Dublin, Ireland: European Association for Machine Translation. Toral, A., Castilho, S., Hu, K., & Way, A. (2018).",
            "In Proceedings of MT Sum- mit (pp. 273\u2013281). Dublin, Ireland: European Association for Machine Translation. Toral, A., Castilho, S., Hu, K., & Way, A. (2018). Attaining the Unattainable? Reassessing Claims of Human Parity in Neural Machine Translation. In Proceedings of WMT (pp. 113\u2013123). Brussels, Belgium. Toral, A., Wieling, M., & Way, A. (2018). Post-editing E\ufb00ort of a Novel With Statistical and Neural Machine Translation. Frontiers in Digital Humanities, 5, 9. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., . . . Polo- sukhin, I. (2017). Attention is All you Need. In Proceedings of NIPS (pp. 5998\u20136008). Long Beach, CA.",
            ". . Polo- sukhin, I. (2017). Attention is All you Need. In Proceedings of NIPS (pp. 5998\u20136008). Long Beach, CA. Vilar, D., Xu, J., Luis Fernando, D., & Ney, H. (2006). Error analysis of statistical machine translation output. In Proceedings of LREC (pp. 697\u2013702). Voigt, R., & Jurafsky, D. (2012). Towards a Literary Machine Translation: The Role of Referential Cohesion. In Proceedings of the NAACL-HLT 2012 Workshop on Compu- tational Linguistics for Literature (pp. 18\u201325). Montr`eal, Canada. Wang, L., Tu, Z., Way, A., & Liu, Q. (2017). Exploiting Cross-Sentence Context for Neural Machine Translation. In Proceedings of EMNLP (pp. 2826\u20132831). Copenhagen, Denmark.",
            "Wang, L., Tu, Z., Way, A., & Liu, Q. (2017). Exploiting Cross-Sentence Context for Neural Machine Translation. In Proceedings of EMNLP (pp. 2826\u20132831). Copenhagen, Denmark. Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., . . . Dean, J. (2016). Google\u2019s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. arXiv preprint 1609.08144."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-2004.01694.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 12498.000228881836,
    "avg_doclen_est": 162.3116912841797
}
