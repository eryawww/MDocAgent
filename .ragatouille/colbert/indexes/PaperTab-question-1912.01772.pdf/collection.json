[
  "arXiv:1912.01772v2  [cs.CL]  5 Apr 2020 A Resource for Computational Experiments on Mapudungun Mingjun Duan\u20201, Carlos Fasola\u20202, Sai Krishna Rallabandi1, Rodolfo M. Vega1, Antonios Anastasopoulos1, Lori Levin1, Alan W Black1 1Language Technologies Institute 2Interpreting and Translation Studies Carnegie Mellon University Wake Forest University 5000 Forbes Ave, Pittsburgh PA 15213, USA 1834 Wake Forest Road, Winston-Salem, NC 27109, USA {mingjund,srallaba,rmvega}@andrew.cmu.edu, fasolaca@wfu.edu, {aanastas, levin, awb}@cs.cmu.edu Abstract We present a resource for computational experiments on Mapudungun, a polysynthetic indigenous language spoken in Chile with upwards of 200 thousand speakers. We provide 142 hours of culturally signi\ufb01cant conversations in the domain of medical treatment. The conversations are fully transcribed and translated into Spanish. The transcriptions also include annotations for code-switching and non-standard pronunciations.",
  "We provide 142 hours of culturally signi\ufb01cant conversations in the domain of medical treatment. The conversations are fully transcribed and translated into Spanish. The transcriptions also include annotations for code-switching and non-standard pronunciations. We also provide baseline results on three core NLP tasks: speech recognition, speech synthesis, and machine translation between Spanish and Mapudungun. We further explore other applications for which the corpus will be suitable, including the study of code-switching, historical orthography change, linguistic structure, and sociological and anthropological studies. Keywords: Mapudungun, Endangered Languages, Speech, Machine Translation 1. Introduction Recent years have seen unprecedented progress for Natural Language Processing (NLP) on almost every NLP subtask. Even though low-resource settings have also been explored, this progress has overwhelmingly been observed in lan- guages with signi\ufb01cant data resources that can be leveraged to train deep neural networks. Low-resource languages still lag behind. Endangered languages pose an additional challenge.",
  "Even though low-resource settings have also been explored, this progress has overwhelmingly been observed in lan- guages with signi\ufb01cant data resources that can be leveraged to train deep neural networks. Low-resource languages still lag behind. Endangered languages pose an additional challenge. The process of documenting an endangered language typically includes the creation of word lists, audio and video record- ings, notes, or grammar fragments, with the created re- sources then stored in large online linguistics archives. This process is often hindered by the Transcription Bottleneck: the linguistic \ufb01eldworker and the language community may not have time to transcribe all of the recordings and may only transcribe segments that are linguistically salient for publication or culturally signi\ufb01cant for the creation of com- munity resources. With this work we make publicly available a large corpus in Mapudungun, a language of the indigenous Mapuche people of southern Chile and western Argentina. We hope to ameliorate the resource gap and the transcription bottle- neck in two ways.",
  "With this work we make publicly available a large corpus in Mapudungun, a language of the indigenous Mapuche people of southern Chile and western Argentina. We hope to ameliorate the resource gap and the transcription bottle- neck in two ways. First, we are providing a larger data set than has previously been available, and second, we are pro- viding baselines for NLP tasks (speech recognition, speech synthesis, and machine translation). In providing baselines and datasets splits, we hope to further facilitate research on low-resource NLP for this language through our data set. Research on low-resource speech recognition is par- ticularly important in relieving the transcription bottleneck, while tackling the research challenges that speech synthe- sis and machine translation pose for such languages could lead to such systems being deployed to serve more under- represented communities. \u2020Equal contribution 2.",
  "Research on low-resource speech recognition is par- ticularly important in relieving the transcription bottleneck, while tackling the research challenges that speech synthe- sis and machine translation pose for such languages could lead to such systems being deployed to serve more under- represented communities. \u2020Equal contribution 2. The Mapudungun Language Mapudungun (iso 639-3: arn) is an indigenous language of the Americas spoken natively in Chile and Argentina, with an estimated 100 to 200 thousand speakers in Chile and 27 to 60 thousand speakers in Argentina (Z\u00b4u\u02dcniga, 2006, 41\u20133). It is an isolate language and is classi\ufb01ed as threatened by Ethnologue, hence the critical importance of all documen- tary efforts. Although the morphology of nouns is relatively simple, Mapudungun verb morphology is highly agglutina- tive and complex. Some analyses provide as many as 36 verb suf\ufb01x slots (Smeets, 1989). A typical complex verb form occurring in our corpus of spoken Mapudungun con- sists of \ufb01ve or six morphemes.",
  "Some analyses provide as many as 36 verb suf\ufb01x slots (Smeets, 1989). A typical complex verb form occurring in our corpus of spoken Mapudungun con- sists of \ufb01ve or six morphemes. Mapudungun has several interesting grammatical prop- erties. It is a polysynthetic language in the sense of (Baker, 1996); see (Loncon Antileo, 2011) for explicit ar- gumentation. As with other polysynthetic languages, Ma- pudungun has Noun Incorporation; however, it is unique insofar as the Noun appears to the right of the Verb, in- stead of to the left, as in most polysynthetic languages (Baker et al., 2005).",
  "As with other polysynthetic languages, Ma- pudungun has Noun Incorporation; however, it is unique insofar as the Noun appears to the right of the Verb, in- stead of to the left, as in most polysynthetic languages (Baker et al., 2005). One further distinction of Mapudun- gun is that, whereas other polysynthetic languages are char- acterized by a lack of in\ufb01nitives, Mapudungun has in\ufb01niti- val verb forms; that is, while subordinate clauses in Ma- pudungun closely resemble possessed nominals and may occur with an analytic marker resembling possessor agree- ment, there is no agreement in\ufb02ection on the verb itself. One further remarkable property of Mapudungun is its inverse voice system of agreement, whereby the highest agreement is with the argument highest in an animacy hier- archy regardless of thematic role (Arnold, 1996). 3. Related Work Mapudungun grammar has been studied since the arrival of European missionaries and colonizers hundreds of years ago.",
  "3. Related Work Mapudungun grammar has been studied since the arrival of European missionaries and colonizers hundreds of years ago. More recent descriptions of Mapudungun gram- mar (Smeets, 1989) and (Z\u00b4u\u02dcniga, 2006) informed the col-",
  "Train Dev Test All Audio Hours 117 4 21 142 Conversations 285 12 46 343 Turns 78.4k 2.5k 14k 94.9k Mapudungun Transcriptions Sentences 221.8k 8.2k 36.3k 266.3k Tokens (arn) 781k 31.2k 126k 938.3k Types (arn) 92.2k 7.1k 22.3k 121.6k Spanish Translations Tokens (spa) 1,029k 38.9k 164k 1,232k Types (spa) 24.6k 3.7k 8.6k 37k Table 1: Basic Statistics of our corpus. lection of the resource that we are presenting in this paper. Portions of our resource have been used in early ef- forts to build language systems for Mapudungun.",
  "lection of the resource that we are presenting in this paper. Portions of our resource have been used in early ef- forts to build language systems for Mapudungun. In particular, (Monson et al., 2004) focused on Mapudun- gun morphology in order to create spelling correction systems, while (Monson et al., 2006), (Levin et al., 2002), (Font-Llitj`os et al., 2005), and (Monson et al., 2008) devel- oped hybrid rule- and phrase-based Statistical Machine Translation systems. Naturally, similar works in collecting corpora in Indige- nous languages of Latin America are abundant, but very few, if any, have the scale and potential of our resource to be useful in many downstream language-speci\ufb01c and inter-disciplinary applications. A general overview of the state of NLP for the under-represented languages of the Americas can be found at (Mager et al., 2018).",
  "A general overview of the state of NLP for the under-represented languages of the Americas can be found at (Mager et al., 2018). To name a few of the many notable works, (Monta\u02dcno et al., 2019) cre- ated a parallel Mixtec-Spanish corpus for Machine Trans- lation and (Kazeminejad et al., 2017) created lexical re- sources for Arapaho, while (Cardenas et al., 2018) and ( \u00b4Cavar et al., 2016) focused on building speech corpora for Southern Quechua and Chatino respectively. 4. The Resource The resource1 is comprised of 142 hours of spoken Mapudungun that was recorded during the AVENUE project (Levin et al., 2002) in 2001 to 2005.",
  "4. The Resource The resource1 is comprised of 142 hours of spoken Mapudungun that was recorded during the AVENUE project (Levin et al., 2002) in 2001 to 2005. The data was recorded under a partnership between the AVENUE project, funded by the US National Science Foundation at Carnegie Mellon University, the Chilean Ministry of Edu- cation (Mineduc), and the Instituto de Estudios Ind\u00b4\u0131genas at Universidad de La Frontera, originally spanning 170 hours of audio. We have recently cleaned the data and are re- leasing it publicly for the \ufb01rst time (although it has been shared with individual researchers in the past) along with NLP baselines. The recordings were transcribed and translated into Spanish at the Instituto de Estudios Ind\u00b4\u0131genas at Universidad de La Frontera. The corpus covers three dialects of Mapudungun: about 110 hours of Nguluche, 20 hours of Lafkenche and 10 hours of Pewenche. The three dialects are quite sim- 1Project page: http://tts.speech.cs.cmu.edu/mapudungun/.",
  "The three dialects are quite sim- 1Project page: http://tts.speech.cs.cmu.edu/mapudungun/. Data also available at https://github.com/mingjund/mapudungun-corpus ilar, with some minor semantic and phonetic differences. The fourth traditionally distinguished dialect, Huilliche, has several grammatical differences from the other three and is classi\ufb01ed by Ethnologue as a separate language, iso 639-3: huh, and as nearly extinct. The recordings are restricted to a single domain: primary, preventive, and treatment health care, including both West- ern and Mapuche traditional medicine. The recording ses- sions were conducted as interactive conversations so as to be natural in Mapuche culture, and they were open-ended, following an ethnographic approach. The interviewer was trained in these methods along with the use of the digital recording systems that were available at the time. We also followed human subject protocol. Each person signed a consent form to release the recordings for research purposes and the data have been accordingly anonymized.",
  "The interviewer was trained in these methods along with the use of the digital recording systems that were available at the time. We also followed human subject protocol. Each person signed a consent form to release the recordings for research purposes and the data have been accordingly anonymized. Because Machi (traditional Mapuche healers) were interviewed, we asked the transcribers to delete any culturally proprietary knowledge that a Machi may have revealed during the con- versation. Similarly, we deleted any names or any informa- tion that may identify the participants. The corpus is culturally relevant because it was created by Mapuche people, using traditional ways of relating to each other in conversations. They discussed personal ex- periences with primary health care in the traditional Ma- puche system and the Chilean health care system, talking about illnesses and the way they were cured. The partici- pants ranged from 16 years old to 100 years old, almost in equal numbers of men and women, and they were all native speakers of Mapudungun. Orthography At the time of the collection and transcrip- tion of the corpus, the orthography of Mapudungun was not standardized.",
  "Orthography At the time of the collection and transcrip- tion of the corpus, the orthography of Mapudungun was not standardized. The Mapuche team at the Instituto de Estu- dios Ind\u00b4\u0131genas (IEI \u2013 Institute for Indigenous Studies) de- veloped a supra-dialectal alphabet that comprises 28 letters that cover 32 phones used in the three Mapudungun vari- ants. The main criterion for choosing alphabetic charac- ters was to use the current Spanish keyboard that was avail- able on all computers in Chilean of\ufb01ces and schools. The alphabet used the same letters used in Spanish for those phonemes that sound like Spanish phonemes. Diacritics such as apostrophes were used for sounds that are not found in Spanish. As a result, certain orthographic conventions that were made at the time deviate from the now-standard orthogra- phy of Mapudungun, Azumchefe. We plan to normalize the orthography of the corpus, and in fact a small sample has already been converted to the modern orthography.",
  "As a result, certain orthographic conventions that were made at the time deviate from the now-standard orthogra- phy of Mapudungun, Azumchefe. We plan to normalize the orthography of the corpus, and in fact a small sample has already been converted to the modern orthography. How- ever, we believe that the original transcriptions will also be invaluable for academic, historical, and cultural purposes, hence we release the corpus using these conventions. Additional Annotations In addition, the transcription in- cludes annotations for noises and dis\ufb02uencies including aborted words, mispronunciations, poor intelligibility, re- peated and corrected words, false starts, hesitations, unde- \ufb01ned sound or pronunciations, non-verbalarticulations, and pauses. Foreign words, in this case Spanish words, are also labelled as such.",
  "Conversation: nfmcp-nmedp2 Speaker (Start \u2013 End) nfmcp: 9min 27.069sec \u2013 10min 6.102sec Transcription: kuydaw\u00a8unmu fey tuntenmurume fey m\u00a8ule rume kutran fey kuydawenulmu weche, wechen- gele che, wechengele ki\u02dcne domo ki\u02dcne wentru ka famechi tui m\u00a8uten kutran fey kuydawu- nunmu m\u00a8uten [SPA]pero kuydawetulele fey newe kontulay kutran [SPA]pues [!1pu\u2019] welu, kuydanunu m\u00a8utewe rangi\u02dcn wentru,",
  "kuydanunu m\u00a8utewe rangi\u02dcn wentru, rangin domolele deuma ki\u02dcne che fey rume kuntueyu ta chi kutran kuydaw\u00a8unmu kuydaw\u00a8unmu fey newe kontulayu kutran Translation: Por no cuidarse hay en cualquier tiempo aparece de repente la enfermedad eso es porque la gente j\u00b4oven no se cuida si es joven la persona si es joven una mujer un hombre as\u00b4\u0131 tambi\u00b4en toma no m\u00b4as la enfermedad eso es por no cuidarse no m\u00b4as pero si se vuelve a cuidar ah\u00b4\u0131 no le entra mucho la enfermedad pero si no se cuida mucho cuando la persona este en la edad mediana hombre o mujer la da mucho esta enfermedad por no cuidarse, por no cuidarse ah\u00b4\u0131 no le entra mucho esa enfermedad. Table 2: Example of an utterance along with the different annotations.",
  "por no cuidarse ah\u00b4\u0131 no le entra mucho esa enfermedad. Table 2: Example of an utterance along with the different annotations. We additionally highlight the code-switching annotations ([SPA] indicates Spanish words) as well as pre-normalized transcriptions that indicating non-standard pro- nunciations ([!1pu\u2019] indicates that the previous 1 word was pronounced as \u2018pu\u2019\u2019 instead of \u2018pues\u2019). Cleaning The dialogues were originally recorded using a Sony DAT recorder (48kHz), model TCD-D8, and Sony digital stereo microphone, model ECM-DS70P. Transcrip- tion was performed with the TransEdit transcription tool v.1.1 beta 102, which synchronizes the transcribed text and the wave \ufb01les. However, we found that a non-trivial number of the utter- ance boundaries and speaker annotations were \ufb02awed. Also some recording sessions did not have a complete set of matching audio, transcription, and translation \ufb01les.",
  "However, we found that a non-trivial number of the utter- ance boundaries and speaker annotations were \ufb02awed. Also some recording sessions did not have a complete set of matching audio, transcription, and translation \ufb01les. Hence, in an effort to provide a relatively \u201cclean\u201d corpus for mod- ern computational experiments, we converted the encoding of the textual transcription from Latin-1 to Unicode, DOS to UNIX line endings, a now more standard text encoding format than what was used when the data was \ufb01rst col- lected. Additionally, we renamed a small portion of \ufb01les which had been misnamed and removed several duplicate \ufb01les. Although all of the data was recorded with similar equip- ment in relatively quiet environments, the acoustics are not as uniform as we would like for building speech synthe- sizers. Thus we applied standardized power normalization. We also moved the boundaries of the turns to standardize the amount of leading and trailing silence in each turn. This is a standard procedurefor speech recognition and synthesis datasets.",
  "Thus we applied standardized power normalization. We also moved the boundaries of the turns to standardize the amount of leading and trailing silence in each turn. This is a standard procedurefor speech recognition and synthesis datasets. Finally we used the techniques in (Black, 2019) for found data to re-align the text to the audio and \ufb01nd out which turns are best (or worst) aligned so that we can select segments that give the most accurate alignments. Some of the misalignments may in part be due to varied orthogra- phy, and we intend, but have not yet, to investigate normal- ization of orthography (i.e. spelling correction) to mitigate this. Training, Dev, and Test Splits We created two train- ing sets, one appropriate for single-speaker speech synthe- sis experiments, and one appropriate for multiple-speaker speech recognition and machine translation experiments. In both cases, our training, development, and test splits are 2developed by Susanne Burger and Uwe Meier performed at the dialogue level, so that all examples from each dialogue belong to exactly one of these sets.",
  "In both cases, our training, development, and test splits are 2developed by Susanne Burger and Uwe Meier performed at the dialogue level, so that all examples from each dialogue belong to exactly one of these sets. For single-speaker speech synthesis, we only used the di- alog turns of the speaker with the largest volume of data (nmlch \u2013 one of the interviewers). The training set includes 221.8 thousand sentences from 285 dialogues, with 12 and 46 conversations reserved for the development and test set. For speech recognition experiments, we ensured that our test set includes unique speakers as well as speakers that overlap with the training set, in order to allow for compar- isons of the ability of the speech recognition system to gen- eralize over seen and new speakers. For consistency, we used the same dataset splits for the machine translation ex- periments. The statistics in Table 1 re\ufb02ect this split. 5. Applications Our resource has the potential to be the basis of computa- tional research in Mapudungun across several areas.",
  "For consistency, we used the same dataset splits for the machine translation ex- periments. The statistics in Table 1 re\ufb02ect this split. 5. Applications Our resource has the potential to be the basis of computa- tional research in Mapudungun across several areas. Since the collected audio has been transcribed, our resource is appropriate for the study of automatic speech recognition and speech synthesis. The Spanish translations enable the creation of machine translation systems between Mapudun- gun and Spanish, as well as end-to-end (or direct) speech translation. We in fact built such speech synthesis, speech recognition, and machine translation systems as a showcase of the usefulness of our corpus in that research direction. Furthermore, our annotations of the Spanish words inter- spersed in Mapudungun speech could allow for a study of code-switching patterns within the Mapuche community. 7.3% of the transcription tokens are in Spanish, and the remaining are in Mapudungun.",
  "Furthermore, our annotations of the Spanish words inter- spersed in Mapudungun speech could allow for a study of code-switching patterns within the Mapuche community. 7.3% of the transcription tokens are in Spanish, and the remaining are in Mapudungun. In addition, our annota- tions of non-standardized orthographic transcriptions could be extremely useful in the study of historical language and orthography change as a language moves from predomi- nantly oral to being written in a standardized orthography, as well as in building spelling normalization and correc- tion systems. The relatively large amount of data that we collected will also allow for the training of large language",
  "models, which in turn could be used as the basis for predic- tive keyboards tailored to Mapudungun. Last, since all data are dialogues annotated for the different speaker turns, they could be useful for building Mapudungun dialogue systems and chatbot-like applications. The potential applications of our resource, however, are not exhausted in language technologies. The resource as a whole could be invaluable for ethnographic and sociolog- ical research, as the conversations contrast traditional and Western medicine practices, and they could reveal interest- ing aspects of the Mapuche culture. In addition, the corpus is a goldmine of data for studying the morphostyntax of Mapudungun (Fasola, 2015). As an isolate polysynthetic language, the study of Mapudungun can provide insights into the range of possibilities within human languages can work. 6. Baseline Results Using the aforementioned higher quality portions of the corpus, we trained baseline systems for Mapudungun speech recognition and speech synthesis, as well as Ma- chine Translation systems between Mapudungun and Span- ish.",
  "6. Baseline Results Using the aforementioned higher quality portions of the corpus, we trained baseline systems for Mapudungun speech recognition and speech synthesis, as well as Ma- chine Translation systems between Mapudungun and Span- ish. Speech Synthesis In our previous work on build- ing speech systems on found data in 700 languages, (Black, 2019), we addressed alignment issues (when audio is not segmented into turn/sentence sized chunks) and cor- rectness issues (when the audio does not match the tran- scription). We used the same techniques here, as described above. For the best quality speech synthesis we need a few hours of phonetically-balanced, single-speaker, read speech. Our \ufb01rst step was to use the start and end points for each turn in the dialogues, and select those of the most frequent speaker, nmlch. This gave us around 18250 segments. We fur- ther automatically removed excessive silence from the start, middle and end of these turns (based on occurrence of F0). This gave us 13 hours and 48 minutes of speech.",
  "This gave us around 18250 segments. We fur- ther automatically removed excessive silence from the start, middle and end of these turns (based on occurrence of F0). This gave us 13 hours and 48 minutes of speech. We phonetically aligned this data and built a speech clus- tergen statistical speech synthesizer (Black, 2006) from all of this data. We resynthesized all of the data and mea- sured the difference between the synthesized data and the original data using Mel Cepstral Distortion, a standard method for automatically measuring quality of speech gen- eration (Kominek et al., 2008). We then ordered the seg- ments by their generation score and took the top 2000 turns to build a new synthesizer, assuming the better scores cor- responded to better alignments, following the techniques of (Black, 2019). The initial build gave an MCD on held out data of 6.483. While the 2000 best segment dataset gives an MCD of 5.551, which is a large improvement.",
  "The initial build gave an MCD on held out data of 6.483. While the 2000 best segment dataset gives an MCD of 5.551, which is a large improvement. The quality of the generated speech goes from understandable, only if you can see the text, to understandable, and transcribable even for non-Mapudungun speakers. We do not believe we are building the best synthesizer with our current (non-neural) techniques, but we do believe we are selecting the best training data for other statistical and neural training techniques in both speech synthesis and speech recognition.",
  "We do not believe we are building the best synthesizer with our current (non-neural) techniques, but we do believe we are selecting the best training data for other statistical and neural training techniques in both speech synthesis and speech recognition. Train Data arn\u2192spa spa\u2192arn (#sentences) BLEU chrF BLEU chrF Results on dev set 220k (all) 20.98 0.5 14.12 0.4 100k 16.89 0.4 10.93 0.3 50k 13.70 0.4 2.05 0.1 10k 6.26 0.3 1.09 0.1 Results on test set 220k (all) 20.4 0.5 12.9 0.4 100k 16.9 0.4 6.4 0.2 50k 13.3 0.4 1.1 0.1 10k 7.8 0.3 0.7 0.1 Table 3: Machine Translation Results Speech Recognition For speech recognition (ASR) we used Kaldi (Povey et al., 2011).",
  "As we do not have ac- cess to pronunciation lexica for Mapudungun, we had to approximate them with two settings. In the \ufb01rst setting, we made the simple assumption that each character corre- sponds to a pronounced phoneme. In the second setting, we instead used the generated phonetic lexicon also used in the above-mentioned speech synthesis techniques. We had a vocabulary of size 10725 and the test set had 7 out of vocabulary (OOV) words. We approximated the pronun- ciations for these words using the tools within Kaldi. The train/dev/test splits are across conversations, as described above. Under the \ufb01rst setting, we obtained character error rate of 60%, and employing the phonemes from generated lexicon, we achieved 30% phone error rate. For both these systems, we used the nnet training recipe from Kaldi. Naturally, these results are relatively far from the quality of ASR sys- tems trained on large amounts of clean data such as those available in English.",
  "For both these systems, we used the nnet training recipe from Kaldi. Naturally, these results are relatively far from the quality of ASR sys- tems trained on large amounts of clean data such as those available in English. Given the quality of the recordings, and the lack of additional resources, we consider our re- sults fairly reasonable and they would still be usable for simple dialog-like tasks. We anticipate, though, that one could signi\ufb01cantly improve ASR quality over our dataset, by using in-domain language models, or by training end- to-end neural recognizers leveraging languages with sim- ilar phonetic inventories (Adams et al., 2019) or by using the available Spanish translations in a multi-source sce- nario (Anastasopoulos and Chiang, 2018). Mapudungun\u2013Spanish Machine Translation We built neural end-to-end machine translation systems between Mapudungun and Spanish in both directions, using state- of-the-art Transformer architecture (Vaswani et al., 2017) with the toolkit of (Nguyen and Salazar, 2019).",
  "Mapudungun\u2013Spanish Machine Translation We built neural end-to-end machine translation systems between Mapudungun and Spanish in both directions, using state- of-the-art Transformer architecture (Vaswani et al., 2017) with the toolkit of (Nguyen and Salazar, 2019). We train our systems at the subword level using Byte-Pair Encod- ing (Sennrich et al., 2016) with a vocabulary of 5000 sub- words, shared between the source and target languages. We use \ufb01ve layers for each of the encoder and the decoder, an embedding size of 512, feed forward transformation size of 2048, and eight attention heads. We use dropout (Srivastava et al., 2014) with 0.4 probability as well as la- bel smoothing set to 0.1. We train with the Adam optimizer (Kingma and Ba, 2014) for up to 200 epochs using learning",
  "decay with a patience of six epochs. The baseline results using different portions of the training set (10k, 50k, 100k, and all (220k) parallel sentences) on both translation directions are presented in Table 3, using detokenized BLEU (Papineni et al., 2002) (a standard MT metric) and chrF (Popovi\u00b4c, 2015) (a metric that we con- sider to be more appropriate for polysynthetic languages, as it does not rely on word n-grams) computed with the sacreBLEU toolkit (Post, 2018). It it worth noting the dif- ference in quality between the two directions, with transla- tion into Spanish reaching 20.4 (almost 21) BLEU points in the development set, while the opposite direction (translat- ing into Mapudungun) shows about a 7 BLEU points worse performance. This is most likely due to Mapudungun being a polysynthetic language, with its complicated morphology posing a challenge for proper generation. 7.",
  "This is most likely due to Mapudungun being a polysynthetic language, with its complicated morphology posing a challenge for proper generation. 7. Conclusion With this work we present a resource that will be extremely useful for building language systems in an endangered, under-represented language, Mapudungun. We benchmark NLP systems for speech synthesis, speech recognition, and machine translation, providing strong baseline results. The size of our resource (142 hours, more than 260k total sen- tences) has the potential to alleviate many of the issues faced when building language technologies for Mapudun- gun, in contrast to other indigenous languages of the Amer- icas that unfortunately remain low-resource. Our resource could also be used for ethnographic and an- thropological research into the Mapuche culture, and has the potential to contribute to intercultural bilingual educa- tion, preservation activities and further general advance- ment of the Mapudungun-speaking community. 8.",
  "Our resource could also be used for ethnographic and an- thropological research into the Mapuche culture, and has the potential to contribute to intercultural bilingual educa- tion, preservation activities and further general advance- ment of the Mapudungun-speaking community. 8. Acknowledgements The data collection described in this paper was supported by NSF grants IIS-0121631 (AVENUE) and IIS-0534217 (LETRAS), with supplemental funding from NSF\u2019s Of\ufb01ce of International Science and Education. Preliminary fund- ing for work on Mapudungun was also provided by DARPA The experimental material is based upon work generously supported by the National Science Foundation under grant 1761548. 9. Bibliographical References Adams, O., Wiesner, M., Watanabe, S., and Yarowsky, D. (2019). Massively multilingual adversarial speech recognition. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 96\u2013108.",
  "(2019). Massively multilingual adversarial speech recognition. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 96\u2013108. Anastasopoulos, A. and Chiang, D. (2018). Leveraging translations for speech transcription in low-resource set- tings. In Proc. INTERSPEECH. Arnold, J. (1996). The inverse system in Mapudungun and other languages. RLA: Revista de Ling\u00a8u\u00b4\u0131stica te\u00b4orica y aplicada, 34:9\u201347. Baker, M. C., Aranovich, R., and Golluscio, L. A. (2005). Two types of syntactic noun incorporation: Noun in- corporation in Mapudungun and its typological implica- tions. Language, 81(1):138\u2013176. Baker, M. C. (1996). The Polysynthesis Parameter.",
  "Two types of syntactic noun incorporation: Noun in- corporation in Mapudungun and its typological implica- tions. Language, 81(1):138\u2013176. Baker, M. C. (1996). The Polysynthesis Parameter. Ox- ford University Press, Oxford. Black, A. (2006). CLUSTERGEN: A statistical paramet- ric synthesizer using trajectory modeling. In Interspeech 2006, Pittsburgh, PA. Black, A. W. (2019). Cmu wilderness multilingual speech dataset. In ICASSP 2019, pages 5971\u20135975. IEEE. Cardenas, R., Zevallos, R., Baquerizo, R., and Camacho, L. (2018). Siminchik: A speech corpus for preservation of southern quechua. In Proceedings of the Eleventh Inter- national Conference on Language Resources and Evalu- ation (LREC\u201918). \u00b4Cavar, M., \u00b4Cavar, D., and Cruz, H. (2016).",
  "In Proceedings of the Eleventh Inter- national Conference on Language Resources and Evalu- ation (LREC\u201918). \u00b4Cavar, M., \u00b4Cavar, D., and Cruz, H. (2016). Endan- gered language documentation: Bootstrapping a chatino speech corpus, forced aligner, asr. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC\u201916), pages 4004\u20134011. Fasola, C. A. (2015). Topics in the syntax of Mapudungun subordinate clauses. Ph.D. thesis, Rutgers University- Graduate School-New Brunswick. Font-Llitj`os, A., Levin, L., and Aranovich, R. (2005). Building machine translation systems for indigenous lan- guages of latin america. In Second Conference on the Indigenous Languages of Latin America (CILLA II). Kazeminejad, G., Cowell, A., and Hulden, M. (2017). Cre- ating lexical resources for polysynthetic languages\u2014the case of Arapaho.",
  "In Second Conference on the Indigenous Languages of Latin America (CILLA II). Kazeminejad, G., Cowell, A., and Hulden, M. (2017). Cre- ating lexical resources for polysynthetic languages\u2014the case of Arapaho. In Proceedings of the 2nd Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 10\u201318, Honolulu, March. Association for Computational Linguistics. Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv:1412.6980. Kominek, J., Schultz, T., and Black, A. (2008). Synthe- sizer voice quality on new languages calibrated with mel- cepstral distorion. In SLTU 2008, Hanoi, Viet Nam. Levin, L., Lavie, A., Vega, R., Carbonell, J., Brown, R., Ca\u02dcnulef, E., and Huenchullan, C. (2002). Data collec- tion and language technologies for mapudungun.",
  "Levin, L., Lavie, A., Vega, R., Carbonell, J., Brown, R., Ca\u02dcnulef, E., and Huenchullan, C. (2002). Data collec- tion and language technologies for mapudungun. In In- ternational Workshop on Resources and Tools in Field Linguistics, LREC (Language Resources and Evaluation Conference), Las Palmas, Spain. Loncon Antileo, E. (2011). Morfolog\u00b4\u0131a y Aspectos del Mapudungun. Universidad Aut\u00b4onoma Metropolitana, M\u00b4exico, D.F. Mager, M., Gutierrez-Vasques, X., Sierra, G., and Meza- Ruiz, I. (2018). Challenges of language technologies for the indigenous languages of the americas. In Proceed- ings of the 27th International Conference on Computa- tional Linguistics, pages 55\u201369.",
  "(2018). Challenges of language technologies for the indigenous languages of the americas. In Proceed- ings of the 27th International Conference on Computa- tional Linguistics, pages 55\u201369. Monson, C., Levin, L., Vega, R., Brown, R., Llitjos, A. F., Lavie, A., Carbonell, J., Ca\u02dcnulef, E., and Huisca, R. (2004). Data collection and analysis of mapudun- gun morphology for spelling correction. In Proceed- ings of the Fourth International Conference on Language Resources and Evaluation (LREC\u201904), Lisbon, Portu- gal, May. European Language Resources Association (ELRA).",
  "Monson, C., Font-Llitj`os, A., Aranovich, R., Levin, L., Brown, R., Peterson, E., Carbonell, J., and Lavie, A. (2006). Building nlp systems for two resource-scarce in- digenous languages: Mapudungun and quechua. In 5th SALTMIL Workshop on Minority Languages: Strategies for Developing Machine Translation for Minority Lan- guages, Proceedings of the Fifth International Confer- ence on Language Resources and Evaluation (LREC). Monson, C., Llitj\u00b4os, A. F., Ambati, V., Levin, L., Lavie, A., Alvarez, A., Aranovich, R., Carbonell, J., Frederk- ing, R., Peterson, E., and Probst, K. (2008). Linguistic structure and bilingual informants help induce machine translation of lesser-resourced languages. In Proceed- ings of the Sixth International Conference on Language Resources and Evaluation (LREC\u201908), Marrakech, Mo- rocco, May. European Language Resources Association (ELRA).",
  "Linguistic structure and bilingual informants help induce machine translation of lesser-resourced languages. In Proceed- ings of the Sixth International Conference on Language Resources and Evaluation (LREC\u201908), Marrakech, Mo- rocco, May. European Language Resources Association (ELRA). Monta\u02dcno, C., Sierra Mart\u00b4\u0131nez, G., Bel-Enguix, G., and Gomez, H. (2019). A parallel corpus mixtec-Spanish. In Proceedings of the 2019 Workshop on Widening NLP, pages 157\u2013159, Florence, Italy. Association for Compu- tational Linguistics. Nguyen, T. Q. and Salazar, J. (2019). Transformers with- out tears: Improving the normalization of self-attention. arXiv:1910.05895. Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002). Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013 318.",
  "(2002). Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013 318. Association for Computational Linguistics. Popovi\u00b4c, M. (2015). chrf: character n-gram f-score for au- tomatic mt evaluation. In Proceedings of the Tenth Work- shop on Statistical Machine Translation, pages 392\u2013395. Post, M. (2018). A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Ma- chine Translation: Research Papers, pages 186\u2013191, Belgium, Brussels, October. Association for Computa- tional Linguistics. Povey, D., Ghoshal, A., Boulianne, G., Burget, L., Glem- bek, O., Goel, N., Hannemann, M., Motlicek, P., Qian, Y., Schwarz, P., Silovsky, J., Stemmer, G., and Vesely, K. (2011). The kaldi speech recognition toolkit.",
  "(2011). The kaldi speech recognition toolkit. In IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Society, De- cember. IEEE Catalog No.: CFP11SRW-USB. Sennrich, R., Haddow, B., and Birch, A. (2016). Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Asso- ciation for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725. Smeets, C. J. (1989). A Mapuche grammar. Ph.D. thesis, Rijksuniversiteit Leiden. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from over\ufb01tting. The journal of machine learning research, 15(1):1929\u20131958.",
  "(2014). Dropout: a simple way to prevent neural networks from over\ufb01tting. The journal of machine learning research, 15(1):1929\u20131958. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. (2017). Attention is all you need. In Advances in neural infor- mation processing systems, pages 5998\u20136008. Z\u00b4u\u02dcniga, F. (2006). Mapudungun: El Habla Mapuche. Centro de Estudios P\u00b4ublicos, Santiago, Chile."
]