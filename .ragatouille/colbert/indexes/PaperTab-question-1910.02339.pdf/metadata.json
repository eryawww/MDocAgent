{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Mapping Natural-language Problems to Formal-language Solutions Using Structured Neural Representations Kezhen Chen 1 2 Qiuyuan Huang 1 Hamid Palangi 1 Paul Smolensky 1 3 Kenneth D. Forbus 2 Jianfeng Gao 1 Abstract Generating formal-language programs repre- sented by relational tuples, such as Lisp pro- grams or mathematical operations, to solve prob- lems stated in natural language is a challeng- ing task because it requires explicitly capturing discrete symbolic structural information implicit in the input. However, most general neural se- quence models do not explicitly capture such structural information, limiting their performance on these tasks. In this paper, we propose a new encoder-decoder model based on a structured neu- ral representation, Tensor Product Representa- tions (TPRs), for mapping Natural-language prob- lems to Formal-language solutions, called TP- N2F.",
      "In this paper, we propose a new encoder-decoder model based on a structured neu- ral representation, Tensor Product Representa- tions (TPRs), for mapping Natural-language prob- lems to Formal-language solutions, called TP- N2F. The encoder of TP-N2F employs TPR \u2018bind- ing\u2019 to encode natural-language symbolic struc- ture in vector space and the decoder uses TPR \u2018unbinding\u2019 to generate, in symbolic space, a se- quential program represented by relational tuples, each consisting of a relation (or operation) and a number of arguments. TP-N2F considerably outperforms LSTM-based seq2seq models on two benchmarks and creates new state-of-the-art re- sults. Ablation studies show that improvements can be attributed to the use of structured TPRs explicitly in both the encoder and decoder. Anal- ysis of the learned structures shows how TPRs enhance the interpretability of TP-N2F. 1. Introduction When people perform explicit reasoning, they can typically describe the way to the conclusion step by step via relational 1Microsoft Research, Redmond, USA.",
      "Anal- ysis of the learned structures shows how TPRs enhance the interpretability of TP-N2F. 1. Introduction When people perform explicit reasoning, they can typically describe the way to the conclusion step by step via relational 1Microsoft Research, Redmond, USA. 2Department of Computer Science, Northwestern University, Evanston, USA. 3Department of Cognitive Science, Johns Hopkins Univer- sity, Baltimore, USA.. Correspondence to: Kezhen Chen <kzchen@u.northwestern.edu>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). descriptions. There is ample evidence that relational, struc- tured representations are important for human cognition, e.g., (Goldin-Meadow & Gentner, 2003; Forbus et al., 2017; Crouse et al., 2018; Chen & Forbus, 2018; Chen et al., 2019; Lee et al., 2019).",
      "Although a rapidly growing number of researchers use deep learning to solve complex symbolic reasoning and language tasks (a recent review is Gao et al. (2019)), most existing deep learning models, including se- quence models such as LSTMs, do not explicitly capture human-like relational structured information. In this paper we propose a novel neural architecture, TP- N2F, for mapping a Natural-language (NL) question to a Formal-language (FL) program represented by a sequence of relational tuples (N2F). In the tasks we study, math or programming problems are stated in natural language, and answers are given as programs: sequences of relational structured representations, to solve the problems step by step like a human being, instead of directly generating the \ufb01- nal answer. For example, from one of our datasets, MathQA: given a natural-language math problem \u201c20 is subtracted from 60 percent of a number, the result is 88.",
      "For example, from one of our datasets, MathQA: given a natural-language math problem \u201c20 is subtracted from 60 percent of a number, the result is 88. Find the num- ber?\u201d, the formal-language solution program is \u201c(add,n0,n2) (divide,n1,const100) (divide,#0,#1)\u201d, where n1 indicates the \ufb01rst number mentioned in the question and #i indicates the output of the ith previous tuple. TP-N2F encodes the natural-language symbolic structure of the problem in an input vector space, maps this to a vector in an intermediate space, and uses that vector to produce a sequence of output vectors that are decoded as relational structures. Both input and output structures are modeled as Tensor Product Rep- resentations (TPRs) (Smolensky, 1990) and the structured representations of inputs are mapped to the structured repre- sentations of outputs. During encoding, NL-input symbolic structures are encoded as vector space embeddings using TPR \u2018binding\u2019 (following Palangi et al.",
      "During encoding, NL-input symbolic structures are encoded as vector space embeddings using TPR \u2018binding\u2019 (following Palangi et al. (2018)); during de- coding, symbolic constituents are extracted from structure- embedding output vectors using TPR \u2018unbinding\u2019 (following Huang et al. (2018; 2019)). By employing TPRs, the model achieves better performance and increased interpretability. Our contributions in this work are as follows. (i) We intro- duce the notion of abstract role-level analysis, and propose such an analysis of N2F tasks. (ii) We present a new TP-N2F arXiv:1910.02339v3  [cs.CL]  2 Aug 2020",
      "Mapping Natural-language Problems to Formal-language Solutions Using Structured Neural Representations model which gives a neural-network-level implementation of a model solving the N2F task under the role-level de- scription proposed in (i). To our knowledge, this is the \ufb01rst model to be proposed which combines both the binding and unbinding operations of TPRs to solve generation tasks through deep learning. (iii) State-of-the-art performance on two recently developed N2F tasks shows that the TP- N2F model has signi\ufb01cant structure learning ability on tasks requiring symbolic reasoning through program synthesis. 2. Related Work N2F tasks include many different subtasks such as symbolic reasoning or semantic parsing (Kamath & Das, 2019; Cai & Lam, 2019; Liao et al., 2018; Amini et al., 2019; Polosukhin & Skidanov, 2018; Bednarek et al., 2019). These tasks re- quire models with strong structure-learning ability. TPR is a promising technique for encoding symbolic structural information and modeling symbolic reasoning in vector space.",
      "These tasks re- quire models with strong structure-learning ability. TPR is a promising technique for encoding symbolic structural information and modeling symbolic reasoning in vector space. TPR binding has been used for encoding and explor- ing grammatical structural information of natural language (Palangi et al., 2018; Huang et al., 2019). TPR unbinding has also been used to generate natural language captions from images (Huang et al., 2018). Some researchers use TPRs for modeling deductive reasoning processes both on a rule-based model and deep learning models in vector space (Lee et al., 2016; Smolensky et al., 2016; Schlag & Schmid- huber, 2018). However, none of these previous models takes advantage of combining TPR binding and TPR unbinding to learn structure representation mappings explicitly, as done in our model. Although researchers are paying increasing attention to N2F tasks, most of the proposed models either do not encode structural information explicitly or are spe- cialized to particular tasks. Our proposed TP-N2F neural model is general and can be applied to many tasks.",
      "Although researchers are paying increasing attention to N2F tasks, most of the proposed models either do not encode structural information explicitly or are spe- cialized to particular tasks. Our proposed TP-N2F neural model is general and can be applied to many tasks. TP-N2F represents inputs and outputs as structures and learns to map these structures. In cognitive science and psy- chology, mapping one domain to another is also an impor- tant \ufb01eld. For example, Goldin-Meadow & Gentner (2003) proposed the Structure Mapping Theory to model human analogy within cognitive science, and Forbus et al. (2017) introduced the computational implementation, the Structure Mapping Engine (SME), of the Structure Mapping Theory. Following these works, Crouse et al. (2018); Chen & Forbus (2018); Chen et al. (2019) applied SME on language and vision problems. Researchers also explore the use of con- cept theory to map structural representations from different domains (Roads & Love, 2019; Martin, 2020).",
      "(2019) applied SME on language and vision problems. Researchers also explore the use of con- cept theory to map structural representations from different domains (Roads & Love, 2019; Martin, 2020). In this paper, we propose the structure-to-structure scheme to build neural models: the TP-N2F model follows this scheme. 3. Structured Representations using TPRs The Tensor Product Representation (TPR) mechanism is a method to create a vector space embedding of complex symbolic structures. The type of a symbol structure is de- \ufb01ned by a set of structural positions or roles, such as the left-child-of-root position in a tree, or the second-argument- of-R position of a given relation R. In a particular instance of a structural type, each of these roles may be occupied by a particular \ufb01ller, which can be an atomic symbol or a substructure (e.g., the entire left sub-tree of a binary tree can serve as the \ufb01ller of the role left-child-of-root).",
      "For now, we assume the \ufb01llers to be atomic symbols.1 The TPR embedding of a symbol structure is the sum of the embeddings of all its constituents, each constituent com- prising a role together with its \ufb01ller. The embedding of a constituent is constructed from the embedding of a role and the embedding of the \ufb01ller of that role: these are joined together by the TPR \u2018binding\u2019 operation, the tensor (or gen- eralized outer) product \u2297. Formally, suppose a symbolic type is de\ufb01ned by the roles {ri}, and suppose that in a particular instance of that type, S, role ri is bound by \ufb01ller fi. The TPR embedding of S is the order-2 tensor T = X i fi \u2297ri = X i fir\u22a4 i (1) where {fi} are vector embeddings of the \ufb01llers and {ri} are vector embeddings of the roles. In Eq. 1, and below, for notational simplicity we con\ufb02ate order-2 tensors and matrices.",
      "In Eq. 1, and below, for notational simplicity we con\ufb02ate order-2 tensors and matrices. A TPR scheme for embedding a set of symbol structures is de\ufb01ned by a decomposition of those structures into roles bound to \ufb01llers, an embedding of each role as a role vector, and an embedding of each \ufb01ller as a \ufb01ller vector. Let the total number of roles and \ufb01llers available be nR, nF, respectively. De\ufb01ne the matrix of all possible role vectors to be R \u2208RdR\u00d7nR, with column i, [R]:i = ri \u2208RdR, comprising the embedding of ri. Similarly let F \u2208RdF\u00d7nF be the matrix of all possible \ufb01ller vectors. The TPR T \u2208 RdF\u00d7dR. Below, dR, nR, dF, nF will be hyper-parameters, while R, F will be learned parameter matrices. Using summation in Eq.1 to combine the vectors embedding the constituents of a structure risks non-recoverability of those constituents given the embedding T of the structure as a whole.",
      "Using summation in Eq.1 to combine the vectors embedding the constituents of a structure risks non-recoverability of those constituents given the embedding T of the structure as a whole. The tensor product is chosen as the binding operation in order to enable recovery of the \ufb01ller of any role in a structure S given its TPR T. This can be done with 1When \ufb01llers are structures themselves, binding can be used recursively, giving tensors of order higher than 2. In general, binding is done with the tensor product, since con\ufb02ation with matrix algebra is only possible for order-2 tensors. Our unbinding of relational tuples involves the order-3 TPRs de\ufb01ned in Sec. 4.1.",
      "Mapping Natural-language Problems to Formal-language Solutions Using Structured Neural Representations perfect precision if the embeddings of the roles are linearly independent. In that case the role matrix R has a left inverse U: UR = I. Now de\ufb01ne the unbinding (or dual) vector for role rj, uj, to be the jth column of U \u22a4: U \u22a4 :j . Then, since [I]ji = [UR]ji = Uj:R:i = [U \u22a4 :j ]\u22a4R:i = u\u22a4 j ri = r\u22a4 i uj, we have r\u22a4 i uj = \u03b4ji.",
      "Then, since [I]ji = [UR]ji = Uj:R:i = [U \u22a4 :j ]\u22a4R:i = u\u22a4 j ri = r\u22a4 i uj, we have r\u22a4 i uj = \u03b4ji. This means that, to recover the \ufb01ller of rj in the structure with TPR T, we can take its tensor inner product (or matrix-vector product) with uj:2 Tuj = \"X i fir\u22a4 i # uj = X i fi\u03b4ij = fj (2) In the architecture proposed here, we make use of TPR \u2018binding\u2019 for the structured embedding encoding the natural- language problem statement; we use TPR \u2018unbinding\u2019 of the structured output embedding to decode the formal-language solution programs represented by relational tuples. Because natural-language and formal-language pertain to different representations (natural-language is an order-2 tensor and formal-language is an order-3 tensor), the NL-binding and FL-unbinding vectors are not related to one another.",
      "Because natural-language and formal-language pertain to different representations (natural-language is an order-2 tensor and formal-language is an order-3 tensor), the NL-binding and FL-unbinding vectors are not related to one another. The structured neural Tensor Product Representations of natural- language and formal-language, and the details of binding and unbinding process used in the architecture, will be in- troduced in 4.1. 4. TP-N2F Model We propose a general TP-N2F neural network architecture operating over TPRs to solve N2F tasks under a proposed role-level description of those tasks. In this description, natural-language input is represented as a straightforward order-2 tensor role structure, and formal-language relational representations of outputs are represented with a new order- 3 tensor recursive role structure proposed here. Figure 1 shows an overview diagram of the TP-N2F model. It depicts the following high-level description. As shown in Figure 1, while the natural-language input is a sequence of words, the output is a sequence of multi- argument relational tuples such as (R A1 A2), a 3-tuple consisting of a binary relation (or operation) R with its two arguments.",
      "As shown in Figure 1, while the natural-language input is a sequence of words, the output is a sequence of multi- argument relational tuples such as (R A1 A2), a 3-tuple consisting of a binary relation (or operation) R with its two arguments. The \u201cTP-N2F encoder\u201d uses two LSTMs to produce a pair consisting of a \ufb01ller vector and a role vector, which are bound together with the tensor product. These tensor products, concatenated, comprise the \u201ccontext\u201d over which attention will operate in the decoder. The sum of the word-level TPRs, \ufb02attened to a vector, is treated as a representation of the entire problem statement; it is fed to 2When the role vectors are not linearly independent, this oper- ation performs unbinding approximately, taking U to be the left pseudo-inverse of R. Because randomly chosen vectors on the unit sphere in a high-dimensional space are approximately orthogonal, the approximation is often excellent. the \u201cReasoning MLP\u201d, which transforms this encoding of the problem into a vector encoding the solution.",
      "the \u201cReasoning MLP\u201d, which transforms this encoding of the problem into a vector encoding the solution. This is the initial state of the \u201cTP-N2F decoder\u201d attentional LSTM, which outputs at each time step an order-3 tensor repre- senting a relational tuple. To generate a correct tuple from decoder operations, the model must learn to give the order-3 tensor the form of a TPR for a (R A1 A2) tuple (detailed explanation in Sec. 4.1). In the following sections, we \ufb01rst introduce the details of our proposed role-level description for N2F tasks, and then present how our proposed TP-N2F model uses TPR binding and unbinding operations to create a neural network implementation of this description of N2F tasks. 4.1. Role-level description of N2F tasks In this section, we propose a role-level description of N2F tasks, which speci\ufb01es the \ufb01ller/role structures of the input natural-language symbolic expressions and the output rela- tional representations. As the two structures are different, we also propose a formal scheme for structure mapping on TPRs.",
      "As the two structures are different, we also propose a formal scheme for structure mapping on TPRs. Role-Level Description for Natural-Language Input Instead of encoding each token of a sentence with a non- compositional embedding vector looked up in a learned dictionary, we use a learned role-\ufb01ller decomposition to compose a tensor representation for each token. Given a sen- tence S with n word tokens {w0, w1, ..., wn\u22121}, each word token wt is assigned a learned role vector rt, soft-selected from the learned dictionary R, and a learned \ufb01ller vector f t, soft-selected from the learned dictionary F (Sec. 3). The mechanism closely follows that of Palangi et al. (2018), and we hypothesize similar results: the role and \ufb01ller ap- proximately encode the structural role of the token and its lexical semantics, respectively.3 Then each word token wt is represented by the tensor product of the role vector and the \ufb01ller vector: Tt = f t \u2297rt. In addition to the set of all its token embeddings {T0, . . .",
      "In addition to the set of all its token embeddings {T0, . . . , Tn\u22121}, the sentence S as a whole is assigned a TPR equal to the sum of the TPR embeddings of all its word tokens: TS = Pn\u22121 t=0 Tt. Using TPRs to encode natural language has several advan- tages. First, natural language TPRs can be interpreted by exploring the distribution of tokens grouped by the role and \ufb01ller vectors they are assigned by a trained model (as in Palangi et al. (2018)). Second, TPRs avoid the Bag of Word (BoW) confusion (Huang et al., 2018): the BoW encoding of Jay saw Kay is the same as the BoW encoding of Kay saw Jay but the encodings are different with TPR embedding, 3Although the TPR formalism treats \ufb01llers and roles symmetri- cally, in use, hyperparameters are selected so that the number of available \ufb01llers is greater than that of roles.",
      "Thus, on average, each role is assigned to more words, encouraging it to take on a more general function, such as a grammatical role.",
      "Mapping Natural-language Problems to Formal-language Solutions Using Structured Neural Representations Figure 1. Overview diagram of TP-N2F. because the role \ufb01lled by a symbol changes with its context. Role-Level Description for Relational Representations In this section, we propose a novel recursive role-level de- scription for representing symbolic relational tuples. Each relational tuple contains a relation token and multiple ar- gument tokens. Given a binary relation rel, a relational tuple can be written as (rel arg1 arg2) where arg1, arg2 indicate two arguments of relation rel. Let us adopt the two positional roles, prel i = argi-of-rel for i = 1, 2. The \ufb01ller of role prel i is argi. Now let us use role decomposition re- cursively, noting that the role prel i can itself be decomposed into a sub-role pi = argi-of- which has a sub-\ufb01ller rel. Suppose that argi, rel, pi are embedded as vectors ai, r, pi.",
      "Now let us use role decomposition re- cursively, noting that the role prel i can itself be decomposed into a sub-role pi = argi-of- which has a sub-\ufb01ller rel. Suppose that argi, rel, pi are embedded as vectors ai, r, pi. Then the TPR encoding of prel i is rrel \u2297pi, so the TPR encoding of \ufb01ller argi bound to role prel i is ai \u2297(rrel \u2297pi). The tensor product is associative, so we can omit parenthe- ses and write the TPR for the formal-language expression, the relational tuple (rel arg1 arg2), as: H = a1 \u2297rrel \u2297p1 + a2 \u2297rrel \u2297p2. (3) Given the unbinding vectors p\u2032 i for positional sub-role vec- tors pi and the unbinding relational vector r\u2032 rel for the rela- tional vector rrel that embeds relation rel, each argument can be unbound in two steps as shown in Eqs. 4\u20135.",
      "4\u20135. H \u00b7 p\u2032 i = ai \u2297rrel (4) [ai \u2297rrel] \u00b7 r\u2032 rel = ai (5) Here \u00b7 denotes the tensor inner product, which for the order-3 tensor H and order-1 p\u2032 i in Eq. 4 can be de\ufb01ned as [H \u00b7 p\u2032 i]jk = P l[H]jkl[p\u2032 i]l; in Eq. 5, \u00b7 is equivalent to the matrix-vector product. Our proposed scheme can be contrasted with the TPR scheme in which (rel arg1 arg2) is embedded as rrel \u2297 a1 \u2297a2, e.g., (Smolensky et al., 2016; Schlag & Schmid- huber, 2018). In that scheme, an n-ary-relation tuple is embedded as an order-(n+1) tensor, and unbinding an argu- ment requires knowing all the other arguments (to use their unbinding vectors). In the scheme proposed here, an n-ary- relation tuple is still embedded as an order-3 tensor: there are just n terms in the sum in Eq.",
      "In the scheme proposed here, an n-ary- relation tuple is still embedded as an order-3 tensor: there are just n terms in the sum in Eq. 3, using n positional sub- role vectors p1, . . . , pn; unbinding simply requires knowing the unbinding vectors for these \ufb01xed position vectors. In the model, the order-3 tensor H of Eq. 3 has a differ- ent status than the order-2 tensor TS of Sec. 4.1. TS is a TPR by construction, whereas H is a TPR as a result of successful learning. To generate the output relational tuples, the decoder assumes each tuple has the form of Eq. 3, and performs the unbinding operations which that structure calls for. In section 4.4, it is shown that, if unbinding each of a set of roles from some unknown tensor T gives a target set of \ufb01llers, then T must equal the TPR generated by those role/\ufb01ller pairs, plus some tensor that is irrelevant because unbinding from it produces the zero vector.",
      "In other words, if the decoder succeeds in producing \ufb01ller vectors that corre- spond to output relational tuples that match the target, then, as far as what the decoder can see, the tensor that it operates on is the TPR of Eq. 3. TP-N2F Scheme for Learning Input-Output Mapping To generate formal relational tuples from natural-language descriptions, a learning strategy for the mapping between the two structures is particularly important. As shown in (6), we formalize the learning scheme as learning a mapping function fmapping(\u00b7), which, given a structural representa- tion of the natural-language input, TS, outputs a tensor TF from which the structural representation of the output can be generated. At the role level of description, there\u2019s nothing more to be said about this mapping; how it is modeled at the neural network level is discussed in Sec. 4.2. TF = fmapping(TS) (6)",
      "Mapping Natural-language Problems to Formal-language Solutions Using Structured Neural Representations Figure 2. Implementation of the TP-N2F encoder. 4.2. TP-N2F Model for N2F Generation In this section, we introduce the TP-N2F model. The en- coder and decoder are described \ufb01rst. Then, the inference and learning strategy are presented. Finally, we prove that the tensor that is input to the decoder\u2019s unbinding module is a TPR. Natural-Language Encoder in TP-N2F As shown in Figure 1, the TP-N2F model is implemented with three steps: encoding, mapping, and decoding. The encoding step is implemented by the TP-N2F natural-language encoder (TP-N2F Encoder), which takes the sequence of word to- kens as inputs, and encodes them via TPR binding according to the TP-N2F role scheme for natural-language input given in Sec. 4.1. The mapping step is implemented by an MLP called the Reasoning Module, which takes the encoding pro- duced by the TP-N2F Encoder as input.",
      "4.1. The mapping step is implemented by an MLP called the Reasoning Module, which takes the encoding pro- duced by the TP-N2F Encoder as input. It learns to map the natural-language-structure encoding of the input to a repre- sentation that will be processed under the assumption that it follows the role scheme for output relational-tuples speci- \ufb01ed in Sec. 4.1: the model needs to learn to produce TPRs such that this processing generates correct output programs. The decoding step is implemented by the TP-N2F relational tuples decoder (TP-N2F Decoder), which takes the output from the Reasoning Module (Sec. 4.1) and decodes the tar- get sequence of relational tuples via TPR unbinding. The TP-N2F Decoder utilizes an attention mechanism over the individual-word TPRs Tt produced by the TP-N2F Encoder. The detailed implementations are introduced below. The TP-N2F encoder follows the role scheme in Sec. 4.1 to encode each word token wt by soft-selecting one of nF \ufb01llers and one of nR roles.",
      "The detailed implementations are introduced below. The TP-N2F encoder follows the role scheme in Sec. 4.1 to encode each word token wt by soft-selecting one of nF \ufb01llers and one of nR roles. The \ufb01llers and roles are embed- ded as vectors. These embedding vectors, and the functions for selecting \ufb01llers and roles, are learned by two LSTMs, the Filler-LSTM and the Role-LSTM. (See Figure 2.) At each time-step t, the Filler-LSTM and the Role-LSTM take a learned word-token embedding wt as input. The hidden state of the Filler-LSTM, ht F, is used to compute softmax scores uF k over nF \ufb01ller slots, and a \ufb01ller vector f t = F uF is computed from the softmax scores (recall from Sec. 3 that F is the learned matrix of \ufb01ller vectors). Similarly, a role vector is computed from the hidden state of the Role-LSTM, ht R. fF and fR denote the functions that generate f t and rt from the hidden states of the two LSTMs.",
      "3 that F is the learned matrix of \ufb01ller vectors). Similarly, a role vector is computed from the hidden state of the Role-LSTM, ht R. fF and fR denote the functions that generate f t and rt from the hidden states of the two LSTMs. The token wt is encoded as Tt, the tensor product of f t and rt. Tt replaces the hidden vector in each LSTM and is passed to the next time step, together with the LSTM cell-state vector ct: see (7)\u2013(9). After encoding the whole sequence, the TP-N2F encoder outputs the sum of all tensor products P t Tt to the next module. We use an MLP, called the Reasoning MLP, for TPR mapping; it takes an order-2 TPR from the encoder and maps it to the initial state of the decoder. Detailed equations and implementation are provided in Appendix.",
      "We use an MLP, called the Reasoning MLP, for TPR mapping; it takes an order-2 TPR from the encoder and maps it to the initial state of the decoder. Detailed equations and implementation are provided in Appendix. ht F = fFiller\u2212LSTM(wt, Tt\u22121, ct\u22121 F ) (7) ht R = fRole\u2212LSTM(wt, Tt\u22121, ct\u22121 R ) (8) Tt = f t \u2297rt = fF(ht F) \u2297fR(ht R) (9) Relational-Tuple Decoder in TP-N2F The TP-N2F De- coder is an RNN that takes the output from the reasoning MLP as its initial hidden state for generating a sequence of relational tuples (Figure 3). This decoder contains an attentional LSTM called the Tuple-LSTM which feeds an unbinding module: attention operates on the context vector of the encoder, consisting of all individual encoder outputs {Tt}. The hidden-state H of the Tuple-LSTM is treated as a TPR of a relational tuple and is unbound to a relation and",
      "Mapping Natural-language Problems to Formal-language Solutions Using Structured Neural Representations Figure 3. Implementation of the TP-N2F decoder. arguments. During training, the Tuple-LSTM needs to learn a way to make H suitably approximate a TPR. At each time step t, the hidden state Ht of the Tuple-LSTM with attention (the version in Luong et al. (2015)) (11) is fed as input to the unbinding module, which regards Ht as if it were the TPR of a relational tuple with m arguments possessing the role structure described in Sec. 4.1: Ht \u2248Pm i=1 at i \u2297rt rel \u2297pi. (In Figure 3, the assumed hypothetical form of Ht, as well as that of Bt i below, is shown in a bubble with dashed border.) To decode a binary relational tuple, the unbinding module decodes it from Ht using the two steps of TPR unbinding given in (4)\u2013(5). The positional unbinding vectors p\u2032 i are learned during training and shared across all time steps.",
      "To decode a binary relational tuple, the unbinding module decodes it from Ht using the two steps of TPR unbinding given in (4)\u2013(5). The positional unbinding vectors p\u2032 i are learned during training and shared across all time steps. Af- ter the \ufb01rst unbinding step (4), i.e., the inner product of Ht with p\u2032 i, we get tensors Bt i (12 and 13). These are treated as the TPRs of two arguments at i bound to a relation rt rel. A relational unbinding vector r\u2032t rel is computed by a linear function from the sum of the Bt i (14) and used to compute the inner product with each Bt i to yield at i, which are treated as the embedding of argument vectors (15 and 16). Based on the TPR theory, r\u2032t rel is passed to a linear function to get rt rel as the embedding of a relation vector. Finally, the softmax probability distribution over symbolic outputs is computed for relations and arguments separately. In generation, the most probable symbol is selected.",
      "Based on the TPR theory, r\u2032t rel is passed to a linear function to get rt rel as the embedding of a relation vector. Finally, the softmax probability distribution over symbolic outputs is computed for relations and arguments separately. In generation, the most probable symbol is selected. (Detailed equations are in Appendix A.2.3 of supplementary) ht = fTuple\u2212LSTM(relt, argt 1, argt 2, Ht\u22121, ct\u22121) (10) Ht = Atten(ht, [T0, ..., Tn\u22121]) (11) Bt 1 = Ht \u00b7 p\u2032 1 (12) Bt 2 = Ht \u00b7 p\u2032 2 (13) r\u2032t rel = flinear(Bt 1 + Bt 2) (14) at 1 = Bt 1 \u00b7 r\u2032t rel (15) at 2 = Bt 2 \u00b7 r\u2032t rel (16) 4.3. Inference and Learning Strategy During inference time, natural language questions are en- coded via the encoder and the Reasoning MLP maps the output of the encoder to the input of the decoder.",
      "Inference and Learning Strategy During inference time, natural language questions are en- coded via the encoder and the Reasoning MLP maps the output of the encoder to the input of the decoder. We use greedy decoding (selecting the most likely class) to decode one relation and its arguments. The relation and argument vectors are concatenated to construct a new vector as the input for the Tuple-LSTM in the next step. TP-N2F is trained using back-propagation (Rumelhart et al., 1986) with the Adam optimizer (Kingma & Ba, 2017) and teacher-forcing. At each time step, the ground-truth rela- tional tuple is provided as the input for the next time step. As the TP-N2F decoder decodes a relational tuple at each time step, the relation token is selected only from the rela- tion vocabulary and the argument tokens from the argument vocabulary. For an input I that generates N output rela- tional tuples, the loss is the sum of the cross entropy loss L between the true labels L and predicted tokens for relations and arguments as shown in (17).",
      "For an input I that generates N output rela- tional tuples, the loss is the sum of the cross entropy loss L between the true labels L and predicted tokens for relations and arguments as shown in (17). LI = N\u22121 X i=0 L(reli, Lreli) + N\u22121 X i=0 2 X j=1 L(argi j, Largi j) (17) 4.4. Input of Decoder\u2019s Unbinding Module is a TPR Here we show that, if learning is successful, the order-3 tensor H that each iteration of the decoder\u2019s Tuple LSTM feeds to the decoder\u2019s Unbinding Module (Figure 3) will be a TPR of the form assumed in Eq. 3 above, repeated here: H = X j aj \u2297rrel \u2297pj. (18)",
      "Mapping Natural-language Problems to Formal-language Solutions Using Structured Neural Representations The operations performed by the decoder are given in Eqs.4\u2013 5, and Eqs.12\u201313, rewritten here: H \u00b7 p\u2032 i = qi (19) qi \u00b7 r\u2032 rel = ai (20) This is the standard TPR unbinding operation, used recur- sively: \ufb01rst with the unbinding vectors for positions, p\u2032 i, then with the unbinding vector for the operator, r\u2032 rel. It therefore suf\ufb01ces to analyze a single unbinding; the result can then be used recursively. This in effect reduces the problem to the order-2 case.",
      "It therefore suf\ufb01ces to analyze a single unbinding; the result can then be used recursively. This in effect reduces the problem to the order-2 case. What we will show is: given a set of unbinding vectors {r\u2032 i} which are dual to a set of role vectors {ri}, with i ranging over some index set I, if H is an order-2 tensor such that H \u00b7 r\u2032 i = fi, \u2200i \u2208I (21) then H = X i\u2208I fir\u22a4 i + Z \u2261HTPR + Z (22) for some tensor Z that annihilates all the unbinding vectors: Z \u00b7 r\u2032 i = 0, \u2200i \u2208I. (23) If learning is successful, the processing in the decoder will generate the target relational tuple (R, A1, A2) by obeying Eq. 19 in the \ufb01rst unbinding, where we have r\u2032 i = p\u2032 i, fi = qi, I = {1, 2}, and obeying Eq.",
      "19 in the \ufb01rst unbinding, where we have r\u2032 i = p\u2032 i, fi = qi, I = {1, 2}, and obeying Eq. 20 in the second unbinding, where we have r\u2032 i = r\u2032 rel, f \u2032 i = ai, with I = the set containing only the null index. Treat rank-2 tensors as matrices; then unbinding is simply matrix-vector multiplication. Assume the set of unbind- ing vectors is linearly independent (otherwise there would not be a general way to satisfy Eq. 21 exactly, contrary to assumption). Then expand the set of unbinding vectors, if necessary, into a basis {r\u2032 k}k\u2208K\u2287I. Find the dual basis, with rk dual to r\u2032 k (so that r\u22a4 l r\u2032 j = \u03b4lj). Because {r\u2032 k}k\u2208K is a basis, so is {rk}k\u2208K, so any matrix H can be expanded as H = P k\u2208K vkr\u22a4 k .",
      "Because {r\u2032 k}k\u2208K is a basis, so is {rk}k\u2208K, so any matrix H can be expanded as H = P k\u2208K vkr\u22a4 k . Since Hr\u2032 i = fi, \u2200i \u2208I are the unbind- ing conditions (Eq. 21), we must have vi = fi, i \u2208I. Let HTPR \u2261P i\u2208I fir\u22a4 i . This is the desired TPR, with \ufb01llers fi bound to the role vectors ri which are the duals of the unbinding vectors r\u2032 i (i \u2208I). Then we have H = HTPR+Z (Eq. 22) where Z \u2261P j\u2208K,j\u0338\u2208I vjr\u22a4 j ; so Zr\u2032 i = 0, i \u2208I (Eq. 23). Thus, if training is successful, the model must have learned how to feed the decoder with order-3 TPRs with the structure posited in Eq. 18.",
      "23). Thus, if training is successful, the model must have learned how to feed the decoder with order-3 TPRs with the structure posited in Eq. 18. The argument so far addresses the case where the unbind- ing vectors are linearly independent, making it possible to satisfy Eq. 21 exactly. In relatively high-dimensional vector spaces, it will often happen that even when the number of unbinding vectors exceeds the dimension of their space by a factor of 2 or 3 (which applies to the TP-N2F models pre- sented here), there is a set of role vectors {rk}k\u2208K approx- imately dual to {r\u2032 k}k\u2208K, such that r\u22a4 l r\u2032 j = \u03b4lj \u2200l, j \u2208K holds to a good approximation. (If the distribution of normal- ized unbinding vectors is approximately uniform on the unit sphere, then choosing the approximate dual vectors to equal the unbinding vectors themselves will do, since they will be nearly orthonormal.",
      "(If the distribution of normal- ized unbinding vectors is approximately uniform on the unit sphere, then choosing the approximate dual vectors to equal the unbinding vectors themselves will do, since they will be nearly orthonormal. If the {r\u2032 k}k\u2208K are not normalized, we just rescale the role vectors, choosing rk = r\u2032 k/\u2225r\u2032 k\u22252.) When the number of such role vectors exceeds the dimen- sion of the embedding space, they will be overcomplete, so while it is still true that any matrix H can be expanded as above (H = P k\u2208K vkr\u22a4 k ), this expansion will no longer be unique. So while it remains true that H a TPR, it is no longer uniquely decomposable into \ufb01ller/role pairs. The claim above does not claim uniqueness in this sense, and remains true.) 5. Experiments The proposed TP-N2F model is evaluated on two N2F tasks, generating operation sequences to solve math problems and generating Lisp programs. In both tasks, TP-N2F achieves state-of-the-art performance.",
      "5. Experiments The proposed TP-N2F model is evaluated on two N2F tasks, generating operation sequences to solve math problems and generating Lisp programs. In both tasks, TP-N2F achieves state-of-the-art performance. We further analyze the behav- ior of the unbinding relation vectors in the proposed model. Results of each task and the analysis of the unbinding rela- tion vectors are introduced in turn. Details of experiments and datasets are described in Appendix A.1 of the supple- mentary materials. 5.1. Generating Operations to Solve Math Problems Given a natural-language math problem, we need to gener- ate a sequence of operations (operators and corresponding arguments) from a set of operators and arguments to solve the given problem. Each operation is regarded as a relational tuple by viewing the operator as relation, e.g., (add, n1, n2). We test TP-N2F for this task on the MathQA dataset (Amini et al., 2019). The MathQA dataset consists of about 37k math word problems, each with a corresponding list of multi- choice options and the corresponding operation sequence.",
      "We test TP-N2F for this task on the MathQA dataset (Amini et al., 2019). The MathQA dataset consists of about 37k math word problems, each with a corresponding list of multi- choice options and the corresponding operation sequence. In this task, TP-N2F is deployed to generate the operation sequence given the question. The generated operations are executed with the execution script from Amini et al. (2019) to select a multi-choice answer. As there are about 30% noisy data (where the execution script returns the wrong answer when given the ground-truth program; see Appendix A.1 in the supplementary materials), we report both execu- tion accuracy (of the \ufb01nal multi-choice answer after run- ning the execution engine) and operation sequence accuracy (where the generated operation sequence must match the ground truth sequence exactly). TP-N2F is compared to a baseline provided by the seq2prog model in Amini et al. (2019), an LSTM-based seq2seq",
      "Mapping Natural-language Problems to Formal-language Solutions Using Structured Neural Representations Table 1. Results on MathQA dataset testing set MODEL Operation Accuracy (%) Execution Accuracy (%) SEQ2PROG-orig 59.4 51.9 SEQ2PROG-best 66.97 54.0 LSTM2TP (ours) 68.21 54.61 TP2LSTM (ours) 68.84 54.61 TP-N2F (ours) 71.89 55.95 model with attention. Our model outperforms both the orig- inal seq2prog, designated SEQ2PROG-orig, and the best reimplemented seq2prog after an extensive hyperparameter search, designated SEQ2PROG-best. Table 1 presents the results. To verify the importance of the TP-N2F encoder and decoder, we conducted experiments to replace either the encoder with a standard LSTM (denoted LSTM2TP) or the decoder with a standard attentional LSTM (denoted TP2LSTM). We observe that both the TPR components of TP-N2F are important for achieving the observed perfor- mance gain relative to the baseline. 5.2.",
      "We observe that both the TPR components of TP-N2F are important for achieving the observed perfor- mance gain relative to the baseline. 5.2. Generating Lisp Programs from NL Descriptions Generating Lisp programs requires sensitivity to structural information because Lisp code and data can be regarded as tree-structured. Given a natural-language query, we need to generate code containing function calls with parameters. Each function call is a relational tuple, which has a function as the relation and parameters as arguments. We evaluate our model on the AlgoLisp dataset for this task and achieve state- of-the-art performance. The AlgoLisp dataset (Polosukhin & Skidanov, 2018) is a program synthesis dataset. Each sample contains a problem description, a corresponding Lisp program tree, and 10 input-output testing pairs. We parse the program tree into a straight-line sequence of tuples (same style as in MathQA).",
      "Each sample contains a problem description, a corresponding Lisp program tree, and 10 input-output testing pairs. We parse the program tree into a straight-line sequence of tuples (same style as in MathQA). AlgoLisp provides an execution script to run the generated program and has three evaluation metrics: the accuracy of passing all test cases (Acc), the accuracy of passing 50% of test cases (50p-Acc), and the accuracy of generating an exactly matching program (M- Acc). AlgoLisp has about 10% noisy data (details in the Appendix), so we report results both on the full test set and the cleaned test set (in which all noisy testing samples are removed). TP-N2F is compared with an LSTM seq2seq with atten- tion model, the Seq2Tree model in Polosukhin & Skidanov (2018), and a seq2seq model with a pre-trained tree de- coder from the Tree2Tree autoencoder (SAPS) reported in Bednarek et al. (2019). As shown in Table 2, TP-N2F out- performs all existing models on both the full test set and the cleaned test set.",
      "(2019). As shown in Table 2, TP-N2F out- performs all existing models on both the full test set and the cleaned test set. Ablation experiments with TP2LSTM and LSTM2TP show that, for this task, the TP-N2F De- coder is more helpful than TP-N2F Encoder. This may be because Lisp code relies more heavily on structured repre- sentations. Comparing the generated programs, TP-N2F can generate longer programs than the LSTM-based Seq2Seq, e.g. TP-N2F correctly generates a program with 55 tuples but the LSTM-based Seq2Seq fails. Generated examples are presented in the Appendix. 6. Interpretation of Learned Structure To interpret the structure learned by the model, we explore both the TP-N2F Encoder and the Decoder. For TP-N2F Encoder, the Softmax scores for roles and \ufb01llers of natural- language are analyzed on selected questions. We explore the signi\ufb01cant \ufb01llers and roles of natural-language questions with large Softmax scores.",
      "For TP-N2F Encoder, the Softmax scores for roles and \ufb01llers of natural- language are analyzed on selected questions. We explore the signi\ufb01cant \ufb01llers and roles of natural-language questions with large Softmax scores. Analysis shows that \ufb01llers tend to represent the semantic information and words or phrases with the same meaning tend to be assigned the same \ufb01ller. For example, in the AlgoLisp dataset, \u201cconsider\u201d, \u201cyou are given\u201d and \u201cgiven\u201d are assigned to \ufb01ller 146. \u201cdecrement\u201d, \u201cdifference of\u201d and \u201cdecremented by\u201d are assigned to \ufb01ller 43. \u201cincrement\u201d and \u201cadd\u201d are assigned to \ufb01ller 105. In the MathQA dataset, \u201cpositive integer\u201d, \u201cpositive number\u201d and \u201cpositive digits\u201d are assigned to \ufb01ller 27. We also \ufb01nd that roles tend to represent the structured schemes of sentences. For example, Figure 4 shows the visualization of assigned roles for two different questions from the Algolisp dataset. Words with role 12 indicate the target of the questions to compute.",
      "We also \ufb01nd that roles tend to represent the structured schemes of sentences. For example, Figure 4 shows the visualization of assigned roles for two different questions from the Algolisp dataset. Words with role 12 indicate the target of the questions to compute. Words with role 3 indicate required information to solve the questions. One interesting \ufb01nding is that the second example from Figure 4 has two occurrences of \u201ca\u201d with different meanings. Therefore, although they are as- signed the same role, they have different \ufb01llers. The detailed visualization of \ufb01llers is shown in Appendix. For the the TP-N2F Decoder, we extract the trained unbind- ing relation vectors and reduce the dimension of vectors via Principal Components Analysis. K-means clustering results on the average vectors are presented in Appendix A.6 of the supplementary material. Results show that unbinding vectors for operators or functions with similar semantics tend to be close to each other. For example, with 5 clus- ters in the MathQA dataset, arithmetic operators such as add, subtract, multiply, divide are clustered together, and operators related to square or volume of geometry are clus- tered together.",
      "For example, with 5 clus- ters in the MathQA dataset, arithmetic operators such as add, subtract, multiply, divide are clustered together, and operators related to square or volume of geometry are clus- tered together. With 4 clusters in the AlgoLisp dataset, par-",
      "Mapping Natural-language Problems to Formal-language Solutions Using Structured Neural Representations Table 2. Results of AlgoLisp dataset Full Testing Set Cleaned Testing Set MODEL Acc (%) 50p-Acc (%) M-Acc (%) Acc (%) 50p-Acc( %) M-Acc (%) Seq2Tree 61.0 LSTM2LSTM+atten 67.54 70.89 75.12 76.83 78.86 75.42 TP2LSTM (ours) 72.28 77.62 79.92 77.67 80.51 76.75 LSTM2TPR (ours) 75.31 79.26 83.05 84.44 86.13 83.43 SAPSpre-VH-Att-256 83.80 87.45 92.98 94.15 TP-N2F (ours) 84.02 88.01 93.06 93.48 94.64 92.78 Figure 4. Visualizations of selected roles in TP-N2F encoder for two questions in the AlgoLisp Dataset.",
      "Visualizations of selected roles in TP-N2F encoder for two questions in the AlgoLisp Dataset. tial/lambda functions and sort functions are in one cluster, and string processing functions are clustered together. Note that there is no direct supervision to inform the model about the nature of the operations, and the TP-N2F decoder has induced this role structure using weak supervision signals from question/operation-sequence-answer pairs. 7. Conclusion and Future Work In this paper we propose a new scheme for neural-symbolic relational representations and a new architecture, TP-N2F, for formal-language generation from natural-language de- scriptions. To our knowledge, TP-N2F is the \ufb01rst model that combines TPR binding and TPR unbinding in the encoder- decoder fashion. TP-N2F achieves the state-of-the-art on two instances of N2F tasks, showing signi\ufb01cant structure learning ability. The results show that both the TP-N2F en- coder and the TP-N2F decoder are important for improving natural- to formal-language generation.",
      "TP-N2F achieves the state-of-the-art on two instances of N2F tasks, showing signi\ufb01cant structure learning ability. The results show that both the TP-N2F en- coder and the TP-N2F decoder are important for improving natural- to formal-language generation. By exploring the learned structures in both encoder and decoder, we show that TPRs enhance the interpretability of sequence-processing deep learning models and provide a step towards better understanding neural models. Next, we will combine large- scale deep learning models such as BERT with TP-N2F to take advantage of structure learning for other generation tasks. Acknowledgements We are grateful to Aida Amini from the University of Wash- ington for providing execution scripts for the MathQA dataset.",
      "Mapping Natural-language Problems to Formal-language Solutions Using Structured Neural Representations References Amini, A., Gabriel, S., Lin, P., Kedziorski, R. K., Choi, Y., and Hajishirzi, H. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. In NACCL, 2019. Bednarek, J., Piaskowski, K., and Krawiec, K. Ain\u2019t nobody got time for coding: Structure-aware program synthesis from natural language. In arXiv.org, 2019. Cai, D. and Lam, W. Core semantic \ufb01rst: A top-down approach for amr parsing. In arXiv:1909.04303, 2019. Chen, K. and Forbus, K. D. Action recognition from skele- ton data via analogical generalization over qualitative rep- resentations. In Thirty-Second AAAI Conference, 2018. Chen, K., Rabkina, I., McLure, M. D., and Forbus, K. D. Human-like sketch object recognition via analogical learning.",
      "In Thirty-Second AAAI Conference, 2018. Chen, K., Rabkina, I., McLure, M. D., and Forbus, K. D. Human-like sketch object recognition via analogical learning. In Thirty-Third AAAI Conference, volume 33, pp. 1336\u20131343, 2019. Crouse, M., McFate, C., and Forbus, K. D. Learning from unannotated qa pairs to analogically disanbiguate and answer questions. In Thirty-Second AAAI Conference, 2018. Forbus, K., Liang, C., and Rabkina, I. Representation and computation in cognitive models. In Top Cognitive Sys- tem, 2017. Gao, J., Galley, M., and Li, L. Neural approaches to con- versational ai. Foundations and Trends R\u20ddin Information Retrieval, 13(2-3):127\u2013298, 2019. Goldin-Meadow, S. and Gentner, D. Language in mind: Advances in the study of language and thought. MIT Press, 2003.",
      "Foundations and Trends R\u20ddin Information Retrieval, 13(2-3):127\u2013298, 2019. Goldin-Meadow, S. and Gentner, D. Language in mind: Advances in the study of language and thought. MIT Press, 2003. Huang, Q., Smolensky, P., He, X., Wu, O., and Deng, L. Ten- sor product generation networks for deep nlp modeling. In NAACL, 2018. Huang, Q., Deng, L., Wu, D., Liu, c., and He, X. Attentive tensor product learning. In Thirty-Third AAAI Conference, volume 33, 2019. Kamath, A. and Das, R. A survey on semantic parsing. In AKBC, 2019. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2017. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.",
      "Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2017. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 1207\u20131216, Stan- ford, CA, 2000. Morgan Kaufmann. Lee, K., Palangi, H., Chen, X., Hu, H., and Gao, J. Learn- ing visual relation priors for image-text matching and image captioning with neural scene graph generators. abs/1909.09953, 2019. URL http://arxiv.org/ abs/1909.09953. Lee, M., He, X., Yih, W.-t., Gao, J., Deng, L., and Smolen- sky, P. Reasoning in vector space: An exploratory study of question answering. In ICLR, 2016.",
      "Lee, M., He, X., Yih, W.-t., Gao, J., Deng, L., and Smolen- sky, P. Reasoning in vector space: An exploratory study of question answering. In ICLR, 2016. Liao, Y., Bing, L., Li, P., Shi, S., Lam, W., and Zhang, T. Core semantic \ufb01rst: A top-down approach for amr parsing. In EMNLP, pp. 3855\u20133864, 2018. Luong, M.-T., Pham, H., and Manning, C. D. Effective approaches to attention-based neural machine translation. EMNLP, pp. 533\u2013536, 2015. Martin, A. E. A compositional neural architecture for lan- guage. Journal of Cognitive Neuroscience, 2020. Palangi, H., Smolensky, P., He, X., and Deng, L. Question- answering with grammatically-interpretable representa- tions. In AAAI, 2018.",
      "Journal of Cognitive Neuroscience, 2020. Palangi, H., Smolensky, P., He, X., and Deng, L. Question- answering with grammatically-interpretable representa- tions. In AAAI, 2018. Polosukhin, I. and Skidanov, A. Neural program search: Solving programming tasks from description and exam- ples. In ICLR workshop, 2018. Roads, B. and Love, B. Learning as the unsuper- vised alignment of conceptual systems. arXiv preprint arXiv:1906.09012, 2019. Rumelhart, D. E., Hinton, G. E., and Williams, R. J. Learn- ing internal representations by error propagation. In Rumelhart, D. E., McClelland, J. L., and the PDP Group (eds.), Parallel distributed processing: Explorations in the microstructure of cognition, volume 1, pp. 318\u2013362. MIT press, Cambridge, MA, 1986.",
      "In Rumelhart, D. E., McClelland, J. L., and the PDP Group (eds.), Parallel distributed processing: Explorations in the microstructure of cognition, volume 1, pp. 318\u2013362. MIT press, Cambridge, MA, 1986. Schlag, I. and Schmidhuber, J. Learning to reason with third order tensor products. In Neural Information Processing Systems, 2018. Smolensky, P. Tensor product variable binding and the representation of symbolic structures in connectionist networks. In Arti\ufb01cial Intelligence, volume 46, pp. 159\u2013 216, 1990. Smolensky, P., Lee, M., He, X., Yih, W.-t., Gao, J., and Deng, L. Basic reasoning with tensor product representa- tions. arXiv preprint arXiv:1601.02745, 2016.",
      "Mapping Natural-language Problems to Formal-language Solutions Using Structured Neural Representations A. Appendix B. Implementations of TP-N2F for experiments In this section, we present details of the experiments of TP- N2F on the two datasets. We present the implementation of TP-N2F on each dataset. The MathQA dataset consists of about 37k math word prob- lems ((80/12/8)% training/dev/testing problems), each with a corresponding list of multi-choice options and an straight- line operation sequence program to solve the problem. An example from the dataset is presented in the Appendix A.4. In this task, TP-N2F is deployed to generate the operation sequence given the question. The generated operations are executed to generate the solution for the given math prob- lem. We use the execution script from (Amini et al., 2019) to execute the generated operation sequence and compute the multi-choice accuracy for each problem. During our experiments we observed that there are about 30% noisy examples (on which the execution script fails to get the correct answer on the ground truth program).",
      "During our experiments we observed that there are about 30% noisy examples (on which the execution script fails to get the correct answer on the ground truth program). Therefore, we report both execution accuracy (the \ufb01nal multi-choice answer after running the execution engine) and operation sequence accuracy (where the generated operation sequence must match the ground truth sequence exactly). The AlgoLisp dataset (Polosukhin & Skidanov, 2018) is a program synthesis dataset, which has 79k/9k/10k train- ing/dev/testing samples. Each sample contains a problem description, a corresponding Lisp program tree, and 10 input-output testing pairs. We parse the program tree into a straight-line sequence of commands from leaves to root and (as in MathQA) use the symbol #i to indicate the result of the ith command (generated previously by the model). A dataset sample with our parsed command sequence is pre- sented in the Appendix A.4.",
      "A dataset sample with our parsed command sequence is pre- sented in the Appendix A.4. AlgoLisp provides an execution script to run the generated program and has three evaluation metrics: accuracy of passing all test cases (Acc), accuracy of passing 50% of test cases (50p-Acc), and accuracy of generating an exactly matched program (M-Acc). AlgoLisp has about 10% noise data (where the execution script fails to pass all test cases on the ground truth program), so we report results both on the full test set and the cleaned test set (in which all noisy testing samples are removed). We use dR, nR, dF, nF to indicate the TP-N2F encoder hy- perparameters, the dimension of role vectors, the number of roles, the dimension of \ufb01ller vectors and the number of \ufb01llers. dRel, dArg, dP os indicate the TP-N2F decoder hyper- parameters, the dimension of relation vectors, the dimension of argument vectors, and the dimension of position vectors.",
      "dRel, dArg, dP os indicate the TP-N2F decoder hyper- parameters, the dimension of relation vectors, the dimension of argument vectors, and the dimension of position vectors. In the experiment on the MathQA dataset, we use nF = 150, nR = 50, dF = 30, dR = 20, dRel = 20, dArg = 10, dP os = 5 and we train the model for 60 epochs with learning rate 0.00115. The reasoning module only contains one layer. As most of the math operators in this dataset are binary, we replace all operators taking three arguments with a set of binary operators based on hand-encoded rules, and for all operators taking one argument, a padding symbol is appended. For the baseline SEQ2PROG-orig, TP2LSTM and LSTM2TP, we use hidden size 100, single-direction, one-layer LSTM. For the SEQ2PROG-best, we performed a hyperparameter search on the hidden size for both encoder and decoder; the best score is reported.",
      "For the SEQ2PROG-best, we performed a hyperparameter search on the hidden size for both encoder and decoder; the best score is reported. In the experiment on the AlgoLisp dataset, we use nF = 150, nR = 50, dF = 30, dR = 30, dRel = 30, dArg = 20, dP os = 5 and we train the model for 50 epochs with learn- ing rate 0.00115. We also use one-layer in the reasoning module like in MathQA. For this dataset, most function calls take three arguments so we simply add padding symbols for those functions with fewer than three arguments. C. Analysis from ablation studies We performed some ablation studies. The explanation stud- ies and \ufb01ndings are discussed here. As TP-N2F model usually needs more parameters for TPRs, we tested the base- line LSTM2LSTM+attention model with similar number of parameters (increasing the hidden size in the encoder and decoder). We found that the performance of baseline model decreased when they had similar degree of parameters. We also tested different number of layers of the reasoning MLP.",
      "We found that the performance of baseline model decreased when they had similar degree of parameters. We also tested different number of layers of the reasoning MLP. Each layer of the MLP is a linear layer following Tanh ac- tivation function. From ablation studies, the performance with 1, 2 and 3 layers were similar. As the number of layers increase, the performance reduced. Finally, we tested using the tensor product of last hidden states of Role-LSTM and Filler-LSTM instead of the sum of all tensor products. Ex- periments showed that using tensor product sums had better performance than using last hidden states. D. Detailed equations of TP-N2F D.1.",
      "Ex- periments showed that using tensor product sums had better performance than using last hidden states. D. Detailed equations of TP-N2F D.1. TP-N2F encoder Filler-LSTM in TP-N2F encoder This is a standard LSTM, governed by the equations: f t f = \u03d5(U\ufb00wt + V\ufb00\u266d(Tt\u22121) + b\ufb00) (24) gt f = tanh(Ufg wt + Vfg \u266d(Tt\u22121) + bfg) (25) it f = \u03d5(U\ufb01wt + V\ufb01\u266d(Tt\u22121) + b\ufb01) (26) ot f = \u03d5(Ufo wt + Vfo \u266d(Tt\u22121) + bfo) (27) ct f = f t f \u2299ct\u22121 f + it f \u2299gt f (28) ht f = ot f \u2299tanh(ct f) (29)",
      "Mapping Natural-language Problems to Formal-language Solutions Using Structured Neural Representations \u03d5, tanh are the logistic sigmoid and tanh functions applied elementwise. \u266d\ufb02attens (reshapes) a matrix in RdF\u00d7dR into a vector in RdT, where dT = dFdR. \u2299is elementwise mul- tiplication.",
      "\u266d\ufb02attens (reshapes) a matrix in RdF\u00d7dR into a vector in RdT, where dT = dFdR. \u2299is elementwise mul- tiplication. The variables have the following dimensions: f t f , gt f, it f, ot f, ct f, ht f, b\ufb00, bfg, b\ufb01, bfo, \u266d(Tt\u22121) \u2208RdT wt \u2208Rd U\ufb00, Ufg, U\ufb01, Ufo \u2208RdT\u00d7d V\ufb00, Vfg, V\ufb01, Vfo \u2208RdT\u00d7dT Filler vector The \ufb01ller vector for input token wt is f t, de\ufb01ned through an attention vector over possible \ufb01llers, at f: at f = softmax((Wfa ht f)/T) (30) f t = Wf at f (31) (Wf is the same as F of Sec.2 in the paper) The variables\u2019 dimensions are: Wfa \u2208RnF\u00d7dT at f \u2208RnF Wf \u2208RdF\u00d7nF f t \u2208RdF T is the temperature factor, which is \ufb01xed at 0.1.",
      "Role-LSTM in TP-N2F encoder Similar to the Filler-LSTM, the Role-LSTM is also a stan- dard LSTM, governed by the equations: f t r = \u03d5(Urf wt + Vrf \u266d(Tt\u22121) + brf) (32) gt r = tanh(Urg wt + Vrg \u266d(Tt\u22121) + brg) (33) it r = \u03d5(Uri wt + Vri \u266d(Tt\u22121) + bri) (34) ot r = \u03d5(Uro wt + Vro \u266d(Tt\u22121) + bro) (35) ct r = f t r \u2299ct\u22121 r + it r \u2299gt r (36) ht r = ot r \u2299tanh(ct r) (37) The variable dimensions are: f t r , gt r, it r, ot r, ct r, ht r, brf, brg, bri, bro, \u266d(Tt\u22121) \u2208RdT wt \u2208Rd Urf, Urg, Uri, Uro \u2208RdT\u00d7d Vrf, Vrg, Vri,",
      "gt r, it r, ot r, ct r, ht r, brf, brg, bri, bro, \u266d(Tt\u22121) \u2208RdT wt \u2208Rd Urf, Urg, Uri, Uro \u2208RdT\u00d7d Vrf, Vrg, Vri, Vro \u2208RdT\u00d7dT Role vector The role vector for input token wt is determined analogously to its \ufb01ller vector: at r = softmax((Wra ht r)/T) (38) rt = Wr at r (39) The dimensions are: Wra \u2208RnR\u00d7dT at r \u2208RnR Wr \u2208RdR\u00d7nR rt \u2208RdR Binding The TPR for the \ufb01ller/role binding for token wt is then: Tt = rt \u2297f t (40) where T t \u2208RdR\u00d7dF D.2. Structure Mapping H0 = fmapping(Tt) (41) H0 \u2208RdH, where dH = dA, dO, dP are dimension of argu- ment vector, operator vector and position vector.",
      "2. Structure Mapping H0 = fmapping(Tt) (41) H0 \u2208RdH, where dH = dA, dO, dP are dimension of argu- ment vector, operator vector and position vector. fmapping is implemented with a MLP (linear layer followed by a tanh) for mapping the Tt \u2208RdT to the initial state of decoder H0. D.3.",
      "fmapping is implemented with a MLP (linear layer followed by a tanh) for mapping the Tt \u2208RdT to the initial state of decoder H0. D.3. TP-N2F decoder Tuple-LSTM The output tuples are also generated via a standard LSTM: wt d = \u03b3(wt\u22121 Rel , wt\u22121 Arg1, wt\u22121 Arg2) (42) f t = \u03d5(Uf wt d + Vf \u266d(Ht\u22121) + bf) (43) gt = tanh(Ug wt d + Vg \u266d(Ht\u22121) + bg) (44) it = \u03d5(Ui wt d + Vi \u266d(Ht\u22121) + bi) (45) ot = \u03d5(Uo wt d + Vo \u266d(Ht\u22121) + bo) (46) ct = f t \u2299ct\u22121 + it \u2299gt (47) ht input = ot \u2299tanh(ct) (48) Ht = Atten(ht input, [T0, ..., Tn\u22121]) (49) Here, \u03b3 is the concatenation function.",
      "wt\u22121 Rel is the trained embedding vector for the Relation of the input binary tuple, wt\u22121 Arg1 is the embedding vector for the \ufb01rst argument and wt\u22121 Arg2 is the embedding vector for the second argument.",
      "Mapping Natural-language Problems to Formal-language Solutions Using Structured Neural Representations Then the input for the Tuple LSTM is the concatenation of the embedding vectors of relation and arguments, with dimension ddec. f t, gt, it, ot, ct, ht input, bf, bg, bi, bo, \u266d(Ht\u22121) \u2208RdH wt d \u2208Rddec Uf, Ug, Ui, Uo \u2208RdH\u00d7ddec Vf, Vg, Vi, Vo \u2208RdH\u00d7dH Ht \u2208RdH Atten is the attention mechanism used in Luong et al. (2015), which computes the dot product between ht input and each Tt\u2032. Then a linear function is used on the concate- nation of ht input and the softmax scores on all dot products to generate Ht. The following equations show the attention mechanism: dt = score(ht input, CT ) (50) st = CT softmax(dt) (51) Ht = K\u03b3(ht input, st) (52) score is the score function of the attention. In this paper, the score function is dot product.",
      "The following equations show the attention mechanism: dt = score(ht input, CT ) (50) st = CT softmax(dt) (51) Ht = K\u03b3(ht input, st) (52) score is the score function of the attention. In this paper, the score function is dot product. CT \u2208RdH\u00d7n dt \u2208Rn st \u2208RdH K \u2208RdH\u00d7(dT+n) Unbinding At each timestep t, the 2-step unbinding process described in Sec.3.1.2 of the paper operates \ufb01rst on an encoding of the triple as a whole, H, using two unbinding vectors p\u2032 i that are learned but \ufb01xed for all tuples. This \ufb01rst unbinding gives an encoding of the two operator-argument bindings, Bi. The second unbinding operates on the Bi, using a generated un- binding vector for the operator, r\u2032 rel, giving encodings of the arguments, ai.",
      "This \ufb01rst unbinding gives an encoding of the two operator-argument bindings, Bi. The second unbinding operates on the Bi, using a generated un- binding vector for the operator, r\u2032 rel, giving encodings of the arguments, ai. The generated unbinding vector for the oper- ator, r\u2032, and the generated encodings of the arguments, ai, each produce a probability distribution over symbolic opera- tor outputs Rel and symbolic argument outputs Argi; these probabilities are used in the cross-entropy loss function. For generating a single symbolic output, the most-probable symbols are selected.",
      "For generating a single symbolic output, the most-probable symbols are selected. Bt 1 = Ht p\u2032 1 (53) Bt 2 = Ht p\u2032 2 (54) r\u2032t rel = Wdual (Bt 1 + Bt 2) (55) at 1 = Bt 1 r\u2032t rel (56) at 2 = Bt 2 r\u2032t rel (57) lt rrel = Lt rrel r\u2032t rel (58) lt a1 = Lt a at 1 (59) lt a2 = Lt a at 2 (60) Relt = argmax(softmax(lt r)) (61) Arg1t = argmax(softmax(lt a1)) (62) Arg2t = argmax(softmax(lt a2)) (63) The dimensions are: r\u2032t rel \u2208RdO at 1, at 2 \u2208RdA p\u2032 1, p\u2032 2 \u2208RdP Bt 1,",
      "at 2 \u2208RdA p\u2032 1, p\u2032 2 \u2208RdP Bt 1, Bt 2 \u2208RdA\u00d7dO Wdual \u2208RdH Lt r \u2208RnO\u00d7dO Lt a \u2208RnA\u00d7dA lt r \u2208RnR lt a1, lt a2 \u2208RnA E. Dataset samples E.0.1. DATA SAMPLE FROM MATHQA DATASET Problem: The present polulation of a town is 3888. Population increase rate is 20%. Find the population of town after 1 year? Options: a) 2500, b) 2100, c) 3500, d) 3600, e) 2700 Operations: multiply(n0,n1), divide(#0,const-100), add(n0,#1) E.0.2. DATA SAMPLE FROM ALGOLISP DATASET Problem: Consider an array of numbers and a number, decrements each element in the given array by the given number, what is the given array?",
      "DATA SAMPLE FROM ALGOLISP DATASET Problem: Consider an array of numbers and a number, decrements each element in the given array by the given number, what is the given array? Program Nested List: (map a (partial1 b \u2013)) Command-Sequence: (partial1 b \u2013), (map a #0) F. Generated programs comparison In this section, we display some generated samples from the two datasets, where the TP-N2F model generates correct programs but LSTM-Seq2Seq does not. Question: A train running at the speed of 50 km per hour crosses a post in 4 seconds. What is the length of the train? TP-N2F(correct):",
      "Mapping Natural-language Problems to Formal-language Solutions Using Structured Neural Representations (multiply,n0,const1000) (divide,#0,const3600) (multi- ply,n1,#1) LSTM(wrong): (multiply,n0,const0.2778) (multiply,n1,#0) Question: 20 is subtracted from 60 percent of a number, the result is 88. Find the number? TP-N2F(correct): (add,n0,n2) (divide,n1,const100) (divide,#0,#1) LSTM(wrong): (add,n0,n2) (divide,n1,const100) (divide,#0,#1) (multi- ply,#2,n3) (subtract,#3,n0) Question: The population of a village is 14300. It increases annually at the rate of 15 percent. What will be its population after 2 years?",
      "It increases annually at the rate of 15 percent. What will be its population after 2 years? TP-N2F(correct): (divide,n1,const100) (add,#0,const1) (power,#1,n2) (multi- ply,n0,#2) LSTM(wrong): (multiply,const4,const100) (sqrt,#0) Question: There are two groups of students in the sixth grade. There are 45 students in group a, and 55 students in group b. If, on a particular day, 20 percent of the students in group a forget their homework, and 40 percent of the students in group b forget their homework, then what percentage of the sixth graders forgot their homework? TP-N2F(correct): (add,n0,n1) (multiply,n0,n2) (multiply,n1,n3) (di- vide,#1,const100) (divide,#2,const100) (add,#3,#4) (divide,#5,#0) (multiply,#6,const100) LSTM(wrong): (multiply,n0,n1) (subtract,n0,",
      "#1,const100) (divide,#2,const100) (add,#3,#4) (divide,#5,#0) (multiply,#6,const100) LSTM(wrong): (multiply,n0,n1) (subtract,n0,n1) (divide,#0,#1) Question: 1 divided by 0.05 is equal to TP-N2F(correct): (divide,n0,n1) LSTM(wrong): (divide,n0,n1) (multiply,n2,#0) Question: Consider a number a, compute factorial of a TP-N2F(correct): ( \u00a1=,arg1,1 ) ( -,arg1,1 ) ( self,#1 ) ( *,#2,arg1 ) ( if,#0,1,#3 ) ( lambda1,#4 ) ( invoke1,#5,a ) LSTM(wrong): ( \u00a1=,arg1,1 ) ( -,arg1,1 ) ( self,#1 ) ( *,#2,arg1 ) ( if,#0,1,#3 ) ( lambda1,#4 ) ( len,",
      "#5,a ) LSTM(wrong): ( \u00a1=,arg1,1 ) ( -,arg1,1 ) ( self,#1 ) ( *,#2,arg1 ) ( if,#0,1,#3 ) ( lambda1,#4 ) ( len,a ) ( invoke1,#5,#6 ) Question: Given an array of numbers and numbers b and c, add c to elements of the product of elements of the given array and b, what is the product of elements of the given array and b? TP-N2F(correct): ( partial, b,* ) ( partial1,c,+ ) ( map,a,#0 ) ( map,#2,#1 ) LSTM(wrong): ( partial1,b,+ ) ( partial1,c,+ ) ( map,a,#0 ) ( map,#2,#1 ) Question: You are given an array of numbers a and numbers b, c and d , let how many times you can replace the median in a with sum of its digits before it becomes a single digit number and b be the coordinates of one end and c and d be the coordinates of another end of segment e , your task is",
      "c and d , let how many times you can replace the median in a with sum of its digits before it becomes a single digit number and b be the coordinates of one end and c and d be the coordinates of another end of segment e , your task is to \ufb01nd the length of segment e rounded down TP-N2F(correct): ( digits arg1 ) ( len #0 ) ( == #1 1 ) ( digits arg1 ) ( reduce #3 0 + ) ( self #4 ) ( + 1 #5 ) ( if #2 0 #6 ) ( lambda1 #7 ) ( sort a ) ( len a ) ( / #10 2 ) ( deref #9 #11 ) ( invoke1 #8 #12 ) ( - #13 c ) ( digits arg1 ) ( len #15 ) ( == #16 1 ) ( digits arg1 ) ( reduce #18 0 + ) ( self #19 ) ( + 1 #20 ) ( if #17 0 #21 ) ( lambda1 #22 ) ( sort a ) ( len a ) ( / #25 2 ) ( deref #24 #26 ) ( invoke1 #23 #27 ) ( - #28 c",
      ") ( self #19 ) ( + 1 #20 ) ( if #17 0 #21 ) ( lambda1 #22 ) ( sort a ) ( len a ) ( / #25 2 ) ( deref #24 #26 ) ( invoke1 #23 #27 ) ( - #28 c ) ( * #14 #29 ) ( - b d ) ( - b d ) ( * #31 #32 ) ( + #30 #33 ) ( sqrt #34 ) ( \ufb02oor #35 ) LSTM(wrong): ( digits arg1 ) ( len #0 ) ( == #1 1 ) ( digits arg1 ) ( reduce #3 0 + ) ( self #4 ) ( + 1 #5 ) ( if #2 0 #6 ) ( lambda1 #7 ) ( sort a ) ( len a ) ( / #10 2 ) ( deref #9 #11 ) ( invoke1 #8 #12 c ) ( - #13 ) ( - b d ) ( - b d ) ( * #15 #16 ) ( * #14 #17 ) ( + #18 ) ( sqrt #19 ) ( \ufb02oor #20 ) Question: Given numbers a ,",
      ") ( invoke1 #8 #12 c ) ( - #13 ) ( - b d ) ( - b d ) ( * #15 #16 ) ( * #14 #17 ) ( + #18 ) ( sqrt #19 ) ( \ufb02oor #20 ) Question: Given numbers a , b , c and e , let d be c , reverse digits in d , let a and the number in the range from 1 to b inclusive that has the maximum value when its digits are reversed be the coordinates of one end and d and e be the coordinates of another end of segment f , \ufb01nd the length of segment f squared TP-N2F(correct): ( digits c ) ( reverse #0 ) ( * arg1 10 ) ( + #2 arg2 ) ( lambda2 #3 ) ( reduce #1 0 #4 ) ( - a #5 ) ( digits c ) ( reverse #7 ) ( * arg1 10 ) ( + #9 arg2 ) ( lambda2 #10 ) ( reduce #8 0 #11 ) ( - a #12 ) ( * #6 #13 ) ( + b 1 ) ( range 0 #15 ) ( digits arg1 ) ( reverse #17 )",
      "* arg1 10 ) ( + #9 arg2 ) ( lambda2 #10 ) ( reduce #8 0 #11 ) ( - a #12 ) ( * #6 #13 ) ( + b 1 ) ( range 0 #15 ) ( digits arg1 ) ( reverse #17 ) ( * arg1 10 ) ( + #19 arg2 ) ( lambda2 #20 ) ( reduce #18 0 #21 ) ( digits arg2 ) ( reverse #23 ) ( * arg1 10 ) ( + #25 arg2 ) ( lambda2 #26 ) ( reduce #24 0 #27 ) ( \u00bf #22 #28 ) ( if #29 arg1 arg2 ) ( lambda2 #30 ) ( reduce #16 0 #31 ) ( - #32 e ) ( + b 1 ) ( range 0 #34 ) ( digits arg1 ) ( reverse #36 ) ( * arg1 10 ) ( + #38 arg2 ) ( lambda2 #39 ) ( reduce #37 0 #40 ) ( digits arg2 ) ( reverse #42 ) ( * arg1 10 ) ( + #44 arg2 ) ( lambda2 #45 ) ( reduce #43",
      ") ( * arg1 10 ) ( + #38 arg2 ) ( lambda2 #39 ) ( reduce #37 0 #40 ) ( digits arg2 ) ( reverse #42 ) ( * arg1 10 ) ( + #44 arg2 ) ( lambda2 #45 ) ( reduce #43 0 #46 ) ( \u00bf #41 #47 ) ( if #48 arg1 arg2 ) ( lambda2 #49 ) ( reduce #35 0 #50 ) ( - #51 e ) ( * #33 #52 ) ( + #14 #53 ) LSTM(wrong): ( - a d ) ( - a d ) ( * #0 #1 ) ( digits c ) ( reverse #3 ) ( * arg1 10 ) ( + #5 arg2 ) ( lambda2 #6 ) ( reduce #4 0 #7 ) ( - #8 e )",
      "Mapping Natural-language Problems to Formal-language Solutions Using Structured Neural Representations ( + b 1 ) ( range 0 #10 ) ( digits arg1 ) ( reverse #12 ) ( * arg1 10 ) ( + #14 arg2 ) ( lambda2 #15 ) ( reduce #13 0 #16 ) ( digits arg2 ) ( reverse #18 ) ( * arg1 10 ) ( + #20 arg2 ) ( lambda2 #21 ) ( reduce #19 0 #22 ) ( \u00bf #17 #23 ) ( if #24 arg1 arg2 ) ( lambda2 #25 ) ( reduce #11 0 #26 ) ( - #27 e ) ( * #9 #28 ) ( + #2 #29 ) G. Analysis of TP-N2F encoder For TP-N2F encoder, we extract the Softmax scores for \ufb01llers and roles of natural-language. We dropped the scores that are less than 0.1 to keep the signi\ufb01cant \ufb01llers and roles for each word.",
      "We dropped the scores that are less than 0.1 to keep the signi\ufb01cant \ufb01llers and roles for each word. After analyzing a subset of questions, we \ufb01nd that \ufb01llers tend to represent the semantic information and words or phrases with same meaning tend to be assigned the same \ufb01ller. Roles tend to represent the structured schemes of sentences. For example, in AlgoLisp dataset, \u201ddecrement\u201d, \u201ddifference of\u201d and \u201ddecremented by\u201d are assigned to \ufb01ller 43. \u201dincrement\u201d and \u201dadd\u201d are assigned to \ufb01ller 105. In MathQA dataset, \u201dpositive integer\u201d, \u201dpositive number\u201d and \u201dpositive digits\u201d are assigned to \ufb01ller 27. Figure 5 shows the visualization of \ufb01llers for four examples from AlgoLisp dataset. From the \ufb01gure, \u201dconsider\u201d and \u201dyou are given\u201d are assigned to the \ufb01ller 146. \u201dwhat is\u201d and \u201d\ufb01nd\u201d are assigned to \ufb01ller 120. Figure 6 presents the visualization of selected for the four examples.",
      "From the \ufb01gure, \u201dconsider\u201d and \u201dyou are given\u201d are assigned to the \ufb01ller 146. \u201dwhat is\u201d and \u201d\ufb01nd\u201d are assigned to \ufb01ller 120. Figure 6 presents the visualization of selected for the four examples. Role 12 indicates the target of the questions needs to be solved and Role 3 indicates the provided information to solve the questions. H. Analysis of TP-N2F decoder For TP-N2F decoder, we run K-means clustering on both datasets with k = 3, 4, 5, 6 clusters and the results are displayed in Figure 7 and Figure 8. As described before, unbinding-vectors for operators or functions with similar semantics tend to be closer to each other. For example, in the MathQA dataset, arithmetic operators such as add, subtract, multiply, divide are clustered together at middle, and operators related to geometry such as square or volume are clustered together at bottom left. In AlgoLisp dataset, basic arithmetic functions are clustered at middle, and string processing functions are clustered at right.",
      "Mapping Natural-language Problems to Formal-language Solutions Using Structured Neural Representations Figure 5. Visualizations of selected \ufb01llers for four examples",
      "Mapping Natural-language Problems to Formal-language Solutions Using Structured Neural Representations Figure 6. Visualizations of selected roles for four examples",
      "Mapping Natural-language Problems to Formal-language Solutions Using Structured Neural Representations Figure 7. MathQA clustering results Figure 8. AlgoLisp clustering results"
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1910.02339.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":16233,
  "avg_doclen":169.09375,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1910.02339.pdf"
    }
  }
}