{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "arXiv:1705.01265v2  [cs.CL]  30 Jul 2018 On the effectiveness of feature set augmentation using clusters of word embeddings Georgios Balikas Kelkoo Grenoble, France georgios.balikas@kelkoogroup.com Ioannis Partalas Expedia Geneva, Switzerland ipartalas@expedia.com Abstract Word clusters have been empirically shown to offer important performance improve- ments on various Natural Language Process- ing (NLP) tasks. Despite their importance, their incorporation in the standard pipeline of feature engineering relies more on a trial-and- error procedure where one evaluates several hyper-parameters, like the number of clus- ters to be used. In order to better understand the role of such features in NLP tasks we perform a systematic empirical evaluation on three tasks, that of named entity recognition, \ufb01ne grained sentiment classi\ufb01cation and \ufb01ne grained sentiment quanti\ufb01cation. 1 Introduction Many research attempts have proposed novel features that improve the performance of learning algorithms in particular tasks. Such features are often motivated by domain knowledge or manual labor.",
      "1 Introduction Many research attempts have proposed novel features that improve the performance of learning algorithms in particular tasks. Such features are often motivated by domain knowledge or manual labor. Although useful and often state-of-the-art, adapting such solu- tions on NLP systems across tasks can be tricky and time-consuming (Dey et al., 2016). Therefore, sim- ple yet general and powerful methods that perform well across several datasets are valuable (Turian et al., 2010). An approach that has become extremely popular lately in NLP tasks, is to train word embeddings in an unsupervised way. These embeddings are dense vectors that project words or short text spans like In: Mark Cieliebak, Don Tuggener and Fernando Benites (eds.): Proceedings of the 3rd Swiss Text Analytics Conference (Swiss- Text 2018), Winterthur, Switzerland, June 2018 phrases in a vector space where dimensions are sup- posed to capture text properties. Such embeddings can then be used either as features with off-the-shelf algorithms like Support Vector Machines, or to ini- tialize deep learning systems (Goldberg, 2015).",
      "Such embeddings can then be used either as features with off-the-shelf algorithms like Support Vector Machines, or to ini- tialize deep learning systems (Goldberg, 2015). How- ever, as shown in (Wang and Manning, 2013) linear architectures perform better in high-dimensional dis- crete spaces compared to continuous ones. The latter is probably the main reason of the high performance of the vector space model (Salton et al., 1975) in tasks like text classi\ufb01cation with linear models like SVMs. Using linear algorithms, while taking advantage of the expressiveness of text embeddings is the focus of this work. In this paper, we explore a hybrid approach, that uses text embeddings as a proxy to create features. Motivated by the argument that text embeddings man- age to encode the semantics of text, we explore how clustering text embeddings can impact the perfor- mance of different NLP tasks. Although such an ap- proach has been used in different studies during fea- ture engineering, the selection of word vectors and the number of clusters remain a trial-end-error procedure.",
      "Although such an ap- proach has been used in different studies during fea- ture engineering, the selection of word vectors and the number of clusters remain a trial-end-error procedure. In this work we present an empirical evaluation across diverse tasks to verify whether and when such features are useful. Word clusters have been used as features in various tasks like Part-of-Speech tagging and NER. Owoputi et al. (2013) use Brown clusters (Brown et al., 1992) in a POS tagger showing that this type of features carry rich lexical knowledge as they can substitute lexical resources like gazetteers. Kiritchenko et al. (2014) discusses their use on sentiment classi\ufb01cation while Hee et al. (2016) incorporate them in the task of irony detection in Twitter. Ritter et al. (2011) in- ject also word clusters in a NER tagger. While these works show that word clusters are bene\ufb01cial no clear guidelines can be concluded of how and when to use 1",
      "them. In this work, we empirically demonstrate that using different types of embeddings on three NLP tasks with twitter data we manage to achieve better or near to the state-of-the art performance on three NLP tasks: (i) Named Entity Recognition (NER) segmentation, (ii) NER classi\ufb01cation, (iii) \ufb01ne-grained sentiment analy- sis and (iv) \ufb01ne-grained sentiment quanti\ufb01cation. For each of the three tasks, we achieve higher perfor- mance than without using features which indicates the effectiveness of the cluster membership features. Im- portantly, our evaluation compared to previous work (Guo et al., 2014) who focus on old and well stud- ied datasets uses recent and challenging datasets com- posed by tweets. The obtained results across all the tasks permits us to reveal important aspects of the use of word clusters and therefore provide guidelines. Al- though our obtained scores are state-of-the-art, our analysis reveals that the performance in such tasks is far from perfect and, hence, identi\ufb01es that there is still much space for improvement and future work.",
      "Al- though our obtained scores are state-of-the-art, our analysis reveals that the performance in such tasks is far from perfect and, hence, identi\ufb01es that there is still much space for improvement and future work. 2 Word Clusters Word embeddings associate words with dense, low-dimensional vectors. Recently, several models have been proposed in order to obtain these embed- dings. Among others, the skipgram (skipgram) model with negative sampling (Mikolov et al., 2013), the continuous bag-of-words (cbow) model (Mikolov et al., 2013) and Glove (glove) (Pennington et al., 2014) have been shown to be effective. Training those models requires no an- notated data and can be done using big amounts of text. Such a model can be seen as a function f that projects a word w in a D-dimensional space: f(w) \u2208RD, where D is prede\ufb01ned. Here, we focus on applications using data from Twitter, which pose several dif\ufb01culties due to being particularly short, using creative vocabulary, abbreviations and slang.",
      "Here, we focus on applications using data from Twitter, which pose several dif\ufb01culties due to being particularly short, using creative vocabulary, abbreviations and slang. For all the tasks in our experimental study, we use 36 millions English tweets collected between August and September 2017. A pre-processing step has been applied to replace URLs with a placeholder and to pad punctuation. The \ufb01nal vocabulary size was around 1.6 millions words.1 Additionally to the in-domain cor- 1We trained the word embeddings using the implementations released from the authors of the papers. Unless differently stated we used the default parameters. pus we collected, we use GloVe vectors trained on Wikipedia articles in order to investigate the impact of out-of-domain word-vectors.2 We cluster the embeddings with k-Means. The k-means clusters are initialized using \u201ck-means++\u201d as proposed in (Arthur and Vassilvitskii, 2007), while the algorithm is run for 300 iterations. We try differ- ent values for k \u2208{100, 250, 500, 1000, 2000}.",
      "We try differ- ent values for k \u2208{100, 250, 500, 1000, 2000}. For each k, we repeat the clustering experiment with dif- ferent seed initialization for 10 times and we select the clustering result that minimizes the cluster inertia. 3 Experimental Evaluation We evaluate the proposed approach for augmenting the feature space in four tasks: (i) NER segmenta- tion, (ii) NER classi\ufb01cation, (iii) \ufb01ne-grained sen- timent classi\ufb01cation and (iv) \ufb01ne-grained sentiment quanti\ufb01cation. The next sections present the evalua- tion settings we used. For each of the tasks, we use the designated training sets to train the learning al- gorithms, and we report the scores of the evaluation measures used in the respective test parts. 3.1 Named-Entity Recognition in Twitter NER concerns the classi\ufb01cation of textual segments in a prede\ufb01ned set of categories, like persons, organi- zation and locations.",
      "3.1 Named-Entity Recognition in Twitter NER concerns the classi\ufb01cation of textual segments in a prede\ufb01ned set of categories, like persons, organi- zation and locations. We use the data of the last com- petition in NER for Twitter which released as a part of the 2nd Workshop on Noisy User-generated Text (Strauss et al., 2016). More speci\ufb01cally, the organiz- ers provided annotated tweets with 10 named-entity types (person, movie, sportsteam, product etc.) and the task comprised two sub-tasks: 1) the detection of entity bounds and 2) the classi\ufb01cation of an entity into one of the 10 types. The evaluation measure for both sub-tasks is the F1 measure. The following is an example of a tweet which con- tains two named entities.",
      "The evaluation measure for both sub-tasks is the F1 measure. The following is an example of a tweet which con- tains two named entities. Note that named entities may span several words in the text: F acility z }| { CLUB BLU tonite ... 90 \u2019s music .. oldskool night wiith dj \ufb01nese | {z } Musicartist 2The pre-trained vectors are obtained from http://nlp.stanford.edu/projects/glove/ 2",
      "Learning algorithm Our model for solving the task is a learning to search approach. More speci\ufb01cally we follow (Partalas et al., 2016) which has been ranked 2nd among 10 participants in the aforementioned com- petition (Strauss et al., 2016). The model uses handcrafted features like n-grams, part-of-speech tags, capitalization and membership in gazetteers. The algorithm used belongs to the family of learning to search for structured prediction tasks (Daum\u00b4e III et al., 2014). These methods decompose the problem in a search space with states, actions and policies and then learn a hypothesis controlling a pol- icy over the state-action space. The BIO encoding is used for attributing the corresponding labels to the to- kens where B-type is used for the \ufb01rst token of the entity, I-type for inside tokens in case of multi-term entities and O for non entity tokens. Results Tables 1 and 2 present the results for the different number of clusters across the three vector models used to induce the clusters.",
      "Results Tables 1 and 2 present the results for the different number of clusters across the three vector models used to induce the clusters. For all the experiments we keep the same parametrization for the learning algorithm and we present the performance of each run on the of\ufb01cial test set. Regarding the segmentation task we notice that adding word clusters as features improve the perfor- mance of the best model up to 1.1 F-score points while it boosts performance in the majority of cases. In only one case, for glove40 vectors, there is a drop across all number of clusters used. As for the number of clusters, the best results are generally obtained between 250 and 1000 classes for all word vector models. These dimensions seem to be suf\ufb01cient for the three-class sub-task that we deal with. The different models of word vectors perform similarly and thus one cannot privilege a certain type of word vectors. Interestingly, the clusters learned on the Wikipedia GloVe vectors offer competitive per- formance with respect to the in-domain word vectors used for the other cases showing that one can rely to out-of-domain data for constructing such representa- tions.",
      "Interestingly, the clusters learned on the Wikipedia GloVe vectors offer competitive per- formance with respect to the in-domain word vectors used for the other cases showing that one can rely to out-of-domain data for constructing such representa- tions. Concerning the classi\ufb01cation task (Table 2) we generally observe a drop in the performance of the tagger as we deal with 10 classes. This essentially corresponds to a multi-class problem with 21 classes: 100 250 500 1000 2000 No clusters 55.29 skipgram40,w5 55.21 54.42 55.4 55.78 55.40 skipgram100,w5 54.94 55.32 54.02 54.51 53.79 skipgram100,w10 55.04 55.37 55.93 55.00 54.77 cbow40,w5 55.97 55.22 55.89 55.61 54.64 cbow100,w5 55.76 56.36 55.03 55.00 55.60 cbow100,w10 55.",
      "77 cbow40,w5 55.97 55.22 55.89 55.61 54.64 cbow100,w5 55.76 56.36 55.03 55.00 55.60 cbow100,w10 55.72 55.55 56.08 56.19 55.46 glove40,w5 53.73 53.93 53.96 54.58 55.07 glove100,w5 53.91 55.09 53.11 54.69 55.81 glove100,w10 55.50 55.84 55.33 53.73 55.15 glove50,wiki 55.56 55.50 55.92 55.87 53.96 glove100,wiki 55.54 56.38 55.68 54.74 54.91 Table 1: Scores on F1-measure for named entities seg- mentation for the different word embeddings across different number of clusters. For each embedding type, we show its dimension and window size.",
      "wiki 55.54 56.38 55.68 54.74 54.91 Table 1: Scores on F1-measure for named entities seg- mentation for the different word embeddings across different number of clusters. For each embedding type, we show its dimension and window size. For instance, glove40,w5 is 40-dimensional glove embed- dings with window size 5. 100 250 500 1000 2000 No clusters 40.10 skipgram40,w5 40.07 39.29 39.98 40.15 40.70 skipgram100,w5 40.03 39.63 39.42 40.25 40.06 skipgram100,w10 41.19 39.92 40.04 40.29 38.54 cbow40,w5 40.20 40.63 39.71 40.00 39.32 cbow100,w5 40.53 40.21 39.22 40.61 40.15 cbow100,w10 40.02 40.80 39.55 40.",
      "20 40.63 39.71 40.00 39.32 cbow100,w5 40.53 40.21 39.22 40.61 40.15 cbow100,w10 40.02 40.80 39.55 40.40 39.71 glove40,w5 39.09 40.06 38.73 39.35 38.23 glove100,w5 39.63 39.40 39.23 39.20 41.42 glove100,w10 39.55 39.98 39.99 39.53 40.67 glove50,wiki 40.40 41.09 40.82 40.84 41.24 glove100,wiki 39.52 39.78 41.04 41.73 40.39 Table 2: Results in terms of F1-score for named enti- ties classi\ufb01cation for the different word clusters across different number of clusters. one for the non-entity type and two classes for each entity type.",
      "78 41.04 41.73 40.39 Table 2: Results in terms of F1-score for named enti- ties classi\ufb01cation for the different word clusters across different number of clusters. one for the non-entity type and two classes for each entity type. In this setting we notice that the best re- sults are obtained in most cases for higher number of classes (1000 or 2000) possibly due to a better dis- criminatory power in higher dimensions. Note also, that in some cases the addition of word cluster fea- tures does not necessarily improve the performance. Contrary, it may degrade it as it is evident in the case of glove40,w5 word clusters. Like in the case of seg- mentation we do not observe a word vector model that clearly outperforms the rest. Finally, we note the same competitive performance of the Wikipedia word clus- ters and notably for the glove100,wiki clusters which obtain the best F1-score.",
      "Like in the case of seg- mentation we do not observe a word vector model that clearly outperforms the rest. Finally, we note the same competitive performance of the Wikipedia word clus- ters and notably for the glove100,wiki clusters which obtain the best F1-score. 3.2 Fine-grained Sentiment Analysis The task of \ufb01ne grained sentiment classi\ufb01cation consists in predicting the sentiment of an input text according to a \ufb01ve point scale (sentiment \u2208 3",
      "{VeryNegative, Negative, Neutral, Positive, VeryPositive}). We use the set- ting of task 4 of SemEval2016 \u201cSentiment Analysis in Twitter\u201d and the dataset released by the organizers for subtask 4 (Nakov et al., 2016). In total, the training (resp. test) data consist of 9,070 (resp. 20,632) tweets. The evaluation measure selected in (Nakov et al., 2016) for the task in the macro-averaged Mean Abso- lute Error (MAEM). It is a measure of error, hence lower values are better. The measure\u2019s goal is to take into account the order of the classes when penalizing the decision of a classi\ufb01er. For instance, misclassify- ing a very negative example as very positive is a big- ger mistake than classifying it as negative or neutral. Penalizing a classi\ufb01er according to how far the pre- dictions are from the true class is captured by MAEM (Baccianella et al., 2009).",
      "Penalizing a classi\ufb01er according to how far the pre- dictions are from the true class is captured by MAEM (Baccianella et al., 2009). Also, the advantage of us- ing the macro- version instead of the standard version of the measure is the robustness against the class im- balance in the data. Learning algorithm To demonstrate the ef\ufb01ciency of cluster membership features we rely on the system of (Balikas and Amini, 2016) which was ranked 1st among 11 participants and uses a Logistic Regression as a learning algorithm. We follow the same feature extraction steps which consist of extracting n-gram and character n-gram features, part-of-speech counts as well as sentiment scores using standard sentiment lexicons such as the Bing Liu\u2019s (Hu and Liu, 2004) and the MPQA lexicons (Wilson et al., 2005). For the full description, we refer the interested reader to (Balikas and Amini, 2016).",
      "For the full description, we refer the interested reader to (Balikas and Amini, 2016). Results To evaluate the performance of the proposed feature augmentation technique, we present in Table 3 the macro-averaged Mean Absolute Error scores for dif- ferent settings on the of\ufb01cial test set of (Nakov et al., 2016). First, notice that the best score in the test data is achieved using cluster membership features, where the word embeddings are trained using the skipgram model. The achieved score improves the state-of-the art on the dataset, which to the best of our knowledge was by (Balikas and Amini, 2016). Also, note that the score on the test data improves for each type of em- beddings used, which means that augmenting the fea- ture space using cluster membership features helps the clusters 100 250 500 1000 2000 no clusters 0.721 skipgram40,w5 0.712 0.691 0.699 0.665 0.720 skipgram100,w5 0.69 0.701 0.684 0.706 0.",
      "721 skipgram40,w5 0.712 0.691 0.699 0.665 0.720 skipgram100,w5 0.69 0.701 0.684 0.706 0.689 skipgram100,w10 0.715 0.722 0.700 0.711 0.681 cbow40,w5 0.705 0.707 0.690 0.700 0.675 cbow100,w5 0.686 0.699 0.713 0.683 0.697 cbow100,w10 0.712 0.690 0.707 0.676 0.703 glove40,w5 0.680 0.689 0.679 0.702 0.706 glove100,w5 0.687 0.670 0.668 0.678 0.693 glove100,w10 0.696 0.689 0.693 0.687 0.703 glove50,wiki 0.712 0.678 0.710 0.711 0.",
      "670 0.668 0.678 0.693 glove100,w10 0.696 0.689 0.693 0.687 0.703 glove50,wiki 0.712 0.678 0.710 0.711 0.694 glove100,wiki 0.699 0.701 0.699 0.689 0.714 Table 3: MAEM scores (lower is better) for sentiment classi\ufb01cation across different types of word embed- dings and number of clusters. sentiment classi\ufb01cation task. Note, also, that using the clusters produced by the out-of-domain embeddings trained on wikipedia that were released as part of (Pennington et al., 2014) per- forms surprisingly well. One might have expected their addition to hurt the performance. However, their value probably stems from the sheer amount of data used for their training as well as the relatively sim- ple type of words (like awesome, terrible) which are discriminative for this task.",
      "One might have expected their addition to hurt the performance. However, their value probably stems from the sheer amount of data used for their training as well as the relatively sim- ple type of words (like awesome, terrible) which are discriminative for this task. Lastly, note that in each of the settings, the best results are achieved when the number of clusters is within {500, 1000, 2000} as in the NER tasks. Comparing the performance across the different embeddings, one cannot claim that a particu- lar embedding performs better. It is evident though that augmenting the feature space with feature de- rived using the proposed method, preferably with in- domain data, helps the classi\ufb01cation performance and reduces MAEM. From the results of Table 3 it is clear that the addi- tion of the cluster membership features improves the sentiment classi\ufb01cation performance. To better un- derstand though why these clusters help, we manu- ally examined a sample of the words associated with the clusters. To improve the eligibility of those re- sults we \ufb01rst removed the hashtags and we \ufb01lter the results using an English vocabulary.",
      "To better un- derstand though why these clusters help, we manu- ally examined a sample of the words associated with the clusters. To improve the eligibility of those re- sults we \ufb01rst removed the hashtags and we \ufb01lter the results using an English vocabulary. In Table 4 we present sample words from two of the most charac- teristic clusters with respect to the task of sentiment classi\ufb01cation. Notice how words with positive and negative meanings are put in the respective clusters. 3.3 Fine-Grained Sentiment Quanti\ufb01cation Quanti\ufb01cation is the problem of estimating the preva- lence of a class in a dataset. While classi\ufb01cation con- 4",
      "Positive hotness, melodious, congratulation, wishers, appreciable, festivity, superb, amazing, awesomeness, awaited, wishes, joyous, phenomenal, anniversaries, memorable, heartiest, excitement, birthdays, unforgettable, gorgeousness, Negative jerk, damnit, carelessly, exaggerates, egoistic, panicking, fault, problematic, harasses, insufferable, terrible, bad, psycho, stupidest, screwed, regret, messed, messes, rest, pisses, pissed, saddest, intimidating, unpleasant, ditched, negative Table 4: Sample from two clusters that were found useful for the sentiment classi\ufb01cation. Words with positive or negative meaning are grouped together. cerns assigning a category to a single instance, like la- beling a tweet with the sentiment it conveys, the goal of quanti\ufb01cation is, given a set of instances, to esti- mate the relative frequency of single class. Therefore, sentiment quanti\ufb01cation tries to answer questions like \u201cGiven a set of tweets about the new iPhone, what is the fraction of VeryPositive ones?\u201d.",
      "Therefore, sentiment quanti\ufb01cation tries to answer questions like \u201cGiven a set of tweets about the new iPhone, what is the fraction of VeryPositive ones?\u201d. In the rest, we show the effect of the features derived from the word embeddings clusters in the \ufb01ne-grained classi\ufb01cation problem, which was also part of the SemEval-2016 \u201cSentiment Analysis in Twitter\u201d task (Nakov et al., 2016). Learning Algorithm To perform the quanti\ufb01cation task, we rely on a classify and count approach, which was shown effective in a related binary quanti\ufb01cation problem (Balikas and Amini, 2016). The idea is that given a set of instances on a particular subject, one \ufb01rst classi\ufb01es the instances and then aggregates the counts. To this end, we use the same feature repre- sentation steps and data with the ones used for \ufb01ne grained classi\ufb01cation (Section 3.2).",
      "To this end, we use the same feature repre- sentation steps and data with the ones used for \ufb01ne grained classi\ufb01cation (Section 3.2). Note that the data of the task are associated with subjects (described in full detail at (Nakov et al., 2016)), and, hence, quan- ti\ufb01cation is performed for the tweets of a subject. For each of the \ufb01ve categories, the output of the approach is a 5-dimensional vector with the estimated preva- lence of the categories. The evaluation measure for the problem is the Earth Movers Distance (EMD) (Rubner et al., 2000). EMD is a measure of error, hence lower values are better. It assumes ordered categories, which in our problem is naturally de\ufb01ned. Further assuming that the distance of consecutive categories (e.g., Positive and VeryPos- itive) is 1, the measure is calculated by: EMD(p,",
      "It assumes ordered categories, which in our problem is naturally de\ufb01ned. Further assuming that the distance of consecutive categories (e.g., Positive and VeryPos- itive) is 1, the measure is calculated by: EMD(p, \u02c6p) = |C|\u22121 X j=1 | j X i=1 \u02c6p(ci) \u2212 j X i=1 p(ci)| where |C| is number of categories (\ufb01ve in our case) clusters 100 250 500 1000 2000 no clusters 0.228 [0.243] skipgram40,w5 0.223 0.223 0.221 0.221 0.222 skipgram100,w5 0.223 0.224 0.221 0.223 0.222 skipgram100,w10 0.225 0.223 0.220 0.221 0.222 cbow40,w5 0.227 0.221 0.228 0.222 0.223 cbow100,w5 0.225 0.225 0.227 0.",
      "225 0.223 0.220 0.221 0.222 cbow40,w5 0.227 0.221 0.228 0.222 0.223 cbow100,w5 0.225 0.225 0.227 0.224 0.225 cbow100,w10 0.226 0.226 0.231 0.224 0.224 glove40,w5 0.222 0.222 0.220 0.221 0.222 glove100,w5 0.223 0.221 0.221 0.220 0.221 glove100,w10 0.220 0.222 0.224 0.224 0.223 glove50,wiki 0.222 0.221 0.219 0.220 0.221 glove100,wiki 0.221 0.221 0.222 0.220 0.222 Table 5: Earth Movers Distance for \ufb01ne-grained senti- ment quanti\ufb01cation across different types of word em- beddings and number of clusters.",
      "221 glove100,wiki 0.221 0.221 0.222 0.220 0.222 Table 5: Earth Movers Distance for \ufb01ne-grained senti- ment quanti\ufb01cation across different types of word em- beddings and number of clusters. The score in brack- ets denotes the best performance achieved in the chal- lenge. and \u02c6p(ci) and p(ci) are the true and predicted preva- lence respectively (Esuli and Sebastiani, 2010). Results Table 5 presents the results of augmenting the feature set with the proposed features. We use Logistic Regression as a base classi\ufb01er for the clas- sify and count approach. Notice the positive im- pact of the features in the performance in the task. Adding the features derived from clustering the em- beddings consistently improves the performance. In- terestingly, the best performance (0.219) is achieved using the out-of-domain vectors, as in the NER classi- \ufb01cation task.",
      "Adding the features derived from clustering the em- beddings consistently improves the performance. In- terestingly, the best performance (0.219) is achieved using the out-of-domain vectors, as in the NER classi- \ufb01cation task. Also, notice how the approach improves over the state-of-the-art performance in the challenge (0.243) (Nakov et al., 2016), held by the method of (Martino et al., 2016). The improvement over the method of (Martino et al., 2016) however, does not necessarily mean that classify and count performs bet- ter in the task. It implies that the feature set we used is richer, that in turn highlights the value of robust fea- ture extraction mechanisms which is the subject of this paper. 4 Conclusion We have shown empirically the effectiveness of in- corporating cluster membership features in the feature extraction pipeline of Named-Entity recognition, sen- timent classi\ufb01cation and quanti\ufb01cation tasks. Our re- sults strongly suggest that incorporating cluster mem- bership features bene\ufb01t the performance in the tasks.",
      "Our re- sults strongly suggest that incorporating cluster mem- bership features bene\ufb01t the performance in the tasks. The fact that the performance improvements are con- sistent in the four tasks we investigated, further high- lights their usefulness, both for practitioners and re- searchers. 5",
      "Although our study does not identify a clear win- ner with respect to the type of word vectors (skip- gram, cbow, or GloVe), our \ufb01ndings suggest that one should \ufb01rst try skip-gram embeddings of low dimen- sionality (D = 40) and high number of clusters (e.g., K \u2208{500, 1000, 2000}) as the results obtained using these settings are consistently competitive. Our re- sults also suggest that using out-of-domain data, like Wikipedia articles in this case, to construct the word embeddings is a good practice, as the results we ob- tained with these vectors are also competitive. The positive of out-of-domain embeddings and their com- bination with in-domain ones remains to be further studied. References David Arthur and Sergei Vassilvitskii. 2007. k-means++: The advantages of careful seeding. In ACM- SIAM@Discrete algorithms. pages 1027\u20131035. Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas- tiani. 2009. Evaluation measures for ordinal regression.",
      "In ACM- SIAM@Discrete algorithms. pages 1027\u20131035. Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas- tiani. 2009. Evaluation measures for ordinal regression. In International Conference on Intelligent Systems De- sign and Applications. pages 283\u2013287. Georgios Balikas and Massih-Reza Amini. 2016. Twise at semeval-2016 task 4: Twitter sentiment classi\ufb01ca- tion. In Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2016, San Diego, CA, USA, June 16-17, 2016. pages 85\u201391. Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin- cent J. Della Pietra, and Jenifer C. Lai. 1992. Class- based n-gram models of natural language. Computa- tional Linguistics 18:467\u2013479. Hal Daum\u00b4e III, John Langford, and St\u00b4ephane Ross. 2014.",
      "1992. Class- based n-gram models of natural language. Computa- tional Linguistics 18:467\u2013479. Hal Daum\u00b4e III, John Langford, and St\u00b4ephane Ross. 2014. Ef\ufb01cient programmable learning to search. CoRR abs/1406.1837. Kuntal Dey, Ritvik Shrivastava, and Saroj Kaushik. 2016. A paraphrase and semantic similarity detection system for user generated short-text content on microblogs. In COLING. pages 2880\u20132890. Andrea Esuli and Fabrizio Sebastiani. 2010. Sentiment quanti\ufb01cation. IEEE Intelligent Systems 25(4):72\u201375. Yoav Goldberg.2015. A primer on neural network models for natural language processing. CoRR abs/1510.00726. http://arxiv.org/abs/1510.00726. Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Revisiting embedding features for simple semi- supervised learning.",
      "CoRR abs/1510.00726. http://arxiv.org/abs/1510.00726. Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Revisiting embedding features for simple semi- supervised learning. In EMNLP. pages 110\u2013120. Cynthia Van Hee, Els Lefever, and V\u00b4eronique Hoste. 2016. Monday mornings are my fave : ) #not exploring the au- tomatic recognition of irony in english tweets. In COL- ING. pages 2730\u20132739. Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In SIGKDD. pages 168\u2013177. https://doi.org/10.1145/1014052.1014073. Svetlana Kiritchenko, Xiaodan Zhu, and Saif M. Moham- mad. 2014. Sentiment analysis of short informal texts. JAIR 50:723\u2013762.",
      "Svetlana Kiritchenko, Xiaodan Zhu, and Saif M. Moham- mad. 2014. Sentiment analysis of short informal texts. JAIR 50:723\u2013762. Giovanni Da San Martino, Wei Gao, and Fabrizio Sebas- tiani. 2016. Ordinal text quanti\ufb01cation. In SIGIR. pages 937\u2013940. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Ef\ufb01cient estimation of word representa- tions in vector space. CoRR abs/1301.3781. Preslav Nakov, Alan Ritter, Sara Rosenthal, Fab- rizio Sebastiani, and Veselin Stoyanov. 2016. Semeval-2016 task 4: Sentiment analysis in twitter. In SemEval@NAACL-HLT 2016. pages 1\u201318. http://aclweb.org/anthology/S/S16/S16-1001.pdf.",
      "2016. Semeval-2016 task 4: Sentiment analysis in twitter. In SemEval@NAACL-HLT 2016. pages 1\u201318. http://aclweb.org/anthology/S/S16/S16-1001.pdf. Olutobi Owoputi, Brendan O\u2019Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved part-of-speech tagging for online conversa- tional text with word clusters. In NAACL. pages 380\u2013 390. Ioannis Partalas, C\u00b4edric Lopez, Nadia Derbas, and Ruslan Kalitvianski. 2016. Learning to search for recognizing named entities in twitter. In W-NUT, Coling. Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word rep- resentation. In EMNLP. pages 1532\u20131543. Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011.",
      "2014. Glove: Global vectors for word rep- resentation. In EMNLP. pages 1532\u20131543. Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named entity recognition in tweets: An experimental study. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. EMNLP \u201911, pages 1524\u20131534. Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. 2000. The earth mover\u2019s distance as a metric for im- age retrieval. International journal of computer vision 40(2):99\u2013121. Gerard Salton, A. Wong, and C. S. Yang. 1975. A vector space model for automatic indexing. Commun. ACM 18(11):613\u2013620. https://doi.org/10.1145/361219.361220. Benjamin Strauss, Bethany E. Toma, Alan Ritter, Marie Catherine de Marneffe, and Wei Xu. 2016.",
      "Commun. ACM 18(11):613\u2013620. https://doi.org/10.1145/361219.361220. Benjamin Strauss, Bethany E. Toma, Alan Ritter, Marie Catherine de Marneffe, and Wei Xu. 2016. Re- sults of the wnut16 named entity recognition shared task. In WNUT@COLING. Osaka, Japan. 6",
      "Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In ACL. pages 384\u2013394. Mengqiu Wang and Christopher D Manning. 2013. Effect of non-linear deep architecture in sequence labeling. In IJCNLP. pages 1285\u20131291. Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level senti- ment analysis. In EMNLP. 7"
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1705.01265.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":7214,
  "avg_doclen":171.7619047619,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1705.01265.pdf"
    }
  }
}