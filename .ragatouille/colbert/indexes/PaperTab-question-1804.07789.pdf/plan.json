{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Generating Descriptions from Structured Data Using a Bifocal Attention Mechanism and Gated Orthogonalization Preksha Nema\u2020\u2217Shreyas Shetty\u2020\u2217Parag Jain\u2022\u2217 Anirban Laha\u2022\u2020 Karthik Sankaranarayanan\u2022 Mitesh M. Khapra\u2020\u2021 \u2020IIT Madras, India \u2022IBM Research \u2021 Robert Bosch Center for Data Science and Arti\ufb01cial Intelligence, IIT Madras {preksha,shshett,miteshk}@cse.iitm.ac.in {pajain34,anirlaha,kartsank}@in.ibm.com Abstract In this work, we focus on the task of generat- ing natural language descriptions from a struc- tured table of facts containing \ufb01elds (such as nationality, occupation, etc) and values (such as Indian, {actor, director}, etc). One sim- ple choice is to treat the table as a sequence of \ufb01elds and values and then use a standard seq2seq model for this task. However, such a model is too generic and does not exploit task- speci\ufb01c characteristics.",
            "One sim- ple choice is to treat the table as a sequence of \ufb01elds and values and then use a standard seq2seq model for this task. However, such a model is too generic and does not exploit task- speci\ufb01c characteristics. For example, while generating descriptions from a table, a hu- man would attend to information at two levels: (i) the \ufb01elds (macro level) and (ii) the values within the \ufb01eld (micro level). Further, a human would continue attending to a \ufb01eld for a few timesteps till all the information from that \ufb01eld has been rendered and then never return back to this \ufb01eld (because there is nothing left to say about it). To capture this behavior we use (i) a fused bifocal attention mechanism which ex- ploits and combines this micro and macro level information and (ii) a gated orthogonalization mechanism which tries to ensure that a \ufb01eld is remembered for a few time steps and then forgotten. We experiment with a recently re- leased dataset which contains fact tables about people and their corresponding one line bi- ographical descriptions in English.",
            "We experiment with a recently re- leased dataset which contains fact tables about people and their corresponding one line bi- ographical descriptions in English. In addi- tion, we also introduce two similar datasets for French and German. Our experiments show that the proposed model gives 21% relative im- provement over a recently proposed state of the art method and 10% relative improvement over basic seq2seq models. The code and the datasets developed as a part of this work are publicly available. 1 \u2217* The \ufb01rst three authors have contributed equally to this work. 1https:\/\/github.com\/PrekshaNema25\/ StructuredData_To_Descriptions 1 Introduction Rendering natural language descriptions from structured data is required in a wide variety of commercial applications such as generating de- scriptions of products, hotels, furniture, etc., from a corresponding table of facts about the entity. Such a table typically contains {\ufb01eld, value} pairs where the \ufb01eld is a property of the entity (e.g., color) and the value is a set of possible assign- ments to this property (e.g., color = red).",
            "Such a table typically contains {\ufb01eld, value} pairs where the \ufb01eld is a property of the entity (e.g., color) and the value is a set of possible assign- ments to this property (e.g., color = red). Another example of this is the recently introduced task of generating one line biography descriptions from a given Wikipedia infobox (Lebret et al., 2016). The Wikipedia infobox serves as a table of facts about a person and the \ufb01rst sentence from the cor- responding article serves as a one line descrip- tion of the person. Figure 1 illustrates an exam- ple input infobox which contains \ufb01elds such as Born, Residence, Nationality, Fields, Institutions and Alma Mater. Each \ufb01eld further contains some words (e.g., particle physics, many-body theory, etc.). The corresponding description is coherent with the information contained in the infobox. Note that the number of \ufb01elds in the infobox and the ordering of the \ufb01elds within the infobox varies from person to person.",
            "The corresponding description is coherent with the information contained in the infobox. Note that the number of \ufb01elds in the infobox and the ordering of the \ufb01elds within the infobox varies from person to person. Given the large size (700K examples) and heterogeneous nature of the dataset which contains biographies of people from different backgrounds (sports, politics, arts, etc.), it is hard to come up with simple rule-based tem- plates for generating natural language descriptions from infoboxes, thereby making a case for data- driven models. Based on the recent success of data-driven neural models for various other NLG tasks (Bahdanau et al., 2014; Rush et al., 2015; Yao et al., 2015; Chopra et al., 2016; Nema et al., 2017), one simple choice is to treat the infobox as arXiv:1804.07789v1  [cs.CL]  20 Apr 2018",
            "Figure 1: Sample Infobox with description : V. Balakrishnan (born 1943 as Venkataraman Bal- akrishnan) is an Indian theoretical physicist who has worked in a number of \ufb01elds of areas, includ- ing particle physics, many-body theory, the me- chanical behavior of solids, dynamical systems, stochastic processes, and quantum dynamics. a sequence of {\ufb01eld, value} pairs and use a stan- dard seq2seq model for this task. However, such a model is too generic and does not exploit the spe- ci\ufb01c characteristics of this task as explained below. First, note that while generating such descrip- tions from structured data, a human keeps track of information at two levels. Speci\ufb01cally, at a macro level, she would \ufb01rst decide which \ufb01eld to men- tion next and then at a micro level decide which of the values in the \ufb01eld needs to be mentioned next.",
            "Speci\ufb01cally, at a macro level, she would \ufb01rst decide which \ufb01eld to men- tion next and then at a micro level decide which of the values in the \ufb01eld needs to be mentioned next. For example, she \ufb01rst decides that at the current step, the \ufb01eld occupation needs attention and then decides which is the next appropriate occupation to attend to from the set of occupations (actor, di- rector, producer, etc.). To enable this, we use a bifocal attention mechanism which computes an attention over \ufb01elds at a macro level and over val- ues at a micro level. We then fuse these atten- tion weights such that the attention weight for a \ufb01eld also in\ufb02uences the attention over the values within it. Finally, we feed a fused context vector to the decoder which contains both \ufb01eld level and word level information.",
            "We then fuse these atten- tion weights such that the attention weight for a \ufb01eld also in\ufb02uences the attention over the values within it. Finally, we feed a fused context vector to the decoder which contains both \ufb01eld level and word level information. Note that such two-level attention mechanisms (Nallapati et al., 2016; Yang et al., 2016; Serban et al., 2016) have been used in the context of unstructured data (as opposed to structured data in our case), where at a macro level one needs to pay attention to sentences and at a micro level to words in the sentences. Next, we observe that while rendering the out- put, once the model pays attention to a \ufb01eld (say, occupation) it needs to stay on this \ufb01eld for a few timesteps (till all the occupations are produced in the output). We refer to this as the stay on be- havior. Further, we note that once the tokens of a \ufb01eld are referred to, they are usually not referred to later.",
            "We refer to this as the stay on be- havior. Further, we note that once the tokens of a \ufb01eld are referred to, they are usually not referred to later. For example, once all the occupations have been listed in the output we will never visit the oc- cupation \ufb01eld again because there is nothing left to say about it. We refer to this as the never look back behavior. To model the stay on behaviour, we in- troduce a forget (or remember) gate which acts as a signal to decide when to forget the current \ufb01eld (or equivalently to decide till when to remember the current \ufb01eld). To model the never look back behaviour we introduce a gated orthogonalization mechanism which ensures that once a \ufb01eld is for- gotten, subsequent \ufb01eld context vectors fed to the decoder are orthogonal to (or different from) the previous \ufb01eld context vectors. We experiment with the WIKIBIO dataset (Le- bret et al., 2016) which contains around 700K {infobox, description} pairs and has a vocabu- lary of around 400K words.",
            "We experiment with the WIKIBIO dataset (Le- bret et al., 2016) which contains around 700K {infobox, description} pairs and has a vocabu- lary of around 400K words. We show that the proposed model gives a relative improvement of 21% and 20% as compared to current state of the art models (Lebret et al., 2016; Mei et al., 2016) on this dataset. The proposed model also gives a relative improvement of 10% as compared to the basic seq2seq model. Further, we introduce new datasets for French and German on the same lines as the English WIKIBIO dataset. Even on these two datasets, our model outperforms the state of the art methods mentioned above. 2 Related work Natural Language Generation has always been of interest to the research community and has re- ceived a lot of attention in the past.",
            "Even on these two datasets, our model outperforms the state of the art methods mentioned above. 2 Related work Natural Language Generation has always been of interest to the research community and has re- ceived a lot of attention in the past. The ap- proaches for NLG range from (i) rule based ap- proaches (e.g., (Dale et al., 2003; Reiter et al., 2005; Green, 2006; Galanis and Androutsopou- los, 2007; Turner et al., 2010)) (ii) modular sta- tistical approaches which divide the process into three phases (planning, selection and surface real- ization) and use data driven approaches for one or more of these phases (Barzilay and Lapata, 2005; Belz, 2008; Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013) (iii) hybrid ap- proaches which rely on a combination of hand- crafted rules and corpus statistics (Langkilde and Knight, 1998; Soricut and Marcu, 2006; Mairesse and Walker, 2011) and (iv) the more recent neural network based models (Bahdanau et al., 2014).",
            "Neural models for NLG have been proposed in the context of various tasks such as machine trans- lation (Bahdanau et al., 2014), document summa- rization (Rush et al., 2015; Chopra et al., 2016), paraphrase generation (Prakash et al., 2016), im- age captioning (Xu et al., 2015), video summa- rization (Venugopalan et al., 2014), query based document summarization (Nema et al., 2017) and so on. Most of these models are data hungry and are trained on large amounts of data. On the other hand, NLG from structured data has largely been studied in the context of small datasets such as WEATHERGOV (Liang et al., 2009), ROBOCUP (Chen and Mooney, 2008), NFL RECAPS (Barzi- lay and Lapata, 2005), PRODIGY-METEO (Belz and Kow, 2009) and TUNA Challenge (Gatt and Belz, 2010). Recently Mei et al.",
            "Recently Mei et al. (2016) pro- posed RNN\/LSTM based neural encoder-decoder models with attention for WEATHERGOV and ROBOCUP datasets. Unlike the datasets mentioned above, the biog- raphy dataset introduced by Lebret et al. (2016) is larger (700K {table, descriptions} pairs) and has a much larger vocabulary (400K words as op- posed to around 350 or fewer words in the above datasets). Further, unlike the feed-forward neural network based model proposed by (Lebret et al., 2016) we use a sequence to sequence model and introduce components to address the peculiar char- acteristics of the task. Speci\ufb01cally, we introduce neural components to address the need for atten- tion at two levels and to address the stay on and never look back behaviour required by the de- coder. Kiddon et al. (2016) have explored the use of checklists to track previously visited in- gredients while generating recipes from ingredi- ents.",
            "Kiddon et al. (2016) have explored the use of checklists to track previously visited in- gredients while generating recipes from ingredi- ents. Note that two-level attention mechanisms have also been used in the context of summariza- tion (Nallapati et al., 2016), document classi\ufb01ca- tion (Yang et al., 2016), dialog systems (Serban et al., 2016), etc. However, these works deal with unstructured data (sentences at the higher level and words at a lower level) as opposed to structured data in our case. 3 Proposed model As input we are given an infobox I = {(gi, ki)}M i=1, which is a set of pairs (gi, ki) where gi corresponds to \ufb01eld names and ki is the se- quence of corresponding values and M is the to- tal number of \ufb01elds in I. For example, (g = occupation, k = actor, writer, director) could be one such pair in this set. Given such an in- put, the task is to generate a description y = y1, y2, . . .",
            "Given such an in- put, the task is to generate a description y = y1, y2, . . . , ym containing m words. A simple so- lution is to treat the infobox as a sequence of \ufb01elds followed by the values corresponding to the \ufb01eld in the order of their appearance in the infobox. For example, the infobox could be \ufb02attened to produce the following input sequence (the words in bold are \ufb01eld names which act as delimiters) [Name] John Doe [Birth Date] 19 March 1981 [Nationality] Indian ..... The problem can then be cast as a seq2seq gen- eration problem and can be modeled using a stan- dard neural architecture comprising of three com- ponents (i) an input encoder (using GRU\/LSTM cells), (ii) an attention mechanism to attend to im- portant values in the input sequence at each time step and (iii) a decoder to decode the output one word at a time (again, using GRU\/LSTM cells). However, this standard model is too generic and does not exploit the speci\ufb01c characteristics of this task.",
            "However, this standard model is too generic and does not exploit the speci\ufb01c characteristics of this task. We propose additional components, viz., (i) a fused bifocal attention mechanism which oper- ates on \ufb01elds (macro) and values (micro) and (ii) a gated orthogonalization mechanism to model stay on and never look back behavior. 3.1 Fused Bifocal Attention Mechanism Intuitively, when a human writes a description from a table she keeps track of information at two levels. At the macro level, it is important to decide which is the appropriate \ufb01eld to attend to next and at a micro level (i.e., within a \ufb01eld) it is important to know which values to attend to next. To capture this behavior, we use a bifocal attention mecha- nism as described below. Macro Attention: Consider the i-th \ufb01eld gi which has values ki = (w1, w2, ..., wp). Let hg i be the representation of this \ufb01eld in the infobox.",
            "Macro Attention: Consider the i-th \ufb01eld gi which has values ki = (w1, w2, ..., wp). Let hg i be the representation of this \ufb01eld in the infobox. This representation can either be (i) the word embed- ding of the \ufb01eld name or (ii) some function f of the values in the \ufb01eld or (iii) a concatenation of (i) and (ii). The function f could simply be the sum or average of the embeddings of the values in the \ufb01eld. Alternately, this function could be a GRU (or LSTM) which treats these values within a \ufb01eld as a sequence and computes the \ufb01eld rep- resentation as the \ufb01nal representation of this se- quence (i.e., the representation of the last time- step). We found that bidirectional GRU is a bet-",
            "Figure 2: Proposed model ter choice for f and concatenating the embedding of the \ufb01eld name with this GRU representation works best. Further, using a bidirectional GRU cell to take contextual information from neighbor- ing \ufb01elds also helps (these are the orange colored cells in the top-left block in Figure 2 with macro attention). Given these representations {hg i }M i=1 for all the M \ufb01elds we compute an attention over the \ufb01elds (macro level). bg t,i = vT g tanh(Ugst\u22121 + Vghg i ) \u03b2t,i = exp(bg t,i) PM l=1 exp(bg t,l) cg t = M X i=1 \u03b2t,ihg i (1) where st\u22121 is the state of the decoder at time step t \u22121. Ug, Vg and vg are parameters, M is the total number of \ufb01elds in the input, cg t is the macro (\ufb01eld level) context vector at the t-th time step of the decoder. Micro Attention: Let hw j be the representation of the j-th value in a given \ufb01eld.",
            "Micro Attention: Let hw j be the representation of the j-th value in a given \ufb01eld. This representa- tion could again either be (i) simply the embed- ding of this value (ii) or a contextual representa- tion computed using a function f which also con- siders the other values in the \ufb01eld. For example, if (w1, w2, ..., wp) are the values in a \ufb01eld then these values can be treated as a sequence and the rep- resentation of the j-th value can be computed us- ing a bidirectional GRU over this sequence. Once again, we found that using a bi-GRU works bet- ter then simply using the embedding of the value. Once we have such a representation computed for all values across all the \ufb01elds, we compute the at- tention over these values (micro level) as shown below : aw t,j = vT w tanh(Uwst\u22121 + Vwhw j ) (2) \u03b1w t,j = exp(aw t,j) PW l=1 exp(aw t,l) (3) where st\u22121 is the state of the decoder at time step t \u22121.",
            "Uw, Vw and vw are parameters, W is the total number of values across all the \ufb01elds. Fused Attention: Intuitively, the attention weights assigned to a \ufb01eld should have an in\ufb02u- ence on all the values belonging to the particu- lar \ufb01eld. To ensure this, we reweigh the micro level attention weights based on the correspond- ing macro level attention weights. In other words, we fuse the attention weights at the two levels as: \u03b1 \u2032 t,j = \u03b1t,j\u03b2t,F(j) PW l=1 \u03b1t,l\u03b2t,F(l) (4) cw t = W X j=1 \u03b1 \u2032 t,jhw j (5) where F(j) is the \ufb01eld corresponding to the j-th value, cw t is the macro level context vector. 3.2 Gated Orthogonalization for Modeling Stay-On and Never Look Back behaviour We now describe a series of choices made to model stay-on and never look back behavior.",
            "3.2 Gated Orthogonalization for Modeling Stay-On and Never Look Back behaviour We now describe a series of choices made to model stay-on and never look back behavior. We \ufb01rst begin with the stay-on property which essen- tially implies that if we have paid attention to the \ufb01eld i at timestep t then we are likely to pay atten- tion to the same \ufb01eld for a few more time steps. For example, if we are focusing on the occupation \ufb01eld at this timestep then we are likely to focus on",
            "it for the next few timesteps till all relevant values in this \ufb01eld have been included in the generated description. In other words, we want to remem- ber the \ufb01eld context vector cg t for a few timesteps. One way of ensuring this is to use a remember (or forget) gate as given below which remembers the previous context vector when required and forgets it when it is time to move on from that \ufb01eld. ft = \u03c3(W f t cg t\u22121 + W f g ct\u22121 + bf) (6) ct = (1 \u2212ft) \u2299cg t + ft \u2299ct\u22121 (7) where Wtf, Wgf, bf are parameters to be learned. The job of the forget gate is to ensure that ct is similar to ct\u22121 when required (i.e., by learn- ing ft \u21921 when we want to continue focusing on the same \ufb01eld) and different when it is time to move on (by learning that ft \u21920). Next, the never look back property implies that once we have moved away from a \ufb01eld we are un- likely to pay attention to it again.",
            "Next, the never look back property implies that once we have moved away from a \ufb01eld we are un- likely to pay attention to it again. For example, once we have rendered all the occupations in the generated description there is no need to return back to the occupation \ufb01eld. In other words, once we have moved on (ft \u21920), we want the suc- cessive \ufb01eld context vectors cg t to be very different from the previous \ufb01eld vectors ct\u22121. One way of ensuring this is to orthogonalize successive \ufb01eld vectors using cg t = cg t \u2212\u03b3t \u2299< ct\u22121, cg t > < ct\u22121, ct\u22121 >ct\u22121 (8) where < a, b > is the dot product between vec- tors a and b. The above equation essentially sub- tracts the component of cg t along ct\u22121. \u03b3t is a learned parameter which controls the degree of or- thogonalization thereby allowing a soft orthogo- nalization (i.e., the entire component along ct\u22121 is not subtracted but only a fraction of it).",
            "\u03b3t is a learned parameter which controls the degree of or- thogonalization thereby allowing a soft orthogo- nalization (i.e., the entire component along ct\u22121 is not subtracted but only a fraction of it). The above equation only ensures that cg t is soft-orthogonal to ct\u22121. Alternately, we could pass the sequence of context vectors, c1, c2, ..., ct generated so far through a GRU cell. The state of this GRU cell at each time step would thus be aware of the his- tory of the \ufb01eld vectors till that timestep. Now instead of orthogonalizing cg t to ct\u22121 we could or- thogonalize cg t to the hidden state of this GRU at time-step t \u22121. In practice, we found this to work better as it accounts for all the \ufb01eld vectors in the history instead of only the previous \ufb01eld vector. In summary, Equation 7 provides a mechanism for remembering the current \ufb01eld vector when ap- propriate (thus capturing stay-on behavior) using a remember gate.",
            "In summary, Equation 7 provides a mechanism for remembering the current \ufb01eld vector when ap- propriate (thus capturing stay-on behavior) using a remember gate. On the other hand, Equation 8 explicitly ensures that the \ufb01eld vector is very different (soft-orthogonal) from the previous \ufb01eld vectors once it is time to move on (thus capturing never look back behavior). The value of cg t com- puted in Equation 8 is then used in Equation 7. The ct (macro) thus obtained is then concatenated with cw t (micro) and fed to the decoder (see Fig. 2) 4 Experimental setup We now describe our experimental setup: 4.1 Datasets We use the WIKIBIO dataset introduced by Lebret et al. (2016). It consists of 728, 321 biography ar- ticles from English Wikipedia. A biography arti- cle corresponds to a person (sportsman, politician, historical \ufb01gure, actor, etc.).",
            "(2016). It consists of 728, 321 biography ar- ticles from English Wikipedia. A biography arti- cle corresponds to a person (sportsman, politician, historical \ufb01gure, actor, etc.). Each Wikipedia ar- ticle has an accompanying infobox which serves as the structured input and the task is to generate the \ufb01rst sentence of the article (which typically is a one-line description of the person). We used the same train, valid and test sets which were made publicly available by Lebret et al. (2016). We also introduce two new biography datasets, one in French and one in German. These datasets were created and pre-processed using the same procedure as outlined in Lebret et al. (2016). Speci\ufb01cally, we extracted the infoboxes and the \ufb01rst sentence from the corresponding Wikipedia article. As with the English dataset, we split the French and German datasets randomly into train (80%), test (10%) and valid (10%).",
            "(2016). Speci\ufb01cally, we extracted the infoboxes and the \ufb01rst sentence from the corresponding Wikipedia article. As with the English dataset, we split the French and German datasets randomly into train (80%), test (10%) and valid (10%). The French and German datasets extracted by us has been made publicly available.2 The number of exam- ples was 170K and 50K and the vocabulary size was 297K and 143K for French and German re- spectively. Although in this work we focus only on generating descriptions in one language, we hope that this dataset will also be useful for develop- ing models which jointly learn to generate descrip- tions from structured data in multiple languages. 4.2 Models compared We compare with the following models: 1. (Lebret et al., 2016): This is a conditional language model which uses a feed-forward neu- ral network to predict the next word in the de- scription conditioned on local characteristics (i.e., 2https:\/\/github.com\/PrekshaNema25\/ StructuredData_To_Descriptions",
            "Model BLEU-4 NIST-4 ROUGE-4 (Lebret et al., 2016) 34.70 7.98 25.80 (Mei et al., 2016) 35.10 7.27 30.90 Basic Seq2Seq 38.20 8.47 34.28 +Fused bifocal attention 41.22 8.96 38.71 +Gated orthogonalization 42.03 9.17 39.11 Table 1: Comparison of different models on the English WIKIBIO dataset words within a \ufb01eld) and global characteristics (i.e., overall structure of the infobox). 2. (Mei et al., 2016): This model was pro- posed in the context of the WEATHERGOV and ROBOCUP datasets which have a much smaller vocabulary. They use an improved attention model with additional regularizer terms which in\ufb02uence the weights assigned to the \ufb01elds. 3. Basic Seq2Seq: This is the vanilla encode- attend-decode model (Bahdanau et al., 2014).",
            "They use an improved attention model with additional regularizer terms which in\ufb02uence the weights assigned to the \ufb01elds. 3. Basic Seq2Seq: This is the vanilla encode- attend-decode model (Bahdanau et al., 2014). Fur- ther, to deal with the large vocabulary (\u223c400K words) we use a copying mechanism as a post- processing step. Speci\ufb01cally, we identify the time steps at which the decoder produces unknown words (denoted by the special symbol UNK). For each such time step, we look at the attention weights on the input words and replace the UNK word by that input word which has received max- imum attention at this timestep. This process is similar to the one described in (Luong et al., 2015). Even Lebret et al. (2016) have a copying mechanism tightly integrated with their model. 4.3 Hyperparameter tuning We tuned the hyperparameters of all the models using a validation set.",
            "This process is similar to the one described in (Luong et al., 2015). Even Lebret et al. (2016) have a copying mechanism tightly integrated with their model. 4.3 Hyperparameter tuning We tuned the hyperparameters of all the models using a validation set. As mentioned earlier, we used a bidirectional GRU cell as the function f for computing the representation of the \ufb01elds and the values (see Section 3.1). For all the models, we ex- perimented with GRU state sizes of 128, 256 and 512. The total number of unique words in the cor- pus is around 400K (this includes the words in the infobox and the descriptions). Of these, we re- tained only the top 20K words in our vocabulary (same as (Lebret et al., 2016)). We initialized the embeddings of these words with 300 dimensional Glove embeddings (Pennington et al., 2014).",
            "Of these, we re- tained only the top 20K words in our vocabulary (same as (Lebret et al., 2016)). We initialized the embeddings of these words with 300 dimensional Glove embeddings (Pennington et al., 2014). We used Adam (Kingma and Ba, 2014) with a learn- ing rate of 0.0004, \u03b21 = 0.9 and \u03b22 = 0.999. We trained the model for a maximum of 20 epochs and used early stopping with the patience set to 5 epochs. 5 Results and Discussions We now discuss the results of our experiments. 5.1 Comparison of different models Following Lebret et al. (2016), we used BLEU- 4, NIST-4 and ROUGE-4 as the evaluation met- rics. We \ufb01rst make a few observations based on the results on the English dataset (Table 1). The basic seq2seq model, as well as the model pro- posed by Mei et al. (2016), perform better than the model proposed by Lebret et al. (2016).",
            "The basic seq2seq model, as well as the model pro- posed by Mei et al. (2016), perform better than the model proposed by Lebret et al. (2016). Our \ufb01- nal model with bifocal attention and gated orthog- onalization gives the best performance and does 10% (relative) better than the closest baseline (ba- sic seq2seq) and 21% (relative) better than the cur- rent state of the art method (Lebret et al., 2016). In Table 2, we show some qualitative examples of the output generated by different models. 5.2 Human Evaluations To make a qualitative assessment of the generated sentences, we conducted a human study on a sam- ple of 500 Infoboxes which were sampled from English dataset. The annotators for this task were undergraduate and graduate students. For each of these infoboxes, we generated summaries us- ing the basic seq2seq model and our \ufb01nal model with bifocal attention and gated orthogonalization. For each description and for each model, we asked three annotators to rank the output of the systems based on i) adequacy (i.e.",
            "For each description and for each model, we asked three annotators to rank the output of the systems based on i) adequacy (i.e. does it capture relevant information from the infobox), (ii) \ufb02uency (i.e. grammar) and (iii) relative preference (i.e., which of the two outputs would be preferred). Overall the average \ufb02uency\/adequacy (on a scale of 5) for basic seq2seq model was 4.04\/3.6 and 4.19\/3.9 for our model respectively. The results from Table 3 suggest that in gen- eral gated orthogonalization model performs bet- ter than the basic seq2seq model. Additionally, an- notators were asked to verify if the generated sum- maries look natural (i.e, as if they were generated by humans). In 423 out of 500 cases, the annota- tors said \u201cYes\u201d suggesting that gated orthogonal- ization model indeed produces good descriptions. 5.3 Performance on different languages The results on the French and German datasets are summarized in Tables 4 and 5 respectively.",
            "In 423 out of 500 cases, the annota- tors said \u201cYes\u201d suggesting that gated orthogonal- ization model indeed produces good descriptions. 5.3 Performance on different languages The results on the French and German datasets are summarized in Tables 4 and 5 respectively. Note that the code of (Lebret et al., 2016) is not pub- licly available, hence we could not report numbers",
            "Reference: Samuel Smiles (23 December 1812 - 16 April 1904), was a Scottish author and government reformer who campaigned on a Chartist platform. Basic Seq2Seq: samuel smiles (23 december 1812 \u2013 16 april 1904) was an english books and author. +Bifocal attention: samuel smiles (23 december 1812 - 16 april 1904) was a british books and books. +Gated Orthogonalization: samuel smiles (23 december 1812 - 16 april 1904) was a british biographies and author. Reference: Thomas Tenison (29 September 1636 - 14 December 1715) was an English church leader, Archbishop of Canterbury from 1694 until his death. Basic Seq2Seq: thomas tenison (14 december 1715 - 29 september 1636) was an english roman catholic archbishop. +Bifocal attention: thomas tenison (29 september 1636 - 14 december 1715) was an english clergyman of the roman catholic church.",
            "+Bifocal attention: thomas tenison (29 september 1636 - 14 december 1715) was an english clergyman of the roman catholic church. +Gated Orthogonalization: thomas tenison (29 september 1636 - 14 december 1715) was archbishop of canterbury from 1695 to 1715. Reference: Guy F. Cordon (April 24, 1890 - June 8, 1969) was a U.S. politician and lawyer from the state of Oregon. Basic Seq2Seq: charles l. mcnary (april 24 , 1890 8 , 1969) was a united states senator from oregon. +Bifocal attention:guy cordon (april 24 , 1890 \u2013 june 8 , 1969) was an american attorney and politician. +Gated Orthogonalization: guy cordon (april 24 , 1890 \u2013 june 8 , 1969) was an american attorney and politician from the state of oregon. Reference: Dr. Harrison B. Wilson Jr.",
            "+Gated Orthogonalization: guy cordon (april 24 , 1890 \u2013 june 8 , 1969) was an american attorney and politician from the state of oregon. Reference: Dr. Harrison B. Wilson Jr. (born April 21, 1925) is an American educator and college basketball coach who served as the second president of Norfolk State University from 1975-1997. Basic Seq2Seq: lyman beecher brooks (born april 21 , 1925) is an american educator and educator. +Bifocal attention: harrison b. wilson , jr. (born april 21 , 1925) is an american educator and academic administrator. +Gated Orthogonalization: harrison b. wilson , jr. (born april 21 , 1925) is an american educator , academic administrator , and former president of norfolk state university. Table 2: Examples of generated descriptions from different models. For the last two examples, name generated by Basic Seq2Seq model is incorrect because it attended to preceded by \ufb01eld.",
            "Table 2: Examples of generated descriptions from different models. For the last two examples, name generated by Basic Seq2Seq model is incorrect because it attended to preceded by \ufb01eld. Metric A <B A == B A >B Adequacy 186 208 106 Fluency 244 108 148 Preference 207 207 86 Table 3: Qualitative Comparison of Model A (Seq2Seq) and Model B (our model) for French and German using their model. We ob- serve that our \ufb01nal model gives the best perfor- mance - though the bifocal attention model per- forms poorly as compared to the basic seq2seq model on French. However, the overall perfor- mance for French and German are much smaller than those for English. There could be multiple reasons for this. First, the amount of training data in these two languages is smaller than that in En- glish. Speci\ufb01cally, the amount of training data available in French (German) is only 24.2 (7.5)% of that available for English.",
            "First, the amount of training data in these two languages is smaller than that in En- glish. Speci\ufb01cally, the amount of training data available in French (German) is only 24.2 (7.5)% of that available for English. Second, on average the descriptions in French and German are longer than that in English (EN: 26.0 words, FR: 36.5 words and DE: 32.3 words). Finally, a manual in- spection across the three languages suggests that the English descriptions have a more consistent structure than the French descriptions. For exam- ple, most English descriptions start with name fol- lowed by date of birth but this is not the case in French.",
            "Finally, a manual in- spection across the three languages suggests that the English descriptions have a more consistent structure than the French descriptions. For exam- ple, most English descriptions start with name fol- lowed by date of birth but this is not the case in French. However, this is only a qualitative obser- vation and it is hard to quantify this characteristic Model BLEU-4 NIST-4 ROUGE-4 (Mei et al., 2016) 10.40 2.51 7.81 Basic Seq2Seq 14.50 3.02 12.22 +Fused bifocal attention 13.80 2.86 12.37 +Gated orthogonalization 15.52 3.30 12.80 Table 4: Comparison of different models on the French WIKIBIO dataset Model BLEU-4 NIST-4 ROUGE-4 (Mei et al., 2016) 9.30 2.23 5.85 Basic Seq2Seq 17.05 3.09 12.16 +Fused bifocal attention 20.38 3.43 14.89 +Gated orthogonalization 23.33 4.24 16.40 Table 5: Comparison of different models on the German WIKIBIO dataset of the French and German datasets.",
            "5.4 Visualizing Attention Weights If the proposed model indeed works well then we should see attention weights that are consistent with the stay on and never look back behavior. To verify this, we plotted the attention weights in cases where the model with gated orthogonaliza- tion does better than the model with only bifocal attention. Figure 3 shows the attention weights corresponding to infobox in Figure 4. Notice that the model without gated orthogonalization has at- tention on both name \ufb01eld and article title while rendering the name. The model with gated orthog- onalization, on the other hand, stays on the name",
            "(a) Fused Bifocal Attention (b) Fused Bifocal Attention + Gated Orthogonalization Figure 3: Comparison of the attention weights and descriptions produced for Infobox in Figure 4 Figure 4: Wikipedia Infobox for Samuel Smiles Figure 5: Wikipedia Infobox for Mark Tobey \ufb01eld for as long as it is required but then moves and never returns to it (as expected). Due to lack of space, we do not show similar plots for French and German but we would like to mention that, in general, the differences between the attention weights learned by the model with and without gated orthogonalization were more pronounced for the French\/German dataset than the English dataset. This is in agreement with the results reported in Table 4 and 5 where the im- provements given by gated orthogonalization are more for French\/German than for English.",
            "This is in agreement with the results reported in Table 4 and 5 where the im- provements given by gated orthogonalization are more for French\/German than for English. Training data Target (test) data Arts Sports Entire dataset 33.6 52.4 Without target domain data 24.5 29.3 +5k target domain data 31.2 41.8 +10k target domain data 32.2 43.3 Table 6: Out of domain results(BLEU-4) 5.5 Out of domain results What if the model sees a different type of per- son at test time? For example, what if the train- ing data does not contain any sportspersons but at test time we encounter the infobox of a sportsper- son. This is the same as seeing out-of-domain data at test time. Such a situation is quite expected in the products domain where new products with new features (\ufb01elds) get frequently added to the cata- log. We were interested in three questions here. First, we wanted to see if testing the model on out- of-domain data indeed leads to a drop in the per- formance.",
            "We were interested in three questions here. First, we wanted to see if testing the model on out- of-domain data indeed leads to a drop in the per- formance. For this, we compared the performance of our best model in two scenarios (i) trained on data from all domains (including the target do- main) and tested on the target domain (sports, arts) and (ii) trained on data from all domains except the target domain and tested on the target domain. Comparing rows 1 and 2 of Table 6 we observed a signi\ufb01cant drop in the performance. Note that the numbers for sports domain in row 1 are much bet- ter than the Arts domain because roughly 40% of the WIKIBIO training data contains sportspersons. Next, we wanted to see if we can use a small",
            "(a) Without \ufb01ne tuning. (b) With \ufb01ne tuning with 5K in-domain data. Figure 6: Comparison of the attention weights and descriptions (see highlighted boxes) produced by an out-of-domain model with and without \ufb01ne tuning for the Infobox in Figure 5 amount of data from the target domain to \ufb01ne tune a model trained on the out of domain data. We ob- serve that even with very small amounts of target domain data the performance starts improving sig- ni\ufb01cantly (see rows 3 and 4 of Table 6). Note that if we train a model from scratch with only lim- ited data from the target domain instead of \ufb01ne- tuning a model trained on a different source do- main then the performance is very poor. In par- ticular, training a model from scratch with 10K training instances we get a BLEU score of 16.2 and 28.4 for arts and sports respectively. Finally, even though the actual words used for describing a sportsperson (footballer, cricketer, etc.) would be very different from the words used to describe an artist (actor, musician, etc.)",
            "Finally, even though the actual words used for describing a sportsperson (footballer, cricketer, etc.) would be very different from the words used to describe an artist (actor, musician, etc.) they might share many \ufb01elds (for example, date of birth, occupation, etc.). As seen in Figure 6 (attention weights correspond- ing to the infobox in Figure 5), the model predicts the attention weights correctly for common \ufb01elds (such as occupation) but it is unable to use the right vocabulary to describe the occupation (since it has not seen such words frequently in the train- ing data). However, once we \ufb01ne tune the model with limited data from the target domain we see that it picks up the new vocabulary and produces a correct description of the occupation. 6 Conclusion We present a model for generating natural lan- guage descriptions from structured data. To ad- dress speci\ufb01c characteristics of the problem we propose neural components for fused bifocal at- tention and gated orthogonalization to address stay on and never look back behavior while decoding.",
            "6 Conclusion We present a model for generating natural lan- guage descriptions from structured data. To ad- dress speci\ufb01c characteristics of the problem we propose neural components for fused bifocal at- tention and gated orthogonalization to address stay on and never look back behavior while decoding. Our \ufb01nal model outperforms an existing state of the art model on a large scale WIKIBIO dataset by 21%. We also introduce datasets for French and German and demonstrate that our model gives state of the art results on these datasets. Finally, we perform experiments with an out-of-domain model and show that if such a model is \ufb01ne-tuned with small amounts of in domain data then it can give an improved performance on the target do- main. Given the multilingual nature of the new datasets, as future work, we would like to build models which can jointly learn to generate natu- ral language descriptions from structured data in multiple languages. One idea is to replace the con- cepts in the input infobox by Wikidata concept ids which are language agnostic. A large amount of input vocabulary could thus be shared across lan- guages thereby facilitating joint learning.",
            "One idea is to replace the con- cepts in the input infobox by Wikidata concept ids which are language agnostic. A large amount of input vocabulary could thus be shared across lan- guages thereby facilitating joint learning. 7 Acknowledgements We thank Google for supporting Preksha Nema through their Google India Ph.D. Fellowship pro- gram. We also thank Microsoft Research India for supporting Shreyas Shetty through their generous travel grant for attending the conference.",
            "References Gabor Angeli, Percy Liang, and Dan Klein. 2010. A simple domain-independent probabilistic approach to generation. In Proceedings of the 2010 Confer- ence on Empirical Methods in Natural Language Processing. Association for Computational Linguis- tics, Stroudsburg, PA, USA, EMNLP \u201910, pages 502\u2013512. http:\/\/dl.acm.org\/citation. cfm?id=1870658.1870707. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2014. Neural machine translation by jointly learning to align and translate. In ICLR 2015. Regina Barzilay and Mirella Lapata. 2005. Collec- tive content selection for concept-to-text generation. In Proceedings of the Conference on Human Lan- guage Technology and Empirical Methods in Natu- ral Language Processing. Association for Compu- tational Linguistics, Stroudsburg, PA, USA, HLT \u201905, pages 331\u2013338. https:\/\/doi.org\/10.",
            "In Proceedings of the Conference on Human Lan- guage Technology and Empirical Methods in Natu- ral Language Processing. Association for Compu- tational Linguistics, Stroudsburg, PA, USA, HLT \u201905, pages 331\u2013338. https:\/\/doi.org\/10. 3115\/1220575.1220617. Anja Belz. 2008. Automatic generation of weather forecast texts using comprehensive probabilis- tic generation-space models. Nat. Lang. Eng. 14(4):431\u2013455. https:\/\/doi.org\/10. 1017\/S1351324907004664. Anja Belz and Eric Kow. 2009. System building cost vs. output quality in data-to-text generation. In Pro- ceedings of the 12th European Workshop on Natu- ral Language Generation. Association for Compu- tational Linguistics, Stroudsburg, PA, USA, ENLG \u201909, pages 16\u201324. http:\/\/dl.acm.org\/ citation.cfm?id=1610195.1610198. David L. Chen and Raymond J. Mooney.",
            "Association for Compu- tational Linguistics, Stroudsburg, PA, USA, ENLG \u201909, pages 16\u201324. http:\/\/dl.acm.org\/ citation.cfm?id=1610195.1610198. David L. Chen and Raymond J. Mooney. 2008. Learn- ing to sportscast: A test of grounded language ac- quisition. In Proceedings of the 25th International Conference on Machine Learning. ACM, New York, NY, USA, ICML \u201908, pages 128\u2013135. https: \/\/doi.org\/10.1145\/1390156.1390173. Sumit Chopra, Michael Auli, and Alexander M. Rush. 2016. Abstractive sentence summarization with at- tentive recurrent neural networks. In Proceedings of the 2016 Conference of the North American Chap- ter of the Association for Computational Linguis- tics: Human Language Technologies. Association for Computational Linguistics, San Diego, Califor- nia, pages 93\u201398. http:\/\/www.aclweb.org\/ anthology\/N16-1012.",
            "Association for Computational Linguistics, San Diego, Califor- nia, pages 93\u201398. http:\/\/www.aclweb.org\/ anthology\/N16-1012. Robert Dale, Sabine Geldof, and Jean-Philippe Prost. 2003. Coral : Using natural language generation for navigational assistance. In Michael J. Oudshoorn, editor, Twenty-Sixth Australasian Computer Science Conference (ACSC2003). ACS, Adelaide, Australia, volume 16 of CRPIT, pages 35\u201344. Dimitrios Galanis and Ion Androutsopoulos. 2007. Generating multilingual descriptions from linguis- tically annotated owl ontologies: The naturalowl system. In Proceedings of the Eleventh Eu- ropean Workshop on Natural Language Genera- tion. Association for Computational Linguistics, Stroudsburg, PA, USA, ENLG \u201907, pages 143\u2013 146. http:\/\/dl.acm.org\/citation.cfm? id=1610163.1610188. Albert Gatt and Anja Belz. 2010.",
            "Association for Computational Linguistics, Stroudsburg, PA, USA, ENLG \u201907, pages 143\u2013 146. http:\/\/dl.acm.org\/citation.cfm? id=1610163.1610188. Albert Gatt and Anja Belz. 2010. Introducing shared tasks to nlg: The tuna shared task evaluation chal- lenges pages 264\u2013293. Nancy Green. 2006. Generation of biomedical ar- guments for lay readers. In Proceedings of the Fourth International Natural Language Generation Conference. Association for Computational Linguis- tics, Stroudsburg, PA, USA, INLG \u201906, pages 114\u2013 121. http:\/\/dl.acm.org\/citation.cfm? id=1706269.1706292. Chlo\u00b4e Kiddon, Luke Zettlemoyer, and Yejin Choi. 2016. Globally coherent text generation with neu- ral checklist models. In EMNLP. The Association for Computational Linguistics, pages 329\u2013339. Joohyun Kim and Raymond J. Mooney. 2010.",
            "2016. Globally coherent text generation with neu- ral checklist models. In EMNLP. The Association for Computational Linguistics, pages 329\u2013339. Joohyun Kim and Raymond J. Mooney. 2010. Gen- erative alignment and semantic parsing for learning from ambiguous supervision. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters. Association for Computa- tional Linguistics, Stroudsburg, PA, USA, COLING \u201910, pages 543\u2013551. http:\/\/dl.acm.org\/ citation.cfm?id=1944566.1944628. Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. CoRR abs\/1412.6980. http:\/\/arxiv.org\/abs\/ 1412.6980. Ioannis Konstas and Mirella Lapata. 2013. In- ducing document plans for concept-to-text gen- eration. In EMNLP. ACL, pages 1503\u20131514.",
            "Ioannis Konstas and Mirella Lapata. 2013. In- ducing document plans for concept-to-text gen- eration. In EMNLP. ACL, pages 1503\u20131514. http:\/\/dblp.uni-trier.de\/db\/conf\/ emnlp\/emnlp2013.html#KonstasL13. Irene Langkilde and Kevin Knight. 1998. Gener- ation that exploits corpus-based statistical knowl- edge. In Proceedings of the 36th Annual Meet- ing of the Association for Computational Linguis- tics and 17th International Conference on Computa- tional Linguistics - Volume 1. Association for Com- putational Linguistics, Stroudsburg, PA, USA, ACL \u201998, pages 704\u2013710. https:\/\/doi.org\/10. 3115\/980845.980963. R\u00b4emi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with application to the biography domain. In Proceedings of the 2016 Conference on Empiri- cal Methods in Natural Language Processing.",
            "R\u00b4emi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with application to the biography domain. In Proceedings of the 2016 Conference on Empiri- cal Methods in Natural Language Processing. As- sociation for Computational Linguistics, Austin, Texas, pages 1203\u20131213. https:\/\/aclweb. org\/anthology\/D16-1128. Percy Liang, Michael I. Jordan, and Dan Klein. 2009. Learning semantic correspondences with less su- pervision. In Proceedings of the Joint Confer- ence of the 47th Annual Meeting of the ACL and",
            "the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1. Association for Computational Linguis- tics, Stroudsburg, PA, USA, ACL \u201909, pages 91\u2013 99. http:\/\/dl.acm.org\/citation.cfm? id=1687878.1687893. Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Wojciech Zaremba. 2015. Addressing the rare word problem in neural machine translation. In ACL. Franois Mairesse and Marilyn A. Walker. 2011. Con- trolling user perceptions of linguistic style: Train- able generation of personality traits. Computational Linguistics 37(3):455\u2013488. https:\/\/doi.org\/ 10.1162\/COLI_a_00063. Hongyuan Mei, Mohit Bansal, and Matthew R. Walter. 2016. What to talk about and how? selective gener- ation using lstms with coarse-to-\ufb01ne alignment.",
            "Hongyuan Mei, Mohit Bansal, and Matthew R. Walter. 2016. What to talk about and how? selective gener- ation using lstms with coarse-to-\ufb01ne alignment. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Compu- tational Linguistics: Human Language Technolo- gies. Association for Computational Linguistics, San Diego, California, pages 720\u2013730. http:\/\/ www.aclweb.org\/anthology\/N16-1086. Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et al. 2016. Abstractive text summa- rization using sequence-to-sequence rnns and be- yond. arXiv preprint arXiv:1602.06023 . Preksha Nema, Mitesh Khapra, Anirban Laha, and Balaraman Ravindran. 2017. Diversity driven atten- tion model for query-based abstractive summariza- tion.",
            "Preksha Nema, Mitesh Khapra, Anirban Laha, and Balaraman Ravindran. 2017. Diversity driven atten- tion model for query-based abstractive summariza- tion. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. As- sociation for Computational Linguistics, Vancouver, Canada. Jeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. Glove: Global vec- tors for word representation. In Empirical Meth- ods in Natural Language Processing (EMNLP). pages 1532\u20131543. http:\/\/www.aclweb.org\/ anthology\/D14-1162. Aaditya Prakash, Sadid A. Hasan, Kathy Lee, Vivek Datla, Ashequl Qadir, Joey Liu, and Oladimeji Farri. 2016. Neural paraphrase generation with stacked residual lstm networks. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers.",
            "2016. Neural paraphrase generation with stacked residual lstm networks. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee, Os- aka, Japan, pages 2923\u20132934. http:\/\/aclweb. org\/anthology\/C16-1275. Ehud Reiter, Somayajulu Sripada, Jim Hunter, Jin Yu, and Ian Davy. 2005. Choosing words in computer-generated weather fore- casts. Artif. Intell. 167(1-2):137\u2013169. http: \/\/dblp.uni-trier.de\/db\/journals\/ ai\/ai167.html#ReiterSHYD05. Alexander M. Rush, Sumit Chopra, and Jason We- ston. 2015. A neural attention model for ab- stractive sentence summarization. In Proceed- ings of the 2015 Conference on Empirical Meth- ods in Natural Language Processing. Associa- tion for Computational Linguistics, Lisbon, Por- tugal, pages 379\u2013389.",
            "A neural attention model for ab- stractive sentence summarization. In Proceed- ings of the 2015 Conference on Empirical Meth- ods in Natural Language Processing. Associa- tion for Computational Linguistics, Lisbon, Por- tugal, pages 379\u2013389. http:\/\/aclweb.org\/ anthology\/D15-1044. Iulian V. Serban, Alessandro Sordoni, Yoshua Ben- gio, Aaron Courville, and Joelle Pineau. 2016. Building end-to-end dialogue systems using gen- erative hierarchical neural network models. In Proceedings of the Thirtieth AAAI Conference on Arti\ufb01cial Intelligence. AAAI Press, AAAI\u201916, pages 3776\u20133783. http:\/\/dl.acm.org\/ citation.cfm?id=3016387.3016435. Radu Soricut and Daniel Marcu. 2006. Stochas- tic language generation using widl-expressions and its application in machine translation and summa- rization.",
            "http:\/\/dl.acm.org\/ citation.cfm?id=3016387.3016435. Radu Soricut and Daniel Marcu. 2006. Stochas- tic language generation using widl-expressions and its application in machine translation and summa- rization. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Com- putational Linguistics. Association for Computa- tional Linguistics, Stroudsburg, PA, USA, ACL- 44, pages 1105\u20131112. https:\/\/doi.org\/10. 3115\/1220175.1220314. Ross Turner, Somayajulu Sripada, and Ehud Reiter. 2010. Generating approximate geographic de- scriptions. In Emiel Krahmer and Marit The- une, editors, Empirical Methods in Natural Lan- guage Generation. Springer, volume 5790 of Lec- ture Notes in Computer Science, pages 121\u2013 140. http:\/\/dblp.uni-trier.de\/db\/ conf\/eacl\/enlg2010.html#TurnerSR10.",
            "Springer, volume 5790 of Lec- ture Notes in Computer Science, pages 121\u2013 140. http:\/\/dblp.uni-trier.de\/db\/ conf\/eacl\/enlg2010.html#TurnerSR10. Subhashini Venugopalan, Huijuan Xu, Jeff Don- ahue, Marcus Rohrbach, Raymond J. Mooney, and Kate Saenko. 2014. Translating videos to natu- ral language using deep recurrent neural networks. CoRR abs\/1412.4729. http:\/\/arxiv.org\/ abs\/1412.4729. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual atten- tion. In David Blei and Francis Bach, editors, Proceedings of the 32nd International Conference on Machine Learning (ICML-15).",
            "2015. Show, attend and tell: Neural image caption generation with visual atten- tion. In David Blei and Francis Bach, editors, Proceedings of the 32nd International Conference on Machine Learning (ICML-15). JMLR Work- shop and Conference Proceedings, pages 2048\u2013 2057. http:\/\/jmlr.org\/proceedings\/ papers\/v37\/xuc15.pdf. Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. 2016. Hierarchi- cal attention networks for document classi\ufb01cation. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies. Association for Computational Linguistics, San Diego, California, pages 1480\u20131489. http:\/\/ www.aclweb.org\/anthology\/N16-1174.",
            "Kaisheng Yao, Geoffrey Zweig, and Baolin Peng. 2015. Attention with intention for a neural net- work conversation model. CoRR abs\/1510.08565. http:\/\/arxiv.org\/abs\/1510.08565."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1804.07789.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 11444.999771118164,
    "avg_doclen_est": 176.07691955566406
}
