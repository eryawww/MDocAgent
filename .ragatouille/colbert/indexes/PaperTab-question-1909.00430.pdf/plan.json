{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Transfer Learning Between Related Tasks Using Expected Label Proportions Matan Ben Noach\u2217\u2020 and Yoav Goldberg\u2217\u2021 \u2217Computer Science Department, Bar-Ilan University, Ramat-Gan Israel \u2020Intel AI Lab, Petah-Tikva Israel \u2021Allen Institute for Arti\ufb01cial Intelligence matan.ben.noach@intel.com, yoav.goldberg@gmail.com Abstract Deep learning systems thrive on abundance of labeled training data but such data is not al- ways available, calling for alternative methods of supervision. One such method is expecta- tion regularization (XR) (Mann and McCal- lum, 2007), where models are trained based on expected label proportions. We propose a novel application of the XR framework for transfer learning between related tasks, where knowing the labels of task A provides an es- timation of the label proportion of task B. We then use a model trained for A to label a large corpus, and use this corpus with an XR loss to train a model for task B. To make the XR framework applicable to large-scale deep-learning setups, we propose a stochastic batched approximation procedure.",
            "We demon- strate the approach on the task of Aspect- based Sentiment classi\ufb01cation, where we ef- fectively use a sentence-level sentiment pre- dictor to train accurate aspect-based predictor. The method improves upon fully supervised neural system trained on aspect-level data, and is also cumulative with LM-based pretrain- ing, as we demonstrate by improving a BERT- based Aspect-based Sentiment model. 1 Introduction Data annotation is a key bottleneck in many data driven algorithms. Speci\ufb01cally, deep learning models, which became a prominent tool in many data driven tasks in recent years, require large datasets to work well. However, many tasks re- quire manual annotations which are relatively hard to obtain at scale.",
            "Speci\ufb01cally, deep learning models, which became a prominent tool in many data driven tasks in recent years, require large datasets to work well. However, many tasks re- quire manual annotations which are relatively hard to obtain at scale. An attractive alternative is lightly supervised learning (Schapire et al., 2002; Jin and Liu, 2005; Chang et al., 2007; Grac\u00b8a et al., 2007; Quadrianto et al., 2009a; Mann and Mc- Callum, 2010a; Ganchev et al., 2010; Hope and Shahaf, 2016), in which the objective function is supplemented by a set of domain-speci\ufb01c soft- constraints over the model\u2019s predictions on unla- beled data. For example, in label regularization (Mann and McCallum, 2007) the model is trained to \ufb01t the true label proportions of an unlabeled dataset.",
            "For example, in label regularization (Mann and McCallum, 2007) the model is trained to \ufb01t the true label proportions of an unlabeled dataset. Label regularization is special case of ex- pectation regularization (XR) (Mann and McCal- lum, 2007), in which the model is trained to \ufb01t the conditional probabilities of labels given features. In this work we consider the case of correlated tasks, in the sense that knowing the labels for task A provides information on the expected la- bel composition of task B. We demonstrate the ap- proach using sentence-level and aspect-level senti- ment analysis, which we use as a running example: knowing that a sentence has positive sentiment label (task A), we can expect that most aspects within this sentence (task B) will also have pos- itive label. While this expectation may be noisy on the individual example level, it holds well in aggregate: given a set of positively-labeled sen- tences, we can robustly estimate the proportion of positively-labeled aspects within this set.",
            "While this expectation may be noisy on the individual example level, it holds well in aggregate: given a set of positively-labeled sen- tences, we can robustly estimate the proportion of positively-labeled aspects within this set. For ex- ample, in a random set of positive sentences, we expect to \ufb01nd 90% positive aspects, while in a set of negative sentences, we expect to \ufb01nd 70% nega- tive aspects. These proportions can be easily either guessed or estimated from a small set. We propose a novel application of the XR framework for transfer learning in this setup. We present an algorithm (Sec 3.1) that, given a cor- pus labeled for task A (sentence-level sentiment), learns a classi\ufb01er for performing task B (aspect- level sentiment) instead, without a direct supervi- sion signal for task B. We note that the label in- formation for task A is only used at training time.",
            "Furthermore, due to the stochastic nature of the estimation, the task A labels need not be fully ac- curate, allowing us to make use of noisy predic- tions which are assigned by an automatic classi- \ufb01er (Sections 3.1 and 4). In other words, given arXiv:1909.00430v1  [cs.LG]  1 Sep 2019",
            "a medium-sized sentiment corpus with sentence- level labels, and a large collection of un-annotated text from the same distribution, we can train an ac- curate aspect-level sentiment classi\ufb01er. The XR loss allows us to use task A labels for training task B predictors. This ability seamlessly integrates into other semi-supervised schemes: we can use the XR loss on top of a pre-trained model to \ufb01ne-tune the pre-trained representation to the target task, and we can also take the model trained using XR loss and plentiful data and \ufb01ne-tune it to the target task using the available small-scale an- notated data. In Section 5.3 we explore these op- tions and show that our XR framework improves the results also when applied on top of a pre- trained BERT-based model (Devlin et al., 2018). Finally, to make the XR framework applicable to large-scale deep-learning setups, we propose a stochastic batched approximation procedure (Sec- tion 3.2). Source code is available at https: \/\/github.com\/MatanBN\/XRTransfer.",
            "Finally, to make the XR framework applicable to large-scale deep-learning setups, we propose a stochastic batched approximation procedure (Sec- tion 3.2). Source code is available at https: \/\/github.com\/MatanBN\/XRTransfer. 2 Background and Related Work 2.1 Lightly Supervised Learning An effective way to supplement small annotated datasets is to use lightly supervised learning, in which the objective function is supplemented by a set of domain-speci\ufb01c soft-constraints over the model\u2019s predictions on unlabeled data.",
            "2 Background and Related Work 2.1 Lightly Supervised Learning An effective way to supplement small annotated datasets is to use lightly supervised learning, in which the objective function is supplemented by a set of domain-speci\ufb01c soft-constraints over the model\u2019s predictions on unlabeled data. Previ- ous work in lightly-supervised learning focused on training classi\ufb01ers by using prior knowledge of label proportions (Jin and Liu, 2005; Chang et al., 2007; Musicant et al., 2007; Mann and Mc- Callum, 2007; Quadrianto et al., 2009b; Liang et al., 2009; Ganchev et al., 2010; Mann and Mc- Callum, 2010b; Chang et al., 2012; Wang et al., 2012; Zhu et al., 2014; Hope and Shahaf, 2016) or prior knowledge of features label associations (Schapire et al., 2002; Haghighi and Klein, 2006; Druck et al., 2008; Melville et al., 2009; Moham- mady and Culotta, 2015).",
            "In the context of NLP, Haghighi and Klein (2006) suggested to use dis- tributional similarities of words to train sequence models for part-of-speech tagging and a classi- \ufb01ed ads information extraction task. Melville et al. (2009) used background lexical information in terms of word-class associations to train a sen- timent classi\ufb01er. Ganchev and Das (2013); Wang and Manning (2014) suggested to exploit the bilin- gual correlations between a resource rich language and a resource poor language to train a classi\ufb01er for the resource poor language in a lightly super- vised manner. 2.2 Expectation Regularization (XR) Expectation Regularization (XR) (Mann and Mc- Callum, 2007) is a lightly supervised learning method, in which the model is trained to \ufb01t the conditional probabilities of labels given features. In the context of NLP, XR was used by Moham- mady and Culotta (2015) to train twitter-user at- tribute prediction using hundreds of noisy distri- butional expectations based on census demograph- ics.",
            "In the context of NLP, XR was used by Moham- mady and Culotta (2015) to train twitter-user at- tribute prediction using hundreds of noisy distri- butional expectations based on census demograph- ics. Here, we suggest using XR to train a target task (aspect-level sentiment) based on the output of a related source-task classi\ufb01er (sentence-level sentiment). Learning Setup The main idea of XR is mov- ing from a fully supervised situation in which each data-point xi has an associated label yi, to a setup in which sets of data points Uj are associated with corresponding label proportions \u02dcpj over that set. Formally, let X = {x1, x2, . . . , xn} \u2286X be a set of data points, Y be a set of |Y| class labels, U = {U1, U2, . . . , Um} be a set of sets where Uj \u2286X for every j \u2208{1, 2, . . . , m}, and let \u02dcpj \u2208R|Y| be the label distribution of set Uj.",
            ". . , Um} be a set of sets where Uj \u2286X for every j \u2208{1, 2, . . . , m}, and let \u02dcpj \u2208R|Y| be the label distribution of set Uj. For example, \u02dcpj = {.7, .2, .1} would indicate that 70% of data points in Uj are expected to have class 0, 20% are expected to have class 1 and 10% are expected to have class 2. Let p\u03b8(x) be a param- eterized function with parameters \u03b8 from X to a vector of conditional probabilities over labels in Y. We write p\u03b8(y|x) to denote the probability as- signed to the yth event (the conditional probability of y given x).",
            "Let p\u03b8(x) be a param- eterized function with parameters \u03b8 from X to a vector of conditional probabilities over labels in Y. We write p\u03b8(y|x) to denote the probability as- signed to the yth event (the conditional probability of y given x). A typically objective when training on fully la- beled data of (xi, yi) pairs is to maximize likeli- hood of labeled data using the cross entropy loss, Lcross(\u03b8) = \u2212 n X i log p\u03b8(yi|xi) Instead, in XR our data comes in the form of pairs (Uj, \u02dcpj) of sets and their corresponding expected label proportions, and we aim to optimize \u03b8 to \ufb01t the label distribution \u02dcpj over Uj, for all j. XR Loss As counting the number of pre- dicted class labels over a set U leads to a non- differentiable objective, Mann and McCallum (2007) suggest to relax it and use instead the",
            "model\u2019s posterior distribution \u02c6pj over the set: \u02c6qj(y) = X x\u2208Uj p\u03b8(y|x) (1) \u02c6pj(y) = \u02c6qj(y) P y\u2032 \u02c6qj(y\u2032) (2) where q(y) indicates the yth entry in q. Then, we would like to set \u03b8 such that \u02c6pj and \u02dcpj are close. Mann and McCallum (2007) suggest to use KL- divergence for this.",
            "Mann and McCallum (2007) suggest to use KL- divergence for this. KL-divergence is composed of two parts: DKL(\u02dcpj||\u02c6pj) = \u2212\u02dcpj \u00b7 log \u02c6pj + \u02dcpj \u00b7 log \u02dcpj = H(\u02dcpj, \u02c6pj) \u2212H(\u02dcpj) Since H(\u02dcpj) is constant, we only need to min- imize H(\u02dcpj, \u02c6pj), therefore the loss function be- comes:1 LXR(\u03b8) = \u2212 m X j=1 \u02dcpj \u00b7 log \u02c6pj (3) Notice that computing \u02c6qj requires summation over p\u03b8(x) for the entire set Uj, which can be pro- hibitive. We present batched approximation (Sec- tion 3.2) to overcome this. Temperature Parameter Mann and McCallum (2007) \ufb01nd that XR might \ufb01nd a degenerate solu- tion.",
            "We present batched approximation (Sec- tion 3.2) to overcome this. Temperature Parameter Mann and McCallum (2007) \ufb01nd that XR might \ufb01nd a degenerate solu- tion. For example, in a three class classi\ufb01cation task, where \u02dcpj = {.5, .35, .15}, it might \ufb01nd a so- lution such that \u02c6p\u03b8(y) = {.5, .35, .15} for every instance, as a result, every instance will be classi- \ufb01ed the same. To avoid this, Mann and McCallum (2007) suggest to penalize \ufb02at distributions by us- ing a temperature coef\ufb01cient T likewise: p\u03b8(y|x) = \u0012 ezW+b P k e(zW+b)k \u0013 1 T (4) Where z is a feature vector and W and b are the linear classi\ufb01er parameters.",
            "2.3 Aspect-based Sentiment Classi\ufb01cation In the aspect-based sentiment classi\ufb01cation (ABSC) task, we are given a sentence and an aspect, and need to determine the sentiment that is expressed towards the aspect.",
            "2.3 Aspect-based Sentiment Classi\ufb01cation In the aspect-based sentiment classi\ufb01cation (ABSC) task, we are given a sentence and an aspect, and need to determine the sentiment that is expressed towards the aspect. For example the sentence \u201cExcellent food, although the in- terior could use some help.\u201c has two aspects: 1Note also that \u2200j|Uj| = 1 \u21d0\u21d2LXR(\u03b8) = Lcross(\u03b8) Algorithm 1 Stochastic Batched XR Inputs: A dataset (U1, ..., Um, \u02dcp1, ..., \u02dcpm), batch size k, differentiable classi\ufb01er p\u03b8(y|x) while not converged do j \u2190random(1, ..., m) U\u2032 \u2190random-choice(Uj,k) \u02c6q\u2032 u \u2190P x\u2208U\u2032 p\u03b8(x) \u02c6p\u2032 u \u2190normalize(\u02c6q\u2032 u) \u2113\u2190\u2212\u02dcpj log \u02c6pu \u25b7Compute loss \u2113(eq (4)) Compute gradients and update \u03b8 end while return \u03b8 food and interior, a positive sentiment is ex- pressed about the food, but a negative sentiment is expressed about the interior.",
            "A sentence \u03b1 = (w1, w2, . . . , wn), may contain 0 or more aspects ai, where each aspect corresponds to a sub-sequence of the original sentence, and has an associated sentiment label (NEG, POS, or NEU). Concretely, we follow the task de\ufb01nition in the SemEval-2015 and SemEval-2016 shared tasks (Pontiki et al., 2015, 2016), in which the relevant aspects are given and the task focuses on \ufb01nding the sentiment label of the aspects. While sentence-level sentiment labels are rela- tively easy to obtain, aspect-level annotation are much more scarce, as demonstrated in the small datasets of the SemEval shared tasks. 3 Technical Contributions 3.1 Transfer-training between related tasks with XR Consider two classi\ufb01cation tasks over a shared in- put space, a source task s from X to Ys and a tar- get task t from X to Yt, which are related through a conditional distribution P(yt = i|ys = j).",
            "In other words, a labeling decision for task s induces an expected label distribution over the task t. For a set of datapoints x1, ..., xn that share a source la- bel ys, we expect to see a target label distribution of P(yt|ys) = \u02dcpys. Given a large unlabeled dataset Du = (xu 1, ..., xu |Du|), a small labeled dataset for the tar- get task Dt = ((xt 1, yt 1), ..., (xt |Dt|, yt |Dt|)), classi- \ufb01er Cs : X 7\u2192Ys (or suf\ufb01cient training data to train one) for the source task,2 we wish to use Cs 2Note that the classi\ufb01er does not need to be trainable or differentiable. It can be a human, a rule based system, a non- parametric model, a probabilistic model, a deep learning net-",
            "and Du to train a good classi\ufb01er Ct : X 7\u2192Yt for the target task. This can be achieved using the following procedure. \u2022 Apply Cs to Dt, resulting in a noisy source- side labels \u02dcys i = Cs(xt i) for the target task. \u2022 Estimate the conditional probability P(yt|\u02dcys) table using MLE estimates over Dt \u02dcpj(yt = i|\u02dcys = j) = #(yt = i, \u02dcys = j) #(\u02dcys = j) where # is a counting function over Dt.3 \u2022 Apply Cs to the unlabeled data Du resulting in labels Cs(xu i ). Split Du into |Ys| sets Uj according to the labeling induced by Cs: Uj = {xu i | xu i \u2208Du \u2227Cs(xu i ) = j} \u2022 Use Algorithm 1 to train a classi\ufb01er for the target task using input pairs (Uj, \u02dcpj) and the XR loss.",
            "In words, by using XR training, we use the ex- pected label proportions over the target task given predicted labels of the source task, to train a target- class classi\ufb01er. 3.2 Stochastic Batched Training for Deep XR Mann and McCallum (2007) and following work take the base classi\ufb01er p\u03b8(y|x) to be a logistic re- gression classi\ufb01er, for which they manually derive gradients for the XR loss and train with LBFGs (Byrd et al., 1995). However, nothing precludes us from using an arbitrary neural network instead, as long as it culminates in a softmax layer. One complicating factor is that the computa- tion of \u02c6qj in equation (1) requires a summation over p\u03b8(x) for the entire set Uj, which in our setup may contain hundreds of thousands of exam- ples, making gradient computation and optimiza- tion impractical.",
            "We instead proposed a stochastic batched approximation in which, instead of requir- ing that the full constraint set Uj will match the ex- pected label posterior distribution, we require that suf\ufb01ciently large random subsets of it will match work, etc. In this work, we use a neural classi\ufb01cation model. 3In theory, we could estimate\u2014or even \u201cguess\u201d\u2014these |Ys|\u00d7|Yt| values without using Dt at all. In practice, and in particular because we care about the target label proportions given noisy source labels \u02dcys assigned by Cs, we use MLE estimates over the tagged Dt. the distribution. At each training step we com- pute the loss and update the gradient with respect to a different random subset.",
            "In practice, and in particular because we care about the target label proportions given noisy source labels \u02dcys assigned by Cs, we use MLE estimates over the tagged Dt. the distribution. At each training step we com- pute the loss and update the gradient with respect to a different random subset. Speci\ufb01cally, in each training step we sample a random pair (Uj, \u02dcpj), sample a random subset U\u2032 of Uj of size k, and compute the local XR loss of set U\u2032: LXR(\u03b8; j, U\u2032) = \u2212\u02dcpj \u00b7 log \u02c6pu\u2032 (5) where \u02c6pu\u2032 is computed by summing over the ele- ments of U\u2032 rather than of Uj in equations (1\u20132). The stochastic batched XR training algorithm is given in Algorithm 1. For large enough k, the ex- pected label distribution of the subset is the same as that of the complete set. 4 Application to Aspect-based Sentiment We demonstrate the procedure given above by training Aspect-based Sentiment Classi\ufb01er (ABSC) using sentence-level4 sentiment signals.",
            "For large enough k, the ex- pected label distribution of the subset is the same as that of the complete set. 4 Application to Aspect-based Sentiment We demonstrate the procedure given above by training Aspect-based Sentiment Classi\ufb01er (ABSC) using sentence-level4 sentiment signals. 4.1 Relating the classi\ufb01cation tasks We observe that while the sentence-level senti- ment does not determine the sentiment of individ- ual aspects (a positive sentence may contain nega- tive remarks about some aspects), it is very pre- dictive of the proportion of sentiment labels of the fragments within a sentence. Positively la- beled sentences are likely to have more positive as- pects and fewer negative ones, and vice-versa for negatively-labeled sentences. While these propor- tions may vary on the individual sentence level, we expect them to be stable when aggregating fragments from several sentences: when consid- ering a large enough sample of fragments that all come from positively labeled sentences, we expect the different samples to have roughly similar label proportions to each other. This situation is idealy suited for performing XR training, as described in section 3.1.",
            "This situation is idealy suited for performing XR training, as described in section 3.1. The application to ABSC is almost straight- forward, but is complicated a bit by the decom- position of sentences into fragments: each sen- tence level decision now corresponds to multi- ple fragment-level decisions. Thus, we apply the sentence-level (task A) classi\ufb01er Cs on the aspect- level corpus Dt by applying it on the sentence level and then associating the predicted sentence labels with each of the fragments, resulting in 4In practice, our \u201csentences\u201d are in fact short documents, some of which are composed of two or more sentences.",
            "Figure 1: Illustration of the algorithm. Cs is applied to Du resulting in \u02dcy for each sentence, Uj is built according with the fragments of the same labelled sentences, the probabilities for each fragment in Uj are summed and normalized, the XR loss in equation (4) is calculated and the network is updated. Figure 2: Illustration of the decomposition procedure, when given a1=\u201cduck con\ufb01t\u201c and a2= \u201cfoie gras terrine with \ufb01gs\u201c as the pivot phrases. fragment-level labeling. Similarly, when we ap- ply Cs to the unlabeled data Du we again do it at the sentence level, but the sets Uj are composed of fragments, not sentences: Uj = {f\u03b1 i | \u03b1 \u2208Du\u2227f\u03b1 i \u2208frags(\u03b1)\u2227Cs(\u03b1) = j} We then apply algorithm 1 as is: at each step of training we sample a source label j \u2208 {POS,NEG,NEU}, sample k fragments from Uj, and use the XR loss to \ufb01t the expected fragment- label proportions over these k fragments to \u02dcpj. Figure 1 illustrates the procedure.",
            "Figure 1 illustrates the procedure. 4.2 Classi\ufb01cation Architecture We model the ABSC problem by associating each (sentence,aspect) pair with a sentence-fragment, and constructing a neural classi\ufb01er from fragments to sentiment labels. We heuristically decompose a sentence into fragments. We use the same BiL- STM based neural architecture for both sentence classi\ufb01cation and fragment classi\ufb01cation. Fragment-decomposition We now describe the procedure we use to associate a sentence fragment with each (sentence,aspect) pairs. The shared tasks data associates each aspect with a pivot- phrase a, where pivot phrase (w1, w2, ...wl) is de- \ufb01ned as a pre-determined sequence of words that is contained within the sentence. For a sentence \u03b1, a set of pivot phrases A = (a1, ..., am) and a spe- ci\ufb01c pivot phrase ai, we consult the constituency parse tree of \u03b1 and look for tree nodes that satisfy the following conditions:5 1. The node governs the desired pivot phrase ai. 2.",
            "The node governs the desired pivot phrase ai. 2. The node governs either a verb (VB, VBD, VBN, VBG, VBP, VBZ) or an adjective (JJ, JJR, JJS), which is different than any aj \u2208A. 3. The node governs a minimal number of pivot phrases from (a1, ..., am), ideally only ai. We then select the highest node in the tree that satis\ufb01es all conditions. The span governed by this node is taken as the fragment associated with as- 5Condition (2) coupled with selecting the highest node pushes towards complete phrases that contain opinions (which are usually expressed with adjectives or verbs), while the other conditions focus the attention on the desired pivot phrase.",
            "pect ai.6 The decomposition procedure is demon- strated in Figure 2. When aspect-level information is given, we take the pivot-phrases to be the requested aspects. When aspect-level information is not available, we take each noun in the sentence to be a pivot- phrase. Neural Classi\ufb01er Our classi\ufb01cation model is a simple 1-layer BiLSTM encoder (a concatenation of the last states of a forward and a backward run- ning LSTMs) followed by a linear-predictor. The encoder is fed either a complete sentence or a sen- tence fragment. 5 Experiments Data Our target task is aspect-based fragment- classi\ufb01cation, with small labeled datasets from the SemEval 2015 and 2016 shared tasks, each dataset containing aspect-level predictions for about 2000 sentences in the restaurants reviews domain. Our source classi\ufb01er is based on training on up to 10,000 sentences from the same domain and 2000 sentences for validation, labeled for only for sentence-level sentiment. We additionally have an unlabeled dataset of up to 670,000 sentences from the same domain7.",
            "Our source classi\ufb01er is based on training on up to 10,000 sentences from the same domain and 2000 sentences for validation, labeled for only for sentence-level sentiment. We additionally have an unlabeled dataset of up to 670,000 sentences from the same domain7. We tokenized all datasets us- ing the Tweet Tokenizer from NLTK package8 and parsed the tokenized sentences with AllenNLP parser.9 Training Details Both the sentence level classi- \ufb01cation models and the models trained with XR have a hidden state vector dimension of size 300, they use dropout (Hinton et al., 2012) on the sentence representation or fragment representa- tion vector (rate=0.5) and optimized using Adam (Kingma and Ba, 2014). The sentence classi\ufb01ca- tion is trained with a batch size of 30 and XR mod- els are trained with batch sizes k that each contain 450 fragments10. We used a temperature param- 6On the rare occasions where we cannot \ufb01nd such a node, we take the root node of the tree (the entire sentence) as the fragment for the given aspect.",
            "We used a temperature param- 6On the rare occasions where we cannot \ufb01nd such a node, we take the root node of the tree (the entire sentence) as the fragment for the given aspect. 7All of the sentence-level sentiment data is obtained from the Yelp dataset challenge: https:\/\/www.yelp.com\/ dataset\/challenge 8https:\/\/www.nltk.org\/ 9https:\/\/allennlp.org\/ 10We also increased the batch sizes of the baselines to match those of the XR setups. This decreased the perfor- mance of the baselines, which is consistent with the folk knowledge in the community according to which smaller batch sizes are more effective overall. eter of 111. We use pre-trained 300-dimensional GloVe embeddings12 (Pennington et al., 2014), and \ufb01ne-tune them during training.",
            "eter of 111. We use pre-trained 300-dimensional GloVe embeddings12 (Pennington et al., 2014), and \ufb01ne-tune them during training. The XR train- ing was validated with a validation set of 20% of SemEval-2015 training set, the sentence level BiL- STM classi\ufb01ers were validated with a validation of 2000 sentences.13 When \ufb01ne-tuning to the aspect based task we used 20% of train in each dataset as validation and evaluated on this set. On each train- ing method the models were evaluated on the vali- dation set, after each epoch and the best model was chosen. The data is highly imbalanced, with only very few sentences receiving a NEU label. We do not deal with this imbalance directly and train both the sentence level and the XR aspect-based train- ing on the imbalanced data. However, when train- ing Cs, we trained \ufb01ve models and chose the best model that predicts correctly at least 20% of the neutral sentences. The models are implemented using DyNet14 (Neubig et al., 2017).",
            "However, when train- ing Cs, we trained \ufb01ve models and chose the best model that predicts correctly at least 20% of the neutral sentences. The models are implemented using DyNet14 (Neubig et al., 2017). Baseline models In recent years many neural network architectures with increasing sophistica- tion were applied to the ABSC task (Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a,b; Wang et al., 2016; Zhang et al., 2016; Ruder et al., 2016; Ma et al., 2017; Liu and Zhang, 2017; Chen et al., 2017; Liu et al., 2018; Yang et al., 2018; Wang et al., 2018b,a; Fan et al., 2018a,b; Li et al., 2018; Ouyang and Su, 2018). We compare to a series of state-of-the- art ABSC neural classi\ufb01ers that participated in the shared tasks.",
            "We compare to a series of state-of-the- art ABSC neural classi\ufb01ers that participated in the shared tasks. TDLSTM-ATT (Tang et al., 2016a) encodes the information around an aspect using forward and backward LSTMs, followed by an attention mechanism. ATAE-LSTM (Wang et al., 2016) is an attention based LSTM vari- ant. MM (Tang et al., 2016b) is a deep mem- ory network with multiple-hops of attention lay- ers. RAM (Chen et al., 2017) uses multiple atten- tion mechanisms combined with a recurrent neu- ral networks and a weighted memory mechanism. LSTM+SynATT+TarRep (He et al., 2018a) is an attention based LSTM which incorporates syn- 11Despite (Mann and McCallum, 2007) claim regarding the temperature parameter, we observed lower performance when using it in our setup. However, in other setups this pa- rameter might be found to be bene\ufb01cial.",
            "However, in other setups this pa- rameter might be found to be bene\ufb01cial. 12https:\/\/nlp.stanford.edu\/projects\/ glove\/ 13We also tested the sentence BiLSTM baselines with a SemEval validation set, and received slightly lower results without a signi\ufb01cant statistical difference. 14https:\/\/github.com\/clab\/dynet",
            "Data Method SemEval-15 SemEval-16 Acc. Macro-F1 Acc. Macro-F1 A TDLSTM+ATT (Tang et al., 2016a) 77.10 59.46 83.11 57.53 A ATAE-LSTM (Wang et al., 2016) 78.48 62.84 83.77 61.71 A MM (Tang et al., 2016b) 77.89 59.52 83.04 57.91 A RAM (Chen et al., 2017) 79.98 60.57 83.88 62.14 A LSTM+SynATT+TarRep (He et al., 2018a) 81.67 66.05 84.61 67.45 S+A Semisupervised (He et al., 2018b) 81.30 68.74 85.58 69.76 S BiLSTM-104 Sentence Training 80.24 \u00b1 1.64 61.89 \u00b1 0.94 80.89 \u00b1 2.79 61.",
            ", 2018b) 81.30 68.74 85.58 69.76 S BiLSTM-104 Sentence Training 80.24 \u00b1 1.64 61.89 \u00b1 0.94 80.89 \u00b1 2.79 61.40 \u00b1 2.49 S+A BiLSTM-104 Sentence Training \u2192Aspect Based Finetuning 77.75 \u00b1 2.09 60.83 \u00b1 4.53 84.87\u00b1 0.31 61.87 \u00b1 5.44 N BiLSTM-XR-Dev Estimation 83.31\u2217\u00b1 0.62 62.24 \u00b1 0.66 87.68\u2217\u00b1 0.47 63.23 \u00b1 1.81 N BiLSTM-XR 83.31\u2217\u00b1 0.77 64.42 \u00b1 2.78 88.12\u2217\u00b1 0.24 68.60 \u00b1 1.79 N+A BiLSTM-XR \u2192Aspect Based Finetuning 83.44\u2217\u00b1 0.74 67.23 \u00b1 1.42 87.66\u2217\u00b1 0.",
            "78 88.12\u2217\u00b1 0.24 68.60 \u00b1 1.79 N+A BiLSTM-XR \u2192Aspect Based Finetuning 83.44\u2217\u00b1 0.74 67.23 \u00b1 1.42 87.66\u2217\u00b1 0.28 71.19\u2020\u00b1 1.40 Table 1: Average accuracies and Macro-F1 scores over \ufb01ve runs with random initialization along with their stan- dard deviations. Bold: best results or within std of them. \u2217indicates that the method\u2019s result is signi\ufb01cantly better than all baseline methods, \u2020 indicates that the method\u2019s result is signi\ufb01cantly better than all baselines methods that use the aspect-based data only, with p < 0.05 according to a one-tailed unpaired t-test. The data annotations S, N and A indicate training with Sentence-level, Noisy sentence-level and Aspect-level data respectively. Numbers for TDLSTM+Att,ATAE-LSTM,MM,RAM and LSTM+SynATT+TarRep are from (He et al., 2018a).",
            "The data annotations S, N and A indicate training with Sentence-level, Noisy sentence-level and Aspect-level data respectively. Numbers for TDLSTM+Att,ATAE-LSTM,MM,RAM and LSTM+SynATT+TarRep are from (He et al., 2018a). Numbers for Semisupervised are from (He et al., 2018b). tactic information into the attention mechanism and uses an auto-encoder structure to produce an aspect representations. All of these models are trained only on the small, fully-supervised ABSC datasets. \u201cSemisupervised\u201d is the semi-supervised setup of (He et al., 2018b), it trains an attention- based LSTM model on 30,000 documents addi- tional to an aspect-based train set, 10,000 doc- uments to each class. We consider additional two simple but strong semi-supervised baselines. Sentence-BiLSTM is our BiLSTM model trained on the 104 sentence-level annotations, and ap- plied as-is to the individual fragments.",
            "We consider additional two simple but strong semi-supervised baselines. Sentence-BiLSTM is our BiLSTM model trained on the 104 sentence-level annotations, and ap- plied as-is to the individual fragments. Sentence- BiLSTM+Finetuning is the same model, but \ufb01ne- tuned on the aspect-based data as explained above. Finetuning is performed using our own implemen- tation of the attention-based model of He et al. (2018b).15 Both these models are on par with the fully-supervised ABSC models.",
            "Finetuning is performed using our own implemen- tation of the attention-based model of He et al. (2018b).15 Both these models are on par with the fully-supervised ABSC models. Empirical Proportions The proportion con- straint sets \u02dcpj based on the SemEval-2015 aspect-based train data are: \u02dcpPOS = {POS : 0.93, NEG : 0.06, NEU : 0.01} \u02dcpNEG = {POS : 0.27, NEG : 0.7, NEU : 0.03} \u02dcpNEU = {POS : 0.45, NEG : 0.41, NEU : 0.14} 5.1 Main Results Table 1 compares these baselines to three XR con- ditions.16 15We changed the LSTM component to a BiLSTM. 16To be consistent with existing research (He et al., 2018b), aspects with con\ufb02icted polarity are removed.",
            "16To be consistent with existing research (He et al., 2018b), aspects with con\ufb02icted polarity are removed. The \ufb01rst condition, BiLSTM-XR-Dev, per- forms XR training on the automatically-labeled sentence-level dataset. The only access it has to aspect-level annotation is for estimating the proportions of labels for each sentence-level la- bel, which is done based on the validation set of SemEval-2015 (i.e., 20% of the train set). The XR setting is very effective: without using any in-task data, this model already surpasses all other mod- els, both supervised and semi-supervised, except for the (He et al., 2018b,a) models which achieve higher F1 scores. We note that in contrast to XR, the competing models have complete access to the supervised aspect-based labels. The second con- dition, BiLSTM-XR, is similar but now the model is allowed to estimate the conditional label pro- portions based on the entire aspect-based training set (the classi\ufb01er still does not have direct access to the labels beyond the aggregate proportion in- formation).",
            "The second con- dition, BiLSTM-XR, is similar but now the model is allowed to estimate the conditional label pro- portions based on the entire aspect-based training set (the classi\ufb01er still does not have direct access to the labels beyond the aggregate proportion in- formation). This improves results further, show- ing the importance of accurately estimating the proportions. Finally, in BiLSTM-XR+Finetuning, we follow the XR training with fully supervised \ufb01ne-tuning on the small labeled dataset, using the attention-based model of He et al. (2018b). This achieves the best results, and surpasses also the semi-supervised He et al. (2018b) baseline on ac- curacy, and matching it on F1.17 We report signi\ufb01cance tests for the robustness 17We note that their setup uses clean and more balanced annotations, i.e. they use 10,000 samples for each label, which helps predicting the infrequent neutral sentiment.",
            "they use 10,000 samples for each label, which helps predicting the infrequent neutral sentiment. We however, use noisy sentence sentiment labels which are auto- matically obtained from a trained classi\ufb01er, which trains on 10,000 samples in their natural imbalanced distribution.",
            "of the method under random parameter initializa- tion. Our reported numbers are averaged over \ufb01ve random initialization. Since the datasets are un- balanced w.r.t the label distribution, we report both accuracy and macro-F1. The XR training is also more stable than the other semi-supervised baselines, achieving sub- stantially lower standard deviations across differ- ent runs. 5.2 Further experiments In each experiment in this section we estimate the proportions using the SemEval-2015 train set. Effect of unlabeled data size How does the XR training scale with the amount of unlabeled data? Figure 3a shows the macro-F1 scores on the entire SemEval-2016 dataset, with different unlabeled corpus sizes (measured in number of sentences). An unannotated corpus of 5\u00d7104 sentences is suf- \ufb01cient to surpass the results of the 104 sentence- level trained classi\ufb01er, and more unannotated data further improves the results. Effect of Base-classi\ufb01er Quality Our method requires a sentence level classi\ufb01er Cs to label both the target-task corpus and the unlabeled corpus.",
            "Effect of Base-classi\ufb01er Quality Our method requires a sentence level classi\ufb01er Cs to label both the target-task corpus and the unlabeled corpus. How does the quality of this classi\ufb01er affect the overall XR training? We vary the amount of super- vision used to train Cs from 0 sentences (assigning the same label to all sentences), to 100, 1000, 5000 and 10000 sentences. We again measure macro-F1 on the entire SemEval 2016 corpus. The results in Figure 3b show that when using the prior distributions of aspects (0), the model struggles to learn from this signal, it learns mostly to predict the majority class, and hence reaches very low F1 scores of 35.28.",
            "The results in Figure 3b show that when using the prior distributions of aspects (0), the model struggles to learn from this signal, it learns mostly to predict the majority class, and hence reaches very low F1 scores of 35.28. The more data given to the sentence level classi\ufb01er, the better the poten- tial results will be when training with our method using the classi\ufb01er labels, with a classi\ufb01ers trained on 100,1000,5000 and 10000 labeled sentences, we get a F1 scores of 53.81, 58.84, 61.81, 65.58 respectively. Improvements in the source task classi\ufb01er\u2019s quality clearly contribute to the target task accuracy. Effect of k The Stochastic Batched XR algo- rithm (Algorithm 1) samples a batch of k examples at each step to estimate the posterior label distri- bution used in the loss computation. How does the size of k affect the results? We use k = 450 frag- ments in our main experiments, but smaller values of k reduce GPU memory load and may train bet- ter in practice.",
            "How does the size of k affect the results? We use k = 450 frag- ments in our main experiments, but smaller values of k reduce GPU memory load and may train bet- ter in practice. We tested our method with varying values of k on a sample of 5 \u00d7 104, using batches that are composed of fragments of 5, 25, 100, 450, 1000 and 4500 sentences. The results are shown in Figure 3c. Setting k = 5 result in low scores. Setting k = 25 yields better F1 score but with high variance across runs. For k = 100 fragments the results begin to stabilize, we also see a slight de- crease in F1-scores with larger batch sizes. We attribute this drop despite having better estimation of the gradients to the general trend of larger batch sizes being harder to train with stochastic gradient methods. 5.3 Pre-training, BERT The XR training can be performed also over pre- trained representations. We experiment with two pre-training methods: (1) pre-training by training the BiLSTM model to predict the noisy sentence- level predictions.",
            "5.3 Pre-training, BERT The XR training can be performed also over pre- trained representations. We experiment with two pre-training methods: (1) pre-training by training the BiLSTM model to predict the noisy sentence- level predictions. (2) Using the pre-trained BERT representation (Devlin et al., 2018). For (1), we compare the effect of pre-train on unlabeled cor- pora of sizes of 5 \u00d7 104, 105 and 6.7 \u00d7 105 sen- tences. Results in Figure 3d show that this form of pre-training is effective for smaller unlabeled cor- pora but evens out for larger ones. BERT For the BERT experiments, we experi- ment with the BERT-base model18 with k = 450 sets, 30 epochs for XR training or sentence level \ufb01ne-tuning19 and 15 epochs for aspect based \ufb01ne- tuning, on each training method we evaluated the model on the dev set after each epoch and the best model was chosen20. We compare the following setups: -BERT\u2192Aspect Based Finetuning: pretrained BERT model \ufb01netuned to the aspect based task.",
            "We compare the following setups: -BERT\u2192Aspect Based Finetuning: pretrained BERT model \ufb01netuned to the aspect based task. -BERT\u2192104: A pretrained BERT model \ufb01netuned to the sentence level task on the 104 sentences, and tested by predicting fragment-level sentiment. -BERT\u2192104\u2192Aspect Based Finetuning: pre- trained BERT model \ufb01netuned to the sentence level task, and \ufb01netuned again to the aspect based one. -BERT\u2192XR: pretrained BERT model followed by 18We could not \ufb01t k = 450 sets of BERT-large on our GPU. 19When \ufb01ne-tuning to the sentence level task, we provide the sentence as input. When \ufb01ne-tuning to the aspect-level task, we provide the sentence, a seperator and then the aspect. 20The other con\ufb01guration parameters were the de- fault ones in https:\/\/github.com\/huggingface\/ pytorch-pretrained-BERT",
            "(a) (b) (c) (d) Figure 3: Macro-F1 scores for the entire SemEval-2016 dataset of the different analyses. (a) the contribution of unlabeled data. (b) the effect of sentence classi\ufb01er quality. (c) the effect of k. (d) the effect of sentence-level pretraining vs. corpus size. Data Training SemEval-15 SemEval-16 Acc. Macro-F1 Acc. Macro-F1 N BiLSTM-XR 83.31 \u00b1 0.77 64.42 \u00b1 2.78 88.12 \u00b1 0.24 68.60 \u00b1 1.79 N+A BiLSTM-XR \u2192Aspect Based Finetuning 83.44 \u00b1 0.74 67.23 \u00b1 1.42 87.66 \u00b1 0.28 71.19 \u00b1 1.40 A BERT\u2192Aspect Based Finetuning 81.87 \u00b1 1.12 59.24 \u00b1 4.94 85.81 \u00b1 1.07 62.46 \u00b1 6.76 S BERT\u2192104 Sent Finetuning 83.",
            "19 \u00b1 1.40 A BERT\u2192Aspect Based Finetuning 81.87 \u00b1 1.12 59.24 \u00b1 4.94 85.81 \u00b1 1.07 62.46 \u00b1 6.76 S BERT\u2192104 Sent Finetuning 83.29 \u00b1 0.77 66.79 \u00b1 1.99 84.53 \u00b1 1.66 65.53 \u00b1 3.03 S+A BERT\u2192104 Sent Finetuning \u2192Aspect Based Finetuning 82.54 \u00b1 1.21 64.13 \u00b1 5.05 85.67 \u00b1 1.14 64.13 \u00b1 7.07 N BERT\u2192XR 85.46\u2217\u00b1 0.59 66.86 \u00b1 2.8 89.5\u2217\u00b1 0.55 70.86\u2020\u00b1 2.96 N+A BERT\u2192XR \u2192Aspect Based Finetuning 85.78\u2217\u00b1 0.65 68.74 \u00b1 1.36 89.57\u2217\u00b1 1.4 73.89\u2217\u00b1 2.",
            "55 70.86\u2020\u00b1 2.96 N+A BERT\u2192XR \u2192Aspect Based Finetuning 85.78\u2217\u00b1 0.65 68.74 \u00b1 1.36 89.57\u2217\u00b1 1.4 73.89\u2217\u00b1 2.05 Table 2: BERT pre-training: average accuracies and Macro-F1 scores from \ufb01ve runs and their stdev. \u2217indicates that the method\u2019s result is signi\ufb01cantly better than all baseline methods, \u2020 indicates that the method\u2019s result is signi\ufb01cantly better than all non XR baseline methods, with p < 0.05 according to a one-tailed unpaired t-test. The data annotations S, N and A indicate training with Sentence-level, Noisy sentence-level and Aspect-level data respectively. XR training using our method. -BERT\u2192XR \u2192Aspect Based Finetuning: pre- trained BERT followed by XR training and then \ufb01ne-tuned to the aspect level task. The results are presented in Table 2.",
            "XR training using our method. -BERT\u2192XR \u2192Aspect Based Finetuning: pre- trained BERT followed by XR training and then \ufb01ne-tuned to the aspect level task. The results are presented in Table 2. As before, aspect-based \ufb01ne-tuning is bene\ufb01cial for both SemEval-16 and SemEval-15. Training a BiL- STM with XR surpasses pre-trained BERT models and using XR training on top of the pre-trained BERT models substantially increases the results even further. 6 Discussion We presented a transfer learning method based on expectation regularization (XR), and demon- strated its effectiveness for training aspect-based sentiment classi\ufb01ers using sentence-level supervi- sion. The method achieves state-of-the-art results for the task, and is also effective for improving on top of a strong pre-trained BERT model. The pro- posed method provides an additional data-ef\ufb01cient tool in the modeling arsenal, which can be ap- plied on its own or together with another training method, in situations where there is a conditional relations between the labels of a source task for which we have supervision, and a target task for which we don\u2019t.",
            "While we demonstrated the approach on the sentiment domain, the required conditional depen- dence between task labels is present in many sit- uations. Other possible application of the method includes training language identi\ufb01cation of tweets given geo-location supervision (knowing the geo- graphical region gives a prior on languages spo- ken), training predictors for renal failure from tex- tual medical records given classi\ufb01er for diabetes (there is a strong correlation between the two con- ditions), training a political af\ufb01liation classi\ufb01er from social media tweets based on age-group clas- si\ufb01ers, zip-code information, or social-status clas- si\ufb01ers (there are known correlations between all of these to political af\ufb01liation), training hate-speech detection based on emotion detection, and so on. Acknowledgements The work was supported in part by The Israeli Sci- ence Foundation (grant number 1555\/15).",
            "References Richard H. Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. 1995. A limited memory algorithm for bound constrained optimization. SIAM J. Scienti\ufb01c Computing, 16(5):1190\u20131208. Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007. Guiding semi-supervision with constraint-driven learning. In Proceedings of the 45th Annual Meet- ing of the Association of Computational Linguistics, pages 280\u2013287. Association for Computational Lin- guistics. Ming-Wei Chang, Lev-Arie Ratinov, and Dan Roth. 2012. Structured learning with constrained condi- tional models. Machine Learning, 88(3):399\u2013431. Peng Chen, Zhongqian Sun, Lidong Bing, and Wei Yang. 2017. Recurrent attention network on mem- ory for aspect sentiment analysis. In Proceedings of the 2017 Conference on Empirical Methods in Nat- ural Language Processing, pages 452\u2013461. Associ- ation for Computational Linguistics.",
            "2017. Recurrent attention network on mem- ory for aspect sentiment analysis. In Proceedings of the 2017 Conference on Empirical Methods in Nat- ural Language Processing, pages 452\u2013461. Associ- ation for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. CoRR, abs\/1810.04805. Gregory Druck, Gideon S. Mann, and Andrew McCal- lum. 2008. Learning from labeled features using generalized expectation criteria. In Proceedings of the 31st Annual International ACM SIGIR Confer- ence on Research and Development in Information Retrieval, SIGIR 2008, Singapore, July 20-24, 2008, pages 595\u2013602. Chuang Fan, Qinghong Gao, Jiachen Du, Lin Gui, Ruifeng Xu, and Kam-Fai Wong. 2018a. Convolution-based memory network for aspect- based sentiment analysis.",
            "Chuang Fan, Qinghong Gao, Jiachen Du, Lin Gui, Ruifeng Xu, and Kam-Fai Wong. 2018a. Convolution-based memory network for aspect- based sentiment analysis. In The 41st International ACM SIGIR Conference on Research & Develop- ment in Information Retrieval, SIGIR 2018, Ann Ar- bor, MI, USA, July 08-12, 2018, pages 1161\u20131164. Feifan Fan, Yansong Feng, and Dongyan Zhao. 2018b. Multi-grained attention network for aspect-level sentiment classi\ufb01cation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing, pages 3433\u20133442. Association for Computational Linguistics. Kuzman Ganchev and Dipanjan Das. 2013. Cross- lingual discriminative learning of sequence models with posterior regularization. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1996\u20132006. Associa- tion for Computational Linguistics.",
            "2013. Cross- lingual discriminative learning of sequence models with posterior regularization. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1996\u20132006. Associa- tion for Computational Linguistics. Kuzman Ganchev, Jo\u02dcao Grac\u00b8a, Jennifer Gillenwater, and Ben Taskar. 2010. Posterior regularization for structured latent variable models. Journal of Ma- chine Learning Research, 11:2001\u20132049. Jo\u02dcao Grac\u00b8a, Kuzman Ganchev, and Ben Taskar. 2007. Expectation maximization and posterior constraints. In Advances in Neural Information Processing Sys- tems 20, Proceedings of the Twenty-First Annual Conference on Neural Information Processing Sys- tems, Vancouver, British Columbia, Canada, De- cember 3-6, 2007, pages 569\u2013576. Aria Haghighi and Dan Klein. 2006. Prototype-driven learning for sequence models.",
            "Aria Haghighi and Dan Klein. 2006. Prototype-driven learning for sequence models. In Human Lan- guage Technology Conference of the North Amer- ican Chapter of the Association of Computational Linguistics, Proceedings, June 4-9, 2006, New York, New York, USA. Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel Dahlmeier. 2018a. Effective attention modeling for aspect-level sentiment classi\ufb01cation. In Proceed- ings of the 27th International Conference on Com- putational Linguistics, pages 1121\u20131131. Associa- tion for Computational Linguistics. Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel Dahlmeier. 2018b. Exploiting document knowledge for aspect-level sentiment classi\ufb01cation. In Pro- ceedings of the 56th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 2: Short Papers), pages 579\u2013585. Association for Computa- tional Linguistics.",
            "In Pro- ceedings of the 56th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 2: Short Papers), pages 579\u2013585. Association for Computa- tional Linguistics. Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhut- dinov. 2012. Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs\/1207.0580. Tom Hope and Dafna Shahaf. 2016. Ballpark learn- ing: Estimating labels from rough group compar- isons. In Machine Learning and Knowledge Dis- covery in Databases - European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19- 23, 2016, Proceedings, Part II, pages 299\u2013314. Rong Jin and Yi Liu. 2005. A framework for in- corporating class priors into discriminative classi\ufb01- cation.",
            "Rong Jin and Yi Liu. 2005. A framework for in- corporating class priors into discriminative classi\ufb01- cation. In Advances in Knowledge Discovery and Data Mining, 9th Paci\ufb01c-Asia Conference, PAKDD 2005, Hanoi, Vietnam, May 18-20, 2005, Proceed- ings, pages 568\u2013577. Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. CoRR, abs\/1412.6980. Lishuang Li, Yang Liu, and AnQiao Zhou. 2018. Hi- erarchical attention based position-aware network for aspect-level sentiment analysis. In Proceedings of the 22nd Conference on Computational Natural Language Learning, pages 181\u2013189. Association for Computational Linguistics. Percy Liang, Michael I. Jordan, and Dan Klein. 2009. Learning from measurements in exponential fam- ilies. In Proceedings of the 26th Annual Inter- national Conference on Machine Learning, ICML",
            "2009, Montreal, Quebec, Canada, June 14-18, 2009, pages 641\u2013648. Fei Liu, Trevor Cohn, and Timothy Baldwin. 2018. Re- current entity networks with delayed memory update for targeted aspect-based sentiment analysis. In Pro- ceedings of the 2018 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol- ume 2 (Short Papers), pages 278\u2013283. Association for Computational Linguistics. Jiangming Liu and Yue Zhang. 2017. Attention mod- eling for targeted sentiment. In Proceedings of the 15th Conference of the European Chapter of the As- sociation for Computational Linguistics: Volume 2, Short Papers, pages 572\u2013577. Association for Com- putational Linguistics. Dehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng Wang. 2017. Interactive attention networks for aspect-level sentiment classi\ufb01cation.",
            "Association for Com- putational Linguistics. Dehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng Wang. 2017. Interactive attention networks for aspect-level sentiment classi\ufb01cation. In Proceed- ings of the Twenty-Sixth International Joint Con- ference on Arti\ufb01cial Intelligence, IJCAI 2017, Mel- bourne, Australia, August 19-25, 2017, pages 4068\u2013 4074. Gideon S. Mann and Andrew McCallum. 2007. Sim- ple, robust, scalable semi-supervised learning via expectation regularization. In Machine Learn- ing, Proceedings of the Twenty-Fourth International Conference (ICML 2007), Corvallis, Oregon, USA, June 20-24, 2007, pages 593\u2013600. Gideon S. Mann and Andrew McCallum. 2010a. Gen- eralized expectation criteria for semi-supervised learning with weakly labeled data. Journal of Ma- chine Learning Research, 11:955\u2013984. Gideon S. Mann and Andrew McCallum.",
            "2010a. Gen- eralized expectation criteria for semi-supervised learning with weakly labeled data. Journal of Ma- chine Learning Research, 11:955\u2013984. Gideon S. Mann and Andrew McCallum. 2010b. Gen- eralized expectation criteria for semi-supervised learning with weakly labeled data. Journal of Ma- chine Learning Research, 11:955\u2013984. Prem Melville, Wojciech Gryc, and Richard D. Lawrence. 2009. Sentiment analysis of blogs by combining lexical knowledge with text classi\ufb01ca- tion. In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Paris, France, June 28 - July 1, 2009, pages 1275\u20131284. Ardehaly Ehsan Mohammady and Aron Culotta. 2015. Inferring latent attributes of twitter users with label regularization. In Proceedings of the 2015 Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies, pages 185\u2013195.",
            "2015. Inferring latent attributes of twitter users with label regularization. In Proceedings of the 2015 Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies, pages 185\u2013195. Association for Computational Linguistics. David R. Musicant, Janara M. Christensen, and Jamie F. Olson. 2007. Supervised learning by train- ing on aggregate outputs. In Proceedings of the 7th IEEE International Conference on Data Mining (ICDM 2007), October 28-31, 2007, Omaha, Ne- braska, USA, pages 252\u2013261.",
            "2007. Supervised learning by train- ing on aggregate outputs. In Proceedings of the 7th IEEE International Conference on Data Mining (ICDM 2007), October 28-31, 2007, Omaha, Ne- braska, USA, pages 252\u2013261. Graham Neubig, Chris Dyer, Yoav Goldberg, Austin Matthews, Waleed Ammar, Antonios Anastasopou- los, Miguel Ballesteros, David Chiang, Daniel Clothiaux, Trevor Cohn, Kevin Duh, Manaal Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji, Lingpeng Kong, Adhiguna Kuncoro, Gaurav Ku- mar, Chaitanya Malaviya, Paul Michel, Yusuke Oda, Matthew Richardson, Naomi Saphra, Swabha Swayamdipta, and Pengcheng Yin. 2017. Dynet: The dynamic neural network toolkit. CoRR, abs\/1701.03980. Thien Hai Nguyen and Kiyoaki Shirai. 2015. Phrasernn: Phrase recursive neural network for aspect-based sentiment analysis.",
            "2017. Dynet: The dynamic neural network toolkit. CoRR, abs\/1701.03980. Thien Hai Nguyen and Kiyoaki Shirai. 2015. Phrasernn: Phrase recursive neural network for aspect-based sentiment analysis. In Proceedings of the 2015 Conference on Empirical Methods in Nat- ural Language Processing, pages 2509\u20132514. Asso- ciation for Computational Linguistics. Zhifan Ouyang and Jindian Su. 2018. Dependency parsing and attention network for aspect-level sen- timent classi\ufb01cation. In Natural Language Process- ing and Chinese Computing - 7th CCF International Conference, NLPCC 2018, Hohhot, China, August 26-30, 2018, Proceedings, Part I, pages 391\u2013403. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543.",
            "Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543. Associa- tion for Computational Linguistics. Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Ion Androutsopoulos, Suresh Manandhar, Moham- mad AL-Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing Qin, Orphee De Clercq, Veronique Hoste, Marianna Apidianaki, Xavier Tannier, Na- talia Loukachevitch, Evgeniy Kotelnikov, N\u00b4uria Bel, Salud Mar\u00b4\u0131a Jim\u00b4enez-Zafra, and G\u00a8uls\u00b8en Eryi\u02d8git. 2016. Semeval-2016 task 5: Aspect based senti- ment analysis. In Proceedings of the 10th Interna- tional Workshop on Semantic Evaluation (SemEval- 2016), pages 19\u201330.",
            "2016. Semeval-2016 task 5: Aspect based senti- ment analysis. In Proceedings of the 10th Interna- tional Workshop on Semantic Evaluation (SemEval- 2016), pages 19\u201330. Association for Computational Linguistics. Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Suresh Manandhar, and Ion Androutsopoulos. 2015. Semeval-2015 task 12: Aspect based sentiment anal- ysis. In Proceedings of the 9th International Work- shop on Semantic Evaluation (SemEval 2015), pages 486\u2013495. Association for Computational Linguis- tics. Novi Quadrianto, Alexander J. Smola, Tib\u00b4erio S. Cae- tano, and Quoc V. Le. 2009a. Estimating labels from label proportions. Journal of Machine Learning Re- search, 10:2349\u20132374. Novi Quadrianto, Alexander J. Smola, Tib\u00b4erio S. Cae- tano, and Quoc V. Le. 2009b.",
            "Estimating labels from label proportions. Journal of Machine Learning Re- search, 10:2349\u20132374. Novi Quadrianto, Alexander J. Smola, Tib\u00b4erio S. Cae- tano, and Quoc V. Le. 2009b. Estimating labels from label proportions. Journal of Machine Learning Re- search, 10:2349\u20132374.",
            "Sebastian Ruder, Parsa Ghaffari, and John G. Breslin. 2016. A hierarchical model of reviews for aspect- based sentiment analysis. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 999\u20131005. Association for Computational Linguistics. Robert E. Schapire, Marie Rochery, Mazin G. Rahim, and Narendra K. Gupta. 2002. Incorporating prior knowledge into boosting. In Machine Learning, Proceedings of the Nineteenth International Confer- ence (ICML 2002), University of New South Wales, Sydney, Australia, July 8-12, 2002, pages 538\u2013545. Duyu Tang, Bing Qin, Xiaocheng Feng, and Ting Liu. 2016a. Effective lstms for target-dependent sen- timent classi\ufb01cation. In Proceedings of COLING 2016, the 26th International Conference on Compu- tational Linguistics: Technical Papers, pages 3298\u2013 3307. The COLING 2016 Organizing Committee.",
            "In Proceedings of COLING 2016, the 26th International Conference on Compu- tational Linguistics: Technical Papers, pages 3298\u2013 3307. The COLING 2016 Organizing Committee. Duyu Tang, Bing Qin, and Ting Liu. 2016b. Aspect level sentiment classi\ufb01cation with deep memory net- work. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Process- ing, pages 214\u2013224. Association for Computational Linguistics. Duy-Tin Vo and Yue Zhang. 2015. Target-dependent twitter sentiment classi\ufb01cation with rich automatic features. In Proceedings of the Twenty-Fourth Inter- national Joint Conference on Arti\ufb01cial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages 1347\u20131353. Jingjing Wang, Jie Li, Shoushan Li, Yangyang Kang, Min Zhang, Luo Si, and Guodong Zhou. 2018a. As- pect sentiment classi\ufb01cation with both word-level and clause-level attention networks.",
            "Jingjing Wang, Jie Li, Shoushan Li, Yangyang Kang, Min Zhang, Luo Si, and Guodong Zhou. 2018a. As- pect sentiment classi\ufb01cation with both word-level and clause-level attention networks. In Proceed- ings of the Twenty-Seventh International Joint Con- ference on Arti\ufb01cial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden., pages 4439\u20134445. Mengqiu Wang and Christopher D. Manning. 2014. Cross-lingual projected expectation regularization for weakly supervised learning. TACL, 2:55\u201366. Shuai Wang, Sahisnu Mazumder, Bing Liu, Mianwei Zhou, and Yi Chang. 2018b. Target-sensitive mem- ory networks for aspect sentiment classi\ufb01cation. In Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), pages 957\u2013967. Association for Com- putational Linguistics.",
            "Target-sensitive mem- ory networks for aspect sentiment classi\ufb01cation. In Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), pages 957\u2013967. Association for Com- putational Linguistics. Yequan Wang, Minlie Huang, xiaoyan zhu, and Li Zhao. 2016. Attention-based lstm for aspect-level sentiment classi\ufb01cation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Lan- guage Processing, pages 606\u2013615. Association for Computational Linguistics. Zuoguan Wang, Siwei Lyu, Gerwin Schalk, and Qiang Ji. 2012. Learning with target prior. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Process- ing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States., pages 2240\u20132248. Min Yang, Qiang Qu, Xiaojun Chen, Chaoxue Guo, Ying Shen, and Kai Lei.",
            "Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States., pages 2240\u20132248. Min Yang, Qiang Qu, Xiaojun Chen, Chaoxue Guo, Ying Shen, and Kai Lei. 2018. Feature-enhanced at- tention network for target-dependent sentiment clas- si\ufb01cation. Neurocomputing, 307:91\u201397. Meishan Zhang, Yue Zhang, and Duy-Tin Vo. 2016. Gated neural networks for targeted sentiment anal- ysis. In Proceedings of the Thirtieth AAAI Con- ference on Arti\ufb01cial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA., pages 3087\u20133093. Jun Zhu, Ning Chen, and Eric P. Xing. 2014. Bayesian inference with posterior regularization and applica- tions to in\ufb01nite latent svms. Journal of Machine Learning Research, 15(1):1799\u20131847."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1909.00430.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 12850.999923706055,
    "avg_doclen_est": 166.89610290527344
}
