{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Cross-Lingual Cross-Corpus Speech Emotion Recognition Shivali Goel Homayoon Beigi Department of Computer Science, Columbia University {sg3629, hb87}@columbia.edu Abstract The majority of existing speech emotion recognition models are trained and evaluated on a single corpus and a single language set- ting. These systems do not perform as well when applied in a cross-corpus and cross- language scenario. This paper presents re- sults for speech emotion recognition for 4 lan- guages in both single corpus and cross corpus setting. Additionally, since multi-task learning (MTL) with gender, naturalness and arousal as auxiliary tasks has shown to enhance the gen- eralisation capabilities of the emotion models, this paper introduces language ID as another auxiliary task in MTL framework to explore the role of spoken language on emotion recog- nition which has not been studied yet. Index Terms: speech emotion recognition, cross-corpus, cross-lingual 1 Introduction Speech conveys human emotions most naturally. In recent years there has been an increased re- search interest in speech emotion recognition do- main.",
      "Index Terms: speech emotion recognition, cross-corpus, cross-lingual 1 Introduction Speech conveys human emotions most naturally. In recent years there has been an increased re- search interest in speech emotion recognition do- main. The \ufb01rst step in a typical SER sys- tem is extracting linguistic and acoustic features from speech signal. Some para-linguistic studies \ufb01nd Low-Level Descriptor (LLD) features of the speech signal to be most relevant to studying emo- tions in speech. These features include frequency related parameters like pitch and jitter, energy pa- rameters like shimmer and loudness, spectral pa- rameters like alpha ratio and other parameters that convey cepstral and dynamic information. Feature extraction is followed with a classi\ufb01cation task to predict the emotions of the speaker. Data scarcity or lack of free speech corpus is a problem for research in speech domain in gen- eral. This also means that there are even fewer re- sources for studying emotion in speech. For those that are available are dissimilar in terms of the spo- ken language, type of emotion (i.e. naturalistic, elicited, or acted) and labelling scheme (i.e.",
      "This also means that there are even fewer re- sources for studying emotion in speech. For those that are available are dissimilar in terms of the spo- ken language, type of emotion (i.e. naturalistic, elicited, or acted) and labelling scheme (i.e. di- mensional or categorical). Across various studies involving SER we ob- serve that performance of model depends heav- ily on whether training and testing is performed from the same corpus or not. Performance is best when focus is on a single corpus at a time, without considering the performance of model in cross- language and cross-corpus scenarios. In this work, we work with diverse SER datasets i.e. tackle the problem in both cross-language and cross-corpus setting. We use transfer learning across SER datasets and investigate the effects of language spoken on the accuracy of the emotion recognition system using our Multi-Task Learning framework.",
      "In this work, we work with diverse SER datasets i.e. tackle the problem in both cross-language and cross-corpus setting. We use transfer learning across SER datasets and investigate the effects of language spoken on the accuracy of the emotion recognition system using our Multi-Task Learning framework. The paper is organized as follows: Section 2 reviewed related work on SER, cross-lingual and cross-corpus SER and the recent studies on role of language identi\ufb01cation in speech emotion recogni- tion system, Section 3 describes the datasets that have been used, Section 4 presents detailed de- scriptions of three types of SER experiments we conduct in this paper. In Section 5, we present our results and evaluations of our models. Section 6 presents some additional experiments to draw a direct comparison with previously published re- search. Finally, we discuss future work and con- clude the paper. 2 Related Work Over the last two decades there have been consid- erable research work on speech emotion recogni- tion. However, all these differ in terms of the train- ing corpora, test conditions, evaluation strategies and more which create dif\ufb01culty in reproducing exact results.",
      "2 Related Work Over the last two decades there have been consid- erable research work on speech emotion recogni- tion. However, all these differ in terms of the train- ing corpora, test conditions, evaluation strategies and more which create dif\ufb01culty in reproducing exact results. In (Schuller et al., 2009a), the au- thors give an overview of types of features, classi- arXiv:2003.07996v1  [cs.SD]  18 Mar 2020",
      "Dataset Language Utterances #Emotion categories Emotion labels EMO-DB German 494 7 Anger, Sadness, Fear, Disgust, Boredom, Neutral, Happiness SAVEE English 480 7 Anger, Sadness, Fear, Disgust, Neutral, Happiness, Surprise EMOVO Italian 588 7 Anger, Sadness, Fear, Disgust, Neutral, Joy, Surprise MASC Chinese 25636 5 Anger, Sadness, Panic, Neutral, Elation IEMOCAP English scripted: 5255 turns; 9 Anger, Happiness, Excitement, Sadness, Frustration, Fear, spontaneous: 4784 turns Surprise, Other and Neutral Table 1: Datasets used for various SER experiments.",
      "Feature Set Classi\ufb01er Dataset EMODB EMOVO SAVEE IEMOCAP MANDARIN MFCC LSTM 44.19 30 35.56 50.48 41.64 IS09 Emotion Logistic Regression 85 38 46 58 49 SVC 88.37 35.71 55.55 61.20 58 LSTM 86.05 27.14 55.56 55.09 50.40 Table 2: SER performance for each of the 5 datasets using different feature sets and classi\ufb01ers \ufb01ers and emotional speech databases used in vari- ous SER research. Speech emotion recognition has evolved over time with regards to both the type of features and models used for classi\ufb01ers. Different types of features that can be used can involve simple features like pitch and intensity (Rychlicki-Kicior and Stasiak, 2014; Noroozi et al., 2017).",
      "Different types of features that can be used can involve simple features like pitch and intensity (Rychlicki-Kicior and Stasiak, 2014; Noroozi et al., 2017). Some studies use low-level descriptor features(LLDs) like jitter, shimmer, HNR and spectral/cepstral pa- rameters like alpha ratio (Lugger and Yang, 2007; Vlasenko et al., 2007). Other features include rhythm and sentence duration (Jin et al., 2009) and non-uniform perceptual linear predictive (UN- PLP) features (Zhou et al., 2009). Sometimes, lin- ear predictive cepstral coef\ufb01cients(LPCCs) (Mao et al., 2009) are used in conjunction with mel- frequency cepstral coef\ufb01cients (MFCCs). There have been studies on SER in languages other than english. For example, (Zhou et al., 2016) propose a deep learning model consisting of stacked auto-encoders and deep belief networks for SER on the famous German dataset EMODB.",
      "There have been studies on SER in languages other than english. For example, (Zhou et al., 2016) propose a deep learning model consisting of stacked auto-encoders and deep belief networks for SER on the famous German dataset EMODB. (Shaukat and Chen, 2008) were the \ufb01rst to study SER work on the GEES, a Serbian emotional speech corpus. The authors developed a multi- stage strategy with SVMs for emotion recognition on a single dataset. Relatively fewer studies address the problem of cross-language and cross-corpus speech emotion recognition. (Schuller et al., 2011, 2010). Recent work by (Latif et al., 2018, 2019) studies SER for languages belonging to different language fami- lies like Urdu vs. Italian or German. Other work involving cross-language emotion recognition in- cludes (Xiao et al., 2016) which studies speech emotion recognition for for mandarin language vs. western languages like German and Danish. (Al- bornoz and Milone, 2017) developed an ensemble SVM for emotion detection with a focus on emo- tion recognition in unseen languages.",
      "western languages like German and Danish. (Al- bornoz and Milone, 2017) developed an ensemble SVM for emotion detection with a focus on emo- tion recognition in unseen languages. Although there are a lot of psychological case studies on the effect of language and culture in SER, there are very few computational linguis- tic studies in the same domain. In (Rajoo and Aun, 2016), the authors support the fact that SER is language independent, however also reveal that there are language speci\ufb01c differences in emo- tion recognition in which English shows a higher recognition rate compared to Malay and Man- darin. In (Heracleous and Yoneyama, 2019) the authors proposed two-pass method based on lan- guage identi\ufb01cation and then emotion recogni- tion. It showed signi\ufb01cant improvement in perfor- mance. They used English IEMOCAP, the Ger- man Emo-DB, and a Japanese corpus to recog- nize four emotions based on the proposed two- pass method.",
      "It showed signi\ufb01cant improvement in perfor- mance. They used English IEMOCAP, the Ger- man Emo-DB, and a Japanese corpus to recog- nize four emotions based on the proposed two- pass method. In (Sagha et al., 2016), the authors also use lan- guage identi\ufb01cation to enhance cross-lingual SER. They concluded that in order to recognize the emo- tions of a speaker whose language is unknown, it is bene\ufb01cial to use a language identi\ufb01er followed by model selection instead of using a model which is trained based on all available languages. This work is to the best of our knowledge the \ufb01rst work",
      "Feature Set Classi\ufb01er Train Test EMODB EMOVO SAVEE IS09 Emotion LSTM Train on IEMOCAP 0.4651 0.3571 0.4555 Fine-tune on smaller dataset 0.8372 0.3142 0.5555 Table 3: Transfer learning for small datasets. Row 1: Training on large English corpus, testing on test sets of small corpses. Row 2: Fine-tune base English model on say EMODB train set and test on EMODB test set Feature Set Classi\ufb01er Test EMODB EMOVO SAVEE IEMOCAP MANDARIN MFCC LSTM (only predict emotion) 58.14 21.43 34.44 50.80 43.37 Multi-task LSTM (predict both emotion and language ID) 53.48 28.00 33.30 50.69 43.10 Table 4: Multitask-learning. Table only shows accuracy scores for emotion recognition. Model always predicted language ID with very high accuracy(>97%). that jointly tries to learn the language and emotion in speech.",
      "Table only shows accuracy scores for emotion recognition. Model always predicted language ID with very high accuracy(>97%). that jointly tries to learn the language and emotion in speech. 3 Datasets EMO-DB This dataset was introduced by (Burkhardt et al., 2005). Language of recordings is German and consists of acted speech with 7 cat- egorical labels. The semantic content in this data is pre-de\ufb01ned in 10 emotionally neutral German short sentences. It contains 494 emotionally la- beled phrases collected from 5 male and 5 female actors in age range of 21-35 years. SAVEE Surrey Audio-Visual Expressed Emo- tion (SAVEE) database (Jackson and ul haq, 2011) is a famous acted-speech multimodal corpus. It consists of 480 British English utterances from 4 male actors in 7 different emotion categories. The text material consisted of 15 TIMIT (Garofolo et al., 1993) sentences per emotion: 3 common, 2 emotion-speci\ufb01c and 10 generic sentences that were different for each emotion and phonetically- balanced.",
      "The text material consisted of 15 TIMIT (Garofolo et al., 1993) sentences per emotion: 3 common, 2 emotion-speci\ufb01c and 10 generic sentences that were different for each emotion and phonetically- balanced. EMOVO This (Costantini et al., 2014) is an Ital- ian language acted speech emotional corpus that contains recordings of 6 actors who acted 14 emo- tionally neutral short sentences sentences to sim- ulate 7 emotional states. It consists of 588 utter- ances and annotated by two different groups of 24 annotators. MASC: Mandarin Affective Speech Corpus This is an Mandarin language acted speech emo- tional corpus that consist of 68 speakers (23 fe- males, 45 males) each reading out read that con- sisted of \ufb01ve phrases, \ufb01fteen sentences and two paragraphs to simulate 5 emotional states. Al- together this database (Wu et al., 2006) contains 25,636 utterances.",
      "Al- together this database (Wu et al., 2006) contains 25,636 utterances. IEMOCAP: The Interactive Emotional Dyadic Motion Capture IEMOCAP database (Busso et al., 2008) is an English language multi-modal emotional speech database. It contains approx- imately 12 hours of audiovisual data, including video, speech, motion capture of face, text tran- scriptions. It consists of dyadic sessions where actors perform improvisations or scripted scenar- ios, speci\ufb01cally selected to elicit emotional ex- pressions. It has categorical labels, such as anger, happiness, sadness, neutrality, as well as dimen- sional labels such as valence, activation and dom- inance. 4 Experiments 4.1 SER on Individual Datasets The \ufb01rst set of experiments focused on perform- ing speech emotion recognition for the 5 datasets individually. We perform a 5-way classi\ufb01cation by choosing 5 emotions common in all datasets i.e. happy, sad, fear, anger and neutral. For each dataset, we experiment with different types of features and classi\ufb01ers.",
      "We perform a 5-way classi\ufb01cation by choosing 5 emotions common in all datasets i.e. happy, sad, fear, anger and neutral. For each dataset, we experiment with different types of features and classi\ufb01ers. To generate Mel- frequency Cepstral Coef\ufb01cients (MFCC) features we used the Kaldi-toolkit. We created spk2utt, utt2spk and wav.scp \ufb01les for each dataset and gen- erated MFCC features in .ark format. We lever- aged kaldiio python library to convert .ark \ufb01les to numpy arrays. Apart from MFCC\u2019s we also com- puted pitch features using the same toolkit. We keep a maximum of 120 frames of the input, and",
      "Figure 1: Transfer learning for small datasets Figure 2: Multi-task learning for learning emotion and language ID simultaneously zero padded the extra signal for short utterances and clipped the extra signal for longer utterances to end up with (120,13) feature vector for each ut- terance. To compare emotion classi\ufb01cation performance using MFCC\u2019s as input features we also tried a different feature set i.e. IS09 emotion feature set (Schuller et al., 2009b) which has in previous re- search shown good performance on SER tasks. The IS09 feature set contains 384 features that re- sult from a systematic combination of 16 Low- Level Descriptors (LLDs) and corresponding \ufb01rst order delta coef\ufb01cients with 12 functionals. The 16 LLDs consist of zero-crossing-rate (ZCR), root mean square (RMS) frame energy, pitch frequency (normalized to 500 Hz), harmonics-to-noise ra- tio (HNR) by autocorrelation function, and mel- frequency cepstral coef\ufb01cients (MFCC) 112 (in full accordance to HTK-based computation).",
      "The 12 functionals used are mean, standard deviation, kurtosis, skewness, minimum, maximum, relative position, range, and offset and slope of linear re- gression of segment contours, as well as its two re- gression coef\ufb01cients with their mean square error (MSE) applied on a chunk. To get these features we had to install OpenSmile toolkit. Script to get these features after installation is included in code submitted (refer IS09 directory). Once we had our input features ready we cre- ated test datasets from each of the 5 datasets by leaving one speaker out for small datasets (EMOVO, EMODB, SAVEE) and 2 speakers out for the larger datasets (IEMOCAP, MASC). Thus, for all corpora, the speakers in the test sets do not appear in the training set. We then performed SER using both classical machine learning and deep learning models. We used Support Vector one- vs-rest classi\ufb01er and Logistic Regression Classi- \ufb01er for classical ML models and a stacked LSTM model for the deep learning based classi\ufb01er.",
      "We then performed SER using both classical machine learning and deep learning models. We used Support Vector one- vs-rest classi\ufb01er and Logistic Regression Classi- \ufb01er for classical ML models and a stacked LSTM model for the deep learning based classi\ufb01er. The LSTM network comprised of 2 hidden layers with 128 LSTM cells, followed by a dense layer of size 5 with softmax activation. We present a comparative study across all datasets, feature sets and classi\ufb01ers in table 2. 4.2 SER using Transfer learning for small sized datasets In the next step of experiments we tried to improve on the results we got for individual datasets by trying to leverage the technique of transfer learn- ing. While we had relatively large support for languages like English and Chinese, speech emo- tion datasets for other languages like Italian and German were very small i.e. only had a total of around 500 labeled utterances. Such small amount of training data is not suf\ufb01cient specially when training a deep learning based model. We used the same LSTM classi\ufb01er as detailed in section 4.1. with an additional dense layer be- fore the \ufb01nal dense layer with softmax.",
      "Such small amount of training data is not suf\ufb01cient specially when training a deep learning based model. We used the same LSTM classi\ufb01er as detailed in section 4.1. with an additional dense layer be- fore the \ufb01nal dense layer with softmax. We train this base model using the large IEMOCAP En- glish dataset. We then freeze the weights of LSTM layers i.e. only trainable weights in the classi\ufb01er remain those of the penultimate dense layer. We \ufb01ne tune the weights of this layer using the small datasets(eg. SAVEE, EMODB, EMOVO) and test",
      "performance on the same test sets we created in section 4.1. Table 3 shows the results of transfer learning ex- periments. 4.3 Multitask learning for SER Last set of experiments focus on studying the role of language being spoken on emotion recognition. Due to the lack of adequately sized emotion cor- pus in many languages, researchers have previ- ously tried training emotion recognition models on cross-corpus data i.e. training with data in one or more language and testing on another. This ap- proach sounds valid only if we consider that ex- pression of emotion is same in all languages i.e. no matter which language you speak, the way you convey your happiness, anger, sadness etc will re- main the same. One example can be that low pitch signals are generally associated with sadness and high pitch and amplitude with anger. If expres- sion of emotion is indeed language agnostic we could train emotion recognition models with high resource languages and use the same models for low resource languages. To verify this hypothesis, we came up with a multi-task framework that jointly learns to predict emotion and the language in which the emotion is being expressed.",
      "If expres- sion of emotion is indeed language agnostic we could train emotion recognition models with high resource languages and use the same models for low resource languages. To verify this hypothesis, we came up with a multi-task framework that jointly learns to predict emotion and the language in which the emotion is being expressed. The framework is illustrated in \ufb01gure 2. The parameters of the LSTM model remain the same as mentioned in section 4.1. The SER performance of using training data from all languages and training a single classi\ufb01er(same as shown in \ufb01gure 1) vs. using training data from all languages in a multi-task setting is mentioned in table 4. 5 Results and Analysis We will discuss the results of each experiment in detail in this section: 1. For SER experiments on individual dataset we see from Table 2 that SVC classi\ufb01er with IS09 input features gave the best performance for four out of 5 datasets. We also note a huge difference in accuracy scores when using the same LSTM classi\ufb01er and only changing the input features i.e. MFCC and IS09.",
      "We also note a huge difference in accuracy scores when using the same LSTM classi\ufb01er and only changing the input features i.e. MFCC and IS09. LSTM model with IS09 input features gives better emotion recognition performance for four out of 5 datasets. These experiments suggest the superiority of IS09 features as compared to MFCC\u2019s for SER tasks. 2. As expected the second set of experiments show that transfer learning is bene\ufb01cial for SER task for small datasets. In table 3 we observe that training on IEMOCAP and then \ufb01ne-tuning on train set of small dataset improves performance for german dataset EMODB and smaller english dataset SAVEE. However, we also note a small drop in perfor- mance for Italian dataset EMOVO. 3. Results in table 4 do not show improvement with using language as an auxiliary task in speech emotion recognition. While a im- provement would have suggested that lan- guage spoken does affect the way people ex- press emotions in speech, the current results are more suggestive of the fact that emotion in speech are universal i.e. language agnostic.",
      "While a im- provement would have suggested that lan- guage spoken does affect the way people ex- press emotions in speech, the current results are more suggestive of the fact that emotion in speech are universal i.e. language agnostic. People speaking different languages express emotions in the same way and SER models could be jointly trained across various SER corpus we have for different languages. 6 Comparison with Previous Research In this section we present comparative study of two previous research papers with our work. We keep this report in a separate section because in order to give a direct comparison with these two papers we had to follow their train-test split, num- ber of emotion classes etc. Train Test IEMOCAP EMOVO SAVEE EMODB Parry et al. 51.45 33.33 33.33 41.99 Ours (IS09 + SVC) 61.00 32.00 51.00 65.00 Ours (IS09 + LSTM) 55.20 31.43 43.33 46.51 Table 5: Comparative results with Parry et al. 1.",
      "1. In Analysis of Deep Learning Architectures for Cross-corpus Speech Emotion Recogni- tion (Parry et al., 2019), the authors discuss cross-corpus training using 6 datasets. In one of their experiments, they report perfor- mance on test set of each corpus for models trained only on IEMOCAP dataset. When we perform the same experiment i.e. train our model only on IEMOCAP and test on other datasets using IS09 as input features and SVC classi\ufb01er, we observe better results even while performing a 5 way classi\ufb01cation task as compared to their 4 way classi\ufb01cation. Results are shown in Table 5.",
      "2. In multi modal emotion recognition on IEMOCAP with neural networks (Tripathi and Beigi, 2018), the authors present three deep learning based speech emotion recogni- tion models. We follow the exact same data pre-processing steps for obtaining same train- test split. We also use the same LSTM model as their best performing model to verify we get the same result i.e. accuracy of 55.65%. However, we could improve this performance to 56.45% by using IS09 features for input and a simple SVC classi\ufb01er. This experiment suggested we could get equal or better perfor- mance in much less training time with classi- cal machine learning models given the right input features as compared to sophisticated deep learning classi\ufb01ers. 7 Future Work In future we would like to experiment with more architectures and feature sets. We would also like to extend this study to include other lan- guages, specially low resource languages. Since all datasets in this study were acted speech, an- other interesting study would be to note the differ- ences that arise when dealing with natural speech.",
      "We would also like to extend this study to include other lan- guages, specially low resource languages. Since all datasets in this study were acted speech, an- other interesting study would be to note the differ- ences that arise when dealing with natural speech. 8 Conclusion Some of the main conclusions that can be drawn from this study are that classical machine learn- ing models may perform as well as deep learning models for SER tasks given we choose the right input features. IS09 features consistently perform well for SER tasks across datasets in different lan- guages. Transfer learning proved to be an effective technique for performing SER for small datasets and multi-task learning experiments shed light on the language agnostic nature of speech emotion recognition task. References E. M. Albornoz and D. H. Milone. 2017. Emotion recognition in never-seen languages using a novel ensemble method with emotion pro\ufb01les. IEEE Trans. Affect. Comput., 8(1):43\u201353. F. Burkhardt, A. Paeschke, M. Rolfes, W. Sendlmeier, and B. Weiss. 2005.",
      "IEEE Trans. Affect. Comput., 8(1):43\u201353. F. Burkhardt, A. Paeschke, M. Rolfes, W. Sendlmeier, and B. Weiss. 2005. A database of german emo- tional speech. In in Proceedings of Interspeech, Lissabon, pages 1517\u20131520. Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower Provost, Samuel Kim, Jeannette Chang, Sungbok Lee, and Shrikanth Narayanan. 2008. Iemocap: Interactive emotional dyadic motion capture database. Language Re- sources and Evaluation, 42:335\u2013359. Giovanni Costantini, Iacopo Iaderola, Andrea Paoloni, and Massimiliano Todisco. 2014. EMOVO cor- pus: an Italian emotional speech database. In Pro- ceedings of the Ninth International Conference on Language Resources and Evaluation (LREC\u201914), pages 3501\u20133504, Reykjavik, Iceland.",
      "2014. EMOVO cor- pus: an Italian emotional speech database. In Pro- ceedings of the Ninth International Conference on Language Resources and Evaluation (LREC\u201914), pages 3501\u20133504, Reykjavik, Iceland. European Language Resources Association (ELRA). J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, D. S. Pallett, and N. L. Dahlgren. 1993. Darpa timit acoustic phonetic continuous speech corpus cdrom. Panikos Heracleous and Akio Yoneyama. 2019. A comprehensive study on bilingual and multilingual speech emotion recognition using a two-pass classi- \ufb01cation scheme. PLOS ONE, 14(8):1\u201320. Philip Jackson and Sana ul haq. 2011. Surrey audio- visual expressed emotion (savee) database. Y. Jin, Y. Zhao, C. Huang, and L. Zhao. 2009. Study on the emotion recognition of whispered speech.",
      "Philip Jackson and Sana ul haq. 2011. Surrey audio- visual expressed emotion (savee) database. Y. Jin, Y. Zhao, C. Huang, and L. Zhao. 2009. Study on the emotion recognition of whispered speech. In 2009 WRI Global Congress on Intelligent Systems, volume 3, pages 242\u2013246. Siddique Latif, Junaid Qadir, and Muhammad Bilal. 2019. Unsupervised adversarial domain adapta- tion for cross-lingual speech emotion recognition. ArXiv, abs/1907.06083. Siddique Latif, Adnan Qayyum, Muhammad U. Us- man, and Junaid Qadir. 2018. Cross lingual speech emotion recognition: Urdu vs. western languages. CoRR, abs/1812.10411. Marko Lugger and Bin Yang. 2007. An incremental analysis of different feature groups in speaker inde- pendent emotion recognition. X. Mao, L. Chen, and L. Fu. 2009. Multi-level speech emotion recognition based on hmm and ann.",
      "Marko Lugger and Bin Yang. 2007. An incremental analysis of different feature groups in speaker inde- pendent emotion recognition. X. Mao, L. Chen, and L. Fu. 2009. Multi-level speech emotion recognition based on hmm and ann. In 2009 WRI World Congress on Computer Science and Information Engineering, volume 7, pages 225\u2013229. Fatemeh Noroozi, Dorota Kamiska, Tomasz Sapiski, and Gholamreza Anbarjafari. 2017. Supervised vocal-based emotion recognition using multiclass support vector machine, random forests, and ad- aboost. Journal of the Audio Engineering Society, 65:562\u2013572. Jack Parry, Dimitri Palaz, Georgia Clarke, Pauline Lecomte, Rebecca Mead, Michael Berger, and Gre- gor Hofer. 2019. Analysis of Deep Learning Archi- tectures for Cross-Corpus Speech Emotion Recogni- tion. In Proc. Interspeech 2019, pages 1656\u20131660. R. Rajoo and C. C. Aun.",
      "2019. Analysis of Deep Learning Archi- tectures for Cross-Corpus Speech Emotion Recogni- tion. In Proc. Interspeech 2019, pages 1656\u20131660. R. Rajoo and C. C. Aun. 2016. In\ufb02uences of languages in speech emotion recognition: A comparative study using malay, english and mandarin languages. In 2016 IEEE Symposium on Computer Applications Industrial Electronics (ISCAIE), pages 35\u201339.",
      "K. Rychlicki-Kicior and B. Stasiak. 2014. Multipitch estimation using judge-based model. Bulletin of the Polish Academy of Sciences: Technical Sciences, 62(No 4):751\u2013757. Hesam Sagha, Pavel Matejka, Maryna Gavryukova, Filip Povolny, Erik Marchi, and Bjrn Schuller. 2016. Enhancing multilingual recognition of emotion in speech by language identi\ufb01cation. Bjorn Schuller, Bogdan Vlasenko, Florian Eyben, Mar- tin Wollmer, Andre Stuhlsatz, Andreas Wendemuth, and Gerhard Rigoll. 2010. Cross-corpus acous- tic emotion recognition: Variances and strategies. IEEE Trans. Affect. Comput., 1(2):119\u2013131. Bjrn Schuller, Anton Batliner, Stefan Steidl, and Dino Seppi. 2011. Recognising realistic emotions and af- fect in speech: State of the art and lessons learnt from the \ufb01rst challenge.",
      "Bjrn Schuller, Anton Batliner, Stefan Steidl, and Dino Seppi. 2011. Recognising realistic emotions and af- fect in speech: State of the art and lessons learnt from the \ufb01rst challenge. Speech Communication, 53(9):1062 \u2013 1087. Sensing Emotion and Affect - Facing Realism in Speech Processing. Bjrn Schuller, Stefan Steidl, and Anton Batliner. 2009a. The interspeech 2009 emotion challenge. pages 312\u2013315. Bjrn Schuller, Stefan Steidl, and Anton Batliner. 2009b. The interspeech 2009 emotion challenge. pages 312\u2013315. Arslan Shaukat and Ke Chen. 2008. Towards auto- matic emotional state categorization from speech signals. In Proceedings of the Annual Confer- ence of the International Speech Communication As- sociation, INTERSPEECH\u2014Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH, pages 2771\u20132774.",
      "In Proceedings of the Annual Confer- ence of the International Speech Communication As- sociation, INTERSPEECH\u2014Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH, pages 2771\u20132774. Samarth Tripathi and Homayoon Beigi. 2018. Multi- modal emotion recognition on iemocap dataset using deep learning. Bogdan Vlasenko, Bj\u00a8orn Schuller, Andreas Wende- muth, and Gerhard Rigoll. 2007. Frame vs. turn- level: Emotion recognition from speech considering static and dynamic processing. In Affective Com- puting and Intelligent Interaction, pages 139\u2013147, Berlin, Heidelberg. Springer Berlin Heidelberg. T. Wu, Y. Yang, Z. Wu, and D. Li. 2006. Masc: A speech corpus in mandarin for emotion analysis and affective speaker recognition. In 2006 IEEE Odyssey - The Speaker and Language Recognition Workshop, pages 1\u20135.",
      "T. Wu, Y. Yang, Z. Wu, and D. Li. 2006. Masc: A speech corpus in mandarin for emotion analysis and affective speaker recognition. In 2006 IEEE Odyssey - The Speaker and Language Recognition Workshop, pages 1\u20135. Z. Xiao, D. Wu, X. Zhang, and Z. Tao. 2016. Speech emotion recognition cross language families: Man- darin vs. western languages. In 2016 International Conference on Progress in Informatics and Comput- ing (PIC), pages 253\u2013257. X. Zhou, J. Guo, and R. Bie. 2016. Deep learn- ing based affective model for speech emotion recognition. In 2016 Intl IEEE Conferences on Ubiquitous Intelligence Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld), pages 841\u2013846. Y. Zhou, Y. Sun, L. Yang, and Y. Yan. 2009.",
      "Y. Zhou, Y. Sun, L. Yang, and Y. Yan. 2009. Applying articulatory features to speech emotion recognition. In 2009 International Conference on Research Chal- lenges in Computer Science, pages 73\u201376."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-2003.07996.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":6521,
  "avg_doclen":176.2432432432,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-2003.07996.pdf"
    }
  }
}