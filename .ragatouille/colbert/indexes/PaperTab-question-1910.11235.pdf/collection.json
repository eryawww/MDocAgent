[
  "Rethinking Exposure Bias in Adversarial Language Modeling Yifan Xu \u2217, Kening Zhang \u2217, Haoyu Dong, Yuezhou Sun, Wenlong Zhao & Zhuowen Tu University of California San Diego {yix081,kez040, had002, yus174, wez094,ztu}@ucsd.edu Abstract Exposure bias refers to the phenomenon that a language model trained under the teacher forcing schema may perform poorly at the in- ference stage when its predictions are condi- tioned on its outputs that diverge from the training corpus. Although several adversar- ial training methods have been proposed to avoid teacher forcing, lacking a clear evalua- tion for the exposure bias remains a concern. The contribution of our work is two-fold. (1) We propose to evaluate exposure bias based on the quality of sentence generated in the sentence completion task. (2) We adopt two strategies, multi-range reinforcing and multi- entropy sampling, to stabilize adversarial train- ing, and show an improvement over the com- peting models with regards to the sentence completion task and corpus BLEUs.",
  "(2) We adopt two strategies, multi-range reinforcing and multi- entropy sampling, to stabilize adversarial train- ing, and show an improvement over the com- peting models with regards to the sentence completion task and corpus BLEUs. 1 Introduction Likelihood-based language models with deep neu- ral networks have been widely adopted to tackle the language modeling tasks (Graves et al., 2013; Karpathy and Fei-Fei, 2015; Bahdanau et al., 2014). By far, one of the most popular training strategies is teacher forcing, which is derived from the general maximum likelihood estimation (MLE) principle (Williams and Zipser, 1989). Under the teacher forcing schema, the language model makes predic- tions conditioned on the ground-truth inputs. This is susceptible to so-called exposure bias: a model may perform poorly at the inference stage, once its pre\ufb01x diverges from the previously learned data (Bengio et al., 2015). However, there is little work on how to expose and quantify such performance degeneration in text generation.",
  "However, there is little work on how to expose and quantify such performance degeneration in text generation. A common strategy to mitigate the exposure bias problem is to impose additional supervision upon the model\u2019s self-generated output via adversarial \u2217Equal contribution. training. The actor-critic (AC) method (Konda and Tsitsiklis, 2000) and SeqGAN (Yu et al., 2017) introduce an additional critic network to offer re- wards on a language model\u2019s self-generated se- quences. Therefore, the language model can later, at the inference stage, predict robustly with its pre- vious outputs. One issue in adversarial training is that the signal from the critic network is very sparse, which leads to stability issues. The second issue is about the non-stationary sampled data with strongly correlated online updates (Pfau and Vinyals, 2016; Mnih et al., 2016).",
  "The second issue is about the non-stationary sampled data with strongly correlated online updates (Pfau and Vinyals, 2016; Mnih et al., 2016). Due to these problems, existing language GANs (Yu et al., 2017; Lin et al., 2017; Guo et al., 2017) have a risk of compromising gen- eration diversity (Caccia et al., 2018). This paper makes the following contributions: 1. We propose to evaluate the exposure bias for a language model by performing the sentence completion task using the ground truth pre\ufb01x. 2. We introduce a new approach, multi-entropy sampling and multi-range reinforcing (MEMR), to overcome the dif\ufb01culties during adversarial training, which demon- strates a signi\ufb01cant improvement over the competing models in the corpus BLEUs metrics, as well as our proposed measures in sentence completion. 2 Related Works A common measure quantifying the exposure bias is still absent.",
  "2 Related Works A common measure quantifying the exposure bias is still absent. Existing works often show perfor- mance gains by introducing adversarial training but questions remain if such gains indeed result in the reduction of the exposure bias (Bahdanau et al., 2016; Yu et al., 2017). Later works add generation diversity into consideration (Shi et al., 2018; Cac- cia et al., 2018; Alihosseini et al., 2019) or take a perspective from traditional language modeling aspects (Tevet et al., 2018). A closely related work to our evaluation measure is (He et al., 2019). The difference is that He et al. (2019) requires inference for ground truth data distribution with experiments performed using synthetic data. arXiv:1910.11235v2  [cs.CL]  31 Mar 2020",
  "An early work addressing the exposure bias prob- lem is (Bengio et al., 2015) in which a curriculum learning approach called scheduled sampling is pro- posed by gradually replacing the ground-truth to- kens with the model\u2019s predictions. In recent RL- inspired works, Ranzato et al. (2015) adopt the REINFORCE algorithm (Sutton et al., 2000) to di- rectly optimize the test-time evaluation score. Bah- danau et al. (2016) employ a similar approach by training a critic network to predict the metric score for the actor\u2019s generated sequence of tokens. In parallel, a language version of generative adver- sarial networks (GANs) (Goodfellow et al., 2014), SeqGAN, is introduced in (Yu et al., 2017). Se- qGAN consists of a generator pre-trained under MLE and a discriminator pre-trained to discern the generator\u2019s distribution from the real data.",
  "Se- qGAN consists of a generator pre-trained under MLE and a discriminator pre-trained to discern the generator\u2019s distribution from the real data. Follow- up works such as RankGAN (Lin et al., 2017) and LeakGAN (Guo et al., 2017) alter the training ob- jectives or model architectures to enhance the guid- ance. RankGAN (Lin et al., 2017) replaces the binary reward with a relative ranking score. Leak- GAN (Guo et al., 2017) allows the discriminator to \u201cleak\u201d its internal states to the generator at interme- diate steps. Shi et al. (2018) model a reward func- tion using inverse reinforcement learning (IRL). 3 Exposure Bias Evaluation 3.1 Exposure Bias Cross-entropy loss adopted in teacher forcing is equivalent to minimizing the forward KL diver- gence DKL(P||Q\u03b8) between data distribution P and model distribution Q\u03b8. However, during the in- ference stage, the model is often evaluated based on the quality of its generated samples.",
  "However, during the in- ference stage, the model is often evaluated based on the quality of its generated samples. The evaluation metrics or human experts can be seen as surrogates of the data distribution P, so what they measure is the reverse KL divergence DKL(Q\u03b8||P). In Bayesian inference, there is a well-known difference between DKL(P||Q) and DKL(Q||P) (MacKay, 2003). Minimizing DKL(P||Q) encour- ages the model to cover all the modes in the training data, which will result in over-generalization in the extreme case. In contrast, minimizing DKL(Q||P) prefers the model to concentrate on the largest mode while ignoring the others, which tends to cause mode collapse (Husz\u00b4ar, 2015). In our lan- guage modeling task, an LSTM strives to cover the entire data distribution at the cost of over- generalization. It is more likely to produce pre\ufb01xes different from those seen at the training stage, and the fact that this model has never learned to pre- dict based on these pre\ufb01xes potentially leads to the exposure bias .",
  "It is more likely to produce pre\ufb01xes different from those seen at the training stage, and the fact that this model has never learned to pre- dict based on these pre\ufb01xes potentially leads to the exposure bias . 3.2 Sentence Completion Task In this section, we form a sentence completion task to evaluate the exposure bias. Given a sentence pre\ufb01x X1:k of length K drawn from a data distribu- tion P, we apply a language model Q\u03b8 to perform sentence completion until \ufb01nal the T step, starting from such pre\ufb01x.",
  "Given a sentence pre\ufb01x X1:k of length K drawn from a data distribu- tion P, we apply a language model Q\u03b8 to perform sentence completion until \ufb01nal the T step, starting from such pre\ufb01x. \u2022 If the pre\ufb01x X1:k is sampled from a seen distribution Pseen, then the exposure bias for the sentence comple- tion task should be relatively low, where Q\u03b8(Xk:T |Pseen)=EX1:k\u223cPseen Q\u03b8(Xk:T |X1:k) \u2022 If the pre\ufb01x X1:k comes from an unseen data distribu- tion Punseen, then the exposure bias for the task can be critical, where Q\u03b8(Xk:T |Punseen)=EX1:k\u223cPunseen Q\u03b8(Xk:T |X1:k) Based on the de\ufb01nition for the exposure bias, Q\u03b8(Xk:T |Punseen) should suffer more from the training-testing deviation than Q\u03b8(Xk:T |Pseen). Also, such performance degeneration should be more signi\ufb01cant when pre\ufb01x k grows longer in both scenarios.",
  "Also, such performance degeneration should be more signi\ufb01cant when pre\ufb01x k grows longer in both scenarios. These two hypotheses are con\ufb01rmed by our result in Figure 1. As a measurement to assess model\u2019s generation quality, forward Corpus BLEU, BLEUF, is evalu- ated. Because precision is the primary concern, we set softmax temperature \u03c4 = 0.5 to sample high- con\ufb01dence sentences from model\u2019s distribution. Based on the task completion task results in Fig- ure 1, we observe that original SeqGAN (Yu et al., 2017) shows more stable result although many text GAN variants are proposed later, which is unex- pected. Therefore, our method MEMR is motivated to improve SeqGAN by introducing denser reward signal from the critic network and further stabiliz- ing the adversarial training.",
  "Therefore, our method MEMR is motivated to improve SeqGAN by introducing denser reward signal from the critic network and further stabiliz- ing the adversarial training. 4 Method Description 4.1 Actor-Critic Training Actor-Critic methods (ACs) formulates language modeling as a generalized Markov Decision Pro- cess (MDP) problem, where the actor learns to optimize its policy guided by the critic, while the critic learns to optimize its value function based on the actor\u2019s output and external reward informa- tion. As Pfau and Vinyals (2016) points out, GAN methods can be seen as a special case of AC where the critic aims to distinguish the actor\u2019s genera- tion from real data and the actor is optimized in an opposite direction to the critic.",
  "In this work, we use a standard single-layer LSTM as the actor network. The training objective is to maximize the model\u2019s expected end rewards with policy gradient (Sutton et al., 2000): L(\u03b8)=\u2212EX1:T \u223c\u03c0\u03b8 PT t=1 Q\u03c6(xt,ht) log \u03c0\u03b8(xt|ht) In practice, we perform a Monte-Carlo (MC) search with roll-out policy following Yu et al. (2017) to sample complete sentences starting from each location in a predicted sequence and compute their end rewards. Empirically, we found out that the maximum, instead of average, of rewards in the MC search better represents each token\u2019s ac- tor value and yields better results during training.",
  "(2017) to sample complete sentences starting from each location in a predicted sequence and compute their end rewards. Empirically, we found out that the maximum, instead of average, of rewards in the MC search better represents each token\u2019s ac- tor value and yields better results during training. Therefore, we compute the action value by: Q\u03c6(xt,ht)=maxXt:T \u2208MC\u03b8(X1:t,T ) Q\u03c6(X1:T ) Then, We use a convolutional neural network (CNN) as the critic to predict the expected rewards for current generated pre\ufb01x: L(\u03c6)=\u2212EX1:T \u223c\u03c0\u03b8(r(X1:T )\u2212Q\u03c6(X1:T ))2 4.2 MEMR During the experiment, we observe a certain level of instability for the learned models. In the previ- ous literature, two major factors behind the training instability are the sparse reward from critic net- work and the update correlation in the sampling process (Pfau and Vinyals, 2016; Mnih et al., 2016; Volodymyr et al., 2013).",
  "We address these prob- lems using the following strategies: Multi-Entropy Sampling: Language GANs can be seen as online RL methods, where the lan- guage model is updated from data generated by a single policy. Most sampled sentences in MC search are highly correlated. Similar to Xu et al. (2019), we empirically observe that increasing the range of the entropy of the actor\u2019s sample distribu- tion during training is bene\ufb01cial to the adversarial training performance. Speci\ufb01cally, we alternate the temperature \u03c4 in the softmax to generate sam- ples under different behavior policies. During the critic\u2019s training, the ground-truth sequences are as- signed a perfect target value of 1. The samples obtained with \u03c4 < 1 are supposed to contain lower entropy, thus they receive a higher target value close to 1. Those samples obtained with \u03c4 > 1 con- tain higher entropy, and the target value is closer to 0. This mechanism decorrelates updates dur- ing sequential sampling by sampling from multiple diverse entropy distributions synchronously.",
  "Those samples obtained with \u03c4 > 1 con- tain higher entropy, and the target value is closer to 0. This mechanism decorrelates updates dur- ing sequential sampling by sampling from multiple diverse entropy distributions synchronously. Multi-Range Reinforcing: Our idea of multi- range supervision takes inspiration from deeply- supervised nets (DSNs) (Lee et al., 2015). By design, lower layers in a CNN have smaller re- ceptive \ufb01elds, allowing them to make better use of local patterns. Differently from DSNs (Lee et al., 2015) which disregard all intermediate predictions in the end, we average the reward predictions from multiple intermediate layers of the critic network with the \ufb01nal output, which attend to local n-grams rather than the whole complete sentence. This is a solution to the reward sparseness, as the language model can receive averaged reward with more local information. 4.3 Effectiveness of Multi-Range Reinforcing and Multi-Entropy Sampling Table 1 demonstrates the effectiveness of multi-entropy sampling (ME) and multi-range reinforcing (MR).",
  "This is a solution to the reward sparseness, as the language model can receive averaged reward with more local information. 4.3 Effectiveness of Multi-Range Reinforcing and Multi-Entropy Sampling Table 1 demonstrates the effectiveness of multi-entropy sampling (ME) and multi-range reinforcing (MR). We observe that ME improves BLEUF5 (precision) signi\ufb01cantly while MEMR further enhances BLEUF5 (precision) and BLEUF5 (recall). Detailed explanations of these metrics can be found in Section 5.2.",
  "We observe that ME improves BLEUF5 (precision) signi\ufb01cantly while MEMR further enhances BLEUF5 (precision) and BLEUF5 (recall). Detailed explanations of these metrics can be found in Section 5.2. Architecture BLEUF5 BLEUB5 TF 15.4 \u00b1 0.17 30.5 \u00b1 0.08 AC 13.8 \u00b1 0.16 30.3 \u00b1 0.13 AC (with ME) 22.4 \u00b1 0.25 30.0 \u00b1 0.09 AC (with MEMR ) 24.5 \u00b1 0.14 31.6 \u00b1 0.10 Table 1: Effectiveness of the proposed ME and MEMR strate- gies on EMNLP2017 WMT News Dataset 5 Experiment 5.1 Datasets We perform evaluations on two datasets: EMNLP2017 WMT News 1 and Google-small, a subset of Google One Billion Words 2. \u2022 EMNLP2017 WMT News is provided in (Zhu et al., 2018), a benchmarking platform for text GANs.",
  "\u2022 EMNLP2017 WMT News is provided in (Zhu et al., 2018), a benchmarking platform for text GANs. The entire dataset is split into a training set of 195,010 sen- tences, a validation set of 83,576 sentences, and a test set of 10,000 sentences. The vocabulary size is 5,254 and the average sentence length is 27. \u2022 Google-small is sampled and pre-processed from the Google One Billion Words. It contains a training set of 699,967 sentences, a validation set of 200,000 sentences, and a test set of 99,985 sentences. The vocabulary size is 61,458 and the average sentence length is 29. 5.2 BLEU metric We adopt three variations of BLEU metric from Shi et al. (2018). BLEUF, or forward BLEU, is a metric for precision, and BLEUB, or backward BLEU, is a metric for recall. BLEUHA computes the harmonic mean of both BLEU.",
  "(2018). BLEUF, or forward BLEU, is a metric for precision, and BLEUB, or backward BLEU, is a metric for recall. BLEUHA computes the harmonic mean of both BLEU. These three metrics take both 1https://github.com/geek-ai/Texygen 2http://www.statmt.org/lm-benchmark/",
  "(a) Train data (Seen pre\ufb01xes) (b) Test data (Unseen pre\ufb01xes) Figure 1: Sentence Completion Task results based on pre\ufb01xes from training and testing datasets on EMNLP2017 WMT News [Higher is better]. In each experiment, the data source for the pre\ufb01xes is used as the reference to calculate BLEUF4. EMNLP2017 WMT Google-small Model BLEUF5 BLEUB5 BLEUHA5 BLEUF5 BLEUB5 BLEUHA5 TEACHER FORCING (TF) 15.4 \u00b1 0.11 30.5 \u00b1 0.05 20.5 \u00b1 0.10 9.6 \u00b1 0.03 12.9 \u00b1 0.02 11.00 \u00b1 0.02 SCHEDULED SAMPLING (SS) (Bengio et al., 2015) 12.1 \u00b1 0.14 30.3 \u00b1 0.06 17.3 \u00b1 0.14 6.2 \u00b1 0.04 10.7 \u00b1 0.02 7.8 \u00b1 0.",
  ", 2015) 12.1 \u00b1 0.14 30.3 \u00b1 0.06 17.3 \u00b1 0.14 6.2 \u00b1 0.04 10.7 \u00b1 0.02 7.8 \u00b1 0.04 SEQGAN (Yu et al., 2017) 16.6 \u00b1 0.09 28.7 \u00b1 0.37 21.0 \u00b1 0.11 20.7 \u00b1 0.02 14.4 \u00b1 0.02 17.0 \u00b1 0.01 RANKGAN (Lin et al., 2017) 17.7 \u00b1 0.14 30.1 \u00b1 0.06 22.3 \u00b1 0.11 21.4 \u00b1 0.06 12.7 \u00b1 0.02 15.9 \u00b1 0.02 LEAKGAN (Guo et al., 2017) 19.8 \u00b1 0.11 31.6 \u00b1 0.04 24.4 \u00b1 0.10 - - - MEMR (ours) 24.5 \u00b1 0.08 31.",
  "02 LEAKGAN (Guo et al., 2017) 19.8 \u00b1 0.11 31.6 \u00b1 0.04 24.4 \u00b1 0.10 - - - MEMR (ours) 24.5 \u00b1 0.08 31.6 \u00b1 0.06 27.9 \u00b1 0.07 22.0 \u00b1 0.07 15.8 \u00b1 0.02 18.4 \u00b1 0.03 Table 2: Corpus BLEUs Results on EMNLP2017 WMT News and the Google-small dataset. The 95 % con\ufb01dence intervals from multiple trials are reported. \u2020 the Google-small was not tested in (Guo et al., 2017) and we are unable to train LeakGAN on this dataset using the of\ufb01cial code due to its training complexity (taking 10+ hours per epoch). diversity and quality into consideration. A model with severe mode collapse or diverse but incorrect outputs receives low scores. 5.3 Implementation Details We implement a standard single-layer LSTM as the generator (actor) and a eight-layer CNN as the discriminator (critic).",
  "diversity and quality into consideration. A model with severe mode collapse or diverse but incorrect outputs receives low scores. 5.3 Implementation Details We implement a standard single-layer LSTM as the generator (actor) and a eight-layer CNN as the discriminator (critic). The LSTM has embedding dimension 32 and hidden dimension 256. The CNN consists of 8 layers with \ufb01lter size 3, where the 3rd, 5th, and 8th layers are directly connected to the output layer for multi-range supervision. Other parameters are consistent with Zhu et al. (2018). Adam optimizer is deployed for both critic and ac- tor with learning rate 10\u22124 and 5\u00b710\u22123 respectively. The target values for the critic network are set to [0, 0.2, 0.4, 0.6, 0.8] for samples generated by the LSTM with softmax temperatures [0.5, 0.75, 1.0, 1.25, 1.5].",
  "6 Results Based on the sentence completion results in Figure 1, all models decrease in precision of generated text (re\ufb02ected via BLEUF4) as the fed-in pre\ufb01x length (K) increases, but the effect is stronger on the un- seen test data, revealing the existence of exposure bias. Nonetheless, our model trained under ME and MR yields the best sentence quality and a relatively moderate performance decline. Although TF and SS demonstrate higher BLEUF5 performance with shorter pre\ufb01xes, their sentence qualities drop drastically on the test dataset with longer pre\ufb01xes. On the other hand, GANs begin with lower BLEUF4 precision scores but demonstrate less performance decay as the pre- \ufb01x grows longer and gradually outperform TF. This robustness against unseen pre\ufb01xes exhibits that su- pervision from a learned critic can boost a model\u2019s stability in completing unseen sequences. The bet- ter generative quality in TF and the stronger robust- ness against exposure bias in GANs are two differ- ent objectives in language modeling, but they can be pursued at the same time. Our model\u2019s improve- ment in both perspectives exhibit one possibility to achieve the goal.",
  "The bet- ter generative quality in TF and the stronger robust- ness against exposure bias in GANs are two differ- ent objectives in language modeling, but they can be pursued at the same time. Our model\u2019s improve- ment in both perspectives exhibit one possibility to achieve the goal. We also report Corpus BLEUs to re\ufb02ect the qual- ity and diversity of generated text in Table 2 with competing models on EMNLP2017 WMT News and Google-small. Our model, MEMR, outper- forms the others in Corpus BLEUs, indicating a high diversity and quality in its sample distribu- tion. 7 Conclusion We propose to use the sentence completion task to reveal exposure bias in text generation. Further, we overcome the hurdles in adversarial training with multi-range reinforcing and multi-entropy sampling (MEMR), which shows an improvement in the sen- tence completion task and Corpus BLEUs.",
  "Acknowledgments The authors are grateful for the supports by NSF IIS-1618477, NSF IIS-1717431, and a grant from Samsung Research America. References Danial Alihosseini, Ehsan Montahaei, and Mahdieh So- leymani Baghshah. 2019. Jointly measuring diver- sity and quality in text generation models. In Pro- ceedings of the Workshop on Methods for Optimiz- ing and Evaluating Neural Language Generation, pages 90\u201398. Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. 2016. An actor-critic algorithm for sequence prediction. arXiv preprint arXiv:1607.07086. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.",
  "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled sampling for se- quence prediction with recurrent neural networks. In Advances in Neural Information Processing Sys- tems, pages 1171\u20131179. Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Charlin. 2018. Language gans falling short. arXiv preprint arXiv:1811.02549. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative ad- versarial nets. In Advances in neural information processing systems, pages 2672\u20132680.",
  "2014. Generative ad- versarial nets. In Advances in neural information processing systems, pages 2672\u20132680. Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. 2013. Speech recognition with deep recur- rent neural networks. In Acoustics, speech and sig- nal processing (icassp), 2013 ieee international con- ference on, pages 6645\u20136649. IEEE. Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and Jun Wang. 2017. Long text generation via adversarial training with leaked information. arXiv preprint arXiv:1709.08624. Tianxing He, Jingzhao Zhang, Zhiming Zhou, and James Glass. 2019. Quantifying exposure bias for neural language generation. arXiv preprint arXiv:1905.10617. Ferenc Husz\u00b4ar. 2015. How (not) to train your genera- tive model: Scheduled sampling, likelihood, adver- sary?",
  "Quantifying exposure bias for neural language generation. arXiv preprint arXiv:1905.10617. Ferenc Husz\u00b4ar. 2015. How (not) to train your genera- tive model: Scheduled sampling, likelihood, adver- sary? arXiv preprint arXiv:1511.05101. Andrej Karpathy and Li Fei-Fei. 2015. Deep visual- semantic alignments for generating image descrip- tions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3128\u20133137. Vijay R Konda and John N Tsitsiklis. 2000. Actor- critic algorithms. In Advances in neural information processing systems, pages 1008\u20131014. Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. 2015. Deeply- supervised nets. In Arti\ufb01cial Intelligence and Statistics, pages 562\u2013570. Kevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang, and Ming-Ting Sun.",
  "2015. Deeply- supervised nets. In Arti\ufb01cial Intelligence and Statistics, pages 562\u2013570. Kevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang, and Ming-Ting Sun. 2017. Adversarial ranking for language generation. In Advances in Neural Infor- mation Processing Systems, pages 3155\u20133165. David JC MacKay. 2003. Information theory, infer- ence and learning algorithms. Cambridge university press. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asyn- chronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928\u20131937. David Pfau and Oriol Vinyals. 2016. Connecting gener- ative adversarial networks and actor-critic methods. arXiv preprint arXiv:1610.01945.",
  "In International conference on machine learning, pages 1928\u20131937. David Pfau and Oriol Vinyals. 2016. Connecting gener- ative adversarial networks and actor-critic methods. arXiv preprint arXiv:1610.01945. Marc\u2019Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2015. Sequence level train- ing with recurrent neural networks. arXiv preprint arXiv:1511.06732. Zhan Shi, Xinchi Chen, Xipeng Qiu, and Xuanjing Huang. 2018. Towards diverse text generation with inverse reinforcement learning. arXiv preprint arXiv:1804.11258. Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. 2000. Policy gradient methods for reinforcement learning with function ap- proximation. In Advances in neural information pro- cessing systems, pages 1057\u20131063. Guy Tevet, Gavriel Habib, Vered Shwartz, and Jonathan Berant.",
  "2000. Policy gradient methods for reinforcement learning with function ap- proximation. In Advances in neural information pro- cessing systems, pages 1057\u20131063. Guy Tevet, Gavriel Habib, Vered Shwartz, and Jonathan Berant. 2018. Evaluating text gans as lan- guage models. arXiv preprint arXiv:1810.12686. Mnih Volodymyr, Koray Kavukcuoglu, David Silver, Alex Graves, and Ioannis Antonoglou. 2013. Play- ing atari with deep reinforcement learning. In NIPS Deep Learning Workshop. Ronald J Williams and David Zipser. 1989. A learn- ing algorithm for continually running fully recurrent neural networks. Neural computation, 1(2):270\u2013 280.",
  "Yifan Xu, Lu Dai, Udaikaran Singh, Kening Zhang, and Zhuowen Tu. 2019. Neural program synthesis by self-learning. arXiv preprint arXiv:1910.05865. Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. Seqgan: Sequence generative adversarial nets with policy gradient. In AAAI, pages 2852\u20132858. Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. 2018. Texy- gen: A benchmarking platform for text generation models. arXiv preprint arXiv:1802.01886."
]