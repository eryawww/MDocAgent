[
  "UniSent: Universal Adaptable Sentiment Lexica for 1000+ Languages Ehsaneddin Asgari\u2020, \u2021, \u22c4, Fabienne Braune\u2021, Benjamin Roth\u22c4, Christoph Ringlstetter\u2021 and Mohammad R.K. Mofrad\u2020, \u2666 \u2020Department of Bioengineering, University of California, Berkeley, CA 94720, USA \u2021NLP Expert Center, Volkswagen Group Data:Lab, Munich, Germany \u22c4Center for Information and Language Processing, Munich 80538, Germany \u2666Molecular Biophysics and Integrated Bioimaging, Lawrence Berkeley National Lab, Berkeley, CA 94720, USA Abstract In this paper, we introduce UniSent universal sentiment lexica for 1000+ languages. Senti- ment lexica are vital for sentiment analysis in absence of document-level annotations, a very common scenario for low-resource languages. To the best of our knowledge, UniSent is the largest sentiment resource to date in terms of the number of covered languages, including many low resource ones.",
  "Senti- ment lexica are vital for sentiment analysis in absence of document-level annotations, a very common scenario for low-resource languages. To the best of our knowledge, UniSent is the largest sentiment resource to date in terms of the number of covered languages, including many low resource ones. In this work, we use a massively parallel Bible corpus to project sen- timent information from English to other lan- guages for sentiment analysis on Twitter data. We introduce a method called DomDrift to mitigate the huge domain mismatch between Bible and Twitter by a con\ufb01dence weighting scheme that uses domain-speci\ufb01c embeddings to compare the nearest neighbors for a can- didate sentiment word in the source (Bible) and target (Twitter) domain. We evaluate the quality of UniSent in a subset of languages for which manually created ground truth was available, Macedonian, Czech, German, Span- ish, and French. We show that the quality of UniSent is comparable to manually created sentiment resources when it is used as the sen- timent seed for the task of word sentiment pre- diction on top of embedding representations.",
  "We show that the quality of UniSent is comparable to manually created sentiment resources when it is used as the sen- timent seed for the task of word sentiment pre- diction on top of embedding representations. In addition, we show that emoticon sentiments could be reliably predicted in the Twitter do- main using only UniSent and monolingual em- beddings in German, Spanish, French, and Italian. With the publication of this paper, we release the UniSent sentiment lexica. 1 Introduction Language technologies permeate our everyday life through web search, translation, online shop- ping, email writing, spell checking systems etc. The existence of such technologies highly de- pends on the existence of the underlying compu- tational linguistic resources for a language. Com- putational linguistic resources such as machine- readable lexica, part-of-speech-taggers and depen- dency parsers are available for at most a few hun- dred languages. This means that the majority of the 7000 languages of the world are low-resource. This gap between advances in language technolo- gies for English versus other languages endangers multilingualism in the digital age.",
  "This means that the majority of the 7000 languages of the world are low-resource. This gap between advances in language technolo- gies for English versus other languages endangers multilingualism in the digital age. Languages with lack of technological support (as a result of hav- ing limited resources) are less used over time and eventually get in danger of extinction. Many EU and US programs are designed to address this is- sue (Cieri et al., 2016). The rationale of these projects is that even \u201csmall\u201d languages are impor- tant for the preservation of the common heritage of humankind and cultural diversity which bene- \ufb01ts everybody. In addition, certain low-resource languages can be also politically and economically important. Large amount of text available on the web and applications in marketing (Bollen et al., 2011), so- cial science (Hopkins and King, 2010), political science (Wang et al., 2012; Wong et al., 2016) motivates sentiment analysis of news, blogs, so- cial networks, reviews, opinions, and recommen- dations.",
  "However, sentiment analysis requires ei- ther word or document level sentiment annota- tions. Typically, these are available only for a limited number of languages, preventing accurate sentiment classi\ufb01cation in low resource setups. In these scenarios sentiment lexica are important game changers because in many cases end-to-end sentiment classi\ufb01cation is not feasible due to a lack of document level annotations (Jurafsky and Martin, 2014). Recently, embedding-based approaches for su- pervised or semi-supervised word sentiment in- ference became popular allowing for lexicon vo- cabulary expansion and implicit domain adapta- tion (Rothe et al., 2016; Hamilton et al., 2016). Although using the embedding space as represen- arXiv:1904.09678v2  [cs.CL]  28 Nov 2019",
  "tation in word sentiment classi\ufb01cation suf\ufb01ciently addresses domain adaptation in many cases, it can be improved in situations where the domain shift for some words from the lexicon seeds in the source vocabulary (e.g Bible1) to the target vocab- ulary (e.g. Twitter) is large. For instance, in Bib- lical texts, the Spanish word sensual has the con- notation of sin which has a negative polarity. But in the Twitter domain, the same word is associated with sexy, which has a positive polarity. In cases where the classi\ufb01er using the embedding space fails to capture this shift, enhancing the model via mitigation of the domain mismatches is required. Contributions: We release the \ufb01rst sentiment lexicon covering 1000+ languages and achieving macro-F1 over 0.75 on word sentiment prediction for most evaluated languages, meaning that we en- able sentiment analysis in many low resource lan- guages. The creation of UniSent requires only a sentiment lexicon in one language (e.g. English) and a small, but massively parallel corpus in a spe- ci\ufb01c domain.",
  "The creation of UniSent requires only a sentiment lexicon in one language (e.g. English) and a small, but massively parallel corpus in a spe- ci\ufb01c domain. We evaluate UniSent for word sen- timent classi\ufb01cation of Macedonian, Czech, Ger- man, Spanish, French against manually assigned sentiment polarities and show that its quality is comparable to the use of manually created re- sources, which is a great evidence that UniSent works well also for low-resource languages where we do not have resources for evaluation. Secondly, we evaluated UniSent w.r.t. the classi\ufb01cation of emoticon sentiments in the Twitter domain, where macro-F1 of 0.79, 0.76, 0.74, and 0.76 were ob- tained for German, Italian, French, and Spanish respectively. To ensure the usability of our lexica for any new domain, we propose DomDrift, a method requir- ing only a pretrained embedding space in the tar- get domain, which is relatively a cheap resource to obtain.",
  "To ensure the usability of our lexica for any new domain, we propose DomDrift, a method requir- ing only a pretrained embedding space in the tar- get domain, which is relatively a cheap resource to obtain. By comparing the source and target em- bedding graphs DomDrift quanti\ufb01es the semantic changes of words in the sentiment lexicon in the new domain. This measure can hence be used to weight words in the sentiment lexicon for down- stream supervised or semi-supervised sentiment analysis models. We show that on top of implicit domain adaptation, using target domain embed- dings, the incorporation of domain drift scores im- proves sentiment classi\ufb01cation for French, Span- 1Massively parallel corpora mainly exist in the Bible do- main and for a smaller text size for the universal declaration of human rights (Emerson et al., 2014) . ish, and Macedonian.",
  "ish, and Macedonian. Related Work Several research efforts tackled the automatic cre- ation of sentiment lexica for a multitude of lan- guages, but these efforts resulted in the creation of resources for at most 136 languages (Chen and Skiena, 2014) or in lexicon covering a very spe- ci\ufb01c low-resource language (A\ufb02i et al., 2017; Dar- wich et al., 2017). Moreover, these approaches heavily rely on linguistic resources, such as Word- Net or fully trained machine translation systems, which limit them to the languages where these are available. An alternative to our approach in lexicon creation for sentiment is using minimal bilingual supervision (Hangya et al., 2018; Barnes et al., 2018a,b) to create document-level annota- tions for end-to-end sentiment classi\ufb01cations of documents. Later approaches only work in an end- to-end fashion and do not allow to directly create sentiment lexica.",
  "Later approaches only work in an end- to-end fashion and do not allow to directly create sentiment lexica. The annotation projection to create UniSent sentiment lexica is inspired by SuperPivot intro- duced in (Asgari and Sch\u00a8utze, 2017) for the typo- logical analysis of tense in 1000 languages. Agi\u00b4c et al. (2016) also use massively parallel corpora to project POS tags and dependency relations across languages. In contrast to these studies, here we perform parallel projection on sentiment informa- tion for resource creation and not for typologi- cal analysis. In addition, we propose a method called DomDrift to mitigate the huge domain mis- match between Bible and target domain via an embedding-based con\ufb01dence weighting scheme. 2 Methods In the next sections, we describe (i) the main re- sources required for UniSent and (ii) the steps of its creation and adaptation to new domains. The overview of these steps is also depicted in Fig- ure 1. 2.1 UniSent Required Resources Super-parallel corpus: The dataset we will work with is the Parallel Bible Corpus (PBC).",
  "The overview of these steps is also depicted in Fig- ure 1. 2.1 UniSent Required Resources Super-parallel corpus: The dataset we will work with is the Parallel Bible Corpus (PBC). PBC consists of translations of the New Testament in 1242 languages covering an order of magnitude more languages than any other parallel corpus currently in use in natural language processing research (Mayer and Cysouw, 2014).",
  "Initial sentiment seeds: We use a high-quality English sentiment lexicon called WKWSCI (Khoo and Johnkhan, 2018) as a resource to be projected on other languages. 2.2 UniSent and DomDrift Our contributions are two-fold (i) creation of UniSent using a cross-lingual projection of sen- timent polarities, which needs to be done only once (ii) introducing DomDrift a novel method for adapting UniSent to any newly observed domain by measuring the domain-drift of words in the new domain. The \ufb01rst part needs a sentiment lexicon in one language (here WKWSCI for English) as well as a massively parallel corpus (PBC). For the second part, only a pre-trained embedding space in the target domain is required. In the next sec- tions, we illustrate our method by creating UniSent for one example language (for better readability). The described steps are however repeated for each of the 1000+ languages composing UniSent. The steps are detailed next and illustrated in Figure 1.",
  "In the next sec- tions, we illustrate our method by creating UniSent for one example language (for better readability). The described steps are however repeated for each of the 1000+ languages composing UniSent. The steps are detailed next and illustrated in Figure 1. (i) UniSent creation using cross-lingual projec- tion of sentiment polarities: We project sen- timent polarities from English (source language) to a target language in the parallel corpus us- ing SuperPivot (Asgari and Sch\u00a8utze, 2017). This method projects annotations across 1000+ lan- guages via an alignment graph generated using FastAlign (Dyer et al., 2013) on the PBC corpus. In the FastAlign word alignment pairs (wsource, wtarget), we replace the source words with their sentiment labels from WKWSCI (where available). Subsequently, we search for the words in the target language that are highly correlated with each of the sentiment labels (pos- itive or negative). We use FDR corrected two- sided \u03c72 (Casella and Berger, 2002) score to \ufb01nd these sentiment seeds.",
  "Subsequently, we search for the words in the target language that are highly correlated with each of the sentiment labels (pos- itive or negative). We use FDR corrected two- sided \u03c72 (Casella and Berger, 2002) score to \ufb01nd these sentiment seeds. We denote the vocabulary of the target language in the parallel corpus by Vlt,ds, where lt is the target language and ds the source domain (here the domain of the parallel corpus, i.e. biblical domain).2 This \ufb01rst step gen- erates, in each target language, pairs (wlt,ds, y), where wlt,ds \u2208Vlt,ds is a word in the target lan- guage and source domain and y is a highly cor- related sentiment annotation with wlt,ds. Vocab- ulary Vlt,ds is limited to the words in the paral- 2We call this domain source domain because it will be adapted to a target domain in the subsequent steps. lel corpus 3.",
  "Vocab- ulary Vlt,ds is limited to the words in the paral- 2We call this domain source domain because it will be adapted to a target domain in the subsequent steps. lel corpus 3. Because most existing super-parallel corpora are from the Bible (Mayer and Johannsen, 2016), Vlt,ds besides of being limited in size has the major drawback of originating from a very speci\ufb01c domain. To overcome this limitation, we de\ufb01ne a method to measure domain drifts. In addition, we leverage word embeddings as used in (Rothe et al., 2016) to propagate the annota- tions y to words of a larger vocabulary than Vlt,ds. The overview of UniSent creation is depicted in Figure 1.a. The UniSent lexica and a complete list of 1242 unique languages4 covered by UniSent along with their language family information are provided in the supplementary material.",
  "The overview of UniSent creation is depicted in Figure 1.a. The UniSent lexica and a complete list of 1242 unique languages4 covered by UniSent along with their language family information are provided in the supplementary material. (ii) DomDrift: unsupervised measurement of domain drift: Word embeddings trained with an unsupervised language modeling objective (e.g., skip-gram) are known to preserve the syntactic and the semantic word similarities in the embed- ding space (Mikolov et al., 2013; Pennington et al., 2014). Domain changes will certainly impact the neighborhoods in the embedding space. Thus a comparison of words relative distances in two em- bedding spaces can be used to measure their de- gree of domain shift (Kulkarni et al., 2015; Asgari and Mofrad, 2016). The main purpose of Dom- Drift is to identify words in the sentiment lexi- con having a domain shift in the target domain by comparison of their neighbors in the embedding spaces.",
  "The main purpose of Dom- Drift is to identify words in the sentiment lexi- con having a domain shift in the target domain by comparison of their neighbors in the embedding spaces. DomDrift quanti\ufb01es the domain drift of a given word in the sentiment lexicon regardless of its la- bel, only by comparison of neighbors in the source and target domains\u2019 embedding spaces \u2126lt,ds : Vlt,ds \u2212\u2192Rhs, and \u2126lt,dt : Vlt,dt \u2212\u2192Rht, where hs, ht are the sizes of the source and target embedding spaces. DomDrift quanti\ufb01es word domain drift as follows: (i) In each embedding space, we compute, for each word wlt,dx in the UniSent lexicon, the distance distribution P(wlt,dx, w\u2126s,t) of word wlt,dx with all other words in intersec- tion of source and target embedding spaces, i.e. \u2200wj \u2208w\u2126s,t. For this, we take the l1 normalized cosine distance of the repre- sentation of wlt,dx with all other words in the embedding space.",
  "\u2200wj \u2208w\u2126s,t. For this, we take the l1 normalized cosine distance of the repre- sentation of wlt,dx with all other words in the embedding space. This distribution can 3Because we use this corpus for the cross-lingual projec- tion 4We consider two languages different if they have differ- ent ISO 639-3 codes",
  "source language Parallel corpora in target languages word alignment High-quality Sentiment seed lexicon SuperPivot UniSent sentiment lexica Source domain Target domain lt,dt Embedding of target language in the source domain Embedding of target language in the target domain wlt,dt wlt,ds normalized cos distance normalized cos distance KL-divergence word drift score Adaptable UniSent other words Domain-drift score other words lt,ds lt,ds ls,ds (a) UniSent (b) DomDrift Figure 1: The overview of universal adaptable sentiment lexica. The approach can be divided into two main steps: (a) UniSent creation using SuperPivot method, (b) DomDrift for measurement of word domain drifts.",
  "The approach can be divided into two main steps: (a) UniSent creation using SuperPivot method, (b) DomDrift for measurement of word domain drifts. be regarded as word pro\ufb01le in the domain x (source or target): Pi(wlt,dx, w\u2126s,t) = 1 \u2212cos(\u2212\u2212\u2212\u2192 wlt,dx, \u2212\u2192 wi) P j[1 \u2212cos(\u2212\u2212\u2212\u2192 wlt,dx, \u2212\u2192 wj)], where wi, wj \u2208\u2126s,t and \u2212\u2192 wk is the embed- ding representation of word wk \u2208\u2126s,t in do- main x (source or target). We use w\u2126s,t so that the word pro\ufb01les in the source and target domains are comparable, i.e. they have the same elements. (ii) We compute, for each word wlt,dx a shift weight between vocabularies Vlt,ds and Vlt,dt of source and target spaces.",
  "they have the same elements. (ii) We compute, for each word wlt,dx a shift weight between vocabularies Vlt,ds and Vlt,dt of source and target spaces. This is done by comparing, for each word in the UniSent lex- ica, its pro\ufb01le in the source P(wlt,ds, w\u2126s,t) and target domain P(wlt,dt, w\u2126s,t) using the Kullback-Leibler divergence. More formally for a word w\u2032 its domain drift (\u03bbw\u2032) between the source and the target domains can be calculated as follows. \u03bbw\u2032 = DKL(P(w\u2032lt,ds, w\u2126s,t)\u2225P(w\u2032lt,dt, w\u2126s,t)) The steps of DomDrift are illustrated in Figure 1.b. As also depicted in the \ufb01gure, the calculated weights enhance the universal senti- ment lexica resulting in a \ufb01nal adaptable version (Adaptable UniSent), e.g. the weights will be used to reduce the in\ufb02uence of huge domain mismatches in a con\ufb01dence weighting scheme.",
  "the weights will be used to reduce the in\ufb02uence of huge domain mismatches in a con\ufb01dence weighting scheme. In Figure 2 we illustrate an example of domain drift and explain the workings of our weighting method in \u00a73.4. Once we computed our shift weights, they can be used in any semi-supervised or supervised approach (e.g., sample weights in the logistic regression model). Source embedding \u2126lt,ds: In order to gener- ate \u2126lt,ds, the only necessary resource is the monolingual text of PBC in the target language (source domain). For embedding creation, we use fasttext (Bojanowski et al., 2017) which leverages subword information within the skip- gram architecture. Target embedding \u2126lt,dt: For generation of \u2126lt,dt, we require a monolingual text collection in the target domain to train the embedding space. An alternative is to use pretrained embeddings in the domain of interest (e.g. Twitter or News).",
  "Target embedding \u2126lt,dt: For generation of \u2126lt,dt, we require a monolingual text collection in the target domain to train the embedding space. An alternative is to use pretrained embeddings in the domain of interest (e.g. Twitter or News). In particular, in our experiments, we use skip-gram embeddings, pretrained on Wikipedia for French, Macedonian, Spanish, Czech, and German (Con- neau et al., 2017), as well as German, Italian, French, and Spanish monolingual pretrained em- beddings on Twitter, provided by (Deriu et al., 2017; Cieliebak et al., 2017).",
  "(b) Twitter embedding graph (a) Bible embedding graph\u00a0 Figure 2: Neighbors of the word \u2019sensual\u2019 in Spanish, in the bible embedding graph (a) and the twitter embedding graph (b). Our unsupervised drift weighting method found this word in Spanish to be the most changing word from bible context to the twitter context. Looking more closely at the neighbors, the word sensual in the biblical context has been associated with negative sentiment of sins. However, in the twitter domain, it has a positive sentiment. This example shows how our unsupervised method can improve the quality of sentiment lexica. 2.3 Word sentiment classi\ufb01cation for the evaluation of sentiment lexica In order to evaluate UniSent, we use it as seed lexi- con for word sentiment classi\ufb01cation on top of em- bedding features. Different methods can be used to predict the sentiment of words in the target do- main using embedding spaces. These include su- pervised methods e.g., UltraDense (Rothe et al., 2016), linear classi\ufb01ers and regressors (e.g.",
  "Different methods can be used to predict the sentiment of words in the target do- main using embedding spaces. These include su- pervised methods e.g., UltraDense (Rothe et al., 2016), linear classi\ufb01ers and regressors (e.g. SVM, SVR, and logistic regression) or semi-supervised methods, e.g. SentProp (Hamilton et al., 2016). In this paper, we use logistic regression for the classi\ufb01cation model. Using the language model based embedding space as the representation has two major bene\ufb01ts: First, the semantic continu- ity of the embedding space allows for propaga- tion of sentiment labels to a larger vocabulary. This enables the annotation of further word pairs (wlt,dt, \u02c6y), where wlt,dt \u2208Vlt,dt is a vocabulary of arbitrary size in the target domain. Secondly, us- ing embeddings trained in a speci\ufb01c domain result in the implicit incorporation of semantic structures speci\ufb01c to the target domain (which are re\ufb02ected in the embedding space), i.e. an implicit domain adaptation.",
  "Secondly, us- ing embeddings trained in a speci\ufb01c domain result in the implicit incorporation of semantic structures speci\ufb01c to the target domain (which are re\ufb02ected in the embedding space), i.e. an implicit domain adaptation. UniSent versus con\ufb01dence weighted UniSent: For the word sentiment classi\ufb01cation we train a logistic regression classi\ufb01er with the annotated pairs of (wlt,dt, y) represented in the target embedding space. In order to incorporate the drift weights, we treat the weights coming from DomDrift as sample weights in the logistic regres- sion model, i.e. (wlt,dt, y, sw)\u2019s are the training instances to the logistic regression classi\ufb01er, where sw = 1 \u03bbw , is the seed weight calculated as the inverse of DomDrift score. Our evaluation in section \u00a73.4 shows that this (simple) method is very effective and creates accurate resources. In the hyper-parameter tuning for logistic regression we also \ufb01ne tune the exponent of this weight.",
  "Our evaluation in section \u00a73.4 shows that this (simple) method is very effective and creates accurate resources. In the hyper-parameter tuning for logistic regression we also \ufb01ne tune the exponent of this weight. 3 Experiments and Evaluation 3.1 Experimental Setup Select Gold Standard Data As gold stan- dard sentiment lexica for the evaluation of UniSent, we select manually created lexica in Czech (Veselovsk\u00b4a and Bojar, 2013), Ger- man (Waltinger, 2010), French (Abdaoui et al., 2014), Macedonian (Jovanoski et al., 2016), and Spanish (Perez-Rosas et al., 2012). These lex- ica contain general domain words (as opposed to Twitter or Bible). As gold standard for Twitter we use the emoticon dataset in (Wiebe et al., 2005; Hogenboom et al., 2013) and perform emoticon sentiment prediction for different languages.",
  "3.2 Train-test split In order to evaluate the UniSent, here we create train-test split for training and testing the seeds created in the projection step (see Section \u00a7 2.2). We \ufb01rst split UniSent and our gold standard lexica as illustrated in Figure 3. In order to design a fair evaluation, we form our training and test sets as follows: (i) UniSent-Train-Lexicon: For the evaluation of the UniSent, we use words in UniSent as sentiment seeds for training in the target domain; for this pur- pose, we use words w \u2208A \u222aC (Figure 3). (ii) Manual-Train-Lexicon: In order to obtain an upper bound for the UniSent performance, we compare the use of UniSent-Train-Lexicon against the use of words in the gold standard lexicon as sentiment seeds for the training in the target do- main. For this purpose, we use words w \u2208B \u222aC (Figure 3). (iii) Test-Lexicon: we randomly exclude a set of words in the {Manual-Train-Lexicon \u222aB}.",
  "For this purpose, we use words w \u2208B \u222aC (Figure 3). (iii) Test-Lexicon: we randomly exclude a set of words in the {Manual-Train-Lexicon \u222aB}. In the selection of the sampling size, we make sure that UniSent\u2212Train\u2212Lexicon and Manual\u2212 Train \u2212Lexicon would contain approximately the same number of words (Figure 3). Target\u00a0 embedding space Source embedding space UniSent Lexicon Ground truth lexicon A C B Test set Figure 3: Data split used in the experimental setup of UniSent evaluation: Set (C) is the intersection of the target embedding space words (Wikipedia or Twitter) and the UniSent lexicon as well as the manually cre- ated lexicon. Set (A) is the intersection of the target embedding space words and the UniSent lexicon, ex- cluding set (C). Set (B) is the intersection of the target embedding space words and the manually created lexi- con, excluding set (C).",
  "Set (A) is the intersection of the target embedding space words and the UniSent lexicon, ex- cluding set (C). Set (B) is the intersection of the target embedding space words and the manually created lexi- con, excluding set (C). 3.3 Evaluation As discussed in \u00a72.2, we use the (manually cre- ated) English sentiment lexicon (WKWSCI) in (Khoo and Johnkhan, 2018) as a resource to be projected to over 1000+ languages. We project positive and negative sentiments to create positive and negative sentiment lexica for each language. Our evaluation of this work is two-fold. On the one hand, we evaluate the overall quality of UniSent by comparing it against our manually created gold standard datasets in the Wikipedia domain. Second, we investigate the in\ufb02uence of DomDrift w.r.t. the adaptation of UniSent for Wikipedia and Twitter domains. (i) Evaluation of UniSent vs.",
  "Second, we investigate the in\ufb02uence of DomDrift w.r.t. the adaptation of UniSent for Wikipedia and Twitter domains. (i) Evaluation of UniSent vs. manually created lexica: We compare the application of Unisent for the word sentiment classi\ufb01cation task against the manually created lexica in the following cases: (i) choice of the most frequent sentiment, (ii) use of manually created lexicon as sentiment seeds. We use the train and test seed lexica as discussed in 3.2 for training and testing of the logistic re- gression on top of target embedding features. We also include the sentiment classi\ufb01cation results us- ing the con\ufb01dence weighted version of UniSent, where the shift between the vocabularies of the Bible and Wikipedia are calculated by DomDrift and are used as sample weights in the logistic re- gression model. (ii) Comparison of UniSent vs.",
  "(ii) Comparison of UniSent vs. con\ufb01dence weighted UniSent in the Twitter domain for emoticon prediction: To show that our adaptation method also works well on domains like Twitter, we propose a second evaluation in which we use UniSent together with DomDrift to predict the sen- timent of emoticons in Twitter. Since emoticons are almost language independent, we could use the same resource for the evaluation of German, Ital- ian, French, and Spanish, where their monolingual pretrained embeddings are available for these lan- guages (Deriu et al., 2017; Cieliebak et al., 2017). In the adaptation step, we compute the shift be- tween the vocabularies of Bible and Twitter. We use the UniSent seeds for training a logistic re- gression model on Twitter embedding and evalu- ate the classi\ufb01er for the Emoticon sentiment pre- diction. We perform this evaluation for German, Italian, French, and Spanish, where Twitter pre- trained embedding is available.",
  "Table 1: We evaluate UniSent against the gold standard datasets in Czech, German, French, Macedonian, and Spanish. The two last columns report the accuracy and macro-F1 (averaged F1 over positive and negative classes) of Unisent before and after the application of the drift weighting step. The two \ufb01rst columns report the performance of the baseline and manually created lexicon. Note that the baseline is constantly considering the majority label. Language Freq.",
  "The two \ufb01rst columns report the performance of the baseline and manually created lexicon. Note that the baseline is constantly considering the majority label. Language Freq. sentiment baseline target-language-speci\ufb01c Manual Lexicon (projection) UniSent Lexicon (projection) Con\ufb01dence Weighted UniSent Lexicon acc acc macro-F1 acc macro-F1 acc macro-F1 French 0.62 0.84 0.83 0.73 0.72 0.74 0.74 Macedonian 0.70 0.86 0.84 0.80 0.77 0.81 0.78 Spanish 0.64 0.82 0.80 0.78 0.76 0.80 0.77 Czech 0.62 0.87 0.87 0.82 0.81 0.79 0.78 German 0.52 0.87 0.87 0.82 0.81 0.81 0.80 Table 2: We evaluate UniSent using twitter emoticon dataset.",
  "We use monolingual Twitter embeddings in German, Italian, French, and Spanish. The two last columns report the accuracy and macro-F1 (averaged F1 over positive and negative classes) of Unisent before and after the application of the drift weighting step. Language Freq. sentiment baseline (projection) UniSent Lexicon (projection) Con\ufb01dence Weighted UniSent Lexicon acc acc macro-F1 acc macro-F1 French 0.62 0.73 0.73 0.75 0.74 Spanish 0.62 0.73 0.73 0.76 0.76 German 0.62 0.80 0.79 0.80 0.79 Italian 0.62 0.76 0.76 0.75 0.75 3.4 Results The evaluation results are reported in Tables 1 and 2. Table 1 compares UniSent and its con\ufb01dence weighted version to the manually created lexica in Czech, German, French, Macedonian, and Span- ish as well as a naive baseline of choosing the most frequent sentiment.",
  "Table 1 compares UniSent and its con\ufb01dence weighted version to the manually created lexica in Czech, German, French, Macedonian, and Span- ish as well as a naive baseline of choosing the most frequent sentiment. For all evaluated lan- guages, accuracy as well as macro-F1 are close to 0.8, showing that UniSent is a high-quality re- source performing close enough to manually cre- ated seeds and clearly better than the most fre- quent sentiment baseline. Since in this evaluation the presented languages did not have any further advantage than having a manually created lexicon for evaluation, we can assume that UniSent would work within the same range of accuracy for any of the low-resource languages as long as a monolin- gual embedding (which is also cheap to obtain for low-resource languages) can be available for the target domain. Our drift weighting method brings gains in several languages: French, Macedonian, and Spanish. In Table 2 we compare the quality of UniSent in prediction of the gold standard emoticon senti- ments in the Twitter domain.",
  "Our drift weighting method brings gains in several languages: French, Macedonian, and Spanish. In Table 2 we compare the quality of UniSent in prediction of the gold standard emoticon senti- ments in the Twitter domain. The results show that (i) UniSent clearly outperforms the baseline of the most frequent sentiment label and (ii) our domain adaptation technique brings small improvements for French and Spanish. In order to illustrate the function of DomDrift we visualized the embedding space of biblical do- main and twitter domain for the word achieving the highest drift score in Spanish, i.e., word sen- sual (Figure 2). The neighborhood of this word in both domains is shown in the \ufb01gure. In Biblical texts, this word has the connotation of sin which has a negative polarity. But in the Twitter domain, the same word is associated with sexy, which has a positive polarity. This example shows that for cer- tain pairs of domains and languages use of Dom- Drift weights in the use of sentiment lexicon can improve the performance of sentiment analysis.",
  "But in the Twitter domain, the same word is associated with sexy, which has a positive polarity. This example shows that for cer- tain pairs of domains and languages use of Dom- Drift weights in the use of sentiment lexicon can improve the performance of sentiment analysis. 4 Discussion and Conclusion In this work, we introduced UniSent universal sentiment lexica for 1000+ languages, which is to the best of our knowledge, the largest senti- ment resource to date in terms of the number of covered languages, including many low resource ones. Although UniSent is created based on a spe- ci\ufb01c domain (bible), our evaluation of UniSent on Czech, German, French, Macedonian, and Span-",
  "ish showed that it can achieve macro-F1 scores \u22480.8 in word sentiment classi\ufb01cation, which is comparable to the use of manually annotated re- sources. Given that many of covered languages in UniSent are very low-resource, UniSent can be regarded among the only computational linguis- tic resources available for those low-resource lan- guages making sentiment analysis possible in such low-resource setups. In addition, through accurate prediction of Twitter emoticon sentiment for Ger- man, Italian, French, and Spanish, we showed that UniSent can be even used in a very different target domain and still performs quite well in sentiment analysis. Furthermore, we proposed DomDrift, a method to quantify domain drift for words in the UniSent given an embedding space in the target domain. DomDrift compares the neighborhood of the word in the embedding spaces of source and target do- mains. Incorporation of DomDrift scores in the use of UniSent for sentiment classi\ufb01cation outper- formed vanilla UniSent in French, Spanish, and Macedonian in the Wikipedia domain and French and Spanish in the twitter domain.",
  "Incorporation of DomDrift scores in the use of UniSent for sentiment classi\ufb01cation outper- formed vanilla UniSent in French, Spanish, and Macedonian in the Wikipedia domain and French and Spanish in the twitter domain. Not further im- proving the results for German, Czech and Ital- ian languages might be because of the suf\ufb01ciency of the target embedding usage for domain adapta- tion (Jurafsky and Martin, 2014) in those. On the other hand, the fact that Spanish and French per- formances improved on both Wikipedia and Twit- ter domains when DomDrift is used, might show that the necessity of DomDrift can be related to certain property of the target language, which can be further explored as future work. References Amine Abdaoui, J\u00b4erome Az\u00b4e, Sandra Bringay, and Pas- cal Poncelet. 2014. Feel: French extended emo- tional lexicon. ELRA Catalogue of Language Re- sources. ISLRN, pages 041\u2013639. Haithem A\ufb02i, Sorcha McGuire, and Andy Way. 2017.",
  "2014. Feel: French extended emo- tional lexicon. ELRA Catalogue of Language Re- sources. ISLRN, pages 041\u2013639. Haithem A\ufb02i, Sorcha McGuire, and Andy Way. 2017. Sentiment translation for low resourced languages: Experiments on irish general election tweets. In 18th International Conference on Computational Linguistics and Intelligent Text Processing. \u02c7Zeljko Agi\u00b4c, Anders Johannsen, Barbara Plank, H\u00b4ector Mart\u00b4\u0131nez Alonso, Natalie Schluter, and Anders S\u00f8gaard. 2016. Multilingual projection for parsing truly low-resource languages. Transactions of the Association for Computational Linguistics, 4:301\u2013 312. Ehsaneddin Asgari and Mohammad R.K. Mofrad. 2016. Comparing \ufb01fty natural languages and twelve genetic languages using word embedding language divergence (weld) as a quantitative measure of lan- guage distance.",
  "Ehsaneddin Asgari and Mohammad R.K. Mofrad. 2016. Comparing \ufb01fty natural languages and twelve genetic languages using word embedding language divergence (weld) as a quantitative measure of lan- guage distance. In In Proceedings of the NAACL- HLT Workshop on Multilingual and Cross-lingual Methods in NLP, San Diego, CA, pages 65\u201374. As- sociation for Computational Linguistics. Ehsaneddin Asgari and Hinrich Sch\u00a8utze. 2017. Past, present, future: A computational investigation of the typology of tense in 1000 languages. In EMNLP. Jeremy Barnes, Roman Klinger, and Sabine Schulte im Walde. 2018a. Bilingual sentiment embeddings: Joint projection of sentiment across languages. In ACL. Jeremy Barnes, Roman Klinger, and Sabine Schulte im Walde. 2018b. Projecting embeddings for domain adaptation: Joint modeling of sentiment analysis in diverse domains. In COLING.",
  "Bilingual sentiment embeddings: Joint projection of sentiment across languages. In ACL. Jeremy Barnes, Roman Klinger, and Sabine Schulte im Walde. 2018b. Projecting embeddings for domain adaptation: Joint modeling of sentiment analysis in diverse domains. In COLING. Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. TACL. Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011. Twitter mood predicts the stock market. Journal of computational science, 2(1):1\u20138. George Casella and Roger L Berger. 2002. Statistical inference, volume 2. Duxbury Paci\ufb01c Grove, CA. Yanqing Chen and Steven Skiena. 2014. Building sen- timent lexicons for all major languages. In Proceed- ings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Pa- pers), pages 383\u2013389, Baltimore, Maryland.",
  "2014. Building sen- timent lexicons for all major languages. In Proceed- ings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Pa- pers), pages 383\u2013389, Baltimore, Maryland. Asso- ciation for Computational Linguistics. Mark Cieliebak, Jan Milan Deriu, Dominic Egger, and Fatih Uzdilli. 2017. A twitter corpus and benchmark resources for german sentiment analysis. In Pro- ceedings of the Fifth International Workshop on Nat- ural Language Processing for Social Media, pages 45\u201351. Christopher Cieri, Mike Maxwell, Stephanie Strassel, and Jennifer Tracey. 2016. Selection criteria for low resource language programs. In LREC. Alexis Conneau, Guillaume Lample, Marc\u2019Aurelio Ranzato, Ludovic Denoyer, and Herv\u00b4e J\u00b4egou. 2017. Word translation without parallel data. arXiv preprint arXiv:1710.04087. Mohammad Darwich, Shahrul Azman Mohd Noah, and Nazlia Omar.",
  "2017. Word translation without parallel data. arXiv preprint arXiv:1710.04087. Mohammad Darwich, Shahrul Azman Mohd Noah, and Nazlia Omar. 2017. Minimally-supervised sen- timent lexicon induction model: A case study of malay sentiment analysis. In International Work- shop on Multi-disciplinary Trends in Arti\ufb01cial Intel- ligence, pages 225\u2013237. Springer. Jan Deriu, Aurelien Lucchi, Valeria De Luca, Aliak- sei Severyn, Simon M\u00a8uller, Mark Cieliebak, Thomas Hofmann, and Martin Jaggi. 2017. Leveraging large amounts of weakly supervised data for multi- language sentiment classi\ufb01cation. In Proceedings of the 26th international conference on world wide web, pages 1045\u20131052. International World Wide Web Conferences Steering Committee. Chris Dyer, Victor Chahuneau, and Noah A Smith. 2013. A simple, fast, and effective reparameteriza- tion of ibm model 2.",
  "International World Wide Web Conferences Steering Committee. Chris Dyer, Victor Chahuneau, and Noah A Smith. 2013. A simple, fast, and effective reparameteriza- tion of ibm model 2. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 644\u2013648.",
  "Guy Emerson, Liling Tan, Susanne Fertmann, Alexis Palmer, and Michaela Regneri. 2014. Seedling: Building and using a seed corpus for the human lan- guage project. In Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 77\u201385. William L Hamilton, Kevin Clark, Jure Leskovec, and Dan Jurafsky. 2016. Inducing domain-speci\ufb01c senti- ment lexicons from unlabeled corpora. In Proceed- ings of the Conference on Empirical Methods in Nat- ural Language Processing. Conference on Empirical Methods in Natural Language Processing, volume 2016, page 595. NIH Public Access. Viktor Hangya, Fabienne Braune, Alexander Fraser, and Hinrich Sch\u00a8utze. 2018. Two methods for do- main adaptation of bilingual tasks: Delightfully sim- ple and broadly applicable. In ACL.",
  "NIH Public Access. Viktor Hangya, Fabienne Braune, Alexander Fraser, and Hinrich Sch\u00a8utze. 2018. Two methods for do- main adaptation of bilingual tasks: Delightfully sim- ple and broadly applicable. In ACL. Alexander Hogenboom, Daniella Bal, Flavius Fras- incar, Malissa Bal, Franciska de Jong, and Uzay Kaymak. 2013. Exploiting emoticons in sentiment analysis. In Proceedings of the 28th annual ACM symposium on applied computing, pages 703\u2013710. ACM. Daniel J Hopkins and Gary King. 2010. A method of automated nonparametric content analysis for so- cial science. American Journal of Political Science, 54(1):229\u2013247. Dame Jovanoski, Veno Pachovski, and Preslav Nakov. 2016. On the impact of seed words on sentiment po- larity lexicon induction.",
  "American Journal of Political Science, 54(1):229\u2013247. Dame Jovanoski, Veno Pachovski, and Preslav Nakov. 2016. On the impact of seed words on sentiment po- larity lexicon induction. In Proceedings of COLING 2016, the 26th International Conference on Compu- tational Linguistics: Technical Papers, pages 1557\u2013 1567. Dan Jurafsky and James H Martin. 2014. Speech and language processing, volume 3. Pearson London. Christopher SG Khoo and Sathik Basha Johnkhan. 2018. Lexicon-based sentiment analysis: Compar- ative evaluation of six sentiment lexicons. Journal of Information Science, 44(4):491\u2013511. Vivek Kulkarni, Rami Al-Rfou, Bryan Perozzi, and Steven Skiena. 2015. Statistically signi\ufb01cant de- tection of linguistic change. In Proceedings of the 24th International Conference on World Wide Web, pages 625\u2013635. International World Wide Web Con- ferences Steering Committee.",
  "2015. Statistically signi\ufb01cant de- tection of linguistic change. In Proceedings of the 24th International Conference on World Wide Web, pages 625\u2013635. International World Wide Web Con- ferences Steering Committee. Thomas Mayer and Michael Cysouw. 2014. Creat- ing a massively parallel bible corpus. Oceania, 135(273):40. Thomas Mayer and Anders Johannsen. 2016. Multilin- gual projection for parsing truly low-resource lan- guages. Transactions of the Association for Compu- tational Linguistics, 4:301\u2013312. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their composition- ality. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Ad- vances in Neural Information Processing Systems 26, pages 3111\u20133119. Curran Associates, Inc.",
  "Curran Associates, Inc. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 confer- ence on empirical methods in natural language pro- cessing (EMNLP), pages 1532\u20131543. Veronica Perez-Rosas, Carmen Banea, and Rada Mi- halcea. 2012. Learning sentiment lexicons in span- ish. In LREC, volume 12, page 73. Sascha Rothe, Sebastian Ebert, and Hinrich Sch\u00a8utze. 2016. Ultradense word embeddings by orthogonal transformation. In Proceedings of the 2016 Con- ference of the North American Chapter of the As- sociation for Computational Linguistics: Human Language Technologies, pages 767\u2013777, San Diego, California. Association for Computational Linguis- tics. Kate\u02c7rina Veselovsk\u00b4a and Ond\u02c7rej Bojar. 2013. Czech sublex 1.0.",
  "Association for Computational Linguis- tics. Kate\u02c7rina Veselovsk\u00b4a and Ond\u02c7rej Bojar. 2013. Czech sublex 1.0. Charles University, Faculty of Mathe- matics and Physics, Institute of Formal .... Ulli Waltinger. 2010. Germanpolarityclues: A lexical resource for german sentiment analysis. In LREC, pages 1638\u20131642. Hao Wang, Dogan Can, Abe Kazemzadeh, Franc\u00b8ois Bar, and Shrikanth Narayanan. 2012. A system for real-time twitter sentiment analysis of 2012 us pres- idential election cycle. In Proceedings of the ACL 2012 System Demonstrations, pages 115\u2013120. As- sociation for Computational Linguistics. Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emo- tions in language. Language resources and evalua- tion, 39(2-3):165\u2013210.",
  "As- sociation for Computational Linguistics. Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emo- tions in language. Language resources and evalua- tion, 39(2-3):165\u2013210. Felix Ming Fai Wong, Chee Wei Tan, Soumya Sen, and Mung Chiang. 2016. Quantifying political lean- ing from tweets, retweets, and retweeters. IEEE transactions on knowledge and data engineering, 28(8):2158\u20132172."
]