[
  "A Joint Model for Multimodal Document Quality Assessment Aili Shen, Bahar Salehi, Timothy Baldwin, Jianzhong Qi School of Computing and Information Systems, The University of Melbourne, Victoria, Australia ailis@student.unimelb.edu.au, salehi.b@unimelb.edu.au, tb@ldwin.net, jianzhong.qi@unimelb.edu.au Abstract The quality of a document is affected by various factors, including grammaticality, readability, stylistics, and ex- pertise depth, making the task of document quality as- sessment a complex one. In this paper, we explore this task in the context of assessing the quality of Wikipedia articles and academic papers. Observing that the vi- sual rendering of a document can capture implicit qual- ity indicators that are not present in the document text \u2014 such as images, font choices, and visual layout \u2014 we propose a joint model that combines the text con- tent with a visual rendering of the document for docu- ment quality assessment. Experimental results over two datasets reveal that textual and visual features are com- plementary, achieving state-of-the-art results.",
  "Experimental results over two datasets reveal that textual and visual features are com- plementary, achieving state-of-the-art results. Introduction The task of document quality assessment is to automatically assess a document according to some prede\ufb01ned inventory of quality labels. This can take many forms, including es- say scoring (quality = language quality, coherence, and rel- evance to a topic), job application \ufb01ltering (quality = suit- ability for role + visual/presentational quality of the applica- tion), or answer selection in community question answer- ing (quality = actionability + relevance of the answer to the question). In the case of this paper, we focus on docu- ment quality assessment in two contexts: Wikipedia docu- ment quality classi\ufb01cation, and whether a paper submitted to a conference was accepted or not. Automatic quality assessment has obvious bene\ufb01ts in terms of time savings and tractability in contexts where the volume of documents is large.",
  "Automatic quality assessment has obvious bene\ufb01ts in terms of time savings and tractability in contexts where the volume of documents is large. In the case of dynamic docu- ments (possibly with multiple authors), such as in the case of Wikipedia, it is particularly pertinent, as any edit potentially has implications for the quality label of that document (and around 10 English Wikipedia documents are edited per sec- ond1). Furthermore, when the quality assessment task is de- centralized (as in the case of Wikipedia and academic paper assessment), quality criteria are often applied inconsistently by different people, where an automatic document quality assessment system could potentially reduce inconsistencies and enable immediate author feedback. 1https://en.wikipedia.org/wiki/Wikipedia: Statistics (a) Featured article (b) Lower quality article Figure 1: Visual renderings of two example Wikipedia doc- uments with different quality labels (not intended to be read- able). Current studies on document quality assessment mainly focus on textual features. For example, Warncke-Wang et al. (2015) examine features such as the article length and the number of headings to predict the quality class of a Wikipedia article.",
  "Current studies on document quality assessment mainly focus on textual features. For example, Warncke-Wang et al. (2015) examine features such as the article length and the number of headings to predict the quality class of a Wikipedia article. In contrast to these studies, in this paper, we propose to combine text features with visual features, based on a visual rendering of the document. Figure 1 il- lustrates our intuition, relative to Wikipedia articles. With- out being able to read the text, we can tell that the article in Figure 1a has higher quality than Figure 1b, as it has a detailed infobox, extensive references, and a variety of im- ages. Based on this intuition, we aim to answer the following question: can we achieve better accuracy on document qual- ity assessment by complementing textual features with visual features? Our visual model is based on \ufb01ne-tuning an Inception V3 model (Szegedy et al. 2016) over visual renderings of doc- uments, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model.",
  "Our visual model is based on \ufb01ne-tuning an Inception V3 model (Szegedy et al. 2016) over visual renderings of doc- uments, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. We perform experiments on two datasets: a Wikipedia dataset novel to this paper, and an arXiv dataset provided by Kang et al. (2018) split into three sub-parts based on subject cate- gory. Experimental results on the visual renderings of docu- ments show that implicit quality indicators, such as images and visual layout, can be captured by an image classi\ufb01er, at a level comparable to a text classi\ufb01er. When we combine the two models, we achieve state-of-the-art results over 3/4 of our datasets. This paper makes the following contributions: arXiv:1901.01010v2  [cs.CL]  14 Jan 2019",
  "(i) this is the \ufb01rst study to use visual renderings of docu- ments to capture implicit quality indicators not present in the document text, such as document visual layout; experimental results show that we can obtain a 2.9% higher accuracy using only visual renderings of docu- ments compared with using only textual features over a Wikipedia dataset, and we can obtain competitive re- sults over an arXiv dataset. (ii) we further propose a joint model to predict document quality combining visual and textual features; we ob- serve further improvements on the Wikipedia dataset and on two of the three arXiv subsets, indicating that visual and textual features are complementary. (iii) we construct a large-scale Wikipedia dataset with full textual data, visual renderings, and quality class labels; we also supplement the existing arXiv datasets with visual renderings of each document. All code and data associated with this research will be re- leased on publication. Related Work A variety of approaches have been proposed for document quality assessment across different domains: Wikipedia arti- cle quality assessment, academic paper rating, content qual- ity assessment in community question answering (cQA), and essay scoring.",
  "Related Work A variety of approaches have been proposed for document quality assessment across different domains: Wikipedia arti- cle quality assessment, academic paper rating, content qual- ity assessment in community question answering (cQA), and essay scoring. Among these approaches, some use hand- crafted features while others use neural networks to learn features from documents. For each domain, we \ufb01rst brie\ufb02y describe feature-based approaches and then review neural network-based approaches. Wikipedia article quality assessment: Quality assess- ment of Wikipedia articles is a task that assigns a quality class label to a given Wikipedia article, mirroring the qual- ity assessment process that the Wikipedia community carries out manually. Many approaches have been proposed that use features from the article itself, meta-data features (e.g., the editors, and Wikipedia article revision history), or a combi- nation of the two. Article-internal features capture informa- tion such as whether an article is properly organized, with supporting evidence, and with appropriate terminology. For example, Lipka and Stein (2010) use writing styles repre- sented by binarized character trigram features to identify featured articles.",
  "Article-internal features capture informa- tion such as whether an article is properly organized, with supporting evidence, and with appropriate terminology. For example, Lipka and Stein (2010) use writing styles repre- sented by binarized character trigram features to identify featured articles. Warncke-Wang, Cosley, and Riedl (2013) and Warncke-Wang et al. (2015) explore the number of headings, images, and references in the article. Dang and Ignat (2016a) use nine readability scores, such as the per- centage of dif\ufb01cult words in the document, to measure the quality of the article. Meta-data features, which are indirect indicators of article quality, are usually extracted from revi- sion history, and the interaction between editors and articles. For example, one heuristic that has been proposed is that higher-quality articles have more edits (Dalip et al. 2017; Dalip et al. 2014). Wang and Iwaihara (2011) use the per- centage of registered editors and the total number of editors of an article.",
  "2017; Dalip et al. 2014). Wang and Iwaihara (2011) use the per- centage of registered editors and the total number of editors of an article. Article\u2013editor dependencies have also been ex- plored. For example, Stein and Hess (2007) use the author- ity of editors to measure the quality of Wikipedia articles, where the authority of editors is determined by the articles they edit. Deep learning approaches to predicting Wikipedia article quality have also been proposed. For example, Dang and Ignat (2016b) use a version of doc2vec (Le and Mikolov 2014) to represent articles, and feed the document embed- dings into a four hidden layer neural network. Shen, Qi, and Baldwin (2017) \ufb01rst obtain sentence representations by av- eraging words within a sentence, and then apply a biLSTM (Hochreiter and Schmidhuber 1997) to learn a document- level representation, which is combined with hand-crafted features as side information. Dang and Ignat (2017) exploit two stacked biLSTMs to learn document representations.",
  "Dang and Ignat (2017) exploit two stacked biLSTMs to learn document representations. Academic paper rating: Academic paper rating is a rel- atively new task in NLP/AI, with the basic formulation be- ing to automatically predict whether to accept or reject a pa- per. Kang et al. (2018) explore hand-crafted features, such as the length of the title, whether speci\ufb01c words (such as outperform, state-of-the-art, and novel) appear in the ab- stract, and an embedded representation of the abstract as input to different downstream learners, such as logistic re- gression, decision tree, and random forest. Yang et al. (2018) exploit a modularized hierarchical convolutional neural net- work (CNN), where each paper section is treated as a mod- ule. For each paper section, they train an attention-based CNN, and an attentive pooling layer is applied to the con- catenated representation of each section, which is then fed into a softmax layer.",
  "For each paper section, they train an attention-based CNN, and an attentive pooling layer is applied to the con- catenated representation of each section, which is then fed into a softmax layer. Content quality assessment in cQA: Automatic quality assessment in cQA is the task of determining whether an an- swer is of high quality, selected as the best answer, or ranked higher than other answers. To measure answer content qual- ity in cQA, researchers have exploited various features from different sources, such as the answer content itself, the an- swerer\u2019s pro\ufb01le, interactions among users, and usage of the content. The most common feature used is the answer length (Jeon et al. 2006; Suryanto et al. 2009), with other features including: syntactic and semantic features, such as read- ability scores. (Agichtein et al. 2008); similarity between the question and the answer at lexical, syntactic, and se- mantic levels (Agichtein et al. 2008; Belinkov et al. 2015; Hou et al.",
  "(Agichtein et al. 2008); similarity between the question and the answer at lexical, syntactic, and se- mantic levels (Agichtein et al. 2008; Belinkov et al. 2015; Hou et al. 2015); or user data (e.g., a user\u2019s status points or the number of answers written by the user). There have also been approaches using neural networks. For example, Suggu et al. (2016) combine CNN-learned representations with hand-crafted features to predict answer quality. Zhou et al. (2015) use a 2-dimensional CNN to learn the se- mantic relevance of an answer to the question, and apply an LSTM to the answer sequence to model thread con- text. Guzm\u00b4an, M`arquez, and Nakov (2016) and Guzm\u00b4an, Nakov, and M`arquez (2016) model the problem similarly to machine translation quality estimation, treating answers as competing translation hypotheses and the question as the reference translation, and apply neural machine translation to the problem.",
  "Essay scoring: Automated essay scoring is the task of assigning a score to an essay, usually in the context of assessing the language ability of a language learner. The quality of an essay is affected by the following four pri-",
  "mary dimensions: topic relevance, organization and coher- ence, word usage and sentence complexity, and grammar and mechanics. To measure whether an essay is relevant to its \u201cprompt\u201d (the description of the essay topic), lexical and semantic overlap is commonly used (Persing and Ng 2014; Phandi, Chai, and Ng 2015). Attali and Burstein (2004) ex- plore word features, such as the number of verb formation errors, average word frequency, and average word length, to measure word usage and lexical complexity. Cummins, Zhang, and Briscoe (2016) use sentence structure features to measure sentence variety. The effects of grammatical and mechanic errors on the quality of an essay are measured via word and part-of-speech n-gram features and \u201cmechanics\u201d features (Persing and Ng 2013) (e.g., spelling, capitalization, and punctuation), respectively. Taghipour and Ng (2016), Alikaniotis, Yannakoudakis, and Rei (2016), and Tay et al.",
  "Taghipour and Ng (2016), Alikaniotis, Yannakoudakis, and Rei (2016), and Tay et al. (2018) use an LSTM to obtain an essay representation, which is used as the basis for classi\ufb01cation. Similarly, Dong, Zhang, and Yang (2017) utilize a CNN to obtain sentence representation and an LSTM to obtain essay representation, with an attention layer at both the sentence and essay levels. The Proposed Joint Model We treat document quality assessment as a classi\ufb01cation problem, i.e., given a document, we predict its quality class (e.g., whether an academic paper should be accepted or rejected). The proposed model is a joint model that inte- grates visual features learned through Inception V3 with tex- tual features learned through a biLSTM. In this section, we present the details of the visual and textual embeddings, and \ufb01nally describe how we combine the two. We return to dis- cuss hyper-parameter settings and the experimental con\ufb01gu- ration in the Experiments section.",
  "In this section, we present the details of the visual and textual embeddings, and \ufb01nally describe how we combine the two. We return to dis- cuss hyper-parameter settings and the experimental con\ufb01gu- ration in the Experiments section. Visual Embedding Learning A wide range of models have been proposed to tackle the im- age classi\ufb01cation task, such as VGG (Simonyan and Zisser- man 2014), ResNet (He et al. 2016), Inception V3 (Szegedy et al. 2016), and Xception (Chollet 2017). However, to the best of our knowledge, there is no existing work that has proposed to use visual renderings of documents to assess document quality. In this paper, we use Inception V3 pre- trained on ImageNet2 (\u201cINCEPTION\u201d hereafter) to obtain vi- sual embeddings of documents, noting that any image classi- \ufb01er could be applied to our task. The input to INCEPTION is a visual rendering (screenshot) of a document, and the out- put is a visual embedding, which we will later integrate with our textual embedding.",
  "The input to INCEPTION is a visual rendering (screenshot) of a document, and the out- put is a visual embedding, which we will later integrate with our textual embedding. Based on the observation that it is dif\ufb01cult to decide what types of convolution to apply to each layer (such as 3\u00d73 or 5\u00d75), the basic Inception model applies multiple convo- lution \ufb01lters in parallel and concatenates the resulting fea- tures, which are fed into the next layer. This has the bene- \ufb01t of capturing both local features through smaller convo- lutions and abstracted features through larger convolutions. INCEPTION is a hybrid of multiple Inception models of dif- ferent architectures. To reduce computational cost, INCEP- 2http://www.image-net.org/ document text document visual rendering (screenshot image) BILSTM INCEPTION GlobalAveragePooling2D Maxpooling Feed Forward Layer \u02c6c softmax Figure 2: Overview of the proposed model. TION also modi\ufb01es the basic model by applying a 1\u00d71 con- volution to the input and factorizing larger convolutions into smaller ones.",
  "TION also modi\ufb01es the basic model by applying a 1\u00d71 con- volution to the input and factorizing larger convolutions into smaller ones. Textual Embedding Learning We adopt a bi-directional LSTM model to generate textual embeddings for document quality assessment, following the method of Shen, Qi, and Baldwin (2017) (\u201cBILSTM\u201d here- after). The input to BILSTM is a textual document, and the output is a textual embedding, which will later integrate with the visual embedding. For BILSTM, each word is represented as a word em- bedding (Bengio et al. 2003), and an average-pooling layer is applied to the word embeddings to obtain the sentence embedding, which is fed into a bi-directional LSTM to gen- erate the document embedding from the sentence embed- dings. Then a max-pooling layer is applied to select the most salient features from the component sentences. The Joint Model The proposed joint model (\u201cJOINT\u201d hereafter) combines the visual and textual embeddings (output of INCEPTION and BILSTM) via a simple feed-forward layer and softmax over the document label set, as shown in Figure 2.",
  "The Joint Model The proposed joint model (\u201cJOINT\u201d hereafter) combines the visual and textual embeddings (output of INCEPTION and BILSTM) via a simple feed-forward layer and softmax over the document label set, as shown in Figure 2. We optimize our model based on cross-entropy loss. Experiments In this section, we \ufb01rst describe the two datasets used in our experiments: (1) Wikipedia, and (2) arXiv. Then, we report the experimental details and results. Datasets Wikipedia dataset The Wikipedia dataset consists of ar- ticles from English Wikipedia, with quality class labels as- signed by the Wikipedia community. Wikipedia articles are labelled with one of six quality classes, in descending order of quality: Featured Article (\u201cFA\u201d), Good Article (\u201cGA\u201d), B-class Article (\u201cB\u201d), C-class Article (\u201cC\u201d), Start Article (\u201cStart\u201d), and Stub Article (\u201cStub\u201d). A description of the criteria associated with the different classes can be found",
  "Class Train Dev Test Total FA 4000 500 500 5000 GA 4000 500 500 5000 B 4000 500 455 4955 C 4000 500 467 4967 Start 4000 500 451 4951 Stub 4000 500 421 4921 Total 24000 3000 2794 29794 Table 1: Wikipedia dataset. in the Wikipedia grading scheme page.3 The quality class of a Wikipedia article is assigned by Wikipedia reviewers or any registered user, who can discuss through the article\u2019s talk page4 to reach consensus. We constructed the dataset by \ufb01rst crawling all articles from each quality class repos- itory, e.g., we get FA articles by crawling pages from the FA repository: https://en.wikipedia.org/wiki/ Category:Featured_articles. This resulted in around 5K FA, 28K GA, 212K B, 533K C, 2.6M Start, and 3.2M Stub articles. We randomly sampled 5,000 articles from each quality class and removed all redirect pages, resulting in a dataset of 29,794 articles.",
  "We randomly sampled 5,000 articles from each quality class and removed all redirect pages, resulting in a dataset of 29,794 articles. As the wikitext contained in each doc- ument contains markup relating to the document category such as {Featured Article} or {geo-stub}, which reveals the label, we remove such information. We additionally ran- domly partitioned this dataset into training, development, and test splits based on a ratio of 8:1:1. Details of the dataset are summarized in Table 1. We generate a visual representation of each document via a 1,000\u00d72,000-pixel screenshot of the article via a Phan- tomJS script over the rendered version of the article,5 en- suring that the screenshot and wikitext versions of the arti- cle are the same version. Any direct indicators of document quality (such as the FA indicator, which is a bronze star icon in the top right corner of the webpage) are removed from the screenshot. arXiv dataset The arXiv dataset (Kang et al.",
  "Any direct indicators of document quality (such as the FA indicator, which is a bronze star icon in the top right corner of the webpage) are removed from the screenshot. arXiv dataset The arXiv dataset (Kang et al. 2018) con- sists of three subsets of academic articles under the arXiv repository of Computer Science (cs), from the three sub- ject areas of: Arti\ufb01cial Intelligence (cs.ai), Computation and Language (cs.cl), and Machine Learning (cs.lg). In line with the original dataset formulation (Kang et al. 2018), a paper is considered to have been accepted (i.e. is positively labeled) if it matches a paper in the DBLP database or is oth- erwise accepted by any of the following conferences: ACL, EMNLP, NAACL, EACL, TACL, NIPS, ICML, ICLR, or AAAI.",
  "is positively labeled) if it matches a paper in the DBLP database or is oth- erwise accepted by any of the following conferences: ACL, EMNLP, NAACL, EACL, TACL, NIPS, ICML, ICLR, or AAAI. Failing this, it is considered to be rejected (noting that some of the papers may not have been submitted to one 3https://en.wikipedia.org/wiki/Template: Grading_scheme 4https://en.wikipedia.org/wiki/Help: Talk_pages 5https://github.com/ariya/phantomjs/blob/ master/examples/rasterize.js Subject Accepted Train Dev Test Total cs.ai 10% 3682 205 205 4092 cs.cl 30% 2374 132 132 2638 cs.lg 32% 4543 252 253 5048 Table 2: arXiv dataset. \u201cAccepted\u201d indicates the proportion of accepted papers in the given subject. of these conferences). The median numbers of pages for pa- pers in cs.ai, cs.cl, and cs.lg are 11, 10, and 12, respectively.",
  "\u201cAccepted\u201d indicates the proportion of accepted papers in the given subject. of these conferences). The median numbers of pages for pa- pers in cs.ai, cs.cl, and cs.lg are 11, 10, and 12, respectively. To make sure each page in the PDF \ufb01le has the same size in the screenshot, we crop the PDF \ufb01le of a paper to the \ufb01rst 12; we pad the PDF \ufb01le with blank pages if a PDF \ufb01le has less than 12 pages, using the PyPDF2 Python package.6 We then use ImageMagick7 to convert the 12-page PDF \ufb01le to a single 1,000\u00d72,000 pixel screenshot. Table 2 details this dataset, where the \u201cAccepted\u201d column denotes the percent- age of positive instances (accepted papers) in each subset. Experimental Setting As discussed above, our model has two main components \u2014 BILSTM and INCEPTION\u2014 which generate textual and vi- sual representations, respectively.",
  "Table 2 details this dataset, where the \u201cAccepted\u201d column denotes the percent- age of positive instances (accepted papers) in each subset. Experimental Setting As discussed above, our model has two main components \u2014 BILSTM and INCEPTION\u2014 which generate textual and vi- sual representations, respectively. For the BILSTM compo- nent, the documents are preprocessed as described in Shen, Qi, and Baldwin (2017), where an article is divided into sentences and tokenized using NLTK (Bird 2006). Words appearing more than 20 times are retained when building the vocabulary. All other words are replaced by the spe- cial UNK token. We use the pre-trained GloVe (Pennington, Socher, and Manning 2014) 50-dimensional word embed- dings to represent words. For words not in GloVe, word em- beddings are randomly initialized based on sampling from a uniform distribution U(\u22121, 1). All word embeddings are up- dated in the training process. We set the LSTM hidden layer size to 256.",
  "For words not in GloVe, word em- beddings are randomly initialized based on sampling from a uniform distribution U(\u22121, 1). All word embeddings are up- dated in the training process. We set the LSTM hidden layer size to 256. The concatenation of the forward and backward LSTMs thus gives us 512 dimensions for the document em- bedding. A dropout layer is applied at the sentence and doc- ument level, respectively, with a probability of 0.5. For INCEPTION, we adopt data augmentation techniques in the training with a \u201cnearest\u201d \ufb01lling mode, a zoom range of 0.1, a width shift range of 0.1, and a height shift range of 0.1. As the original screenshots have the size of 1,000\u00d72,000 pixels, they are resized to 500\u00d7500 to feed into INCEPTION, where the input shape is (500, 500, 3). A dropout layer is ap- plied with a probability of 0.5. Then, a GlobalAveragePool- ing2D layer is applied, which produces a 2,048 dimensional representation.",
  "A dropout layer is ap- plied with a probability of 0.5. Then, a GlobalAveragePool- ing2D layer is applied, which produces a 2,048 dimensional representation. For the JOINT model, we get a representation of 2,560 dimensions by concatenating the 512 dimensional represen- tation from the BILSTM with the 2,048 dimensional rep- resentation from INCEPTION. The dropout layer is applied to the two components with a probability of 0.5. For BIL- STM, we use a mini-batch size of 128 and a learning rate of 6https://pypi.org/project/PyPDF2/ 7https://www.imagemagick.org/script/index. php",
  "0.001. For both INCEPTION and joint model, we use a mini- batch size of 16 and a learning rate of 0.0001. All hyper- parameters were set empirically over the development data, and the models were optimized using the Adam optimizer (Kingma and Ba 2014). In the training phase, the weights in INCEPTION are initialized by parameters pretrained on ImageNet, and the weights in BILSTM are randomly initialized (except for the word embeddings). We train each model for 50 epochs. However, to prevent over\ufb01tting, we adopt early stopping, where we stop training the model if the performance on the development set does not improve for 20 epochs. For eval- uation, we use (micro-)accuracy, following previous studies (Dang and Ignat 2016a; Kang et al. 2018). Baseline Approaches We compare our models against the following \ufb01ve baselines: \u2022 MAJORITY: the model labels all test samples with the ma- jority class of the training data. \u2022 BENCHMARK: a benchmark method from the literature.",
  "2018). Baseline Approaches We compare our models against the following \ufb01ve baselines: \u2022 MAJORITY: the model labels all test samples with the ma- jority class of the training data. \u2022 BENCHMARK: a benchmark method from the literature. In the case of Wikipedia, this is Dang and Ignat (2016a), who use structural features and readability scores as fea- tures to build a random forest classi\ufb01er; for arXiv, this is Kang et al. (2018), who use hand-crafted features, such as the number of references and TF-IDF weighted bag-of- words in abstract, to build a classi\ufb01er based on the best of logistic regression, multi-layer perception, and AdaBoost. \u2022 DOC2VEC: doc2vec (Le and Mikolov 2014) to learn doc- ument embeddings with a dimension of 500, and a 4- layer feed-forward classi\ufb01cation model on top of this, with 2000, 1000, 500, and 200 dimensions, respectively.",
  "\u2022 BILSTM: \ufb01rst derive a sentence representation by averag- ing across words in a sentence, then feed the sentence rep- resentation into a biLSTM and a maxpooling layer over output sequence to learn a document level representation with a dimension of 512, which is used to predict docu- ment quality. \u2022 INCEPTIONFIXED: the frozen INCEPTION model, where only parameters in the last layer are \ufb01ne-tuned during training. The hyper-parameters of BENCHMARK, DOC2VEC, and BILSTM are based on the corresponding papers except that: (1) we \ufb01ne-tune the feed forward layer of DOC2VEC on the development set and train the model 300 epochs on Wikipedia and 50 epochs on arXiv; (2) we do not use hand- crafted features for BILSTM as we want the baselines to be comparable to our models, and the main focus of this paper is not to explore the effects of hand-crafted features (e.g., see Shen, Qi, and Baldwin (2017)).",
  "Experimental Results Table 3 shows the performance of the different models over our two datasets, in the form of the average accuracy on the test set (along with the standard deviation) over 10 runs, with different random initializations. On Wikipedia, we observe that the performance of BIL- STM, INCEPTION, and JOINT is much better than that of all four baselines. INCEPTION achieves 2.9% higher accu- racy than BILSTM. The performance of JOINT achieves an accuracy of 59.4%, which is 5.3% higher than using tex- tual features alone (BILSTM) and 2.4% higher than using visual features alone (INCEPTION). Based on a one-tailed Wilcoxon signed-rank test, the performance of JOINT is sta- tistically signi\ufb01cant (p < 0.05). This shows that the textual and visual features complement each other, achieving state- of-the-art results in combination.",
  "Based on a one-tailed Wilcoxon signed-rank test, the performance of JOINT is sta- tistically signi\ufb01cant (p < 0.05). This shows that the textual and visual features complement each other, achieving state- of-the-art results in combination. For arXiv, baseline methods MAJORITY, BENCHMARK, and INCEPTIONFIXED outperform BILSTM over cs.ai, in large part because of the class imbalance in this dataset (90% of papers are rejected). Surprisingly, INCEPTIONFIXED is bet- ter than MAJORITY and BENCHMARK over the arXiv cs.lg subset, which veri\ufb01es the usefulness of visual features, even when only the last layer is \ufb01ne-tuned. Table 3 also shows that INCEPTION and BILSTM achieve similar performance on arXiv, showing that textual and visual representations are equally discriminative: INCEPTION and BILSTM are indis- tinguishable over cs.cl; BILSTM achieves 1.8% higher ac- curacy over cs.lg, while INCEPTION achieves 1.3% higher accuracy over cs.ai.",
  "Once again, the JOINT model achieves the highest accuracy on cs.ai and cs.cl by combining textual and visual representations (at a level of statistical signi\ufb01- cance for cs.ai). This, again, con\ufb01rms that textual and visual features complement each other, and together they achieve state-of-the-art results. On arXiv cs.lg, JOINT achieves a 0.6% higher accuracy than INCEPTION by combining vi- sual features and textual features, but BILSTM achieves the highest accuracy. One characteristic of cs.lg documents is that they tend to contain more equations than the other two arXiv datasets, and preliminary analysis suggests that the BILSTM is picking up on a correlation between the vol- ume/style of mathematical presentation and the quality of the document. Analysis In this section, we \ufb01rst analyze the performance of INCEP- TION and JOINT. We also analyze the performance of dif- ferent models on different quality classes. The high-level representations learned by different models are also visual- ized and discussed. As the Wikipedia test set is larger and more balanced than that of arXiv, our analysis will focus on Wikipedia.",
  "We also analyze the performance of dif- ferent models on different quality classes. The high-level representations learned by different models are also visual- ized and discussed. As the Wikipedia test set is larger and more balanced than that of arXiv, our analysis will focus on Wikipedia. INCEPTION To better understand the performance of INCEPTION, we generated the gradient-based class activation map (Selvaraju et al. 2017), by maximizing the outputs of each class in the penultimate layer, as shown in Figure 3. From Figure 3a and Figure 3b, we can see that INCEPTION identi\ufb01es the two most important regions (one at the top corresponding to the table of contents, and the other at the bottom, cap- turing both document length and references) that contribute to the FA class prediction, and a region in the upper half of the image that contributes to the GA class prediction (cap- turing the length of the article body). From Figure 3c and Figure 3d, we can see that the most important regions in",
  "MAJORITY BENCHMARK DOC2VEC INCEPTIONFIXED BILSTM INCEPTION JOINT Wikipedia 16.7% 46.7\u00b10.34% 23.2\u00b11.41% 43.7\u00b10.51 54.1\u00b10.47% 57.0\u00b10.63% 59.4\u00b10.47%\u2020 arXiv cs.ai 92.2% 92.6% 73.3\u00b19.81% 92.3\u00b10.29 91.5\u00b11.03% 92.8\u00b10.79% 93.4\u00b11.07%\u2020 cs.cl 68.9% 75.7% 66.2\u00b18.38% 75.0\u00b11.95 76.2\u00b11.30% 76.2\u00b12.92% 77.1\u00b13.10% cs.lg 67.9% 70.7% 64.7\u00b19.08% 73.9\u00b11.23 81.1\u00b10.83% 79.3\u00b12.94% 79.9\u00b12.",
  "1\u00b13.10% cs.lg 67.9% 70.7% 64.7\u00b19.08% 73.9\u00b11.23 81.1\u00b10.83% 79.3\u00b12.94% 79.9\u00b12.54% Table 3: Experimental results. The best result for each dataset is indicated in bold, and marked with \u201c\u2020\u201d if it is signi\ufb01cantly higher than the second best result (based on a one-tailed Wilcoxon signed-rank test; p < 0.05). The results of BENCHMARK on the arXiv dataset are from the original paper, where the standard deviation values were not reported. All neural models except for INCEPTIONFIXED have larger standard deviation values on arXiv than Wikipedia, which can be explained by the small size of the arXiv test set. (a) FA (b) GA (c) B (d) C (e) Start (f) Stub Figure 3: Heatmap overlapped onto screenshots of each Wikipedia quality class. Best viewed in color.",
  "(a) FA (b) GA (c) B (d) C (e) Start (f) Stub Figure 3: Heatmap overlapped onto screenshots of each Wikipedia quality class. Best viewed in color. terms of B and C class prediction capture images (down the left and right of the page, in the case of B and C), and doc- ument length/references. From Figure 3e and Figure 3f, we can see that INCEPTION \ufb01nds that images in the top right corner are the strongest predictor of Start class prediction, and (the lack of) images/the link bar down the left side of the document are the most important for Stub class prediction. JOINT Table 4 shows the confusion matrix of JOINT on Wikipedia.",
  "JOINT Table 4 shows the confusion matrix of JOINT on Wikipedia. We can see that more than 50% of documents for each qual- ity class are correctly classi\ufb01ed, except for the C class where Quality FA GA B C Start Stub FA 397 83 20 0 0 0 GA 112 299 65 22 2 0 B 23 53 253 75 44 7 C 5 33 193 124 100 12 Start 1 6 36 85 239 84 Stub 0 0 6 7 63 345 Table 4: Confusion matrix of the JOINT model on Wikipedia. Rows are the actual quality classes and columns are the predicted quality classes. The diagonal (gray cells) indicates correct predictions. Quality Metric BILSTM INCEPTION JOINT FA P 76.6 74.8 73.8 R 72.0 68.2 79.4 F\u03b2=1 74.2 71.3 76.5 GA P 51.3 57.7 63.1 R 59.8 59.0 59.",
  "6 74.8 73.8 R 72.0 68.2 79.4 F\u03b2=1 74.2 71.3 76.5 GA P 51.3 57.7 63.1 R 59.8 59.0 59.8 F\u03b2=1 55.2 58.3 61.4 B P 37.6 41.8 44.2 R 42.4 44.0 55.6 F\u03b2=1 39.9 42.9 49.2 C P 36.3 38.9 39.6 R 27.0 36.0 26.6 F\u03b2=1 31.0 37.4 31.8 Start P 48.2 49.4 53.3 R 44.8 57.2 53.0 F\u03b2=1 46.4 53.0 53.1 Stub P 71.9 83.3 77.0 R 78.9 78.2 81.9 F\u03b2=1 75.2 80.7 79.",
  "2 53.0 F\u03b2=1 46.4 53.0 53.1 Stub P 71.9 83.3 77.0 R 78.9 78.2 81.9 F\u03b2=1 75.2 80.7 79.4 Table 5: Precision (\u201cP\u201d), recall (\u201cR\u201d), and F1 (\u201cF\u03b2=1\u201d) of BILSTM, INCEPTION, and JOINT on Wikipedia. more documents are misclassi\ufb01ed into B. Analysis shows that when misclassi\ufb01ed, documents are usually misclassi\ufb01ed into adjacent quality classes, which can be explained by the Wikipedia grading scheme, where the criteria for adjacent",
  "BILSTM FA GA B C Start Stub INCEPTION JOINT Figure 4: t-SNE scatter plot of Wikipedia article representations (representations from penultimate layer of each model, 200 random samples from each quality class; best viewed in color) quality classes are more similar.8 We also provide a breakdown of precision (\u201cP\u201d), recall (\u201cR\u201d), and F1 score (\u201cF\u03b2=1\u201d) for BILSTM, INCEPTION, and JOINT across the quality classes in Table 5. We can see that JOINT achieves the highest accuracy in 11 out of 18 cases. It is also worth noting that all models achieve higher scores for FA, GA, and Stub articles than B, C and Start ar- ticles. This can be explained in part by the fact that FA and GA articles must pass an of\ufb01cial review based on structured criteria, and in part by the fact that Stub articles are usu- ally very short, which is discriminative for INCEPTION, and JOINT. All models perform worst on the B and C quality classes. It is dif\ufb01cult to differentiate B articles from C ar- ticles even for Wikipedia contributors.",
  "All models perform worst on the B and C quality classes. It is dif\ufb01cult to differentiate B articles from C ar- ticles even for Wikipedia contributors. As evidence of this, when we crawled a new dataset including talk pages with quality class votes from Wikipedia contributors, we found that among articles with three or more quality labels, over 20% percent of B and C articles have inconsistent votes from Wikipedia contributors, whereas for FA and GA articles the number is only 0.7%. We further visualize the learned document representations of BILSTM, INCEPTION, and JOINT in the form of a t- SNE plot (van der Maaten and Hinton 2008) in Figure 4. The degree of separation between Start and Stub achieved by INCEPTION is much greater than for BILSTM, with the separation between Start and Stub achieved by JOINT be- ing the clearest among the three models. INCEPTION and JOINT are better than BILSTM at separating Start and C. JOINT achieves slightly better performance than INCEPTION in separating GA and FA.",
  "INCEPTION and JOINT are better than BILSTM at separating Start and C. JOINT achieves slightly better performance than INCEPTION in separating GA and FA. We can also see that it is dif\ufb01cult for all models to separate B and C, which is consistent with the \ufb01ndings of Tables 4 and 5. Conclusions We proposed to use visual renderings of documents to capture implicit document quality indicators, such as font choices, images, and visual layout, which are not captured in 8Suggesting that ordinal regression should boost accuracy, but preliminary experiments with various methods led to no improve- ment over simple classi\ufb01cation. textual content. We applied neural network models to cap- ture visual features given visual renderings of documents. Experimental results show that we achieve a 2.9% higher accuracy than state-of-the-art approaches based on textual features over Wikipedia, and performance competitive with or surpassing state-of-the-art approaches over arXiv. We fur- ther proposed a joint model, combining textual and visual representations, to predict the quality of a document.",
  "We fur- ther proposed a joint model, combining textual and visual representations, to predict the quality of a document. Exper- imental results show that our joint model outperforms the visual-only model in all cases, and the text-only model on Wikipedia and two subsets of arXiv. These results under- line the feasibility of assessing document quality via visual features, and the complementarity of visual and textual doc- ument representations for quality assessment. References [Agichtein et al. 2008] Agichtein, E.; Castillo, C.; Donato, D.; Gionis, A.; and Mishne, G. 2008. Finding high-quality content in social media. In WSDM, 183\u2013194. [Alikaniotis, Yannakoudakis, and Rei 2016] Alikaniotis, D.; Yannakoudakis, H.; and Rei, M. 2016. Automatic text scor- ing using neural networks. In ACL, 715\u2013725. [Attali and Burstein 2004] Attali, Y., and Burstein, J. 2004.",
  "2016. Automatic text scor- ing using neural networks. In ACL, 715\u2013725. [Attali and Burstein 2004] Attali, Y., and Burstein, J. 2004. Automated essay scoring with e-rater R\u20ddv. 2.0. ETS Re- search Report Series 2004(2). [Belinkov et al. 2015] Belinkov, Y.; Mohtarami, M.; Cyphers, S.; and Glass, J. R. 2015. VectorSLU: A continu- ous word vector approach to answer selection in community question answering systems. In SemEval@NAACL-HLT, 282\u2013287. [Bengio et al. 2003] Bengio, Y.; Ducharme, R.; Vincent, P.; and Jauvin, C. 2003. A neural probabilistic language model. Journal of Machine Learning Research 3:1137\u20131155. [Bird 2006] Bird, S. 2006. NLTK: the natural language toolkit. In ACL, 69\u201372.",
  "2003. A neural probabilistic language model. Journal of Machine Learning Research 3:1137\u20131155. [Bird 2006] Bird, S. 2006. NLTK: the natural language toolkit. In ACL, 69\u201372. [Chollet 2017] Chollet, F. 2017. Xception: Deep learning with depthwise separable convolutions. In CVPR, 1800\u2013 1807.",
  "[Cummins, Zhang, and Briscoe 2016] Cummins, R.; Zhang, M.; and Briscoe, T. 2016. Constrained multi-task learning for automated essay scoring. In ACL, 789\u2013799. [Dalip et al. 2014] Dalip, D. H.; Lima, H.; Gonc\u00b8alves, M. A.; Cristo, M.; and Calado, P. 2014. Quality assessment of collaborative content with minimal information. In JCDL, 201\u2013210. [Dalip et al. 2017] Dalip, D. H.; Gonc\u00b8alves, M. A.; Cristo, M.; and Calado, P. 2017. A general multiview framework for assessing the quality of collaboratively created content on web 2.0. Journal of the Association for Information Science and Technology 68(2):286\u2013308. [Dang and Ignat 2016a] Dang, Q.-V., and Ignat, C.-L. 2016a. Measuring quality of collaboratively edited documents: the case of Wikipedia.",
  "[Dang and Ignat 2016a] Dang, Q.-V., and Ignat, C.-L. 2016a. Measuring quality of collaboratively edited documents: the case of Wikipedia. In The 2nd IEEE International Confer- ence on Collaboration and Internet Computing, 266\u2013275. [Dang and Ignat 2016b] Dang, Q.-V., and Ignat, C.-L. 2016b. Quality assessment of Wikipedia articles without feature engineering. In JCDL, 27\u201330. [Dang and Ignat 2017] Dang, Q. V., and Ignat, C. 2017. An end-to-end learning solution for assessing the quality of Wikipedia articles. In Proceedings of the 13th International Symposium on Open Collaboration, 4:1\u20134:10. [Dong, Zhang, and Yang 2017] Dong, F.; Zhang, Y.; and Yang, J. 2017. Attention-based recurrent convolutional neu- ral network for automatic essay scoring. In CoNLL, 153\u2013 162.",
  "[Dong, Zhang, and Yang 2017] Dong, F.; Zhang, Y.; and Yang, J. 2017. Attention-based recurrent convolutional neu- ral network for automatic essay scoring. In CoNLL, 153\u2013 162. [Guzm\u00b4an, M`arquez, and Nakov 2016] Guzm\u00b4an, F.; M`arquez, L.; and Nakov, P. 2016. Machine transla- tion evaluation meets community question answering. In ACL, 460\u2013466. [Guzm\u00b4an, Nakov, and M`arquez 2016] Guzm\u00b4an, F.; Nakov, P.; and M`arquez, L. 2016. MTE-NN at SemEval-2016 task 3: Can machine translation evaluation help community ques- tion answering? In SemEval@NAACL-HLT, 887\u2013895. [He et al. 2016] He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learning for image recognition. In CVPR, 770\u2013778.",
  "In SemEval@NAACL-HLT, 887\u2013895. [He et al. 2016] He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learning for image recognition. In CVPR, 770\u2013778. [Hochreiter and Schmidhuber 1997] Hochreiter, S., and Schmidhuber, J. 1997. Long short-term memory. Neural Computation 9(8):1735\u20131780. [Hou et al. 2015] Hou, Y.; Tan, C.; Wang, X.; Zhang, Y.; Xu, J.; and Chen, Q. 2015. HITSZ-ICRC: exploiting classi\ufb01ca- tion approach for answer selection in community question answering. In SemEval@NAACL-HLT, 196\u2013202. [Jeon et al. 2006] Jeon, J.; Croft, W. B.; Lee, J. H.; and Park, S. 2006. A framework to predict the quality of answers with non-textual features.",
  "[Jeon et al. 2006] Jeon, J.; Croft, W. B.; Lee, J. H.; and Park, S. 2006. A framework to predict the quality of answers with non-textual features. In SIGIR, 228\u2013235. [Kang et al. 2018] Kang, D.; Ammar, W.; Dalvi, B.; van Zuylen, M.; Kohlmeier, S.; Hovy, E. H.; and Schwartz, R. 2018. A dataset of peer reviews (peerread): Collection, in- sights and NLP applications. In NAACL-HLT, 1647\u20131661. [Kingma and Ba 2014] Kingma, D. P., and Ba, J. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. [Le and Mikolov 2014] Le, Q. V., and Mikolov, T. 2014. Distributed representations of sentences and documents. In ICML, 1188\u20131196.",
  "[Le and Mikolov 2014] Le, Q. V., and Mikolov, T. 2014. Distributed representations of sentences and documents. In ICML, 1188\u20131196. [Lipka and Stein 2010] Lipka, N., and Stein, B. 2010. Iden- tifying featured articles in Wikipedia: writing style matters. In WWW, 1147\u20131148. [Pennington, Socher, and Manning 2014] Pennington, J.; Socher, R.; and Manning, C. D. 2014. GloVe: Global vectors for word representation. In EMNLP, 1532\u20131543. [Persing and Ng 2013] Persing, I., and Ng, V. 2013. Model- ing thesis clarity in student essays. In ACL, 260\u2013269. [Persing and Ng 2014] Persing, I., and Ng, V. 2014. Model- ing prompt adherence in student essays. In ACL, 1534\u20131543.",
  "Model- ing thesis clarity in student essays. In ACL, 260\u2013269. [Persing and Ng 2014] Persing, I., and Ng, V. 2014. Model- ing prompt adherence in student essays. In ACL, 1534\u20131543. [Phandi, Chai, and Ng 2015] Phandi, P.; Chai, K. M. A.; and Ng, H. T. 2015. Flexible domain adaptation for automated essay scoring using correlated linear regression. In EMNLP, 431\u2013439. [Selvaraju et al. 2017] Selvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.; Parikh, D.; and Batra, D. 2017. Grad- cam: Visual explanations from deep networks via gradient- based localization. In ICCV, 618\u2013626. [Shen, Qi, and Baldwin 2017] Shen, A.; Qi, J.; and Bald- win, T. 2017. A hybrid model for quality assessment of Wikipedia articles.",
  "In ICCV, 618\u2013626. [Shen, Qi, and Baldwin 2017] Shen, A.; Qi, J.; and Bald- win, T. 2017. A hybrid model for quality assessment of Wikipedia articles. In Proceedings of the Australasian Lan- guage Technology Association Workshop, 43\u201352. [Simonyan and Zisserman 2014] Simonyan, K., and Zisser- man, A. 2014. Very deep convolutional networks for large- scale image recognition. arXiv preprint arXiv:1409.1556 abs/1409.1556. [Stein and Hess 2007] Stein, K., and Hess, C. 2007. Does it matter who contributes: a study on featured articles in the german wikipedia. In HYPERTEXT, 171\u2013174. [Suggu et al. 2016] Suggu, S. P.; Goutham, K. N.; Chin- nakotla, M. K.; and Shrivastava, M. 2016.",
  "In HYPERTEXT, 171\u2013174. [Suggu et al. 2016] Suggu, S. P.; Goutham, K. N.; Chin- nakotla, M. K.; and Shrivastava, M. 2016. Hand in glove: Deep feature fusion network architectures for answer quality prediction in community question answering. In COLING, 1429\u20131440. [Suryanto et al. 2009] Suryanto, M. A.; Lim, E.; Sun, A.; and Chiang, R. H. L. 2009. Quality-aware collaborative question answering: methods and evaluation. In WSDM, 142\u2013151. [Szegedy et al. 2016] Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2016. Rethinking the Inception architecture for computer vision. In CVPR, 2818\u20132826. [Taghipour and Ng 2016] Taghipour, K., and Ng, H. T. 2016.",
  "2016. Rethinking the Inception architecture for computer vision. In CVPR, 2818\u20132826. [Taghipour and Ng 2016] Taghipour, K., and Ng, H. T. 2016. A neural approach to automated essay scoring. In EMNLP, 1882\u20131891. [Tay et al. 2018] Tay, Y.; Phan, M. C.; Tuan, L. A.; and Hui, S. C. 2018. SkipFlow: Incorporating neural coherence fea- tures for end-to-end automatic text scoring. In AAAI. [van der Maaten and Hinton 2008] van der Maaten, L., and Hinton, G. 2008. Visualizing data using t-SNE. JMLR 9:2579\u20132605. [Wang and Iwaihara 2011] Wang, S., and Iwaihara, M. 2011. Quality evaluation of wikipedia articles through edit history and editor groups. In APWeb, 188\u2013199.",
  "[Warncke-Wang et al. 2015] Warncke-Wang, M.; Ayukaev, V. R.; Hecht, B.; and Terveen, L. 2015. The success and failure of quality improvement projects in peer production communities. In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Com- puting, CSCW 2015, 743\u2013756. [Warncke-Wang, Cosley, and Riedl 2013] Warncke-Wang, M.; Cosley, D.; and Riedl, J. 2013. Tell me more: an actionable quality model for Wikipedia. In Proceedings of the 9th International Symposium on Open Collaboration, 8:1\u20138:10. [Yang et al. 2018] Yang, P.; Sun, X.; Li, W.; and Ma, S. 2018. Automatic academic paper rating based on modularized hi- erarchical convolutional neural network. In ACL, 496\u2013502. [Zhou et al. 2015] Zhou, X.; Hu, B.; Chen, Q.; Tang, B.; and Wang, X.",
  "2018. Automatic academic paper rating based on modularized hi- erarchical convolutional neural network. In ACL, 496\u2013502. [Zhou et al. 2015] Zhou, X.; Hu, B.; Chen, Q.; Tang, B.; and Wang, X. 2015. Answer sequence learning with neural net- works for answer selection in community question answer- ing. arXiv preprint arXiv:1506.06490."
]