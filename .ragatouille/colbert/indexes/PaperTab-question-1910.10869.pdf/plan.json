{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "COMBINING ACOUSTICS, CONTENT AND INTERACTION FEATURES TO FIND HOT SPOTS IN MEETINGS Dave Makhervaks1\u2217 William Hinthorn2 Dimitrios Dimitriadis2 Andreas Stolcke3\u2217 1Dept. of Computer Science, Harvey Mudd College, Claremont, CA, USA 2Speech and Dialog Research Group, Microsoft, Redmond, WA, USA 3Amazon Alexa Speech, Sunnyvale, CA, USA dmakhervaks@g.hmc.edu {wihintho,didimit}@microsoft.com stolcke@amazon.com ABSTRACT Involvement hot spots have been proposed as a useful concept for meeting analysis and studied off and on for over 15 years. These are regions of meetings that are marked by high participant involve- ment, as judged by human annotators. However, prior work was ei- ther not conducted in a formal machine learning setting, or focused on only a subset of possible meeting features or downstream ap- plications (such as summarization). In this paper we investigate to what extent various acoustic, linguistic and pragmatic aspects of the meetings, both in isolation and jointly, can help detect hot spots.",
            "In this paper we investigate to what extent various acoustic, linguistic and pragmatic aspects of the meetings, both in isolation and jointly, can help detect hot spots. In this context, the openSMILE toolkit [1] is to used to extract features based on acoustic-prosodic cues, BERT word embeddings [2] are used for encoding the lexical content, and a variety of statistics based on speech activity are used to describe the verbal interaction among participants. In experiments on the annotated ICSI meeting corpus, we \ufb01nd that the lexical model is the most informative, with incre- mental contributions from interaction and acoustic-prosodic model components. Index Terms\u2014 Hot spots, involvement, meeting understanding, feature fusion. 1. INTRODUCTION AND PRIOR WORK A de\ufb01nition of meeting \u201chot spots\u201d was \ufb01rst introduced in [3], where it was investigated whether human annotators could reliably iden- tify regions in which participants are \u201chighly involved in the discus- sion.\u201d The motivation was that meetings generally have low infor- mation density and are tedious to review verbatim after the fact.",
            "An automatic system that could detect regions of high interest (as in- dicated by the involvement of the participants during the meeting) would thus be useful. Relatedly, automatic meeting summarization could also bene\ufb01t from such information by giving extra weight to hot spot regions in selecting or abstracting material for inclusion in the summary. Later work on the relationship between involvement and summarization [4] de\ufb01ned a different approach: hot spots are those regions chosen for inclusion in a summary by human annota- tors (\u201csummarization hot spots\u201d). In the present work we stick with the original \u201cinvolvement hot spot\u201d notion, and refer to such regions simply as \u201chot spots\u201d, re- gardless of their possible role in summarization. We note that high involvement may be triggered both by a meeting\u2019s content (\u201cwhat is being talked about\u201d or \u201cwhat may be included in a textual sum- mary\u201d), as well as behavioral and social factors, such as a desire to \u2217Research done while the authors were with Microsoft participate, to stake out a position, or to oppose another participant. A related notion in dialog system research is \u201clevel of interest\u201d [5].",
            "A related notion in dialog system research is \u201clevel of interest\u201d [5]. The initial research on hot spots focused on the reliability of human annotators and correlations with certain low-level acoustic features, such as pitch [3]. Also investigated were the correlation be- tween hot spots and dialog acts [6] and hot spots and speaker overlap [7], without however conducting experiments in automatic hot spot prediction using machine learning techniques. Laskowski [8] rede- \ufb01ned the hot spot annotations in terms of time-based windows over meetings, and investigated various classi\ufb01er models to detect \u201chot- ness\u201d (i.e., elevated involvement). However, that work focused on only two types of speech features: presence of laughter and the tem- poral patterns of speech activity across the various participants, both of which were found to be predictive of involvement. For the related problem of level-of-interest prediction in dialog systems [9], it was found that content-based classi\ufb01cation can also be effective, using both a discriminative TF-IDF model and lexical affect scores, as well as prosodic features.",
            "For the related problem of level-of-interest prediction in dialog systems [9], it was found that content-based classi\ufb01cation can also be effective, using both a discriminative TF-IDF model and lexical affect scores, as well as prosodic features. In line with the earlier hot spot research on interaction patterns and speaker overlap, turn- taking features were shown to be helpful for spotting summarization hot spots, and even more so than the human involvement annota- tions [4]. The latter result con\ufb01rms our intuition that summarization- worthiness and involvement are different notions of \u201chotness.\u201d In this paper, following Laskowski, we focus on the auto- matic prediction of the speakers\u2019 involvement in sliding-time win- dows\/segments. We evaluate machine learning models based on a range of features that can be extracted automatically from audio recordings, either directly via signal processing or via the use of au- tomatic transcriptions (ASR outputs).",
            "We evaluate machine learning models based on a range of features that can be extracted automatically from audio recordings, either directly via signal processing or via the use of au- tomatic transcriptions (ASR outputs). In particular, we investigate the relative contributions of three classes of information: \u2022 low-level acoustic-prosodic features, such as those commonly used in other paralinguistic tasks, like sentiment analysis (ex- tracted using openSMILE [1]); \u2022 spoken word content, as encoded with a state-of-the-art lexi- cal embedding approach such as BERT [2]; \u2022 speaker interaction, based on speech activity over time and across different speakers. We attach lower importance to laughter, even though it was found to be highly predictive of involvement in the ICSI corpus, partly be- cause we believe it would not transfer well to more general types of (e.g., business) meetings, and partly because laughter detec- tion is still a hard problem in itself [10]. Generation of speaker- attributed meeting transcriptions, on the other hand, has seen re- markable progress [11] and could support the features we focus on here.",
            "Generation of speaker- attributed meeting transcriptions, on the other hand, has seen re- markable progress [11] and could support the features we focus on here. arXiv:1910.10869v2  [cs.CL]  14 Feb 2020",
            "Fig. 1. Visualization of the sliding window de\ufb01ning data points. The area bounded by the red box indicates labeled involvement, causing 4 windows to be marked as \u2018hot\u2019. 2. DATA The ICSI Meeting Corpus [12] is a collection of meeting record- ings that has been thoroughly annotated, including annotations for involvement hot spots [13], linguistic utterance units, and word time boundaries based on forced alignment. The dataset is comprised of 75 meetings with a real-time audio duration of about 70 hours. Meetings have six speakers on average. Most of the participants are well-acquainted and friendly with each other. Hot spots were origi- nally annotated with 8 levels and degrees, ranging from \u2018not hot\u2019 to \u2018lukewarm\u2019 to \u2018hot +\u2019. Every utterance was labeled with one of these discrete labels by a single annotator. Hightened involvement is rare, being marked on only 1% of utterances. Due to the severe imbalance in the label distribution, Laskowski [14] proposed extending the involvement, or hotness, labels to slid- ing time windows.",
            "Hightened involvement is rare, being marked on only 1% of utterances. Due to the severe imbalance in the label distribution, Laskowski [14] proposed extending the involvement, or hotness, labels to slid- ing time windows. In our implementation (details below), this re- sulted in 21.7% of samples (windows) being labeled as \u201cinvolved\u201d. We split the corpus into three subsets: training, development, and evaluation, keeping meetings intact. Table 1 gives statistics of these partitions. Table 1. Partitions of the ICSI dataset Purpose Training Development Evaluation # Meetings 51 9 15 # Words 478,593 120,533 147,478 # Utterances 69,755 18,360 21,283 # Windows 10,197 2,452 3,174 Share of hot windows 21.5% 22.3% 21.9% We were concerned with the relatively small number of meet- ings in the test sets, and repeated several of our experiments with a (jackkni\ufb01ng) cross-validation setup over the training set.",
            "The re- sults obtained were very similar to those with the \ufb01xed train\/test split results that we report here. 2.1. Time Windowing As stated above, the corpus was originally labeled for hot spots at the utterance level, where involvement was marked by either a \u2018b\u2019 or a \u2018b+\u2019 label. Training and test samples for our experiments correspond to 60 s-long sliding windows, with a 15 s step size. If a certain win- dow, e.g., a segment spanning the times 15 s ...75 s, overlaps with any involved speech utterance, then we label that whole window as \u2018hot\u2019. Fig. 1 gives a visual representation. 2.2. Metric In spite of the windowing approach, the class distribution is still skewed, and an accuracy metric would re\ufb02ect the particular class distribution in our data set. Therefore, we adopt the unweighted av- erage recall (UAR) metric commonly used in emotion classi\ufb01cation research. UAR is a reweighted accuracy where the samples of both classes are weighted equally in aggregate. UAR thus simulates a uniform class distribution.",
            "Therefore, we adopt the unweighted av- erage recall (UAR) metric commonly used in emotion classi\ufb01cation research. UAR is a reweighted accuracy where the samples of both classes are weighted equally in aggregate. UAR thus simulates a uniform class distribution. To match the objective, our classi\ufb01ers are trained on appropriately weighted training data. Note that chance performance for UAR is by de\ufb01nition 50%, making results more comparable across different data sets and tasks. 3. FEATURE DESCRIPTION 3.1. Acoustic-Prosodic Features Prosody encompasses pitch, energy, and durational features of speech. Prosody is thought to convey emphasis, sentiment, and emo- tion, all of which are presumably correlated with expressions of in- volvement. We used the openSMILE toolkit [1] to compute 988 features as de\ufb01ned by the emobase988 con\ufb01guration \ufb01le, operating on the close-talking meeting recordings. This feature set consists of low-level descriptors such as intensity, loudness, Mel-frequency cepstral coef\ufb01cients, and pitch.",
            "This feature set consists of low-level descriptors such as intensity, loudness, Mel-frequency cepstral coef\ufb01cients, and pitch. For each low-level descriptor, func- tionals such as max\/min value, mean, standard deviation, kurtosis, and skewness are computed. Finally, global mean and variance nor- malization are applied to each feature, using training set statistics. The feature vector thus captures acoustic-prosodic features aggre- gated over what are typically utterances. We tried extracting openS- MILE features directly from 60 s windows, but found better results by extracting subwindows of 5 s, followed by pooling over the longer 60 s duration. We attribute this to the fact that emobase features are designed to operate on individual utterances, which have durations closer to 5 s than 60 s. 3.2. Word-Based Features 3.2.1. Bag of words with TF-IDF Initially, we investigated a simple bag-of-words model including all unigrams, bigrams, and trigrams found in the training set.",
            "3.2. Word-Based Features 3.2.1. Bag of words with TF-IDF Initially, we investigated a simple bag-of-words model including all unigrams, bigrams, and trigrams found in the training set. Occur- rences of the top 10,000 n-grams were encoded to form a 10,000- dimensional vector, with values weighted according to TD-IDF. TF- IDF weights n-grams according to both their frequency (TF) and their salience (inverse document frequency, IDF) in the data, where each utterance was treated as a separate document. The resulting feature vectors are very sparse. 3.2.2. Embeddings The ICSI dataset is too small to train a neural embedding model from scratch. Therefore, it is convenient to use the pre-trained BERT embedding architecture [2] to create an utterance-level embedding vector for each region of interest. Having been trained on a large text corpus, the resulting embeddings encode semantic similarities among utterances, and would enable generalization from word pat- terns seen in the ICSI training data to those that have not been ob- served on that limited corpus.",
            "Having been trained on a large text corpus, the resulting embeddings encode semantic similarities among utterances, and would enable generalization from word pat- terns seen in the ICSI training data to those that have not been ob- served on that limited corpus. We had previously also created an adapted version of the BERT model, tuned to perform utterance-level sentiment classi\ufb01cation, on a separate dataset [15]. As proposed in [2], we \ufb01ne-tuned all layers of the pre-trained BERT model by adding a single fully-connected layer and classifying using only the embedding corresponding to the classi\ufb01cation ([CLS]) token prepended to each utterance. The",
            "difference in UAR between the hot spot classi\ufb01ers using the pre- trained embeddings and those using the sentiment-adapted embed- dings is small. Since the classi\ufb01er using embeddings extracted by the sentiment-adapted model yielded slightly better performance, we report all results using these as input. To obtain a single embedding for each 60 s window, we experi- mented with various approaches of pooling the token and utterance- level embeddings. For our \ufb01rst approach, we ignored the ground- truth utterance segmentation and speaker information. We merged all words spoken within a particular window into a single contigu- ous span. Following [2], we added the appropriate classi\ufb01cation and separation tokens to the text and selected the embedding correspond- ing to the [CLS] token as the window-level embedding. Our second approach used the ground-truth segmentation of the dialogue. Each speaker turn was independently modeled, and utterance-level em- beddings were extracted using the representation corresponding to the [CLS] token. Utterances that cross window boundaries are trun- cated using the word timestamps, so only words spoken within the given time window are considered.",
            "Each speaker turn was independently modeled, and utterance-level em- beddings were extracted using the representation corresponding to the [CLS] token. Utterances that cross window boundaries are trun- cated using the word timestamps, so only words spoken within the given time window are considered. For all reported experiments, we use L2-norm pooling to form the window-level embeddings for the \ufb01nal classi\ufb01er, as this performed better than either mean or max pooling. 3.3. Speaker Activity Features These features were a compilation of three different feature types: Speaker overlap percentages: Based on the available word-level times, we computed a 6-dimensional feature vector, where the ith component indicates the fraction of time that i or more speakers are talking within a given window. This can be expressed by ti 60 with ti indicating the time in seconds that i or more people were speaking at the same time. Unique speaker count: Counts the unique speakers within a win- dow, as a useful metric to track the diversity of participation within a certain window. Turn switch count: Counts the number of times a speaker begins talking within a window. This is a metric similar to the number of utterances.",
            "Unique speaker count: Counts the unique speakers within a win- dow, as a useful metric to track the diversity of participation within a certain window. Turn switch count: Counts the number of times a speaker begins talking within a window. This is a metric similar to the number of utterances. However, unlike utterance count, turn switches can be computed entirely from speech activity, without requiring a linguis- tic segmentation. 3.4. Laughter Count Laskowski found that laughter is highly predictive of involvement in the ICSI data [8]. Laughter is annotated on an utterance level and falls into two categories: laughter solely on its own (no words) or laughter contained within an utterance (i.e., while speaking). The feature is a simple tally of the number of times people laughed within a window. We include it in some of our experiments for comparison purposes, though we do not trust it as general feature. (The partici- pants in the ICSI meetings are far too familiar and at ease with each other to be representative with regards to laughter.) 4. MODELING 4.1.",
            "We include it in some of our experiments for comparison purposes, though we do not trust it as general feature. (The partici- pants in the ICSI meetings are far too familiar and at ease with each other to be representative with regards to laughter.) 4. MODELING 4.1. Non-Neural Models In preliminary experiments, we compared several non-neural classi- \ufb01ers, including logistic regression (LR), random forests, linear sup- port vector machines, and multinomial naive Bayes. Logistic regres- sion gave the best results all around, and we used it exclusively for the results shown here, unless neural networks are used instead. Fig. 2. Overview of fusion model 4.2. Feed-Forward Neural Networks 4.2.1. Pooling Techniques For BERT and openSMILE vector classi\ufb01cation, we designed two different feed-forward neural network architectures. The sentiment- adapted embeddings described in Section 3 produce one 1024- dimensional vector per utterance.",
            "Feed-Forward Neural Networks 4.2.1. Pooling Techniques For BERT and openSMILE vector classi\ufb01cation, we designed two different feed-forward neural network architectures. The sentiment- adapted embeddings described in Section 3 produce one 1024- dimensional vector per utterance. Since all classi\ufb01cation operates on time windows, we had to pool over all utterances falling with- ing a given window, taking care to truncate words falling outside the window. We tested four pooling methods: L2-norm, mean, max, and min, with L2-norm giving the best results. As for the prosodic model, each vector extracted from openSMILE represents a 5 s interval. Since there was both a channel\/speaker-axis and a time-axis, we needed to pool over both dimensions in order to have a single vector representing the prosodic features of a 60 s window. The second to last layer is the pooling layer, max-pooling across all the channels, and then mean-pooling over time. The output of the pooling layer is directly fed into the classi\ufb01er. 4.2.2.",
            "The second to last layer is the pooling layer, max-pooling across all the channels, and then mean-pooling over time. The output of the pooling layer is directly fed into the classi\ufb01er. 4.2.2. Hyperparameters The hyperparameters of the neural networks (hidden layer number and sizes) were also tuned in preliminary experiments. Details are given in Section 5. 4.3. Model Fusion Fig. 2 depicts the way features from multiple categories are com- bined. Speech activity and word features are fed directly into a \ufb01nal LR step. Acoustic-prosodic features are \ufb01rst combined in a feed- forward neural classi\ufb01er, whose output log posteriors are in turn fed into the LR step for fusion. (When using only prosodic features, the ANN outputs are used directly.) 5. EXPERIMENTS We group experiments by the type of feaures they are based on: acoustic-prosodic, word-based, and speech activity, evaluating each group \ufb01rst by itself, and then in combination with others. 5.1.",
            "5. EXPERIMENTS We group experiments by the type of feaures they are based on: acoustic-prosodic, word-based, and speech activity, evaluating each group \ufb01rst by itself, and then in combination with others. 5.1. Speech Feature Results As discussed in Section 3, a multitude of input features were in- vestigated, with some being more discriminative. The most useful speech activity features were speaker overlap percentage, number of unique speakers, and number of turn switches, giving evaluation set UARs of 63.5%, 63.9%, and 66.6%, respectively. When combined",
            "Table 2. Hot spot classi\ufb01cation results with individual feature sub- sets, all features, and with individual feature sets left out. Feature Set UAR w\/ Features UAR w\/o Features Prosody 62.0% 71.7% Speech-act 68.0% 72.2% Words 70.5% 68.4% All 72.6% N\/A the UAR improved to 68.0%, showing that these features are partly complementary. 5.2. Word-Based Results The TF-IDF model alone gave a UAR of 59.8%. A drastic increase in performance to 70.5% was found when using the BERT embed- dings instead. Therefore we adopted embeddings for all further ex- periments based on word information. Three different types of embeddings were investigated, i.e. sentiment-adapted embeddings at an utterance-level, unadapted em- beddings at the utterance-level, and unadapted embeddings over time windows. The adapted embeddings (on utterances) performed best, indi- cating that adaptation to sentiment task is useful for involvement classi\ufb01cation.",
            "The adapted embeddings (on utterances) performed best, indi- cating that adaptation to sentiment task is useful for involvement classi\ufb01cation. It is important to note, however, that the utterance- level embeddings are larger than the window-level embeddings. This is due to there being more utterances than windows in the meeting corpus. The best neural architecture we found for these embeddings is a 5-layer neural network with sizes 1024-64-32-12-2. Other hyperpa- rameters for this model are dropout rate = 0.4, learning rate = 10\u22127 and tanh activation function. The UAR on the evaluation set with just BERT embeddings as input was 65.2%. Interestingly, the neural model was outperformed by a LR di- rectly on the component values of the embedding vectors, with a UAR of 70.5%. Perhaps the neural network requires further \ufb01ne- tuning, or the neural model is too prone to over\ufb01tting, given the small training corpus. In any case, we use LR on embeddings for all subsequent results. 5.3.",
            "Perhaps the neural network requires further \ufb01ne- tuning, or the neural model is too prone to over\ufb01tting, given the small training corpus. In any case, we use LR on embeddings for all subsequent results. 5.3. Acoustic-Prosodic Feature Results Our prosodic model is a 5-layer ANN, as described in Section 4.2. The architecture is: 988-512-128-16-Pool-2. The hyperparameters are: dropout rate = 0.4, learning rate = 10\u22127, tanh activation. The UAR on the evaluation set with just openSMILE features was 62.0%. 5.4. Fusion Results and Discussion Table 2 gives the UAR for each feature subset individually, for all features combined, and for a combination in which one feature sub- set in turn is left out. The one-feature-set-at-a-time results suggest that prosody, speech activity and words are of increasing importance in that order. The leave-one-out analysis agrees that the words are the most important (largest drop in accuracy when removed), but on that criterion the prosodic features are more important than speech- activity.",
            "The leave-one-out analysis agrees that the words are the most important (largest drop in accuracy when removed), but on that criterion the prosodic features are more important than speech- activity. The combination of all features was 0.4% absolute bet- ter than any other subset, showing that all feature subsets are non- redundant. Fig. 3 shows the same results in histogram form, but also in- cludes those with laughter information. Laughter count by itself is Fig. 3. Graph of different combinations of features. Green rectan- gles indicate models using laughter. Prosody = openSMILE features with NN, Words = embeddings, Spch-act = speech activity, Laugh = laughter count. Combination was by logistic regression. the strongest cue to involvement, as Laskowski [8] had found. How- ever, even given the strong individual laughter feature, the other fea- tures add information, pushing the UAR from from 75.1% to 77.5%. 6. CONCLUSION We studied detection of areas of high involvement, or \u201chot spots\u201d, within meetings using the ICSI corpus.",
            "6. CONCLUSION We studied detection of areas of high involvement, or \u201chot spots\u201d, within meetings using the ICSI corpus. The features that yielded the best results are in line with our intuitions. Word embeddings, speech activity features such as number of turn changes, and prosodic fea- tures are all plausible and effective indicators of high involvement. Furthermore, the feature sets are partly complementary and yield best results when combined using a simple logistic regression model. The combined model achieves 72.6% UAR, or 77.5% with laughter feature. For future work, we would want to see a validation on an inde- pendent meeting collection, such as business meetings. Some fea- tures, in particular laughter, may not be as useful in this case. More data could also enable the training of joint models that perform an early fusion of the different feature types. Also, the present study still relied on human transcripts, and it would be important to know how much UAR suffers with a realistic amount of speech recogni- tion error. Transcription errors are expected to boost the importance of the features types that do not rely on words. 7.",
            "Also, the present study still relied on human transcripts, and it would be important to know how much UAR suffers with a realistic amount of speech recogni- tion error. Transcription errors are expected to boost the importance of the features types that do not rely on words. 7. ACKNOWLEDGMENTS We thank Britta Wrede, Elizabeth Shriberg and Kornel Laskowski for sharing details about the data and annotations. 8. REFERENCES [1] Florian Eyben, Felix Weninger, Florian Gross, and Bj\u00a8orn Schuller, \u201cRecent developments in openSMILE, the Munich open-source multimedia feature extractor,\u201d in Proceedings ACM Multimedia, Barcelona, Oct. 2013, pp. 835\u2013838. [2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, \u201cBERT: Pre-training of deep bidirectional trans- formers for language understanding,\u201d arXiv:1810.04805, Oct. 2018.",
            "[3] Britta Wrede and Elizabeth Shriberg, \u201cSpotting \u201chotspots\u201d in meetings: Human judgments and prosodic cues,\u201d in Proceed- ings Eurospeech, Geneva, Sept. 2003, pp. 2805\u20132808. [4] Catherine Lai, Jean Carletta, and Steve Renals, \u201cDetecting summarization hot spots in meetings using group level involve- ment and turn-taking features,\u201d in Proc. Interspeech, Lyon, Aug. 2013, pp. 2723\u20132727. [5] Bj\u00a8orn Schuller, Niels K\u00a8ohler, Ronald M\u00a8uller, and Gerhard Rigoll, \u201cRecognition of interest in human conversational speech,\u201d in Proc. ICSLP, Pittsburgh, PA, Sept. 2006, pp. 793\u2013 796. [6] Britta Wrede and Elizabeth Shriberg, \u201cThe relationship be- tween dialogue acts and hot spots in meetings,\u201d in Proceed- ings IEEE Workshop on Automatic Speech Recognition and Understanding, St. Thomas, U. S. Virgin Islands, Dec. 2003, pp. 180\u2013185.",
            "Thomas, U. S. Virgin Islands, Dec. 2003, pp. 180\u2013185. [7] \u00a8Ozg\u00a8ur Cetin and Elizabeth Shriberg, \u201cOverlap in meetings: ASR effects and analysis by dialog factors, speakers, and col- lection site,\u201d in Machine Learning for Multimodal Interac- tion: Third International Workshop, MLMI 2006, Steve Re- nals, Samy Bengio, and Jonathan Fiscus, Eds. 2006, vol. 4299 of Lecture Notes in Computer Science, pp. 212\u2013224, Springer. [8] Kornel Laskowski, \u201cModeling vocal interaction for text- independent detection of involvement hotspots in multi-party meetings,\u201d in Proceedings IEEE Spoken Language Technology Workshop, Goa, India, Dec. 2008, pp. 81\u201384. [9] William Yang Wang and Julia Hirschberg, \u201cDetecting levels of interest from spoken dialog with multistream prediction feed- back and similarity based hierarchical fusion learning,\u201d in Pro- ceedings SIGDIAL Conference, Portland, Ore., June 2011, pp. 152\u2013161.",
            "[9] William Yang Wang and Julia Hirschberg, \u201cDetecting levels of interest from spoken dialog with multistream prediction feed- back and similarity based hierarchical fusion learning,\u201d in Pro- ceedings SIGDIAL Conference, Portland, Ore., June 2011, pp. 152\u2013161. [10] G. Gosztolya, T. Grsz, and L. Tth, \u201cSocial signal detection by probabilistic sampling DNN training,\u201d IEEE Transactions on Affective Computing, pp. 1\u20131, 2018. [11] Takuya Yoshioka, Dimitrios Dimitriadis, Andreas Stolcke, William Hinthorn, Zhuo Chen, Michael Zeng, and Xuedong Huang, \u201cMeeting transcription using asynchronous distant mi- crophones,\u201d in Proc. Interspeech, Graz, Sept. 2019, pp. 2968\u2013 2972.",
            "Interspeech, Graz, Sept. 2019, pp. 2968\u2013 2972. [12] A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Mor- gan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Woot- ers, \u201cThe ICSI meeting corpus,\u201d in Proc. Int. Conf. Acoust., Speech, Signal Process., 2003, pp. I\u2013364\u2013I\u2013367. [13] Britta Wrede, Sonali Bhagat, Raj Dhillon, and Elizabeth Shriberg, \u201cMeeting recorder project: Hot spot labeling guide,\u201d Tech. Rep. TR-05-004, International Computer Science Insti- tute, Berkeley, CA, 2005. [14] Q. Jin, K. Laskowski, T. Schultz, and A. Waibel, \u201cSpeaker seg- mentation and clustering in meetings,\u201d in Rich Transcription Evaluation Workshop, Montreal, CA, 2004.",
            "[14] Q. Jin, K. Laskowski, T. Schultz, and A. Waibel, \u201cSpeaker seg- mentation and clustering in meetings,\u201d in Rich Transcription Evaluation Workshop, Montreal, CA, 2004. [15] Bryan Li, Dimitrios Dimitriadis, and Andreas Stolcke, \u201cAcous- tic and lexical sentiment analysis for customer service calls,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brighton, May 2019, pp. 5876\u2013 5880."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1910.10869.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 5941.0,
    "avg_doclen_est": 185.65625
}
