{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Published as a conference paper at ICLR 2020 RTFM: GENERALISING TO NOVEL ENVIRONMENT DYNAMICS VIA READING Victor Zhong\u2217. Paul G. Allen School of Computer Science & Engineering University of Washington vzhong@cs.washington.edu Tim Rockt\u00a8aschel Facebook AI Research & University College London rockt@fb.com Edward Grefenstette Facebook AI Research & University College London egrefen@fb.com ABSTRACT Obtaining policies that can generalise to new environments in reinforcement learn- ing is challenging. In this work, we demonstrate that language understanding via a reading policy learner is a promising vehicle for generalisation to new envi- ronments. We propose a grounded policy learning problem, Read to Fight Mon- sters (RTFM), in which the agent must jointly reason over a language goal, rele- vant dynamics described in a document, and environment observations. We pro- cedurally generate environment dynamics and corresponding language descrip- tions of the dynamics, such that agents must read to understand new environ- ment dynamics instead of memorising any particular information.",
      "We pro- cedurally generate environment dynamics and corresponding language descrip- tions of the dynamics, such that agents must read to understand new environ- ment dynamics instead of memorising any particular information. In addition, we propose txt2\u03c0, a model that captures three-way interactions between the goal, document, and observations. On RTFM, txt2\u03c0 generalises to new en- vironments with dynamics not seen during training via reading. Furthermore, our model outperforms baselines such as FiLM and language-conditioned CNNs on RTFM. Through curriculum learning, txt2\u03c0 produces policies that excel on complex RTFM tasks requiring several reasoning and coreference steps. 1 INTRODUCTION Reinforcement learning (RL) has been successful in a variety of areas such as continuous con- trol (Lillicrap et al., 2015), dialogue systems (Li et al., 2016), and game-playing (Mnih et al., 2013). However, RL adoption in real-world problems is limited due to poor sample ef\ufb01ciency and failure to generalise to environments even slightly different from those seen during training.",
      "However, RL adoption in real-world problems is limited due to poor sample ef\ufb01ciency and failure to generalise to environments even slightly different from those seen during training. We explore language-conditioned policy learning, where agents use machine reading to discover strategies re- quired to solve a task, thereby leveraging language as a means to generalise to new environments. Prior work on language grounding and language-based RL (see Luketina et al. (2019) for a recent survey) are limited to scenarios in which language speci\ufb01es the goal for some \ufb01xed environment dynamics (Branavan et al., 2011; Hermann et al., 2017; Bahdanau et al., 2019; Fried et al., 2018; Co-Reyes et al., 2019), or the dynamics of the environment vary and are presented in language for some \ufb01xed goal (Branavan et al., 2012). In practice, changes to goals and to environment dynamics tend to occur simultaneously\u2014given some goal, we need to \ufb01nd and interpret relevant information to understand how to achieve the goal.",
      "In practice, changes to goals and to environment dynamics tend to occur simultaneously\u2014given some goal, we need to \ufb01nd and interpret relevant information to understand how to achieve the goal. That is, the agent should account for variations in both by selectively reading, thereby generalising to environments with dynamics not seen during training. Our contributions are two-fold. First, we propose a grounded policy learning problem that we call Read to Fight Monsters (RTFM). In RTFM, the agent must jointly reason over a language goal, a document that speci\ufb01es environment dynamics, and environment observations. In particular, it must identify relevant information in the document to shape its policy and accomplish the goal. To necessitate reading comprehension, we expose the agent to ever changing environment dynam- ics and corresponding language descriptions such that it cannot avoid reading by memorising any particular environment dynamics. We procedurally generate environment dynamics and natural lan- \u2217Work done during an internship at Facebook AI Research. We open-sourced this project at https: //github.com/facebookresearch/RTFM 1 arXiv:1910.08210v6  [cs.CL]  1 Feb 2021",
      "Published as a conference paper at ICLR 2020 guage templated descriptions of dynamics and goals to produced a combinatorially large number of environment dynamics to train and evaluate RTFM. Second, we propose txt2\u03c0 to model the joint reasoning problem in RTFM. We show that txt2\u03c0 generalises to goals and environment dynamics not seen during training, and outper- forms previous language-conditioned models such as language-conditioned CNNs and FiLM (Perez et al., 2018; Bahdanau et al., 2019) both in terms of sample ef\ufb01ciency and \ufb01nal win-rate on RTFM. Through curriculum learning where we adapt txt2\u03c0 trained on simpler tasks to more complex tasks, we obtain agents that generalise to tasks with natural language documents that require \ufb01ve hops of reasoning between the goal, document, and environment observations. Our qualitative anal- yses show that txt2\u03c0 attends to parts of the document relevant to the goal and environment ob- servations, and that the resulting agents exhibit complex behaviour such as retrieving correct items, engaging correct enemies after acquiring correct items, and avoiding incorrect enemies.",
      "Our qualitative anal- yses show that txt2\u03c0 attends to parts of the document relevant to the goal and environment ob- servations, and that the resulting agents exhibit complex behaviour such as retrieving correct items, engaging correct enemies after acquiring correct items, and avoiding incorrect enemies. Finally, we highlight the complexity of RTFM in scaling to longer documents, richer dynamics, and natural language variations. We show that signi\ufb01cant improvement in language-grounded policy learning is needed to solve these problems in the future. 2 RELATED WORK Language-conditioned policy learning. A growing body of research is learning policies that fol- low imperative instructions. The granularity of instructions vary from high-level instructions for application control (Branavan, 2012) and games (Hermann et al., 2017; Bahdanau et al., 2019) to step-by-step navigation (Fried et al., 2018). In contrast to learning policies for imperative instruc- tions, Branavan et al. (2011; 2012); Narasimhan et al.",
      "In contrast to learning policies for imperative instruc- tions, Branavan et al. (2011; 2012); Narasimhan et al. (2018) infer a policy for a \ufb01xed goal using features extracted from high level strategy descriptions and general information about domain dy- namics. Unlike prior work, we study the combination of imperative instructions and descriptions of dynamics. Furthermore, we require that the agent learn to \ufb01lter out irrelevant information to focus on dynamics relevant to accomplishing the goal. Language grounding. Language grounding refers to interpreting language in a non-linguistic con- text. Examples of such context include images (Barnard & Forsyth, 2001), games (Chen & Mooney, 2008; Wang et al., 2016), robot control (Kollar et al., 2010; Tellex et al., 2011), and navigation (An- derson et al., 2018). We study language grounding in interactive games similar to Branavan (2012); Hermann et al. (2017) or Co-Reyes et al.",
      "We study language grounding in interactive games similar to Branavan (2012); Hermann et al. (2017) or Co-Reyes et al. (2019), where executable semantics are not provided and the agent must learn through experience. Unlike prior work, we require grounding between an un- derspeci\ufb01ed goal, a document of environment dynamics, and world observations. In addition, we focus on generalisation to not only new goal descriptions but new environments dynamics. 3 READ TO FIGHT MONSTERS We consider a scenario where the agent must jointly reason over a language goal, relevant envi- ronment dynamics speci\ufb01ed in a text document, and environment observations. In reading the document, the agent should identify relevant information key to solving the goal in the environment. A successful agent needs to perform this language grounding to generalise to new environments with dynamics not seen during training. To study generalisation via reading, the environment dynamics must differ every episode such that the agent cannot avoid reading by memorising a limited set of dynamics. Consequently, we proce- durally generate a large number of unique environment dynamics (e.g.",
      "To study generalisation via reading, the environment dynamics must differ every episode such that the agent cannot avoid reading by memorising a limited set of dynamics. Consequently, we proce- durally generate a large number of unique environment dynamics (e.g. effective(blessed items, poison monsters)), along with language descriptions of environment dynamics (e.g. blessed items are effective against poison monsters) and goals (e.g. Defeat the order of the forest). We couple a large, customisable ontology inspired by rogue-like games such as NetHack or Diablo, with natural language templates to create a combinatorially rich set of environment dynam- ics to learn from and evaluate on. In RTFM, the agent is given a document of environment dynamics, observations of the environment, and an underspeci\ufb01ed goal instruction. Figure 1 illustrates an instance of the game. Concretely, we design a set of dynamics that consists of monsters (e.g. wolf, goblin), teams (e.g. Order of the For- est), element types (e.g. \ufb01re, poison), item modi\ufb01ers (e.g.",
      "Concretely, we design a set of dynamics that consists of monsters (e.g. wolf, goblin), teams (e.g. Order of the For- est), element types (e.g. \ufb01re, poison), item modi\ufb01ers (e.g. fanatical, arcane), and items (e.g. sword, 2",
      "Published as a conference paper at ICLR 2020 1 You Fire  goblin Poison  bat Arcane hammer Fanatical  sword Doc: The Rebel Enclave consists of jackal,  spider, and warg. Arcane, blessed items  are useful for poison monsters. Star  Alliance contains bat, panther, and wolf.  Goblin, jaguar, and lynx are on the same  team - they are in the Order of the Forest.  Gleaming and mysterious weapons beat  cold monsters. Lightning monsters are  weak against Grandmaster\u2019s and  Soldier\u2019s weapons. Fire monsters are  defeated by fanatical and shimmering  weapons. Goal: Defeat the Order of the Forest 4 Arcane hammer Fanatical  sword 10 Arcane hammer 5 Arcane hammer 11 Arcane hammer You Poison  bat Fire  goblin You Poison  bat Fire  goblin You Fire  goblin Poison  bat Poison  bat You Inv: none Inv: none Inv: Fanatical sword Inv: Fanatical sword Inv: Fanatical sword Figure 1: RTFM requires jointly reasoning over the goal, a document describing environment dy- namics, and environment observations.",
      "This \ufb01gure shows key snapshots from a trained policy on one randomly sampled environment. Frame 1 shows the initial world. In 4, the agent approaches \u201cfanatical sword\u201d, which beats the target \u201c\ufb01re goblin\u201d. In 5, the agent acquires the sword. In 10, the agent evades the distractor \u201cpoison bat\u201d while chasing the target. In 11, the agent engages the target and defeats it, thereby winning the episode. Sprites are used for visualisation \u2014 the agent observes cell content in text (shown in white). More examples are in appendix A. hammer). When the player is in the same cell with a monster or weapon, the player picks up the item or engages in combat with the monster. The player can possess one item at a time, and drops existing weapons if they pick up a new weapon. A monster moves towards the player with 60% probability, and otherwise moves randomly. The dynamics, the agent\u2019s inventory, and the underspeci\ufb01ed goal are rendered as text. The game world is rendered as a matrix of text in which each cell describes the entity occupying the cell.",
      "A monster moves towards the player with 60% probability, and otherwise moves randomly. The dynamics, the agent\u2019s inventory, and the underspeci\ufb01ed goal are rendered as text. The game world is rendered as a matrix of text in which each cell describes the entity occupying the cell. We use human-written templates for stating which monsters belong to which team, which modi\ufb01ers are effective against which element, and which team the agent should defeat (see appendix H for details on collection and G for a list of entities in the game). In order to achieve the goal, the agent must cross-reference relevant information in the document and as well as in the observations. During every episode, we subsample a set of groups, monsters, modi\ufb01ers, and elements to use. We randomly generate group assignments of which monsters belong to which team and which modi\ufb01er is effective against which element. A document that consists of randomly ordered statements cor- responding to this group assignment is presented to the agent. We sample one element, one team, and a monster from that team (e.g. \u201c\ufb01re goblin\u201d from \u201cOrder of the forest\u201d) to be the target monster.",
      "A document that consists of randomly ordered statements cor- responding to this group assignment is presented to the agent. We sample one element, one team, and a monster from that team (e.g. \u201c\ufb01re goblin\u201d from \u201cOrder of the forest\u201d) to be the target monster. Additionally, we sample one modi\ufb01er that beats the element and an item to be the item that defeats the target monster (e.g. \u201cfanatical sword\u201d). Similarly, we sample an element, a team, and a monster from a different team to be the distractor monster (e.g. poison bat), as well as an item that defeats the distractor monster (e.g. arcane hammer). In order to win the game (e.g. Figure 1), the agent must 1. identify the target team from the goal (e.g. Order of the Forest) 2. identify the monsters that belong to that team (e.g. goblin, jaguar, and ghost) 3. identify which monster is in the world (e.g. goblin), and its element (e.g.",
      "Order of the Forest) 2. identify the monsters that belong to that team (e.g. goblin, jaguar, and ghost) 3. identify which monster is in the world (e.g. goblin), and its element (e.g. \ufb01re) 4. identify the modi\ufb01ers that are effective against this element (e.g. fanatical, shimmering) 3",
      "Published as a conference paper at ICLR 2020 5. \ufb01nd which modi\ufb01er is present (e.g. fanatical), and the item with the modi\ufb01er (e.g. sword) 6. pick up the correct item (e.g. fanatical sword) 7. engage the correct monster in combat (e.g. \ufb01re goblin). If the agent deviates from this trajectory (e.g. does not have correct item before engaging in combat, engages with distractor monster), it cannot defeat the target monster and therefore will lose the game. The agent receives a reward of +1 if it wins the game and -1 otherwise. RTFM presents challenges not found in prior work in that it requires a large number of grounding steps in order to solve a task. In order to perform this grounding, the agent must jointly reason over a language goal and document of dynamics, as well as environment observations. In addition to the environment, the positions of the target and distractor within the document are randomised\u2014the agent cannot memorise ordering patterns in order to solve the grounding problems, and must instead identify information relevant to the goal and environment at hand.",
      "In addition to the environment, the positions of the target and distractor within the document are randomised\u2014the agent cannot memorise ordering patterns in order to solve the grounding problems, and must instead identify information relevant to the goal and environment at hand. We split environments into train and eval sets. No assignments of monster-team-modi\ufb01er-element are shared between train and eval to test whether the agent is able to generalise to new environments with dynamics not seen during training via reading. There are more than 2 million train or eval environments without considering the natural language templates, and 200 million otherwise. With random ordering of templates, the number of unique documents exceeds 15 billion. 4 MODEL We propose the txt2\u03c0 model, which builds representations that capture three-way interactions between the goal, document describing environment dynamics, and environment observations. We begin with de\ufb01nition of the Bidirectional Feature-wise Linear Modulation (FiLM2) layer, which forms the core of our model.",
      "We begin with de\ufb01nition of the Bidirectional Feature-wise Linear Modulation (FiLM2) layer, which forms the core of our model. 4.1 BIDIRECTIONAL FEATURE-WISE LINEAR MODULATION (FILM2) LAYER Text features Visual features Linear\u03b2 Linear\u03b2 Lineartext Lineartext Conv\u03b3 Conv\u03b3 Linear\u03b3 Linear\u03b3 Conv\u03b2 Conv\u03b2 Convvis Convvis + \u21e4 + 1 ReLU + 1 \u21e4 + ReLU + MaxPool output summary Figure 2: The FiLM2 layer. Feature-wise linear modulation (FiLM), which modulates visual inputs using representations of textual instructions, is an effective method for image captioning (Perez et al., 2018) and instruction fol- lowing (Bahdanau et al., 2019). In RTFM, the agent must not only \ufb01lter concepts in the visual domain using language but \ufb01lter concepts in the text domain using visual observa- tions. To support this, FiLM2 builds codependent representations of text and visual inputs by further incorporating conditional representations of the text given visual observations. Figure 2 shows the FiLM2 layer.",
      "To support this, FiLM2 builds codependent representations of text and visual inputs by further incorporating conditional representations of the text given visual observations. Figure 2 shows the FiLM2 layer. We use upper-case bold letters to denote tensors, lower-case bold letters for vectors, and non-bold letters for scalars. Exact dimensions of these variables are shown in Table 4 in appendix B. Let xtext denote a \ufb01xed-length dtext-dimensional representation of the text and Xvis the representation of visual inputs with height H, width W, and dvis channels. Let Conv denote a convolution layer. Let + and * symbols denote element-wise addition and multiplication operations that broadcast over spatial dimensions. We \ufb01rst modulate visual features using text features: \u03b3text = W\u03b3xtext + b\u03b3 (1) \u03b2text = W\u03b2xtext + b\u03b2 (2) Vvis = ReLU((1 + \u03b3text) \u2217Convvis(Xvis) + \u03b2text) (3) 4",
      "Published as a conference paper at ICLR 2020 FiLM2 Goal  BiLSTM Inventory  BiLSTM Goal-doc  BiLSTM Vis-doc  BiLSTM Attn Goal Inventory Document Positional features cat FiLM2 FiLM2 Attn Attn Attn Visual features Linear Policy  MLP Baseline  MLP policy baseline Selfattn Selfattn cat cat cat \u2026 \u2026 Pool Linear Figure 3: txt2\u03c0 models interactions between the goal, document, and observations. Unlike FiLM, we additionally modulate text features using visual features: \u0393vis = Conv\u03b3(Xvis) (4) Bvis = Conv\u03b2(Xvis) (5) Vtext = ReLU((1 + \u0393vis) \u2217(Wtextxtext + btext) + Bvis) (6) The output of the FiLM2 layer consists of the sum of the modulated features V , as well as a max-pooled summary s over this sum across spatial dimensions. V = Vvis + Vtext (7) s = MaxPool(V ) (8) 4.2 THE TXT2\u03c0 MODEL We model interactions between observations from the environment, goal, and document us- ing FiLM2 layers.",
      "V = Vvis + Vtext (7) s = MaxPool(V ) (8) 4.2 THE TXT2\u03c0 MODEL We model interactions between observations from the environment, goal, and document us- ing FiLM2 layers. We \ufb01rst encode text inputs using bidirectional LSTMs, then compute summaries using self-attention and conditional summaries using attention. We concatenate text summaries into text features, which, along with visual features, are processed through consecutive FiLM2 layers. In this case of a textual environment, we consider the grid of word embeddings as the visual features for FiLM2. The \ufb01nal FiLM2 output is further processed by MLPs to compute a policy distribution over actions and a baseline for advantage estimation. Figure 3 shows the txt2\u03c0 model. Let Eobs denote word embeddings corresponding to the observations from the environment, where Eobs[:, :, i, j] represents the embeddings corresponding to the lobs-word string that describes the objects in location (i, j) in the grid-world.",
      "Figure 3 shows the txt2\u03c0 model. Let Eobs denote word embeddings corresponding to the observations from the environment, where Eobs[:, :, i, j] represents the embeddings corresponding to the lobs-word string that describes the objects in location (i, j) in the grid-world. Let Edoc, Einv, and Egoal respectively denote the em- beddings corresponding to the ldoc-word document, the linv-word inventory, and the lgoal-word goal. We \ufb01rst compute a \ufb01xed-length summary cgoal of the the goal using a bidirectional LSTM (Hochre- iter & Schmidhuber, 1997) followed by self-attention (Lee et al., 2017; Zhong et al., 2018). Hgoal = BiLSTMgoal(Egoal) (9) a\u2032 goal,i = wgoalh\u22ba goal,i + bgoal (10) agoal = softmax(a\u2032 goal) (11) cgoal = lgoal X i=1 agoal,ihgoal,i (12) We abbreviate self-attention over the goal as cgoal = selfattn(Hgoal).",
      "We similarly compute a summary of the inventory as cinv = selfattn(BiLSTMinv(Einv)). Next, we represent the document encoding conditioned on the goal using dot-product attention (Luong et al., 2015). Hdoc = BiLSTMgoal-doc(Edoc) (13) a\u2032 doc,i = cgoalh\u22ba doc,i (14) adoc = softmax(a\u2032 doc) (15) cdoc = ldoc X i=1 adoc,ihdoc,i (16) We abbreviate attention over the document encoding conditioned on the goal summary as cdoc = attend(Hdoc, cgoal). Next, we build the joint representation of the inputs using succes- sive FiLM2 layers. At each layer, the visual input to the FiLM2 layer is the concatenation of the output of the previous layer with positional features. For each cell, the positional feature Xpos consists of the x and y distance from the cell to the agent\u2019s position respectively, normalized by 5",
      "Published as a conference paper at ICLR 2020 0 1 2 3 4 5 frames 1e7 0.2 0.4 0.6 0.8 1.0 win rate     exp  txt2\u03c0  film  conv no_text_mod no_vis_attn no_goal_attn Figure 4: Ablation training curves on simplest variant of RTFM. Individual runs are in light colours. Average win rates are in bold, dark lines. Model Win rate Train Eval 6\u00d76 Eval 10\u00d710 conv 24 \u00b1 0 25 \u00b1 1 13 \u00b1 1 FiLM 49 \u00b1 1 49 \u00b1 2 32 \u00b1 3 no task attn 49 \u00b1 2 49 \u00b1 2 35 \u00b1 6 no vis attn 49 \u00b1 2 49 \u00b1 1 40\u00b112 no text mod 49 \u00b1 1 49 \u00b1 2 35 \u00b1 2 txt2\u03c0 84\u00b121 83\u00b121 66\u00b122 Table 1: Final win rate on simplest variant of RTFM. The models are trained on one set of dynamics (e.g.",
      "The models are trained on one set of dynamics (e.g. training set) and evaluated on another set of dynamics (e.g. evaluation set). \u201cTrain\u201d and \u201cEval\u201d show \ufb01nal win rates on training and eval environments. the width and height of the grid-world. The text input is the concatenation of the goal summary, the inventory summary, the attention over the document given the goal, and the attention over the document given the previous visual summary. Let [a; b] denote the feature-wise concatenation of a and b. For the ith layer, we have R(i) = [V (i\u22121); Xpos] (17) T (i) = [cgoal; cinv; cdoc; attend(BiLSTMvis-doc(Edoc), s(i\u22121))] (18) V (i), s(i) = FiLM2(i)(R(i), T(i)) (19) BiLSTMvis-doc(Edoc) is another encoding of the document similar to Hgoal, produced using a sep- arate LSTM, such that the document is encoded differently for attention with the visual features and with the goal.",
      "For i = 0, we concatenate the bag-of-words embeddings of the grid with positional features as the initial visual features V (0) = [P j Eobs,j; Xpos]. We max pool a linear transform of the initial visual features to compute the initial visual summary s(0) = MaxPool(WiniV (0) +bini). Let s(last) denote visual summary of the last FiLM2 layer. We compute the policy ypolicy and baseline ybaseline as o = ReLU(Wos(last) + bo) (20) ypolicy = MLPpolicy(o) (21) ybaseline = MLPbaseline(o) (22) where MLPpolicy and MLPbaseline are 2-layer multi-layer perceptrons with ReLU activation. We train using TorchBeast (K\u00a8uttler et al., 2019), an implementation of IMPALA (Espeholt et al., 2018). Please refer to appendix D for details.",
      "We train using TorchBeast (K\u00a8uttler et al., 2019), an implementation of IMPALA (Espeholt et al., 2018). Please refer to appendix D for details. 5 EXPERIMENTS We consider variants of RTFM by varying the size of the grid-world (6 \u00d7 6 vs 10 \u00d7 10), allowing many-to-one group assignments to make disambiguation more dif\ufb01cult (group), allowing dynamic, moving monsters that hunt down the player (dyna), and using natural language templated docu- ments (nl). In the absence of many-to-one assignments, the agent does not need to perform steps 3 and 5 in section 3 as there is no need to disambiguate among many assignees, making it easier to identify relevant information. We compare txt2\u03c0 to the FiLM model by Bahdanau et al. (2019) and a language-conditioned residual CNN model. We train on one set of dynamics (e.g. group assignments of monsters and modi\ufb01ers) and evaluated on a held-out set of dynamics. We also study three variants of txt2\u03c0. 6",
      "Published as a conference paper at ICLR 2020 Transfer from Transfer to 6 \u00d7 6 6 \u00d7 6 dyna 6 \u00d7 6 groups 6 \u00d7 6 nl 6 \u00d7 6 dyna groups 6 \u00d7 6 group nl 6 \u00d7 6 dyna nl 6 \u00d7 6 dyna group nl random 84 \u00b1 20 26 \u00b1 7 25 \u00b1 3 45 \u00b1 6 23 \u00b1 2 25 \u00b1 3 23 \u00b1 2 23 \u00b1 2 +6 \u00d7 6 85 \u00b1 9 82 \u00b1 19 78 \u00b1 24 64 \u00b1 12 52 \u00b1 13 53 \u00b1 18 40 \u00b1 8 +dyna 77 \u00b1 10 65 \u00b1 16 43 \u00b1 4 +group 65 \u00b1 17 Table 2: Curriculum training results. We keep 5 randomly initialised models through the entire curriculum. A cell in row i and column j shows transfer from the best-performing setting in the previous stage (bold in row i \u22121) to the new setting in column j. Each cell shows \ufb01nal mean and standard deviation of win rate on the training environments.",
      "A cell in row i and column j shows transfer from the best-performing setting in the previous stage (bold in row i \u22121) to the new setting in column j. Each cell shows \ufb01nal mean and standard deviation of win rate on the training environments. Each experiment trains for 50 million frames, except for the initial stage (\ufb01rst row, 100 million instead). For the last stage (row 4), we also transfer to a 10 \u00d7 10 + dyna + group + nl variant and obtain 61 \u00b1 18 win rate. In no task attn, the document attention conditioned on the goal utterance (equation 16) is re- moved and the goal instead represented through self-attention and concatenated with the rest of the text features. In no vis attn, we do not attend over the document given the visual output of the previous layer (equation 18), and the document is instead represented through self-attention. In no text mod, text modulation using visual features (equation 6) is removed. Please see ap- pendix C for model details on our model and baselines, and appendix D for training details.",
      "In no text mod, text modulation using visual features (equation 6) is removed. Please see ap- pendix C for model details on our model and baselines, and appendix D for training details. 5.1 COMPARISON TO BASELINES AND ABLATIONS We compare txt2\u03c0 to baselines and ablated variants on a simpli\ufb01ed variant of RTFM in which there are one-to-one group assignments (no group), stationary monsters (no dyna), and no nat- ural language templated descriptions (no nl). Figure 4 shows that compared to baselines and ab- lated variants, txt2\u03c0 is more sample ef\ufb01cient and converges to higher performance. Moreover, no ablated variant is able to solve the tasks\u2014it is the combination of ablated features that en- ables txt2\u03c0 to win consistently. Qualitatively, the ablated variants converge to locally optimum policies in which the agent often picks up a random item and then attacks the correct monster, re- sulting in a \u223c50% win rate.",
      "Qualitatively, the ablated variants converge to locally optimum policies in which the agent often picks up a random item and then attacks the correct monster, re- sulting in a \u223c50% win rate. Table 1 shows that all models, with the exception of the CNN baseline, generalise to new evaluation environments with dynamics and world con\ufb01gurations not seen during training, with txt2\u03c0 outperforming FiLM and the CNN model. We \ufb01nd similar results for txt2\u03c0, its ablated variants, and baselines on a separate, language-based rock-paper-scissors task in which the agent needs to deduce cyclic dependencies (which type beats which other type) through reading in order to acquire the correct item and defeat a monster. We observe that the performance of reading models transfer from training environments to new envi- ronments with unseen types and unseen dependencies. Compared to ablated variants and baselines, txt2\u03c0 is more sample ef\ufb01cient and achieves higher performance on both training and new envi- ronment dynamics. When transferring to new environments, txt2\u03c0 remains more sample ef\ufb01cient than the other models. Details on these experiments are found in appendix E.",
      "When transferring to new environments, txt2\u03c0 remains more sample ef\ufb01cient than the other models. Details on these experiments are found in appendix E. 5.2 CURRICULUM LEARNING FOR COMPLEX ENVIRONMENTS Train env Eval env Win rate Train Eval 6 \u00d7 6 6 \u00d7 6 65 \u00b1 17 55 \u00b1 22 10 \u00d7 10 55 \u00b1 27 10 \u00d7 10 10 \u00d7 10 61 \u00b1 18 43 \u00b1 13 Table 3: Win rate when evaluating on new dynamics and world con\ufb01gurations for txt2\u03c0 on the full RTFM problem. Due to the long sequence of co-references the agent must perform in order to solve the full RTFM (10 \u00d7 10 with moving monsters, many-to-one group as- signments, and natural language templated docu- ments) we design a curriculum to facilitate policy learning by starting with simpler variants of RTFM. We start with the simplest variant (no group, no dyna, no nl) and then add in an additional di- mension of complexity. We repeatedly add more 7",
      "Published as a conference paper at ICLR 2020 gl a0 a1 a2 a3 a4                                                     you should use grandmasters   ,  shimmering items to beat fire monsters . the shaman , zombie , ghost monsters are rebel enclave . poison is weak against fanatical , gleaming . lightning is defeated by blessed , mysterious . wolf , bat , panther are order of the forest . cold is not good against arcane , soldiers . star alliance team is made up of goblin , imp , jaguar . pad (a) The entities present are shimmering morning star, mysterious spear, \ufb01re jaguar, and lightning ghost. gl a0 a1 a2 a3 a4                                          lightning is defeated by blessed , gleaming . poison is defeated by grandmasters  , soldiers . cold is weak against arcane , mysterious . the zombie , ghost , wolf monsters are star alliance . shaman , imp , bat are on the rebel enclave team . the goblin , jaguar , panther monsters are order of the forest . fire is not good against fanatical , shimmering .",
      "cold is weak against arcane , mysterious . the zombie , ghost , wolf monsters are star alliance . shaman , imp , bat are on the rebel enclave team . the goblin , jaguar , panther monsters are order of the forest . fire is not good against fanatical , shimmering . pad (b) The entities present are soldier\u2019s axe, shimmering axe, \ufb01re shaman, and poison wolf. Figure 5: txt2\u03c0 attention on the full RTFM. These include the document attention conditioned on the goal (top) as well as those conditioned on summaries produced by intermediate FiLM2 layers. Weights are normalised across words (e.g. horizontally). Darker means higher attention weight. complexity until we obtain 10\u00d710 worlds with mov- ing monsters, many-to-one group assignments and natural language templated descriptions. The per- formance across the curriculum is shown in Table 2 (see Figure 13 in appendix F for training curves of each stage).",
      "complexity until we obtain 10\u00d710 worlds with mov- ing monsters, many-to-one group assignments and natural language templated descriptions. The per- formance across the curriculum is shown in Table 2 (see Figure 13 in appendix F for training curves of each stage). We see that curriculum learning is crucial to making progress on RTFM, and that initial policy training (\ufb01rst row of Table 2) with addi- tional complexities in any of the dimensions result in signi\ufb01cantly worse performance. We take each of the 5 runs after training through the whole curriculum and evaluate them on dynamics not seen during training. Table 3 shows variants of the last stage of the curriculum in which the model was trained on 6 \u00d7 6 versions of the full RTFM and in which the model was trained on 10 \u00d7 10 versions of the full RTFM. We see that models trained on smaller worlds generalise to bigger worlds. Despite curriculum learning, however, performance of the \ufb01nal model trail that of human players, who can consistently solve RTFM.",
      "We see that models trained on smaller worlds generalise to bigger worlds. Despite curriculum learning, however, performance of the \ufb01nal model trail that of human players, who can consistently solve RTFM. This highlights the dif\ufb01culties of the RTFM problem and suggests that there is signi\ufb01cant room for improvement in developing better language grounded policy learners. Attention maps. Figure 5 shows attention conditioned on the goal and on observation summaries produced by intermediate FiLM2 layers. Goal-conditioned attention consistently locates the clause that contains the team the agent is supposed to attack. Intermediate layer attentions focus on regions near modi\ufb01ers and monsters, particularly those that are present in the observations. These results suggests that attention mechanisms in txt2\u03c0 help identify relevant information in the document. Analysis of trajectories and failure modes. We examine trajectories from well-performing poli- cies (80% win rate) as well as poorly-performing policies (50% win rate) on the full RTFM.",
      "Analysis of trajectories and failure modes. We examine trajectories from well-performing poli- cies (80% win rate) as well as poorly-performing policies (50% win rate) on the full RTFM. We \ufb01nd that well-performing policies exhibit a number of consistent behaviours such as identifying the correct item to pick up to \ufb01ght the target monster, avoiding distractors, and engaging target monsters after acquiring the correct item. In contrast, the poorly-performing policies occasionally pick up the wrong item, causing the agent to lose when engaging with a monster. In addition, it occasionally gets stuck in evading monsters inde\ufb01nitely, causing the agent to lose when the time runs out. Replays of both policies can be found in GIFs in the supplementary materials1. 1Trajectories by txt2\u03c0 on RTFM can be found at https://gofile.io/?c=9k7ZLk 8",
      "Published as a conference paper at ICLR 2020 6 CONCLUSION We proposed RTFM, a grounded policy learning problem in which the agent must jointly reason over a language goal, relevant dynamics speci\ufb01ed in a document, and environment observations. In order to study RTFM, we procedurally generated a combinatorially large number of environment dynam- ics such that the model cannot memorise a set of environment dynamics and must instead generalise via reading. We proposed txt2\u03c0, a model that captures three-way interactions between the goal, document, and observations, and that generalises to new environments with dynamics not seen dur- ing training. txt2\u03c0 outperforms baselines such as FiLM and language-conditioned CNNs. Through curriculum learning, txt2\u03c0 performs well on complex RTFM tasks that require several reasoning and coreference steps with natural language templated goals and descriptions of the dynamics. Our work suggests that language understanding via reading is a promising way to learn policies that generalise to new environments. Despite curriculum learning, our best models trail performance of human players, suggesting that there is ample room for improvement in grounded policy learning on complex RTFM problems.",
      "Our work suggests that language understanding via reading is a promising way to learn policies that generalise to new environments. Despite curriculum learning, our best models trail performance of human players, suggesting that there is ample room for improvement in grounded policy learning on complex RTFM problems. In addition to jointly learning policies based on external documentation and language goals, we are interested in exploring how to use supporting evidence in external doc- umentation to reason about plans (Andreas et al., 2018) and induce hierarchical policies (Hu et al., 2019; Jiang et al., 2019). ACKNOWLEDGEMENT We thank Heinrich K\u00a8uttler and Nantas Nardelli for their help in adapting TorchBeast and the FAIR London team for their feedback and support. REFERENCES Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S\u00a8underhauf, Ian D. Reid, Stephen Gould, and Anton van den Hengel. Vision-and-language navigation: Interpret- ing visually-grounded navigation instructions in real environments. In CVPR, 2018. Jacob Andreas, Dan Klein, and Sergey Levine. Learning with latent language. In NAACL, 2018.",
      "Vision-and-language navigation: Interpret- ing visually-grounded navigation instructions in real environments. In CVPR, 2018. Jacob Andreas, Dan Klein, and Sergey Levine. Learning with latent language. In NAACL, 2018. Dzmitry Bahdanau, Felix Hill, Jan Leike, Edward Hughes, Pushmeet Kohli, and Edward Grefen- stette. Learning to follow language instructions with adversarial reward induction. In ICLR, 2019. K. Barnard and D. Forsyth. Learning the semantics of words and pictures. In ICCV, 2001. S. R. K. Branavan, David Silver, and Regina Barzilay. Learning to win by reading manuals in a monte-carlo framework. In ACL, 2011. S. R. K. Branavan, Nate Kushman, Tao Lei, and Regina Barzilay. Learning high-level planning from text. In ACL, 2012. S.R.K. Branavan. Grounding Linguistic Analysis in Control Applications. PhD thesis, MIT, 2012.",
      "Learning high-level planning from text. In ACL, 2012. S.R.K. Branavan. Grounding Linguistic Analysis in Control Applications. PhD thesis, MIT, 2012. David L. Chen and Raymond J. Mooney. Learning to sportscast: A test of grounded language acquisition. In ICML, 2008. John D. Co-Reyes, Abhishek Gupta, Suvansh Sanjeev, Nick Altieri, John DeNero, Pieter Abbeel, and Sergey Levine. Guiding policies with language via meta-learning. In ICLR, 2019. Lasse Espeholt, Hubert Soyer, R\u00b4emi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA: scalable distributed deep-rl with importance weighted actor-learner architectures. In ICML, 2018.",
      "IMPALA: scalable distributed deep-rl with importance weighted actor-learner architectures. In ICML, 2018. Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. Speaker-follower models for vision-and-language navigation. In NeurIPS, 2018. 9",
      "Published as a conference paper at ICLR 2020 Karl Moritz Hermann, Felix Hill, Simon Green, Fumin Wang, Ryan Faulkner, Hubert Soyer, David Szepesvari, Wojciech Marian Czarnecki, Max Jaderberg, Denis Teplyashin, Marcus Wainwright, Chris Apps, Demis Hassabis, and Phil Blunsom. Grounded language learning in a simulated 3d world. CoRR, abs/1706.06551, 2017. Sepp Hochreiter and J\u00a8urgen Schmidhuber. Long short-term memory. Neural Compututation, 9(8), 1997. Hengyuan Hu, Denis Yarats, Qucheng Gong, Yuandong Tian, and Mike Lewis. Hierarchical decision making by generating and following natural language instructions. CoRR, abs/1906.00744, 2019. Yiding Jiang, Shixiang Gu, Kevin Murphy, and Chelsea Finn. Language as an abstraction for hier- archical deep reinforcement learning. CoRR, abs/1906.07343, 2019.",
      "CoRR, abs/1906.00744, 2019. Yiding Jiang, Shixiang Gu, Kevin Murphy, and Chelsea Finn. Language as an abstraction for hier- archical deep reinforcement learning. CoRR, abs/1906.07343, 2019. Thomas Kollar, Stefanie Tellex, Deb Roy, and Nicholas Roy. Toward understanding natural language directions. In HRI, 2010. Heinrich K\u00a8uttler, Nantas Nardelli, Thibaut Lavril, Marco Selvatici, Viswanath Sivakumar, Tim Rockt\u00a8aschel, and Edward Grefenstette. TorchBeast: A PyTorch Platform for Dis- tributed RL. arXiv preprint arXiv:1910.03552, 2019. URL https://github.com/ facebookresearch/torchbeast. Kenton Lee, Luheng He, Mike Lewis, and Luke Zettlemoyer. End-to-end neural coreference reso- lution. In EMNLP, 2017.",
      "URL https://github.com/ facebookresearch/torchbeast. Kenton Lee, Luheng He, Mike Lewis, and Luke Zettlemoyer. End-to-end neural coreference reso- lution. In EMNLP, 2017. Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, and Jianfeng Gao. Deep reinforce- ment learning for dialogue generation. In EMNLP, 2016. Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Manfred Otto Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learn- ing. CoRR, abs/1509.02971, 2015. Jelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob Foerster, Jacob Andreas, Edward Grefen- stette, Shimon Whiteson, and Tim Rockt\u00a8aschel. A Survey of Reinforcement Learning Informed by Natural Language. In IJCAI, 2019.",
      "A Survey of Reinforcement Learning Informed by Natural Language. In IJCAI, 2019. Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention- based neural machine translation. In ACL, 2015. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR, abs/1312.5602, 2013. Karthik Narasimhan, Regina Barzilay, and Tommi S. Jaakkola. Deep transfer in reinforcement learning by language grounding. JAIR, 2018. Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron C. Courville. Film: Visual reasoning with a general conditioning layer. In AAAI, 2018. Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew R. Walter, Ashis Gopal Banerjee, Seth Teller, and Nicholas Roy.",
      "Film: Visual reasoning with a general conditioning layer. In AAAI, 2018. Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew R. Walter, Ashis Gopal Banerjee, Seth Teller, and Nicholas Roy. Understanding natural language commands for robotic navigation and mobile manipulation. In AAAI, 2011. T. Tieleman and G. Hinton. Lecture 6.5\u2014RmsPropG: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012. Sida I. Wang, Percy Liang, and Christopher D. Manning. Learning language games through inter- action. In ACL, 2016. Victor Zhong, Caiming Xiong, and Richard Socher. Global-locally self-attentive dialogue state tracker. In ACL, 2018. 10",
      "Published as a conference paper at ICLR 2020 A PLAYTHROUGH EXAMPLES These \ufb01gures shows key snapshots from a trained policy on randomly sampled environments. 1 You Doc: You should use arcane and mysterious  items to beat lightning monsters. Panther,  warg, and wolf are from the Order of the  Forest. Blessed and Grandmaster\u2019s items  beat poison monsters. Cold is not good  against fanatical and shimmering  weapons. The Star Alliance team is made  up of beetle, jackal, and shaman. Imp,  jaguar, and lynx make up the Rebel  Enclave. Gleaming and Soldier\u2019s weapons  beat \ufb01re monsters.",
      "The Star Alliance team is made  up of beetle, jackal, and shaman. Imp,  jaguar, and lynx make up the Rebel  Enclave. Gleaming and Soldier\u2019s weapons  beat \ufb01re monsters. Goal: Defeat the Star Alliance Fire imp Arcane  spear Soldier\u2019s  sword 3 Arcane  spear Soldier\u2019s  sword 8 Arcane  spear 7 Arcane  spear Soldier\u2019s  sword 9 Arcane  spear Ligtning  shaman You Ligtning  shaman Fire imp You Ligtning  shaman Fire imp You Ligtning  shaman Fire imp Fire imp Ligtning  shaman Inv: none Inv: none Inv: none Inv: Soldier\u2019s sword Inv: Soldier\u2019s sword Figure 6: The initial world is shown in 1. In 4, the agent avoids the target \u201clightning shaman\u201d because it does not yet have \u201carcane spear\u201d, which beats the target. In 7 and 8, the agent is cornered by monsters. In 9, the agent is forced to engage in combat and loses.",
      "In 4, the agent avoids the target \u201clightning shaman\u201d because it does not yet have \u201carcane spear\u201d, which beats the target. In 7 and 8, the agent is cornered by monsters. In 9, the agent is forced to engage in combat and loses. 1 You Soldier\u2019s  knife Doc: Cold monsters are defeated by gleaming  and Soldier\u2019s weapons. The Order of the  Forest team consists of ant, lynx, and  wolf. Mysterious and shimmering  weapons are good against lightning  monsters. Poison monster are defeated  by blessed and fanatical items. Get  arcane and Grandmaster\u2019s weapons to  slay \ufb01re monsters. Beetle, panther, and  zombie are Star Alliance. Jackal, jaguar,  and ghost are on the Rebel Enclave. Goal: Fight the monster in the Rebel Enclave.",
      "Get  arcane and Grandmaster\u2019s weapons to  slay \ufb01re monsters. Beetle, panther, and  zombie are Star Alliance. Jackal, jaguar,  and ghost are on the Rebel Enclave. Goal: Fight the monster in the Rebel Enclave. Poison  zombie Cold  ghost Fanatical  katana 5 You Soldier\u2019s  knife Poison  zombie Cold  ghost Fanatical  katana 11 You Soldier\u2019s  knife Poison  zombie Cold  ghost Fanatical  katana 13 You Poison  zombie Cold  ghost Fanatical  katana 14 You Poison  zombie Fanatical  katana Inv: none Inv: Soldier\u2019s knife Inv: Soldier\u2019s knife Inv: none Inv: none Figure 7: The initial world is shown in 1. In 5 the agent evades the target \u201ccold ghost\u201d because it does not yet have \u201csoldier\u2019s knife\u201d, which beats the target. In 11 and 13, the agent obtains \u201csoldier\u2019s knife\u201d while evading monsters. In 14, the agent defeats the target and wins. 11",
      "Published as a conference paper at ICLR 2020 B VARIABLE DIMENSIONS Let xtext \u2208Rdtext denote a \ufb01xed-length dtext-dimensional representation of the text and Xvis \u2208 Rdvis\u00d7H\u00d7W denote the representation of visual inputs with Variable Symbol Dimension dtext-dim text representation xtext dtext dvis-dim visual representation with height H, width W, dvis channels Xvis dvis \u00d7 H \u00d7 W Environment observations embeddings Eobs lobs \u00d7 demb \u00d7 H \u00d7 W lobs-word string that describes the objects in location (i, j) in the grid-world Eobs[:, :, i, j] lobs \u00d7 demb ldoc-word document embeddings Edoc ldoc \u00d7 demb linv-word inventory embeddings Einv linv \u00d7 demb lgoal-word goal embeddings Egoal lgoal \u00d7 demb Table 4: Variable dimensions 12",
      "Published as a conference paper at ICLR 2020 C MODEL DETAILS C.1 TXT2\u03c0 Hyperparameters. The txt2\u03c0 used in our experiments consists of 5 consecutive FiLM2 layers, each with 3x3 convolutions and padding and stride sizes of 1. The txt2\u03c0 layers have channels of 16, 32, 64, 64, and 64, with residual connections from the 3rd layer to the 5th layer. The Goal-doc LSTM (see Figure 3) shares weight with the Goal LSTM. The Inventory and Goal LSTMs have a hidden dimension of size 10, whereas the Vis-doc LSTM has a dimension of 100. We use a word embedding dimension of 30. C.2 CNN WITH RESIDUAL CONNECTIONS Conv Goal  BiLSTM Inventory  BiLSTM Doc BiLSTM Selfattn Goal Inventory Document Positional features cat Conv Conv Visual features Linear Policy  MLP Baseline  MLP policy baseline Selfattn Selfattn cat cat cat \u2026 Figure 8: The convolutional network baseline. The FiLM baseline has the same structure, but with convolutional layers replaced by FiLM layers.",
      "The FiLM baseline has the same structure, but with convolutional layers replaced by FiLM layers. Like txt2\u03c0, the CNN baseline consists of 5 layers of convolutions with channels of 16, 32, 64, 64, and 64. There are residual connections from the 3rd layer to the 5th layer. The input to each layer consists of the output of the previous layer, concatenated with positional features. The input to the network is the concatenation of the observations V (0) and text representations. The text representations consist of self-attention over bidirectional LSTM-encoded goal, document, and inventory. These attention outputs are replicated over the dimensions of the grid and concatenated feature-wise with the observation embeddings in each cell. Figure 8 illustrates the CNN baseline. C.3 FILM BASELINE The FiLM baseline encodes text in the same fashion as the CNN model. However, instead of using convolutional layers, each layer is a FiLM layer from Bahdanau et al. (2019). Note that in our case, the language representation is a self-attention over the LSTM states instead of a concatenation of terminal LSTM states. 13",
      "Published as a conference paper at ICLR 2020 D TRAINING PROCEDURE We train using an implementation of IMPALA (Espeholt et al., 2018). In particular, we use 20 actors and a batch size of 24. When unrolling actors, we use a maximum unroll length of 80 frames. Each episode lasts for a maximum of 1000 frames. We optimise using RMSProp (Tieleman & Hinton, 2012) with a learning rate of 0.005, which is annealed linearly for 100 million frames. We set \u03b1 = 0.99 and \u03f5 = 0.01. During training, we apply a small negative reward for each time step of \u22120.02 and a discount factor of 0.99 to facilitate convergence. We additionally include a entropy cost to encourage exploration. Let ypolicy denote the policy. The entropy loss is calculated as Lpolicy = \u2212 X i ypolicyi log ypolicyi (23) In addition to policy gradient, we add in the entropy loss with a weight of 0.005 and the baseline loss with a weight of 0.5.",
      "Let ypolicy denote the policy. The entropy loss is calculated as Lpolicy = \u2212 X i ypolicyi log ypolicyi (23) In addition to policy gradient, we add in the entropy loss with a weight of 0.005 and the baseline loss with a weight of 0.5. The baseline loss is computed as the root mean square of the advantages (Es- peholt et al., 2018). When tuning models, we perform a grid search using the training environments to select hyperpa- rameters for each model. We train 5 runs for each con\ufb01guration in order to report the mean and standard deviation. When transferring, we transfer each of the 5 runs to the new task and once again report the mean and standard deviation. 14",
      "Published as a conference paper at ICLR 2020 Scenario # graphs # edges # nodes train dev unseen train dev % new train dev % new permutation 30 30 y 20 20 n 60 60 n new edge 20 20 y 48 36 y 17 13 n new edge+nodes 60 60 y 20 20 y 5 5 y Table 5: Statistics of the three variations of the Rock-paper-scissors task 9x9 10x10 11x11 world size 0.0 0.2 0.4 0.6 0.8 1.0 win rate seen environment dynamics conv film film^2 9x9 10x10 11x11 world size new environment dynamics conv film film^2 Figure 10: Performance on the Rock-paper-scissors task across models. Left shows \ufb01nal perfor- mance on environments whose goals and dynamics were seen during training. Right shows perfor- mance on the environments whose goals and dynamics were not seen during training.",
      "Left shows \ufb01nal perfor- mance on environments whose goals and dynamics were seen during training. Right shows perfor- mance on the environments whose goals and dynamics were not seen during training. E ROCK-PAPER-SCISSORS Doc: e beats d.  c beats e.  d beats c. Inventory: empty B goblin A You C B Figure 9: The Rock-paper-scissors task requires jointly reasoning over the game observations and a document describing environment dynamics. The agent observes cell content in the form of text (shown in white). In addition to the main RTFM tasks, we also study a simpler formulation called Rock-paper-scissors that has a \ufb01xed goal. In Rock-paper-scissors, the agent must interpret a document that describes the environment dynamics in order to solve the task. Given an set of characters (e.g. a-z), we sample 3 characters and set up a rock-paper-scissors-like dependency graph between the characters (e.g. \u201ca beats b, b beats c, c beats a\u201d). We then spawn a monster in the world with a randomly as- signed type (e.g.",
      "a-z), we sample 3 characters and set up a rock-paper-scissors-like dependency graph between the characters (e.g. \u201ca beats b, b beats c, c beats a\u201d). We then spawn a monster in the world with a randomly as- signed type (e.g. \u201cb goblin\u201d), as well as an item corresponding to each type (e.g. \u201ca\u201d, \u201cb\u201d, and \u201cc\u201d). The attributes of the agent, monster, and items are set up such that the player must obtain the correct item and then engage the monster in order to win. Any other sequence of actions (e.g. en- gaging the monster without the correct weapon) results in a loss. The winning policy should then be to \ufb01rst identify the type of mon- ster present, then cross-reference the document to \ufb01nd which item defeats that type, then pick up the item, and \ufb01nally engage the monster in combat. Figure 9 shows an instance of Rock-paper-scissors. Reading models generalise to new environments. We split environment dynamics by per- muting 3-character dependency graphs from an alphabet, which we randomly split into training and held-out sets.",
      "Figure 9 shows an instance of Rock-paper-scissors. Reading models generalise to new environments. We split environment dynamics by per- muting 3-character dependency graphs from an alphabet, which we randomly split into training and held-out sets. This corresponds to the \u201cpermutations\u201d setting in Table 5. 15",
      "Published as a conference paper at ICLR 2020 0 1 2 3 4 5 frames 1e7 0.0 0.2 0.4 0.6 0.8 1.0 win rate permutations 0 1 2 3 4 5 frames 1e7 new edge 0 1 2 3 4 5 frames 1e7 new edge+node conv film film^2 conv transfer film transfer film^2 transfer Figure 11: Learning curve while transferring to the development environments. Win rates of indi- vidual runs are shown in light colours. Average win rates are shown in bold, dark lines. 0 1 2 3 4 5 frames 1e7 0.0 0.2 0.4 0.6 0.8 1.0 win rate exp film^2 film conv no_text_mod no_vis_attn no_goal_attn Figure 12: Ablation training curves. Win rates of individual runs are shown in light colours. Aver- age win rates are shown in bold, dark lines.",
      "Win rates of individual runs are shown in light colours. Aver- age win rates are shown in bold, dark lines. We train models on the 10 \u00d7 10 worlds from the training set and evaluate them on both seen and not seen during training. The left of Figure 10 shows the performance of mod- els on worlds of varying sizes with training en- vironment dynamics. In this case, the dynam- ics (e.g. dependency graphs) were seen during training. For 9 \u00d7 9 and 11 \u00d7 11 worlds, the world con\ufb01guration not seen during training. For 10 \u00d7 10 worlds, there is a 5% chance that the initial frame was seen during training.2 Fig- ure 10 shows the performance on held-out envi- ronments not seen during training. We see that all models generalise to environments not seen during training, both when the world con\ufb01gura- tion is not seen (left) and when the environment dynamics are not seen (right). Reading models generalise to new concepts.",
      "We see that all models generalise to environments not seen during training, both when the world con\ufb01gura- tion is not seen (left) and when the environment dynamics are not seen (right). Reading models generalise to new concepts. In addition to splitting via permutations, we de- vise two additional ways of splitting environment dynamics by introducing new edges and nodes into the held-out set. Table 5 shows the three different settings. For each, we study the transfer be- haviour of models on new environments. Figure 11 shows the learning curve when training a model on the held-out environments directly and when transferring the model trained on train environments to held-out environments. We observe that all models are signi\ufb01cantly more sample-ef\ufb01cient when transferring from training environments, despite the introduction of new edges and new nodes. txt2\u03c0 is more sample-ef\ufb01cient and learns better policies. In Figure 10, we see that the FiLM model outperforms the CNN model on both training environment dynamics and held-out environ- ment dynamics. txt2\u03c0 further outperforms FiLM, and does so more consistently in that the \ufb01nal performance has less variance.",
      "In Figure 10, we see that the FiLM model outperforms the CNN model on both training environment dynamics and held-out environ- ment dynamics. txt2\u03c0 further outperforms FiLM, and does so more consistently in that the \ufb01nal performance has less variance. This behaviour is also observed in the in Figure 11. When training on the held-out set without transferring, txt2\u03c0 is more sample ef\ufb01cient than FiLM and the CNN model, and achieves higher win-rate. When transferring to the held-out set, txt2\u03c0 remains more sample ef\ufb01cient than the other models. 2There are 24360 unique grid con\ufb01gurations given a particular dependency graph, 4060 unique dependency graphs in the training set, and 50 million frames seen during training. After training, the model \ufb01nishes an episode in approximately 10 frames. Hence the probability of seeing a redundant initial frame is 5e7/10 24360\u22174060 = 5%. 16",
      "Published as a conference paper at ICLR 2020 F CURRICULUM LEARNING TRAINING CURVES 0.0 0.2 0.4 0.6 0.8 1.0 frames 1e8 0.0 0.2 0.4 0.6 0.8 1.0 win rate rand 0 1 2 3 4 5 frames 1e7 rand +6x6 0 1 2 3 4 5 frames 1e7 0.0 0.2 0.4 0.6 0.8 1.0 win rate rand +6x6 +dyna 0 1 2 3 4 5 frames 1e7 rand +6x6 +dyna +groups 6 10 move 6 extra 6 nl 6 extra,nl 6 move,nl 6 extra,move 6 extra,move,nl 6 extra,move,nl 10 Figure 13: Curriculum learning results for txt2\u03c0 on RTFM. Win rates of individual runs are shown in light colours.",
      "Win rates of individual runs are shown in light colours. Average win rates are shown in bold, dark lines. G ENTITIES AND MODIFIERS Below is a list of entities and modi\ufb01ers contained in RTFM: Monsters: wolf, jaguar, panther, goblin, bat, imp, shaman, ghost, zombie Weapons: sword, axe, morningstar, polearm, knife, katana, cutlass, spear Elements: cold, \ufb01re, lightning, poison Modi\ufb01ers: Grandmaster\u2019s, blessed, shimmering, gleaming, fanatical, mysterious, Soldier\u2019s, arcane Teams: Star Alliance, Order of the Forest, Rebel Enclave H LANGUAGE TEMPLATES We collect human-written natural language templates for the goal and the dynamics. The goal statements in RTFM describe which team the agent should defeat. We collect 12 language templates for goal statements. The document of environment dynamics consists of two types of statements. The \ufb01rst type describes which monsters are assigned to with team. The second type describes which modi\ufb01ers, which describe items, are effective against which element types, which are associated with monsters.",
      "The document of environment dynamics consists of two types of statements. The \ufb01rst type describes which monsters are assigned to with team. The second type describes which modi\ufb01ers, which describe items, are effective against which element types, which are associated with monsters. We collection 10 language templates for each type of statements. The entire document is composed from statements, which are randomly shuf\ufb02ed. We randomly sample a template for each statement, which we \ufb01ll with the monsters and team for the \ufb01rst type and modi\ufb01ers and element for the second type. 17"
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1910.08210.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":12045,
  "avg_doclen":179.776119403,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1910.08210.pdf"
    }
  }
}