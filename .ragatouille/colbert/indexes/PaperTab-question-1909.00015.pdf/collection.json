[
  "Adaptively Sparse Transformers Gonc\u00b8alo M. Correia\u00e4 goncalo.correia@lx.it.pt Vlad Niculae\u00e4 vlad@vene.ro \u00e4Instituto de Telecomunicac\u00b8\u02dcoes, Lisbon, Portugal \u00e3Unbabel, Lisbon, Portugal Andr\u00b4e F.T. Martins\u00e4 \u00e3 andre.martins@unbabel.com Abstract Attention mechanisms have become ubiqui- tous in NLP. Recent architectures, notably the Transformer, learn powerful context-aware word representations through layered, multi- headed attention. The multiple heads learn diverse types of word relationships. How- ever, with standard softmax attention, all at- tention heads are dense, assigning a non-zero weight to all context words. In this work, we introduce the adaptively sparse Transformer, wherein attention heads have \ufb02exible, context- dependent sparsity patterns. This sparsity is accomplished by replacing softmax with \u03b1- entmax: a differentiable generalization of soft- max that allows low-scoring words to receive precisely zero weight.",
  "This sparsity is accomplished by replacing softmax with \u03b1- entmax: a differentiable generalization of soft- max that allows low-scoring words to receive precisely zero weight. Moreover, we derive a method to automatically learn the \u03b1 parameter \u2013 which controls the shape and sparsity of \u03b1- entmax \u2013 allowing attention heads to choose between focused or spread-out behavior. Our adaptively sparse Transformer improves inter- pretability and head diversity when compared to softmax Transformers on machine transla- tion datasets. Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more di- verse in their attention distributions than soft- max Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations.",
  "Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more di- verse in their attention distributions than soft- max Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations. 1 Introduction The Transformer architecture (Vaswani et al., 2017) for deep neural networks has quickly risen to promi- nence in NLP through its ef\ufb01ciency and perfor- mance, leading to improvements in the state of the art of Neural Machine Translation (NMT; Junczys- Dowmunt et al., 2018; Ott et al., 2018), as well as inspiring other powerful general-purpose models like BERT (Devlin et al., 2019) and GPT-2 (Rad- ford et al., 2019).",
  "At the heart of the Transformer The quick brown fox jumps over The quick brown fox jumps over The quick brown fox jumps over head 1 head 4 head 3 head 2 Sparse Transformer Adaptive Span  Transformer Adaptively Sparse  Transformer (Ours) Figure 1: Attention distributions of different self- attention heads for the time step of the token \u201cover\u201d, shown to compare our model to other related work. While the sparse Transformer (Child et al., 2019) and the adaptive span Transformer (Sukhbaatar et al., 2019) only attend to words within a contiguous span of the past tokens, our model is not only able to obtain differ- ent and not necessarily contiguous sparsity patterns for each attention head, but is also able to tune its support over which tokens to attend adaptively. lie multi-head attention mechanisms: each word is represented by multiple different weighted aver- ages of its relevant context.",
  "lie multi-head attention mechanisms: each word is represented by multiple different weighted aver- ages of its relevant context. As suggested by recent works on interpreting attention head roles, sepa- rate attention heads may learn to look for various relationships between tokens (Tang et al., 2018; Ra- ganato and Tiedemann, 2018; Mare\u02c7cek and Rosa, 2018; Tenney et al., 2019; Voita et al., 2019). The attention distribution of each head is pre- dicted typically using the softmax normalizing transform. As a result, all context words have non-zero attention weight. Recent work on sin- gle attention architectures suggest that using sparse normalizing transforms in attention mechanisms such as sparsemax \u2013 which can yield exactly zero probabilities for irrelevant words \u2013 may improve performance and interpretability (Malaviya et al., 2018; Deng et al., 2018; Peters et al., 2019). Qual- itative analysis of attention heads (Vaswani et al., 2017, Figure 5) suggests that, depending on what phenomena they capture, heads tend to favor \ufb02atter or more peaked distributions.",
  "Qual- itative analysis of attention heads (Vaswani et al., 2017, Figure 5) suggests that, depending on what phenomena they capture, heads tend to favor \ufb02atter or more peaked distributions. Recent works have proposed sparse Transform- arXiv:1909.00015v2  [cs.CL]  6 Sep 2019",
  "ers (Child et al., 2019) and adaptive span Trans- formers (Sukhbaatar et al., 2019). However, the \u201csparsity\u201d of those models only limits the attention to a contiguous span of past tokens, while in this work we propose a highly adaptive Transformer model that is capable of attending to a sparse set of words that are not necessarily contiguous. Figure 1 shows the relationship of these methods with ours. Our contributions are the following: \u2022 We introduce sparse attention into the Trans- former architecture, showing that it eases inter- pretability and leads to slight accuracy gains. \u2022 We propose an adaptive version of sparse at- tention, where the shape of each attention head is learnable and can vary continuously and dynamically between the dense limit case of softmax and the sparse, piecewise-linear sparsemax case.1 \u2022 We make an extensive analysis of the added interpretability of these models, identifying both crisper examples of attention head behav- ior observed in previous work, as well as novel behaviors unraveled thanks to the sparsity and adaptivity of our proposed model.",
  "2 Background 2.1 The Transformer In NMT, the Transformer (Vaswani et al., 2017) is a sequence-to-sequence (seq2seq) model which maps an input sequence to an output sequence through hierarchical multi-head attention mech- anisms, yielding a dynamic, context-dependent strategy for propagating information within and across sentences. It contrasts with previous seq2seq models, which usually rely either on costly gated recurrent operations (often LSTMs: Bahdanau et al., 2015; Luong et al., 2015) or static convo- lutions (Gehring et al., 2017). Given n query contexts and m sequence items under consideration, attention mechanisms com- pute, for each query, a weighted representation of the items. The particular attention mechanism used in Vaswani et al. (2017) is called scaled dot-product attention, and it is computed in the following way: Att(Q, K, V ) = \u03c0 \u0012QK\u22a4 \u221a d \u0013 V , (1) 1Code and pip package available at https://github. com/deep-spin/entmax.",
  "com/deep-spin/entmax. where Q \u2208Rn\u00d7d contains representations of the queries, K, V \u2208Rm\u00d7d are the keys and values of the items attended over, and d is the dimen- sionality of these representations. The \u03c0 mapping normalizes row-wise using softmax, \u03c0(Z)ij = softmax(zi)j, where softmax(z) = exp(zj) P j\u2032 exp(zj\u2032). (2) In words, the keys are used to compute a relevance score between each item and query. Then, normal- ized attention weights are computed using softmax, and these are used to weight the values of each item at each query context. However, for complex tasks, different parts of a sequence may be relevant in different ways, moti- vating multi-head attention in Transformers.",
  "Then, normal- ized attention weights are computed using softmax, and these are used to weight the values of each item at each query context. However, for complex tasks, different parts of a sequence may be relevant in different ways, moti- vating multi-head attention in Transformers. This is simply the application of Equation 1 in paral- lel H times, each with a different, learned linear transformation that allows specialization: Headi(Q,K,V )=Att(QW Q i , KW K i , V W V i ) (3) In the Transformer, there are three separate multi- head attention mechanisms for distinct purposes: \u2022 Encoder self-attention: builds rich, layered representations of each input word, by attend- ing on the entire input sentence. \u2022 Context attention: selects a representative weighted average of the encodings of the input words, at each time step of the decoder. \u2022 Decoder self-attention: attends over the par- tial output sentence fragment produced so far. Together, these mechanisms enable the contextual- ized \ufb02ow of information between the input sentence and the sequential decoder.",
  "\u2022 Decoder self-attention: attends over the par- tial output sentence fragment produced so far. Together, these mechanisms enable the contextual- ized \ufb02ow of information between the input sentence and the sequential decoder. 2.2 Sparse Attention The softmax mapping (Equation 2) is elementwise proportional to exp, therefore it can never assign a weight of exactly zero. Thus, unnecessary items are still taken into consideration to some extent. Since its output sums to one, this invariably means less weight is assigned to the relevant items, po- tentially harming performance and interpretabil- ity (Jain and Wallace, 2019). This has motivated a line of research on learning networks with sparse mappings (Martins and Astudillo, 2016; Niculae and Blondel, 2017; Louizos et al., 2018; Shao et al.,",
  "2019). We focus on a recently-introduced \ufb02exible family of transformations, \u03b1-entmax (Blondel et al., 2019; Peters et al., 2019), de\ufb01ned as: \u03b1-entmax(z) := argmax p\u2208\u25b3d p\u22a4z + HT \u03b1(p), (4) where \u25b3d := {p \u2208Rd : P i pi = 1} is the prob- ability simplex, and, for \u03b1 \u22651, HT \u03b1 is the Tsallis continuous family of entropies (Tsallis, 1988): HT \u03b1(p):= ( 1 \u03b1(\u03b1\u22121) P j \u0010 pj \u2212p\u03b1 j \u0011 , \u03b1 \u0338= 1, \u2212P j pj log pj, \u03b1 = 1. (5) This family contains the well-known Shannon and Gini entropies, corresponding to the cases \u03b1 = 1 and \u03b1 = 2, respectively. Equation 4 involves a convex optimization sub- problem.",
  "(5) This family contains the well-known Shannon and Gini entropies, corresponding to the cases \u03b1 = 1 and \u03b1 = 2, respectively. Equation 4 involves a convex optimization sub- problem. Using the de\ufb01nition of HT \u03b1, the optimality conditions may be used to derive the following form for the solution (Appendix B.2): \u03b1-entmax(z) = [(\u03b1 \u22121)z \u2212\u03c41] 1/\u03b1\u22121 + , (6) where [\u00b7]+ is the positive part (ReLU) function, 1 denotes the vector of all ones, and \u03c4 \u2013 which acts like a threshold \u2013 is the Lagrange multiplier corresponding to the P i pi = 1 constraint. Properties of \u03b1-entmax. The appeal of \u03b1- entmax for attention rests on the following prop- erties. For \u03b1 = 1 (i.e., when HT \u03b1 becomes the Shannon entropy), it exactly recovers the softmax mapping (We provide a short derivation in Ap- pendix B.3.). For all \u03b1 > 1 it permits sparse solu- tions, in stark contrast to softmax.",
  "For \u03b1 = 1 (i.e., when HT \u03b1 becomes the Shannon entropy), it exactly recovers the softmax mapping (We provide a short derivation in Ap- pendix B.3.). For all \u03b1 > 1 it permits sparse solu- tions, in stark contrast to softmax. In particular, for \u03b1 = 2, it recovers the sparsemax mapping (Martins and Astudillo, 2016), which is piecewise linear. In- between, as \u03b1 increases, the mapping continuously gets sparser as its curvature changes. To compute the value of \u03b1-entmax, one must \ufb01nd the threshold \u03c4 such that the r.h.s. in Equa- tion 6 sums to one. Blondel et al. (2019) propose a general bisection algorithm. Peters et al. (2019) introduce a faster, exact algorithm for \u03b1 = 1.5, and enable using \u03b1-entmax with \ufb01xed \u03b1 within a neu- ral network by showing that the \u03b1-entmax Jacobian w.r.t.",
  "Peters et al. (2019) introduce a faster, exact algorithm for \u03b1 = 1.5, and enable using \u03b1-entmax with \ufb01xed \u03b1 within a neu- ral network by showing that the \u03b1-entmax Jacobian w.r.t. z for p\u22c6= \u03b1-entmax(z) is \u2202\u03b1-entmax(z) \u2202z = diag(s) \u2212 1 P j sj ss\u22a4, where si = ( (p\u22c6 i )2\u2212\u03b1, p\u22c6 i > 0, 0, p\u22c6 i = 0. (7) Our work furthers the study of \u03b1-entmax by providing a derivation of the Jacobian w.r.t. the hyper-parameter \u03b1 (Section 3), thereby allowing the shape and sparsity of the mapping to be learned automatically. This is particularly appealing in the context of multi-head attention mechanisms, where we shall show in Section 5.1 that different heads tend to learn different sparsity behaviors. 3 Adaptively Sparse Transformers with \u03b1-entmax We now propose a novel Transformer architecture wherein we simply replace softmax with \u03b1-entmax in the attention heads.",
  "3 Adaptively Sparse Transformers with \u03b1-entmax We now propose a novel Transformer architecture wherein we simply replace softmax with \u03b1-entmax in the attention heads. Concretely, we replace the row normalization \u03c0 in Equation 1 by \u03c0(Z)ij = \u03b1-entmax(zi)j (8) This change leads to sparse attention weights, as long as \u03b1 > 1; in particular, \u03b1 = 1.5 is a sensible starting point (Peters et al., 2019). Different \u03b1 per head. Unlike LSTM-based seq2seq models, where \u03b1 can be more easily tuned by grid search, in a Transformer, there are many attention heads in multiple layers. Crucial to the power of such models, the different heads capture different linguistic phenomena, some of them iso- lating important words, others spreading out atten- tion across phrases (Vaswani et al., 2017, Figure 5). This motivates using different, adaptive \u03b1 values for each attention head, such that some heads may learn to be sparser, and others may become closer to softmax.",
  "This motivates using different, adaptive \u03b1 values for each attention head, such that some heads may learn to be sparser, and others may become closer to softmax. We propose doing so by treating the \u03b1 values as neural network parameters, optimized via stochastic gradients along with the other weights. Derivatives w.r.t. \u03b1. In order to optimize \u03b1 au- tomatically via gradient methods, we must com- pute the Jacobian of the entmax output w.r.t. \u03b1. Since entmax is de\ufb01ned through an optimization problem, this is non-trivial and cannot be simply handled through automatic differentiation; it falls within the domain of argmin differentiation, an ac- tive research topic in optimization (Gould et al., 2016; Amos and Kolter, 2017). One of our key contributions is the derivation of a closed-form expression for this Jacobian. The next proposition provides such an expression, en- abling entmax layers with adaptive \u03b1. To the best of our knowledge, ours is the \ufb01rst neural network module that can automatically, continuously vary",
  "in shape away from softmax and toward sparse mappings like sparsemax. Proposition 1. Let p\u22c6:= \u03b1-entmax(z) be the so- lution of Equation 4. Denote the distribution \u02dcpi := (p\u22c6 i )2\u2212\u03b1/P j(p\u22c6 j )2\u2212\u03b1 and let hi := \u2212p\u22c6 i log p\u22c6 i . The ith component of the Jacobian g := \u2202\u03b1-entmax(z) \u2202\u03b1 is gi = \uf8f1 \uf8f2 \uf8f3 p\u22c6 i \u2212\u02dcpi (\u03b1\u22121)2 + hi\u2212\u02dcpi P j hj \u03b1\u22121 , \u03b1 > 1, hi log p\u22c6 i \u2212p\u22c6 i P j hj log p\u22c6 j 2 , \u03b1 = 1. (9) The proof uses implicit function differentiation and is given in Appendix C. Proposition 1 provides the remaining missing piece needed for training adaptively sparse Trans- formers. In the following section, we evaluate this strategy on neural machine translation, and analyze the behavior of the learned attention heads.",
  "(9) The proof uses implicit function differentiation and is given in Appendix C. Proposition 1 provides the remaining missing piece needed for training adaptively sparse Trans- formers. In the following section, we evaluate this strategy on neural machine translation, and analyze the behavior of the learned attention heads. 4 Experiments We apply our adaptively sparse Transformers on four machine translation tasks. For comparison, a natural baseline is the standard Transformer ar- chitecture using the softmax transform in its multi- head attention mechanisms. We consider two other model variants in our experiments that make use of different normalizing transformations: \u2022 1.5-entmax: a Transformer with sparse ent- max attention with \ufb01xed \u03b1 = 1.5 for all heads. This is a novel model, since 1.5-entmax had only been proposed for RNN-based NMT models (Peters et al., 2019), but never in Transformers, where attention modules are not just one single component of the seq2seq model but rather an integral part of all of the model components. \u2022 \u03b1-entmax: an adaptive Transformer with sparse entmax attention with a different, learned \u03b1t i,j for each head.",
  "\u2022 \u03b1-entmax: an adaptive Transformer with sparse entmax attention with a different, learned \u03b1t i,j for each head. The adaptive model has an additional scalar pa- rameter per attention head per layer for each of the three attention mechanisms (encoder self-attention, context attention, and decoder self-attention), i.e., \b at i,j \u2208R : i \u2208{1, . . . , L}, j \u2208{1, . . . , H}, t \u2208{enc, ctx, dec} \t , (10) and we set \u03b1t i,j = 1 + sigmoid(at i,j) \u2208]1, 2[. All or some of the \u03b1 values can be tied if desired, but we keep them independent for analysis purposes. Datasets. Our models were trained on 4 machine translation datasets of different training sizes: \u2022 IWSLT 2017 German \u2192English (DE\u0001EN, Cet- tolo et al., 2017): 200K sentence pairs. \u2022 KFTT Japanese \u2192English (JA\u0001EN, Neubig, 2011): 300K sentence pairs.",
  "\u2022 KFTT Japanese \u2192English (JA\u0001EN, Neubig, 2011): 300K sentence pairs. \u2022 WMT 2016 Romanian \u2192English (RO\u0001EN, Bo- jar et al., 2016): 600K sentence pairs. \u2022 WMT 2014 English \u2192German (EN\u0001DE, Bojar et al., 2014): 4.5M sentence pairs. All of these datasets were preprocessed with byte-pair encoding (BPE; Sennrich et al., 2016), using joint segmentations of 32k merge operations. Training. We follow the dimensions of the Transformer-Base model of Vaswani et al. (2017): The number of layers is L = 6 and number of heads is H = 8 in the encoder self-attention, the context attention, and the decoder self-attention. We use a mini-batch size of 8192 tokens and warm up the learning rate linearly until 20k steps, after which it decays according to an inverse square root schedule.",
  "We use a mini-batch size of 8192 tokens and warm up the learning rate linearly until 20k steps, after which it decays according to an inverse square root schedule. All models were trained until conver- gence of validation accuracy, and evaluation was done at each 10k steps for RO\u0001EN and EN\u0001DE and at each 5k steps for DE\u0001EN and JA\u0001EN. The end-to-end computational overhead of our methods, when compared to standard softmax, is relatively small; in training tokens per second, the models using \u03b1-entmax and 1.5-entmax are, respectively, 75% and 90% the speed of the softmax model. Results. We report test set tokenized BLEU (Pa- pineni et al., 2002) results in Table 1. We can see that replacing softmax by entmax does not hurt performance in any of the datasets; indeed, sparse attention Transformers tend to have slightly higher BLEU, but their sparsity leads to a better poten- tial for analysis. In the next section, we make use of this potential by exploring the learned internal mechanics of the self-attention heads.",
  "In the next section, we make use of this potential by exploring the learned internal mechanics of the self-attention heads. 5 Analysis We conduct an analysis for the higher-resource dataset WMT 2014 English \u2192German of the at- tention in the sparse adaptive Transformer model (\u03b1-entmax) at multiple levels: we analyze high- level statistics as well as individual head behavior. Moreover, we make a qualitative analysis of the interpretability capabilities of our models.",
  "activation DE\u0001EN JA\u0001EN RO\u0001EN EN\u0001DE softmax 29.79 21.57 32.70 26.02 1.5-entmax 29.83 22.13 33.10 25.89 \u03b1-entmax 29.90 21.74 32.89 26.93 Table 1: Machine translation tokenized BLEU test results on IWSLT 2017 DE\u0001EN, KFTT JA\u0001EN, WMT 2016 RO\u0001EN and WMT 2014 EN\u0001DE, respectively. 5.1 High-Level Statistics What kind of \u03b1 values are learned? Figure 2 shows the learning trajectories of the \u03b1 parameters of a selected subset of heads. We generally observe a tendency for the randomly-initialized \u03b1 parame- ters to decrease initially, suggesting that softmax- like behavior may be preferable while the model is still very uncertain. After around one thousand steps, some heads change direction and become sparser, perhaps as they become more con\ufb01dent and specialized. This shows that the initialization of \u03b1 does not predetermine its sparsity level or the role the head will have throughout.",
  "After around one thousand steps, some heads change direction and become sparser, perhaps as they become more con\ufb01dent and specialized. This shows that the initialization of \u03b1 does not predetermine its sparsity level or the role the head will have throughout. In particular, head 8 in the encoder self-attention layer 2 \ufb01rst drops to around \u03b1 = 1.3 before becoming one of the sparsest heads, with \u03b1 \u22482. The overall distribution of \u03b1 values at conver- gence can be seen in Figure 3. We can observe that the encoder self-attention blocks learn to con- centrate the \u03b1 values in two modes: a very sparse one around \u03b1 \u21922, and a dense one between soft- max and 1.5-entmax. However, the decoder self and context attention only learn to distribute these parameters in a single mode. We show next that this is re\ufb02ected in the average density of attention weight vectors as well. Attention weight density when translating. For any \u03b1 > 1, it would still be possible for the weight matrices in Equation 3 to learn re-scalings so as to make attention sparser or denser.",
  "Attention weight density when translating. For any \u03b1 > 1, it would still be possible for the weight matrices in Equation 3 to learn re-scalings so as to make attention sparser or denser. To visu- alize the impact of adaptive \u03b1 values, we compare the empirical attention weight density (the aver- age number of tokens receiving non-zero attention) within each module, against sparse Transformers with \ufb01xed \u03b1 = 1.5. Figure 4 shows that, with \ufb01xed \u03b1 = 1.5, heads tend to be sparse and similarly-distributed in all three attention modules.",
  "Figure 4 shows that, with \ufb01xed \u03b1 = 1.5, heads tend to be sparse and similarly-distributed in all three attention modules. With learned \u03b1, there are two notable changes: (i) a prominent mode corre- sponding to fully dense probabilities, showing that our models learn to combine sparse and dense atten- tion, and (ii) a distinction between the encoder self- 0 2000 4000 6000 8000 10000 12000 training steps 1.0 1.2 1.4 1.6 1.8 decoder, layer 1, head 8 encoder, layer 1, head 3 encoder, layer 1, head 4 encoder, layer 2, head 8 encoder, layer 6, head 2 Figure 2: Trajectories of \u03b1 values for a subset of the heads during training. Initialized at random, most heads become denser in the beginning, before converg- ing. This suggests that dense attention may be more bene\ufb01cial while the network is still uncertain, being re- placed by sparse attention afterwards.",
  "Initialized at random, most heads become denser in the beginning, before converg- ing. This suggests that dense attention may be more bene\ufb01cial while the network is still uncertain, being re- placed by sparse attention afterwards. attention \u2013 whose background distribution tends toward extreme sparsity \u2013 and the other two mod- ules, who exhibit more uniform background distri- butions. This suggests that perhaps entirely sparse Transformers are suboptimal. The fact that the decoder seems to prefer denser attention distributions might be attributed to it be- ing auto-regressive, only having access to past to- kens and not the full sentence. We speculate that it might lose too much information if it assigned weights of zero to too many tokens in the self- attention, since there are fewer tokens to attend to in the \ufb01rst place. Teasing this down into separate layers, Figure 5 shows the average (sorted) density of each head for each layer.",
  "Teasing this down into separate layers, Figure 5 shows the average (sorted) density of each head for each layer. We observe that \u03b1-entmax is able to learn different sparsity patterns at each layer, lead- ing to more variance in individual head behavior, to clearly-identi\ufb01ed dense and sparse heads, and over- all to different tendencies compared to the \ufb01xed case of \u03b1 = 1.5. Head diversity. To measure the overall disagree- ment between attention heads, as a measure of head",
  "0 10 20 Encoder Self-Attention 0 10 20 Context Attention 1.0 1.2 1.4 1.6 1.8 2.0 0 10 20 Decoder Self-Attention Figure 3: Distribution of learned \u03b1 values per attention block. While the encoder self-attention has a bimodal distribution of values of \u03b1, the decoder self-attention and context attention have a single mode. diversity, we use the following generalization of the Jensen-Shannon divergence: JS = HS \uf8eb \uf8ed1 H H X j=1 pj \uf8f6 \uf8f8\u22121 H H X j=1 HS(pj) (11) where pj is the vector of attention weights as- signed by head j to each word in the sequence, and HS is the Shannon entropy, base-adjusted based on the dimension of p such that JS \u22641. We average this measure over the entire validation set. The higher this metric is, the more the heads are taking different roles in the model. Figure 6 shows that both sparse Transformer variants show more diversity than the traditional softmax one.",
  "We average this measure over the entire validation set. The higher this metric is, the more the heads are taking different roles in the model. Figure 6 shows that both sparse Transformer variants show more diversity than the traditional softmax one. Interestingly, diversity seems to peak in the middle layers of the encoder self-attention and context attention, while this is not the case for the decoder self-attention. The statistics shown in this section can be found for the other language pairs in Appendix A. 5.2 Identifying Head Specializations Previous work pointed out some speci\ufb01c roles played by different heads in the softmax Trans- former model (Voita et al., 2018; Tang et al., 2018; Voita et al., 2019).",
  "5.2 Identifying Head Specializations Previous work pointed out some speci\ufb01c roles played by different heads in the softmax Trans- former model (Voita et al., 2018; Tang et al., 2018; Voita et al., 2019). Identifying the specialization of a head can be done by observing the type of tokens 0.0 0.5 1.0 0 10k 30k 50k Encoder Self-Attention 1.5-entmax 0.0 0.5 1.0 -entmax 0.0 0.5 1.0 0 10k 30k 50k Context Attention 0.0 0.5 1.0 0.0 0.5 1.0 density 0 10k 30k 50k Decoder Self-Attention 0.0 0.5 1.0 density Figure 4: Distribution of attention densities (average number of tokens receiving non-zero attention weight) for all attention heads and all validation sentences.",
  "When compared to 1.5-entmax, \u03b1-entmax distributes the sparsity in a more uniform manner, with a clear mode at fully dense attentions, corresponding to the heads with low \u03b1. In the softmax case, this distribution would lead to a single bar with density 1. or sequences that the head often assigns most of its attention weight; this is facilitated by sparsity. Positional heads. One particular type of head, as noted by Voita et al. (2019), is the positional head. These heads tend to focus their attention on either the previous or next token in the sequence, thus obtaining representations of the neighborhood of the current time step. In Figure 7, we show atten- tion plots for such heads, found for each of the studied models. The sparsity of our models allows these heads to be more con\ufb01dent in their represen- tations, by assigning the whole probability distribu- tion to a single token in the sequence. Concretely, we may measure a positional head\u2019s con\ufb01dence as the average attention weight assigned to the pre- vious token.",
  "Concretely, we may measure a positional head\u2019s con\ufb01dence as the average attention weight assigned to the pre- vious token. The softmax model has three heads for position \u22121, with median con\ufb01dence 93.5%. The 1.5-entmax model also has three heads for this position, with median con\ufb01dence 94.4%. The adaptive model has four heads, with median con- \ufb01dences 95.9%, the lowest-con\ufb01dence head being dense with \u03b1 = 1.18, while the highest-con\ufb01dence head being sparse (\u03b1 = 1.91).",
  "0.0 0.5 1.0 Encoder Self-Attention fixed  = 1.5 learned  0.0 0.5 1.0 Context Attention 1 2 3 4 5 6 Layers 0.0 0.5 1.0 Decoder Self-Attention 1 2 3 4 5 6 Layers Figure 5: Head density per layer for \ufb01xed and learned \u03b1. Each line corresponds to an attention head; lower values mean that that attention head is sparser. Learned \u03b1 has higher variance. For position +1, the models each dedicate one head, with con\ufb01dence around 95%, slightly higher for entmax. The adaptive model sets \u03b1 = 1.96 for this head. BPE-merging head. Due to the sparsity of our models, we are able to identify other head special- izations, easily identifying which heads should be further analysed. In Figure 8 we show one such head where the \u03b1 value is particularly high (in the encoder, layer 1, head 4 depicted in Figure 2).",
  "In Figure 8 we show one such head where the \u03b1 value is particularly high (in the encoder, layer 1, head 4 depicted in Figure 2). We found that this head most often looks at the cur- rent time step with high con\ufb01dence, making it a positional head with offset 0. However, this head often spreads weight sparsely over 2-3 neighbor- ing tokens, when the tokens are part of the same BPE cluster2 or hyphenated words. As this head is in the \ufb01rst layer, it provides a useful service to the higher layers by combining information evenly within some BPE clusters. For each BPE cluster or cluster of hyphenated words, we computed a score between 0 and 1 that corresponds to the maximum attention mass as- signed by any token to the rest of the tokens inside the cluster in order to quantify the BPE-merging 2BPE-segmented words are denoted by \u223cin the \ufb01gures.",
  "0.4 0.5 Encoder Self-Attention softmax 1.5-entmax -entmax 0.20 0.25 0.30 0.35 Context Attention 1 2 3 4 5 6 Layers 0.25 0.30 0.35 Decoder Self-Attention Figure 6: Jensen-Shannon Divergence between heads at each layer. Measures the disagreement between heads: the higher the value, the more the heads are dis- agreeing with each other in terms of where to attend. Models using sparse entmax have more diverse atten- tion than the softmax baseline. capabilities of these heads.3 There are not any at- tention heads in the softmax model that are able to obtain a score over 80%, while for 1.5-entmax and \u03b1-entmax there are two heads in each (83.3% and 85.6% for 1.5-entmax and 88.5% and 89.8% for \u03b1-entmax). Interrogation head.",
  "Interrogation head. On the other hand, in Fig- ure 9 we show a head for which our adaptively sparse model chose an \u03b1 close to 1, making it closer to softmax (also shown in encoder, layer 1, head 3 depicted in Figure 2). We observe that this head assigns a high probability to question marks at the end of the sentence in time steps where the current token is interrogative, thus making it an interrogation-detecting head. We also observe this type of heads in the other models, which we also depict in Figure 9. The average attention weight placed on the question mark when the current to- ken is an interrogative word is 98.5% for softmax, 97.0% for 1.5-entmax, and 99.5% for \u03b1-entmax. Furthermore, we can examine sentences where some tendentially sparse heads become less so, thus identifying sources of ambiguity where the head 3If the cluster has size 1, the score is the weight the token assigns to itself.",
  "weweren 'tfaraway last season . softmax we weren 't far away last season . weweren 'tfaraway last season . 1.5-entmax weweren 'tfaraway last season . -entmax Figure 7: Self-attention from the most con\ufb01dently previous-position head in each model. The learned pa- rameter in the \u03b1-entmax model is \u03b1 = 1.91. Quanti- tatively more con\ufb01dent, visual inspection con\ufb01rms that the adaptive head behaves more consistently. rules forblo~ wing upbal~ lo~ ons , forbananas and acir~ cus rules for blo~ wing up bal~ lo~ ons , for bananas and a cir~ cus one - two - three - four . one - two - three - four . are you not confir~ ming this with what you have stated ? are you not confir~ ming this with what you have stated ? this could come back to ha~ unt them . this could come back to ha~ unt them .",
  "one - two - three - four . are you not confir~ ming this with what you have stated ? are you not confir~ ming this with what you have stated ? this could come back to ha~ unt them . this could come back to ha~ unt them . Figure 8: BPE-merging head (\u03b1 = 1.91) discovered in the \u03b1-entmax model. Found in the \ufb01rst encoder layer, this head learns to discover some subword units and combine their information, leaving most words in- tact. It places 99.09% of its probability mass within the same BPE cluster as the current token: more than any head in any other model. is less con\ufb01dent in its prediction. An example is shown in Figure 10 where sparsity in the same head differs for sentences of similar length. 6 Related Work Sparse attention.",
  "is less con\ufb01dent in its prediction. An example is shown in Figure 10 where sparsity in the same head differs for sentences of similar length. 6 Related Work Sparse attention. Prior work has developed sparse attention mechanisms, including appli- cations to NMT (Martins and Astudillo, 2016; Malaviya et al., 2018; Niculae and Blondel, 2017; Shao et al., 2019; Maruf et al., 2019). Peters et al. (2019) introduced the entmax function this work builds upon. In their work, there is a single atten- tion mechanism which is controlled by a \ufb01xed \u03b1. In contrast, this is the \ufb01rst work to allow such atten- however , what isAr~ man~ i Polo ? softmax however , what is Ar~ man~ i Polo ? however , what isAr~ man~ i Polo ? 1.5-entmax however , what isAr~ man~ i Polo ? -entmax you wonder what more people expect . softmax you wonder what more people expect . you wonder what more people expect .",
  "however , what isAr~ man~ i Polo ? 1.5-entmax however , what isAr~ man~ i Polo ? -entmax you wonder what more people expect . softmax you wonder what more people expect . you wonder what more people expect . 1.5-entmax you wonder what more people expect . -entmax Figure 9: Interrogation-detecting heads in the three models. The top sentence is interrogative while the bottom one is declarative but includes the interrogative word \u201cwhat\u201d. In the top example, these interrogation heads assign a high probability to the question mark in the time step of the interrogative word (with \u226597.0% probability), while in the bottom example since there is no question mark, the same head does not assign a high probability to the last token in the sentence dur- ing the interrogative word time step. Surprisingly, this head prefers a low \u03b1 = 1.05, as can be seen from the dense weights. This allows the head to identify the noun phrase \u201cArmani Polo\u201d better. tion mappings to dynamically adapt their curvature and sparsity, by automatically adjusting the contin- uous \u03b1 parameter.",
  "This allows the head to identify the noun phrase \u201cArmani Polo\u201d better. tion mappings to dynamically adapt their curvature and sparsity, by automatically adjusting the contin- uous \u03b1 parameter. We also provide the \ufb01rst results using sparse attention in a Transformer model. Fixed sparsity patterns. Recent research im- proves the scalability of Transformer-like networks through static, \ufb01xed sparsity patterns (Child et al., 2019; Wu et al., 2019). Our adaptively-sparse Transformer can dynamically select a sparsity pat- tern that \ufb01nds relevant words regardless of their po- sition (e.g., Figure 9). Moreover, the two strategies could be combined. In a concurrent line of research, Sukhbaatar et al. (2019) propose an adaptive atten- tion span for Transformer language models. While their work has each head learn a different contigu- ous span of context tokens to attend to, our work \ufb01nds different sparsity patterns in the same span.",
  "(2019) propose an adaptive atten- tion span for Transformer language models. While their work has each head learn a different contigu- ous span of context tokens to attend to, our work \ufb01nds different sparsity patterns in the same span. Interestingly, some of their \ufb01ndings mirror ours \u2013 we found that attention heads in the last layers tend to be denser on average when compared to the ones in the \ufb01rst layers, while their work has found that lower layers tend to have a shorter attention span compared to higher layers.",
  "here , this layer is thin . here , this layer is thin . which symptoms indicate asex~ ually transmitted disease ? which symptoms indicate a sex~ ually transmitted disease ? Figure 10: Example of two sentences of similar length where the same head (\u03b1 = 1.33) exhibits different spar- sity. The longer phrase in the example on the right \u201ca sexually transmitted disease\u201d is handled with higher con\ufb01dence, leading to more sparsity. Transformer interpretability. The original Transformer paper (Vaswani et al., 2017) shows attention visualizations, from which some specula- tion can be made of the roles the several attention heads have. Mare\u02c7cek and Rosa (2018) study the syntactic abilities of the Transformer self-attention, while Raganato and Tiedemann (2018) extract dependency relations from the attention weights. Tenney et al. (2019) \ufb01nd that the self-attentions in BERT (Devlin et al., 2019) follow a sequence of processes that resembles a classical NLP pipeline. Regarding redundancy of heads, Voita et al.",
  "Tenney et al. (2019) \ufb01nd that the self-attentions in BERT (Devlin et al., 2019) follow a sequence of processes that resembles a classical NLP pipeline. Regarding redundancy of heads, Voita et al. (2019) develop a method that is able to prune heads of the multi-head attention module and make an empirical study of the role that each head has in self-attention (positional, syntactic and rare words). Li et al. (2018) also aim to reduce head redundancy by adding a regularization term to the loss that maximizes head disagreement and obtain improved results. While not considering Transformer attentions, Jain and Wallace (2019) show that traditional attention mechanisms do not necessarily improve interpretability since softmax attention is vulnerable to an adversarial attack leading to wildly different model predictions for the same attention weights. Sparse attention may mitigate these issues; however, our work focuses mostly on a more mechanical aspect of interpretation by analyzing head behavior, rather than on explanations for predictions. 7 Conclusion and Future Work We contribute a novel strategy for adaptively sparse attention, and, in particular, for adaptively sparse Transformers.",
  "Sparse attention may mitigate these issues; however, our work focuses mostly on a more mechanical aspect of interpretation by analyzing head behavior, rather than on explanations for predictions. 7 Conclusion and Future Work We contribute a novel strategy for adaptively sparse attention, and, in particular, for adaptively sparse Transformers. We present the \ufb01rst empirical analy- sis of Transformers with sparse attention mappings (i.e., entmax), showing potential in both translation accuracy as well as in model interpretability. In particular, we analyzed how the attention heads in the proposed adaptively sparse Trans- former can specialize more and with higher con- \ufb01dence. Our adaptivity strategy relies only on gradient-based optimization, side-stepping costly per-head hyper-parameter searches. Further speed- ups are possible by leveraging more parallelism in the bisection algorithm for computing \u03b1-entmax. Finally, some of the automatically-learned be- haviors of our adaptively sparse Transformers \u2013 for instance, the near-deterministic positional heads or the subword joining head \u2013 may provide new ideas for designing static variations of the Transformer.",
  "Finally, some of the automatically-learned be- haviors of our adaptively sparse Transformers \u2013 for instance, the near-deterministic positional heads or the subword joining head \u2013 may provide new ideas for designing static variations of the Transformer. Acknowledgments This work was supported by the European Re- search Council (ERC StG DeepSPIN 758969), and by the Fundac\u00b8\u02dcao para a Ci\u02c6encia e Tecnolo- gia through contracts UID/EEA/50008/2019 and CMUPERI/TIC/0046/2014 (GoLocal). We are grateful to Ben Peters for the \u03b1-entmax code and Erick Fonseca, Marcos Treviso, Pedro Martins, and Tsvetomila Mihaylova for insightful group discus- sion. We thank Mathieu Blondel for the idea to learn \u03b1. We would also like to thank the anony- mous reviewers for their helpful feedback. References Brandon Amos and J. Zico Kolter. 2017. OptNet: Differentiable optimization as a layer in neural net- works. In Proc. ICML.",
  "References Brandon Amos and J. Zico Kolter. 2017. OptNet: Differentiable optimization as a layer in neural net- works. In Proc. ICML. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate. In Proc. ICLR. Mathieu Blondel, Andr\u00b4e FT Martins, and Vlad Nicu- lae. 2019. Learning classi\ufb01ers with Fenchel-Young losses: Generalized entropies, margins, and algo- rithms. In Proc. AISTATS. Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al. 2014. Findings of the 2014 workshop on statistical machine translation. In Proc. Workshop on Statistical Machine Translation.",
  "2014. Findings of the 2014 workshop on statistical machine translation. In Proc. Workshop on Statistical Machine Translation. Ondrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, An- tonio Jimeno Yepes, Philipp Koehn, Varvara Lo- gacheva, Christof Monz, et al. 2016. Findings of the 2016 conference on machine translation. In Proc. WMT.",
  "M Cettolo, M Federico, L Bentivogli, J Niehues, S St\u00a8uker, K Sudoh, K Yoshino, and C Federmann. 2017. Overview of the IWSLT 2017 evaluation cam- paign. In Proc. IWSLT. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse Transformers. preprint arXiv:1904.10509. Frank H Clarke. 1990. Optimization and Nonsmooth Analysis. SIAM. Yuntian Deng, Yoon Kim, Justin Chiu, Demi Guo, and Alexander Rush. 2018. Latent alignment and varia- tional attention. In Proc. NeurIPS. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proc. NAACL-HLT.",
  "In Proc. NeurIPS. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proc. NAACL-HLT. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. 2017. Convolutional sequence to sequence learning. In Proc. ICML. Stephen Gould, Basura Fernando, Anoop Cherian, Pe- ter Anderson, Rodrigo Santa Cruz, and Edison Guo. 2016. On differentiating parameterized argmin and argmax problems with application to bi-level opti- mization. preprint arXiv:1607.05447. Michael Held, Philip Wolfe, and Harlan P Crowder. 1974. Validation of subgradient optimization. Math- ematical Programming, 6(1):62\u201388. Sarthak Jain and Byron C. Wallace. 2019. Attention is not explanation. In Proc. NAACL-HLT.",
  "1974. Validation of subgradient optimization. Math- ematical Programming, 6(1):62\u201388. Sarthak Jain and Byron C. Wallace. 2019. Attention is not explanation. In Proc. NAACL-HLT. Marcin Junczys-Dowmunt, Kenneth Hea\ufb01eld, Hieu Hoang, Roman Grundkiewicz, and Anthony Aue. 2018. Marian: Cost-effective high-quality neural machine translation in C++. In Proc. WNMT. Jian Li, Zhaopeng Tu, Baosong Yang, Michael R Lyu, and Tong Zhang. 2018. Multi-Head Attention with Disagreement Regularization. In Proc. EMNLP. Christos Louizos, Max Welling, and Diederik P Kingma. 2018. Learning sparse neural networks through L0 regularization. Proc. ICLR. Minh-Thang Luong, Hieu Pham, and Christopher D Manning. 2015. Effective approaches to attention- based neural machine translation. In Proc. EMNLP.",
  "2018. Learning sparse neural networks through L0 regularization. Proc. ICLR. Minh-Thang Luong, Hieu Pham, and Christopher D Manning. 2015. Effective approaches to attention- based neural machine translation. In Proc. EMNLP. Chaitanya Malaviya, Pedro Ferreira, and Andr\u00b4e FT Martins. 2018. Sparse and constrained attention for neural machine translation. In Proc. ACL. David Mare\u02c7cek and Rudolf Rosa. 2018. Extract- ing syntactic trees from Transformer encoder self- attentions. In Proc. BlackboxNLP. Andr\u00b4e FT Martins and Ram\u00b4on Fernandez Astudillo. 2016. From softmax to sparsemax: A sparse model of attention and multi-label classi\ufb01cation. In Proc. of ICML. Sameen Maruf, Andr\u00b4e FT Martins, and Gholam- reza Haffari. 2019. Selective attention for context-aware neural machine translation. preprint arXiv:1903.08788. Graham Neubig. 2011.",
  "Sameen Maruf, Andr\u00b4e FT Martins, and Gholam- reza Haffari. 2019. Selective attention for context-aware neural machine translation. preprint arXiv:1903.08788. Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt. Vlad Niculae and Mathieu Blondel. 2017. A regular- ized framework for sparse and structured neural at- tention. In Proc. NeurIPS. Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. 2018. Scaling neural machine trans- lation. In Proc. WMT. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. BLEU: a method for automatic eval- uation of machine translation. In Proc. ACL. Ben Peters, Vlad Niculae, and Andr\u00b4e FT Martins. 2019. Sparse sequence-to-sequence models. In Proc. ACL.",
  "2002. BLEU: a method for automatic eval- uation of machine translation. In Proc. ACL. Ben Peters, Vlad Niculae, and Andr\u00b4e FT Martins. 2019. Sparse sequence-to-sequence models. In Proc. ACL. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Lan- guage models are unsupervised multitask learners. preprint. Alessandro Raganato and J\u00a8org Tiedemann. 2018. An analysis of encoder representations in Transformer- based machine translation. In Proc. BlackboxNLP. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proc. ACL. Wenqi Shao, Tianjian Meng, Jingyu Li, Ruimao Zhang, Yudian Li, Xiaogang Wang, and Ping Luo. 2019. SSN: Learning sparse switchable normalization via SparsestMax. In Proc.",
  "In Proc. ACL. Wenqi Shao, Tianjian Meng, Jingyu Li, Ruimao Zhang, Yudian Li, Xiaogang Wang, and Ping Luo. 2019. SSN: Learning sparse switchable normalization via SparsestMax. In Proc. CVPR. Sainbayar Sukhbaatar, Edouard Grave, Piotr Bo- janowski, and Armand Joulin. 2019. Adaptive At- tention Span in Transformers. In Proc. ACL. Gongbo Tang, Mathias M\u00a8uller, Annette Rios, and Rico Sennrich. 2018. Why self-attention? A targeted evaluation of neural machine translation architec- tures. In Proc. EMNLP. Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT rediscovers the classical NLP pipeline. In Proc. ACL. Constantino Tsallis. 1988. Possible generalization of Boltzmann-Gibbs statistics. Journal of Statistical Physics, 52:479\u2013487.",
  "2019. BERT rediscovers the classical NLP pipeline. In Proc. ACL. Constantino Tsallis. 1988. Possible generalization of Boltzmann-Gibbs statistics. Journal of Statistical Physics, 52:479\u2013487. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proc. NeurIPS. Elena Voita, Pavel Serdyukov, Rico Sennrich, and Ivan Titov. 2018. Context-aware neural machine transla- tion learns anaphora resolution. In Proc. ACL.",
  "Elena Voita, David Talbot, Fedor Moiseev, Rico Sen- nrich, and Ivan Titov. 2019. Analyzing multi-head self-attention: Specialized heads do the heavy lift- ing, the rest can be pruned. In Proc. ACL. Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. 2019. Pay less atten- tion with lightweight and dynamic convolutions. In Proc. ICLR.",
  "Supplementary Material A High-Level Statistics Analysis of Other Language Pairs 0 10 20 30 Encoder Self-Attention 0 10 20 30 Context Attention 1.0 1.2 1.4 1.6 1.8 2.0 0 10 20 30 Decoder Self-Attention (a) WMT 2016 RO\u0001EN. 0 10 20 30 Encoder Self-Attention 0 10 20 30 Context Attention 1.0 1.2 1.4 1.6 1.8 2.0 0 10 20 30 Decoder Self-Attention (b) KFTT JA\u0001EN. 0 10 20 Encoder Self-Attention 0 10 20 Context Attention 1.0 1.2 1.4 1.6 1.8 2.0 0 10 20 Decoder Self-Attention (c) WMT 2014 EN\u0001DE.",
  "0 10 20 Encoder Self-Attention 0 10 20 Context Attention 1.0 1.2 1.4 1.6 1.8 2.0 0 10 20 Decoder Self-Attention (c) WMT 2014 EN\u0001DE. 0 10 20 30 Encoder Self-Attention 0 10 20 30 Context Attention 1.0 1.2 1.4 1.6 1.8 2.0 0 10 20 30 Decoder Self-Attention (d) IWSLT 2017 DE\u0001EN. Figure 11: Histograms of \u03b1 values.",
  "0.0 0.5 1.0 0 10k 30k 50k Encoder Self-Attention 1.5-entmax 0.0 0.5 1.0 -entmax 0.0 0.5 1.0 0 10k 30k 50k Context Attention 0.0 0.5 1.0 0.0 0.5 1.0 density 0 10k 30k 50k Decoder Self-Attention 0.0 0.5 1.0 density (a) WMT 2016 RO\u0001EN.",
  "0.0 0.5 1.0 0 10k 30k 50k Encoder Self-Attention 1.5-entmax 0.0 0.5 1.0 -entmax 0.0 0.5 1.0 0 10k 30k 50k Context Attention 0.0 0.5 1.0 0.0 0.5 1.0 density 0 10k 30k 50k Decoder Self-Attention 0.0 0.5 1.0 density (b) KFTT JA\u0001EN.",
  "0.0 0.5 1.0 0 10k 30k 50k Encoder Self-Attention 1.5-entmax 0.0 0.5 1.0 -entmax 0.0 0.5 1.0 0 10k 30k 50k Context Attention 0.0 0.5 1.0 0.0 0.5 1.0 density 0 10k 30k 50k Decoder Self-Attention 0.0 0.5 1.0 density (c) WMT 2014 EN\u0001DE.",
  "0.0 0.5 1.0 0 50k 100k 150k Encoder Self-Attention 1.5-entmax 0.0 0.5 1.0 -entmax 0.0 0.5 1.0 0 50k 100k 150k Context Attention 0.0 0.5 1.0 0.0 0.5 1.0 density 0 50k 100k 150k Decoder Self-Attention 0.0 0.5 1.0 density (d) IWSLT 2017 DE\u0001EN. Figure 12: Histograms of head densities.",
  "0.4 0.5 Encoder Self-Attention softmax 1.5-entmax -entmax 0.3 0.4 0.5 Context Attention 1 2 3 4 5 6 Layers 0.3 0.4 Decoder Self-Attention (a) WMT 2016 RO\u0001EN. 0.4 0.5 Encoder Self-Attention softmax 1.5-entmax -entmax 0.2 0.3 0.4 0.5 Context Attention 1 2 3 4 5 6 Layers 0.25 0.30 0.35 0.40 Decoder Self-Attention (b) KFTT JA\u0001EN. 0.4 0.5 Encoder Self-Attention softmax 1.5-entmax -entmax 0.20 0.25 0.30 0.35 Context Attention 1 2 3 4 5 6 Layers 0.25 0.30 0.35 Decoder Self-Attention (c) WMT 2014 EN\u0001DE.",
  "0.4 0.5 Encoder Self-Attention softmax 1.5-entmax -entmax 0.3 0.4 Context Attention 1 2 3 4 5 6 Layers 0.25 0.30 0.35 Decoder Self-Attention (d) IWSLT 2017 DE\u0001EN. Figure 13: Jensen-Shannon divergence over layers.",
  "0.0 0.5 1.0 Encoder Self-Attention fixed  = 1.5 learned  0.0 0.5 1.0 Context Attention 1 2 3 4 5 6 Layers 0.0 0.5 1.0 Decoder Self-Attention 1 2 3 4 5 6 Layers (a) WMT 2016 RO\u0001EN. 0.0 0.5 1.0 Encoder Self-Attention fixed  = 1.5 learned  0.0 0.5 1.0 Context Attention 1 2 3 4 5 6 Layers 0.0 0.5 1.0 Decoder Self-Attention 1 2 3 4 5 6 Layers (b) KFTT JA\u0001EN.",
  "0.0 0.5 1.0 Encoder Self-Attention fixed  = 1.5 learned  0.0 0.5 1.0 Context Attention 1 2 3 4 5 6 Layers 0.0 0.5 1.0 Decoder Self-Attention 1 2 3 4 5 6 Layers (c) WMT 2014 EN\u0001DE. 0.0 0.5 1.0 Encoder Self-Attention fixed  = 1.5 learned  0.0 0.5 1.0 Context Attention 1 2 3 4 5 6 Layers 0.0 0.5 1.0 Decoder Self-Attention 1 2 3 4 5 6 Layers (d) IWSLT 2017 DE\u0001EN. Figure 14: Head densities over layers.",
  "B Background B.1 Regularized Fenchel-Young prediction functions De\ufb01nition 1 (Blondel et al. 2019). Let \u2126: \u25b3d \u2192R \u222a{\u221e} be a strictly convex regularization function. We de\ufb01ne the prediction function \u03c0\u2126as \u03c0\u2126(z) = argmax p\u2208\u25b3d \u0000p\u22a4z \u2212\u2126(p) \u0001 (12) B.2 Characterizing the \u03b1-entmax mapping Lemma 1 (Peters et al. 2019). For any z, there exists a unique \u03c4 \u22c6such that \u03b1-entmax(z) = [(\u03b1 \u22121)z \u2212\u03c4 \u22c61] 1/\u03b1\u22121 + . (13) Proof: From the de\ufb01nition of \u03b1-entmax, \u03b1-entmax(z) := argmax p\u2208\u25b3d p\u22a4z + HT \u03b1(p), (14) we may easily identify it with a regularized prediction function (Def. 1): \u03b1-entmax(z) \u2261\u03c0\u2212HT\u03b1(z).",
  "1): \u03b1-entmax(z) \u2261\u03c0\u2212HT\u03b1(z). We \ufb01rst note that for all p \u2208\u25b3d, \u2212(\u03b1 \u22121)HT \u03b1(p) = 1 \u03b1 d X i=1 p\u03b1 i + const. (15) From the constant invariance and scaling properties of \u03c0\u2126(Blondel et al., 2019, Proposition 1, items 4\u20135), \u03c0\u2212HT\u03b1(z) = \u03c0\u2126((\u03b1 \u22121)z), with \u2126(p) = d X j=1 g(pj), g(t) = t\u03b1 \u03b1 . Using (Blondel et al., 2019, Proposition 5), noting that g\u2032(t) = t\u03b1\u22121 and (g\u2032)\u22121(u) = u 1/\u03b1\u22121, yields \u03c0\u2126(z) = [z \u2212\u03c4 \u22c61] 1/\u03b1\u22121 + , and therefore \u03b1-entmax(z) = [(\u03b1 \u22121)z \u2212\u03c4 \u22c61] 1/\u03b1\u22121 + .",
  "(16) Since HT \u03b1 is strictly convex on the simplex, \u03b1-entmax has a unique solution p\u22c6. Equation 16 implicitly de\ufb01nes a one-to-one mapping between p\u22c6and \u03c4 \u22c6as long as p\u22c6\u2208\u25b3, therefore \u03c4 \u22c6is also unique. B.3 Connections to softmax and sparsemax The Euclidean projection onto the simplex, sometimes referred to, in the context of neural attention, as sparsemax (Martins and Astudillo, 2016), is de\ufb01ned as sparsemax(z) := argmin p\u2208\u25b3 \u2225p \u2212z\u22252 2. (17) The solution can be characterized through the unique threshold \u03c4 such that P i sparsemax(z)i = 1 and (Held et al., 1974) sparsemax(z) = [z \u2212\u03c41]+ . (18)",
  "Thus, each coordinate of the sparsemax solution is a piecewise-linear function. Visibly, this expression is recovered when setting \u03b1 = 2 in the \u03b1-entmax expression (Equation 21); for other values of \u03b1, the exponent induces curvature. On the other hand, the well-known softmax is usually de\ufb01ned through the expression softmax(z)i := exp(zi) P j exp(zj), (19) which can be shown to be the unique solution of the optimization problem softmax(z)i = argmax p\u2208\u25b3 p\u22a4z + HS(p), (20) where HS(p) := \u2212P i pi log pi is the Shannon entropy. Indeed, setting the gradient to 0 yields the condition log pi = zj \u2212\u03bdi \u2212\u03c4 \u22121, where \u03c4 and \u03bd > 0 are Lagrange multipliers for the simplex constraints P i pi = 1 and pi \u22650, respectively. Since the l.h.s. is only \ufb01nite for pi > 0, we must have \u03bdi = 0 for all i, by complementary slackness. Thus, the solution must have the form pi = exp(zi)/Z, yielding Equation 19.",
  "Since the l.h.s. is only \ufb01nite for pi > 0, we must have \u03bdi = 0 for all i, by complementary slackness. Thus, the solution must have the form pi = exp(zi)/Z, yielding Equation 19. C Jacobian of \u03b1-entmax w.r.t. the shape parameter \u03b1: Proof of Proposition 1 Recall that the entmax transformation is de\ufb01ned as: \u03b1-entmax(z) := argmax p\u2208\u25b3d p\u22a4z + HT \u03b1(p), (21) where \u03b1 \u22651 and HT \u03b1 is the Tsallis entropy, HT \u03b1(p):= ( 1 \u03b1(\u03b1\u22121) P j \u0010 pj \u2212p\u03b1 j \u0011 , \u03b1 \u0338= 1, HS(p), \u03b1 = 1, (22) and HS(p) := \u2212P j pj log pj is the Shannon entropy. In this section, we derive the Jacobian of entmax with respect to the scalar parameter \u03b1. C.1 General case of \u03b1 > 1 From the KKT conditions associated with the optimization problem in Eq.",
  "In this section, we derive the Jacobian of entmax with respect to the scalar parameter \u03b1. C.1 General case of \u03b1 > 1 From the KKT conditions associated with the optimization problem in Eq. 21, we have that the solution p\u22c6has the following form, coordinate-wise: p\u22c6 i = [(\u03b1 \u22121)(zi \u2212\u03c4 \u22c6)]1/(\u03b1\u22121) + , (23) where \u03c4 \u22c6is a scalar Lagrange multiplier that ensures that p\u22c6normalizes to 1, i.e., it is de\ufb01ned implicitly by the condition: X i [(\u03b1 \u22121)(zi \u2212\u03c4 \u22c6)]1/(\u03b1\u22121) + = 1. (24) For general values of \u03b1, Eq. 24 lacks a closed form solution. This makes the computation of the Jacobian \u2202\u03b1-entmax(z) \u2202\u03b1 (25) non-trivial. Fortunately, we can use the technique of implicit differentiation to obtain this Jacobian.",
  "(24) For general values of \u03b1, Eq. 24 lacks a closed form solution. This makes the computation of the Jacobian \u2202\u03b1-entmax(z) \u2202\u03b1 (25) non-trivial. Fortunately, we can use the technique of implicit differentiation to obtain this Jacobian. The Jacobian exists almost everywhere, and the expressions we derive expressions yield a generalized Jacobian (Clarke, 1990) at any non-differentiable points that may occur for certain (\u03b1, z) pairs. We begin",
  "by noting that \u2202p\u22c6 i \u2202\u03b1 = 0 if p\u22c6 i = 0, because increasing \u03b1 keeps sparse coordinates sparse.4 Therefore we need to worry only about coordinates that are in the support of p\u22c6. We will assume hereafter that the ith coordinate of p\u22c6is non-zero.",
  "We will assume hereafter that the ith coordinate of p\u22c6is non-zero. We have: \u2202p\u22c6 i \u2202\u03b1 = \u2202 \u2202\u03b1[(\u03b1 \u22121)(zi \u2212\u03c4 \u22c6)] 1 \u03b1\u22121 = \u2202 \u2202\u03b1 exp \u0014 1 \u03b1 \u22121 log[(\u03b1 \u22121)(zi \u2212\u03c4 \u22c6)] \u0015 = p\u22c6 i \u2202 \u2202\u03b1 \u0014 1 \u03b1 \u22121 log[(\u03b1 \u22121)(zi \u2212\u03c4 \u22c6)] \u0015 = p\u22c6 i (\u03b1 \u22121)2 \" \u2202 \u2202\u03b1[(\u03b1 \u22121)(zi \u2212\u03c4 \u22c6)] zi \u2212\u03c4 \u22c6 \u2212log[(\u03b1 \u22121)(zi \u2212\u03c4 \u22c6)] # = p\u22c6 i (\u03b1 \u22121)2 \" zi \u2212\u03c4 \u22c6\u2212(\u03b1 \u22121)\u2202\u03c4 \u22c6 \u2202\u03b1 zi \u2212\u03c4 \u22c6 \u2212log[(\u03b1 \u22121)(zi \u2212\u03c4 \u22c6)] # = p\u22c6 i (\u03b1 \u22121)2 \u0014 1 \u2212\u03b1 \u22121 zi \u2212\u03c4 \u22c6 \u2202\u03c4 \u22c6 \u2202\u03b1 \u2212log[(\u03b1 \u22121)(zi \u2212\u03c4 \u22c6)] \u0015 .",
  "(26) We can see that this Jacobian depends on \u2202\u03c4 \u22c6 \u2202\u03b1 , which we now compute using implicit differentiation. Let S = {i : p\u22c6 i > 0}). By differentiating both sides of Eq. 24, re-using some of the steps in Eq. 26, and recalling Eq. 23,",
  "Let S = {i : p\u22c6 i > 0}). By differentiating both sides of Eq. 24, re-using some of the steps in Eq. 26, and recalling Eq. 23, we get 0 = X i\u2208S \u2202 \u2202\u03b1[(\u03b1 \u22121)(zi \u2212\u03c4 \u22c6)]1/(\u03b1\u22121) = X i\u2208S p\u22c6 i (\u03b1 \u22121)2 \u0014 1 \u2212\u03b1 \u22121 zi \u2212\u03c4 \u22c6 \u2202\u03c4 \u22c6 \u2202\u03b1 \u2212log[(\u03b1 \u22121)(zi \u2212\u03c4 \u22c6)] \u0015 = 1 (\u03b1 \u22121)2 \u2212\u2202\u03c4 \u22c6 \u2202\u03b1 X i\u2208S p\u22c6 i (\u03b1 \u22121)(zi \u2212\u03c4 \u22c6) \u2212 X i\u2208S p\u22c6 i (\u03b1 \u22121)2 log[(\u03b1 \u22121)(zi \u2212\u03c4 \u22c6)] = 1 (\u03b1 \u22121)2 \u2212\u2202\u03c4 \u22c6 \u2202\u03b1 X i (p\u22c6 i )2\u2212\u03b1 \u2212 X i p\u22c6 i \u03b1 \u22121 log p\u22c6 i = 1 (\u03b1 \u22121)2 \u2212\u2202\u03c4 \u22c6 \u2202\u03b1 X i (p\u22c6 i )2\u2212\u03b1 + HS(p\u2217) \u03b1 \u22121 ,",
  "(27) from which we obtain: \u2202\u03c4 \u22c6 \u2202\u03b1 = 1 (\u03b1\u22121)2 + HS(p\u22c6) \u03b1\u22121 P i(p\u22c6 i )2\u2212\u03b1 . (28) Finally, plugging Eq. 28 into Eq.",
  "(27) from which we obtain: \u2202\u03c4 \u22c6 \u2202\u03b1 = 1 (\u03b1\u22121)2 + HS(p\u22c6) \u03b1\u22121 P i(p\u22c6 i )2\u2212\u03b1 . (28) Finally, plugging Eq. 28 into Eq. 26, we get: \u2202p\u22c6 i \u2202\u03b1 = p\u22c6 i (\u03b1 \u22121)2 \u0014 1 \u2212 1 (p\u22c6 i )\u03b1\u22121 \u2202\u03c4 \u22c6 \u2202\u03b1 \u2212(\u03b1 \u22121) log p\u22c6 i \u0015 = p\u22c6 i (\u03b1 \u22121)2 \uf8ee \uf8f01 \u2212 1 (p\u22c6 i )\u03b1\u22121 1 (\u03b1\u22121)2 + HS(p\u22c6) \u03b1\u22121 P i(p\u22c6 i )2\u2212\u03b1 \u2212(\u03b1 \u22121) log p\u22c6 i \uf8f9 \uf8fb = p\u22c6 i \u2212\u02dcpi(\u03b1) (\u03b1 \u22121)2 \u2212p\u22c6 i log p\u22c6 i + \u02dcpi(\u03b1)HS(p\u22c6) \u03b1 \u22121 , (29) 4This follows from the margin property of HT \u03b1 (Blondel et al., 2019).",
  "where we denote by \u02dcpi(\u03b1) = (p\u22c6 i )2\u2212\u03b1 P j(p\u22c6 j)2\u2212\u03b1 . (30) The distribution \u02dcp(\u03b1) can be interpreted as a \u201cskewed\u201d distribution obtained from p\u22c6, which appears in the Jacobian of \u03b1-entmax(z) w.r.t. z as well (Peters et al., 2019). C.2 Solving the indetermination for \u03b1 = 1 We can write Eq. 29 as \u2202p\u22c6 i \u2202\u03b1 = p\u22c6 i \u2212\u02dcpi(\u03b1) \u2212(\u03b1 \u22121)(p\u22c6 i log p\u22c6 i + \u02dcpi(\u03b1)HS(p\u22c6)) (\u03b1 \u22121)2 . (31) When \u03b1 \u21921+, we have \u02dcp(\u03b1) \u2192p\u22c6, which leads to a 0 0 indetermination. To solve this indetermination, we will need to apply L\u2019H\u02c6opital\u2019s rule twice.",
  "(31) When \u03b1 \u21921+, we have \u02dcp(\u03b1) \u2192p\u22c6, which leads to a 0 0 indetermination. To solve this indetermination, we will need to apply L\u2019H\u02c6opital\u2019s rule twice. Let us \ufb01rst compute the derivative of \u02dcpi(\u03b1) with respect to \u03b1. We have \u2202 \u2202\u03b1(p\u22c6 i )2\u2212\u03b1 = \u2212(p\u22c6 i )2\u2212\u03b1 log p\u22c6 i , (32) therefore \u2202 \u2202\u03b1 \u02dcpi(\u03b1) = \u2202 \u2202\u03b1 (p\u22c6 i )2\u2212\u03b1 P j(p\u22c6 j)2\u2212\u03b1 = \u2212(p\u22c6 i )2\u2212\u03b1 log p\u22c6 i P j(p\u22c6 j)2\u2212\u03b1 + (p\u22c6 i )2\u2212\u03b1 P j(p\u22c6 j)2\u2212\u03b1 log p\u22c6 j \u0010P j(p\u22c6 j)2\u2212\u03b1 \u00112 = \u2212\u02dcpi(\u03b1) log p\u22c6 i + \u02dcpi(\u03b1) X j \u02dcpj(\u03b1) log p\u22c6 j.",
  "(33) Differentiating the numerator and denominator in Eq. 31, we get: \u2202p\u22c6 i \u2202\u03b1 = lim \u03b1\u21921+ (1 + (\u03b1 \u22121)HS(p\u22c6))\u02dcpi(\u03b1)(log p\u22c6 i \u2212P j \u02dcpj(\u03b1) log p\u22c6 j) \u2212p\u22c6 i log p\u22c6 i \u2212\u02dcpi(\u03b1)HS(p\u22c6) 2(\u03b1 \u22121) = A + B, (34) with A = lim \u03b1\u21921+ HS(p\u22c6)\u02dcpi(\u03b1)(log p\u22c6 i \u2212P j \u02dcpj(\u03b1) log p\u22c6 j)HS(p\u22c6) 2 = HS(p\u22c6)p\u22c6 i log p\u22c6 i + p\u22c6 i (HS(p\u22c6))2 2 ,",
  "(35) and B = lim \u03b1\u21921+ \u02dcpi(\u03b1)(log p\u22c6 i \u2212P j \u02dcpj(\u03b1) log p\u22c6 j) \u2212p\u22c6 i log p\u22c6 i \u2212\u02dcpi(\u03b1)HS(p\u22c6) 2(\u03b1 \u22121) . (36) When \u03b1 \u21921+, B becomes again a 0 0 indetermination, which we can solve by applying again L\u2019H\u02c6opital\u2019s",
  "rule. Differentiating the numerator and denominator in Eq. 36: B = 1 2 lim \u03b1\u21921+ \uf8f1 \uf8f2 \uf8f3\u02dcpi(\u03b1) log p\u22c6 i \uf8eb \uf8edX j \u02dcpj(\u03b1) log p\u22c6 j \u2212log p\u22c6 i \uf8f6 \uf8f8 \u2212\u02dcpi(\u03b1) \uf8eb \uf8edX j \u02dcpj(\u03b1) log p\u22c6 j \u2212log p\u22c6 i \uf8f6 \uf8f8 \uf8eb \uf8edX j \u02dcpj(\u03b1) log p\u22c6 j + HS(p\u22c6) \uf8f6 \uf8f8 \u2212\u02dcpi(\u03b1) X j \u02dcpj(\u03b1) log p\u22c6 j  X k \u02dcpk(\u03b1) log p\u22c6 k \u2212log p\u22c6 j !\uf8fc \uf8fd \uf8fe = \u2212p\u22c6 i log p\u22c6 i (HS(p\u22c6) + log p\u22c6 i ) + p\u22c6 i P j p\u22c6 j log p\u22c6",
  "k \u2212log p\u22c6 j !\uf8fc \uf8fd \uf8fe = \u2212p\u22c6 i log p\u22c6 i (HS(p\u22c6) + log p\u22c6 i ) + p\u22c6 i P j p\u22c6 j log p\u22c6 j(HS(p\u22c6) + log p\u22c6 j) 2 = \u2212HS(p\u22c6)p\u22c6 i log p\u22c6 i \u2212p\u22c6 i (HS(p\u22c6))2 \u2212p\u22c6 i log2 p\u22c6 i + p\u22c6 i P j p\u22c6 j log2 p\u22c6 j 2 . (37) Finally, summing Eq. 35 and Eq. 37, we get \u2202p\u22c6 i \u2202\u03b1 \f\f\f\f \u03b1=1 = \u2212p\u22c6 i log2 p\u22c6 i + p\u22c6 i P j p\u22c6 j log2 p\u22c6 j 2 .",
  "37, we get \u2202p\u22c6 i \u2202\u03b1 \f\f\f\f \u03b1=1 = \u2212p\u22c6 i log2 p\u22c6 i + p\u22c6 i P j p\u22c6 j log2 p\u22c6 j 2 . (38) C.3 Summary To sum up, we have the following expression for the Jacobian of \u03b1-entmax with respect to \u03b1: \u2202p\u22c6 i \u2202\u03b1 = \uf8f1 \uf8f2 \uf8f3 p\u22c6 i \u2212\u02dcpi(\u03b1) (\u03b1\u22121)2 \u2212p\u22c6 i log p\u22c6 i +\u02dcpi(\u03b1)HS(p\u22c6) \u03b1\u22121 , for \u03b1 > 1 \u2212p\u22c6 i log2 p\u22c6 i +p\u22c6 i P j p\u22c6 j log2 p\u22c6 j 2 , for \u03b1 = 1. (39)"
]