{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping Jian Ni and Radu Florian IBM Research AI 1101 Kitchawan Road, Yorktown Heights, NY 10598, USA {nij, raduf}@us.ibm.com Abstract Relation extraction (RE) seeks to detect and classify semantic relationships between enti- ties, which provides useful information for many NLP applications. Since the state-of- the-art RE models require large amounts of manually annotated data and language-speci\ufb01c resources to achieve high accuracy, it is very challenging to transfer an RE model of a resource-rich language to a resource-poor lan- guage. In this paper, we propose a new ap- proach for cross-lingual RE model transfer based on bilingual word embedding mapping. It projects word embeddings from a target lan- guage to a source language, so that a well- trained source-language neural network RE model can be directly applied to the target language. Experiment results show that the proposed approach achieves very good perfor- mance for a number of target languages on both in-house and open datasets, using a small bilingual dictionary with only 1K word pairs.",
      "Experiment results show that the proposed approach achieves very good perfor- mance for a number of target languages on both in-house and open datasets, using a small bilingual dictionary with only 1K word pairs. 1 Introduction Relation extraction (RE) is an important informa- tion extraction task that seeks to detect and clas- sify semantic relationships between entities like persons, organizations, geo-political entities, lo- cations, and events. It provides useful informa- tion for many NLP applications such as knowl- edge base construction, text mining and question answering. For example, the entity Washington, D.C. and the entity United States have a CapitalOf relationship, and extraction of such relationships can help answer questions like \u201cWhat is the capi- tal city of the United States?\u201d Traditional RE models (e.g., Zelenko et al. (2003); Kambhatla (2004); Li and Ji (2014)) re- quire careful feature engineering to derive and combine various lexical, syntactic and seman- tic features. Recently, neural network RE mod- els (e.g., Zeng et al. (2014); dos Santos et al.",
      "Recently, neural network RE mod- els (e.g., Zeng et al. (2014); dos Santos et al. (2015); Miwa and Bansal (2016); Nguyen and Grishman (2016)) have become very successful. These models employ a certain level of auto- matic feature learning by using word embeddings, which signi\ufb01cantly simpli\ufb01es the feature engineer- ing task while considerably improving the accu- racy, achieving the state-of-the-art performance for relation extraction. All the above RE models are supervised ma- chine learning models that need to be trained with large amounts of manually annotated RE data to achieve high accuracy. However, anno- tating RE data by human is expensive and time- consuming, and can be quite dif\ufb01cult for a new language. Moreover, most RE models require language-speci\ufb01c resources such as dependency parsers and part-of-speech (POS) taggers, which also makes it very challenging to transfer an RE model of a resource-rich language to a resource- poor language.",
      "Moreover, most RE models require language-speci\ufb01c resources such as dependency parsers and part-of-speech (POS) taggers, which also makes it very challenging to transfer an RE model of a resource-rich language to a resource- poor language. There are a few existing weakly supervised cross-lingual RE approaches that require no hu- man annotation in the target languages, e.g., Kim et al. (2010); Kim and Lee (2012); Faruqui and Kumar (2015); Zou et al. (2018). However, the ex- isting approaches require aligned parallel corpora or machine translation systems, which may not be readily available in practice. In this paper, we make the following contribu- tions to cross-lingual RE: \u2022 We propose a new approach for direct cross- lingual RE model transfer based on bilin- gual word embedding mapping. It projects word embeddings from a target language to a source language (e.g., English), so that a well-trained source-language RE model can be directly applied to the target language, with no manually annotated RE data needed for the target language.",
      "It projects word embeddings from a target language to a source language (e.g., English), so that a well-trained source-language RE model can be directly applied to the target language, with no manually annotated RE data needed for the target language. arXiv:1911.00069v1  [cs.CL]  31 Oct 2019",
      "\u2022 We design a deep neural network archi- tecture for the source-language (English) RE model that uses word embeddings and generic language-independent features as the input. The English RE model achieves the- state-of-the-art performance without using language-speci\ufb01c resources. \u2022 We conduct extensive experiments which show that the proposed approach achieves very good performance (up to 79% of the accuracy of the supervised target-language RE model) for a number of target languages on both in-house and the ACE05 datasets (Walker et al., 2006), using a small bilin- gual dictionary with only 1K word pairs. To the best of our knowledge, this is the \ufb01rst work that includes empirical studies for cross-lingual RE on several languages across a variety of language families, without using aligned parallel corpora or machine transla- tion systems. We organize the paper as follows. In Section 2 we provide an overview of our approach. In Sec- tion 3 we describe how to build monolingual word embeddings and learn a linear mapping between two languages. In Section 4 we present a neural network architecture for the source-language (En- glish).",
      "In Section 2 we provide an overview of our approach. In Sec- tion 3 we describe how to build monolingual word embeddings and learn a linear mapping between two languages. In Section 4 we present a neural network architecture for the source-language (En- glish). In Section 5 we evaluate the performance of the proposed approach for a number of target languages. We discuss related work in Section 6 and conclude the paper in Section 7. 2 Overview of the Approach We summarize the main steps of our neural cross- lingual RE model transfer approach as follows. 1. Build word embeddings for the source lan- guage and the target language separately us- ing monolingual data. 2. Learn a linear mapping that projects the target-language word embeddings into the source-language embedding space using a small bilingual dictionary. 3. Build a neural network source-language RE model that uses word embeddings and generic language-independent features as the input. 4.",
      "2. Learn a linear mapping that projects the target-language word embeddings into the source-language embedding space using a small bilingual dictionary. 3. Build a neural network source-language RE model that uses word embeddings and generic language-independent features as the input. 4. For a target-language sentence and any two entities in it, project the word embeddings of the words in the sentence to the source- language word embeddings using the linear mapping, and then apply the source-language RE model on the projected word embeddings to classify the relationship between the two entities. An example is shown in Figure 1, where the target language is Portuguese and the source language is English. We will describe each component of our ap- proach in the subsequent sections. 3 Cross-Lingual Word Embeddings In recent years, vector representations of words, known as word embeddings, become ubiquitous for many NLP applications (Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014). A monolingual word embedding model maps words in the vocabulary V of a language to real- valued vectors in Rd\u00d71.",
      "A monolingual word embedding model maps words in the vocabulary V of a language to real- valued vectors in Rd\u00d71. The dimension of the vec- tor space d is normally much smaller than the size of the vocabulary V = |V| for ef\ufb01cient represen- tation. It also aims to capture semantic similarities between the words based on their distributional properties in large samples of monolingual data. Cross-lingual word embedding models try to build word embeddings across multiple languages (Upadhyay et al., 2016; Ruder et al., 2017). One approach builds monolingual word embeddings separately and then maps them to the same vec- tor space using a bilingual dictionary (Mikolov et al., 2013b; Faruqui and Dyer, 2014). Another approach builds multilingual word embeddings in a shared vector space simultaneously, by gener- ating mixed language corpora using aligned sen- tences (Luong et al., 2015; Gouws et al., 2015).",
      "Another approach builds multilingual word embeddings in a shared vector space simultaneously, by gener- ating mixed language corpora using aligned sen- tences (Luong et al., 2015; Gouws et al., 2015). In this paper, we adopt the technique in (Mikolov et al., 2013b) because it only requires a small bilingual dictionary of aligned word pairs, and does not require parallel corpora of aligned sentences which could be more dif\ufb01cult to obtain. 3.1 Monolingual Word Embeddings To build monolingual word embeddings for the source and target languages, we use a variant of the Continuous Bag-of-Words (CBOW) word2vec model (Mikolov et al., 2013a). The standard CBOW model has two matrices, the input word matrix \u02dcX \u2208Rd\u00d7V and the output word matrix X \u2208Rd\u00d7V . For the ith word wi in V, let e(wi) \u2208RV \u00d71 be a one-hot vector with 1 at in- dex i and 0s at other indexes, so that \u02dcxi = \u02dcXe(wi)",
      "Figure 1: Neural cross-lingual relation extraction based on bilingual word embedding mapping - target language: Portuguese, source language: English. (the ith column of \u02dcX) is the input vector repre- sentation of word wi, and xi = Xe(wi) (the ith column of X) is the output vector representation (i.e., word embedding) of word wi.",
      "(the ith column of \u02dcX) is the input vector repre- sentation of word wi, and xi = Xe(wi) (the ith column of X) is the output vector representation (i.e., word embedding) of word wi. Given a sequence of training words w1, w2, ..., wN, the CBOW model seeks to predict a target word wt using a window of 2c context words surrounding wt, by maximizing the following objective function: L = 1 N N X t=1 log P(wt|wt\u2212c, ..., wt\u22121, wt+1, ..., wt+c) The conditional probability is calculated using a softmax function: P(wt|wt\u2212c, ..., wt+c) = exp(xT t \u02dcxc(t)) PV i=1 exp(xT i \u02dcxc(t)) (1) where xt = Xe(wt) is the output vector represen- tation of word wt, and \u02dcxc(t) = X \u2212c\u2264j\u2264c,j\u0338=0 \u02dcXe(wt+j) (2) is the sum of the input vector representations of the context words.",
      "In our variant of the CBOW model, we use a separate input word matrix \u02dcXj for a context word at position j, \u2212c \u2264j \u2264c, j \u0338= 0. In addition, we employ weights that decay with the distances of the context words to the target word. Under these modi\ufb01cations, we have \u02dcxnew c(t) = X \u2212c\u2264j\u2264c,j\u0338=0 1 |j| \u02dcXje(wt+j) (3) We use the variant to build monolingual word embeddings because experiments on named entity recognition and word similarity tasks showed this variant leads to small improvements over the stan- dard CBOW model (Ni et al., 2017). 3.2 Bilingual Word Embedding Mapping Mikolov et al. (2013b) observed that word em- beddings of different languages often have similar geometric arrangements, and suggested to learn a linear mapping between the vector spaces.",
      "3.2 Bilingual Word Embedding Mapping Mikolov et al. (2013b) observed that word em- beddings of different languages often have similar geometric arrangements, and suggested to learn a linear mapping between the vector spaces. Let D be a bilingual dictionary with aligned word pairs (wi, vi)i=1,...,D between a source lan- guage s and a target language t, where wi is a source-language word and vi is the translation of wi in the target language. Let xi \u2208Rd\u00d71 be the word embedding of the source-language word wi, yi \u2208Rd\u00d71 be the word embedding of the target- language word vi. We \ufb01nd a linear mapping (matrix) Mt\u2192s such that Mt\u2192syi approximates xi, by solving the fol-",
      "lowing least squares problem using the dictionary as the training set: Mt\u2192s = arg min M\u2208Rd\u00d7d D X i=1 ||xi \u2212Myi||2 (4) Using Mt\u2192s, for any target-language word v with word embedding y, we can project it into the source-language embedding space as Mt\u2192sy. 3.2.1 Length Normalization and Orthogonal Transformation To ensure that all the training instances in the dic- tionary D contribute equally to the optimization objective in (4) and to preserve vector norms after projection, we have tried length normalization and orthogonal transformation for learning the bilin- gual mapping as in (Xing et al., 2015; Artetxe et al., 2016; Smith et al., 2017).",
      "First, we normalize the source-language and target-language word embeddings to be unit vec- tors: x\u2032 = x ||x|| for each source-language word em- bedding x, and y\u2032 = y ||y|| for each target-language word embedding y. Next, we add an orthogonality constraint to (4) such that M is an orthogonal matrix, i.e., MTM = I where I denotes the identity matrix: MO t\u2192s = arg min M\u2208Rd\u00d7d,MTM=I D X i=1 ||x\u2032 i \u2212My\u2032 i||2 (5) MO t\u2192s can be computed using singular-value de- composition (SVD). 3.2.2 Semi-Supervised and Unsupervised Mappings The mapping learned in (4) or (5) requires a seed dictionary. To relax this requirement, Artetxe et al. (2017) proposed a self-learning procedure that can be combined with a dictionary-based mapping technique. Starting with a small seed dictionary, the procedure iteratively 1) learns a mapping us- ing the current dictionary; and 2) computes a new dictionary using the learned mapping. Artetxe et al.",
      "Starting with a small seed dictionary, the procedure iteratively 1) learns a mapping us- ing the current dictionary; and 2) computes a new dictionary using the learned mapping. Artetxe et al. (2018) proposed an unsupervised method to learn the bilingual mapping without us- ing a seed dictionary. The method \ufb01rst uses a heuristic to build an initial dictionary that aligns the vocabularies of two languages, and then ap- plies a robust self-learning procedure to itera- tively improve the mapping. Another unsuper- vised method based on adversarial training was proposed in Conneau et al. (2018). We compare the performance of different map- pings for cross-lingual RE model transfer in Sec- tion 5.3.2. 4 Neural Network RE Models For any two entities in a sentence, an RE model determines whether these two entities have a rela- tionship, and if yes, classi\ufb01es the relationship into one of the pre-de\ufb01ned relation types. We focus on neural network RE models since these models achieve the state-of-the-art performance for rela- tion extraction.",
      "We focus on neural network RE models since these models achieve the state-of-the-art performance for rela- tion extraction. Most importantly, neural network RE models use word embeddings as the input, which are amenable to cross-lingual model trans- fer via cross-lingual word embeddings. In this pa- per, we use English as the source language. Our neural network architecture has four lay- ers. The \ufb01rst layer is the embedding layer which maps input words in a sentence to word embed- dings. The second layer is a context layer which transforms the word embeddings to context-aware vector representations using a recurrent or convo- lutional neural network layer. The third layer is a summarization layer which summarizes the vec- tors in a sentence by grouping and pooling. The \ufb01nal layer is the output layer which returns the classi\ufb01cation label for the relation type. 4.1 Embedding Layer For an English sentence with n words s = (w1, w2, ..., wn), the embedding layer maps each word wt to a real-valued vector (word embedding) xt \u2208Rd\u00d71 using the English word embedding model (Section 3.1).",
      "4.1 Embedding Layer For an English sentence with n words s = (w1, w2, ..., wn), the embedding layer maps each word wt to a real-valued vector (word embedding) xt \u2208Rd\u00d71 using the English word embedding model (Section 3.1). In addition, for each entity m in the sentence, the embedding layer maps its entity type to a real-valued vector (entity label em- bedding) lm \u2208Rdm\u00d71 (initialized randomly). In our experiments we use d = 300 and dm = 50. 4.2 Context Layer Given the word embeddings xt\u2019s of the words in the sentence, the context layer tries to build a sentence-context-aware vector representation for each word. We consider two types of neural net- work layers that aim to achieve this. 4.2.1 Bi-LSTM Context Layer The \ufb01rst type of context layer is based on Long Short-Term Memory (LSTM) type recurrent neu- ral networks (Hochreiter and Schmidhuber, 1997;",
      "Graves and Schmidhuber, 2005). Recurrent neural networks (RNNs) are a class of neural networks that operate on sequential data such as sequences of words. LSTM networks are a type of RNNs that have been invented to better capture long-range dependencies in sequential data. We pass the word embeddings xt\u2019s to a for- ward and a backward LSTM layer. A forward or backward LSTM layer consists of a set of recur- rently connected blocks known as memory blocks.",
      "We pass the word embeddings xt\u2019s to a for- ward and a backward LSTM layer. A forward or backward LSTM layer consists of a set of recur- rently connected blocks known as memory blocks. The memory block at the t-th word in the forward LSTM layer contains a memory cell \u2212\u2192c t and three gates: an input gate \u2212\u2192i t, a forget gate \u2212\u2192f t and an output gate \u2212\u2192o t (\u2212\u2192\u00b7 indicates the forward direc- tion), which are updated as follows: \u2212\u2192i t = \u03c3 \u0000\u2212\u2192 W ixt + \u2212\u2192 U i \u2212\u2192 h t\u22121 + \u2212\u2192 b i \u0001 \u2212\u2192f t = \u03c3 \u0000\u2212\u2192 W fxt + \u2212\u2192 U f \u2212\u2192 h t\u22121 + \u2212\u2192 b f \u0001 \u2212\u2192o t = \u03c3 \u0000\u2212\u2192 W oxt + \u2212\u2192 U o \u2212\u2192 h t\u22121 + \u2212\u2192 b o \u0001 \u2212\u2192c t = \u2212\u2192f t \u2299\u2212\u2192c t\u22121 + \u2212\u2192i t \u2299tanh \u0000\u2212\u2192 W cxt + \u2212\u2192 U c \u2212\u2192 h t\u22121 + \u2212\u2192 b c \u0001 \u2212\u2192 h t = \u2212\u2192o t \u2299tanh(\u2212\u2192c t) (6) where \u03c3 is the element-wise sigmoid function and \u2299is the element-wise multiplication.",
      "The hidden state vector \u2212\u2192 h t in the forward LSTM layer incorporates information from the left (past) tokens of wt in the sentence. Similarly, we can compute the hidden state vector \u2190\u2212 h t in the backward LSTM layer, which incorporates infor- mation from the right (future) tokens of wt in the sentence. The concatenation of the two vectors ht = [\u2212\u2192 h t, \u2190\u2212 h t] is a good representation of the word wt with both left and right contextual infor- mation in the sentence. 4.2.2 CNN Context Layer The second type of context layer is based on Con- volutional Neural Networks (CNNs) (Zeng et al., 2014; dos Santos et al., 2015), which applies convolution-like operation on successive windows of size k around each word in the sentence. Let zt = [xt\u2212(k\u22121)/2, ..., xt+(k\u22121)/2] be the concate- nation of k word embeddings around wt.",
      "Let zt = [xt\u2212(k\u22121)/2, ..., xt+(k\u22121)/2] be the concate- nation of k word embeddings around wt. The con- volutional layer computes a hidden state vector ht = tanh(Wzt + b) (7) for each word wt, where W is a weight matrix and b is a bias vector, and tanh(\u00b7) is the element-wise hyperbolic tangent function. 4.3 Summarization Layer After the context layer, the sentence (w1, w2, ..., wn) is represented by (h1, ...., hn). Suppose m1 = (wb1, .., we1) and m2 = (wb2, .., we2) are two entities in the sentence where m1 is on the left of m2 (i.e., e1 < b2). As different sentences and entities may have various lengths, the summarization layer tries to build a \ufb01xed-length vector that best summarizes the representations of the sentence and the two entities for relation type classi\ufb01cation.",
      "As different sentences and entities may have various lengths, the summarization layer tries to build a \ufb01xed-length vector that best summarizes the representations of the sentence and the two entities for relation type classi\ufb01cation. We divide the hidden state vectors ht\u2019s into 5 groups: \u2022 G1 = {h1, .., hb1\u22121} includes vectors that are left to the \ufb01rst entity m1. \u2022 G2 = {hb1, .., he1} includes vectors that are in the \ufb01rst entity m1. \u2022 G3 = {he1+1, .., hb2\u22121} includes vectors that are between the two entities. \u2022 G4 = {hb2, .., he2} includes vectors that are in the second entity m2. \u2022 G5 = {he2+1, .., hn} includes vectors that are right to the second entity m2. We perform element-wise max pooling among the vectors in each group: hGi(j) = max h\u2208Gi h(j), 1 \u2264j \u2264dh, 1 \u2264i \u22645 (8) where dh is the dimension of the hidden state vec- tors.",
      "We perform element-wise max pooling among the vectors in each group: hGi(j) = max h\u2208Gi h(j), 1 \u2264j \u2264dh, 1 \u2264i \u22645 (8) where dh is the dimension of the hidden state vec- tors. Concatenating the hGi\u2019s we get a \ufb01xed- length vector hs = [hG1, ..., hG5]. 4.4 Output Layer The output layer receives inputs from the previous layers (the summarization vector hs, the entity la- bel embeddings lm1 and lm2 for the two entities under consideration) and returns a probability dis- tribution over the relation type labels: p = softmax \u0000Wshs+Wm1lm1 +Wm2lm2 +bo \u0001 (9) 4.5 Cross-Lingual RE Model Transfer Given the word embeddings of a sequence of words in a target language t, (y1, ..., yn), we project them into the English embedding space by applying the linear mapping Mt\u2192s learned in Sec- tion 3.2: (Mt\u2192sy1, Mt\u2192sy2, ..., Mt\u2192syn). The",
      "neural network English RE model is then applied on the projected word embeddings and the entity label embeddings (which are language indepen- dent) to perform relationship classi\ufb01cation. Note that our models do not use language- speci\ufb01c resources such as dependency parsers or POS taggers because these resources might not be readily available for a target language. Also our models do not use precise word position features since word positions in sentences can vary a lot across languages. 5 Experiments In this section, we evaluate the performance of the proposed cross-lingual RE approach on both in-house dataset and the ACE (Automatic Con- tent Extraction) 2005 multilingual dataset (Walker et al., 2006). 5.1 Datasets Our in-house dataset includes manually anno- tated RE data for 6 languages: English, Ger- man, Spanish, Italian, Japanese and Portuguese. It de\ufb01nes 56 entity types (e.g., Person, Organi- zation, Geo-Political Entity, Location, Facility, Time, Event Violence, etc.) and 53 relation types between the entities (e.g., AgentOf, LocatedAt, PartOf, TimeOf, AffectedBy, etc.).",
      "and 53 relation types between the entities (e.g., AgentOf, LocatedAt, PartOf, TimeOf, AffectedBy, etc.). The ACE05 dataset includes manually anno- tated RE data for 3 languages: English, Arabic and Chinese. It de\ufb01nes 7 entity types (Person, Orga- nization, Geo-Political Entity, Location, Facility, Weapon, Vehicle) and 6 relation types between the entities (Agent-Artifact, General-Af\ufb01liation, ORG- Af\ufb01liation, Part-Whole, Personal-Social, Physi- cal). For both datasets, we create a class label \u201cO\u201d to denote that the two entities under consideration do not have a relationship belonging to one of the relation types of interest. 5.2 Source (English) RE Model Performance We build 3 neural network English RE models un- der the architecture described in Section 4: \u2022 The \ufb01rst neural network RE model does not have a context layer and the word embed- dings are directly passed to the summariza- tion layer. We call it Pass-Through for short.",
      "We call it Pass-Through for short. \u2022 The second neural network RE model has a Bi-LSTM context layer. We call it Bi-LSTM for short. Model F1 FCM (S) (Gormley et al., 2015) 55.06 Hybrid FCM (E) (Gormley et al., 2015) 58.26 BIDIRECT (S) (Nguyen and Grishman, 2016) 57.73 VOTE-BW (E) (Nguyen and Grishman, 2016) 60.60 Pass-Through (S) 54.99 Bi-LSTM (S) 58.92 CNN (S) 57.91 Table 1: Comparison with the state-of-the-art RE mod- els on the ACE05 English data (S: Single Model; E: Ensemble Model).",
      "In-House Training Dev Test English (Source) 1137 140 140 German (Target) 280 35 35 Spanish (Target) 451 55 55 Italian (Target) 322 40 40 Japanese (Target) 396 50 50 Portuguese (Target) 390 50 50 ACE05 Training Dev Test English (Source) 479 60 60 Arabic (Target) 323 40 40 Chinese (Target) 507 63 63 Table 2: Number of documents in the training/dev/test sets of the in-house and ACE05 datasets. \u2022 The third neural network model has a CNN context layer with a window size 3. We call it CNN for short. First we compare our neural network English RE models with the state-of-the-art RE models on the ACE05 English data. The ACE05 English data can be divided to 6 different domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and webblogs (wl).",
      "The ACE05 English data can be divided to 6 different domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and webblogs (wl). We apply the same data split in (Plank and Moschitti, 2013; Gormley et al., 2015; Nguyen and Grishman, 2016), which uses news (the union of bn and nw) as the training set, a half of bc as the development set and the remaining data as the test set. We learn the model parameters using Adam (Kingma and Ba, 2015). We apply dropout (Sri- vastava et al., 2014) to the hidden layers to reduce over\ufb01tting. The development set is used for tuning the model hyperparameters and for early stopping. In Table 1 we compare our models with the best models in (Gormley et al., 2015) and (Nguyen and Grishman, 2016).",
      "The development set is used for tuning the model hyperparameters and for early stopping. In Table 1 we compare our models with the best models in (Gormley et al., 2015) and (Nguyen and Grishman, 2016). Our Bi-LSTM model out- performs the best model (single or ensemble) in (Gormley et al., 2015) and the best single model in (Nguyen and Grishman, 2016), without using any language-speci\ufb01c resources such as dependency",
      "Figure 2: Cross-lingual RE performance (F1 score) vs. dictionary size (number of bilingual word pairs for learning the mapping (4)) under the Bi-LSTM English RE model on the target-language development data. Model In-House ACE05 P R F1 P R F1 Pass-Through 60.8 63.1 61.9 59.4 55.1 57.2 Bi-LSTM 66.5 67.7 67.1 65.1 65.8 65.5 CNN 63.4 68.8 66.0 61.7 67.1 64.3 Table 3: Performance of the supervised English RE models on the in-house and ACE05 English test data. parsers. While the data split in the previous works was motivated by domain adaptation, the focus of this paper is on cross-lingual model transfer, and hence we apply a random data split as follows. For the source language English and each target language, we randomly select 80% of the data as the training set, 10% as the development set, and keep the re- maining 10% as the test set.",
      "For the source language English and each target language, we randomly select 80% of the data as the training set, 10% as the development set, and keep the re- maining 10% as the test set. The sizes of the sets are summarized in Table 2. We report the Precision, Recall and F1 score of the 3 neural network English RE models in Table 3. Note that adding an additional context layer with either Bi-LSTM or CNN signi\ufb01cantly im- proves the performance of our English RE model, compared with the simple Pass-Through model. Therefore, we will focus on the Bi-LSTM model and the CNN model in the subsequent experi- ments. 5.3 Cross-Lingual RE Performance We apply the English RE models to the 7 target languages across a variety of language families. 5.3.1 Dictionary Size The bilingual dictionary includes the most fre- quent target-language words and their translations in English.",
      "5.3 Cross-Lingual RE Performance We apply the English RE models to the 7 target languages across a variety of language families. 5.3.1 Dictionary Size The bilingual dictionary includes the most fre- quent target-language words and their translations in English. To determine how many word pairs are needed to learn an effective bilingual word embed- ding mapping for cross-lingual RE, we \ufb01rst evalu- ate the performance (F1 score) of our cross-lingual RE approach on the target-language development sets with an increasing dictionary size, as plotted in Figure 2. We found that for most target languages, once the dictionary size reaches 1K, further increasing the dictionary size may not improve the transfer performance. Therefore, we select the dictionary size to be 1K. 5.3.2 Comparison of Different Mappings We compare the performance of cross-lingual RE model transfer under the following bilingual word embedding mappings: \u2022 Regular-1K: the regular mapping learned in (4) using 1K word pairs;",
      "Mapping German Spanish Italian Japanese Portuguese Arabic Chinese Average Regular-1K 41.4 53.0 38.7 30.0 46.4 34.4 49.0 41.8 Orthogonal-1K 41.0 49.2 35.7 27.1 44.0 32.1 47.2 39.5 Semi-Supervised-1K 38.1 46.7 31.2 25.3 43.1 37.3 47.1 38.4 Unsupervised 37.8 45.5 28.9 21.2 40.8 37.5 49.1 37.3 Table 4: Comparison of the performance (F1 score) using different mappings on the target-language development data under the Bi-LSTM model.",
      "\u2022 Orthogonal-1K: the orthogonal mapping with length normalization learned in (5) us- ing 1K word pairs (in this case we train the English RE models with the normalized En- glish word embeddings); \u2022 Semi-Supervised-1K: the mapping learned with 1K word pairs and improved by the self- learning method in (Artetxe et al., 2017); \u2022 Unsupervised: the mapping learned by the unsupervised method in (Artetxe et al., 2018). The results are summarized in Table 4. The reg- ular mapping outperforms the orthogonal mapping consistently across the target languages. While the orthogonal mapping was shown to work bet- ter than the regular mapping for the word transla- tion task (Xing et al., 2015; Artetxe et al., 2016; Smith et al., 2017), our cross-lingual RE approach directly maps target-language word embeddings to the English embedding space without conduct- ing word translations. Moreover, the orthogonal mapping requires length normalization, but we ob- served that length normalization adversely affects the performance of the English RE models (about 2.0 F1 points drop).",
      "Moreover, the orthogonal mapping requires length normalization, but we ob- served that length normalization adversely affects the performance of the English RE models (about 2.0 F1 points drop). We apply the vecmap toolkit1 to obtain the semi-supervised and unsupervised mappings. The unsupervised mapping has the lowest average ac- curacy over the target languages, but it does not re- quire a seed dictionary. Among all the mappings, the regular mapping achieves the best average ac- curacy over the target languages using a dictionary with only 1K word pairs, and hence we adopt it for the cross-lingual RE task. 5.3.3 Performance on Test Data The cross-lingual RE model transfer results for the in-house test data are summarized in Table 5 and the results for the ACE05 test data are summa- rized in Table 6, using the regular mapping learned 1https://github.com/artetxem/vecmap with a bilingual dictionary of size 1K.",
      "In the ta- bles, we also provide the performance of the su- pervised RE model (Bi-LSTM) for each target lan- guage, which is trained with a few hundred thou- sand tokens of manually annotated RE data in the target-language, and may serve as an upper bound for the cross-lingual model transfer performance. Among the 2 neural network models, the Bi- LSTM model achieves a better cross-lingual RE performance than the CNN model for 6 out of the 7 target languages. In terms of absolute perfor- mance, the Bi-LSTM model achieves over 40.0 F1 scores for German, Spanish, Portuguese and Chi- nese. In terms of relative performance, it reaches over 75% of the accuracy of the supervised target- language RE model for German, Spanish, Ital- ian and Portuguese. While Japanese and Ara- bic appear to be more dif\ufb01cult to transfer, it still achieves 55% and 52% of the accuracy of the su- pervised Japanese and Arabic RE model, respec- tively, without using any manually annotated RE data in Japanese/Arabic.",
      "While Japanese and Ara- bic appear to be more dif\ufb01cult to transfer, it still achieves 55% and 52% of the accuracy of the su- pervised Japanese and Arabic RE model, respec- tively, without using any manually annotated RE data in Japanese/Arabic. We apply model ensemble to further improve the accuracy of the Bi-LSTM model. We train 5 Bi-LSTM English RE models initiated with differ- ent random seeds, apply the 5 models on the tar- get languages, and combine the outputs by select- ing the relation type labels with the highest prob- abilities among the 5 models. This Ensemble ap- proach improves the single model by 0.6-1.9 F1 points, except for Arabic. 5.3.4 Discussion Since our approach projects the target-language word embeddings to the source-language embed- ding space preserving the word order, it is ex- pected to work better for a target language that has more similar word order as the source lan- guage. This has been veri\ufb01ed by our experiments.",
      "This has been veri\ufb01ed by our experiments. The source language, English, belongs to the SVO (Subject, Verb, Object) language family where in a sentence the subject comes \ufb01rst, the verb second, and the object third. Spanish, Italian, Portuguese, German (in conventional typology) and Chinese",
      "Model German Spanish Italian Japanese Portuguese P R F1 P R F1 P R F1 P R F1 P R F1 Bi-LSTM 39.6 48.9 43.8 54.5 47.6 50.8 41.8 34.2 37.6 33.9 25.1 28.9 52.9 44.5 48.4 CNN 32.5 50.5 39.5 49.3 48.3 48.8 36.6 34.9 35.7 27.3 31.5 29.3 49.0 44.0 46.3 Ensemble 39.6 50.5 44.4 56.9 49.1 52.7 42.6 35.3 38.6 35.3 26.4 30.2 54.9 45.2 49.6 Supervised 59.3 56.4 57.8 68.4 65.4 66.8 51.4 48.3 49.8 52.7 52.",
      "4 30.2 54.9 45.2 49.6 Supervised 59.3 56.4 57.8 68.4 65.4 66.8 51.4 48.3 49.8 52.7 52.0 52.4 64.0 61.3 62.6 Table 5: Performance of the cross-lingual RE approach on the in-house target-language test data. Model Arabic Chinese P R F1 P R F1 Bi-LSTM 30.3 45.7 36.4 61.7 37.8 46.8 CNN 24.0 39.7 29.9 56.4 33.8 42.3 Ensemble 27.5 48.7 35.2 61.0 40.4 48.6 Supervised 70.0 69.1 69.5 66.9 69.4 68.1 Table 6: Performance of the cross-lingual RE approach on the ACE05 target-language test data.",
      "also belong to the SVO language family, and our approach achieves over 70% relative accuracy for these languages. On the other hand, Japanese be- longs to the SOV (Subject, Object, Verb) language family and Arabic belongs to the VSO (Verb, Sub- ject, Object) language family, and our approach achieves lower relative accuracy for these two lan- guages. 6 Related Work There are a few weakly supervised cross-lingual RE approaches. Kim et al. (2010) and Kim and Lee (2012) project annotated English RE data to Korean to create weakly labeled training data via aligned parallel corpora. Faruqui and Kumar (2015) translates a target-language sentence into English, performs RE in English, and then projects the relation phrases back to the target-language sentence. Zou et al. (2018) proposes an adversarial feature adaptation approach for cross-lingual rela- tion classi\ufb01cation, which uses a machine transla- tion system to translate source-language sentences into target-language sentences. Unlike the ex- isting approaches, our approach does not require aligned parallel corpora or machine translation systems.",
      "Unlike the ex- isting approaches, our approach does not require aligned parallel corpora or machine translation systems. There are also several multilingual RE approaches, e.g., Verga et al. (2016); Min et al. (2017); Lin et al. (2017), where the focus is to im- prove monolingual RE by jointly modeling texts in multiple languages. Many cross-lingual word embedding models have been developed recently (Upadhyay et al., 2016; Ruder et al., 2017). An important applica- tion of cross-lingual word embeddings is to enable cross-lingual model transfer. In this paper, we ap- ply the bilingual word embedding mapping tech- nique in (Mikolov et al., 2013b) to cross-lingual RE model transfer. Similar approaches have been applied to other NLP tasks such as dependency parsing (Guo et al., 2015), POS tagging (Gouws and S\u00f8gaard, 2015) and named entity recognition (Ni et al., 2017; Xie et al., 2018).",
      "Similar approaches have been applied to other NLP tasks such as dependency parsing (Guo et al., 2015), POS tagging (Gouws and S\u00f8gaard, 2015) and named entity recognition (Ni et al., 2017; Xie et al., 2018). 7 Conclusion In this paper, we developed a simple yet effective neural cross-lingual RE model transfer approach, which has very low resource requirements (a small bilingual dictionary with 1K word pairs) and can be easily extended to a new language. Extensive experiments for 7 target languages across a va- riety of language families on both in-house and open datasets show that the proposed approach achieves very good performance (up to 79% of the accuracy of the supervised target-language RE model), which provides a strong baseline for building cross-lingual RE models with minimal resources. Acknowledgments We thank Mo Yu for sharing their ACE05 English data split and the anonymous reviewers for their valuable comments. References Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016. Learning principled bilingual mappings of word em- beddings while preserving monolingual invariance.",
      "References Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016. Learning principled bilingual mappings of word em- beddings while preserving monolingual invariance. In Proceedings of the 2016 Conference on Empiri- cal Methods in Natural Language Processing, pages 2289\u20132294, Austin, Texas. Association for Compu- tational Linguistics. Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017. Learning bilingual word embeddings with (almost) no bilingual data. In Proceedings of the 55th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), pages 451\u2013462, Vancouver, Canada. Association for Computational Linguistics.",
      "Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018. A robust self-learning method for fully unsuper- vised cross-lingual mappings of word embeddings. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 789\u2013798, Melbourne, Aus- tralia. Association for Computational Linguistics. Ronan Collobert, Jason Weston, L\u00b4eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493\u20132537. Alexis Conneau, Guillaume Lample, Marc\u2019Aurelio Ranzato, Ludovic Denoyer, and Herv\u00b4e J\u00b4egou. 2018. Word translation without parallel data. In Interna- tional Conference on Learning Representations. Manaal Faruqui and Chris Dyer. 2014. Improving vec- tor space word representations using multilingual correlation.",
      "2018. Word translation without parallel data. In Interna- tional Conference on Learning Representations. Manaal Faruqui and Chris Dyer. 2014. Improving vec- tor space word representations using multilingual correlation. In Proceedings of the 14th Conference of the European Chapter of the Association for Com- putational Linguistics, pages 462\u2013471, Gothenburg, Sweden. Association for Computational Linguistics. Manaal Faruqui and Shankar Kumar. 2015. Multi- lingual open relation extraction using cross-lingual projection. In Proceedings of the 2015 Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies, pages 1351\u20131356. Association for Computational Linguistics. Matthew R. Gormley, Mo Yu, and Mark Dredze. 2015. Improved relation extraction with feature-rich com- positional embedding models. In Proceedings of the 2015 Conference on Empirical Methods in Nat- ural Language Processing, pages 1774\u20131784, Lis- bon, Portugal. Association for Computational Lin- guistics.",
      "2015. Improved relation extraction with feature-rich com- positional embedding models. In Proceedings of the 2015 Conference on Empirical Methods in Nat- ural Language Processing, pages 1774\u20131784, Lis- bon, Portugal. Association for Computational Lin- guistics. Stephan Gouws, Yoshua Bengio, and Greg Corrado. 2015. Bilbowa: Fast bilingual distributed repre- sentations without word alignments. In Proceed- ings of the 32nd International Conference on Ma- chine Learning, pages 748\u2013756. JMLR Workshop and Conference Proceedings. Stephan Gouws and Anders S\u00f8gaard. 2015. Sim- ple task-speci\ufb01c bilingual word embeddings. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 1386\u20131390, Denver, Colorado. Association for Computational Linguistics. Alex Graves and J\u00a8urgen Schmidhuber. 2005. Frame- wise phoneme classi\ufb01cation with bidirectional LSTM and other neural network architectures.",
      "Association for Computational Linguistics. Alex Graves and J\u00a8urgen Schmidhuber. 2005. Frame- wise phoneme classi\ufb01cation with bidirectional LSTM and other neural network architectures. NEURAL NETWORKS, 18(5-6):602\u2013610. Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng Wang, and Ting Liu. 2015. Cross-lingual de- pendency parsing based on distributed representa- tions. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1234\u20131244, Beijing, China. Association for Computational Linguistics. Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735\u20131780. Nanda Kambhatla. 2004. Combining lexical, syntac- tic, and semantic features with maximum entropy models for extracting relations.",
      "1997. Long short-term memory. Neural Computation, 9(8):1735\u20131780. Nanda Kambhatla. 2004. Combining lexical, syntac- tic, and semantic features with maximum entropy models for extracting relations. In Proceedings of the ACL 2004 on Interactive Poster and Demonstra- tion Sessions, Stroudsburg, PA, USA. Association for Computational Linguistics. Seokhwan Kim, Minwoo Jeong, Jonghoon Lee, and Gary Geunbae Lee. 2010. A cross-lingual annota- tion projection approach for relation detection. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING \u201910, pages 564\u2013571, Stroudsburg, PA, USA. Association for Computational Linguistics. Seokhwan Kim and Gary Geunbae Lee. 2012. A graph-based cross-lingual projection approach for weakly supervised relation extraction.",
      "Association for Computational Linguistics. Seokhwan Kim and Gary Geunbae Lee. 2012. A graph-based cross-lingual projection approach for weakly supervised relation extraction. In Proceed- ings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Vol- ume 2, ACL \u201912, pages 48\u201353, Stroudsburg, PA, USA. Association for Computational Linguistics. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceed- ings of the 3rd International Conference on Learn- ing Representations (ICLR), ICLR \u201915. Qi Li and Heng Ji. 2014. Incremental joint extrac- tion of entity mentions and relations. In Proceed- ings of the 52nd Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers), pages 402\u2013412. Association for Computa- tional Linguistics. Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2017.",
      "Association for Computa- tional Linguistics. Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2017. Neural relation extraction with multi-lingual atten- tion. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 34\u201343. Association for Computational Linguistics. Thang Luong, Hieu Pham, and Christopher D. Man- ning. 2015. Bilingual word representations with monolingual quality in mind. In Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing, pages 151\u2013159. Association for Computational Linguistics. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Ef\ufb01cient estimation of word represen- tations in vector space. CoRR, abs/1301.3781. Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013b. Exploiting similarities among languages for machine translation.",
      "CoRR, abs/1301.3781. Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013b. Exploiting similarities among languages for machine translation. CoRR, abs/1309.4168.",
      "Bonan Min, Zhuolin Jiang, Marjorie Freedman, and Ralph Weischedel. 2017. Learning transferable rep- resentation for bilingual relation extraction via con- volutional neural networks. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 674\u2013684, Taipei, Taiwan. Asian Federation of Natural Language Processing. Makoto Miwa and Mohit Bansal. 2016. End-to-end re- lation extraction using LSTMs on sequences and tree structures. In Proceedings of the 54th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1105\u20131116. Asso- ciation for Computational Linguistics. Thien Huu Nguyen and Ralph Grishman. 2016. Com- bining neural networks and log-linear models to im- prove relation extraction. In Proceedings of IJCAI Workshop on Deep Learning for Arti\ufb01cial Intelli- gence (DLAI). Jian Ni, Georgiana Dinu, and Radu Florian. 2017.",
      "Com- bining neural networks and log-linear models to im- prove relation extraction. In Proceedings of IJCAI Workshop on Deep Learning for Arti\ufb01cial Intelli- gence (DLAI). Jian Ni, Georgiana Dinu, and Radu Florian. 2017. Weakly supervised cross-lingual named entity recognition via effective annotation and represen- tation projection. In Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1470\u2013 1480. Association for Computational Linguistics. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, Doha, Qatar. Association for Computational Linguistics. Barbara Plank and Alessandro Moschitti. 2013. Em- bedding semantic similarity in tree kernels for do- main adaptation of relation extraction.",
      "Association for Computational Linguistics. Barbara Plank and Alessandro Moschitti. 2013. Em- bedding semantic similarity in tree kernels for do- main adaptation of relation extraction. In Proceed- ings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers), pages 1498\u20131507, So\ufb01a, Bulgaria. Associa- tion for Computational Linguistics. Sebastian Ruder, Ivan Vuli\u00b4c, and Anders S\u00f8gaard. 2017. A survey of cross-lingual embedding models. CoRR, abs/1706.04902. Cicero dos Santos, Bing Xiang, and Bowen Zhou. 2015. Classifying relations by ranking with con- volutional neural networks. In Proceedings of the 53rd Annual Meeting of the Association for Compu- tational Linguistics and the 7th International Joint Conference on Natural Language Processing (Vol- ume 1: Long Papers), pages 626\u2013634. Association for Computational Linguistics.",
      "In Proceedings of the 53rd Annual Meeting of the Association for Compu- tational Linguistics and the 7th International Joint Conference on Natural Language Processing (Vol- ume 1: Long Papers), pages 626\u2013634. Association for Computational Linguistics. Samuel L. Smith, David H. P. Turban, Steven Hamblin, and Nils Y. Hammerla. 2017. Of\ufb02ine bilingual word vectors, orthogonal transformations and the inverted softmax. In 5th International Conference on Learn- ing Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from over\ufb01tting. Journal of Machine Learning Re- search, 15(1):1929\u20131958. Shyam Upadhyay, Manaal Faruqui, Chris Dyer, and Dan Roth. 2016.",
      "Journal of Machine Learning Re- search, 15(1):1929\u20131958. Shyam Upadhyay, Manaal Faruqui, Chris Dyer, and Dan Roth. 2016. Cross-lingual models of word em- beddings: An empirical comparison. In Proceed- ings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers), pages 1661\u20131670. Association for Computa- tional Linguistics. Patrick Verga, David Belanger, Emma Strubell, Ben- jamin Roth, and Andrew McCallum. 2016. Multi- lingual relation extraction using compositional uni- versal schema. In Proceedings of the 2016 Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies, pages 886\u2013896. Association for Computational Linguistics. Christopher Walker, Stephanie Strassel, Julie Medero, and Kazuaki Maeda. 2006. ACE 2005 multilingual training corpus. Philadelphia: Linguistic Data Con- sortium.",
      "Association for Computational Linguistics. Christopher Walker, Stephanie Strassel, Julie Medero, and Kazuaki Maeda. 2006. ACE 2005 multilingual training corpus. Philadelphia: Linguistic Data Con- sortium. Jiateng Xie, Zhilin Yang, Graham Neubig, Noah A. Smith, and Jaime Carbonell. 2018. Neural cross- lingual named entity recognition with minimal re- sources. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Process- ing, pages 369\u2013379, Brussels, Belgium. Association for Computational Linguistics. Chao Xing, Dong Wang, Chao Liu, and Yiye Lin. 2015. Normalized word embedding and orthog- onal transform for bilingual word translation. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 1006\u20131011, Denver, Colorado. Association for Computational Linguistics. Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation ex- traction.",
      "Association for Computational Linguistics. Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation ex- traction. The Journal of Machine Learning Re- search, 3:1083\u20131106. Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. 2014. Relation classi\ufb01cation via con- volutional deep neural network. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2335\u20132344. Dublin City University and As- sociation for Computational Linguistics. Bowei Zou, Zengzhuang Xu, Yu Hong, and Guodong Zhou. 2018. Adversarial feature adaptation for cross-lingual relation classi\ufb01cation. In Proceedings of the 27th International Conference on Computa- tional Linguistics, pages 437\u2013448, Santa Fe, New Mexico, USA. Association for Computational Lin- guistics."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1911.00069.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":10562,
  "avg_doclen":173.1475409836,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1911.00069.pdf"
    }
  }
}