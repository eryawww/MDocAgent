{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Generating Personalized Recipes from Historical User Preferences Bodhisattwa Prasad Majumder\u2217, Shuyang Li\u2217, Jianmo Ni, Julian McAuley Computer Science and Engineering University of California, San Diego {bmajumde, shl008, jin018, jmcauley}@ucsd.edu Abstract Existing approaches to recipe generation are unable to create recipes for users with culinary preferences but incomplete knowledge of in- gredients in speci\ufb01c dishes. We propose a new task of personalized recipe generation to help these users: expanding a name and incom- plete ingredient details into complete natural- text instructions aligned with the user\u2019s histor- ical preferences. We attend on technique- and recipe-level representations of a user\u2019s previ- ously consumed recipes, fusing these \u2018user- aware\u2019 representations in an attention fusion layer to control recipe text generation. Exper- iments on a new dataset of 180K recipes and 700K interactions show our model\u2019s ability to generate plausible and personalized recipes compared to non-personalized baselines. 1 Introduction In the kitchen, we increasingly rely on instructions from cooking websites: recipes.",
      "Exper- iments on a new dataset of 180K recipes and 700K interactions show our model\u2019s ability to generate plausible and personalized recipes compared to non-personalized baselines. 1 Introduction In the kitchen, we increasingly rely on instructions from cooking websites: recipes. A cook with a predilection for Asian cuisine may wish to pre- pare chicken curry, but may not know all neces- sary ingredients apart from a few basics. These users with limited knowledge cannot rely on ex- isting recipe generation approaches that focus on creating coherent recipes given all ingredients and a recipe name (Kiddon et al., 2016). Such mod- els do not address issues of personal preference (e.g. culinary tastes, garnish choices) and incom- plete recipe details. We propose to approach both problems via personalized generation of plausi- ble, user-speci\ufb01c recipes using user preferences extracted from previously consumed recipes. Our work combines two important tasks from natural language processing and recommender systems: data-to-text generation (Gatt and Krah- mer, 2018) and personalized recommendation \u2217denotes equal contribution (Rashid et al., 2002).",
      "Our work combines two important tasks from natural language processing and recommender systems: data-to-text generation (Gatt and Krah- mer, 2018) and personalized recommendation \u2217denotes equal contribution (Rashid et al., 2002). Our model takes as user in- put the name of a speci\ufb01c dish, a few key ingre- dients, and a calorie level. We pass these loose input speci\ufb01cations to an encoder-decoder frame- work and attend on user pro\ufb01les\u2014learned latent representations of recipes previously consumed by a user\u2014to generate a recipe personalized to the user\u2019s tastes. We fuse these \u2018user-aware\u2019 represen- tations with decoder output in an attention fusion layer to jointly determine text generation. Quan- titative (perplexity, user-ranking) and qualitative analysis on user-aware model outputs con\ufb01rm that personalization indeed assists in generating plau- sible recipes from incomplete ingredients.",
      "Quan- titative (perplexity, user-ranking) and qualitative analysis on user-aware model outputs con\ufb01rm that personalization indeed assists in generating plau- sible recipes from incomplete ingredients. While personalized text generation has seen success in conveying user writing styles in the product review (Ni et al., 2017; Ni and McAuley, 2018) and dialogue (Zhang et al., 2018) spaces, we are the \ufb01rst to consider it for the problem of recipe generation, where output quality is heavily depen- dent on the content of the instructions\u2014such as ingredients and cooking techniques. To summarize, our main contributions are as follows: 1. We explore a new task of generating plausi- ble and personalized recipes from incomplete input speci\ufb01cations by leveraging historical user preferences;1 2. We release a new dataset of 180K+ recipes and 700K+ user reviews for this task; 3. We introduce new evaluation strategies for generation quality in instructional texts, cen- tering on quantitative measures of coher- ence.",
      "We release a new dataset of 180K+ recipes and 700K+ user reviews for this task; 3. We introduce new evaluation strategies for generation quality in instructional texts, cen- tering on quantitative measures of coher- ence. We also show qualitatively and quan- titatively that personalized models generate high-quality and speci\ufb01c recipes that align with historical user preferences. 1Our source code and appendix are at https://github.com/majumderb/ recipe-personalization arXiv:1909.00105v1  [cs.CL]  31 Aug 2019",
      "2 Related Work Large-scale transformer-based language models have shown surprising expressivity and \ufb02uency in creative and conditional long-text generation (Vaswani et al., 2017; Radford et al., 2019). Re- cent works have proposed hierarchical methods that condition on narrative frameworks to generate internally consistent long texts (Fan et al., 2018; Xu et al., 2018; Yao et al., 2018). Here, we gener- ate procedurally structured recipes instead of free- form narratives. Recipe generation belongs to the \ufb01eld of data- to-text natural language generation (Gatt and Krahmer, 2018), which sees other applications in automated journalism (Lepp\u00a8anen et al., 2017), question-answering (Agrawal et al., 2017), and abstractive summarization (Paulus et al., 2018), among others. Kiddon et al. (2015); Bosselut et al. (2018b) model recipes as a structured collection of ingredient entities acted upon by cooking actions. Kiddon et al.",
      "Kiddon et al. (2015); Bosselut et al. (2018b) model recipes as a structured collection of ingredient entities acted upon by cooking actions. Kiddon et al. (2016) imposes a \u2018checklist\u2019 atten- tion constraint emphasizing hitherto unused ingre- dients during generation. Yang et al. (2017) at- tend over explicit ingredient references in the prior recipe step. Similar hierarchical approaches that infer a full ingredient list to constrain generation will not help personalize recipes, and would be in- feasible in our setting due to the potentially un- constrained number of ingredients (from a space of 10K+) in a recipe. We instead learn historical preferences to guide full recipe generation. A recent line of work has explored user- and item-dependent aspect-aware review generation (Ni et al., 2017; Ni and McAuley, 2018). This work is related to ours in that it combines con- textual language generation with personalization. Here, we attend over historical user preferences from previously consumed recipes to generate recipe content, rather than writing styles.",
      "This work is related to ours in that it combines con- textual language generation with personalization. Here, we attend over historical user preferences from previously consumed recipes to generate recipe content, rather than writing styles. 3 Approach Our model\u2019s input speci\ufb01cation consists of: the recipe name as a sequence of tokens, a partial list of ingredients, and a caloric level (high, medium, low). It outputs the recipe instructions as a token sequence: Wr = {wr,0, . . . , wr,T } for a recipe r of length T. To personalize output, we use histor- ical recipe interactions of a user u \u2208U. Encoder: Our encoder has three embedding lay- ers: vocabulary embedding V, ingredient embed- ding I, and caloric-level embedding C. Each token in the (length Ln) recipe name is embedded via V; the embedded token sequence is passed to a two- layered bidirectional GRU (BiGRU) (Cho et al., 2014), which outputs hidden states for names {nenc,j \u2208R2dh}, with hidden size dh.",
      "Similarly each of the Li input ingredients is embedded via I, and the embedded ingredient sequence is passed to another two-layered BiGRU to output ingredi- ent hidden states as {ienc,j \u2208R2dh}. The caloric level is embedded via C and passed through a pro- jection layer with weights Wc to generate calorie hidden representation cenc \u2208R2dh. Ingredient Attention: We apply attention (Bah- danau et al., 2015) over the encoded ingredients to use encoder outputs at each decoding time step. We de\ufb01ne an attention-score function \u03b1 with key K and query Q: \u03b1(K, Q) = exp (tanh (W\u03b1 [K + Q] + b\u03b1)) Z , with trainable weights W\u03b1, bias b\u03b1, and normal- ization term Z. At decoding time t, we calculate the ingredient context ai t \u2208Rdh as: ai t = Li X j=1 \u03b1 (ienc,j, ht) \u00d7 ienc,j. Decoder: The decoder is a two-layer GRU with hidden state ht conditioned on previous hidden state ht\u22121 and input token wr,t from the original recipe text.",
      "Decoder: The decoder is a two-layer GRU with hidden state ht conditioned on previous hidden state ht\u22121 and input token wr,t from the original recipe text. We project the concatenated encoder outputs as the initial decoder hidden state: h0 \u00c4 \u2208Rdh\u00e4 = Wh0 [nenc,Ln; ienc,Li; cenc] + bh0 ht, ot = GRU \u00c4\u00ee wr,t; ai t \u00f3 , ht\u22121 \u00e4 . To bias generation toward user preferences, we attend over a user\u2019s previously reviewed recipes to jointly determine the \ufb01nal output token distribu- tion. We consider two different schemes to model preferences from user histories: (1) recipe inter- actions, and (2) techniques seen therein (de\ufb01ned in Section 4). Rendle et al. (2009); Quadrana et al. (2018); Ueda et al. (2011) explore similar schemes for personalized recommendation.",
      "Rendle et al. (2009); Quadrana et al. (2018); Ueda et al. (2011) explore similar schemes for personalized recommendation. Prior Recipe Attention: We obtain the set of prior recipes for a user u: R+ u , where each recipe can be represented by an embedding from a recipe embedding layer R or an average of the name to- kens embedded by V. We attend over the k-most recent prior recipes, Rk+ u , to account for tempo- ral drift of user preferences (Moore et al., 2013).",
      "Figure 1: Sample data \ufb02ow through model architec- ture. Emphasis on prior recipe attention scores (darker is stronger). Ingredient attention omitted for clarity. These embeddings are used in the \u2018Prior Recipe\u2019 and \u2018Prior Name\u2019 models, respectively. Given a recipe representation r \u2208Rdr (where dr is recipe- or vocabulary-embedding size de- pending on the recipe representation) the prior recipe attention context aru t is calculated as aru t = X r\u2208Rk+ u \u03b1 (r, ht) \u00d7 r. Prior Technique Attention: We calculate prior technique preference (used in the \u2018Prior Tech\u2018 model) by normalizing co-occurrence between users and techniques seen in R+ u , to obtain a pref- erence vector \u03c1u. Each technique x is embedded via a technique embedding layer X to x \u2208Rdx.",
      "Each technique x is embedded via a technique embedding layer X to x \u2208Rdx. Prior technique attention is calculated as axu t = X x seen in R+ u (\u03b1 (x, ht) + \u03c1u,x) \u00d7 x, where, inspired by copy mechanisms (See et al., 2017; Gu et al., 2016), we add \u03c1u,x for technique x to emphasize the attention by the user\u2019s prior tech- nique preference. Attention Fusion Layer: We fuse all contexts calculated at time t, concatenating them with de- coder GRU output and previous token embedding: af t =ReLU \u00c4 Wf \u00ee wr,t; ot; ai t; (aru t or axu t ) \u00f3 +bf \u00e4 . We then calculate the token probability: P(Sr,t) = softmax \u00c4 WP [af t ] + bP \u00e4 , and maximize the log-likelihood of the generated sequence conditioned on input speci\ufb01cations and user preferences. Figure 1 shows a case where the Prior Name model attends strongly on previously consumed savory recipes to suggest the usage of an additional ingredient (\u2018cilantro\u2019).",
      "Figure 1 shows a case where the Prior Name model attends strongly on previously consumed savory recipes to suggest the usage of an additional ingredient (\u2018cilantro\u2019). Split # Users # Recipes # Actions Sparsity3 Train 25,076 160,901 698,901 99.983% Dev 7,023 6,621 7,023 \u2013 Test 12,455 11,695 12,455 \u2013 Table 1: Statistics of Food.com interactions 4 Recipe Dataset: Food.com We collect a novel dataset of 230K+ recipe texts and 1M+ user interactions (reviews) over 18 years (2000-2018) from Food.com.2 Here, we restrict to recipes with at least 3 steps, and at least 4 and no more than 20 ingredients. We discard users with fewer than 4 reviews, giving 180K+ recipes and 700K+ reviews, with splits as in Table 1. Our model must learn to generate from a di- verse recipe space: in our training data, the av- erage recipe length is 117 tokens with a maxi- mum of 256. There are 13K unique ingredients across all recipes.",
      "Our model must learn to generate from a di- verse recipe space: in our training data, the av- erage recipe length is 117 tokens with a maxi- mum of 256. There are 13K unique ingredients across all recipes. Rare words dominate the vocab- ulary: 95% of words appear <100 times, account- ing for only 1.65% of all word usage. As such, we perform Byte-Pair Encoding (BPE) tokeniza- tion (Sennrich et al., 2016; Radford et al., 2018), giving a training vocabulary of 15K tokens across 19M total mentions. User pro\ufb01les are similarly di- verse: 50% of users have consumed \u22646 recipes, while 10% of users have consumed >45 recipes. We order reviews by timestamp, keeping the most recent review for each user as the test set, the second most recent for validation, and the remain- der for training (sequential leave-one-out evalua- tion (Kang and McAuley, 2018)). We evaluate only on recipes not in the training set.",
      "We evaluate only on recipes not in the training set. We manually construct a list of 58 cooking techniques from 384 cooking actions collected by Bosselut et al. (2018b); the most common tech- niques (bake, combine, pour, boil) account for 36.5% of technique mentions. We approximate technique adherence via string match between the recipe text and technique list. 5 Experiments and Results For training and evaluation, we provide our model with the \ufb01rst 3-5 ingredients listed in each recipe. We decode recipe text via top-k sampling (Rad- ford et al., 2019), \ufb01nding k = 3 to produce sat- isfactory results. We use a hidden size dh = 256 2https://www.kaggle.com/shuyangli94/ food-com-recipes-and-user-interactions 3Ratio of unobserved actions to all possible actions.",
      "Model BPE PPL BLEU-1 BLEU-4 ROUGE-L D-1 (%) D-2 (%) UMA MRR PP (%) NN \u2013 20.279 0.465 16.871 0.931 9.394 0.100 0.293 \u2013 Enc-Dec 9.611 28.391 3.385 25.001 0.220 1.928 0.100 0.293 \u2013 Prior Tech 9.572 28.864 3.312 24.920 0.233 2.158 0.128 0.319 62.821 Prior Recipe 9.551 27.858 3.215 24.822 0.231 2.062 0.302 0.412 66.026 Prior Name 9.516 28.046 3.211 24.794 0.233 2.080 0.505 0.628 61.165 Table 2: Metrics on generated recipes from test set.",
      "D-1/2 = Distinct-1/2, UMA = User Matching Accuracy, MRR = Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model). Input Name: Pomberrytini; Ingredients: pomegranate-blueberry juice, cranberry juice, vodka ; Calorie: Low Gold Place everything except the orange slices in a cocktail shaker. Shake until well mixed and well chilled. Pour into martini glasses and \ufb02oat an orange slice in each glass. Enc-Dec Combine all ingredients. Cover and refrigerate. Serve with whipped topping. Prior Tech Combine all ingredients. Store in refrigerator. Serve over ice. Enjoy! Prior Recipe Pour the ice into a cocktail shaker. Pour in the vodka and vodka. Add a little water and shake to mix. Pour into the glass and garnish with a slice of orange slices. Enjoy! Prior Name Combine all ingredients except for the ice in a blender or food processor. Process to make a smooth paste and then add the remaining vodka and blend until smooth.",
      "Add a little water and shake to mix. Pour into the glass and garnish with a slice of orange slices. Enjoy! Prior Name Combine all ingredients except for the ice in a blender or food processor. Process to make a smooth paste and then add the remaining vodka and blend until smooth. Pour into a chilled glass and garnish with a little lemon and fresh mint. Table 3: Sample generated recipe. Emphasis on personalization and explicit ingredient mentions via highlights. for both the encoder and decoder. Embedding di- mensions for vocabulary, ingredient, recipe, tech- niques, and caloric level are 300, 10, 50, 50, and 5 (respectively). For prior recipe attention, we set k = 20, the 80th %-ile for the number of user interactions. We use the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 10\u22123, annealed with a decay rate of 0.9 (Howard and Ruder, 2018). We also use teacher-forcing (Williams and Zipser, 1989) in all training epochs.",
      "We also use teacher-forcing (Williams and Zipser, 1989) in all training epochs. In this work, we investigate how leveraging historical user preferences can improve genera- tion quality over strong baselines in our setting. We compare our personalized models against two baselines. The \ufb01rst is a name-based Nearest- Neighbor model (NN). We initially adapted the Neural Checklist Model of Kiddon et al. (2016) as a baseline; however, we ultimately use a sim- ple Encoder-Decoder baseline with ingredient at- tention (Enc-Dec), which provides comparable performance and lower complexity. All person- alized models outperform baseline in BPE per- plexity (Table 2) with Prior Name performing the best. While our models exhibit comparable perfor- mance to baseline in BLEU-1/4 and ROUGE-L, we generate more diverse (Distinct-1/2: percent- age of distinct unigrams and bigrams) and accept- able recipes. BLEU and ROUGE are not the most appropriate metrics for generation quality. A \u2018cor- rect\u2019 recipe can be written in many ways with the same main entities (ingredients).",
      "BLEU and ROUGE are not the most appropriate metrics for generation quality. A \u2018cor- rect\u2019 recipe can be written in many ways with the same main entities (ingredients). As BLEU-1/4 capture structural information via n-gram match- ing, they are not correlated with subjective recipe quality. This mirrors observations from Baheti et al. (2018); Fan et al. (2018). We observe that personalized models make more diverse recipes than baseline. They thus perform better in BLEU-1 with more key enti- ties (ingredient mentions) present, but worse in BLEU-4, as these recipes are written in a person- alized way and deviate from gold on the phrasal level. Similarly, the \u2018Prior Name\u2019 model generates more unigram-diverse recipes than other personal- ized models and obtains a correspondingly lower BLEU-1 score. Qualitative Analysis: We present sample out- puts for a cocktail recipe in Table 3, and addi- tional recipes in the appendix. Generation qual- ity progressively improves from generic baseline output to a blended cocktail produced by our best performing model.",
      "Qualitative Analysis: We present sample out- puts for a cocktail recipe in Table 3, and addi- tional recipes in the appendix. Generation qual- ity progressively improves from generic baseline output to a blended cocktail produced by our best performing model. Models attending over prior recipes explicitly reference ingredients. The Prior Name model further suggests the addition of lemon and mint, which are reasonably asso- ciated with previously consumed recipes like co- conut mousse and pork skewers.",
      "Personalization: To measure personalization, we evaluate how closely the generated text corre- sponds to a particular user pro\ufb01le. We compute the likelihood of generated recipes using identical input speci\ufb01cations but conditioned on ten differ- ent user pro\ufb01les\u2014one \u2018gold\u2019 user who consumed the original recipe, and nine randomly generated user pro\ufb01les. Following Fan et al. (2018), we ex- pect the highest likelihood for the recipe condi- tioned on the gold user. We measure user match- ing accuracy (UMA)\u2014the proportion where the gold user is ranked highest\u2014and Mean Reciprocal Rank (MRR) (Radev et al., 2002) of the gold user. All personalized models beat baselines in both metrics, showing our models personalize gener- ated recipes to the given user pro\ufb01les. The Prior Name model achieves the best UMA and MRR by a large margin, revealing that prior recipe names are strong signals for personalization. Moreover, the addition of attention mechanisms to capture these signals improves language modeling perfor- mance over a strong non-personalized baseline.",
      "The Prior Name model achieves the best UMA and MRR by a large margin, revealing that prior recipe names are strong signals for personalization. Moreover, the addition of attention mechanisms to capture these signals improves language modeling perfor- mance over a strong non-personalized baseline. Recipe Level Coherence: A plausible recipe should possess a coherent step order, and we eval- uate this via a metric for recipe-level coherence. We use the neural scoring model from Bosselut et al. (2018a) to measure recipe-level coherence for each generated recipe. Each recipe step is en- coded by BERT (Devlin et al., 2019). Our scor- ing model is a GRU network that learns the over- all recipe step ordering structure by minimizing the cosine similarity of recipe step hidden repre- sentations presented in the correct and reverse or- ders. Once pretrained, our scorer calculates the similarity of a generated recipe to the forward and backwards ordering of its corresponding gold la- bel, giving a score equal to the difference between the former and latter. A higher score indicates better step ordering (with a maximum score of 2).",
      "Once pretrained, our scorer calculates the similarity of a generated recipe to the forward and backwards ordering of its corresponding gold la- bel, giving a score equal to the difference between the former and latter. A higher score indicates better step ordering (with a maximum score of 2). Table 4 shows that our personalized models achieve average recipe-level coherence scores of 1.78-1.82, surpassing the baseline at 1.77. Recipe Step Entailment: Local coherence is also crucial to a user following a recipe: it is crucial that subsequent steps are logically consistent with prior ones. We model local coherence as an entail- ment task: predicting the likelihood that a recipe step follows the preceding. We sample several consecutive (positive) and non-consecutive (neg- ative) pairs of steps from each recipe. We train a BERT (Devlin et al., 2019) model to predict the Model Recipe Level Coherence Recipe Step Entailment Enc-Dec 1.77 0.72 Prior Tech 1.78 0.73 Prior Recipe 1.80 0.76 Prior Name 1.82 0.78 Table 4: Coherence metrics on generated recipes from test set.",
      "entailment score of a pair of steps separated by a [SEP] token, using the \ufb01nal representation of the [CLS] token. The step entailment score is com- puted as the average of scores for each set of con- secutive steps in each recipe, averaged over every generated recipe for a model, as shown in Table 4. Human Evaluation: We presented 310 pairs of recipes for pairwise comparison (Fan et al., 2018) (details in appendix) between baseline and each personalized model, with results shown in Table 2. On average, human evaluators preferred personal- ized model outputs to baseline 63% of the time, con\ufb01rming that personalized attention improves the semantic plausibility of generated recipes. We also performed a small-scale human coherence survey over 90 recipes, in which 60% of users found recipes generated by personalized models to be more coherent and preferable to those gen- erated by baseline models. 6 Conclusion In this paper, we propose a novel task: to gen- erate personalized recipes from incomplete in- put speci\ufb01cations and user histories.",
      "6 Conclusion In this paper, we propose a novel task: to gen- erate personalized recipes from incomplete in- put speci\ufb01cations and user histories. On a large novel dataset of 180K recipes and 700K reviews, we show that our personalized generative models can generate plausible, personalized, and coher- ent recipes preferred by human evaluators for con- sumption. We also introduce a set of automatic co- herence measures for instructional texts as well as personalization metrics to support our claims. Our future work includes generating structured repre- sentations of recipes to handle ingredient proper- ties, as well as accounting for references to collec- tions of ingredients (e.g. \u201cdry mix\u201d). Acknowledgements. This work is partly sup- ported by NSF #1750063. We thank all reviewers for their constructive suggestions, as well as Rei M., Sujoy P., Alicia L., Eric H., Tim S., Kathy C., Allen C., and Micah I. for their feedback.",
      "References Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Mar- garet Mitchell, C. Lawrence Zitnick, Devi Parikh, and Dhruv Batra. 2017. VQA: visual question an- swering. IJCV, 123(1):4\u201331. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate. In ICLR. Ashutosh Baheti, Alan Ritter, Jiwei Li, and Bill Dolan. 2018. Generating more interesting responses in neural conversation models with distributional con- straints. In EMNLP. Antoine Bosselut, Asli C\u00b8 elikyilmaz, Xiaodong He, Jianfeng Gao, Po-Sen Huang, and Yejin Choi. 2018a. Discourse-aware neural rewards for coher- ent text generation. In NAACL-HLT.",
      "2018a. Discourse-aware neural rewards for coher- ent text generation. In NAACL-HLT. Antoine Bosselut, Omer Levy, Ari Holtzman, Corin Ennis, Dieter Fox, and Yejin Choi. 2018b. Simulat- ing action dynamics with neural process networks. In ICLR. Kyunghyun Cho, Bart van Merrienboer, C\u00b8 aglar G\u00a8ulc\u00b8ehre, Dzmitry Bahdanau, Fethi Bougares, Hol- ger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In EMNLP. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language under- standing. In NAACL-HLT 2019. Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi- erarchical neural story generation. In ACL. Albert Gatt and Emiel Krahmer. 2018.",
      "In NAACL-HLT 2019. Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi- erarchical neural story generation. In ACL. Albert Gatt and Emiel Krahmer. 2018. Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. J. Artif. Intell. Res., 61:65\u2013170. Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O. K. Li. 2016. Incorporating copying mechanism in sequence-to-sequence learning. In ACL. Jeremy Howard and Sebastian Ruder. 2018. Universal language model \ufb01ne-tuning for text classi\ufb01cation. In ACL. Wang-Cheng Kang and Julian McAuley. 2018. Self- attentive sequential recommendation. In ICDM. Chlo\u00b4e Kiddon, Ganesa Thandavam Ponnuraj, Luke Zettlemoyer, and Yejin Choi. 2015. Mise en place: Unsupervised interpretation of instructional recipes.",
      "Self- attentive sequential recommendation. In ICDM. Chlo\u00b4e Kiddon, Ganesa Thandavam Ponnuraj, Luke Zettlemoyer, and Yejin Choi. 2015. Mise en place: Unsupervised interpretation of instructional recipes. In EMNLP. Chlo\u00b4e Kiddon, Luke Zettlemoyer, and Yejin Choi. 2016. Globally coherent text generation with neu- ral checklist models. In EMNLP. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In ICLR. Leo Lepp\u00a8anen, Myriam Munezero, Mark Granroth- Wilding, and Hannu Toivonen. 2017. Data-driven news generation for automated journalism. In INLG. Joshua L. Moore, Shuo Chen, Douglas Turnbull, and Thorsten Joachims. 2013. Taste over time: The tem- poral dynamics of user preferences. In ISMIR.",
      "Data-driven news generation for automated journalism. In INLG. Joshua L. Moore, Shuo Chen, Douglas Turnbull, and Thorsten Joachims. 2013. Taste over time: The tem- poral dynamics of user preferences. In ISMIR. Jianmo Ni, Zachary C. Lipton, Sharad Vikram, and Ju- lian McAuley. 2017. Estimating reactions and rec- ommending products with generative models of re- views. In IJCNLP. Jianmo Ni and Julian McAuley. 2018. Personalized re- view generation by expanding phrases and attending on aspect-aware representations. In ACL. Romain Paulus, Caiming Xiong, and Richard Socher. 2018. A deep reinforced model for abstractive sum- marization. In ICLR. Massimo Quadrana, Paolo Cremonesi, and Dietmar Jannach. 2018. Sequence-aware recommender sys- tems. In UMAP. Dragomir R. Radev, Hong Qi, Harris Wu, and Weiguo Fan.",
      "In ICLR. Massimo Quadrana, Paolo Cremonesi, and Dietmar Jannach. 2018. Sequence-aware recommender sys- tems. In UMAP. Dragomir R. Radev, Hong Qi, Harris Wu, and Weiguo Fan. 2002. Evaluating web-based question answer- ing systems. In LREC. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing by generative pre-training. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Al Mamunur Rashid, Istvan Albert, Dan Cosley, Shy- ong K. Lam, Sean M. McNee, Joseph A. Konstan, and John Riedl. 2002. Getting to know you: learn- ing new user preferences in recommender systems. In IUI.",
      "2002. Getting to know you: learn- ing new user preferences in recommender systems. In IUI. Steffen Rendle, Christoph Freudenthaler, Zeno Gant- ner, and Lars Schmidt-Thieme. 2009. BPR: bayesian personalized ranking from implicit feed- back. In UAI. Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointer- generator networks. In ACL. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In ACL. Mayumi Ueda, Mari Takahata, and Shinsuke Naka- jima. 2011. User\u2019s food preference extraction for personalized cooking recipe recommendation. In SPIM. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need.",
      "In SPIM. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NIPS.",
      "Ronald J. Williams and David Zipser. 1989. A learn- ing algorithm for continually running fully recurrent neural networks. Neural Computation, 1(2):270\u2013 280. Jingjing Xu, Xuancheng Ren, Yi Zhang, Qi Zeng, Xi- aoyan Cai, and Xu Sun. 2018. A skeleton-based model for promoting coherence among sentences in narrative story generation. In EMNLP. Zichao Yang, Phil Blunsom, Chris Dyer, and Wang Ling. 2017. Reference-aware language models. In EMNLP. Lili Yao, Nanyun Peng, Ralph M. Weischedel, Kevin Knight, Dongyan Zhao, and Rui Yan. 2018. Plan- and-write: Towards better automatic storytelling. CoRR, abs/1811.05701. Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Per- sonalizing dialogue agents: I have a dog, do you have pets too? In ACL.",
      "Appendix Food.com: Dataset Details Our raw data consists of 270K recipes and 1.4M user-recipe interactions (reviews) scraped from Food.com, covering a period of 18 years (January 2000 to December 2018). See Table 5 for dataset summary statistics, and Table 6 for sample infor- mation about one user-recipe interaction and the recipe involved. # Recipes # Users # Reviews Sparsity (%) Raw 231,637 226,570 1,132,367 99.998 Processed 178,265 25,076 749,053 99.983 Table 5: Interaction statistics for Food.com dataset be- fore and after data processing. Generated Examples See Table 7 for a sample recipe for chicken chili and Table 8 for a sample recipe for sweet waf\ufb02es. Human Evaluation We prepared a set of 15 pairwise comparisons per evaluation session, and collected 930 pairwise evaluations (310 per personalized model) over 62 sessions.",
      "Generated Examples See Table 7 for a sample recipe for chicken chili and Table 8 for a sample recipe for sweet waf\ufb02es. Human Evaluation We prepared a set of 15 pairwise comparisons per evaluation session, and collected 930 pairwise evaluations (310 per personalized model) over 62 sessions. For each pair, users were given a par- tial recipe speci\ufb01cation (name and 3-5 key ingredi- ents), as well as two generated recipes labeled \u2018A\u2019 and \u2018B\u2019. One recipe is generated from our base- line encoder-decoder model and one recipe is gen- erated by one of our three personalized models (Prior Tech, Prior Name, Prior Recipe). The order of recipe presentation (A/B) is randomly selected for each question. A screenshot of the user evalua- tion interface is given in Figure 2. We ask the user to indicate which recipe they \ufb01nd more coherent, and which recipe best accomplishes the goal in- dicated by the recipe name. A screenshot of this survey interface is given in Figure 3.",
      "Field Value date 2002-03-30 user id 27395 recipe id 23933 name chinese candy n steps 4 steps [\u2018melt butterscotch chips in heavy saucepan over low heat\u2019, \u2018fold in peanuts and chinese noodles until coated\u2019, \u2019drop by tablespoon onto waxed paper\u2019, \u2018let stand in cool place until \ufb01rm\u2019] n ingredients 3 ingredients [\u2018butterscotch chips\u2019, \u2018chinese noodles\u2019, \u2018salted peanuts\u2019] calorie level 0 (Low) Table 6: Sample data from GeniusKitchen with recipe and user interaction details. Figure 2: A sample question for pairwise evaluation survey.",
      "Input Name: Chicken Bell Pepper Chili Weight Watchers Ingredients: olive oil, red bell pepper, green bell pepper, yellow bell pepper Calorie: Medium Gold In a large nonstick dutch oven or saucepan, heat the oil. Saute the bell peppers and onion until softened, about 5 minutes. Add the chicken. Cook, breaking apart with a wooden spoon, until no longer pink, 5 - 7 minutes. Sprinkle with the \ufb02our, chili powder, cumin, cocoa powder, and cayenne. Cook, stirring briskly and constantly, 1 minute. Stir in the tomatoes and vinegar. Bring to a boil. Reduce the heat and simmer, stirring frequently, until thick, 30 - 40 minutes. Stir in the chickpeas. Simmer until heated through, about 5 minutes. Enc-Dec Heat oil in a large, heavy pot over medium heat. Add chicken and cook until browned on both sides, about 10 minutes, or until browned; drain. Add onion and garlic and saute for 2 minutes. Add chicken broth, chicken broth, and corn starch.",
      "Enc-Dec Heat oil in a large, heavy pot over medium heat. Add chicken and cook until browned on both sides, about 10 minutes, or until browned; drain. Add onion and garlic and saute for 2 minutes. Add chicken broth, chicken broth, and corn starch. Bring to a slow boil, reduce heat, cover and simmer for 30 minutes. Remove from heat and add the tomatoes and corn. Simmer until chicken is cooked through, about 10 minutes. Serve over rice or noodles. Prior Tech Heat oil in large saucepan over medium - high heat. Add chicken and cook until browned. Add bell pepper and onions, cook 2 minutes, stirring frequently. Add chicken broth. Cover, simmer, stirring occasionally, for 10 minutes or until vegetables are tender, stirring occasionally. Add chicken, cover, and cook 10 more minutes or until chicken is cooked through. Remove from heat. Stir in remaining ingredients. Season with salt and pepper to taste. Serve over rice and top with additional shredded cheese and cilantro. Prior Recipe Heat oil in large dutch oven over medium - high.",
      "Remove from heat. Stir in remaining ingredients. Season with salt and pepper to taste. Serve over rice and top with additional shredded cheese and cilantro. Prior Recipe Heat oil in large dutch oven over medium - high. Add chicken, bell peppers, bell peppers and onion. Saute 2 - 3 minutes. Add broth and tomato paste; stir. Bring to a boil. Reduce heat and simmer, covered, for 10 minutes or until rice is tender, stirring often. Stir in chicken. Cover and simmer for 10 minutes. Stir in parsley. Serve over rice. Garnish with cilantro. Prior Name Heat the oil in a large skillet over medium - high heat. Add the peppers and onions. Cook, stirring, until the vegetables are soft and beginning to soften, about 5 minutes. Stir in the tomatoes, corn, corn, and corn. Bring to a boil. Reduce heat to low, cover, and simmer for 10 minutes. Add the chicken and cook for another 10 minutes or until the vegetables are tender. Stir in the cilantro and serve.",
      "Stir in the tomatoes, corn, corn, and corn. Bring to a boil. Reduce heat to low, cover, and simmer for 10 minutes. Add the chicken and cook for another 10 minutes or until the vegetables are tender. Stir in the cilantro and serve. Table 7: Sample generated recipe \u201cChicken Bell Pepper Chili Weight Watchers\u201d for all models.",
      "Input Name: Cinna Nilla Waf\ufb02es Ingredients: \ufb02our, egg, milk, vegetable oil, sugar Calorie: Medium Gold Heat waf\ufb02e iron. Beat eggs in large bowl with hand beater until \ufb02uffy. Beat in remaining ingredients until smooth. Pour batter into waf\ufb02e iron and cook for approximately 5 minutes. Serve immediately or cool to pack into freezer bags and freeze. Enc-Dec Mix the \ufb02our, baking soda, and salt in a bowl. In a separate bowl, whisk together the milk, oil, eggs and sugar. Stir the \ufb02our mixture into the egg mixture, and continue to mix well. Add the \ufb02our to the egg mixture. Mix well and pour into a greased waf\ufb02e iron. Cook for 2 minutes, remove from heat and serve. Prior Tech In a medium bowl mix \ufb02our, eggs, and milk until combined. Add the dry ingredients and stir until just combined and do not mix.",
      "Cook for 2 minutes, remove from heat and serve. Prior Tech In a medium bowl mix \ufb02our, eggs, and milk until combined. Add the dry ingredients and stir until just combined and do not mix. Heat griddle over medium heat, add the oil, oil, and cook the pancakes until golden brown and cooked through. Serve with a little milk or cream. Enjoy Prior Recipe In a mixing bowl, whisk together the eggs, milk, oil, sugar, vanilla, salt and vanilla. Cover and let the mixture stand in the fridge for about 1 hour. Spoon batter into waf\ufb02e iron and close the grill. Prior Name Preheat waf\ufb02e iron. Beat together the eggs, milk and oil until well blended, add the vanilla and mix well with a mixer. Fold in \ufb02our, baking powder, and cinnamon. Spread 1 / 2 the mixture in a greased waf\ufb02e iron. Bake until golden brown, about 15 minutes per side. Sprinkle with powdered sugar and serve warm. Table 8: Sample generated waf\ufb02e recipe for all models.",
      "Figure 3: A sample question for coherence evaluation survey."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1909.00105.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":7865,
  "avg_doclen":170.9782608696,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1909.00105.pdf"
    }
  }
}