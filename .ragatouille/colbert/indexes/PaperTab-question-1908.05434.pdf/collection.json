[
  "Sex Traf\ufb01cking Detection with Ordinal Regression Neural Networks Longshaokan Wang1\u2217, Eric Laber2, Yeng Saanchi3, Sherrie Caltagirone4 1Alexa AI, Amazon, 23Department of Statistics, North Carolina State University, 4Global Emancipation Network 1longsha@amazon.com, 23{eblaber, ysaanch}@ncsu.edu, 4sherrie@globalemancipation.ngo Abstract Sex traf\ufb01cking is a global epidemic. Escort websites are a pri- mary vehicle for selling the services of such traf\ufb01cking vic- tims and thus a major driver of traf\ufb01cker revenue. Many law enforcement agencies do not have the resources to manually identify leads from the millions of escort ads posted across dozens of public websites. We propose an ordinal regression neural network to identify escort ads that are likely linked to sex traf\ufb01cking. Our model uses a modi\ufb01ed cost function to mitigate inconsistencies in predictions often associated with nonparametric ordinal regression and leverages recent ad- vancements in deep learning to improve prediction accuracy.",
  "Our model uses a modi\ufb01ed cost function to mitigate inconsistencies in predictions often associated with nonparametric ordinal regression and leverages recent ad- vancements in deep learning to improve prediction accuracy. The proposed method signi\ufb01cantly improves on the previ- ous state-of-the-art on Traf\ufb01cking-10K, an expert-annotated dataset of escort ads. Additionally, because traf\ufb01ckers use acronyms, deliberate typographical errors, and emojis to re- place explicit keywords, we demonstrate how to expand the lexicon of traf\ufb01cking \ufb02ags through word embeddings and t- SNE. 1 Introduction Globally, human traf\ufb01cking is one of the fastest growing crimes and, with annual pro\ufb01ts estimated to be in excess of 150 billion USD, it is also among the most lucrative (Amin 2010). Sex traf\ufb01cking is a form of human traf\ufb01cking which involves sexual exploitation through coercion.",
  "Sex traf\ufb01cking is a form of human traf\ufb01cking which involves sexual exploitation through coercion. Recent esti- mates suggest that nearly 4 million adults and 1 million chil- dren are being victimized globally on any given day; further- more, it is estimated that 99 percent of victims are female (International Labour Organization, Walk Free Foundation, and International Organization for Migration 2017). Escort websites are an increasingly popular vehicle for selling the services of traf\ufb01cking victims. According to a recent sur- vivor survey (THORN and Bouch\u00b4e 2018), 38% of underage traf\ufb01cking victims who were enslaved prior to 2004 were advertised online, and that number rose to 75% for those enslaved after 2004. Prior to its shutdown in April 2018, the website Backpage was the most frequently used online \u2217This work was done when Wang was a PhD student at North Carolina State University. Copyright c\u20dd2020, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved.",
  "Copyright c\u20dd2020, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved. advertising platform; other popular websites used for ad- vertising escort service include Craigslist, Redbook, Sugar- Daddy, and Facebook (THORN and Bouch\u00b4e 2018). Despite the seizure of Backpage, there were nearly 150,000 new on- line sex advertisements posted per day in the U.S. alone in late 2018 (Tarinelli 2018); even with many of these new ads being re-posts of existing ads and traf\ufb01ckers often post- ing multiple ads for the same victims (THORN and Bouch\u00b4e 2018), this volume is staggering. Because of their ubiquity and public access, escort web- sites are a rich resource for anti-traf\ufb01cking operations. How- ever, many law enforcement agencies do not have the re- sources to sift through the volume of escort ads to identify those coming from potential traf\ufb01ckers.",
  "Because of their ubiquity and public access, escort web- sites are a rich resource for anti-traf\ufb01cking operations. How- ever, many law enforcement agencies do not have the re- sources to sift through the volume of escort ads to identify those coming from potential traf\ufb01ckers. One scalable and ef- \ufb01cient solution is to build a statistical model to predict the likelihood of an ad coming from a traf\ufb01cker using a dataset annotated by anti-traf\ufb01cking experts. We propose an ordi- nal regression neural network tailored for text input. This model comprises three components: (i) a Word2Vec model (Mikolov et al. 2013b) that maps each word from the text input to a numeric vector, (ii) a gated-feedback recurrent neural network (Chung et al. 2015) that sequentially pro- cesses the word vectors, and (iii) an ordinal regression layer (Cheng, Wang, and Pollastri 2008) that produces a predicted ordinal label. We use a modi\ufb01ed cost function to mitigate inconsistencies in predictions associated with nonparamet- ric ordinal regression.",
  "We use a modi\ufb01ed cost function to mitigate inconsistencies in predictions associated with nonparamet- ric ordinal regression. We also leverage several regulariza- tion techniques for deep neural networks to further improve model performance, such as residual connection (He et al. 2016) and batch normalization (Ioffe and Szegedy 2015). We conduct our experiments on Traf\ufb01cking-10k (Tong et al. 2017), a dataset of escort ads for which anti-traf\ufb01cking experts assigned each sample one of seven ordered labels ranging from \u201c1: Very Unlikely (to come from traf\ufb01ckers)\u201d to \u201c7: Very Likely\u201d. Our proposed model signi\ufb01cantly out- performs previously published models (Tong et al. 2017) on Traf\ufb01cking-10k as well as a variety of baseline ordinal re- gression models. In addition, we analyze the emojis used in escort ads with Word2Vec and t-SNE (van der Maaten and Hinton 2008), and we show that the lexicon of traf\ufb01cking- related emojis can be subsequently expanded.",
  "In addition, we analyze the emojis used in escort ads with Word2Vec and t-SNE (van der Maaten and Hinton 2008), and we show that the lexicon of traf\ufb01cking- related emojis can be subsequently expanded. The main contributions of this paper are summarized as arXiv:1908.05434v2  [cs.LG]  12 Jan 2020",
  "follows: 1. We propose a neural network architecture for text data with ordinal labels, which outperforms the previ- ous state-of-the-art (Tong et al. 2017) on Traf\ufb01cking-10k. 2. We propose a simple penalty term in the cost function to mitigate the monotonicity violation and improve the in- terpretability of the output of the ordinal regression layer, where the monotonicity violation was previously deemed too computationally costly to resolve (Niu et al. 2016). 3. We provide a qualitative analysis on the top escort ads \ufb02agged by our model, which offers patterns that anti-traf\ufb01cking ex- perts can potentially con\ufb01rm or make use of. 4. We provide an emoji analysis that shows how to use unsupervised learn- ing techniques on the raw data to generate leads on new traf- \ufb01cking key words. 5. We open source our code base and trained model to encourage further research on traf\ufb01cking detection and to allow the law enforcement to make use of our research for free.",
  "5. We open source our code base and trained model to encourage further research on traf\ufb01cking detection and to allow the law enforcement to make use of our research for free. In Section 2, we discuss related work on human traf- \ufb01cking detection and ordinal regression. In Section 3, we present our proposed model and detail its components. In Section 4, we present the experimental results, including the Traf\ufb01cking-10K benchmark, a qualitative analysis of the predictions on raw data, and the emoji analysis. In Section 5, we summarize our \ufb01ndings and discuss future work. 2 Related Work Traf\ufb01cking detection: There have been several software products designed to aid anti-traf\ufb01cking efforts. Examples include Memex1 which focuses on search functionalities in the dark web; Spotlight2 which \ufb02ags suspicious ads and links images appearing in multiple ads; Traf\ufb01c Jam3 which seeks to identify patterns that connect multiple ads to the same traf\ufb01cking organization; and Traf\ufb01ckCam4 which aims to construct a crowd-sourced database of hotel room images to geo-locate victims.",
  "These research efforts have largely been isolated, and few research articles on machine learn- ing for traf\ufb01cking detection have been published. Closest to our work is the Human Traf\ufb01cking Deep Network (HTDN) (Tong et al. 2017). HTDN has three main components: a lan- guage network that uses pretrained word embeddings and a long short-term memory network (LSTM) to process text in- put; a vision network that uses a convolutional network to process image input; and another convolutional network to combine the output of the previous two networks and pro- duce a binary classi\ufb01cation. Compared to the language net- work in HTDN, our model replaces LSTM with a gated- feedback recurrent neural network, adopts certain regular- izations, and uses an ordinal regression layer on top. It sig- ni\ufb01cantly improves HTDN\u2019s benchmark despite only using text input. As in the work of Tong et al. (2017), we pre- train word embeddings using a skip-gram model (Mikolov et al.",
  "It sig- ni\ufb01cantly improves HTDN\u2019s benchmark despite only using text input. As in the work of Tong et al. (2017), we pre- train word embeddings using a skip-gram model (Mikolov et al. 2013b) applied to unlabeled data from escort ads, how- ever, we go further by analyzing the emojis\u2019 embeddings and thereby expand the traf\ufb01cking lexicon. 1darpa.mil/program/memex 2htspotlight.com 3marinusanalytics.com/traf\ufb01cjam 4traf\ufb01ckcam.com Ordinal regression: We brie\ufb02y review ordinal regression before introducing the proposed methodology. We assume that the training data are Dtrain = {(Xi, Yi)}n i=1, where Xi \u2208X are the features and Yi \u2208Y is the response; Y is the set of k ordered labels {1, 2, . . . , k} with 1 \u227a2 . . . \u227ak.",
  ". . , k} with 1 \u227a2 . . . \u227ak. Many ordinal regression methods learn a composite map \u03b7 = h \u25e6g, where g : X \u2192R and h : R \u2192{1, 2, . . . , k} have the interpretation that g(X) is a latent \u201cscore\u201d which is subsequently discretized into a category by h. \u03b7 is often estimated by empirical risk minimization, i.e., by minimiz- ing a loss function C{\u03b7(X), Y } averaged over the training data. Standard choices of \u03b7 and C are reviewed by Rennie and Srebro (2005). Another common approach to ordinal regression, which we adopt in our proposed method, is to transform the la- bel prediction into a series of k \u22121 binary classi\ufb01cation sub-problems, wherein the ith sub-problem is to predict whether the true label exceeds i (Frank and Hall 2001; Li and Lin 2006).",
  "For example, one might use a series of lo- gistic regression models to estimate the conditional proba- bilities fi(X) = P(Y > i \f\fX) for each i = 1, . . . , k \u22121. Cheng, Wang, and Pollastri (2008) estimated these prob- abilities jointly using a neural network; this was later ex- tended to image data (Niu et al. 2016) as well as text data (Irsoy and Cardie 2015; Ruder, Ghaffari, and Breslin 2016). However, as acknowledged by Cheng, Wang, and Pollastri (2008), the estimated probabilities need not respect the or- dering fi(X) \u2265fi+1(X) for all i and X. We force our estimator to respect this ordering through a penalty on its violation. 3 Method Our proposed ordinal regression model consists of the fol- lowing three components: Word embeddings pre-trained by a Skip-gram model, a gated-feedback recurrent neural net- work that constructs summary features from sentences, and a multi-labeled logistic regression layer tailored for ordinal regression. See Figure 1 for a schematic.",
  "See Figure 1 for a schematic. The details of its components and their respective alternatives are discussed below. 3.1 Word Embeddings Vector representations of words, also known as word em- beddings, can be obtained through unsupervised learning on a large text corpus so that certain linguistic regularities and patterns are encoded. Compared to Latent Semantic Analy- sis (Dumais 2004), embedding algorithms using neural net- works are particularly good at preserving linear regularities among words in addition to grouping similar words together (Mikolov et al. 2013a). Such embeddings can in turn help other algorithms achieve better performances in various nat- ural language processing tasks (Mikolov et al. 2013b). Unfortunately, the escort ads contain a plethora of emojis, acronyms, and (sometimes deliberate) typographical errors that are not encountered in more standard text data, which suggests that it is likely better to learn word embeddings from scratch on a large collection of escort ads instead of using previously published embeddings (Tong et al. 2017).",
  "Figure 1: Overview of the ordinal regression neural net- work for text input. H represents a hidden state in a gated- feedback recurrent neural network. We use 168,337 ads scraped from Backpage as our train- ing corpus and the Skip-gram model with Negative sampling (Mikolov et al. 2013b) as our model. 3.2 Gated-Feedback Recurrent Neural Network To process entire sentences and paragraphs after mapping the words to embeddings, we need a model to handle se- quential data. Recurrent neural networks (RNNs) have re- cently seen great success at modeling sequential data, es- pecially in natural language processing tasks (LeCun, Ben- gio, and Hinton 2015). On a high level, an RNN is a neural network that processes a sequence of inputs one at a time, taking the summary of the sequence seen so far from the previous time point as an additional input and producing a summary for the next time point.",
  "On a high level, an RNN is a neural network that processes a sequence of inputs one at a time, taking the summary of the sequence seen so far from the previous time point as an additional input and producing a summary for the next time point. One of the most widely used variations of RNNs, a Long short-term memory net- work (LSTM), uses various gates to control the information \ufb02ow and is able to better preserve long-term dependencies in the running summary compared to a basic RNN (see Good- fellow, Bengio, and Courville 2016 and references therein). In our implementation, we use a further re\ufb01nement of multi- layed LSTMs, Gated-feedback recurrent neural networks (GF-RNNs), which tend to capture dependencies across dif- ferent timescales more easily (Chung et al. 2015). Regularization techniques for neural networks including Dropout (Srivastava et al. 2014), Residual connection (He et al. 2016), and Batch normalization (Ioffe and Szegedy 2015) are added to GF-RNN for further improvements.",
  "2015). Regularization techniques for neural networks including Dropout (Srivastava et al. 2014), Residual connection (He et al. 2016), and Batch normalization (Ioffe and Szegedy 2015) are added to GF-RNN for further improvements. After GF-RNN processes an entire escort ad, the average of the hidden states of the last layer becomes the input for the multi-labeled logistic regression layer which we discuss next. 3.3 Multi-Labeled Logistic Regression Layer As noted previously, the ordinal regression problem can be cast into a series of binary classi\ufb01cation problems and thereby utilize the large repository of available classi\ufb01cation algorithms (Frank and Hall 2001; Li and Lin 2006; Niu et al. 2016). One formulation is as follows. Given k total ranks, the i-th binary classi\ufb01er is trained to predict the probability that a sample X has rank larger than i : bfi(X) = bP(Y > i|X). Then the predicted rank is bY = 1 + k\u22121 X i=1 Round n bfi(X) o .",
  "Then the predicted rank is bY = 1 + k\u22121 X i=1 Round n bfi(X) o . In a classi\ufb01cation task, the \ufb01nal layer of a deep neu- ral network is typically a softmax layer with dimension equal to the number of classes (Goodfellow, Bengio, and Courville 2016). Using the ordinal-regression-to-binary- classi\ufb01cations formulation described above, Cheng, Wang, and Pollastri (2008) replaced the softmax layer in their neu- ral network with a (k\u22121)-dimensional sigmoid layer, where each neuron serves as a binary classi\ufb01er (see Figure 2 but without the order penalty to be discussed later).",
  "With the sigmoid activation function, the output of the ith neuron can be viewed as the predicted probability that the sample has rank greater5 than i. Alternatively, the entire sig- moid layer can be viewed as performing multi-labeled lo- gistic regression, where the ith label is the indicator of the sample\u2019s rank being greater than i. The training data are thus re-formatted accordingly so that response variable for a sample with rank i becomes (1\u22ba i\u22121, 0\u22ba k\u2212i)\u22ba. The k \u22121 binary classi\ufb01ers share the features constructed by the ear- lier layers of the neural network and can be trained jointly with mean squared error loss. A key difference between the multi-labeled logistic regression and the naive classi\ufb01cation (ignoring the order and treating all ranks as separate classes) is that the loss for bY \u0338= Y is constant in the naive classi\ufb01ca- tion but proportional to |bY \u2212Y | in the multi-labeled logistic regression. Cheng, Wang, and Pollastri\u2019s (2008) \ufb01nal layer was pre- ceded by a simple feed-forward network.",
  "Cheng, Wang, and Pollastri\u2019s (2008) \ufb01nal layer was pre- ceded by a simple feed-forward network. In our case, word embeddings and GF-RNN allow us to construct a feature vector of \ufb01xed length from text input, so we can simply at- tach the multi-labeled logistic regression layer to the output of GF-RNN to complete an ordinal regression neural net- work for text input. The violation of the monotonicity in the estimated prob- abilities (e.g., bfi(X) < bfi+1(X) for some X and i) has remained an open issue since the original ordinal regres- sion neural network proposal of Cheng, Wang, and Pollastri (2008). This is perhaps owed in part to the belief that correct- ing this issue would signi\ufb01cantly increase training complex- ity (Niu et al. 2016).",
  "This is perhaps owed in part to the belief that correct- ing this issue would signi\ufb01cantly increase training complex- ity (Niu et al. 2016). We propose an effective and computa- tionally ef\ufb01cient solution to avoid the con\ufb02icting predictions 5Actually, in Cheng, Wang, and Pollastri\u2019s original formulation, the \ufb01nal layer is k-dimensional with the i-th neuron predicting the probability that the sample has rank greater than or equal to i. This is redundant because the \ufb01rst neuron should always be equal to 1. Hence we make the slight adjustment of using only k \u22121 neurons.",
  "Sigmoid Order  Penalty Figure 2: Ordinal regression layer with order penalty. as follows: penalize such con\ufb02icts in the training phase by adding P(X; \u03bb) = \u03bb k\u22122 X i=1 max n bfi+1(X) \u2212bfi(X), 0 o to the loss function for a sample X, where \u03bb is a penalty parameter (Figure 2). For suf\ufb01ciently large \u03bb the estimated probabilities will respect the monotonicity condition; re- specting this condition improves the interpretability of the predictions, which is vital in applications like the one we consider here as stakeholders are given the estimated prob- abilities. We also hypothesize that the order penalty may serve as a regularizer to improve each binary classi\ufb01er (see the ablation test in Section 4.3). All three components of our model (word embeddings, GF-RNN, and multi-labeled logistic regression layer) can be trained jointly, with word embeddings optionally held \ufb01xed or given a smaller learning rate for \ufb01ne-tuning. The hyperpa- rameters for all components are given in the Appendix. They are selected according to either literature or grid-search.",
  "The hyperpa- rameters for all components are given in the Appendix. They are selected according to either literature or grid-search. 4 Experiments We \ufb01rst describe the datasets we use to train and evaluate our models. Then we present a detailed comparison of our pro- posed model with commonly used ordinal regression models as well as the previous state-of-the-art classi\ufb01cation model by Tong et al. (2017). To assess the effect of each component in our model, we perform an ablation test where the compo- nents are swapped by their more standard alternatives one at a time. Next, we perform a qualitative analysis on the model predictions on the raw data, which are scraped from a dif- ferent escort website than the one that provides the labeled training data. Finally, we conduct an emoji analysis using the word embeddings trained on raw escort ads. 4.1 Datasets We use raw texts scraped from Backpage and TNABoard to pre-train the word embeddings, and use the same la- beled texts Tong et al. (2017) used to conduct model com- parisons.",
  "4.1 Datasets We use raw texts scraped from Backpage and TNABoard to pre-train the word embeddings, and use the same la- beled texts Tong et al. (2017) used to conduct model com- parisons. The raw text dataset consists of 44,105 ads from TNABoard and 124,220 ads from Backpage. Data clean- ing/preprocessing includes joining the title and the body of an ad; adding white spaces around every emoji so that it can be tokenized properly; stripping tabs, line breaks, punctua- tions, and extra white spaces; removing phone numbers; and converting all letters to lower case. We have ensured that the raw dataset has no overlap with the labeled dataset to avoid bias in test accuracy. While it is possible to scrape more raw data, we did not observe signi\ufb01cant improvements in model performances when the size of raw data increased from \u223c70,000 to \u223c170,000, hence we assume that the cur- rent raw dataset is suf\ufb01ciently large. The labeled dataset is called Traf\ufb01cking-10k.",
  "The labeled dataset is called Traf\ufb01cking-10k. It consists of 12,350 ads from Backpage labeled by experts in human traf\ufb01cking detection6 (Tong et al. 2017). Each label is one of seven ordered levels of likelihood that the corresponding ad comes from a human traf\ufb01cker. Descriptions and sam- ple proportions of the labels are in Table 1. The original Traf\ufb01cking-10K includes both texts and images, but as men- tioned in Section 1, only the texts are used in our case. We apply the same preprocessing to Traf\ufb01cking-10k as we do to raw data.",
  "The original Traf\ufb01cking-10K includes both texts and images, but as men- tioned in Section 1, only the texts are used in our case. We apply the same preprocessing to Traf\ufb01cking-10k as we do to raw data. 4.2 Comparison with Baselines We compare our proposed ordinal regression neural net- work (ORNN) to Immediate-Threshold ordinal logistic re- gression (IT) (Rennie and Srebro 2005), All-Threshold or- dinal logistic regression (AT) (Rennie and Srebro 2005), Least Absolute Deviation (LAD) (Bloom\ufb01eld and Steiger 1980; Narula and Wellington 1982), and multi-class logis- tic regression (MC) which ignores the ordering. The pri- mary evaluation metrics are Mean Absolute Error (MAE) and macro-averaged Mean Absolute Error (MAEM) (Bac- cianella, Esuli, and Sebastiani 2009).",
  "The pri- mary evaluation metrics are Mean Absolute Error (MAE) and macro-averaged Mean Absolute Error (MAEM) (Bac- cianella, Esuli, and Sebastiani 2009). To compare our model with the previous state-of-the-art classi\ufb01cation model for escort ads, the Human Traf\ufb01cking Deep Network (HTDN) (Tong et al. 2017), we also polarize the true and predicted labels into two classes, \u201c1-4: Unlikely\u201d and \u201c5-7: Likely\u201d; then we compute the binary classi\ufb01cation accuracy (Acc.) as well as the weighted binary classi\ufb01cation accuracy (Wt. Acc.) given by Wt. Acc. = 1 2 \u0010 True Positives Total Positives + True Negatives Total Negatives \u0011 . Note that for applications in human traf\ufb01cking detection, MAE and Acc. are of primary interest. Whereas for a more general comparison among the models, the class imbalance robust metrics, MAEM and Wt. Acc., might be more suit- able.",
  "Note that for applications in human traf\ufb01cking detection, MAE and Acc. are of primary interest. Whereas for a more general comparison among the models, the class imbalance robust metrics, MAEM and Wt. Acc., might be more suit- able. Bootstrapping or increasing the weight of samples in smaller classes can improve MAEM and Wt. Acc. at the cost of MAE and Acc.. The text data need to be vectorized before they can be fed into the baseline models (whereas vectorization is built into 6 Backpage was seized by FBI in April 2018, but we have ob- served that escort ads across different websites are often similar, and a survivor survey shows that traf\ufb01ckers post their ads on multi- ple websites (THORN and Bouch\u00b4e 2018). Thus, we argue that the training data from Backpage are still useful, which is empirically supported by our qualitative analysis in Section 4.4.",
  "Label 1 2 3 4 5 6 7 Description Strongly Unlikely Slightly Unsure Weakly Likely Strongly Unlikely Unlikely Likely Likely Count 1,977 1,904 3,619 796 3,515 457 82 Table 1: Description and distribution of labels in Traf\ufb01cking-10K. ORNN). The standard practice is to tokenize the texts us- ing n-grams and then create weighted term frequency vec- tors using the term frequency (TF)-inverse document fre- quency (IDF) scheme (Beel et al. 2016; Manning, Ragha- van, and Sch\u00a8utze 2009). The speci\ufb01c variation we use is the recommended unigram + sublinear TF + smooth IDF (Manning, Raghavan, and Sch\u00a8utze 2009; Pedregosa et al. 2011).",
  "The speci\ufb01c variation we use is the recommended unigram + sublinear TF + smooth IDF (Manning, Raghavan, and Sch\u00a8utze 2009; Pedregosa et al. 2011). Dimension reduction techniques such as Latent Se- mantic Analysis (Dumais 2004) can be optionally applied to the frequency vectors, but Schuller, Mousa, and Vrynio- tis (2015) concluded from their experiments that dimension reduction on frequency vectors actually hurts model perfor- mance, which our preliminary experiments agree with. All models are trained and evaluated using the same (w.r.t. data shuf\ufb02e and split) 10-fold cross-validation (CV) on Traf\ufb01cking-10k, except for HTDN, whose result is read from the original paper (Tong et al. 2017)7. During each train-test split, 2/9 of the training set is further reserved as the validation set for tuning hyperparameters such as L2- penalty in IT, AT and LAD, and learning rate in ORNN.",
  "2017)7. During each train-test split, 2/9 of the training set is further reserved as the validation set for tuning hyperparameters such as L2- penalty in IT, AT and LAD, and learning rate in ORNN. So the overall train-validation-test ratio is 70%-20%-10%. We report the mean metrics from the CV in Table 2. As previous research has pointed out that there is no unbiased estima- tor of the variance of CV (Bengio and Grandvalet 2004), we report the naive standard error treating metrics across CV as independent. Recall that a 95% con\ufb01dence interval is roughly the point estimate \u00b1 1.96 \u00d7 the standard error. We can see that ORNN has the best MAE, MAEM and Acc. as well as a close 2nd best Wt. Acc. among all models. Its Wt. Acc. is a substantial improvement over HTDN de- spite the fact that the latter use both text and image data.",
  "as well as a close 2nd best Wt. Acc. among all models. Its Wt. Acc. is a substantial improvement over HTDN de- spite the fact that the latter use both text and image data. It is important to note that HTDN is trained using binary labels, whereas the other models are trained using ordinal labels and then have their ordinal predictions converted to binary predictions. This is most likely the reason that even the base- line models except for LAD can yield better Wt. Acc. than HTDN, con\ufb01rming our earlier claim that polarizing the or- dinal labels during training may lead to information loss. 4.3 Ablation Test To ensure that we do not unnecessarily complicate our ORNN model, and to assess the impact of each compo- nent on the \ufb01nal model performance, we perform an ab- lation test. Using the same CV and evaluation metrics, we make the following replacements separately and re-evaluate the model: 1.",
  "Using the same CV and evaluation metrics, we make the following replacements separately and re-evaluate the model: 1. Replace word embeddings pre-trained from skip-gram model with randomly initialized word embed- dings; 2. replace gated-feedback recurrent neural network 7The authors of HTDN used a single train-validation-test split instead of CV. with long short-term memory network (LSTM); 3. disable batch normalization; 4. disable residual connection; 5. re- place the multi-labeled logistic regression layer with a soft- max layer (i.e., let the model perform classi\ufb01cation, treating the ordinal response variable as a categorical variable with k classes); 6. replace the multi-labeled logistic regression layer with a 1-dimensional linear layer (i.e., let the model perform regression, treating the ordinal response variable as a continuous variable) and round the prediction to the near- est integer during testing; 7. set the order penalty to 0. The results are shown in Table 3. The proposed ORNN once again has all the best met- rics except for Wt. Acc. which is the 2nd best.",
  "The results are shown in Table 3. The proposed ORNN once again has all the best met- rics except for Wt. Acc. which is the 2nd best. Note that if we disregard the ordinal labels and perform classi\ufb01cation or regression, MAE falls off by a large margin. Setting order penalty to 0 does not deteriorate the performance by much, however, the percent of con\ufb02icting binary predictions (see Section 3.3) rises from 1.4% to 5.2%. So adding an order penalty helps produce more interpretable results8. 4.4 Qualitative Analysis of Predictions To qualitatively evaluate how well our model predicts on raw data and observe potential patterns in the \ufb02agged samples, we obtain predictions on the 44,105 unlabelled ads from TNABoard with the ORNN model trained on Traf\ufb01cking- 10k, then we examine the samples with high predicted like- lihood to come from traf\ufb01ckers.",
  "Below are the top three sam- ples that the model considers likely: \u2022 \u201camazing reviewed crystal only here till fri book now please check our site for the services the girls provide all updates specials photos rates reviews njfantasygirls ...look who s back amazing reviewed model saman- tha...brand new spinner jessica special rate today 250 hr 21 5 4 120 34b total gfe total anything goes no limits...\u201d \u2022 \u201c2 hot toght 18y o spinners 4 amazing providers today specials...\u201d \u2022 \u201casian college girl is visiting bellevue service type escort hair color brown eyes brown age 23 height 5 4 body type slim cup size c cup ethnicity asian service type escort i am here for you settle men i am a tiny asian girl who is waiting for a gentlemen...\u201d Some interesting patterns in the samples with high predicted likelihood (here we only showed three) include: mentioning of multiple names or > 1 providers in a single ad; possibly intentional typos and abbreviations for the sensitive words such as \u201ctight\u201d \u2192\u201ctoght\u201d and \u201c18 year old\u201d \u2192\u201c18y o\u201d;",
  "possibly intentional typos and abbreviations for the sensitive words such as \u201ctight\u201d \u2192\u201ctoght\u201d and \u201c18 year old\u201d \u2192\u201c18y o\u201d; 8It is possible to increase the order penalty to further reduce or eliminate con\ufb02icting predictions, but we \ufb01nd that a large order penalty harms model performance.",
  "Model MAE MAEM Acc. Wt. Acc. ORNN 0.769 (0.009) 1.238 (0.016) 0.818 (0.003) 0.772 (0.004) IT 0.807 (0.010) 1.244 (0.011) 0.801 (0.003) 0.781 (0.004) AT 0.778 (0.009) 1.246 (0.012) 0.813 (0.003) 0.755 (0.004) LAD 0.829 (0.008) 1.298 (0.016) 0.786 (0.004) 0.686 (0.003) MC 0.794 (0.012) 1.286 (0.018) 0.804 (0.003) 0.767 (0.004) HTDN - - 0.800 0.753 Table 2: Comparison of the proposed ordinal regression neural network (ORNN) against Immediate-Threshold ordinal logistic regression (IT), All-Threshold ordinal logistic regression (AT),",
  "804 (0.003) 0.767 (0.004) HTDN - - 0.800 0.753 Table 2: Comparison of the proposed ordinal regression neural network (ORNN) against Immediate-Threshold ordinal logistic regression (IT), All-Threshold ordinal logistic regression (AT), Least Absolute Deviation (LAD), multi-class logistic regression (MC), and the Human Traf\ufb01cking Deep Network (HTDN) in terms of Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM), binary classi\ufb01cation accuracy (Acc.) and weighted binary classi\ufb01cation accuracy (Wt. Acc.). The results are averaged across 10-fold CV on Traf\ufb01cking-10k with naive standard errors in the parentheses. The best and second best results are highlighted. Model MAE MAEM Acc. Wt. Acc. 0. Proposed ORNN 0.769 (0.009) 1.238 (0.016) 0.818 (0.003) 0.772 (0.004) 1.",
  "Model MAE MAEM Acc. Wt. Acc. 0. Proposed ORNN 0.769 (0.009) 1.238 (0.016) 0.818 (0.003) 0.772 (0.004) 1. Random Embeddings 0.789 (0.007) 1.254 (0.013) 0.810 (0.002) 0.757 (0.003) 2. LSTM 0.778 (0.009) 1.261 (0.021) 0.815 (0.003) 0.764 (0.003) 3. No Batch Norm. 0.780 (0.009) 1.311 (0.013) 0.815 (0.003) 0.754 (0.004) 4. No Res. Connect. 0.775 (0.008) 1.271 (0.020) 0.816 (0.003) 0.766 (0.004) 5.",
  "No Res. Connect. 0.775 (0.008) 1.271 (0.020) 0.816 (0.003) 0.766 (0.004) 5. Classi\ufb01cation 0.785 (0.012) 1.253 (0.017) 0.812 (0.004) 0.780 (0.004) 6. Regression 0.850 (0.009) 1.279 (0.016) 0.784 (0.004) 0.686 (0.006) 7. No Order Penalty 0.769 (0.009) 1.251 (0.016) 0.818 (0.003) 0.769 (0.004) Table 3: Ablation test. Except for models everything is the same as Table 2. keywords that indicate traveling of the providers such as \u201ctill fri\u201d, \u201clook who s back\u201d, and \u201cvisiting\u201d; keywords that hint on the providers potentially being underage such as \u201c18y o\u201d, \u201ccollege girl\u201d, and \u201ctiny\u201d; and switching between third person and \ufb01rst person narratives.",
  "4.5 Emoji Analysis The \ufb01ght against human traf\ufb01ckers is adversarial and dy- namic. Traf\ufb01ckers often avoid using explicit keywords when advertising victims, but instead use acronyms, intentional ty- pos, and emojis (Tong et al. 2017). Law enforcement main- tains a lexicon of traf\ufb01cking \ufb02ags mapping certain emojis to their potential true meanings (e.g., the cherry emoji can indicate an underaged victim), but compiling such a lexicon manually is expensive, requires frequent updating, and re- lies on domain expertise that is hard to obtain (e.g., insider information from traf\ufb01ckers or their victims). To make mat- ters worse, traf\ufb01ckers change their dictionaries over time and regularly switch to new emojis to replace certain keywords (Tong et al. 2017). In such a dynamic and adversarial envi- ronment, the need for a data-driven approach in updating the existing lexicon is evident. As mentioned in Section 3.1, training a skip-gram model on a text corpus can map words (including emojis) used in similar contexts to similar numeric vectors.",
  "As mentioned in Section 3.1, training a skip-gram model on a text corpus can map words (including emojis) used in similar contexts to similar numeric vectors. Besides using the vectors learned from the raw escort ads to train ORNN, we can directly visualize the vectors for the emojis to help identify their relationships, by mapping the vectors to a 2- dimensional space using t-SNE9 (van der Maaten and Hin- ton 2008) (Figure 3). We can \ufb01rst empirically assess the quality of the emoji map by noting that similar emojis do seem clustered to- gether: the smileys near the coordinate (2, 3), the \ufb02owers near (-6, -1), the heart shapes near (-8, 1), the phones near (-2, 4) and so on. It is worth emphasizing that the skip-gram model learns the vectors of these emojis based on their con- texts in escort ads and not their visual representations, so the fact that the visually similar emojis are close to one another in the map suggests that the vectors have been learned as desired.",
  "It is worth emphasizing that the skip-gram model learns the vectors of these emojis based on their con- texts in escort ads and not their visual representations, so the fact that the visually similar emojis are close to one another in the map suggests that the vectors have been learned as desired. The emoji map can assist anti-traf\ufb01cking experts in ex- panding the existing lexicon of traf\ufb01cking \ufb02ags. For ex- ample, according to the lexicon we obtained from Global Emancipation Network10, the cherry emoji and the lollipop emoji are both \ufb02ags for underaged victims.",
  "For ex- ample, according to the lexicon we obtained from Global Emancipation Network10, the cherry emoji and the lollipop emoji are both \ufb02ags for underaged victims. Near (-3, -4) in the map, right next to these two emojis are the porce- lain dolls emoji, the grapes emoji, the strawberry emoji, the candy emoji, the ice cream emojis, and maybe the 18-slash emoji, indicating that they are all used in similar contexts and perhaps should all be \ufb02ags for underaged victims in the 9t-SNE is known to produce better 2-dimensional visualizations than other dimension reduction techniques such as Principal Com- ponent Analysis, Multi-dimensional Scaling, and Local Linear Em- bedding (van der Maaten and Hinton 2008). 10Global Emancipation Network is a non-pro\ufb01t organization dedicated to combating human traf\ufb01cking. For more information see https://www.globalemancipation.ngo.",
  "dimension 2 dimension 1 Figure 3: Emoji map produced by applying t-SNE to the emojis\u2019 vectors learned from escort ads using skip-gram model. For visual clarity, only the emojis that appeared most frequently in the escort ads we scraped are shown out of the total 968 emojis that appeared. updated lexicon. If we re-train the skip-gram model and update the emoji map periodically on new escort ads, when traf\ufb01ckers switch to new emojis, the map can link the new emojis to the old ones, assisting anti-traf\ufb01cking experts in expanding the lexicon of traf\ufb01cking \ufb02ags. This approach also works for acronyms and deliberate typos. 5 Discussion Human traf\ufb01cking is a form of modern day slavery that vic- timizes millions of people. It has become the norm for sex traf\ufb01ckers to use escort websites to openly advertise their victims. We designed an ordinal regression neural network (ORNN) to predict the likelihood that an escort ad comes from a traf\ufb01cker, which can drastically narrow down the set of possible leads for law enforcement.",
  "We designed an ordinal regression neural network (ORNN) to predict the likelihood that an escort ad comes from a traf\ufb01cker, which can drastically narrow down the set of possible leads for law enforcement. Our ORNN achieved the state-of-the-art performance on Traf\ufb01cking-10K (Tong et al. 2017), outperforming all baseline ordinal regression models as well as improving the classi\ufb01cation accuracy over the Human Traf\ufb01cking Deep Network (Tong et al. 2017). We also conducted an emoji analysis and showed how to use word embeddings learned from raw text data to help expand the lexicon of traf\ufb01cking \ufb02ags. Since our experiments, there have been considerable ad- vancements in language representation models, such as BERT (Devlin et al. 2018). The new language representa- tion models can be combined with our ordinal regression layer, replacing the skip-gram model and GF-RNN, to po- tentially further improve our results.",
  "2018). The new language representa- tion models can be combined with our ordinal regression layer, replacing the skip-gram model and GF-RNN, to po- tentially further improve our results. However, our contri- butions of improving the cost function for ordinal regres- sion neural networks, qualitatively analyzing patterns in the predicted samples, and expanding the traf\ufb01cking lexicon through a data-driven approach are not dependent on a par- ticular choice of language representation model. As for future work in traf\ufb01cking detection, we can design multi-modal ordinal regression networks that utilize both image and text data. But given the time and resources re- quired to label escort ads, we may explore more unsuper- vised learning or transfer learning algorithms, such as using object detection (Ren et al. 2015) and matching algorithms to match hotel rooms in the images. Acknowledgments We thank Cara Jones and Marinus Analytics LLC for shar- ing the Traf\ufb01cking-10K dataset. We thank Praveen Bod- igutla for his suggestions on Natural Language Processing literature.",
  "Supplemental Materials Hyperparameters of the Proposed Ordinal Regression Neural Network Word Embeddings: speedup method: negative sampling; number of negative samples: 100; noise distribution: uni- gram distribution raised to 3/4rd; batch size: 16; window size: 5; minimum word count: 5; number of epochs: 50; em- bedding size: 128; pretraining learning rate: 0.2; \ufb01ne-tuning learning rate scale: 1.0. GF-RNN: hidden size: 128; dropout: 0.2; number of layers: 3; gradient clipping norm: 0.25; L2 penalty: 0.00001; learn- ing rate decay factor: 2.0; learning rate decay patience: 3; early stop patience: 9; batch size: 200; output layer type: mean-pooling; minimum word count: 5; maximum input length: 120. Multi-labeled Logistic Regression Layer: task weight scheme: uniform; con\ufb02ict penalty: 0.5.",
  "Multi-labeled Logistic Regression Layer: task weight scheme: uniform; con\ufb02ict penalty: 0.5. Access to the Source Materials The \ufb01ght against human traf\ufb01cking is adversarial, hence the access to the source materials in anti-traf\ufb01cking research is typically not available to the general public by choice, but granted to researchers and law enforcement individually upon request. Source code: https://gitlab.com/BlazingBlade/Traf\ufb01cKill Traf\ufb01cking-10k: cara@marinusanalytics.com Traf\ufb01cking lexicon: sherrie@globalemancipation.ngo References [Amin 2010] Amin, S. 2010. A step towards modeling and destabilizing human traf\ufb01cking networks using machine learning methods. Conference: Arti\ufb01cial intelligence for de- velopment, papers from the 2010 AAAI Spring Symposium, Techinical Report SS10-01 (pp. 2-7), Stanford. [Baccianella, Esuli, and Sebastiani 2009] Baccianella, S.; Esuli, A.; and Sebastiani, F.",
  "2-7), Stanford. [Baccianella, Esuli, and Sebastiani 2009] Baccianella, S.; Esuli, A.; and Sebastiani, F. 2009. Evaluation measures for ordinal regression. 9th International Conference on Intelligent Systems Design and Applications. [Beel et al. 2016] Beel, J.; Gipp, B.; Langer, S.; and Bre- itinger, C. 2016. Research-paper recommender systems: a literature survey. International Journal on Digital Libraries 17(4):305\u2013338. [Bengio and Grandvalet 2004] Bengio, Y., and Grandvalet, Y. 2004. No unbiased estimator of the variance of k-fold cross- validation. Journal of Machine Learning Research 5:1089\u2013 1105. [Bloom\ufb01eld and Steiger 1980] Bloom\ufb01eld, P., and Steiger, W. 1980. Least absolute deviations curve-\ufb01tting. SIAM Journal on Scienti\ufb01c and Statistical Computing 1(2):290\u2013301.",
  "[Bloom\ufb01eld and Steiger 1980] Bloom\ufb01eld, P., and Steiger, W. 1980. Least absolute deviations curve-\ufb01tting. SIAM Journal on Scienti\ufb01c and Statistical Computing 1(2):290\u2013301. [Cheng, Wang, and Pollastri 2008] Cheng, J.; Wang, Z.; and Pollastri, G. 2008. A neural network approach to ordinal re- gression. 2008 IEEE International Joint Conference on Neu- ral Networks (IEEE World Congress on Computational Intel- ligence) 1279\u20131284. [Chung et al. 2015] Chung, J.; Gulcehre, C.; Cho, K.; and Bengio, Y. 2015. Gated feedback recurrent neural networks. ICML-15. [Devlin et al. 2018] Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018. Bert: Pre-training of deep bidi- rectional transformers for language understanding. CoRR abs/1810.04805.",
  "2018] Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018. Bert: Pre-training of deep bidi- rectional transformers for language understanding. CoRR abs/1810.04805. [Dumais 2004] Dumais, S. 2004. Latent semantic analy- sis. Annual Review of Information Science and Technology 38(1):188\u2013230. [Fan et al. 2008] Fan, R.; Chang, K.; Hsieh, C.; Wang, X.; and Lin, C. 2008. Liblinear: A library for large linear classi\ufb01- cation. The Journal of Machine Learning Research 9:1871\u2013 1874. [Frank and Hall 2001] Frank, E., and Hall, M. 2001. A simple approach to ordinal classi\ufb01cation. Lecture Notes in Arti\ufb01cial Intelligence 145\u2013156. [Goodfellow, Bengio, and Courville 2016] Goodfellow, I.; Bengio, Y.; and Courville, A.",
  "A simple approach to ordinal classi\ufb01cation. Lecture Notes in Arti\ufb01cial Intelligence 145\u2013156. [Goodfellow, Bengio, and Courville 2016] Goodfellow, I.; Bengio, Y.; and Courville, A. 2016. Deep Learning. MIT Press. [Graves, Fern\u00b4andez, and Schmidhuber 2005] Graves, A.; Fern\u00b4andez, S.; and Schmidhuber, J. 2005. Bidirectional lstm networks for improved phoneme classi\ufb01cation and recognition. Proc. Int\u2019l Conf. Arti\ufb01cial Neural Networks 799\u2013804. [He et al. 2016] He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learning for image recognition. CVPR. [Ho and Lin 2012] Ho, C., and Lin, C. 2012. Large-scale lin- ear support vector regression. The Journal of Machine Learn- ing Research 13(1):3323\u20133348.",
  "Deep residual learning for image recognition. CVPR. [Ho and Lin 2012] Ho, C., and Lin, C. 2012. Large-scale lin- ear support vector regression. The Journal of Machine Learn- ing Research 13(1):3323\u20133348. [International Labour Organization, Walk Free Foundation, and Internationa International Labour Organization; Walk Free Foundation; and International Organization for Migration. 2017. Global estimates of modern slavery: forced labour and forced mar- riage. Geneva: International Labour Organization. ISBN: 978-92-2-130131-8. [Ioffe and Szegedy 2015] Ioffe, S., and Szegedy, C. 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift. ICML. [Irsoy and Cardie 2015] Irsoy, O., and Cardie, C. 2015. Mod- eling compositionality with multiplicative recurrent neural networks. ICLR. [Kim 2014] Kim, Y. 2014.",
  "[Irsoy and Cardie 2015] Irsoy, O., and Cardie, C. 2015. Mod- eling compositionality with multiplicative recurrent neural networks. ICLR. [Kim 2014] Kim, Y. 2014. Convolutional neural networks for sentence classi\ufb01cation. In Proceedings of the 2014 Confer- ence on Empirical Methods in Natural Language Processing (EMNLP), 1746\u20131751. [LeCun, Bengio, and Hinton 2015] LeCun, Y.; Bengio, Y.; and Hinton, G. 2015. Deep learning. Nature 521:436\u2013444. [Li and Lin 2006] Li, L., and Lin, H. 2006. Ordinal regression by extended binary classi\ufb01cation. NIPS 865\u2013872. [Manning, Raghavan, and Sch\u00a8utze 2009] Manning, C.; Raghavan, P.; and Sch\u00a8utze, H. 2009. An Introduction to Information Retrieval. Cambridge University Press.",
  "NIPS 865\u2013872. [Manning, Raghavan, and Sch\u00a8utze 2009] Manning, C.; Raghavan, P.; and Sch\u00a8utze, H. 2009. An Introduction to Information Retrieval. Cambridge University Press. [Mikolov et al. 2013a] Mikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013a. Ef\ufb01cient estimation of word representa- tions in vector space. ICLR Workshop Papers. [Mikolov et al. 2013b] Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G.; and Dean, J. 2013b. Distributed representa-",
  "tions of words and phrases and their compositionality. NIPS 3111\u20133119. [Narula and Wellington 1982] Narula, S., and Wellington, J. 1982. The minimum sum of absolute errors regression: A state of the art survey. International Statistical Review 317\u2013 326. [Niu et al. 2016] Niu, Z.; Zhou, M.; Wang, L.; Gao, X.; and Hua, G. 2016. Ordinal regression with multiple output cnn for age estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 4920\u20134928. [Pedregosa et al. 2011] Pedregosa, F.; Varoquaux, G.; Gram- fort, A.; Michel, V.; Thirion, B.; Grisel, O.; Blondel, M.; Pret- tenhofer, P.; Weiss, R.; Dubourg, V.; Vanderplas, J.; Passos, A.; Cournapeau, D.; Brucher, M.; Perrot, M.; and Duchesnay, E. 2011.",
  "2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research 12:2825\u20132830. [Pedregosa-Izquierdo 2015] Pedregosa-Izquierdo, F. 2015. Feature extraction and supervised learning on fMRI: from practice to theory. Ph.D. Dissertation, Universit\u00b4e Pierre et Marie Curie, Paris VI. [Ren et al. 2015] Ren, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster r-cnn: Towards real-time object detection with region proposal networks. NIPS. [Rennie and Srebro 2005] Rennie, J., and Srebro, N. 2005. Loss functions for preference levels: Regression with dis- crete ordered labels. In Proc. Int\u2019l Joint Conf. Arti\ufb01cial Intel- ligence Multidisciplinary Workshop Advances in Preference Handling. [Rosenthal, Farra, and Nakov 2017] Rosenthal, S.; Farra, N.; and Nakov, P.",
  "In Proc. Int\u2019l Joint Conf. Arti\ufb01cial Intel- ligence Multidisciplinary Workshop Advances in Preference Handling. [Rosenthal, Farra, and Nakov 2017] Rosenthal, S.; Farra, N.; and Nakov, P. 2017. Semeval-2017 task 4: Sentiment analysis in twitter. In Proceedings of the 11th International Workshop on Semantic Evaluation, volume 3 of 4, 502\u2013518. [Ruder, Ghaffari, and Breslin 2016] Ruder, S.; Ghaffari, P.; and Breslin, J. 2016. Insight-1 at semeval-2016 task 4: Con- volutional neural networks for sentiment classi\ufb01cation and quanti\ufb01cation. In Proceedings of the 10th International Work- shop on Semantic Evaluation (SemEval 2016). [Schuller, Mousa, and Vryniotis 2015] Schuller, B.; Mousa, A.; and Vryniotis, V. 2015.",
  "[Schuller, Mousa, and Vryniotis 2015] Schuller, B.; Mousa, A.; and Vryniotis, V. 2015. Sentiment analysis and opin- ion mining: on optimal parameters and performances. WIREs Data Mining Knowl. Discov. 5:255\u2013263. [Socher et al. 2013] Socher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning, C.; Ng, A.; and Potts, C. 2013. Recur- sive deep models for semantic compositionality over a senti- ment treebank. In Proceedings of EMNLP. [Srivastava et al. 2014] Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from over\ufb01tting. Journal of Machine Learning Research 15:1929\u20131958.",
  "2014. Dropout: A simple way to prevent neural networks from over\ufb01tting. Journal of Machine Learning Research 15:1929\u20131958. [Tarinelli 2018] Tarinelli, R. 2018. Online sex ads rebound, months after shutdown of backpage. The Associated Press. [THORN and Bouch\u00b4e 2018] THORN, and Bouch\u00b4e, V. 2018. Survivor insights: The role of technology in domestic minor sex traf\ufb01cking. THORN. [Tong et al. 2017] Tong, E.; Zadeh, A.; Jones, C.; and Morency, L. 2017. Combating human traf\ufb01cking with deep multimodal models. Association for Computational Linguis- tics. [van der Maaten and Hinton 2008] van der Maaten, L., and Hinton, G. 2008. Visualizing data using t-sne. Journal of Machine Learning Research 9:2431\u20132456."
]