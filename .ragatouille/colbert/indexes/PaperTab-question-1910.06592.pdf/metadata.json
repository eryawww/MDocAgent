{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "FacTweet: Pro\ufb01ling Fake News Twitter Accounts Bilal Ghanem1, Simone Paolo Ponzetto2, and Paolo Rosso1 1 PRHLT Research Center, Universitat Polit`ecnica de Val`encia, Spain {bigha@doctor, prosso@dsic}.upv.es 2 University of Mannheim, Germany simone@informatik.uni-mannheim.de Abstract. We present an approach to detect fake news in Twitter at the account level using a neural recurrent model and a variety of different semantic and stylis- tic features. Our method extracts a set of features from the timelines of news Twitter accounts by reading their posts as chunks, rather than dealing with each tweet independently. We show the experimental bene\ufb01ts of modeling latent stylis- tic signatures of mixed fake and real news with a sequential model over a wide range of strong baselines. Keywords: Fake News \u00b7 Twitter Accounts \u00b7 Factual Accounts 1 Introduction Social media platforms have made the spreading of fake news easier, faster as well as able to reach a wider audience. Social media offer another feature which is the anonymity for the authors, and this opens the door to many suspicious individuals or organizations to utilize these platforms.",
      "Social media offer another feature which is the anonymity for the authors, and this opens the door to many suspicious individuals or organizations to utilize these platforms. Recently, there has been an increased number of spreading fake news and rumors over the web and social media [22]. Fake news in social media vary considering the intention to mislead. Some of these news are spread with the intention to be ironic or to deliver the news in an ironic way (satirical news). Others, such as propaganda, hoaxes, and clickbaits, are spread to mis- lead the audience or to manipulate their opinions. In the case of Twitter, suspicious news annotations should be done on a tweet rather than an account level, since some accounts mix fake with real news. However, these annotations are extremely costly and time consuming \u2013 i.e., due to high volume of available tweets Consequently, a \ufb01rst step in this direction, e.g., as a pre-\ufb01ltering step, can be viewed as the task of detecting fake news at the account level. The main obstacle for detecting suspicious Twitter accounts is due to the behavior of mixing some real news with the misleading ones.",
      "The main obstacle for detecting suspicious Twitter accounts is due to the behavior of mixing some real news with the misleading ones. Consequently, we investigate ways to detect suspicious accounts by considering their tweets in groups (chunks). Our hy- pothesis is that suspicious accounts have a unique pattern in posting tweet sequences. Since their intention is to mislead, the way they transition from one set of tweets to the next has a hidden signature, biased by their intentions. Therefore, reading these tweets in chunks has the potential to improve the detection of the fake news accounts. In this work, we investigate the problem of discriminating between factual and non- factual accounts in Twitter. To this end, we collect a large dataset of tweets using a list of arXiv:1910.06592v1  [cs.CL]  15 Oct 2019",
      "2 B. Ghanem et al. propaganda, hoax and clickbait accounts and compare different versions of sequential chunk-based approaches using a variety of feature sets against several baselines. Several approaches have been proposed for news veri\ufb01cation, whether in social media (rumors detection) [22,21,17,24,4], or in news claims [14,16,9,2]. The main orientation in the previous works is to verify the textual claims/tweets but not their sources. To the best of our knowledge, this is the \ufb01rst work aiming to detect factuality at the account level, and especially from a textual perspective. Our contributions are: \u2013 We propose an approach to detect non-factual Twitter accounts by treating post streams as a sequence of tweets\u2019 chunks. We test several semantic and dictionary- based features together with a neural sequential approach, and apply an ablation test to investigate their contribution. \u2013 We benchmark our approach against other approaches that discard the chronological order of the tweets or read the tweets individually. The results show that our approach produces superior results at detecting non-factual accounts.",
      "\u2013 We benchmark our approach against other approaches that discard the chronological order of the tweets or read the tweets individually. The results show that our approach produces superior results at detecting non-factual accounts. 2 Methodology Given a news Twitter account, we read its tweets from the account\u2019s timeline. Then we sort the tweets by the posting date in ascending way and we split them into N chunks. Each chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account. We extract a set of features from each chunk and we feed them into a recurrent neural network to model the sequential \ufb02ow of the chunks\u2019 tweets. We use an attention layer with dropout to attend over the most important tweets in each chunk. Finally, the representation is fed into a softmax layer to produce a probability distribution over the account types and thus predict the factuality of the accounts. Since we have many chunks for each account, the label for an account is obtained by taking the majority class of the account\u2019s chunks. Input Representation. Let t be a Twitter account that contains m tweets. These tweets are sorted by date and split into a sequence of chunks ck = \u27e8ck1, . . .",
      "Input Representation. Let t be a Twitter account that contains m tweets. These tweets are sorted by date and split into a sequence of chunks ck = \u27e8ck1, . . . , ckn\u27e9, where each cki contains s tweets. Each tweet in cki is represented by a vector v \u2208IRd , where v is the concatenation of a set of features\u2019 vectors, that is v = \u27e8f1, . . . , fn\u27e9. Each feature vector fi is built by counting the presence of tweet\u2019s words in a set of lexical lists. The \ufb01nal representation of the tweet is built by averaging the single word vectors. Features. We argue that different kinds of features like the sentiment of the text, moral- ity, and other text-based features are critical to detect the nonfactual Twitter accounts by utilizing their occurrence during reporting the news in an account\u2019s timeline. We employ a rich set of features borrowed from previous works in fake news, bias, and rumors detection [22,21,2,13]. \u2013 Emotion: We build an emotions vector using word occurrences of 15 emotion types from two available emotional lexicons.",
      "We employ a rich set of features borrowed from previous works in fake news, bias, and rumors detection [22,21,2,13]. \u2013 Emotion: We build an emotions vector using word occurrences of 15 emotion types from two available emotional lexicons. We use the NRC lexicon [18], which contains \u223c14K words labeled using the eight Plutchik\u2019s emotions [20]. The other lexicon is SentiSense [1] which is a concept-based affective lexicon that attaches emotional",
      "FacTweet: Pro\ufb01ling Fake News Twitter Accounts 3 Fig. 1: The FacTweet\u2019s architecture. meanings to concepts from the WordNet3 lexical database. It has \u223c5.5 words labeled with emotions from a set of 14 emotional categories We use the categories that do not exist in the NRC lexicon. \u2013 Sentiment: We extract the sentiment of the tweets by employing EffectWordNet [5], SenticNet [3], NRC [18]4, and subj lexicon [23], where each has the two sentiment classes, positive and negative. \u2013 Morality: Features based on morality foundation theory [11] where words are la- beled in one of the following 10 categories (care, harm, fairness, cheating, loyalty, betrayal, authority, subversion, sanctity, and degradation). \u2013 Style: We use canonical stylistic features, such as the count of question marks, excla- mation marks, consecutive characters and letters5, links, hashtags, users\u2019 mentions. In addition, we extract the uppercase ratio and the tweet length.",
      "\u2013 Style: We use canonical stylistic features, such as the count of question marks, excla- mation marks, consecutive characters and letters5, links, hashtags, users\u2019 mentions. In addition, we extract the uppercase ratio and the tweet length. \u2013 Words embeddings: We extract words embeddings of the words of the tweet using Glove840B\u2212300d [19] pretrained model. The tweet \ufb01nal representation is obtained by averaging its words embeddings. Model. To account for chunk sequences we make use of a de facto standard approach and opt for a recurrent neural model using long short-term memory (LSTM) [12]. In our model, the sequence consists of a sequence of tweets belonging to one chunk (Figure 1). The LSTM learns the hidden state ht by capturing the previous timesteps (past tweets). The produced hidden state ht at each time step is passed to the attention layer which computes a \u2018context\u2019 vector ct as the weighted mean of the state sequence h by: ct = T X j=1 \u03b1tjhj, (1) 3 https://wordnet.princeton.edu 4 NRC has also two sentiment categories, positive and negative.",
      "5 We considered 2 or more consecutive characters, and 3 or more consecutive letters.",
      "4 B. Ghanem et al. Table 1: Statistics on the data with respect to each account type: propaganda (P), click- bait (C), hoax (H), and real news (R). Accounts Types P C H R # of accounts 96 36 7 32 Max # of tweets/account 3,250 3,246 3,250 3,250 Min # of tweets/account 33 877 453 212 Avg # of tweets/account 2,978 3,112 2,723 3,124 Total # of tweets 291,885 112,050 19,065 99,967 Where T is the total number of timesteps in the input sequence and \u03b1tj is a weight computed at each time step j for each state hj. 3 Experiments and Results Data. We build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts, we rely on a list of 180 Twitter accounts from [21]6. This list was created based on public resources7 where suspicious Twitter ac- counts were annotated with the main fake news types (clickbait, propaganda, satire, and hoax).",
      "For the non-factual accounts, we rely on a list of 180 Twitter accounts from [21]6. This list was created based on public resources7 where suspicious Twitter ac- counts were annotated with the main fake news types (clickbait, propaganda, satire, and hoax). We discard the satire labeled accounts since their intention is not to mis- lead or deceive. On the other hand, for the factual accounts, we use a list with another 32 Twitter accounts from [15] that are considered trustworthy by independent third par- ties8. We discard some accounts that publish news in languages other than English (e.g., Russian or Arabic). Moreover, to ensure the quality of the data, we remove the dupli- cate, media-based, and link-only tweets. For each account, we collect the maximum amount of tweets allowed by Twitter API. Table 1 presents statistics on our dataset. Baselines. We compare our approach (FacTweet) to the following set of baselines: \u2013 LR + Bag-of-words: We aggregate the tweets of a feed and we use a bag-of-words representation with a logistic regression (LR) classi\ufb01er.",
      "Baselines. We compare our approach (FacTweet) to the following set of baselines: \u2013 LR + Bag-of-words: We aggregate the tweets of a feed and we use a bag-of-words representation with a logistic regression (LR) classi\ufb01er. \u2013 Tweet2vec: We use the Bidirectional Gated recurrent neural network model pro- posed in [6]. We keep the default parameters that were provided with the imple- mentation. To represent the tweets, we use the decoded embedding produced by the model. With this baseline we aim at assessing if the tweets\u2019 hashtags may help de- tecting the non-factual accounts. \u2013 LR + All Features (tweet-level): We extract all our features from each tweet and feed them into a LR classi\ufb01er. Here, we do not aggregate over tweets and thus view each tweet independently. 6 Many of the accounts were deactivated during the collecting process, consequently only 144 accounts were used. 7 http://www.propornot.com/p/the-list.html 8 https://tinyurl.com/yctvve9h",
      "FacTweet: Pro\ufb01ling Fake News Twitter Accounts 5 Fig. 2: Results on the top-K replied, linked or re-tweeted tweets. Fig. 3: The FacTweet performance on difference chunk sizes. \u2013 LR + All Features (chunk-level): We concatenate the features\u2019 vectors of the tweets in a chunk and feed them into a LR classi\ufb01er. \u2013 FacTweet (tweet-level): Similar to the FacTweet approach, but at tweet-level; the sequential \ufb02ow of the tweets is not utilized. We aim at investigating the importance of the sequential \ufb02ow of tweets. \u2013 Top-k replies, likes, or re-tweets: Some approaches in rumors detection use the number of replies, likes, and re-tweets to detect rumors [7]. Thus, we extract top k replied, liked or re-tweeted tweets from each account to assess the accounts factual- ity. We tested different k values between 10 tweets to the max number of tweets from each account. Figure 2 shows the macro-F1 values for different k values. It seems that k = 500 for the top replied tweets achieves the highest result.",
      "We tested different k values between 10 tweets to the max number of tweets from each account. Figure 2 shows the macro-F1 values for different k values. It seems that k = 500 for the top replied tweets achieves the highest result. Therefore, we consider this as a baseline. Experimental Setup. We apply a 5 cross-validation on the account\u2019s level. For the FacTweet model, we experiment with 25% of the accounts for validation and parameters selection. We use hyperopt library9 to select the hyper-parameters on the following values: LSTM layer size (16, 32, 64), dropout (0.0 \u22120.9), activation function (relu, selu, tanh), optimizer (sgd, adam, rmsprop) with varying the value of the learning rate (1e-1,..,-5), and batch size (4, 8, 16). The validation split is extracted on the class level using strati\ufb01ed sampling: we took a random 25% of the accounts from each class since the dataset is unbalanced. Discarding the classes\u2019 size in the splitting process may affect the minority classes (e.g. hoax).",
      "The validation split is extracted on the class level using strati\ufb01ed sampling: we took a random 25% of the accounts from each class since the dataset is unbalanced. Discarding the classes\u2019 size in the splitting process may affect the minority classes (e.g. hoax). For the baselines\u2019 classi\ufb01er, we tested many classi\ufb01ers and the LR showed the best overall performance. Results. Table 2.1 presents the results. We present the results using a chunk size of 20, which was found to be the best size on the held-out data. Figure 3 shows the results of different chunks sizes. FacTweet performs better than the proposed baselines and 9 https://github.com/hyperopt",
      "6 B. Ghanem et al. Table 2: FacTweet: experimental results. 2.1: Results on account classi\ufb01cation. Methods Acc. Macro-F1 Baselines Majority Class 0.56 0.18 Random class 0.25 0.21 Bag-of-Words 0.60 0.28 Tweet2vec 0.56 0.18 Tweet-level approaches LR + All 0.67 0.39 LR + All (top-500 replied) 0.69 0.41 LR + FacTweet 0.65 0.351 Chunk-level approaches LR + All 0.737 0.559 FacTweet 0.74 0.565 2.2: Ablation tests. Methods Acc. Macro-F1 LR + All 0.737 0.559 \u2212Emotion 0.731 0.557 \u2212Sentiment 0.731 0.554 \u2212Morality 0.725 0.548 \u2212Style 0.737 0.514 \u2212Words embeddings 0.678 0.437 obtains the highest macro-F1 value of 0.565.",
      "Our results indicate the importance of taking into account the sequence of the tweets in the accounts\u2019 timelines. The sequence of these tweets is better captured by our proposed model sequence-agnostic or non- neural classi\ufb01ers. Moreover, the results demonstrate that the features at tweet-level do not perform well to detect the Twitter accounts factuality, since they obtain a result near to the majority class (0.18). Another \ufb01nding from our experiments shows that the performance of the Tweet2vec is weak. This demonstrates that tweets\u2019 hashtags are not informative to detect non-factual accounts. In Table 2.2, we present ablation tests so as to quantify the contribution of subset of features. The results indicate that most performance gains come from words embeddings, style, and morality features. Other features (emotion and sentiment) show lower importance: nevertheless, they still improve the overall system performance (on average 0.35% Macro-F1 improvement). These performance \ufb01gures suggest that non-factual accounts use semantic and stylistic hidden signatures mostly while tweeting news, so as to be able to mislead the readers and behave as reputable (i.e., factual) sources.",
      "These performance \ufb01gures suggest that non-factual accounts use semantic and stylistic hidden signatures mostly while tweeting news, so as to be able to mislead the readers and behave as reputable (i.e., factual) sources. We leave a more \ufb01ne-grained, diachronic analysis of semantic and stylistic features \u2013 how semantic and stylistic signature evolve across time and change across the accounts\u2019 timelines \u2013 for future work. 4 Conclusions In this paper, we proposed a model that utilizes chunked timelines of tweets and a recurrent neural model in order to infer the factuality of a Twitter news account. Our experimental results indicate the importance of analyzing tweet stream into chunks, as well as the bene\ufb01ts of heterogeneous knowledge source (i.e., lexica as well as text) in order to capture factuality. In future work, we would like to extend this line of research with further in-depth analysis to understand the \ufb02ow change of the used features in the accounts\u2019 streams. Moreover, we would like to take our approach one step further",
      "FacTweet: Pro\ufb01ling Fake News Twitter Accounts 7 incorporating explicit temporal information, e.g., using timestamps. Crucially, we are also interested in developing a multilingual version of our approach, for instance by leveraging the now ubiquitous cross-lingual embeddings [8,10]. References 1. de Albornoz, J.C., Plaza, L., Gerv\u00b4as, P.: SentiSense: An Easily Scalable Concept-Based Af- fective Lexicon for Sentiment Analysis. In: The Eighth International Conference on Lan- guage Resources and Evaluation (LREC). pp. 3562\u20133567 (2012) 2. Baly, R., Karadzhov, G., Alexandrov, D., Glass, J., Nakov, P.: Predicting Factuality of Report- ing and Bias of News Media Sources. In: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 3528\u20133539 (2018) 3.",
      "In: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 3528\u20133539 (2018) 3. Cambria, E., Olsher, D., Rajagopal, D.: SenticNet 3: a Common and Common-sense Knowl- edge Base for Cognition-driven Sentiment Analysis. In: Proceedings of the 28th AAAI Con- ference on Arti\ufb01cial Intelligence (AAAI) (2014) 4. Castillo, C., Mendoza, M., Poblete, B.: Information Credibility on Twitter. In: Proceedings of the 20th International Conference on World Wide Web (WWW). pp. 675\u2013684. ACM (2011) 5. Choi, Y., Wiebe, J.: +/-EffectWordNet: Sense-level Lexicon Acquisition for Opinion Infer- ence. In: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 1181\u20131191 (2014) 6.",
      "In: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 1181\u20131191 (2014) 6. Dhingra, B., Zhou, Z., Fitzpatrick, D., Muehl, M., Cohen, W.W.: Tweet2Vec: Character- Based Distributed Representations for Social Media. In: The 54th Annual Meeting of the Association for Computational Linguistics (ACL). p. 269 (2016) 7. Ghanem, B., Cignarella, A.T., Bosco, C., Rosso, P., Rangel, F.: UPV-28-UNITO at SemEval- 2019 Task 7: Exploiting Post\u2019s Nesting and Syntax Information for Rumor Stance Classi\ufb01ca- tion. In: Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval). pp. 1125\u20131131 (2019) 8. Ghanem, B., Glavas, G., Giachanou, A., Ponzetto, S.P., Rosso, P., Pardo, F.M.R.",
      "pp. 1125\u20131131 (2019) 8. Ghanem, B., Glavas, G., Giachanou, A., Ponzetto, S.P., Rosso, P., Pardo, F.M.R.: UPV- UMA at checkthat! lab: Verifying arabic claims using a cross lingual approach. In: Working Notes of CLEF 2019 - Conference and Labs of the Evaluation Forum, Lugano, Switzerland, September 9-12, 2019. (2019) 9. Ghanem, B., Rosso, P., Rangel, F.: Stance Detection in Fake News A Combined Feature Representation. In: Proceedings of the First Workshop on Fact Extraction and VERi\ufb01cation (FEVER). pp. 66\u201371 (2018) 10. Glavas, G., Litschko, R., Ruder, S., Vulic, I.: How to (properly) evaluate cross-lingual word embeddings: On strong baselines, comparative analyses, and some misconceptions.",
      "66\u201371 (2018) 10. Glavas, G., Litschko, R., Ruder, S., Vulic, I.: How to (properly) evaluate cross-lingual word embeddings: On strong baselines, comparative analyses, and some misconceptions. In: Pro- ceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers. pp. 710\u2013721 (2019) 11. Graham, J., Haidt, J., Nosek, B.A.: Liberals and Conservatives Rely on Different Sets of Moral Foundations. Journal of Personality and Social Psychology 96(5), 1029 (2009) 12. Hochreiter, S., Schmidhuber, J.: Long Short-Term Memory. Neural Computation 9(8), 1735\u2013 1780 (1997) 13.",
      "Hochreiter, S., Schmidhuber, J.: Long Short-Term Memory. Neural Computation 9(8), 1735\u2013 1780 (1997) 13. Horne, B.D., Adali, S.: This Just In: Fake News Packs a Lot in Title, Uses Simpler, Repetitive Content in Text Body, More Similar to Satire than Real News. In: Eleventh International AAAI Conference on Web and Social Media (ICWSM) (2017) 14. Karadzhov, G., Nakov, P., M`arquez, L., Barr\u00b4on-Cede\u02dcno, A., Koychev, I.: Fully Automated Fact Checking Using External Sources. In: Proceedings of the International Conference Re- cent Advances in Natural Language Processing (RANLP). pp. 344\u2013353 (2017)",
      "8 B. Ghanem et al. 15. Karduni, A., Wesslen, R., Santhanam, S., Cho, I., Volkova, S., Arendt, D., Shaikh, S., Dou, W.: Can You Veri\ufb01This? Studying Uncertainty and Decision-Making about Misinformation Using Visual Analytics. In: In Twelfth International AAAI Conference on Web and Social Media (ICWSM) (2018) 16. Li, X., Meng, W., Yu, C.: T-veri\ufb01er: Verifying Truthfulness of Fact Statements. In: 27th International Conference on Data Engineering (ICDE). pp. 63\u201374. IEEE (2011) 17. Ma, J., Gao, W., Mitra, P., Kwon, S., Jansen, B.J., Wong, K.F., Cha, M.: Detecting Rumors from Microblogs with Recurrent Neural Networks. In: International Joint Conference on Arti\ufb01cial Intelligence (IJCAI). pp. 3818\u20133824 (2016) 18.",
      "In: International Joint Conference on Arti\ufb01cial Intelligence (IJCAI). pp. 3818\u20133824 (2016) 18. Mohammad, S.M., Turney, P.D.: Emotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion Lexicon. In: Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text. pp. 26\u201334 (2010) 19. Pennington, J., Socher, R., D. Manning, C.: GloVe: Global Vectors for Word Representation. In: Empirical Methods in Natural Language Processing (EMNLP). pp. 1532\u20131543 (2014) 20. Plutchik, R.: A General Psychoevolutionary Theory of Emotion. In: Theories of Emotion, pp. 3\u201333. Elsevier (1980) 21.",
      "pp. 1532\u20131543 (2014) 20. Plutchik, R.: A General Psychoevolutionary Theory of Emotion. In: Theories of Emotion, pp. 3\u201333. Elsevier (1980) 21. Volkova, S., Shaffer, K., Jang, J.Y., Hodas, N.: Separating Facts from Fiction: Linguistic Models to Classify Suspicious and Trusted News Posts on Twitter. In: Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL) (Volume 2: Short Papers). vol. 2, pp. 647\u2013653 (2017) 22. Vosoughi, S., Roy, D., Aral, S.: The Spread of True and False News Online. Science 359(6380), 1146\u20131151 (2018) 23. Wilson, T., Wiebe, J., Hoffmann, P.: Recognizing Contextual Polarity in Phrase-Level Senti- ment Analysis.",
      "Science 359(6380), 1146\u20131151 (2018) 23. Wilson, T., Wiebe, J., Hoffmann, P.: Recognizing Contextual Polarity in Phrase-Level Senti- ment Analysis. In: Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (EMNLP) (2005) 24. Zhao, Z., Resnick, P., Mei, Q.: Enquiring Minds: Early Detection of Rumors in Social Media from Enquiry Posts. In: Proceedings of the 24th International Conference on World Wide Web (WWW). pp. 1395\u20131405 (2015)"
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1910.06592.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":5023,
  "avg_doclen":173.2068965517,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1910.06592.pdf"
    }
  }
}