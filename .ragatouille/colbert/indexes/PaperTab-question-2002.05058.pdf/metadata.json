{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Learning to Compare for Better Training and Evaluation of Open Domain Natural Language Generation Models Wangchunshu Zhou, Ke Xu State Key Lab of Software Development Environment, Beihang University, Beijing, China zhouwangchunshu@buaa.edu.cn, kexu@nlsde.buaa.edu.cn Abstract Automated evaluation of open domain natural language generation (NLG) models remains a challenge and widely used metrics such as BLEU and Perplexity can be misleading in some cases. In our paper, we pro- pose to evaluate natural language generation models by learning to compare a pair of generated sentences by \ufb01ne-tuning BERT, which has been shown to have good natural language understanding ability. We also propose to evaluate the model-level quality of NLG models with sample-level comparison results with skill rating sys- tem. While able to be trained in a fully self-supervised fashion, our model can be further \ufb01ne-tuned with a little amount of human preference annotation to bet- ter imitate human judgment. In addition to evaluating trained models, we propose to apply our model as a per- formance indicator during training for better hyperpa- rameter tuning and early-stopping.",
      "In addition to evaluating trained models, we propose to apply our model as a per- formance indicator during training for better hyperpa- rameter tuning and early-stopping. We evaluate our ap- proach on both story generation and chit-chat dialogue response generation. Experimental results show that our model correlates better with human preference com- pared with previous automated evaluation approaches. Training with the proposed metric yields better perfor- mance in human evaluation, which further demonstrates the effectiveness of the proposed model. 1 Introduction Recent advances in sequence-to-sequence learning ar- chitecture (Sutskever et al. 2014) and the transformer model (Vaswani et al. 2017) have raised increasing interest in natural language generation (NLG) tasks, including story generation (Fan et al. 2018), open-domain dialogue response generation (Sordoni et al. 2015) and abstractive summariza- tion (See et al. 2017). Despite the fast advances of models, there remains a huge gap in the evaluation of NLG models and it is hard to measure the progress due to the lack of good evaluation metrics.",
      "2015) and abstractive summariza- tion (See et al. 2017). Despite the fast advances of models, there remains a huge gap in the evaluation of NLG models and it is hard to measure the progress due to the lack of good evaluation metrics. While perplexity is a good measure of how well a model \ufb01ts some data, it does not measure perfor- mance at the desired task. Word overlap based metrics such as BLEU (Papineni et al. 2002), METEOR (Banerjee and Lavie 2005) and ROUGE (Lin 2004) capture quality better than the perplexity and are useful in translation and sum- marization. However, they still correlate poorly with human Copyright c\u20dd2020, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved. evaluation (Liu et al. 2016) in open domain text generation tasks including story generation and dialogue response gen- eration because two equally good generated texts may have no n-gram overlap.",
      "All rights reserved. evaluation (Liu et al. 2016) in open domain text generation tasks including story generation and dialogue response gen- eration because two equally good generated texts may have no n-gram overlap. Human evaluation is generally consid- ered to be the gold standard evaluation, however, it does not scale well as it is generally expensive and time-consuming to conduct human evaluation. Apart from measuring relative progress between different models, automated evaluation metrics also play an impor- tant role in the training stage of NLG models. It is a com- mon practice to tune the model hyperparameter, detect con- vergence, perform early-stopping, and select the best check- points based on the model\u2019s performance on automated eval- uation metrics. While acceptable for tasks where automated metrics correlate well with human evaluations, including machine translation and text summarization, this can be er- roneous and result in sub-optimal training in open domain NLG tasks because available automated metrics correlate poorly with human evaluation, as demonstrated in the ex- perimental section of this paper.",
      "To tackle the aforementioned problems, in this paper, we propose a self-supervised approach with transfer learning to learn to compare the quality of two samples as an automated comparative Turing test. The motivation of our approach is that we can better assess the quality of generated samples or trained NLG model by comparing it with another one. Our model is a text pair classi\ufb01cation model trained to compare the task-speci\ufb01c quality of two samples, which is then used to evaluate the quality of trained NLG models. As human preference annotation is generally expensive, our model is designed to be able to perform self-supervised training using only generated samples and gold reference samples with- out human preference annotation. When human preference annotation is available, our model can be further \ufb01ne-tuned to better imitate human judgment. To evaluate the model- level quality of NLG models based on pairwise comparison in sample-level, we adopt the skill rating system similar to ELO (Elo 1978) and Trueskill (Herbrich et al. 2007), which is a method for assigning a numerical skill to players in a player-vs-player game, given a win-loss record of games played.",
      "2007), which is a method for assigning a numerical skill to players in a player-vs-player game, given a win-loss record of games played. In our scenario, the players are NLG models to be evaluated and a higher rating indicates a better model. The skill rating system makes it possible to evaluate all n mod- els without needing to run n2 matches and is able to take arXiv:2002.05058v1  [cs.CL]  12 Feb 2020",
      "into account the amount of new information each compari- son provides. The contribution of this paper is threefold: \u2022 We propose a \u201clearning to compare\u201d model to better as- sess the quality of text generated by NLG models based on pairwise comparison. Our model is able to transfer natural language understanding knowledge from BERT by \ufb01ne- tuning in a self-supervised way while also able to be fur- ther \ufb01ne-tuned with human preference annotation. Once trained, our model is able to perform inter-model compar- ison without the need for gold references, which greatly enlarges the potentially available test set and reduces the potential risk of over\ufb01tting the reference in the test set. \u2022 We propose to use the skill rating system to perform model-level evaluation based on the sample-level evalu- ation information provided by our pairwise comparison model. The skill rating system is more ef\ufb01cient and accu- rate than several baseline approaches. \u2022 We conduct experiments on both story generation task and open domain dialogue response generation task. Exper- imental results show that our approach correlates better with human evaluation on both datasets.",
      "The skill rating system is more ef\ufb01cient and accu- rate than several baseline approaches. \u2022 We conduct experiments on both story generation task and open domain dialogue response generation task. Exper- imental results show that our approach correlates better with human evaluation on both datasets. Moreover, we show that using automated metrics such as BLEU to per- form hyperparameter tuning and early-stopping results in sub-optimal model and our approach helps alleviate this problem. 2 Related Work Evaluation of NLG models has been a long-standing open problem. While human evaluation may be ideal, it is gener- ally expensive to conduct and does not scale well. Various automated evaluation approaches are proposed to facilitate the development and evaluation of NLG models. We sum- marize these evaluation approaches below. Text Overlap Metrics, including BLEU (Papineni et al. 2002), METEOR (Banerjee and Lavie 2005) and ROUGE (Lin 2004), are the most popular metrics employed in the evaluation of NLG models. They evaluate generated text by comparing the similarity between the generated text and human written references.",
      "2002), METEOR (Banerjee and Lavie 2005) and ROUGE (Lin 2004), are the most popular metrics employed in the evaluation of NLG models. They evaluate generated text by comparing the similarity between the generated text and human written references. While this works well in tasks where the diversity of acceptable output is limited, such as machine translation and text summarization, text overlap metrics are shown to have weak or no correlation with hu- man judgments in open domain natural language generation tasks (Liu et al. 2016). There are two major drawbacks in these metrics. First, text overlap metrics can not distinguish minor variations in a generated text which may make the sentence not equally grammatically correct or semantically meaningful. Second, there may exist multiple equally good outputs for the given input and comparing against one gold reference can be erroneous. Perplexity is commonly used to evaluate the quality of a language model. It measures how well a probability distribu- tion predicts a sample and captures the degree of uncertainty in the model. It is used to evaluate models in open-domain NLG tasks such as story generation (Fan et al. 2018) and open domain dialogue systems.",
      "It measures how well a probability distribu- tion predicts a sample and captures the degree of uncertainty in the model. It is used to evaluate models in open-domain NLG tasks such as story generation (Fan et al. 2018) and open domain dialogue systems. However, \u201chow likely a sen- tence is generated by a given model\u201d may not be comparable across different models and does not indicate the quality of the sentence. Parameterized Metrics learn a parameterized model to evaluate generated text. Adversarial evaluation mod- els (Kannan and Vinyals 2017; Li et al. 2017a) assigns a score based on how easy it is to distinguish the dialogue model responses from human responses. However, training such a discriminator can be dif\ufb01cult as the binary classi\ufb01ca- tion task can be easily over-\ufb01tted and leads to poor gener- alizability (Kannan and Vinyals 2017). Moreover, the infor- mation we get from the discriminator accuracy is limited as we can not compare the quality of two generated sentences when they both succeed or fail in fooling the discriminator.",
      "Moreover, the infor- mation we get from the discriminator accuracy is limited as we can not compare the quality of two generated sentences when they both succeed or fail in fooling the discriminator. Recent study shows that the discriminator accuracy does not correlate well with human preference (Garbacea et al. 2019). Automated Dialogue Evaluation Model (ADEM) (Lowe et al. 2017) is another parameterized metric proposed for di- alogue system evaluation. It learns to score a generated di- alogue response based on the context and the human writ- ten reference. However, it requires human-annotated scores for generated sentences. It is generally hard to design ap- propriate questions for crowdsourcing these scores, which makes the annotation very expensive to get and the inter- annotator agreement score is only moderate (Lowe et al. 2017). As a result, the training data is limited and noisy, which makes the scoring task even harder. It can be prob- lematic when comparing models with similar quality. In ad- dition, this model is designed only for evaluating dialogue response generation models.",
      "2017). As a result, the training data is limited and noisy, which makes the scoring task even harder. It can be prob- lematic when comparing models with similar quality. In ad- dition, this model is designed only for evaluating dialogue response generation models. More recently, embedding sim- ilarity based metrics such as HUSE (Shimanaka et al. 2018) and BERTScore (Zhang et al. 2019). These metrics alleviate the \ufb01rst problem of text overlap metrics by modeling seman- tic similarity better. However, they can not address the re- sponse diversity problem and thus are only suitable for ma- chine translation and text summarization. Another line of research on NLG evaluation is to unify human evaluation with statistical evaluation (Hashimoto et al. 2019; Chaganty et al. 2018). These works are orthogo- nal to our paper as they mainly focus on the combination of human evaluation and automated evaluation.",
      "Another line of research on NLG evaluation is to unify human evaluation with statistical evaluation (Hashimoto et al. 2019; Chaganty et al. 2018). These works are orthogo- nal to our paper as they mainly focus on the combination of human evaluation and automated evaluation. Another related work of our research is the skill rating system, which evaluates players by observing a record of wins and losses of multiple players and inferring the value of a latent, unobserved skill variable for each player that explains the records of wins and losses. It is \ufb01rst adopted to evaluate GANs (Goodfellow et al. 2014) for synthesiz- ing images (Olsson et al. 2018) by competing generators against discriminators. Their approach is an approximation of skill rating as the original skill rating system requires game played by two symmetric players, while in their sys- tem the players are asymmetric. Their approach does not in- clude the \u201ctie\u201d option, thus can not distinguish cases where the discriminator is con\ufb01dent enough or not.",
      "Their approach does not in- clude the \u201ctie\u201d option, thus can not distinguish cases where the discriminator is con\ufb01dent enough or not. More impor- tantly, their approach is only designed for evaluating GANs while our approach can be used for any NLG models. 3 Methodology We present the proposed approach in this section. We be- gin with the sample-level pairwise comparison model. Af-",
      "terwards, we introduce how to adopt the skill rating system to perform model-level evaluation of NLG models. 3.1 Learning to Compare The proposed comparative evaluator is a text pair relation classi\ufb01er which is trained to compare the task-speci\ufb01c qual- ity of two samples. The motivation of evaluating one sample by comparing it with another sample is drawn from the in- sight learned when conducting human evaluation for NLG models. We \ufb01nd that when comparing two NLG models, in- stead of asking human annotator to assign scores separately for samples generated by different models, which resembles the case in the ADEM model (Lowe et al. 2017), it is much easier for human annotators to directly compare one sample generated by the \ufb01rst model against another sample from the second model pairwisely and compute the win/loss rate. The comparison-based evaluation may also be more accurate, which is demonstrated by a higher inter-annotator agreement score in our preliminary experiments.",
      "The comparison-based evaluation may also be more accurate, which is demonstrated by a higher inter-annotator agreement score in our preliminary experiments. The comparative evaluator learns a total order of sample quality by classifying whether the \ufb01rst compared sample is better (>), worse (<), or indistinguishable (\u2248) in terms of its quality compared with another sample. In this way, our model encodes the inductive bias that sometimes two sam- ples can have similar quality and it is hard and unreliable to choose the better sample. By giving our model the third \u201ctie\u201d option, it can explicitly express its uncertainty and choose its preference only when being con\ufb01dent enough. This design choice is motivated by the practice that adding the \u201ctie\u201d op- tion for human annotator when performing pairwise human evaluation can often make the comparison easier and more reliable. For a text sample, our comparative evaluator can provide a more informative assessment than the binary dis- criminative evaluator because one evaluated sample can re- ceive multiple feedback from the comparative evaluator by comparing it with multiple other samples. In contrast, the discriminative evaluator can only evaluate a sample once, which is more likely to suffer from the inherent uncertainty of the evaluator.",
      "In contrast, the discriminative evaluator can only evaluate a sample once, which is more likely to suffer from the inherent uncertainty of the evaluator. We propose two approaches to construct pairwise training examples for training a comparative evaluator. The \ufb01rst ap- proach generates strong supervision examples. It is based on the intuition that human written references are gener- ally of better quality than machine-generated samples, and it is hard to tell the difference in term of the quality when two compared samples are both generated by machines or human written reference. We denote S+/S\u2212as the set of real/generated samples. For a real sample s+ \u2208S+ and a generated sample s\u2212\u2208S\u2212, we assign the label \u201cbet- ter (>)\u201d to the pair (s+, s\u2212) and \u201cworse (<)\u201d to (s\u2212, s+). For two samples both from real data or from the generated samples, we assign the label \u201cindistinguishable (\u2248)\u201d to such pairs (i.e., (si +, sj +) and (si \u2212, sj \u2212)).",
      "For two samples both from real data or from the generated samples, we assign the label \u201cindistinguishable (\u2248)\u201d to such pairs (i.e., (si +, sj +) and (si \u2212, sj \u2212)). For a training set with n real samples and n generated samples, we can construct \u00002n 2 \u0001 pairwise training examples for the comparative evaluator, al- lowing to enhance the generalization ability and introduce more informative learning signals than the standard real/fake binary discriminative evaluator. Note that when construct- ing a sample pair (si \u2212, sj \u2212), si \u2212and sj \u2212are sampled from the same checkpoint of the same model in order to ensure that they are of similar quality in expectation. One problem of the strong supervision approach is that it always labels two generated samples as indistinguishable. However, during inference, the input of the comparative evaluator is a pair of two generated samples from different models. Thus it requires the model to capture the quality relation in training examples and generalize well to success- fully compare two samples rather than simply classifying them as indistinguishable, which provides relatively less in- formation for evaluating NLG models.",
      "Thus it requires the model to capture the quality relation in training examples and generalize well to success- fully compare two samples rather than simply classifying them as indistinguishable, which provides relatively less in- formation for evaluating NLG models. To tackle this problem, we propose an approach to con- struct weak supervision examples for training the compar- ative evaluator. The intuition of our weak supervision ap- proach is that during training, the quality of the NLG model keeps improving until convergence. Given two checkpoints of the same model, we can thus consider samples generated by the more recent checkpoint are of better quality compared with samples generated by the earlier version of the same model. This approach is considered to be weak supervision because the model quality may not improve monotonically and sometimes it is hard to decide whether the model begins to over\ufb01t the training data and its quality starts to decline. To minimize the noise introduced by these problems, we empir- ically set the minimal margin between two selected check- points to be 10% of the total training iteration and do not se- lect two \u201calmost converged\u201d checkpoints. The construction of training samples is similar to the \ufb01rst approach.",
      "To minimize the noise introduced by these problems, we empir- ically set the minimal margin between two selected check- points to be 10% of the total training iteration and do not se- lect two \u201calmost converged\u201d checkpoints. The construction of training samples is similar to the \ufb01rst approach. In addi- tion, motivated by the fact that the larger the margin between the quality two selected version of the model, the easier for the comparative evaluator to learn to distinguish the training examples, we propose to use curriculum learning (Bengio et al. 2009) by feeding the comparative evaluator with sam- ple pairs with larger margin (i.e. more training iterations be- tween two selected checkpoints) during initial training stage and gradually decrease the margin to let the model gradu- ally learn to capture smaller quality differences. Moreover, when human preference annotation is available, we can ad- ditionally \ufb01ne-tune the comparative evaluator with human annotations.",
      "more training iterations be- tween two selected checkpoints) during initial training stage and gradually decrease the margin to let the model gradu- ally learn to capture smaller quality differences. Moreover, when human preference annotation is available, we can ad- ditionally \ufb01ne-tune the comparative evaluator with human annotations. The comparative evaluator is trained with maximum like- lihood estimation (MLE) objective, as described in eq 1 L = \u2212E(x1,x2)\u223cX [log DQ(x1,x2) \u03c6 (x1, x2)] (1) where X is the set of pairwise training examples contructed as described above, Q(x1, x2) \u2208{>, <, \u2248} is the true label for the pair (x1, x2), Dq \u03c6(x1, x2) is the probability of the comparative discriminator\u2019s prediction being q (q \u2208{>, < , \u2248}) for the pair (x1, x2). As comparing the quality of generated text requires good natural language understanding ability and our compara- tive evaluator is formulated as a sentence pair classi\ufb01cation model, we propose to \ufb01ne-tune BERT (Devlin et al.",
      "As comparing the quality of generated text requires good natural language understanding ability and our compara- tive evaluator is formulated as a sentence pair classi\ufb01cation model, we propose to \ufb01ne-tune BERT (Devlin et al. 2018) as the comparative evaluator, the architecture of the result- ing comparative evaluator is illustrated by Figure 1. Note that the compared sample A and B are based on the same context, which ensures that they are comparable.",
      "Figure 1: model architecture of the comparative evaluator, the context is concatenated with generated samples. 3.2 Skill Rating In player-vs-player games such as chess or tennis, skill rat- ing systems such as Elo (Elo 1978) or Glicko2 (Glickman 2012) evaluate players by observing a record of wins and losses of multiple players and inferring the value of a latent, unobserved skill variable for each player that explains the records of wins and losses. We adopt the skill rating system for model-level evaluation of NLG models. By taking the trained comparative evaluator as the \u201cplayground\u201d and NLG models as \u201cplayer\u201d, the \u201cplayer-vs-player\u201d game is played by sampling one output sample from each NLG model con- ditioning on the same input and the game output is decided by the comparative evaluator. Following previous work (Olsson et al. 2018), in our pa- per, we use the Glicko2 system (Glickman 2012).",
      "Following previous work (Olsson et al. 2018), in our pa- per, we use the Glicko2 system (Glickman 2012). The em- ployed system can be summarized as follows: each player\u2019s skill rating is represented as a Gaussian distribution, with a mean and standard deviation, representing the current state of the evidence about their \u201ctrue\u201d skill rating. As we eval- uate frozen snapshots of NLG models, we disabled an ir- relevant feature of Glicko2 that increases uncertainty about a human players skill when they have not participated in a match for some time. Another difference is that conven- tional skill rating systems do not support the \u201ctie\u201d option, which is important for the system to be stable and reliable in our case because the evaluator is not perfect. To incorporate this feature, we follow the intuition that a player\u2019s skill rat- ing should be increased when it draws with another player with a higher skill rating and vice versa. We come up with a simple rule which increases/decreases the skill rating of one player by a ratio (e.g.",
      "To incorporate this feature, we follow the intuition that a player\u2019s skill rat- ing should be increased when it draws with another player with a higher skill rating and vice versa. We come up with a simple rule which increases/decreases the skill rating of one player by a ratio (e.g. 0.1) of the changes in its skill rat- ing when it wins/loses if it draws with another player with higher/lower skill rating. In our experiments, the skill rating is performed by randomly sampling two compared models, simulating a \u201cgame\u201d between two selected models by sam- pling one sample from each model and comparing them with the comparative evaluator, and then updating the skill rating of selected models according to the outcome. This procedure is performed iteratively until convergence, which is de\ufb01ned as the order of skill ratings of compared models keeps the same after each model is selected at least 50 times. While the sampling procedure can be optimized by bayesian op- timization (Snoek et al. 2012) or multi-armed bandit algo- rithms (Vermorel and Mohri 2005), we choose to keep the method as simple as possible and use random sampling.",
      "While the sampling procedure can be optimized by bayesian op- timization (Snoek et al. 2012) or multi-armed bandit algo- rithms (Vermorel and Mohri 2005), we choose to keep the method as simple as possible and use random sampling. 4 Experiments We set up experiments in order to answer the following re- search questions: \u2022 RQ1: Can the comparative evaluator correlate better with human preference in sample-level than previous auto- mated metrics when evaluating open domain NLG mod- els? \u2022 RQ2: Can the comparative evaluator correlate better with human preference in model-level, so that our approach can measure the progress on open domain NLG better? \u2022 RQ3: As existing approaches fail to correlate well with human preference, whether and to what extent this prob- lem affects the quality of the \ufb01nal NLG model when per- forming hyperparameter search and early-stopping? \u2022 RQ4: If the previous problem exists, can proposed com- parative evaluator reduce this problem?",
      "\u2022 RQ4: If the previous problem exists, can proposed com- parative evaluator reduce this problem? 4.1 Experimental Settings Datasets We evaluate the effectiveness of the proposed approach on two open domain natural language generation tasks: story generation and open domain dialogue response generation. For story generation, we use the WritingPrompts dataset released by Fan et al.. The WritingPrompts dataset is a large dataset of 303,358 human-generated stories paired with writing prompts from an online forum. NLG models are trained by taking writing prompts as input and generating the whole story. The average length of prompts is 28.4 and the average length of stories is 734.5 words, which makes hu- man evaluation very expensive and better automated metrics are thus critical. For open domain dialogue response gener- ation task, we use the Dailydialog dataset (Li et al. 2017b),",
      "which consists of dialogues that resemble daily conversa- tions across multiple topics. It comprises of 13k dialogues with an average of 7.9 turns per dialog. Compared Models and Metrics As our objective is to evaluate the evaluators rather than comparing state-of-the- art models, we choose three representative sequence-to- sequence architectures: LSTM (Hochreiter and Schmidhu- ber 1997) seq2seq, Convolutional seq2seq (Gehring et al. 2017), and transformer (Vaswani et al. 2017) model. We compare models with different architectures, hyperparam- eter choices, and early-stopping criteria with different auto- mated metrics, as well as human evaluation. Regarding the evaluation metric (and criteria for choosing hyperparameter choice and early-stopping), we compare the proposed approach with the discriminative evaluator, BLEU score (average of 2-, 3-, 4-grams), perplexity, and ADEM. When evaluating generated stories, we cut off the story at the nearest sentence for stories longer than 250 words.",
      "When evaluating generated stories, we cut off the story at the nearest sentence for stories longer than 250 words. The proposed comparative evaluator is employed for choosing hyperparameter by performing skill rating among all models trained with different hyperparameter choices1. For early-stopping, as incrementally performing skill rating is computationally expensive, we propose to perform n (e.g. 1000) pairwise comparison between the samples generated by the latest checkpoint and the previous k (e.g. 2) check- points and stop training when the wining rate of latest check- point keeps being smaller than its losing rate for 5 iterations. Detail of Parameterized Evaluators The proposed com- parative evaluator is trained by \ufb01ne-tuning BERT-large as a sentence-pair classi\ufb01er. To ensure fair evaluation, we also train the discriminative evaluator by \ufb01ne-tuning BERT. For ADEM, we adopt its original implementation as its archi- tecture is relatively complicated.",
      "To ensure fair evaluation, we also train the discriminative evaluator by \ufb01ne-tuning BERT. For ADEM, we adopt its original implementation as its archi- tecture is relatively complicated. In addition, we perform ablation study by evaluating three variants of the compar- ative evaluator where it is trained without strong supervi- sion examples, without weak supervision examples, without \ufb01ne-tuning with human preference annotations, and without transferring from BERT. Human Evaluation Procedure As human evaluation is expensive, sample-level evaluation is performed jointly with model-level evaluation, which is also used for evaluating the ability of different metrics for performing hyperparam- eter search and early-stopping. Concretely, we perform 10 groups of evaluations for performing hyperparameter select- ing and early-stopping with \ufb01ve compared automated met- rics. In each evaluation, each of the \ufb01ve compared metrics is used to select the best hyperparameter combination or early- stopping checkpoint with other variants \ufb01xed.",
      "In each evaluation, each of the \ufb01ve compared metrics is used to select the best hyperparameter combination or early- stopping checkpoint with other variants \ufb01xed. We choose to perform score-based human evaluation for four reasons: 1) the ADEM baseline requires human- annotated score as training examples, 2) we can construct up to \u00002n 2 \u0001 training examples for our comparative evaluator with n human-annotated scores, 3) score-based human eval- uation facilitates the evaluation of correlation scores, and 4) 1For each model, we randomly sample 5 hyperparameter choices in a prede\ufb01ned range. as all other metrics do not perform pairwise comparison, us- ing pairwise human evaluation will likely be biased toward our approach. We sample 20 generated samples from each model (out of 5) of the 20 evaluation groups. We invite 20 human an- notators which are all graduate students with good English language pro\ufb01ciency to score these samples. Each annotator scores one sample from each model, such that each model is uniformly evaluated. The score scales from 1 to 5, higher score indicates better overall sample quality.",
      "Each annotator scores one sample from each model, such that each model is uniformly evaluated. The score scales from 1 to 5, higher score indicates better overall sample quality. According to experimental results from Lowe et al., we do not ask annota- tors to provide speci\ufb01c scores for \ufb02uency or informativeness. To test the inner-annotator agreement score, we additionally ask them to evaluate another 40 generated samples, of which 20 samples are scored from 1 to 5 and another 20 are eval- uated based on pairwise comparison with 4 other generated samples and scored to 1-5 based on how many times they are considered to be better than a reference sample. We get an inter-annotator agreement score \u03ba = 0.53 for directly scoring and \u03ba = 0.76 with pairwise comparison, which val- idates our intuition that evaluation by comparison may be more accurate. These additional human annotations are used as training data for ADEM and the comparative evaluator.",
      "We get an inter-annotator agreement score \u03ba = 0.53 for directly scoring and \u03ba = 0.76 with pairwise comparison, which val- idates our intuition that evaluation by comparison may be more accurate. These additional human annotations are used as training data for ADEM and the comparative evaluator. 4.2 Experimental Designs & Results RQ1: Sample-Level Correlation To test the correlation of different automated metrics with respect to human pref- erence, we employ different metrics to score the collected 2000 samples and calculate their Pearson and Spearman cor- relation with human scores. For comparative evaluator, as the evaluation is performed pairwisely and no absolute score is available, we use two different approaches to get an abso- lute score for each sample: 1) we sample 50 common ref- erences from machine-generated samples for each task and compare each sample with all references by the comparative evaluator.",
      "A sample gets 3 points when beats a reference, 1 point when draws with the reference, and get 0 point when loses, 2) we adopt skill rating system by regarding each sam- ple as an NLG model which always outputs the same sample and use the skill rating for each sample as its score. To en- sure the computational budget to be roughly the same, we \ufb01x the number of plays in skill rating to 10,000. The experimental results are summarized in Table 1. We can see that the proposed comparative evaluator correlates far better with human judgment than BLEU and perplexity. When compared with recently proposed parameterized met- rics including adversarial evaluator and ADEM, our model consistently outperforms them by a large margin, which demonstrates that our comparison-based evaluation metric is able to evaluate sample quality more accurately. In addition, we \ufb01nd that evaluating generated samples by comparing it with a set of randomly selected samples or using sample- level skill rating performs almost equally well. This is not surprising as the employed skill rating is able to handle the inherent variance of players (i.e. NLG models).",
      "In addition, we \ufb01nd that evaluating generated samples by comparing it with a set of randomly selected samples or using sample- level skill rating performs almost equally well. This is not surprising as the employed skill rating is able to handle the inherent variance of players (i.e. NLG models). As this vari- ance does not exist when we regard a sample as a model which always generates the same sample.",
      "Task Story Generation Dialogue Metric Spearman Pearson Spearman Pearson our approach - with skill rating 0.392(< 0.001) 0.387(< 0.001) 0.473(< 0.001) 0.469(< 0.001) - with random sampled reference 0.389(< 0.001) 0.378(< 0.001) 0.461(< 0.001) 0.472(< 0.001) ADEM 0.162 0.148 0.353 0.341 Adversarial Evaluator 0.105 0.111 0.197 0.182 BLEU 0.032 0.028 0.053 0.061 Perplexity 0.052 0.057 0.024 0.015 Table 1: Sample-level correlation between metrics and human judgments, with p-values shown in brackets.",
      "Task Story Generation Dialogue Metric Spearman Pearson Spearman Pearson our approach - with skill rating 0.612(< 0.001) 0.631(< 0.001) 0.764(< 0.001) 0.783(< 0.001) - averaged sample-level skill rating 0.518 0.541 0.651 0.675 - averaged reference-based score 0.473 0.482 0.634 0.653 ADEM 0.291 0.302 0.541 0.553 Adversarial Evaluator 0.248 0.272 0.491 0.502 BLEU 0.096 0.103 0.217 0.293 Perplexity 0.113 0.127 0.071 0.083 Table 2: Model-level correlation between metrics and human judgments, with p-values shown in brackets. RQ2: Model-Level Correlation As for model-level eval- uation, we employ the average score of the evaluated 100 samples as each model\u2019s score and calculate their correlation with human scores.",
      "RQ2: Model-Level Correlation As for model-level eval- uation, we employ the average score of the evaluated 100 samples as each model\u2019s score and calculate their correlation with human scores. For comparative evaluator, we propose three different approaches to get an absolute score for each sample: 1) we calculate the average reference-based score (method 1 for sample-level comparison) of each sample as model-level score, 2) we calculate the average skill rating of each sample obtained in the experiments of RQ1 as model- level score, 2) we use the proposed skill rating system to get a model-level skill rating for each compared model. Results are shown in Table 2. We can see that the proposed comparative evaluator with skill rating signi\ufb01cantly outper- forms all compared baselines, including comparative eval- uator with averaged sample-level scores. This demonstrates the effectiveness of the skill rating system for performing model-level comparison with pairwise sample-level evalua- tion. In addition, the poor correlation between conventional evaluation metrics including BLEU and perplexity demon- strates the necessity of better automated evaluation metrics in open domain NLG evaluation.",
      "This demonstrates the effectiveness of the skill rating system for performing model-level comparison with pairwise sample-level evalua- tion. In addition, the poor correlation between conventional evaluation metrics including BLEU and perplexity demon- strates the necessity of better automated evaluation metrics in open domain NLG evaluation. RQ3&4: Automated Metrics for Model Training We further investigate the impact of imperfect metrics on train- ing NLG models. As described in the human evaluation pro- cedure, we perform 10 runs to test the reliability of each met- ric when used to perform hyperparameter tuning and early- stopping respectively. In each run, we select the best hyper- parameter combination or early-stopping checkpoint based on each of the \ufb01ve compared metrics. Human evaluation is then employed to identify the best choice. We evaluate the performance of each metric by how many times (out of 10) they succeeded in selecting the best hyperparameter combi- nation or early-stopping checkpoint (out of 4) and the aver- age human-annotated score for their selected models. The results are shown in Table 3.",
      "The results are shown in Table 3. We can see that conven- tional automated metrics perform poorly and result in sub- optimal result when performing hyperparameter search and selecting the best performing checkpoints. Converting eval- uation metric from BLEU or perplexity to the proposed com- parative evaluator can yield non-neglectable improvements without changing model architecture or training objective. While previous work on NLG evaluation mostly focuses on the evaluation stage and does not explore the in\ufb02uence of imperfect metrics during model training, our experiments demonstrate the existence of this problem and that the pro- posed method can, to some extent, alleviate this problem. 4.3 Qualitative Analysis We present several comparison examples in the Dailydialog dataset for qualitative analysis of the proposed comparative evaluator. From the \ufb01rst example, we can see that the com- parative evaluator is capable of identifying that generic and dull responses (i.e. \u201cI don\u2019t know\u201d) should be considered as of worse quality. The second example suggests that our ap- proach handles the diversity in possible responses well, as it regards both positive response and negative response as valid responses.",
      "\u201cI don\u2019t know\u201d) should be considered as of worse quality. The second example suggests that our ap- proach handles the diversity in possible responses well, as it regards both positive response and negative response as valid responses. Hopefully, these examples may provide us with some insights about why the proposed metric correlates better with human preference. 4.4 Ablation Study To better understand the proposed comparative evaluator and analyze the relative importance of its different components,",
      "Task Hyperparameter Search Early-stopping Metric win times averaged score win times averaged score our approach 8 3.14 9 3.41 ADEM 5 3.03 6 3.29 Adversarial Evaluator 4 2.92 4 3.23 BLEU 2 2.75 3 3.15 Perplexity 3 2.77 3 3.11 Table 3: Performance of different metrics in hyperparameter tuning and earlystop checkpoint selecting. Context Sample A Sample B Output Say,Jim,how about going for a few beers after dinner? I do not know about it. No, it is not good. A < B I suggest a walk over to the gym where we can meet some friends. That\u2019s a good idea, ok. No, I do not like to. Tie What shall we do ? I don\u2019t feel like sitting at home. We can go for a walk. I suggest staying at home. A > B Table 4: Examples of comparison results between two generated samples given context.",
      "No, I do not like to. Tie What shall we do ? I don\u2019t feel like sitting at home. We can go for a walk. I suggest staying at home. A > B Table 4: Examples of comparison results between two generated samples given context. Metric Spearman Pearson full model 0.764 0.783 w/o comparison 0.491 0.502 w/o tie option 0.557 0.561 w/o strong supervision 0.697 0.703 w/o weak supervision 0.728 0.737 w/o human annotation 0.602 0.609 w/o BERT 0.644 0.662 Table 5: Model-level correlation between ablated variants and human judgments in the Dailydialog dataset we conduct an ablation study with several variants of the proposed model: \u2022 w/o comparison: Evaluating generated samples without comparison, which degrades to the adversarial evaluation method. \u2022 w/o strong supervision: Training the comparative evalua- tor without \u201cstrong supervision\u201d, which models the induc- tive bias that human written reference samples are gen- erally of better quality compared with that generated by NLG models.",
      "\u2022 w/o strong supervision: Training the comparative evalua- tor without \u201cstrong supervision\u201d, which models the induc- tive bias that human written reference samples are gen- erally of better quality compared with that generated by NLG models. \u2022 w/o weak supervision: Training without \u201cweak supervi- sion\u201d, which models the inductive bias that the quality of NLG models generally improves during training. \u2022 w/o human preference annotation Training without hu- man annotated preference data (i.e. only with strong and weak supervision). \u2022 w/o tie option The variant of comparative evaluator where the model must select the better sample rather than able to admit its uncertainty. \u2022 w/o BERT The variant where the model is trained from scratch instead of \ufb01ne-tuning BERT. We evaluate these model variants on the Dailydialog dataset. Results are presented in Table 5. We can see that comparison-based evaluation is very effective as our model correlates much better than adversarial evaluator.",
      "We evaluate these model variants on the Dailydialog dataset. Results are presented in Table 5. We can see that comparison-based evaluation is very effective as our model correlates much better than adversarial evaluator. The tie op- tion is also very important as it can prevent the comparative evaluator from making uncertain decision and model the in- ductive bias that samples generated by the same model are generally of similar quality, which may help our model gen- eralize better. As for different sources of training examples, we \ufb01nd that human preference annotation is the most im- portant, which is not surprising. In addition, we \ufb01nd that the proposed weak supervision also helps, but is of smaller rela- tive importance compared with strong supervision. This may be due to the fact that examples constructed by the weak su- pervision approach may contain a lot of noise. We can also see that our model correlates well with human preference without training with human preference annotation, this is very important in practice as human annotations are not al- ways available. Finally, we \ufb01nd that transferring the natural language understanding ability from BERT to be very im- portant for the \ufb01nal performance.",
      "Finally, we \ufb01nd that transferring the natural language understanding ability from BERT to be very im- portant for the \ufb01nal performance. 5 Discussion and Conclusion In this paper, we present a novel comparison-based parame- terized automated evaluation metric for evaluating open do- main NLG models. The proposed model is based on the intuition that we can better evaluate the quality of a sam- ple by comparing it with other samples. Our model allows the model to admit its uncertainty with the \u201ctie\u201d option. We adopt the skill rating system to perform model-level evalua- tion based on sample-level pairwise comparison. By transferring pretrained natural language understand- ing knowledge from BERT and \ufb01ne-tuning with strong and weak supervision examples and human preference annota- tions, our model correlates better with human judgment than other compared metrics. In addition, we \ufb01nd that when used as evaluation metrics, conventional metrics such as BLEU and perplexity may affect the training stage of NLG models as they may lead to sub-optimal hyperparameter choice and checkpoint selection. Our model, in contrast, is much more reliable when performing these choices.",
      "References [Banerjee and Lavie] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with im- proved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation mea- sures for machine translation and/or summarization, pages 65\u201372, 2005. [Bengio et al.] Yoshua Bengio, J\u00b4er\u02c6ome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Pro- ceedings of the 26th annual international conference on ma- chine learning, pages 41\u201348. ACM, 2009. [Chaganty et al.] Arun Tejasvi Chaganty, Stephen Muss- man, and Percy Liang. The price of debiasing automatic metrics in natural language evaluation. arXiv preprint arXiv:1807.02202, 2018. [Devlin et al.] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.",
      "The price of debiasing automatic metrics in natural language evaluation. arXiv preprint arXiv:1807.02202, 2018. [Devlin et al.] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidi- rectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [Elo] Arpad E Elo. The rating of chessplayers, past and present. Arco Pub., 1978. [Fan et al.] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. arXiv preprint arXiv:1805.04833, 2018. [Garbacea et al.] Cristina Garbacea, Samuel Carton, Shiyan Yan, and Qiaozhu Mei. Judge the judges: A large-scale eval- uation study of neural language models for online review generation. arXiv preprint arXiv:1901.00398, 2019. [Gehring et al.]",
      "Judge the judges: A large-scale eval- uation study of neural language models for online review generation. arXiv preprint arXiv:1901.00398, 2019. [Gehring et al.] Jonas Gehring, Michael Auli, David Grang- ier, Denis Yarats, and Yann N Dauphin. Convolutional se- quence to sequence learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1243\u20131252. JMLR. org, 2017. [Glickman] Mark E Glickman. Example of the glicko-2 sys- tem. Boston University, pages 1\u20136, 2012. [Goodfellow et al.] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversar- ial nets. In Advances in neural information processing sys- tems, pages 2672\u20132680, 2014.",
      "Generative adversar- ial nets. In Advances in neural information processing sys- tems, pages 2672\u20132680, 2014. [Hashimoto et al.] Tatsunori B Hashimoto, Hugh Zhang, and Percy Liang. Unifying human and statistical eval- uation for natural language generation. arXiv preprint arXiv:1904.02792, 2019. [Herbrich et al.] Ralf Herbrich, Tom Minka, and Thore Grae- pel. Trueskill: a bayesian skill rating system. In Advances in neural information processing systems, pages 569\u2013576, 2007. [Hochreiter and Schmidhuber] Sepp Hochreiter and J\u00a8urgen Schmidhuber. Long short-term memory. Neural computa- tion, 9(8):1735\u20131780, 1997. [Kannan and Vinyals] Anjuli Kannan and Oriol Vinyals. Ad- versarial evaluation of dialogue models.",
      "Long short-term memory. Neural computa- tion, 9(8):1735\u20131780, 1997. [Kannan and Vinyals] Anjuli Kannan and Oriol Vinyals. Ad- versarial evaluation of dialogue models. arXiv preprint arXiv:1701.08198, 2017. [Li et al.] Jiwei Li, Will Monroe, Tianlin Shi, S\u00b4ebastien Jean, Alan Ritter, and Dan Jurafsky. Adversarial learning for neu- ral dialogue generation. arXiv preprint arXiv:1701.06547, 2017. [Li et al.] Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. Dailydialog: A manu- ally labelled multi-turn dialogue dataset. arXiv preprint arXiv:1710.03957, 2017. [Lin] Chin-Yew Lin. Rouge: A package for automatic eval- uation of summaries.",
      "Dailydialog: A manu- ally labelled multi-turn dialogue dataset. arXiv preprint arXiv:1710.03957, 2017. [Lin] Chin-Yew Lin. Rouge: A package for automatic eval- uation of summaries. In Text summarization branches out, pages 74\u201381, 2004. [Liu et al.] Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael Noseworthy, Laurent Charlin, and Joelle Pineau. How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue re- sponse generation. arXiv preprint arXiv:1603.08023, 2016. [Lowe et al.] Ryan Lowe, Michael Noseworthy, Iulian V Ser- ban, Nicolas Angelard-Gontier, Yoshua Bengio, and Joelle Pineau. Towards an automatic turing test: Learning to eval- uate dialogue responses. arXiv preprint arXiv:1708.07149, 2017. [Olsson et al.]",
      "Towards an automatic turing test: Learning to eval- uate dialogue responses. arXiv preprint arXiv:1708.07149, 2017. [Olsson et al.] Catherine Olsson, Surya Bhupatiraju, Tom Brown, Augustus Odena, and Ian Goodfellow. Skill rat- ing for generative models. arXiv preprint arXiv:1808.04888, 2018. [Papineni et al.] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th an- nual meeting on association for computational linguistics, pages 311\u2013318. Association for Computational Linguistics, 2002. [See et al.] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with pointer- generator networks.",
      "Association for Computational Linguistics, 2002. [See et al.] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with pointer- generator networks. Proceedings of the 55th Annual Meet- ing of the Association for Computational Linguistics (Vol- ume 1: Long Papers), 2017. [Shimanaka et al.] Hiroki Shimanaka, Tomoyuki Kajiwara, and Mamoru Komachi. Ruse: Regressor using sentence em- beddings for automatic machine translation evaluation. In Proceedings of the Third Conference on Machine Transla- tion: Shared Task Papers, pages 751\u2013758, 2018. [Snoek et al.] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine learn- ing algorithms. In Advances in neural information process- ing systems, pages 2951\u20132959, 2012. [Sordoni et al.]",
      "Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine learn- ing algorithms. In Advances in neural information process- ing systems, pages 2951\u20132959, 2012. [Sordoni et al.] Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Margaret Mitchell, Jian- Yun Nie, Jianfeng Gao, and Bill Dolan. A neural net- work approach to context-sensitive generation of conversa- tional responses. Proceedings of the 2015 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, 2015. [Sutskever et al.] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104\u20133112, 2014.",
      "[Vaswani et al.] Ashish Vaswani, Noam Shazeer, Niki Par- mar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998\u20136008, 2017. [Vermorel and Mohri] Joannes Vermorel and Mehryar Mohri. Multi-armed bandit algorithms and empirical evaluation. In European conference on machine learning, pages 437\u2013448. Springer, 2005. [Zhang et al.] Tianyi Zhang, Varsha Kishore, Felix Wu, Kil- ian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-2002.05058.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":10468,
  "avg_doclen":186.9285714286,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-2002.05058.pdf"
    }
  }
}