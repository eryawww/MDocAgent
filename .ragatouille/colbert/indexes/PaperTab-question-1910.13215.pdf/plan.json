{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Transformer-based Cascaded Multimodal Speech Translation Zixiu Wu1, Ozan Caglayan1, Julia Ive2, Josiah Wang1, Lucia Specia1 1Department of Computing, Imperial College London, UK 2Department of Computer Science, University of Shef\ufb01eld, UK {zixiu.wu18, o.caglayan, josiah.wang, l.specia}@imperial.ac.uk j.ive@sheffield.ac.uk Abstract This paper describes the cascaded multimodal speech trans- lation systems developed by Imperial College London for the IWSLT 2019 evaluation campaign. The architecture con- sists of an automatic speech recognition (ASR) system fol- lowed by a Transformer-based multimodal machine transla- tion (MMT) system. While the ASR component is identi- cal across the experiments, the MMT model varies in terms of the way of integrating the visual context (simple condi- tioning vs. attention), the type of visual features exploited (pooled, convolutional, action categories) and the underlying architecture.",
            "While the ASR component is identi- cal across the experiments, the MMT model varies in terms of the way of integrating the visual context (simple condi- tioning vs. attention), the type of visual features exploited (pooled, convolutional, action categories) and the underlying architecture. For the latter, we explore both the canonical transformer [1] and its deliberation version [2] with additive and cascade variants which differ in how they integrate the textual attention. Upon conducting extensive experiments, we found that (i) the explored visual integration schemes of- ten harm the translation performance for the transformer and additive deliberation, but considerably improve the cascade deliberation; (ii) the transformer and cascade deliberation in- tegrate the visual modality better than the additive delibera- tion, as shown by the incongruence analysis. 1. Introduction The recently introduced How2 dataset [3] has stimulated re- search around multimodal language understanding through the availability of 300h instructional videos, English subti- tles and their Portuguese translations.",
            "1. Introduction The recently introduced How2 dataset [3] has stimulated re- search around multimodal language understanding through the availability of 300h instructional videos, English subti- tles and their Portuguese translations. For example, [4] suc- cessfully demonstrates that semantically rich action-based visual features are helpful in the context of machine trans- lation (MT), especially in the presence of input noise that manifests itself as missing source words. Therefore, we hy- pothesize that a speech-to-text translation (STT) system may also bene\ufb01t from the visual context, especially in the tra- ditional cascaded framework [5, 6] where noisy automatic transcripts are obtained from an automatic speech recogni- tion system (ASR) and further translated into the target lan- guage using a machine translation (MT) component. The dataset enables the design of such multimodal STT systems, since we have access to a bilingual corpora as well as the corresponding audio-visual stream.",
            "The dataset enables the design of such multimodal STT systems, since we have access to a bilingual corpora as well as the corresponding audio-visual stream. Hence, in this paper, we propose a cascaded multimodal STT with two components: (i) an English ASR system trained on the How2 dataset and (ii) a transformer-based [1] visually grounded MMT system. MMT is a relatively new research topic which is interested in leveraging auxiliary modalities such as audio or vision in order to improve translation performance [7]. MMT has proved effective in scenarios such as for disambiguation [8] or when the source sentences are corrupted [9]. So far, MMT has mostly focused on integrating visual features into neural MT (NMT) systems using visual attention through convolutional feature maps [10, 11] or visual conditioning of encoder\/decoder blocks through fully-connected features [12, 13, 14, 15]. Inspired by previous research in MMT, we explore several multimodal integration schemes using action-level video fea- tures. Speci\ufb01cally, we experiment with visually conditioning the encoder output and adding visual attention to the decoder.",
            "Inspired by previous research in MMT, we explore several multimodal integration schemes using action-level video fea- tures. Speci\ufb01cally, we experiment with visually conditioning the encoder output and adding visual attention to the decoder. We further extend the proposed schemes to the deliberation variant [2] of the canonical transformer in two ways: addi- tive and cascade multimodal deliberation, which are distinct in their textual attention regimes. Overall, the results show that multimodality in general leads to performance degrada- tion for the canonical transformer and the additive deliber- ation variant, but can result in substantial improvements for the cascade deliberation. Our incongruence analysis [16] re- veals that the transformer and cascade deliberation are more sensitive to and therefore more reliant on visual features for translation, whereas the additive deliberation is much less impacted. We also observe that incongruence sensitivity and translation performance are not necessarily correlated. 2. Methods In this section, we brie\ufb02y describe the proposed multimodal speech translation system and its components. 2.1.",
            "We also observe that incongruence sensitivity and translation performance are not necessarily correlated. 2. Methods In this section, we brie\ufb02y describe the proposed multimodal speech translation system and its components. 2.1. Automatic Speech Recognition The baseline ASR system that we use to obtain English tran- scripts is an attentive sequence-to-sequence architecture with a stacked encoder of 6 bidirectional LSTM layers [17]. Each LSTM layer is followed by a tanh projection layer. The mid- arXiv:1910.13215v3  [cs.CL]  8 Nov 2019",
            "dle two LSTM layers apply a temporal subsampling [18] by skipping every other input, reducing the length of the se- quence X from T to T\/4. All LSTM and projection layers have 320 hidden units. The forward-pass of the encoder pro- duces the source encodings on top of which attention will be applied within the decoder. The hidden and cell states of all LSTM layers are initialized with 0. The decoder is a 2-layer stacked GRU [19], where the \ufb01rst GRU receives the previ- ous hidden state of the second GRU in a transitional way. GRU layers, attention layer and embeddings have 320 hidden units. We share the input and output embeddings to reduce the number of parameters [20]. At timestep t=0, the hidden state of the \ufb01rst GRU is initialized with the average-pooled source encoding states. 2.2. Deliberation-based NMT A human translator typically produces a translation draft \ufb01rst, and then re\ufb01nes it towards the \ufb01nal translation.",
            "2.2. Deliberation-based NMT A human translator typically produces a translation draft \ufb01rst, and then re\ufb01nes it towards the \ufb01nal translation. The idea behind the deliberation networks [21] simulates this process by extending the conventional attentive encoder- decoder architecture [22] with a second pass re\ufb01nement de- coder. Speci\ufb01cally, the encoder \ufb01rst encodes a source sen- tence of length N into a sequence of hidden states H = {h1, h2, . . . , hN} on top of which the \ufb01rst pass decoder 1P applies the attention. The pre-softmax hidden states {\u02c6s1, \u02c6s2, . . . , \u02c6sM} produced by the decoder leads to a \ufb01rst pass translation {\u02c6y1, \u02c6y2, . . . , \u02c6yM}.",
            ". . , \u02c6sM} produced by the decoder leads to a \ufb01rst pass translation {\u02c6y1, \u02c6y2, . . . , \u02c6yM}. The second pass decoder 2P intervenes at this point and generates a second translation by attending separately to both H and the concatenated state vectors {[\u02c6s1; \u02c6y1], [\u02c6s2; \u02c6y2], . . . , [\u02c6sM; \u02c6yM]}. Two context vec- tors are produced as a result, and they are joint inputs with st\u22121 (previous hidden state of 2P) and yt\u22121 (previous output of 2P) to 2P to yield st and then yt. A transformer-based deliberation architecture is proposed by [2]. It follows the same two-pass re\ufb01nement process, with every second-pass decoder block attending to both the en- coder output H and the \ufb01rst-pass pre-softmax hidden states \u02c6S.",
            "A transformer-based deliberation architecture is proposed by [2]. It follows the same two-pass re\ufb01nement process, with every second-pass decoder block attending to both the en- coder output H and the \ufb01rst-pass pre-softmax hidden states \u02c6S. However, it differs from [21] in that the actual \ufb01rst-pass translation \u02c6Y is not used for the second-pass attention. 2.3. Multimodality 2.3.1. Visual Features We experiment with three types of video features, namely average-pooled vector representations (AvgPool), convolu- tional layer outputs (Conv), and Ten-Hot action category embeddings (Emb). The AvgPool features are provided by the How2 dataset using the following approach: a video is segmented into smaller parts of 16 frames each, and the segments are fed to a 3D ResNeXt-101 CNN [23], trained to recognise 400 action classes [24]. The 2048-D fully- connected features are then averaged across the segments to obtain a single AvgPool feature vector for the overall video.",
            "The 2048-D fully- connected features are then averaged across the segments to obtain a single AvgPool feature vector for the overall video. In order to obtain the Conv features, 16 equi-distant frames are sampled from a video, and they are then used as input to an in\ufb02ated 3D ResNet-50 CNN [25] \ufb01ne-tuned on the Mo- ments in Time action video dataset. The CNN hence takes in a video and classi\ufb01es it into one of 339 categories. The Conv features, taken at the CONV4 layer of the network, has a 7 \u00d7 7 \u00d7 2048 dimensionality. Higher-level semantic information can be more helpful than convolutional features. We apply the same CNN to a video as we do for Conv features, but this time the focus is on the softmax layer output: we process the embedding matrix to keep the 10 most probable category embeddings intact while zeroing out the remaining ones. We call this representation ten-hot action category embeddings (Emb). 2.3.2.",
            "We call this representation ten-hot action category embeddings (Emb). 2.3.2. Integration Approaches Encoder with Additive Visual Conditioning (Enc-Cond) In this approach, inspired by [8], we add a projection of the visual features to each output of the vanilla transformer en- coder (Enc-Van). This projection is strictly linear from the 2048-D AvgPool features to the 1024-D space in which the self attention hidden states reside, and the projection matrix is learned jointly with the translation model. Decoder with Visual Attention (Dec-Attn) In order to accommodate attention to visual features at the decoder side and inspired by [26], we insert one layer of vi- sual cross attention at a decoder block immediately before the fully-connected layer. We name the transformer decoder with such an extra layer as Trans-Dec-Attn, where this layer is immediately after the textual attention to the encoder output. Speci\ufb01cally, we experiment with attention to Conv, Emb and AvgPool features separately.",
            "We name the transformer decoder with such an extra layer as Trans-Dec-Attn, where this layer is immediately after the textual attention to the encoder output. Speci\ufb01cally, we experiment with attention to Conv, Emb and AvgPool features separately. The visual attention is distributed across the 49 video regions in Conv, the 339 action category word embeddings in Emb, or the 32 rows in AvgPool where we reshape the 2048-D AvgPool vector into a 32 \u00d7 64 matrix. 2.3.3. Multimodal Transformers The vanilla text-only transformer (Trans-Baseline) is used as a baseline, and we design two variants: with addi- tive visual conditioning (Trans-Cond) and with attention to visual features (Trans-Attn). A Trans-Cond fea- tures a Enc-Cond and a vanilla transformer decoder (Dec- Van), therefore utilising visual information only at the en- coder side. In contrast, a Trans-Attn is con\ufb01gured with a Enc-Van and a Trans-Dec-Attn, exploiting visual cues only at the decoder. Figure 1 summarises the two approaches.",
            "Figure 1: Unimodal and multimodal transformers: Trans-Cond and the Trans-Attn extend the text-only Trans-Baseline with dashed green- and blue-arrow routes, respectively. Each multimodal model activates either the dashed green-arrow route for Trans-Cond or one of the three dashed blue-arrow routes (i.e. VideoSum, Action Category Embeddings or Convolutional Layer Output, as shown) for Trans-Attn. 2.3.4. Multimodal Deliberation Our multimodal deliberation models differ from each other in two ways: whether to use additive (A) [8] or cascade (C) textual deliberation to integrate the textual attention to the original input and to the \ufb01rst pass, and whether to employ vi- sual attention (Delib-Attn) or additive visual condition- ing (Delib-Cond) to integrate the visual features into the textual MT model. Figures 2 and 3 show the con\ufb01gurations of our additive and cascade deliberation models, respectively, each also showing the connections necessary for Delib- Cond and Delib-Attn.",
            "Figures 2 and 3 show the con\ufb01gurations of our additive and cascade deliberation models, respectively, each also showing the connections necessary for Delib- Cond and Delib-Attn. Additive (A) & Cascade (C) Textual Deliberation In an additive-deliberation second-pass decoder (A- Delib-2P) block, the \ufb01rst layer is still self-attention, whereas the second layer is the addition of two separate at- tention sub-layers. The \ufb01rst sub-layer attends to the encoder output in the same way Dec-Van does, while the attention of the second sub-layer is distributed across the concatenated \ufb01rst pass outputs and hidden states. The input to both sub- layers is the output of the self-attention layer, and the outputs of the sub-layers are summed as the \ufb01nal output and then (with a residual connection) fed to the visual attention layer if the decoder is multimodal or to the fully connected layer otherwise. For the cascade version, the only difference is that, instead of two sub-layers, we have two separate, successive layers with the same functionalities.",
            "For the cascade version, the only difference is that, instead of two sub-layers, we have two separate, successive layers with the same functionalities. It is worth mentioning that we introduce the attention to the \ufb01rst pass only at the initial three decoder blocks out of the total six of the second pass decoder (Delib-2P), following [8]. Additive Visual Conditioning (Delib-Cond) & Visual Attention (Delib-Attn) Delib-Cond and Delib-Attn are simply applying Enc- Cond and Dec-Attn respectively to a deliberation model, therefore more details have been introduced in Section 2.3.2. For Delib-Cond, similar to in Trans-Cond, we add a pro- jection of the visual features to the output of Enc-Van, and use Dec-Van as the \ufb01rst pass decoder and either additive or cascade deliberation as the Delib-2P.",
            "For Delib-Cond, similar to in Trans-Cond, we add a pro- jection of the visual features to the output of Enc-Van, and use Dec-Van as the \ufb01rst pass decoder and either additive or cascade deliberation as the Delib-2P. For Delib-Attn, in a similar vein as Trans-Attn, the encoder in this setting is simply Enc-Van and the \ufb01rst pass decoder is just Dec-Van, but this time Delib-2P is respon- sible for attending to the \ufb01rst pass output as well as the visual features. For both additive and cascade deliberation, a vi- sual attention layer is inserted immediately before the fully- connected layer, so that the penultimate layer of a decoder block now attends to visual information. 3. Experiments 3.1. Dataset We stick to the default training\/validation\/test splits and the pre-extracted speech features for the How2 dataset, as pro- vided by the organizers. As for the pre-processing, we low-",
            "Figure 2: Unimodal and multimodal additive deliberation: Delib-Cond and Delib-Attn extend the text-only Delib- Baseline with dashed green- and blue-arrow routes, respectively. Each multimodal model activates either the dashed green- arrow route for Delib-Cond or one of the three dashed blue-arrow routes (i.e. VideoSum, Action Category Embeddings or Convolutional Layer Output, as shown) for Delib-Attn. Figure 3: Unimodal and multimodal cascade deliberation: Delib-Cond and Delib-Attn extend the text-only Delib- Baseline with dashed green- and blue-arrow routes, respectively. Each multimodal model activates either the dashed green- arrow route for Delib-Cond or one of the three dashed blue-arrow routes (i.e. VideoSum, Action Category Embeddings or Convolutional Layer Output, as shown) for Delib-Attn. ercase the sentences and then tokenise them using Moses [27]. We then apply subword segmentation [28] by learning separate English and Portuguese models with 20,000 merge operations each.",
            "ercase the sentences and then tokenise them using Moses [27]. We then apply subword segmentation [28] by learning separate English and Portuguese models with 20,000 merge operations each. The English corpus used when training the subword model consists of both the ground-truth video sub- titles and the noisy transcripts produced by the underlying",
            "ASR system. We do not share vocabularies between the source and target domains. Finally for the post-processing step, we merge the subword tokens, apply recasing and deto- kenisation. The recasing model is a standard Moses baseline trained again on the parallel How2 corpus. The baseline ASR system is trained on the How2 dataset as well. This system is then used to obtain noisy transcripts for the whole dataset, using beam-search with beam size of 10. The pre-processing pipeline for the ASR is different from the MT pipeline in the sense that the punctuations are removed and the subword segmentation is performed using Senten- cePiece [29] with a vocabulary size of 5,000. The test-set performance of this ASR is around 19% WER. 3.2. Training We train our transformer and deliberation models until con- vergence largely with transformer big hyperparame- ters: 16 attention heads, 1024-D hidden states and a dropout of 0.1. During inference, we apply beam-search with beam size of 10.",
            "Training We train our transformer and deliberation models until con- vergence largely with transformer big hyperparame- ters: 16 attention heads, 1024-D hidden states and a dropout of 0.1. During inference, we apply beam-search with beam size of 10. For deliberation, we \ufb01rst train the underlying transformer model until convergence, and use its weights to initialise the encoder and the \ufb01rst pass decoder. After freez- ing those weights, we train Delib-2P until convergence. The reason for the partial freezing is that our preliminary experiments showed that it enabled better performance com- pared to updating the whole model. Following [21], we ob- tain 10-best samples from the \ufb01rst pass with beam-search for source augmentation during the training of Delib-2P.",
            "The reason for the partial freezing is that our preliminary experiments showed that it enabled better performance com- pared to updating the whole model. Following [21], we ob- tain 10-best samples from the \ufb01rst pass with beam-search for source augmentation during the training of Delib-2P. We train all the models on an Nvidia RTX 2080Ti with a batch size of 1024, a base learning rate of 0.02 with 8,000 warm-up steps for the Adam [30] optimiser, and a patience of 10 epochs for early stopping based on approx-BLEU (tensor2tensor) for the transformers and 3 epochs for the deliberation models. After the training \ufb01nishes, we eval- uate all the checkpoints on the validation set and compute the real BLEU [31] scores, based on which we select the best model for inference on the test set. The transformer and the deliberation models are based upon the tensor2tensor1 library [32] (v1.3.0 RC1) as well as the vanilla transformer- based deliberation2 [21] and their multimodal variants3 [8]. 4.",
            "The transformer and the deliberation models are based upon the tensor2tensor1 library [32] (v1.3.0 RC1) as well as the vanilla transformer- based deliberation2 [21] and their multimodal variants3 [8]. 4. Results & Analysis 4.1. Quantitative Results We report tokenised results obtained using the multeval toolkit [33]. We focus on single system performance and thus, do not perform any ensembling or checkpoint averag- ing. 1https:\/\/github.com\/tensorflow\/tensor2tensor 2https:\/\/github.com\/ustctf\/delibnet 3https:\/\/github.com\/ImperialNLP\/MMT-Delib Table 1: BLEU scores for the test set: bold highlights our best results. \u2020 indicates a system is signi\ufb01cantly different from its text-only counterpart (p-value \u22640.05).",
            "\u2020 indicates a system is signi\ufb01cantly different from its text-only counterpart (p-value \u22640.05). SETUP TR A N S A-DE L I B C-DELIB BA S E L I N E 39.8 37.6 36.4 CO N D-AV GPO O L 39.7 36.0 \u2020 36.2 AT T N-AV GPO O L 39.7 37.6 37.4 \u2020 AT T N-EM B 39.7 37.0 \u2020 37.3 \u2020 AT T N-CO N V 39.8 37.2 37.0 The BLEU scores of the models are shown in Table 1. Evi- dent from the table is that the best models overall are Trans- Baseline and Trans-Attn-Conv with a BLEU score of 39.8, and the other multimodal transformers have slightly worse performance, showing score drops around 0.1.",
            "Evi- dent from the table is that the best models overall are Trans- Baseline and Trans-Attn-Conv with a BLEU score of 39.8, and the other multimodal transformers have slightly worse performance, showing score drops around 0.1. Also, none of the multimodal transformer systems are signi\ufb01cantly different from the baseline, which is a sign of the limited ex- tent to which visual features affect the output. For additive deliberation (A-Delib), the performance variation is considerably larger: Attn-AvgPool and Baseline take the lead with 37.6 BLEU, but the next best system (Attn-Conv) plunges to 37.2. The other two (Cond-AvgPool & Attn-Emb) also have noticeably worse results (36.0 and 37.0). Overall, however, A-Delib is still similar to the transformers in that the baseline generally yields higher-quality translations. Cascade deliberation, on the other hand, is different in that its text-only baseline is outperformed by most of its multi- modal counterparts.",
            "Overall, however, A-Delib is still similar to the transformers in that the baseline generally yields higher-quality translations. Cascade deliberation, on the other hand, is different in that its text-only baseline is outperformed by most of its multi- modal counterparts. Multimodality enables boosts as large as around 1 BLEU point in the cases of Attn-AvgPool and Attn-Emb, both of which achieve about 37.4 BLEU and are signi\ufb01cantly different from the baseline. Another observation is that the deliberation models as a whole lead to worse performance than the canonical trans- formers, with BLEU deterioration ranging from 2.3 (across Attn-AvgPool variants) to 3.5 (across Cond-AvgPool systems), which de\ufb01es the \ufb01ndings of [8]. We leave this to future investigations. 4.2. Incongruence Analysis To further probe the effect of multimodality, we follow the incongruent decoding approach [16], where our multimodal models are fed with mismatched visual features.",
            "We leave this to future investigations. 4.2. Incongruence Analysis To further probe the effect of multimodality, we follow the incongruent decoding approach [16], where our multimodal models are fed with mismatched visual features. The general assumption is that a model will have learned to exploit visual information to help with its translation, if it shows substantial performance degradation when given wrong visual features. The results are reported in Table 2. Overall, there are considerable parallels between the trans-",
            "Table 2: Incongruent decoding results for the test set: BLEU changes are w.r.t the congruent counterparts from Table 1. \u2020 marks incongruent decoding results that are signi\ufb01cantly different (p-value \u22640.05) from congruent counterparts.",
            "Table 2: Incongruent decoding results for the test set: BLEU changes are w.r.t the congruent counterparts from Table 1. \u2020 marks incongruent decoding results that are signi\ufb01cantly different (p-value \u22640.05) from congruent counterparts. SETUP TRANS A-DELIB C-DE L I B CO N D-AV GPO OL \u21930.5 \u2020 \u21910.1 \u21930.6 \u2020 AT T N-AV GPO O L \u21930.3 0 \u21930.1 AT T N-EM B \u21930.4 \u2020 \u21910.1 \u21930.2 \u2020 AT T N-CO N V \u21930.1 \u21930.2 \u21930.2 formers and the cascade deliberation models in terms of the incongruence effect, such as universal performance de- terioration (ranging from 0.1 to 0.6 BLEU) and more no- ticeable score changes (\u21930.5 BLEU for Trans-Cond- AvgPool and \u21930.6 BLEU for C-Delib-Cond-AvgPool) in the Cond-AvgPool setting compared to the other sce- narios.",
            "Additive deliberation, however, manifests a drasti- cally different pattern, showing almost no incongruence ef- fect for Attn-AvgPool, only a 0.2 BLEU decrease for Attn-Conv, and even a 0.1 BLEU boost for Attn-Emb and Cond-AvgPool. Therefore, the determination can be made that Trans and C- Delib models are considerably more sensitive to incorrect visual information than A-Delib, which means the former better utilise visual clues during translation. Interestingly, the extent of performance degradation caused by incongruence is not necessarily correlated with the con- gruent BLEU scores. For example, Trans-Attn-Conv is on par with Trans-Attn-Emb in congruent decoding (dif- fering by around 0.1 BLEU), but the former suffers only a 0.1-BLEU loss with incongruence whereas the \ufb01gure for the latter is 0.4, in addition to the fact that the latter becomes sig- ni\ufb01cantly different after incongruent decoding.",
            "This means that some multimodal models that are sensitive to incongru- ence likely complement visual attention with textual atten- tion but without getting higher-quality translation as a result. The differences between the multimodal behaviour of addi- tive and cascade deliberation also warrant more investiga- tion, since the two types of deliberation are identical in their utilisation of visual features and only vary in their handling of the textual attention to the outputs of the encoder and the \ufb01rst pass decoder. 5. Conclusions We explored a series of transformers and deliberation based models to approach cascaded multimodal speech translation as our participation in the How2-based speech translation task of IWSLT 2019. We submitted the Trans-Attn-Conv system, which is a canonical transformer with visual atten- tion over the convolutional features, as our primary system with the remaining ones marked as contrastive ones. The primary system obtained a BLEU of 39.63 on the public IWSLT19 test set, whereas Trans-Baseline, the top con- trastive system on the same set, achieved 39.85.",
            "The primary system obtained a BLEU of 39.63 on the public IWSLT19 test set, whereas Trans-Baseline, the top con- trastive system on the same set, achieved 39.85. Our main conclusions are as follows: (i) the visual modality causes varying levels of translation quality damage to the transform- ers and additive deliberation, but boosts cascade delibera- tion; (ii) the multimodal transformers and cascade delibera- tion show performance degradation due to incongruence, but additive deliberation is not as affected; (iii) there is no strict correlation between incongruence sensitivity and translation performance. 6. Acknowledgements This work was supported by the MultiMT (H2020 ERC Starting Grant No. 678017) and MMVC (Newton Fund In- stitutional Links Grant, ID 352343575) projects. 7.",
            "6. Acknowledgements This work was supported by the MultiMT (H2020 ERC Starting Grant No. 678017) and MMVC (Newton Fund In- stitutional Links Grant, ID 352343575) projects. 7. References [1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in Advances in neural in- formation processing systems, 2017, pp. 5998\u20136008. [2] H. Hassan, A. Aue, C. Chen, V. Chowdhary, J. Clark, C. Federmann, X. Huang, M. Junczys-Dowmunt, W. Lewis, M. Li, et al., \u201cAchieving human parity on automatic chinese to english news translation,\u201d arXiv preprint arXiv:1803.05567, 2018.",
            "[3] R. Sanabria, O. Caglayan, S. Palaskar, D. Elliott, L. Barrault, L. Specia, and F. Metze, \u201cHow2: a large- scale dataset for multimodal language understanding,\u201d in Proceedings of the Workshop on Visually Grounded Interaction and Language (ViGIL). NeurIPS, 2018. [Online]. Available: http:\/\/arxiv.org\/abs\/1811.00347 [4] Z. Wu, J. Ive, J. Wang, P. Madhyastha, and L. Specia, \u201cPredicting actions to help predict translations,\u201d arXiv preprint arXiv:1908.01665, 2019. [5] F. Casacuberta, M. Federico, H. Ney, and E. Vidal, \u201cRecent efforts in spoken language translation,\u201d IEEE Signal Processing Magazine, vol. 25, no. 3, pp. 80\u201388, May 2008. [6] A. Waibel and C. Fugen, \u201cSpoken language transla- tion,\u201d IEEE Signal Processing Magazine, vol.",
            "25, no. 3, pp. 80\u201388, May 2008. [6] A. Waibel and C. Fugen, \u201cSpoken language transla- tion,\u201d IEEE Signal Processing Magazine, vol. 25, no. 3, pp. 70\u201379, 2008. [7] L. Specia, S. Frank, K. Sima\u2019an, and D. Elliott, \u201cA shared task on multimodal machine translation and crosslingual image description,\u201d in Proceedings of the First Conference on Machine Translation,",
            "Berlin, Germany, August 2016, pp. 543\u2013553. [Online]. Available: http:\/\/www.aclweb.org\/anthology\/W\/W16\/ W16-2346 [8] J. Ive, Madhyastha, P. Swaroop, and L. Specia, \u201cDistill- ing Translations with Visual Awareness,\u201d in Proceed- ings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. [9] O. Caglayan, P. Madhyastha, L. Specia, and L. Barrault, \u201cProbing the need for visual context in multimodal machine translation,\u201d in Proceedings of the 2019 Con- ference of the North American Chapter of the Associa- tion for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), 2019, pp. 4159\u20134170. [Online].",
            "4159\u20134170. [Online]. Available: https:\/\/www.aclweb.org\/anthology\/N19-1422\/ [10] O. Caglayan, W. Aransa, Y. Wang, M. Masana, M. Garc\u00b4\u0131a-Mart\u00b4\u0131nez, F. Bougares, L. Barrault, and J. van de Weijer, \u201cDoes multimodality help human and machine for translation and image captioning?\u201d in Proceedings of the First Conference on Machine Translation. Berlin, Germany: Association for Computational Linguistics, August 2016, pp. 627\u2013633. [Online]. Available: http:\/\/www.aclweb.org\/anthology\/ W\/W16\/W16-2358 [11] J. Libovick\u00b4y and J. Helcl, \u201cAttention strategies for multi-source sequence-to-sequence learning,\u201d in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, 2017, pp. 196\u2013202. [Online].",
            "Association for Computational Linguistics, 2017, pp. 196\u2013202. [Online]. Available: http:\/\/aclweb.org\/anthology\/P17-2031 [12] O. Caglayan, W. Aransa, A. Bardet, M. Garc\u00b4\u0131a- Mart\u00b4\u0131nez, F. Bougares, L. Barrault, M. Masana, L. Her- ranz, and J. van de Weijer, \u201cLIUM-CVC submissions for WMT17 multimodal translation task,\u201d in Proceed- ings of the Second Conference on Machine Translation, Volume 2: Shared Task Papers. Copenhagen, Den- mark: Association for Computational Linguistics, September 2017, pp. 432\u2013439. [Online]. Available: http:\/\/www.aclweb.org\/anthology\/W17-4746.pdf [13] I. Calixto and Q. Liu, \u201cIncorporating global vi- sual features into attention-based neural machine translation.\u201d in Proceedings of the 2017 Con- ference on Empirical Methods in Natural Lan- guage Processing.",
            "Copenhagen, Denmark: As- sociation for Computational Linguistics, Septem- ber 2017, pp. 992\u20131003. [Online]. Available: https:\/\/www.aclweb.org\/anthology\/D17-1105 [14] P. S. Madhyastha, J. Wang, and L. Specia, \u201cShef\ufb01eld multimt: Using object posterior predictions for mul- timodal machine translation,\u201d in Proceedings of the Second Conference on Machine Translation, 2017, pp. 470\u2013476. [15] S.-A. Grnroos, B. Huet, M. Kurimo, J. Laaksonen, B. Merialdo, P. Pham, M. Sjberg, U. Suluba- cak, J. Tiedemann, R. Troncy, and R. Vzquez, \u201cThe MeMAD submission to the WMT18 multi- modal translation task,\u201d in Proceedings of the Third Conference on Machine Translation. Belgium, Brus- sels: Association for Computational Linguistics, October 2018, pp. 609\u2013617. [Online].",
            "Belgium, Brus- sels: Association for Computational Linguistics, October 2018, pp. 609\u2013617. [Online]. Available: http:\/\/www.aclweb.org\/anthology\/W18-64066 [16] D. Elliott, \u201cAdversarial evaluation of multimodal ma- chine translation,\u201d in Proceedings of the 2018 Confer- ence on Empirical Methods in Natural Language Pro- cessing, 2018, pp. 2974\u20132978. [17] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u2013 1780, 1997. [18] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, \u201cListen, at- tend and spell: A neural network for large vocabulary conversational speech recognition,\u201d in 2016 IEEE In- ternational Conference on Acoustics, Speech and Sig- nal Processing (ICASSP), March 2016, pp. 4960\u20134964. [19] J. Chung, C\u00b8 .",
            "4960\u20134964. [19] J. Chung, C\u00b8 . G\u00a8ulc\u00b8ehre, K. Cho, and Y. Bengio, \u201cEm- pirical evaluation of gated recurrent neural networks on sequence modeling,\u201d CoRR, vol. abs\/1412.3555, 2014. [Online]. Available: http:\/\/arxiv.org\/abs\/1412.3555 [20] O. Press and L. Wolf, \u201cUsing the output embedding to improve language models,\u201d in Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers. Valencia, Spain: Association for Computational Linguistics, Apr. 2017, pp. 157\u2013 163. [Online]. Available: https:\/\/www.aclweb.org\/ anthology\/E17-2025 [21] Y. Xia, F. Tian, L. Wu, J. Lin, T. Qin, N. Yu, and T.- Y. Liu, \u201cDeliberation networks: Sequence generation beyond one-pass decoding,\u201d in Advances in Neural In- formation Processing Systems, 2017, pp.",
            "1784\u20131794. [22] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine translation by jointly learning to align and translate,\u201d CoRR, vol. abs\/1409.0473, 2014. [Online]. Available: http:\/\/arxiv.org\/abs\/1409.0473 [23] S. Xie, R. Girshick, P. Doll\u00b4ar, Z. Tu, and K. He, \u201cAg- gregated residual transformations for deep neural net- works,\u201d in Proceedings of the IEEE conference on com- puter vision and pattern recognition, 2017, pp. 1492\u2013 1500. [24] K. Hara, H. Kataoka, and Y. Satoh, \u201cCan spatiotempo- ral 3d cnns retrace the history of 2d cnns and imagenet,\u201d",
            "in Proceedings of the IEEE conference on Computer Vi- sion and Pattern Recognition, 2018, pp. 6546\u20136555. [25] M. Monfort, A. Andonian, B. Zhou, K. Ramakrishnan, S. A. Bargal, T. Yan, L. Brown, Q. Fan, D. Gutfru- end, C. Vondrick, et al., \u201cMoments in time dataset: one million videos for event understanding,\u201d IEEE Trans- actions on Pattern Analysis and Machine Intelligence, pp. 1\u20138, 2019. [26] J. Helcl, J. Libovick, and D. Varis, \u201cCUNI system for the WMT18 multimodal translation task,\u201d in Proceedings of the Third Conference on Machine Translation. Belgium, Brussels: Association for Computational Linguistics, October 2018, pp. 622\u2013 629. [Online].",
            "Belgium, Brussels: Association for Computational Linguistics, October 2018, pp. 622\u2013 629. [Online]. Available: http:\/\/www.aclweb.org\/ anthology\/W18-64068 [27] P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst, \u201cMoses: Open source toolkit for statistical machine translation,\u201d in Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ser. ACL \u201907. Stroudsburg, PA, USA: Association for Computational Linguistics, 2007, pp. 177\u2013180. [Online].",
            "ACL \u201907. Stroudsburg, PA, USA: Association for Computational Linguistics, 2007, pp. 177\u2013180. [Online]. Available: http:\/\/dl.acm.org\/citation.cfm?id=1557769.1557821 [28] R. Sennrich, B. Haddow, and A. Birch, \u201cNeural machine translation of rare words with subword units,\u201d in Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Berlin, Germany: Association for Computational Linguistics, Aug. 2016, pp. 1715\u2013 1725. [Online]. Available: https:\/\/www.aclweb.org\/ anthology\/P16-1162 [29] T. Kudo, \u201cSentencePiece: A simple and language inde- pendent subword tokenizer and detokenizer for neural text processing,\u201d in Proceedings of the 2018 Confer- ence on Empirical Methods in Natural Language Pro- cessing, Brussels, Belgium, October 2018.",
            "[30] D. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.6980, 2014. [Online]. Available: http:\/\/arxiv.org\/abs\/1412.6980 [31] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, \u201cBleu: a method for automatic evaluation of machine transla- tion,\u201d in Proceedings of the 40th annual meeting on as- sociation for computational linguistics. Association for Computational Linguistics, 2002, pp. 311\u2013318. [32] A. Vaswani, S. Bengio, E. Brevdo, F. Chollet, A. Gomez, S. Gouws, L. Jones, \u0141. Kaiser, N. Kalch- brenner, N. Parmar, R. Sepassi, N. Shazeer, and J. Uszkoreit, \u201cTensor2Tensor for neural machine translation,\u201d in Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers).",
            "Boston, MA: Association for Machine Translation in the Amer- icas, Mar. 2018, pp. 193\u2013199. [Online]. Available: https:\/\/www.aclweb.org\/anthology\/W18-1819 [33] J. H. Clark, C. Dyer, A. Lavie, and N. A. Smith, \u201cBetter hypothesis testing for statistical machine translation: Controlling for optimizer instability,\u201d in Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Portland, Oregon, USA: Association for Computational Linguistics, June 2011, pp. 176\u2013181. [Online]. Available: https:\/\/www.aclweb. org\/anthology\/P11-2031"
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1910.13215.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 8058.999694824219,
    "avg_doclen_est": 171.46807861328125
}
