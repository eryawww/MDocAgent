{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow Xuezhe Ma\u2217,1 Chunting Zhou\u2217,1 Xian Li2 Graham Neubig1 Eduard Hovy1 1Language Technologies Institute, Carnegie Mellon University 2Facebook AI {xuezhem, chuntinz, gneubig, ehovy}@cs.cmu.edu xianl@fb.com Abstract Most sequence-to-sequence (seq2seq) models are autoregressive; they generate each token by conditioning on previously generated to- kens. In contrast, non-autoregressive seq2seq models generate all tokens in one pass, which leads to increased ef\ufb01ciency through parallel processing on hardware such as GPUs. How- ever, directly modeling the joint distribution of all tokens simultaneously is challenging, and even with increasingly complex model struc- tures accuracy lags signi\ufb01cantly behind au- toregressive models. In this paper, we propose a simple, ef\ufb01cient, and effective model for non-autoregressive sequence generation using latent variable models.",
            "In this paper, we propose a simple, ef\ufb01cient, and effective model for non-autoregressive sequence generation using latent variable models. Speci\ufb01cally, we turn to generative \ufb02ow, an elegant technique to model complex distributions using neural net- works, and design several layers of \ufb02ow tai- lored for modeling the conditional density of sequential latent variables. We evaluate this model on three neural machine translation (NMT) benchmark datasets, achieving com- parable performance with state-of-the-art non- autoregressive NMT models and almost con- stant decoding time w.r.t the sequence length.1 1 Introduction Neural sequence-to-sequence (seq2seq) models (Bahdanau et al., 2015; Rush et al., 2015; Vinyals et al., 2015; Vaswani et al., 2017) generate an output sequence y = {y1, . . . , yT } given an in- put sequence x = {x1, . . . , xT \u2032} using condi- tional probabilities P\u03b8(y|x) predicted by neural networks (parameterized by \u03b8).",
            ". . , yT } given an in- put sequence x = {x1, . . . , xT \u2032} using condi- tional probabilities P\u03b8(y|x) predicted by neural networks (parameterized by \u03b8). Most seq2seq models are autoregressive, mean- ing that they factorize the joint probability of the output sequence given the input sequence P\u03b8(y|x) into the product of probabilities over the next to- \u2217Equal contribution, in alphabetical order. 1https:\/\/github.com\/XuezheMax\/flowseq y x (a) (c) y x (b) x z y Figure 1: (a) Autoregressive (b) non-autoregressive and (c) our proposed sequence generation models. x is the source, y is the target, and z are latent variables. ken in the sequence given the input sequence and previously generated tokens: P\u03b8(y|x) = T Y t=1 P\u03b8(yt|y<t, x).",
            "x is the source, y is the target, and z are latent variables. ken in the sequence given the input sequence and previously generated tokens: P\u03b8(y|x) = T Y t=1 P\u03b8(yt|y<t, x). (1) Each factor, P\u03b8(yt|y<t, x), can be implemented by function approximators such as RNNs (Bah- danau et al., 2015) and Transformers (Vaswani et al., 2017). This factorization takes the com- plicated problem of joint estimation over an ex- ponentially large output space of outputs y, and turns it into a sequence of tractable multi-class classi\ufb01cation problems predicting yt given the pre- vious words, allowing for simple maximum log- likelihood training. However, this assumption of left-to-right factorization may be sub-optimal from a modeling perspective (Gu et al., 2019; Stern et al., 2019), and generation of outputs must be done through a linear left-to-right pass through the output tokens using beam search, which is not easily parallelizable on hardware such as GPUs.",
            "Recently, there has been work on non- autoregressive sequence generation for neural ma- chine translation (NMT; Gu et al. (2018); Lee et al. (2018); Ghazvininejad et al. (2019)) and lan- guage modeling (Ziegler and Rush, 2019). Non- autoregressive models attempt to model the joint distribution P\u03b8(y|x) directly, decoupling the de- pendencies of decoding history during generation. arXiv:1909.02480v3  [cs.CL]  9 Oct 2019",
            "A na\u00a8\u0131ve solution is to assume that each token of the target sequence is independent given the input: P\u03b8(y|x) = T Y t=1 P\u03b8(yt|x). (2) Unfortunately, the performance of this simple model falls far behind autoregressive models, as seq2seq tasks usually do have strong conditional dependencies between output variables (Gu et al., 2018). This problem can be mitigated by introduc- ing a latent variable z to model these conditional dependencies: P\u03b8(y|x) = Z z P\u03b8(y|z, x)p\u03b8(z|x)dz, (3) where p\u03b8(z|x) is the prior distribution over la- tent z and P\u03b8(y|z, x) is the \u201cgenerative\u201d distri- bution (a.k.a decoder). Non-autoregressive gen- eration can be achieved by the following indepen- dence assumption in the decoding process: P\u03b8(y|z, x) = T Y t=1 P\u03b8(yt|z, x). (4) Gu et al.",
            "Non-autoregressive gen- eration can be achieved by the following indepen- dence assumption in the decoding process: P\u03b8(y|z, x) = T Y t=1 P\u03b8(yt|z, x). (4) Gu et al. (2018) proposed a z representing fertil- ity scores specifying the number of output words each input word generates, signi\ufb01cantly improv- ing the performance over Eq. (2). But the per- formance still falls behind state-of-the-art autore- gressive models due to the limited expressiveness of fertility to model the interdependence between words in y. In this paper, we propose a simple, effective, and ef\ufb01cient model, FlowSeq, which models ex- pressive prior distribution p\u03b8(z|x) using a pow- erful mathematical framework called generative \ufb02ow (Rezende and Mohamed, 2015).",
            "This frame- work can elegantly model complex distributions, and has obtained remarkable success in model- ing continuous data such as images and speech through ef\ufb01cient density estimation and sampling (Kingma and Dhariwal, 2018; Prenger et al., 2019; Ma and Hovy, 2019). Based on this, we posit that generative \ufb02ow also has potential to introduce more meaningful latent variables z in the non- autoregressive generation in Eq. (3). FlowSeq is a \ufb02ow-based sequence-to-sequence model, which is (to our knowledge) the \ufb01rst non-autoregressive seq2seq model utilizing gen- erative \ufb02ows. It allows for ef\ufb01cient parallel decoding while modeling the joint distribution of the output sequence. Experimentally, on three benchmark datasets for machine transla- tion \u2013 WMT2014, WMT2016 and IWSLT-2014, FlowSeq achieves comparable performance with state-of-the-art non-autoregressive models, and al- most constant decoding time w.r.t. the sequence length compared to a typical left-to-right Trans- former model, which is super-linear.",
            "the sequence length compared to a typical left-to-right Trans- former model, which is super-linear. 2 Background As noted above, incorporating expressive latent variables z is essential to decouple the depen- dencies between tokens in the target sequence in non-autoregressive models. However, in order to model all of the complexities of sequence gener- ation to the point that we can read off all of the words in the output in an independent fashion (as in Eq. (4)), the prior distribution p\u03b8(z|x) will nec- essarily be quite complex. In this section, we de- scribe generative \ufb02ows (Rezende and Mohamed, 2015), an effective method for arbitrary model- ing of complicated distributions, before describing how we apply them to sequence-to-sequence gen- eration in \u00a73. 2.1 Flow-based Generative Models Put simply, \ufb02ow-based generative models work by transforming a simple distribution (e.g. a simple Gaussian) into a complex one (e.g. the complex prior distribution over z that we want to model) through a chain of invertible transformations.",
            "2.1 Flow-based Generative Models Put simply, \ufb02ow-based generative models work by transforming a simple distribution (e.g. a simple Gaussian) into a complex one (e.g. the complex prior distribution over z that we want to model) through a chain of invertible transformations. Formally, a set of latent variables \u03c5 \u2208\u03a5 are introduced with a simple prior distribution p\u03a5(\u03c5). We then de\ufb01ne a bijection function f : Z \u2192\u03a5 (with g = f\u22121), whereby we can de\ufb01ne a genera- tive process over variables z: \u03c5 \u223c p\u03a5(\u03c5) z = g\u03b8(\u03c5). (5) An important insight behind \ufb02ow-based models is that given this bijection function, the change of variable formula de\ufb01nes the model distribution on z \u2208Z by: p\u03b8(z) = p\u03a5(f\u03b8(z)) \f\f\f\fdet(\u2202f\u03b8(z) \u2202z ) \f\f\f\f . (6) Here \u2202f\u03b8(z) \u2202z is the Jacobian matrix of f\u03b8 at z. Eq.",
            "(6) Here \u2202f\u03b8(z) \u2202z is the Jacobian matrix of f\u03b8 at z. Eq. (6) provides a way to calculate the (com- plex) density of z by calculating the (simple) den- sity of \u03c5 and the Jacobian of the transforma- tion from z to \u03c5. For ef\ufb01ciency purposes, \ufb02ow- based models generally use certain types of trans- formations f\u03b8 where both the inverse functions",
            "g\u03b8 and the Jacobian determinants are tractable to compute. A stacked sequence of such invert- ible transformations is also called a (normalizing) \ufb02ow (Rezende and Mohamed, 2015): z f1 \u2190\u2192 g1 H1 f2 \u2190\u2192 g2 H2 f3 \u2190\u2192 g3 \u00b7 \u00b7 \u00b7 fK \u2190\u2192 gK \u03c5, where f = f1 \u25e6f2 \u25e6\u00b7 \u00b7 \u00b7 \u25e6fK is a \ufb02ow of K trans- formations (omitting \u03b8s for brevity). 2.2 Variational Inference and Training In the context of maximal likelihood estimation (MLE), we wish to minimize the negative log- likelihood of the parameters: min \u03b8\u2208\u0398 1 N N X i=1 \u2212log P\u03b8(yi|xi), (7) where D = {(xi, yi)}N i=1 is the set of train- ing data. However, the likelihood P\u03b8(y|x) af- ter marginalizing out latent variables z (LHS in Eq. (3)) is intractable to compute or differentiate directly.",
            "However, the likelihood P\u03b8(y|x) af- ter marginalizing out latent variables z (LHS in Eq. (3)) is intractable to compute or differentiate directly. Variational inference (Wainwright et al., 2008) provides a solution by introducing a para- metric inference model q\u03c6(z|y, x) (a.k.a poste- rior) which is then used to approximate this inte- gral by sampling individual examples of z. These models then optimize the evidence lower bound (ELBO), which considers both the \u201creconstruction error\u201d log P\u03b8(y|z, x) and KL-divergence between the posterior and the prior: log P\u03b8(y|x) \u2265Eq\u03c6(z|y,x)[log P\u03b8(y|z, x)] \u2212KL(q\u03c6(z|y, x)||p\u03b8(z|x)). (8) Both inference model \u03c6 and decoder \u03b8 parameters are optimized according to this objective. 3 FlowSeq We \ufb01rst overview FlowSeq\u2019s architecture (shown in Figure 2) and training process here before detailing each component in following sections.",
            "(8) Both inference model \u03c6 and decoder \u03b8 parameters are optimized according to this objective. 3 FlowSeq We \ufb01rst overview FlowSeq\u2019s architecture (shown in Figure 2) and training process here before detailing each component in following sections. Similarly to classic seq2seq models, at both train- ing and test time FlowSeq \ufb01rst reads the whole in- put sequence x and calculates a vector for each word in the sequence, the source encoding. At training time, FlowSeq\u2019s parameters are learned using a variational training paradigm overviewed in \u00a72.2. First, we draw samples of la- tent codes z from the current posterior q\u03c6(z|y, x). Next, we feed z together with source encod- ings into the decoder network and the prior \ufb02ow to compute the probabilities of P\u03b8(y|z, x) and p\u03b8(z|x) for optimizing the ELBO (Eq. (8)). At test time, generation is performed by \ufb01rst sampling a latent code z from the prior \ufb02ow by ex- ecuting the generative process de\ufb01ned in Eq. (5).",
            "(8)). At test time, generation is performed by \ufb01rst sampling a latent code z from the prior \ufb02ow by ex- ecuting the generative process de\ufb01ned in Eq. (5). In this step, the source encodings produced from the encoder are used as conditional inputs. Then the decoder receives both the sampled latent code z and the source encoder outputs to generate the target sequence y from P\u03b8(y|z, x). 3.1 Source Encoder The source encoder encodes the source sequences into hidden representations, which are used in computing attention when generating latent vari- ables in the posterior network and prior network as well as the cross-attention with decoder. Any standard neural sequence model can be used as its encoder, including RNNs (Bahdanau et al., 2015) or Transformers (Vaswani et al., 2017). 3.2 Posterior Generation of Latent Variables. The latent variables z are represented as a sequence of con- tinuous random vectors z = {z1, . . .",
            "3.2 Posterior Generation of Latent Variables. The latent variables z are represented as a sequence of con- tinuous random vectors z = {z1, . . . , zT } with the same length as the target sequence y. Each zt is a dz-dimensional vector, where dz is the dimen- sion of the latent space. The posterior distribution q\u03c6(z|y, x) models each zt as a diagonal Gaussian with learned mean and variance: q\u03c6(z|y, x) = T Y t=1 N(zt|\u00b5t(x, y), \u03c32 t (x, y)) (9) where \u00b5t(\u00b7) and \u03c3t(\u00b7) are neural networks such as RNNs or Transformers. Zero initialization. While we perform standard random initialization for most layers of the net- work, we initialize the last linear transforms that generate the \u00b5 and log \u03c32 values with zeros. This ensures that the posterior distribution as a simple normal distribution, which we found helps train very deep generative \ufb02ows more stably. Token Dropout.",
            "This ensures that the posterior distribution as a simple normal distribution, which we found helps train very deep generative \ufb02ows more stably. Token Dropout. The motivation of introducing the latent variable z into the model is to model the uncertainty in the generative process. Thus, it is preferable that z capture contextual interde- pendence between tokens in y. However, there is an obvious local optimum where the posterior network generates a latent vector zt that only en- codes the information about the corresponding tar- get token yt, and the decoder simply generates the",
            "Emb1 Emb2 Emb3 Emb4 Emb5 A dog runs away . Target Encoder \u008f1 \u008f2 \u008f3 \u008f4 \u008f5 Target Decoder Output Probabilities \u001e1 \u001e2 \u001e3 \u001e4 \u001e5 One Step of Flow \u223c\ue23a(0;\u009a) Squeeze Split One Step of Flow \u00d7(L \u22121) \u00d7K \u00d7K \u008f1 \u008f2 \u008f3 \u008f4 \u008f5 Emb1 Emb2 Emb3 Emb4 Emb5 ein hund rannte weg . Source Encoder Source Encodings + + \u00a0forward pass \u00a0backward pass multi-head attention Prior Flow \u00c5\u00b4 \u00c5h + + + Posterior Figure 2: Neural architecture of FlowSeq, including the encoder, the decoder and the posterior networks, together with the multi-scale architecture of the prior \ufb02ow. The architecture of each \ufb02ow step is in Figure 3. \u201ccorrect\u201d token at each step t with zt as input. In this case, FlowSeq reduces to the baseline model in Eq. (2).",
            "The architecture of each \ufb02ow step is in Figure 3. \u201ccorrect\u201d token at each step t with zt as input. In this case, FlowSeq reduces to the baseline model in Eq. (2). To escape this undesired local opti- mum, we apply token-level dropout to randomly drop an entire token when calculating the poste- rior, to ensure the model also has to learn how to use contextual information. This technique is sim- ilar to the \u201cmasked language model\u201d in previous studies (Melamud et al., 2016; Devlin et al., 2018; Ma et al., 2018). 3.3 Decoder As the decoder, we take the latent sequence z as input, run it through several layers of a neural se- quence model such as a Transformer, then directly predict the output tokens in y individually and in- dependently. Notably, unlike standard seq2seq de- coders, we do not perform causal masking to pre- vent attending to future tokens, making the model fully non-autoregressive.",
            "Notably, unlike standard seq2seq de- coders, we do not perform causal masking to pre- vent attending to future tokens, making the model fully non-autoregressive. 3.4 Flow Architecture for Prior The \ufb02ow architecture is based on Glow (Kingma and Dhariwal, 2018). It consists of a series of steps of \ufb02ow, combined in a multi-scale architecture (see Figure 2.) Each step of \ufb02ow consists three types of elementary \ufb02ows \u2013 actnorm, invertible multi-head linear, and coupling. Note that all three functions are invertible and conducive to calcula- tion of log determinants (details in Appendix A). Actnorm. The activation normalization layer (actnorm; Kingma and Dhariwal (2018)) is an alternative for batch normalization (Ioffe and Szegedy, 2015), that has mainly been used in the context of image data to alleviate problems in model training. Actnorm performs an af\ufb01ne trans- formation of the activations using a scale and bias parameter per feature for sequences: z\u2032 t = s \u2299zt + b.",
            "Actnorm performs an af\ufb01ne trans- formation of the activations using a scale and bias parameter per feature for sequences: z\u2032 t = s \u2299zt + b. (10) Both z and z\u2032 are tensors of shape [T \u00d7 dz] with time dimension t and feature dimension dz. The parameters are initialized such that over each fea- ture z\u2032 t has zero mean and unit variance given an initial mini-batch of data. Invertible Multi-head Linear Layers. To in- corporate general permutations of variables along the feature dimension to ensure that each dimen- sion can affect every other ones after a suf\ufb01cient number of steps of \ufb02ow, Kingma and Dhariwal (2018) proposed a trainable invertible 1\u00d71 convo- lution layer for 2D images. It is straightforward to apply similar transformations to sequential data: z\u2032 t = ztW, (11) where W is the weight matrix of shape [dz \u00d7 dz].",
            "It is straightforward to apply similar transformations to sequential data: z\u2032 t = ztW, (11) where W is the weight matrix of shape [dz \u00d7 dz]. The log-determinant of this transformation is: log \f\f\f\fdet \u0012\u2202linear(z; W) \u2202z \u0013\f\f\f\f = T \u00b7 log |det(W)| The cost of computing det(W) is O(d3 z).",
            "Encoder Inter-Attention source encodings ActNorm Linear Layer Af\ufb01ne Coupling Layer (a) One step of \ufb02ow. (b) Coupling layer splits. (c) NN function on the split of the coupling layer. Figure 3: (a) The architecture of one step of our \ufb02ow. (b) The visualization of three split pattern for coupling layers, where the red color denotes za and the blue color denotes zvb. (c) The attention-based architecture of the NN function in coupling layers. Unfortunately, dz in Seq2Seq generation is commonly large, e.g. 512, signi\ufb01cantly slowing down the model for computing det(W). To apply this to sequence generation, we propose a multi- head invertible linear layer, which \ufb01rst splits each dz-dimensional feature vector into h heads with dimension dh = dz\/h. Then the linear trans- formation in (11) is applied to each head, with dh \u00d7 dh weight matrix W, signi\ufb01cantly reduc- ing the dimension.",
            "Then the linear trans- formation in (11) is applied to each head, with dh \u00d7 dh weight matrix W, signi\ufb01cantly reduc- ing the dimension. For splitting of heads, one step of \ufb02ow contains one linear layer with either row- major or column-major splitting format, and these steps with different linear layers are composed in an alternating pattern. Af\ufb01ne Coupling Layers. To model interdepen- dence across time steps, we use af\ufb01ne coupling layers (Dinh et al., 2016): za, zb = split(z) z\u2032 a = za z\u2032 b = s(za, x) \u2299zb + b(za, x) z\u2032 = concat(z\u2032 a, z\u2032 b), where s(za, x) and b(za, x) are outputs of two neural networks with za and x as input. These are shown in Figure 3 (c).",
            "These are shown in Figure 3 (c). In experiments, we imple- ment s(\u00b7) and b(\u00b7) with one Transformer decoder layer (Vaswani et al., 2017): multi-head self- attention over za, followed by multi-head inter- attention over x, followed by a position-wise feed- forward network. The input za is fed into this layer in one pass, without causal masking. As in Dinh et al. (2016), the split() function splits z the input tensor into two halves, while the concat operation performs the corresponding re- verse concatenation operation. In our architecture, three types of split functions are used, based on the split dimension and pattern. Figure 3 (b) il- lustrates the three splitting types. The \ufb01rst type of split groups z along the time dimension on alter- nate indices. In this case, FlowSeq mainly models the interactions between time-steps. The second and third types of splits perform on the feature di- mension, with continuous and alternate patterns, respectively.",
            "The \ufb01rst type of split groups z along the time dimension on alter- nate indices. In this case, FlowSeq mainly models the interactions between time-steps. The second and third types of splits perform on the feature di- mension, with continuous and alternate patterns, respectively. For each type of split, we alternate za and zb to increase the \ufb02exibility of the split func- tion. Different types of af\ufb01ne coupling layers al- ternate in the \ufb02ow, similar to the linear layers. Multi-scale Architecture. We follow Dinh et al. (2016) in implementing a multi-scale architecture using the squeezing operation on the feature di- mension, which has been demonstrated helpful for training deep \ufb02ows. Formally, each scale is a com- bination of several steps of the \ufb02ow (see Figure 3 (a)). After each scale, the model drops half of the dimensions with the third type of split in Figure 3 (b) to reduce computational and memory cost, out- putting the tensor with shape [T \u00d7 d 2].",
            "After each scale, the model drops half of the dimensions with the third type of split in Figure 3 (b) to reduce computational and memory cost, out- putting the tensor with shape [T \u00d7 d 2]. Then the squeezing operation transforms the T \u00d7 d 2 tensor into an T 2 \u00d7d one as the input of the next scale. We pad each sentence with EOS tokens to ensure T is divisible by 2. The right component of Figure 2 illustrates the multi-scale architecture. 3.5 Predicting Target Sequence Length In autoregressive seq2seq models, it is natural to determine the length of the sequence dynamically by simply predicting a special EOS token. How- ever, for FlowSeq to predict the entire sequence in parallel, it needs to know its length in advance to generate the latent sequence z. Instead of pre- dicting the absolute length of the target sequence, we predict the length difference between source",
            "and target sequences using a classi\ufb01er with a range of [\u221220, 20]. Numbers in this range are predicted by max-pooling the source encodings into a single vector,2 running this through a linear layer, and taking a softmax. This classi\ufb01er is learned jointly with the rest of the model. 3.6 Decoding Process At inference time, the model needs to identify the sequence with the highest conditional probability by marginalizing over all possible latent variables (see Eq. (3)), which is intractable in practice. We propose three approximating decoding algorithms to reduce the search space. Argmax Decoding. Following Gu et al. (2018), one simple and effective method is to select the best sequence by choosing the highest-probability latent sequence z: z\u2217 = argmax z\u2208Z p\u03b8(z|x) y\u2217 = argmax y P\u03b8(y|z\u2217, x) where identifying y\u2217only requires independently maximizing the local probability for each output position (see Eq. 4). Noisy Parallel Decoding (NPD). A more accu- rate approximation of decoding, proposed in Gu et al.",
            "4). Noisy Parallel Decoding (NPD). A more accu- rate approximation of decoding, proposed in Gu et al. (2018), is to draw samples from the latent space and compute the best output for each la- tent sequence. Then, a pre-trained autoregres- sive model is adopted to rank these sequences. In FlowSeq, different candidates can be generated by sampling different target lengths or different sam- ples from the prior, and both of the strategies can be batched via masks during decoding. In our experiments, we \ufb01rst select the top l length can- didates from the length predictor in \u00a73.5. Then, for each length candidate we use r random sam- ples from the prior network to generate output se- quences, yielding a total of l \u00d7 r candidates. Importance Weighted Decoding (IWD) The third approximating method is based on the lower bound of importance weighted estimation (Burda et al., 2015). Similarly to NPD, IWD \ufb01rst draws samples from the latent space and computes the best output for each latent sequence.",
            "Importance Weighted Decoding (IWD) The third approximating method is based on the lower bound of importance weighted estimation (Burda et al., 2015). Similarly to NPD, IWD \ufb01rst draws samples from the latent space and computes the best output for each latent sequence. Then, IWD 2We experimented with other methods such as mean- pooling or taking the last hidden state and found no major difference in our experiments ranks these candidate sequences with K impor- tance samples: zi \u223c p\u03b8(z|x), \u2200i = 1, . . . , N \u02c6yi = argmax y P\u03b8(y|zi, x) z(k) i \u223c q\u03c6(z|\u02c6yi, x), \u2200k = 1, . . . , K P(\u02c6yi|x) \u2248 1 K K P k=1 P\u03b8(\u02c6yi|z(k) i ,x)p\u03b8(z(k) i |x) q\u03c6(z(k) i |\u02c6yi,x) IWD does not rely on a separate pre-trained model, though it signi\ufb01cantly slows down the de- coding speed.",
            "The detailed comparison of these three decoding methods is provided in \u00a74.2. 3.7 Discussion Different from the architecture proposed in Ziegler and Rush (2019), the architecture of FlowSeq is not using any autoregressive \ufb02ow (Kingma et al., 2016; Papamakarios et al., 2017), yielding a truly non-autoregressive model with both ef\ufb01cient den- sity estimation and generation. Note that FlowSeq remains non-autoregressive even if we use an RNN in the architecture because RNN is only used to encode a complete sequence of codes and all the input tokens can be fed into the RNN in parallel. This makes it possible to use highly-optimized im- plementations of RNNs such as those provided by cuDNN.3 Thus while RNNs do experience some drop in speed, it is less extreme than that experi- enced when using autoregressive models.",
            "This makes it possible to use highly-optimized im- plementations of RNNs such as those provided by cuDNN.3 Thus while RNNs do experience some drop in speed, it is less extreme than that experi- enced when using autoregressive models. 4 Experiments 4.1 Experimental Setups Translation Datasets We evaluate FlowSeq on three machine translation benchmark datasets: WMT2014 DE-EN (around 4.5M sentence pairs), WMT2016 RO-EN (around 610K sentence pairs) and a smaller dataset IWSLT2014 DE-EN (around 150K sentence pairs) (Cettolo et al., 2012). We use scripts from fairseq (Ott et al., 2019) to prepro- cess WMT2014 and IWSLT2014, where the pre- processing steps follow Vaswani et al. (2017) for WMT2014. We use the data provided by Lee et al. (2018) for WMT2016.",
            "(2017) for WMT2014. We use the data provided by Lee et al. (2018) for WMT2016. For both WMT datasets, the source and target languages share the same set of subword embeddings while for IWSLT2014 we use separate embeddings. During training, we \ufb01l- ter out sentences longer than 80 for WMT dataset and 60 for IWSLT, respectively. 3https:\/\/devblogs.nvidia.com\/optimizing-recurrent- neural-networks-cudnn-5\/",
            "WMT2014 WMT2016 IWSLT2014 Models EN-DE DE-EN EN-RO RO-EN DE-EN Raw Data CMLM-base 10.88 \u2013 20.24 \u2013 \u2013 LV NAR 11.80 \u2013 \u2013 \u2013 \u2013 FlowSeq-base 18.55 23.36 29.26 30.16 24.75 FlowSeq-large 20.85 25.40 29.86 30.69 \u2013 Knowledge Distillation NAT-IR 13.91 16.77 24.45 25.73 21.86 CTC Loss 17.68 19.80 19.93 24.71 \u2013 NAT w\/ FT 17.69 21.47 27.29 29.06 20.32 NAT-REG 20.65 24.77 \u2013 \u2013 23.89 CMLM-small 15.06 19.26 20.12 20.36 \u2013 CMLM-base 18.12 22.26 23.65 22.78 \u2013 FlowSeq-base 21.45 26.16 29.34 30.44 27.",
            "06 19.26 20.12 20.36 \u2013 CMLM-base 18.12 22.26 23.65 22.78 \u2013 FlowSeq-base 21.45 26.16 29.34 30.44 27.55 FlowSeq-large 23.72 28.39 29.73 30.72 \u2013 Table 1: BLEU scores on three MT benchmark datasets for FlowSeq with argmax decoding and baselines with purely non-autoregressive decoding methods. The \ufb01rst and second block are results of models trained w\/w.o. knowledge distillation, respectively. Modules and Hyperparameters We imple- ment the encoder, decoder and posterior net- works with standard (unmasked) Transformer lay- ers (Vaswani et al., 2017). For WMT datasets, we use 8 attention heads, the encoder consists of 6 layers, and the decoder and posterior are com- posed of 4 layers. For IWSLT, we use 4 at- tention heads, the encoder has 5 layers, and de- coder and posterior have 3 layers.",
            "For IWSLT, we use 4 at- tention heads, the encoder has 5 layers, and de- coder and posterior have 3 layers. The prior \ufb02ow consists of 3 scales with the number of steps [48, 48, 16] from bottom to top. To dissect the im- pact of model dimension on translation quality and speed, we perform experiments on two versions of FlowSeq with dmodel\/dhidden = 256\/512 (base) and dmodel\/dhidden = 512\/1024 (large). More model details are provided in Appendix B. Optimization Parameter optimization is per- formed with the Adam optimizer (Kingma and Ba, 2014) with \u03b2 = (0.9, 0.999), \u03f5 = 1e\u22128 and AMS- Grad (Reddi et al., 2018). Each mini-batch con- sist of 2048 sentences. The learning rate is ini- tialized to 5e \u22124, and exponentially decays with rate 0.999995. The gradient clipping cutoff is 1.0.",
            "Each mini-batch con- sist of 2048 sentences. The learning rate is ini- tialized to 5e \u22124, and exponentially decays with rate 0.999995. The gradient clipping cutoff is 1.0. For all the FlowSeq models, we apply 0.1 label smoothing (Vaswani et al., 2017) and averaged the 5 best checkpoints to create the \ufb01nal model. At the beginning of training, the posterior net- work is randomly initialized, producing noisy su- pervision to the prior. To mitigate this issue, we \ufb01rst set the weight of the KL term in the ELBO to zero for 30,000 updates to train the encoder, de- coder and posterior networks. Then the KL weight WMT2014 WMT2016 Models EN-DE DE-EN EN-RO RO-EN Autoregressive Methods Transformer-base 27.30 \u2013 \u2013 \u2013 Our Implementation 27.16 31.44 32.92 33.09 Raw Data CMLM-base (re\ufb01nement 4) 22.06 \u2013 30.",
            "30 \u2013 \u2013 \u2013 Our Implementation 27.16 31.44 32.92 33.09 Raw Data CMLM-base (re\ufb01nement 4) 22.06 \u2013 30.89 \u2013 CMLM-base (re\ufb01nement 10) 24.65 \u2013 32.53 \u2013 FlowSeq-base (IWD n = 15) 20.20 24.63 30.61 31.50 FlowSeq-base (NPD n = 15) 20.81 25.76 31.38 32.01 FlowSeq-base (NPD n = 30) 21.15 26.04 31.74 32.45 FlowSeq-large (IWD n = 15) 22.94 27.16 31.08 32.03 FlowSeq-large (NPD n = 15) 23.14 27.71 31.97 32.46 FlowSeq-large (NPD n = 30) 23.64 28.29 32.35 32.91 Knowledge Distillation NAT-IR (re\ufb01nement 10) 21.61 25.48 29.",
            "71 31.97 32.46 FlowSeq-large (NPD n = 30) 23.64 28.29 32.35 32.91 Knowledge Distillation NAT-IR (re\ufb01nement 10) 21.61 25.48 29.32 30.19 NAT w\/ FT (NPD n = 10) 18.66 22.42 29.02 31.44 NAT-REG (NPD n = 9) 24.61 28.90 \u2013 \u2013 LV NAR (re\ufb01nement 4) 24.20 \u2013 \u2013 \u2013 CMLM-small (re\ufb01nement 10) 25.51 29.47 31.65 32.27 CMLM-base (re\ufb01nement 10) 26.92 30.86 32.42 33.06 FlowSeq-base (IWD n = 15) 22.49 27.40 30.59 31.58 FlowSeq-base (NPD n = 15) 23.08 28.07 31.35 32.",
            "86 32.42 33.06 FlowSeq-base (IWD n = 15) 22.49 27.40 30.59 31.58 FlowSeq-base (NPD n = 15) 23.08 28.07 31.35 32.11 FlowSeq-base (NPD n = 30) 23.48 28.40 31.75 32.49 FlowSeq-large (IWD n = 15) 24.70 29.44 31.02 31.97 FlowSeq-large (NPD n = 15) 25.03 30.48 31.89 32.43 FlowSeq-large (NPD n = 30) 25.31 30.68 32.20 32.84 Table 2: BLEU scores on two WMT datasets of mod- els using advanced decoding methods. The \ufb01rst block are autoregressive Transformer-base (Vaswani et al., 2017). The second and third blocks are results of mod- els trained w\/w.o. knowledge distillation, respectively. n = l \u00d7 r is the total number of rescoring candidates.",
            "The \ufb01rst block are autoregressive Transformer-base (Vaswani et al., 2017). The second and third blocks are results of mod- els trained w\/w.o. knowledge distillation, respectively. n = l \u00d7 r is the total number of rescoring candidates. linearly increases to one for another 10,000 up- dates, which we found essential to accelerate train- ing and achieve stable performance. Knowledge Distillation Previous work on non- autoregressive generation (Gu et al., 2018; Ghazvininejad et al., 2019) has used translations produced by a pre-trained autoregressive NMT model as the training data, noting that this can sig- ni\ufb01cantly improve the performance. We analyze the impact of distillation in \u00a7 4.2.",
            "We analyze the impact of distillation in \u00a7 4.2. 4.2 Main Results We \ufb01rst conduct experiments to compare the per- formance of FlowSeq with strong baseline mod- els, including NAT w\/ Fertility (Gu et al., 2018), NAT-IR (Lee et al., 2018), NAT-REG (Wang et al., 2019), LV NAR (Shu et al., 2019), CTC Loss (Libovick`y and Helcl, 2018), and CMLM (Ghazvininejad et al., 2019). Table 1 provides the BLEU scores of FlowSeq with argmax decoding, together with baselines with purely non-autoregressive decoding meth- ods that generate output sequence in one parallel pass. The \ufb01rst block lists results of models trained",
            "0 20 40 60 80 100 120 Batch Size 0.00 0.05 0.10 0.15 0.20 0.25 Decoding Time (s \/ sent) Transformer FlowSeq-base FlowSeq-large FlowSeq-base speedup FlowSeq-large speedup 1 2 3 4 5 6 Relative Decoding Speed-up (a) batch size (0, 10] (10, 20] (20, 30] (30, 40] >40 Length of Target Sentence 0.010 0.015 0.020 0.025 0.030 0.035 0.040 0.045 0.050 Decoding Time (s \/ sent) Transformer FlowSeq-base FlowSeq-large FlowSeq-base speedup FlowSeq-large speedup 0 1 2 3 4 5 6 Relative Decoding Speed-up (b) target length Figure 4: The decoding speed of the Transformer (batched, beam size 5) and FlowSeq on WMT14 EN-DE test set (a) w.r.t.",
            "different batch sizes (b) bucketed by different target sentence lengths (batch size 32). on raw data, while the second block shows re- sults using knowledge distillation. Without using knowledge distillation, the FlowSeq base model achieves signi\ufb01cant improvements (more than 9 BLEU points) over the baselines. This demon- strates the effectiveness of FlowSeq in modeling complex interdependences in the target languages. Regarding the effect of knowledge distillation, we can mainly obtain two observations: i) Sim- ilar to the \ufb01ndings in previous work, knowledge distillation still bene\ufb01ts the translation quality of FlowSeq. ii) Compared to previous models, the bene\ufb01t of knowledge distillation for FlowSeq is less signi\ufb01cant, yielding less than 3 BLEU im- provement on WMT2014 DE-EN corpus, and even no improvement on WMT2016 RO-EN cor- pus. We hypothesize that the reason for this is that FlowSeq\u2019s stronger model is more robust against multi-modality, making it less necessary to rely on knowledge distillation.",
            "We hypothesize that the reason for this is that FlowSeq\u2019s stronger model is more robust against multi-modality, making it less necessary to rely on knowledge distillation. Table 2 illustrates the BLEU scores of FlowSeq and baselines with advanced decoding methods such as iterative re\ufb01nement, IWD and NPD rescoring. The \ufb01rst block in Table 2 includes the baseline results from autoregressive Trans- former. For the sampling procedure in IWD and NPD, we sampled from a reduced-temperature model (Kingma and Dhariwal, 2018) to obtain high-quality samples. We vary the tempera- ture within {0.1, 0.2, 0.3, 0.4, 0.5, 1.0} and select the best temperature based on the performance on development sets. The analysis of the im- pact of sampling temperature and other hyper- parameters on samples is shown in \u00a7 4.4. For FlowSeq, NPD obtains better results than IWD, showing that FlowSeq still falls behind the autore- gressive Transformer on modeling the distribu- tions of target languages.",
            "For FlowSeq, NPD obtains better results than IWD, showing that FlowSeq still falls behind the autore- gressive Transformer on modeling the distribu- tions of target languages. Compared with CMLM (Ghazvininejad et al., 2019) with 10 iterations of re\ufb01nement, which is a contemporaneous work that achieves state-of-the-art translation performance, FlowSeq obtains competitive performance on both WMT2014 and WMT2016 corpora, with only slight degradation in translation quality. Notably we did not attempt to perform iterative re\ufb01nement, but there is nothing that makes FlowSeq inherently incompatible with re\ufb01nement \u2013 we leave connect- ing the two techniques to future work. 4.3 Analysis on Decoding Speed In this section, we compare the decoding speed (measured in average time in seconds required to decode one sentence) of FlowSeq at test time with that of the autoregressive Transformer model. We use the test set of WMT14 EN-DE for evalua- tion and all experiments are conducted on a single NVIDIA TITAN X GPU. How does batch size affect the decoding speed?",
            "We use the test set of WMT14 EN-DE for evalua- tion and all experiments are conducted on a single NVIDIA TITAN X GPU. How does batch size affect the decoding speed? First, we investigate how different decoding batch size can affect the decoding speed. We vary the decoding batch size within {1, 4, 8, 32, 64, 128}. Figure. 4a shows that for both FlowSeq and the au- toregressive Transformer decoding is faster when using a larger batch size. However, FlowSeq has much larger gains in the decoding speed w.r.t. the increase in batch size, gaining a speed up of 594% of the base model and 403% of the large model when using a batch size of 128. We hypothesize that this is because the operations in FlowSeq are more friendly to batching while the incremental nature of left-to-right search in the autoregressive model is less ef\ufb01cient in bene\ufb01ting from batching.",
            "0.2 0.4 0.6 0.8 1.0 Sampling Temperature 25.0 25.5 26.0 26.5 27.0 27.5 28.0 28.5 29.0 BLEU l=1, r=15 l=1, r=30 l=2, r=8 l=2, r=15 l=3, r=5 l=3, r=10 Figure 5: Impact of sampling hyperparameters on the rescoring BLEU on the dev set of WMT14 DE-EN. Experiments are performed with FlowSeq-base trained with distillation data. l is the number of length candi- dates. r is the number of samples for each length. How does sentence length affect the decod- ing speed? Next, we examine if sentence length is a major factor affecting the decoding speed. We bucket the test data by the target sentence length. From Fig.",
            "r is the number of samples for each length. How does sentence length affect the decod- ing speed? Next, we examine if sentence length is a major factor affecting the decoding speed. We bucket the test data by the target sentence length. From Fig. 4b, we can see that as the sen- tence length increases, FlowSeq achieves almost a constant decoding time while the autoregres- sive Transformer has a linearly increasing decod- ing time. The relative decoding speed of FlowSeq versus the Transformer linearly increases as the se- quence length increases. The potential of decod- ing long sequences with constant time is an attrac- tive property of FlowSeq. 4.4 Analysis of Rescoring Candidates In Fig. 5, we analyze how different sampling hy- perparameters affect the performance of rescoring. First, we observe that the number of samples r for each length is the most important factor. The per- formance is always improved with a larger sample size. Second, a larger number of length candidates does not necessarily increase the rescoring perfor- mance.",
            "First, we observe that the number of samples r for each length is the most important factor. The per- formance is always improved with a larger sample size. Second, a larger number of length candidates does not necessarily increase the rescoring perfor- mance. Third, we \ufb01nd that a larger sampling tem- perature (0.3 - 0.5) can increase the diversity of translations and leads to better rescoring BLEU. However, the latent samples become noisy when a large temperature (1.0) is used. 4.5 Analysis of Translation Diversity Following He et al. (2018) and Shen et al. (2019), we analyze the output diversity of FlowSeq. They proposed pairwise-BLEU and BLEU computed in a leave-one-out manner to calibrate the diversity and quality of translation hypotheses.",
            "(2018) and Shen et al. (2019), we analyze the output diversity of FlowSeq. They proposed pairwise-BLEU and BLEU computed in a leave-one-out manner to calibrate the diversity and quality of translation hypotheses. A lower pairwise-BLEU score implies a more diverse hy- 20 30 40 50 60 70 80 Pairwise BLEU 35 40 45 50 55 60 65 70 75 Leave-one-out BLEU 0.1 0.2 0.3 0.4 0.5 1.0 0.1 0.20.30.40.5 1.0 human Beam search Hard MoE Sampling FlowSeq-large, distill, l=3, r=4 FlowSeq-large, distill, l=2, r=5 Figure 6: Comparisons of FlowSeq with human translations, beam search and sampling results of Transformer-base, and mixture-of-experts model (Hard MoE (Shen et al., 2019)) on the averaged leave-one-out BLEU score v.s pairwise-BLEU in descending order. pothesis set.",
            "pothesis set. And a higher BLEU score implies a better translation quality. We experiment on a subset of the test set of WMT14-ENDE with ten references for each sentence (Ott et al., 2018). In Fig. 6, we compare FlowSeq with other multi- hypothesis generation methods (ten hypotheses each sentence) to analyze how well the genera- tion outputs of FlowSeq are in terms of diversity and quality. The right corner area of the \ufb01gure in- dicates the ideal generations: high diversity and high quality. While FlowSeq still lags behind the autoregressive generation, by increasing the sam- pling temperature it provides a way of generating more diverse outputs while keeping the translation quality almost unchanged. More analysis of trans- lation outputs and detailed results are provided in the Appendix D and E. 5 Conclusion We propose FlowSeq, an ef\ufb01cient and effective model for non-autoregressive sequence generation by using generative \ufb02ows. One potential direc- tion for future work is to leverage iterative re\ufb01ne- ment techniques such as masked language models to further improve translation quality.",
            "5 Conclusion We propose FlowSeq, an ef\ufb01cient and effective model for non-autoregressive sequence generation by using generative \ufb02ows. One potential direc- tion for future work is to leverage iterative re\ufb01ne- ment techniques such as masked language models to further improve translation quality. Another ex- citing direction is to, theoretically and empirically, investigate the latent space in FlowSeq, hence pro- viding deeper insights into the model, and allow- ing for additional applications such as controllable text generation. Acknowledgments Xuezhe MA was supported in part by DARPA grant FA8750-18-2-0018 funded under the AIDA program and Chunting Zhou was supported by",
            "DARPA grant HR0011-15-C-0114 funded under the LORELEI program. Any opinions, \ufb01ndings, and conclusions expressed in this material are those of the authors and do not necessarily re\ufb02ect the views of DARPA. The authors thank Amazon for their gift of AWS cloud credits and anonymous reviewers for their helpful suggestions. References Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate. In International Con- ference on Learning Representations (ICLR). Samuel R Bowman, Luke Vilnis, Oriol Vinyals, An- drew M Dai, Rafal Jozefowicz, and Samy Ben- gio. 2015. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349. Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. 2015. Importance weighted autoencoders. arXiv preprint arXiv:1509.00519.",
            "arXiv preprint arXiv:1511.06349. Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. 2015. Importance weighted autoencoders. arXiv preprint arXiv:1509.00519. Mauro Cettolo, Christian Girardi, and Marcello Fed- erico. 2012. Wit3: Web inventory of transcribed and translated talks. In Conference of European Associ- ation for Machine Translation, pages 261\u2013268. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805. Laurent Dinh, Jascha Sohl-Dickstein, and Samy Ben- gio. 2016. Density estimation using real nvp. arXiv preprint arXiv:1605.08803. Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. 2019.",
            "2016. Density estimation using real nvp. arXiv preprint arXiv:1605.08803. Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. 2019. Constant-time machine translation with conditional masked language mod- els. arXiv preprint arXiv:1904.09324. Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. 2018. Non-autoregressive neural machine translation. Proceedings of the 6th International Conference on Learning Representa- tions (ICLR-2018). Jiatao Gu, Qi Liu, and Kyunghyun Cho. 2019. Insertion-based decoding with automatically inferred generation order. arXiv preprint arXiv:1902.01370. Xuanli He, Gholamreza Haffari, and Mohammad Norouzi. 2018. Sequence to sequence mixture model for diverse machine translation.",
            "Insertion-based decoding with automatically inferred generation order. arXiv preprint arXiv:1902.01370. Xuanli He, Gholamreza Haffari, and Mohammad Norouzi. 2018. Sequence to sequence mixture model for diverse machine translation. In Proceed- ings of the 22nd Conference on Computational Nat- ural Language Learning, pages 583\u2013592. Sergey Ioffe and Christian Szegedy. 2015. Batch nor- malization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pages 448\u2013456. Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. 2016. Improving variational inference with inverse autore- gressive \ufb02ow. The 29th Conference on Neural Infor- mation Processing Systems.",
            "2016. Improving variational inference with inverse autore- gressive \ufb02ow. The 29th Conference on Neural Infor- mation Processing Systems. Durk P Kingma and Prafulla Dhariwal. 2018. Glow: Generative \ufb02ow with invertible 1x1 convolutions. In Advances in Neural Information Processing Sys- tems, pages 10215\u201310224. Jason Lee, Elman Mansimov, and Kyunghyun Cho. 2018. Deterministic non-autoregressive neural se- quence modeling by iterative re\ufb01nement. In Pro- ceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing, pages 1173\u2013 1182. Jind\u02c7rich Libovick`y and Jind\u02c7rich Helcl. 2018. End-to- end non-autoregressive neural machine translation with connectionist temporal classi\ufb01cation. In Pro- ceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing, pages 3016\u2013 3021.",
            "2018. End-to- end non-autoregressive neural machine translation with connectionist temporal classi\ufb01cation. In Pro- ceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing, pages 3016\u2013 3021. Xuezhe Ma and Eduard Hovy. 2019. Macow: Masked convolutional generative \ufb02ow. arXiv preprint arXiv:1902.04208. Xuezhe Ma, Zecong Hu, Jingzhou Liu, Nanyun Peng, Graham Neubig, and Eduard Hovy. 2018. Stack- pointer networks for dependency parsing. In Pro- ceedings of the 56th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers), pages 1403\u20131414. Xuezhe Ma, Chunting Zhou, and Eduard Hovy. 2019. Mae: Mutual posterior-divergence regularization for variational autoencoders.",
            "Xuezhe Ma, Chunting Zhou, and Eduard Hovy. 2019. Mae: Mutual posterior-divergence regularization for variational autoencoders. In Proceedings of the 7th International Conference on Learning Representa- tions (ICLR-2019), New Orleans, Louisiana, USA. Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning generic context em- bedding with bidirectional LSTM. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pages 51\u201361, Berlin, Germany. Association for Computational Linguis- tics. Myle Ott, Michael Auli, David Grangier, et al. 2018. Analyzing uncertainty in neural machine translation. In International Conference on Machine Learning, pages 3953\u20133962. Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019: Demonstrations.",
            "George Papamakarios, Theo Pavlakou, and Iain Mur- ray. 2017. Masked autoregressive \ufb02ow for density estimation. In Advances in Neural Information Pro- cessing Systems, pages 2338\u20132347. Ryan Prenger, Rafael Valle, and Bryan Catanzaro. 2019. Waveglow: A \ufb02ow-based generative net- work for speech synthesis. In ICASSP 2019- 2019 IEEE International Conference on Acous- tics, Speech and Signal Processing (ICASSP), pages 3617\u20133621. IEEE. Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. 2018. On the convergence of adam and beyond. In Proceedings of the 6th International Conference on Learning Representations (ICLR-2018). Danilo Jimenez Rezende and Shakir Mohamed. 2015. Variational inference with normalizing \ufb02ows. In Proceedings of the 32nd International Conference on International Conference on Machine Learning- Volume 37, pages 1530\u20131538. JMLR. org.",
            "2015. Variational inference with normalizing \ufb02ows. In Proceedings of the 32nd International Conference on International Conference on Machine Learning- Volume 37, pages 1530\u20131538. JMLR. org. Alexander M Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sen- tence summarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan- guage Processing, pages 379\u2013389. Tianxiao Shen, Myle Ott, Michael Auli, et al. 2019. Mixture models for diverse machine translation: Tricks of the trade. In International Conference on Machine Learning, pages 5719\u20135728. Raphael Shu, Jason Lee, Hideki Nakayama, and Kyunghyun Cho. 2019. Latent-variable non- autoregressive neural machine translation with de- terministic inference using a delta posterior. arXiv preprint arXiv:1908.07181. Mitchell Stern, William Chan, Jamie Kiros, and Jakob Uszkoreit. 2019.",
            "Latent-variable non- autoregressive neural machine translation with de- terministic inference using a delta posterior. arXiv preprint arXiv:1908.07181. Mitchell Stern, William Chan, Jamie Kiros, and Jakob Uszkoreit. 2019. Insertion transformer: Flexible se- quence generation via insertion operations. arXiv preprint arXiv:1902.03249. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information pro- cessing systems, pages 5998\u20136008. Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show and tell: A neural im- age caption generator. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 3156\u20133164. Martin J Wainwright, Michael I Jordan, et al.",
            "2015. Show and tell: A neural im- age caption generator. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 3156\u20133164. Martin J Wainwright, Michael I Jordan, et al. 2008. Graphical models, exponential families, and varia- tional inference. Foundations and Trends R\u20ddin Ma- chine Learning, 1(1\u20132):1\u2013305. Yiren Wang, Fei Tian, Di He, Tao Qin, ChengXiang Zhai, and Tie-Yan Liu. 2019. Non-autoregressive machine translation with auxiliary regularization. arXiv preprint arXiv:1902.10245. Zachary Ziegler and Alexander Rush. 2019. Latent normalizing \ufb02ows for discrete sequences. In Inter- national Conference on Machine Learning, pages 7673\u20137682.",
            "Appendix: FlowSeq A Flow Layers ActNorm z\u2032 t = s \u2299zt + b. Log-determinant: T \u00b7 sum(log |s|) Invertible Linear z\u2032 t = ztW, Log-determinant: T \u00b7 h \u00b7 log |det(W)| where h is the number of heads. Af\ufb01ne Coupling za, zb = split(z) z\u2032 a = za z\u2032 b = s(za, x) \u2299zb + b(za, x) z\u2032 = concat(z\u2032 a, z\u2032 b), Log-determinant: sum(log |s|) B Model Details Model Dimensions (Model\/Hidden) #Params Transformer-base 512\/2048 65M Transformer-large 2014\/4096 218M FlowSeq-base 256\/512 73M FlowSeq=large 512\/2014 258M Table 3: Comparison of model size in our experiments.",
            "C Analysis of training dynamics 0 10 20 30 40 50 Epoch 60 70 80 Train Reconstruction Loss Reconstruction Loss KL loss 100 200 300 Train KL loss (a) training loss 0 10 20 30 40 50 Epoch 10 20 30 40 Dev Reconstruction Loss Reconstruction Loss KL loss 50 100 150 Dev KL loss (b) dev loss 0 10 20 30 40 50 Epoch 75 100 125 150 Dev NLL loss NLL loss BLEU 15.0 17.5 20.0 22.5 Dev BLEU (c) dev BLEU Figure 7: Training dynamics. In Fig. 7, we plot the train and dev loss together with dev BLEU scores for the \ufb01rst 50 epochs. We can see that the reconstruction loss is increasing at the initial stage of training, then starts to decrease when training with full KL loss. In addition, we observed that FlowSeq does not suffer the KL collapse prob- lem (Bowman et al., 2015; Ma et al., 2019).",
            "We can see that the reconstruction loss is increasing at the initial stage of training, then starts to decrease when training with full KL loss. In addition, we observed that FlowSeq does not suffer the KL collapse prob- lem (Bowman et al., 2015; Ma et al., 2019). This is because the decoder of FlowSeq is non-autogressive, with latent variable z as the only input.",
            "D Analysis of Translation Results Source Grundnahrungsmittel gibt es schlielich berall und jeder Supermarkt hat mit- tlerweile Sojamilch und andere Produkte. Ground Truth There are basic foodstuffs available everywhere , and every supermarket now has soya milk and other products. Sample 1 After all, there are basic foods everywhere and every supermarket now has soya amch and other products. Sample 2 After all, the food are available everywhere everywhere and every supermarket has soya milk and other products. Sample 3 After all, basic foods exist everywhere and every supermarket has now had soy milk and other products. Source Es kann nicht erkl\u00a8aren, weshalb die National Security Agency Daten ber das Privatleben von Amerikanern sammelt und warum Whistleblower bestraft wer- den, die staatliches Fehlverhalten offenlegen. Ground Truth And, most recently, it cannot excuse the failure to design a simple website more than three years since the Affordable Care Act was signed into law. Sample 1 And recently, it cannot apologise for the inability to design a simple website in the more than three years since the adoption of Affordable Care Act.",
            "Ground Truth And, most recently, it cannot excuse the failure to design a simple website more than three years since the Affordable Care Act was signed into law. Sample 1 And recently, it cannot apologise for the inability to design a simple website in the more than three years since the adoption of Affordable Care Act. Sample 2 And recently, it cannot excuse the inability to design a simple website in more than three years since the adoption of Affordable Care Act. Sample 3 Recently, it cannot excuse the inability to design a simple website in more than three years since the Affordable Care Act has passed. Source Doch wenn ich mir die oben genannten Beispiele ansehe, dann scheinen sie weitgehend von der Regierung selbst gew\u00a8ahlt zu sein. Ground Truth Yet, of all of the examples that I have listed above, they largely seem to be of the administration\u2019s own choosing. Sample 1 However, when I look at the above mentioned examples, they seem to be largely elected by the government itself. Sample 2 But if I look at the above mentioned examples, they seem to have been largely elected by the government itself.",
            "Sample 1 However, when I look at the above mentioned examples, they seem to be largely elected by the government itself. Sample 2 But if I look at the above mentioned examples, they seem to have been largely elected by the government itself. Sample 3 But when I look at the above examples, they seem to be largely chosen by the government itself. Source Damit wollte sie auf die Gefahr von noch gr\u00a8oeren Ruinen auf der Schweizer Wiese hinweisen - sollte das Riesenprojekt eines Tages scheitern. Ground Truth In so doing they wanted to point out the danger of even bigger ruins on the Schweizer Wiese - should the huge project one day fail. Sample 1 In so doing, it wanted to highlight the risk of even greater ruins on the Swiss meadow - the giant project should fail one day. Sample 2 In so doing, it wanted to highlight the risk of even greater ruins on the Swiss meadow - if the giant project fail one day. Sample 3 In doing so, it wanted point out the risk of even greater ruins on the Swiss meadow - the giant project would fail one day.",
            "Sample 2 In so doing, it wanted to highlight the risk of even greater ruins on the Swiss meadow - if the giant project fail one day. Sample 3 In doing so, it wanted point out the risk of even greater ruins on the Swiss meadow - the giant project would fail one day. Table 4: Examples of translation outputs from FlowSeq-base with sampling hyperparameters l = 3, r = 10, \u03c4 = 0.4 on WMT14-DEEN. In Tab. 4, we present randomly picked translation outputs from the test set of WMT14-DEEN. For each German input sentence, we pick three hypotheses from 30 samples. We have the following observations: First, in most cases, it can accurately express the meaning of the source sentence, sometimes in a different way from the reference sentence, which cannot be precisely re\ufb02ected by the BLEU score. Second, by",
            "controlling the sampling hyper-parameters such as the length candidates l, the sampling temperature \u03c4 and the number of samples r under each length, FlowSeq is able to generate diverse translations expressing the same meaning. Third, repetition and broken translations also exist in some cases due to the lack of direct modeling of dependencies between target words. E Results of Translation Diversity Table 5 shows the detailed results of translation diversity. Models \u03c4 Pairwise BLEU LOO BLEU Human \u2013 35.48 69.07 Sampling \u2013 24.10 37.80 Beam Search \u2013 73.00 69.90 Hard-MoE \u2013 50.02 63.80 FlowSeq l=1, r=10 0.1 79.39 61.61 0.2 72.12 61.05 0.3 67.85 60.79 0.4 64.75 60.07 0.5 61.12 59.54 1.0 43.53 52.86 FlowSeq l=2, r=5 0.1 70.32 60.54 0.2 66.45 60.21 0.",
            "07 0.5 61.12 59.54 1.0 43.53 52.86 FlowSeq l=2, r=5 0.1 70.32 60.54 0.2 66.45 60.21 0.3 63.72 59.81 0.4 61.29 59.47 0.5 58.49 58.80 1.0 42.93 52.58 FlowSeq l=3, r=4 0.1 62.21 58.70 0.2 59.74 58.59 0.3 57.57 57.96 0.4 55.66 57.45 0.5 53.49 56.93 1.0 39.75 50.94 Table 5: Translation diversity results of FlowSeq-large model on WMT14 EN-DE with knowledge distillation."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1909.02480.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 13060.999465942383,
    "avg_doclen_est": 178.9178009033203
}
