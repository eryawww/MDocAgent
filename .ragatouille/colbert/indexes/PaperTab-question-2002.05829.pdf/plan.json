{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "HULK: An Energy Ef\ufb01ciency Benchmark Platform for Responsible Natural Language Processing Xiyou Zhou, Zhiyu Chen, Xiaoyong Jin, William Yang Wang Department of Computer Science, University of California Santa Barbara {xiyou, zhiyuchen, x jin, william}@cs.ucsb.edu Abstract Computation-intensive pretrained models have been taking the lead of many natural language processing benchmarks such as GLUE (Wang et al., 2018). However, energy ef\ufb01ciency in the process of model training and inference becomes a critical bottleneck. We introduce HULK, a multi-task energy ef\ufb01ciency bench- marking platform for responsible natural lan- guage processing. With HULK, we compare pretrained models\u2019 energy ef\ufb01ciency from the perspectives of time and cost. Baseline bench- marking results are provided for further anal- ysis. The \ufb01ne-tuning ef\ufb01ciency of different pretrained models can differ a lot among dif- ferent tasks and fewer parameter number does not necessarily imply better ef\ufb01ciency. We analyzed such phenomenon and demonstrate the method of comparing the multi-task ef\ufb01- ciency of pretrained models.",
            "We analyzed such phenomenon and demonstrate the method of comparing the multi-task ef\ufb01- ciency of pretrained models. Our platform is available at https:\/\/sites.engineering. ucsb.edu\/\u02dcxiyou\/hulk\/. 1 Introduction Environmental concerns of machine learning re- search has been rising as the carbon emission of certain tasks like neural architecture search reached an exceptional \u201cocean boiling\u201d level (Strubell et al., 2019). Increased carbon emission has been one of the key factors to aggravate global warming 1. Research and development process like parame- ter search further increase the environment impact. When using cloud-based machines, the environ- ment impact is strongly correlated with budget. The recent emergence of leaderboards such as SQuAD (Rajpurkar et al., 2016), GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) has greatly boosted the development of advanced models in the NLP community. Pretrained models 1Source: https:\/\/climate.nasa.gov\/causes\/ have proven to be the key ingredient for achieving state of the art in conventional metrics. However, such models can be extremely expensive to train.",
            "Pretrained models 1Source: https:\/\/climate.nasa.gov\/causes\/ have proven to be the key ingredient for achieving state of the art in conventional metrics. However, such models can be extremely expensive to train. For example, XLNet-Large (Yang et al., 2019) was trained on 512 TPU v3 chips for 500K steps, which costs around 61,440 dollars2, let alone staggeringly large carbon emission. Moreover, despite impressive performance gain, the \ufb01ne-tuning and inference ef\ufb01ciency of NLP models remain under-explored. As recently men- tioned in a tweet3, the popular AI text adventure game AI Dungeon has reached 100 million infer- ences. The energy ef\ufb01ciency of inference cost could be critical to both business planning and en- vironment impact. Previous work (Schwartz et al., 2019; Dodge et al., 2019) on this topic proposed new metrics like FPO (\ufb02oating point operations) and new prac- tice to report experimental results based on com- puting budget.",
            "Previous work (Schwartz et al., 2019; Dodge et al., 2019) on this topic proposed new metrics like FPO (\ufb02oating point operations) and new prac- tice to report experimental results based on com- puting budget. Other benchmarks like (Coleman et al., 2017) and (Mattson et al., 2019) compares the ef\ufb01ciency of models on the classic reading com- prehension task SQuAD and machine translation tasks. However, there has not been a concrete or practical reference for accurate estimation on NLP model pretraining, \ufb01ne-tunning and inference con- sidering multi-task energy ef\ufb01ciency. Energy ef\ufb01ciency can be re\ufb02ected in many met- rics including carbon emission, electricity usage, time consumption, number of parameters and FPO as shown in (Schwartz et al., 2019). Carbon emis- sion and electricity are intuitive measures yet ei- ther hard to track or hardware-dependent. Number of parameteres does not re\ufb02ect the acutal cost for model training and inference.",
            "Carbon emis- sion and electricity are intuitive measures yet ei- ther hard to track or hardware-dependent. Number of parameteres does not re\ufb02ect the acutal cost for model training and inference. FPO is steady for models but cannot be directly used for cost estima- tion. Here in order to provide a practical reference 2Source: https:\/\/bit.ly\/301qUMo 3Source: https:\/\/bit.ly\/2GAFBNO arXiv:2002.05829v1  [cs.CL]  14 Feb 2020",
            "Model Hardware Time Cost Params BERTBASE (Devlin et al., 2018) 4 TPU Pods 4 days $1,728 108M BERTLARGE (Devlin et al., 2018) 16 TPU Pods 4 days $6,912 334M XLNetBASE (Yang et al., 2019) \u2013 \u2013 \u2013 117M XLNetLARGE (Yang et al., 2019) 512 TPU v3 2.5 days $61,440 361M RoBERTaBASE (Liu et al., 2019) 1024 V100 GPUs 1 day $75,203 125M RoBERTaLARGE (Liu et al., 2019) 1024 V100 GPUs 1 day $75,203 356M ALBERTBASE (Lan et al., 2019) 64 TPU v3 \u2013 \u2013 12M ALBERTLARGE (Lan et al., 2019) \u2013 \u2013 \u2013 18M ALBERTXLARGE (Lan et al., 2019) \u2013 \u2013 \u2013 59M ALBERTXXLARGE (Lan et al.",
            ", 2019) \u2013 \u2013 \u2013 18M ALBERTXLARGE (Lan et al., 2019) \u2013 \u2013 \u2013 59M ALBERTXXLARGE (Lan et al., 2019) 1024 TPU v3 32 hours $65,536 223M DistilBERT* (Sanh et al., 2019) 8\u00d716G V100 GPU 90 hours $2203.2 66M Table 1: Pretraining costs of baseline models. Hardware and pretraining time are collected from original papers, with which costs are estimated with current TPU price at $8 per hour with 4 core TPU v3 chips and V100 GPU at $3.06 per hour. DistilBERT model is trained upon a pretrained BERT model. Parameter numbers are esti- mated using the pretrained models implemented in the Transformers (https:\/\/github.com\/huggingface\/ transformers) library (Wolf et al., 2019), shown in million. for model selection for real applications, especially model development outside of academia, we keep track of the time consumption and acutal budget for comparison.",
            "for model selection for real applications, especially model development outside of academia, we keep track of the time consumption and acutal budget for comparison. Cloud based machines are employed for cost estimation as they are easily accessible and consistent in hardware con\ufb01guration and per- formance. In the following sections, we would use time and cost to denote the time elapsed and the acutal budget in model pretraining \/ training \/ inference. In most NLP pretrained model setting, there are three phases: pretraining, \ufb01ne-tuning and inference. If a model is trained from scratch, we consider such model has no pretraining phase but \ufb01ne-tuned from scratch. Typically pretraining takes several days and hundreds of dollars, according to Table 1. Fine- tuning takes a few minutes to hours, costing a lot less than pretraining phase. Inference takes several milli-seconds to seconds, costing much less than \ufb01ne-tuning phase. Meanwhile, pretraining is done before \ufb01ne-tuning once for all, while \ufb01ne-tuning could be performed multiple times as training data updates. Inference is expected to be called numer- ous times for downstream applications.",
            "Meanwhile, pretraining is done before \ufb01ne-tuning once for all, while \ufb01ne-tuning could be performed multiple times as training data updates. Inference is expected to be called numer- ous times for downstream applications. Such char- acteristics make it an intuitive choice to separate different phases during benchmarking. Our HULK benchmark, as shown in Figure 1, utilizes several classic datasets that have been widely adopted in the community as benchmark- ing tasks to benchmark energy ef\ufb01ciency and com- pares pretrained models in a multi-task fashion. The tasks include natural language inference task MNLI (Williams et al., 2017), sentiment analy- sis task SST-2 (Socher et al., 2013) and Named Entity Recognition Task CoNLL-2003 (Sang and De Meulder, 2003). Such tasks are selected to pro- vide a thourough comparison of end-to-end energy ef\ufb01ciency in pretraining, \ufb01ne-tuning and inference.",
            "Such tasks are selected to pro- vide a thourough comparison of end-to-end energy ef\ufb01ciency in pretraining, \ufb01ne-tuning and inference. With the HULK benchmark, we quantify the en- ergy ef\ufb01ciency of model pretraining, \ufb01ne-tuning and inference phase by comparing the time and cost they require to reach certain overall task-speci\ufb01c performance level on selected datasets. The design principle and benchmarking process are detailed in section 2. We also explore the relation between model parameter and \ufb01ne-tuning ef\ufb01ciency and demonstrate consistency of energy ef\ufb01ciency be- tween tasks for different pretrained models. 2 Benchmark Overview For pretraining phase, the benchmark is designed to favor energy ef\ufb01cient models in terms of time and cost that each model takes to reach certain multi-task performance pretrained from scratch. For example, we keep track of the time and cost of a BERT model pretrained from scratch. After every thousand of pretraining steps, we clone the model for \ufb01ne-tuning and see if the \ufb01nal perfor- mance can reach our cut-off level.",
            "For example, we keep track of the time and cost of a BERT model pretrained from scratch. After every thousand of pretraining steps, we clone the model for \ufb01ne-tuning and see if the \ufb01nal perfor- mance can reach our cut-off level. When the level is reached, time and cost for pretraining is used for comparison. Models faster or cheaper to pretrain are recommended. For \ufb01ne-tuning phase, we consider the time and cost each model requires to reach certain multi-",
            "CoNLL 2003 MNLI SST-2 Train Size 14,041 392,702 67,349 Dev Size 3,250 19,647 872 Cut-off 91 85 90 Metric F1 Acc Acc SOTA 93.5 91.85 97.4 Table 2: Dataset Information task performance \ufb01ne-tuned from given pretrained models because for each single task with different dif\ufb01culty and instance number, the \ufb01ne-tuning char- acteristics may differ a lot. When pretrained mod- els are used to deal with non-standard downstream task, especially ad hoc application in industry, the training set\u2019s dif\ufb01culty cannot be accurately esti- mated. Therefore, it\u2019s important to compare the multi-task ef\ufb01ciency for model choice. For inference phase, the time and cost of each model making inference for single instance on mul- tiple tasks are considered in the similar fashion as the \ufb01ne-tuning phase. 2.1 Dataset Overview The datasets we used are widely adopted in NLP community. Quantitative details of datasets can be found in Table 2.",
            "2.1 Dataset Overview The datasets we used are widely adopted in NLP community. Quantitative details of datasets can be found in Table 2. The selected tasks are shown below: CoNLL 2003 The Conference on Com- putational Natural Language Learning (CoNLL-2003) shared task concerns language- independent named entity recognition (Sang and De Meulder, 2003). The task concentrates on four types of named entities: persons, loca- tions, organizations and other miscellaneous entities. Here we only use the English dataset. The English data is a collection of news wire articles from the Reuters Corpus. Result is re\ufb02ected as F1 score considering the label accuracy and recall on dev set. MNLI The Multi-Genre Natural Language Inference Corpus (Williams et al., 2017) is a crowdsourced collection of sentence pairs with textual entailment annotations. Given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment), contradicts the hypoth- esis (contradiction), or neither (neutral).",
            "Given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment), contradicts the hypoth- esis (contradiction), or neither (neutral). The premise sentences are gathered from ten differ- ent sources, including transcribed speech, \ufb01c- tion, and government reports. The accuracy score is reported as the average of performance on matched and mismatched dev sets. SST-2 The Stanford Sentiment Treebank (Socher et al., 2013) consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the senti- ment of a given sentence. Following the set- ting of GLUE, we also use the two-way (posi- tive\/negative) class split, and use only sentence- level labels. The tasks are selected based on how represen- titve the dataset is. CoNLL 2003 has been a widely used dataset for named entity recognition and acu- tally requires output of token level labeling. NER is a core NLP task and CoNLL 2003 has been a clas- sic dataset in this area.",
            "CoNLL 2003 has been a widely used dataset for named entity recognition and acu- tally requires output of token level labeling. NER is a core NLP task and CoNLL 2003 has been a clas- sic dataset in this area. SST-2 and MNLI are part of the GLUE benchmark, representing sentence level labeling tasks. SST-2 has been frequently used in sentiment analysis across different genera- tions of models. MNLI is a newly introduced large dataset for natural language inference. The train- ing time for MNLI is relatively long and the task requires a lot more training instances. We select the three tasks for a diverse yet practical bench- mark for pretrained models without constrain the models to sentence level classi\ufb01cation tasks. In addition, their ef\ufb01ciency differ signi\ufb01cantly in the \ufb01ne-tuning and inference phase. Such difference can still be re\ufb02ected on the \ufb01nal score after nor- malization as shown in Table 3. Provided with more computing resource , we can bring in more datasets for even more thorough benchmarking in the furture.",
            "Such difference can still be re\ufb02ected on the \ufb01nal score after nor- malization as shown in Table 3. Provided with more computing resource , we can bring in more datasets for even more thorough benchmarking in the furture. We illustrate the evaluation criteria in the following subsection. 2.2 Evaluation Criteria In machine learning model training and inference, slight parameter change can have subtle impact on the \ufb01nal result. In order to make a practical refer- ence for pretrained model selection, we compare models\u2019 end-to-end performance with respect to the pretraining time, pretraining cost, training time, training cost, inference time, infernce latency and cost following the setting of (Coleman et al., 2017). For pretraining phase, we design the process to explore how much computing resource is re- quired to reach certain multi-task performance by \ufb01ne-tuning after the pretraining. Therefore, during",
            "Figure 1: Screenshot of the leaderboard of website. Datasets CoNLL 2003 SST-2 MNLI Model Time Score Time Score Time Score Overall Score BERTBASE 43.43 2.08 207.15 0.45 N\/A 0.00 2.53 BERTLARGE 90.26 1.00 92.45 1.00 9,106.72 1.00 3.00 XLNetBASE 67.14 1.34 102.45 0.90 7,704.71 1.18 3.42 XLNetLARGE 243.00 0.37 367.11 0.25 939.62 9.69 10.31 RoBERTaBASE 70.57 1.28 38.45 2.40 274.87 7.14 10.82 RoBERTaLARGE 155.43 0.58 57.65 1.60 397.12 22.93 25.11 ALBERTBASE 340.64 0.26 2,767.90 0.03 N\/A 0.",
            "14 10.82 RoBERTaLARGE 155.43 0.58 57.65 1.60 397.12 22.93 25.11 ALBERTBASE 340.64 0.26 2,767.90 0.03 N\/A 0.00 0.29 ALBERTLARGE 844.85 0.11 3,708.49 0.02 N\/A 0.00 0.13 Table 3: Multi-task Baseline Fine-tuning Costs. Time is given in seconds and score is computed by the division of TimeBERTLARGE\/Timemodel.The experiments are conducted on a single GTX 2080 Ti GPU following the evaluation ceriteria. The overall score is computed by summing up scores of each individual task. For cost based leaderboads, we also use the budget to compute a new score for each task and summarize similarly. \u201cN\/A\u201d means fail to reach the given performance after 5 epochs.",
            "The overall score is computed by summing up scores of each individual task. For cost based leaderboads, we also use the budget to compute a new score for each task and summarize similarly. \u201cN\/A\u201d means fail to reach the given performance after 5 epochs. model pretraining, after a number of steps, we use the half-pretrained model for \ufb01ne-tuning and see if the \ufb01ne-tuned model can reach our cut-off perfor- mance. When it does, we count the time and cost in the pretraining process for benchmarking and analysis. For \ufb01ne-tuning phase, we want to compare the general ef\ufb01ciency of pretrained model reaching cut-off performance on selected dataset. During \ufb01ne-tuning, we evaluate the half-\ufb01ne-tuned model on development set after a certain number of steps. When the performance reach our cut-off perfor- mance, we count the time and cost in this \ufb01ne- tuning process for benchmarking and analysis.",
            "During \ufb01ne-tuning, we evaluate the half-\ufb01ne-tuned model on development set after a certain number of steps. When the performance reach our cut-off perfor- mance, we count the time and cost in this \ufb01ne- tuning process for benchmarking and analysis. To be speci\ufb01c, for a single pretrained model, the ef\ufb01- ciency score on different tasks is de\ufb01ned as the sum of normalized time and cost. Here we normalize the time and cost because they vary dramatically between tasks. In order to simplify the process, we compute the ratio of BERTLARGE\u2019s time and cost to that of each model as the normalized measure as shown in Table 3 and Table 4. For inference phase, we follow the principles in fune-tuning except we use the time and cost of inference for benchmarking. 2.3 Performance Cut-off Selection The selection of performance cutoff could be very critical because we consider certrain models being quali\ufb01ed after reaching certrain performance on development set.",
            "For inference phase, we follow the principles in fune-tuning except we use the time and cost of inference for benchmarking. 2.3 Performance Cut-off Selection The selection of performance cutoff could be very critical because we consider certrain models being quali\ufb01ed after reaching certrain performance on development set. Meanwhile, certrain tasks can reach a \u201csweet point\u201d where after relatively smaller amount of training time, the model reaches perfor- mance close to the \ufb01nal results despite negelagi- ble difference. We select the cut-off performance threshold by obersvering the recent state-of-the-art performance on selected tasks.",
            "Datasets CoNLL 2003 SST-2 MNLI Model Time Score Time Score Time Score Overall Score BERTBASE 2.68 3.18 2.70 3.13 2.67 3.19 9.5 BERTLARGE 8.51 1.00 8.46 1.00 8.53 1.00 3.00 XLNetBASE 5.16 1.65 5.01 1.69 5.10 1.67 5.01 XLNetLARGE 14.84 0.57 14.69 0.58 15.27 0.56 1.71 RoBERTaBASE 2.65 3.21 2.68 3.16 2.70 3.16 9.53 RoBERTaLARGE 8.35 1.02 8.36 1.01 8.70 0.98 3.01 ALBERTBASE 2.65 3.21 2.68 3.18 2.72 3.14 9.53 ALBERTLARGE 8.49 1.",
            "02 8.36 1.01 8.70 0.98 3.01 ALBERTBASE 2.65 3.21 2.68 3.18 2.72 3.14 9.53 ALBERTLARGE 8.49 1.00 8.44 1.00 8.78 0.97 2.97 Table 4: Multi-task Baseline Inference Costs. Time is given in milliseconds and score is computed by the division of TimeBERTLARGE\/Timemodel.The experiments are conducted on a single GTX 2080 Ti GPU following the evaluation ceriteria similar to \ufb01ne-tuning part. It\u2019s clear that the inference time between tasks is more consistent compared to \ufb01ne-tuning phase. 2.4 Submission to Benchmark Submissions can be made to our benchmark through sending code and results to our HULK benchmark CodaLab competition4 following the guidelines in both our FAQ part of website and competition introduction. We require the submis- sions to include detailed end-to-end model training information including model run time, cost(cloud based machine only), parameter number and part of the development set output for result validation.",
            "We require the submis- sions to include detailed end-to-end model training information including model run time, cost(cloud based machine only), parameter number and part of the development set output for result validation. A training \/ \ufb01ne-tuning log including time consump- tion and dev set performance after certain steps is also required. For inference, development set output, time consumption and hardware \/ software details should be provided. In order for model re- producity, source code is required. 3 Baseline Settings and Analysis For computation-heavy tasks, we adopt the re- ported resource requirements in the original papers as the pretraining phase baselines. For \ufb01ne-tuning and inference phase, we conduct extensive experiments on given hardware (GTX 2080Ti GPU) with different model settings as shown in Table 3 and Table 4. We also collect the devlopment set performance with time in \ufb01ne- tuning to investigate in how the model are \ufb01ne- tuned for different tasks.",
            "We also collect the devlopment set performance with time in \ufb01ne- tuning to investigate in how the model are \ufb01ne- tuned for different tasks. In our \ufb01ne-tuning setting, we are given a speci\ufb01c hardware and software con\ufb01guration, we adjust the hyper-parameter to minimize the time required for \ufb01ne-tuning towards cut-off performance. For exam- ple, we choose proper batchsize and learning rate 4The CodaLab competition is accessible from the website. for BERTBASE to make sure the model converges and can reach expected performance as soon as possible with parameter searching. As shown in Figure 2, the \ufb01ne-tuning perfor- mance curve differs a lot among pretrained models. The x-axis denoting time consumed is shown in log-scale for better comparison of different models. None of the models acutally take the lead in all tasks. However, if two pretrained models are in the same family, such as BERTBASE and BERTLARGE, the model with smaller number of parameters tend to converge a bit faster than the other in the NER and SST-2 task.",
            "However, if two pretrained models are in the same family, such as BERTBASE and BERTLARGE, the model with smaller number of parameters tend to converge a bit faster than the other in the NER and SST-2 task. In the MNLI task, such trend does not apply possibly due to increased diffculty level and training instance number which favor larger model capacity. Even though ALBERT model has a lot less pa- rameters than BERT, according to Table 1, the \ufb01ne-tuning time of ALBERT model is signi\ufb01cantly more than BERT models. This is probably because ALBERT uses large hidden size and more expen- sive matrix computation. The parameter sharing technique actually makes it harder to \ufb01ne-tune the model. RoBERTaLARGE model relatively stable in all tasks. 4 Related Work GLUE benchmark (Wang et al., 2018) is a popular multi-task benchmarking and diagnosis platform providing score evaluating multi-task NLP mod- els considering multiple single task performance.",
            "RoBERTaLARGE model relatively stable in all tasks. 4 Related Work GLUE benchmark (Wang et al., 2018) is a popular multi-task benchmarking and diagnosis platform providing score evaluating multi-task NLP mod- els considering multiple single task performance. SuperGLUE (Wang et al., 2019) further develops the task and enriches the dataset used in evalua- tion, making the task more challenging. These",
            "101 102 103 Time-sec 0.2 0.4 0.6 0.8 F1(Dev) BERT-BASE BERT-LARGE RoBERTa-BASE RoBERTa-LARGE XLNet-BASE XLNet-LARGE ALBERT-BASE ALBERT-LARGE 101 102 103 Time-sec 0.5 0.6 0.7 0.8 0.9 Accuracy(Dev) BERT-BASE BERT-LARGE RoBERTa-BASE RoBERTa-LARGE XLNet-BASE XLNet-LARGE ALBERT-BASE ALBERT-LARGE 102 103 104 Time-sec 0.4 0.5 0.6 0.7 0.8 Accuracy(Dev) BERT-BASE BERT-LARGE RoBERTa-BASE RoBERTa-LARGE XLNet-LARGE XLNet-BASE ALBERT-BASE ALBERT-LARGE Figure 2: The comparison between different pretrained models for CoNLL 2003, SST-2 and MNLI datasets trained on a single GTX 2080Ti GPU. The curves are smoothed by computing average with 2 adjacent data points.",
            "The curves are smoothed by computing average with 2 adjacent data points. The experiments are conducted by selecting hyper-parameters to minimize the time consumption yet making sure the model can converge after certain amount of time. Results are demonstrated using per- formance on development score after certain steps \ufb01ne- tuned on the training dataset. multi-task benchmarks does not take computation ef\ufb01ciency into consideration but still innovates the development of pretrained models. MLPerf (Mattson et al., 2019) compares training and inference ef\ufb01ciency from hardware perspective, providing helpful resources on hardware selection and model training. Their benchmark is limited to focusing on several typical applications including image classi\ufb01cation and machine translation. Previous work (Schwartz et al., 2019; Dodge et al., 2019) on related topic working towards \u201cGreen AI\u201d proposes new metrics like FPO and new principle in ef\ufb01ciency evaluation. We further make more detailed and practical contributions towards model energy ef\ufb01ciency benchmarking.",
            "We further make more detailed and practical contributions towards model energy ef\ufb01ciency benchmarking. Other work like DAWNBenchmark (Coleman et al., 2017) looks into the area of end-to-end model ef\ufb01ciency comparison for both computer vision and NLP task SQuAD. The benchmark does not com- pare multi-task ef\ufb01ciency performance and covered only one NLP task. The Ef\ufb01cient NMT shared task of The 2nd Work- shop on Neural Machine Translation and Genera- tion proposed ef\ufb01ciency track to compare neural machine translation models\u2019 inference time. Our platform covers more phases and support multi-task comparison. 5 Conclusion We developed the HULK platform focusing on the energy ef\ufb01ciency evaluation of NLP models based on their end-to-end performance on selected NLP tasks. The HULK platform compares models in pretraining, \ufb01ne-tuning and inference phase, mak- ing it clear to follow and propose more training and inference ef\ufb01cient models.",
            "The HULK platform compares models in pretraining, \ufb01ne-tuning and inference phase, mak- ing it clear to follow and propose more training and inference ef\ufb01cient models. We have compared the \ufb01ne-tuning ef\ufb01ciency of given models during baseline testing and demonstrated more parame- ters lead to slower \ufb01ne-tuning when using same model but does not hold when model changes.We expect more submissions in the future to \ufb02ourish and enrich our benchmark. Acknowledgments This work is supported by the Institute of Energy Ef\ufb01ciency (IEE) at UCSB\u2019s seed grant in Summer 2019 to improve the energy ef\ufb01ciency of AI and machine learning.5. 5https:\/\/iee.ucsb.edu\/news\/making-ai-more-energy- ef\ufb01cient",
            "References Cody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis, Kunle Olukotun, Chris R\u00b4e, and Matei Zaharia. 2017. Dawnbench: An end-to-end deep learning bench- mark and competition. Training, 100(101):102. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805. Jesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, and Noah A Smith. 2019. Show your work: Improved reporting of experimental results. arXiv preprint arXiv:1909.03004. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learn- ing of language representations. arXiv preprint arXiv:1909.11942.",
            "2019. Albert: A lite bert for self-supervised learn- ing of language representations. arXiv preprint arXiv:1909.11942. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap- proach. arXiv preprint arXiv:1907.11692. Peter Mattson, Christine Cheng, Cody Coleman, Greg Diamos, Paulius Micikevicius, David Patterson, Hanlin Tang, Gu-Yeon Wei, Peter Bailis, Victor Bit- torf, et al. 2019. Mlperf training benchmark. arXiv preprint arXiv:1910.01500. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text.",
            "arXiv preprint arXiv:1910.01500. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250. Erik F Sang and Fien De Meulder. 2003. Intro- duction to the conll-2003 shared task: Language- independent named entity recognition. arXiv preprint cs\/0306050. Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108. Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. 2019. Green ai. arXiv preprint arXiv:1907.10597. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.",
            "2019. Green ai. arXiv preprint arXiv:1907.10597. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment tree- bank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u20131642. Emma Strubell, Ananya Ganesh, and Andrew Mc- Callum. 2019. Energy and policy considera- tions for deep learning in nlp. arXiv preprint arXiv:1906.02243. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2019. Super- glue: A stickier benchmark for general-purpose language understanding systems. arXiv preprint arXiv:1905.00537. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018.",
            "Super- glue: A stickier benchmark for general-purpose language understanding systems. arXiv preprint arXiv:1905.00537. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461. Adina Williams, Nikita Nangia, and Samuel R Bow- man. 2017. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, R\u2019emi Louf, Morgan Funtow- icz, and Jamie Brew. 2019. Huggingface\u2019s trans- formers: State-of-the-art natural language process- ing. ArXiv, abs\/1910.03771.",
            "2019. Huggingface\u2019s trans- formers: State-of-the-art natural language process- ing. ArXiv, abs\/1910.03771. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car- bonell, Ruslan Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretrain- ing for language understanding. arXiv preprint arXiv:1906.08237."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-2002.05829.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 6512.0,
    "avg_doclen_est": 176.0
}
