{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims Isabelle Augenstein Christina Lioma Dongsheng Wang Lucas Chaves Lima Casper Hansen Christian Hansen Jakob Grue Simonsen Department of Computer Science University of Copenhagen {augenstein,c.lioma,wang,lcl,c.hansen,chrh,simonsen}@di.ku.dk Abstract We contribute the largest publicly available dataset of naturally occurring factual claims for the purpose of automatic claim veri\ufb01cation. It is collected from 26 fact checking websites in English, paired with textual sources and rich metadata, and labelled for veracity by hu- man expert journalists. We present an in-depth analysis of the dataset, highlighting character- istics and challenges. Further, we present re- sults for automatic veracity prediction, both with established baselines and with a novel method for joint ranking of evidence pages and predicting veracity that outperforms all base- lines. Signi\ufb01cant performance increases are achieved by encoding evidence, and by mod- elling metadata. Our best-performing model achieves a Macro F1 of 49.2%, showing that this is a challenging testbed for claim veracity prediction.",
            "Signi\ufb01cant performance increases are achieved by encoding evidence, and by mod- elling metadata. Our best-performing model achieves a Macro F1 of 49.2%, showing that this is a challenging testbed for claim veracity prediction. 1 Introduction Misinformation and disinformation are two of the most pertinent and dif\ufb01cult challenges of the in- formation age, exacerbated by the popularity of social media. In an effort to counter this, a signif- icant amount of manual labour has been invested in fact checking claims, often collecting the results of these manual checks on fact checking portals or websites such as politifact.com or snopes.com. In a parallel development, researchers have recently started to view fact checking as a task that can be partially automated, using machine learning and NLP to automatically predict the veracity of claims. However, existing efforts either use small datasets consisting of naturally occurring claims (e.g. Mihalcea and Strapparava (2009); Zubiaga et al. (2016)), or datasets consisting of arti\ufb01cially constructed claims such as FEVER (Thorne et al., 2018).",
            "Mihalcea and Strapparava (2009); Zubiaga et al. (2016)), or datasets consisting of arti\ufb01cially constructed claims such as FEVER (Thorne et al., 2018). While the latter offer valuable contribu- tions to further automatic claim veri\ufb01cation work, they cannot replace real-world datasets. Feature Value ClaimID farg-00004 Claim Mexico and Canada assemble cars with foreign parts and send them to the U.S. with no tax. Label distorts Claim URL https:\/\/www.factcheck.org\/2018\/ 10\/factchecking-trump-on-trade\/ Reason None Category the-factcheck-wire Speaker Donald Trump Checker Eugene Kiely Tags North American Free Trade Agreement Claim Entities United States, Canada, Mexico Article Title FactChecking Trump on Trade Publish Date October 3, 2018 Claim Date Monday, October 1, 2018 Table 1: An example of a claim instance. Entities are obtained via entity linking. Article and outlink texts, evidence search snippets and pages are not shown. Contributions.",
            "Entities are obtained via entity linking. Article and outlink texts, evidence search snippets and pages are not shown. Contributions. We introduce the currently largest claim veri\ufb01cation dataset of naturally occurring claims.1 It consists of 34,918 claims, collected from 26 fact checking websites in English; evidence pages to verify the claims; the context in which they occurred; and rich metadata (see Table 1 for an example). We perform a thorough analysis to identify characteristics of the dataset such as entities mentioned in claims. We demonstrate the utility of the dataset by training state of the art veracity prediction models, and \ufb01nd that evidence pages as well as metadata signi\ufb01cantly contribute to model performance. Fi- nally, we propose a novel model that jointly ranks evidence pages and performs veracity prediction. The best-performing model achieves a Macro F1 of 49.2%, showing that this is a non-trivial dataset with remaining challenges for future work. 1The dataset is found here: https:\/\/copenlu.",
            "The best-performing model achieves a Macro F1 of 49.2%, showing that this is a non-trivial dataset with remaining challenges for future work. 1The dataset is found here: https:\/\/copenlu. github.io\/publication\/2019_emnlp_ augenstein\/ arXiv:1909.03242v2  [cs.CL]  21 Oct 2019",
            "2 Related Work 2.1 Datasets Over the past few years, a variety of mostly small datasets related to fact checking have been re- leased. An overview over core datasets is given in Table 2. The datasets can be grouped into four cat- egories (I\u2013IV). Category I contains datasets aimed at testing how well the veracity3 of a claim can be predicted using the claim alone, without context or evidence documents. Category II contains datasets bundled with documents related to each claim \u2013 ei- ther topically related to provide context, or serving as evidence. Those documents are, however, not annotated. Category III is for predicting veracity; they encourage retrieving evidence documents as part of their task description, but do not distribute them. Finally, category IV comprises datasets an- notated for both veracity and stance. Thus, ev- ery document is annotated with a label indicat- ing whether the document supports or denies the claim, or is unrelated to it. Additional labels can then be added to the datasets to better predict ve- racity, for instance by jointly training stance and veracity prediction models.",
            "Thus, ev- ery document is annotated with a label indicat- ing whether the document supports or denies the claim, or is unrelated to it. Additional labels can then be added to the datasets to better predict ve- racity, for instance by jointly training stance and veracity prediction models. Methods not shown in the table, but related to fact checking, are stance detection for claims (Ferreira and Vlachos, 2016; Pomerleau and Rao, 2017; Augenstein et al., 2016a; Kochkina et al., 2017; Augenstein et al., 2016b; Zubiaga et al., 2018; Riedel et al., 2017), satire detection (Ru- bin et al., 2016), clickbait detection (Karadzhov et al., 2017), conspiracy news detection (Tacchini et al., 2017), rumour cascade detection (Vosoughi et al., 2018) and claim perspectives detection (Chen et al., 2019).",
            "Claims are obtained from a variety of sources, including Wikipedia, Twitter, criminal reports and fact checking websites such as politifact.com and snopes.com. The same goes for documents \u2013 these are often websites obtained through Web search queries, or Wikipedia documents, tweets or Face- book posts. Most datasets contain a fairly small number of claims, and those that do not, often lack evidence documents. An exception is Thorne et al. (2018), who create a Wikipedia-based fact check- ing dataset. While a good testbed for develop- ing deep neural architectures, their dataset is arti- \ufb01cially constructed and can thus not take metadata 3We use veracity, claim credibility, and fake news predic- tion interchangeably here \u2013 these terms are often con\ufb02ated in the literature and meant to have the same meaning. about claims into account. Contributions: We provide a dataset that, uniquely among extant datasets, contains a large number of naturally occurring claims and rich ad- ditional meta-information. 2.2 Methods Fact checking methods partly depend on the type of dataset used.",
            "about claims into account. Contributions: We provide a dataset that, uniquely among extant datasets, contains a large number of naturally occurring claims and rich ad- ditional meta-information. 2.2 Methods Fact checking methods partly depend on the type of dataset used. Methods only taking into account claims typically encode those with CNNs or RNNs (Wang, 2017; P\u00b4erez-Rosas et al., 2018), and poten- tially encode metadata (Wang, 2017) in a similar way. Methods for small datasets often use hand- crafted features that are a mix of bag of word and other lexical features, e.g. LIWC, and then use those as input to a SVM or MLP (Mihalcea and Strapparava, 2009; P\u00b4erez-Rosas et al., 2018; Baly et al., 2018). Some use additional Twitter-speci\ufb01c features (Enayet and El-Beltagy, 2017).",
            "Some use additional Twitter-speci\ufb01c features (Enayet and El-Beltagy, 2017). More in- volved methods taking into account evidence doc- uments, often trained on larger datasets, consist of evidence identi\ufb01cation and ranking following a neural model that measures the compatibility be- tween claim and evidence (Thorne et al., 2018; Mihaylova et al., 2018; Yin and Roth, 2018). Contributions: The latter category above is the most related to our paper as we consider ev- idence documents. However, existing models are not trained jointly for evidence identi\ufb01cation, or for stance and veracity prediction, but rather em- ploy a pipeline approach. Here, we show that a joint approach that learns to weigh evidence pages by their importance for veracity prediction can improve downstream veracity prediction perfor- mance. 3 Dataset Construction We crawled a total of 43,837 claims with their metadata (see details in Table 11).",
            "Here, we show that a joint approach that learns to weigh evidence pages by their importance for veracity prediction can improve downstream veracity prediction perfor- mance. 3 Dataset Construction We crawled a total of 43,837 claims with their metadata (see details in Table 11). We present the data collection in terms of selecting sources, crawling claims and associated metadata (Section 3.1); retrieving evidence pages; and linking enti- ties in the crawled claims (Section 3.3). 3.1 Selection of sources We crawled all active fact checking websites in English listed by Duke Reporters\u2019 Lab4 and on the Fact Checking Wikipedia page.5 This resulted in 4https:\/\/reporterslab.org\/ fact-checking\/ 5https:\/\/en.wikipedia.org\/wiki\/Fact_ checking",
            "Dataset # Claims Labels metadata Claim Sources I: Veracity prediction w\/o evidence Wang (2017) 12,836 6 Yes Politifact P\u00b4erez-Rosas et al. (2018) 980 2 No News Websites II: Veracity Bachenko et al. (2008) 275 2 No Criminal Reports Mihalcea and Strapparava (2009) 600 2 No Crowd Authors Mitra and Gilbert (2015)\u2020 1,049 5 No Twitter Ciampaglia et al. (2015)\u2020 10,000 2 No Google, Wikipedia Popat et al. (2016) 5,013 2 Yes Wikipedia, Snopes Shu et al. (2018)\u2020 23,921 2 Yes Politifact, gossipcop.com Datacommons Fact Check2 10,564 2-6 Yes Fact Checking Websites III: Veracity (evidence encouraged, but not provided) Barrn-Cedeo et al.",
            "(2018)\u2020 23,921 2 Yes Politifact, gossipcop.com Datacommons Fact Check2 10,564 2-6 Yes Fact Checking Websites III: Veracity (evidence encouraged, but not provided) Barrn-Cedeo et al. (2018) 150 3 No factcheck.org, Snopes IV: Veracity + stance Vlachos and Riedel (2014) 106 5 Yes Politifact, Channel 4 News Zubiaga et al. (2016) 330 3 Yes Twitter Derczynski et al. (2017) 325 3 Yes Twitter Baly et al. (2018) 422 2 No ara.reuters.com, verify-sy.com Thorne et al. (2018)\u2020 185,445 3 No Wikipedia V: Veracity + evidence relevancy MultiFC 36,534 2-40 Yes Fact Checking Websites Table 2: Comparison of fact checking datasets. \u2020 indicates claims are not \u2018naturally occuring\u2019: Mitra and Gilbert (2015) use events as claims; Ciampaglia et al.",
            "\u2020 indicates claims are not \u2018naturally occuring\u2019: Mitra and Gilbert (2015) use events as claims; Ciampaglia et al. (2015) use DBPedia tiples as claims; Shu et al. (2018) use tweets as claims; and Thorne et al. (2018) rewrite sentences in Wikipedia as claims. 38 websites in total (shown in Table 11). Out of these, ten websites could not be crawled, as fur- ther detailed in Table 9. In the later experimen- tal descriptions, we refer to the part of the dataset crawled from a speci\ufb01c fact checking website as a domain, and we refer to each website as source. From each source, we crawled the ID, claim, label, URL, reason for label, categories, person making the claim (speaker), person fact checking the claim (checker), tags, article title, publication date, claim date, as well as the full text that ap- pears when the claim is clicked. Lastly, the above full text contains hyperlinks, so we further crawled the full text that appears when each of those hyper- links are clicked (outlinks).",
            "Lastly, the above full text contains hyperlinks, so we further crawled the full text that appears when each of those hyper- links are clicked (outlinks). There were a number of crawling issues, e.g. se- curity protection of websites with SSL\/TLS pro- tocols, time out, URLs that pointed to pdf \ufb01les instead of HTML content, or unresolvable encod- ing. In all of these cases, the content could not be retrieved. For some websites, no veracity labels were available, in which case, they were not se- lected as domains for training a veracity prediction model. Moreover, not all types of metadata (cat- egory, speaker, checker, tags, claim date, publish date) were available for all websites; and availabil- ity of articles and full texts differs as well. We performed semi-automatic cleansing of the dataset as follows. First, we double-checked that the veracity labels would not appear in claims. For some domains, the \ufb01rst or last sentence of the claim would sometimes contain the veracity label, in which case we would discard either the full sen- tence or part of the sentence.",
            "First, we double-checked that the veracity labels would not appear in claims. For some domains, the \ufb01rst or last sentence of the claim would sometimes contain the veracity label, in which case we would discard either the full sen- tence or part of the sentence. Next, we checked the dataset for duplicate claims. We found 202 such instances, 69 of them with different labels. Upon manual inspection, this was mainly due to them appearing on different websites, with labels not differing much in practice (e.g. \u2018Not true\u2019, vs. \u2018Mostly False\u2019). We made sure that all such du- plicate claims would be in the training split of the dataset, so that the models would not have an un- fair advantage. Finally, we performed some minor manual merging of label types for the same do- main where it was clear that they were supposed to denote the same level of veracity (e.g. \u2018distorts\u2019, \u2018distorts the facts\u2019). This resulted in a total of 36,534 claims with their metadata.",
            "\u2018distorts\u2019, \u2018distorts the facts\u2019). This resulted in a total of 36,534 claims with their metadata. For the purposes of fact veri\ufb01ca- tion, we discarded instances with labels that occur fewer than 5 times, resulting in 34,918 claims. The number of instances, as well as labels per domain, are shown in Table 6 and label names in Table 10 in the appendix. The dataset is split into a train- ing part (80%) and a development and testing part (10% each) in a label-strati\ufb01ed manner. Note that",
            "the domains vary in the number of labels, ranging from 2 to 27. Labels include both straight-forward ratings of veracity (\u2018correct\u2019, \u2018incorrect\u2019), but also labels that would be more dif\ufb01cult to map onto a veracity scale (e.g. \u2018grass roots movement!\u2019, \u2018mis- attributed\u2019, \u2018not the whole story\u2019). We therefore do not postprocess label types across domains to map them onto the same scale, and rather treat them as is. In the methodology section (Section 4), we show how a model can be trained on this dataset regardless by framing this multi-domain veracity prediction task as a multi-task learning (MTL) one. 3.2 Retrieving Evidence Pages The text of each claim is submitted verbatim as a query to the Google Search API (without quotes). The 10 most highly ranked search results are re- trieved, for each of which we save the title; Google search rank; URL; time stamp of last update; search snippet; as well as the full Web page. We acknowledge that search results change over time, which might have an effect on veracity prediction. However, studying such temporal effects is outside the scope of this paper.",
            "We acknowledge that search results change over time, which might have an effect on veracity prediction. However, studying such temporal effects is outside the scope of this paper. Similar to Web crawl- ing claims, as described in Section 3.1, the cor- responding Web pages can in some cases not be retrieved, in which case fewer than 10 evidence pages are available. The resulting evidence pages are from a wide variety of URL domains, though with a predictable skew towards popular websites, such as Wikipedia or The Guardian (see Table 3 for detailed statistics). 3.3 Entity Detection and Linking To better understand what claims are about, we conduct entity linking for all claims. Speci\ufb01cally, mentions of people, places, organisations, and other named entities within a claim are recognised and linked to their respective Wikipedia pages, if available. Where there are different entities with the same name, they are disambiguated. For this, we apply the state-of-the-art neural entity linking model by Kolitsas et al. (2018).",
            "Where there are different entities with the same name, they are disambiguated. For this, we apply the state-of-the-art neural entity linking model by Kolitsas et al. (2018). This results in a total of 25,763 entities detected and linked to Wikipedia, with a total of 15,351 claims involved, meaning that 42% of all claims contain entities that can be linked to Wikipedia. Later on, we use entities as additional metadata (see Section 4.3). The distribution of claim numbers according to the number of entities they contain is shown in Figure 1. We observe that the majority of claims have Domain % https:\/\/en.wikipedia.org\/ 4.425 https:\/\/www.snopes.com\/ 3.992 https:\/\/www.washingtonpost.com\/ 3.025 https:\/\/www.nytimes.com\/ 2.478 https:\/\/www.theguardian.com\/ 1.807 https:\/\/www.youtube.com\/ 1.712 https:\/\/www.dailymail.co.uk\/ 1.558 https:\/\/www.usatoday.com\/ 1.279 https:\/\/www.politico.",
            "478 https:\/\/www.theguardian.com\/ 1.807 https:\/\/www.youtube.com\/ 1.712 https:\/\/www.dailymail.co.uk\/ 1.558 https:\/\/www.usatoday.com\/ 1.279 https:\/\/www.politico.com\/ 1.241 http:\/\/www.politifact.com\/ 1.231 https:\/\/www.pinterest.com\/ 1.169 https:\/\/www.factcheck.org\/ 1.09 https:\/\/www.gossipcop.com\/ 1.073 https:\/\/www.cnn.com\/ 1.065 https:\/\/www.npr.org\/ 0.957 https:\/\/www.forbes.com\/ 0.911 https:\/\/www.vox.com\/ 0.89 https:\/\/www.theatlantic.com\/ 0.88 https:\/\/twitter.com\/ 0.767 https:\/\/www.hoax-slayer.net\/ 0.655 http:\/\/time.com\/ 0.554 https:\/\/www.bbc.com\/ 0.551 https:\/\/www.nbcnews.com\/ 0.515 https:\/\/www.cnbc.",
            "com\/ 0.767 https:\/\/www.hoax-slayer.net\/ 0.655 http:\/\/time.com\/ 0.554 https:\/\/www.bbc.com\/ 0.551 https:\/\/www.nbcnews.com\/ 0.515 https:\/\/www.cnbc.com\/ 0.514 https:\/\/www.cbsnews.com\/ 0.503 https:\/\/www.facebook.com\/ 0.5 https:\/\/www.newyorker.com\/ 0.495 https:\/\/www.foxnews.com\/ 0.468 https:\/\/people.com\/ 0.439 http:\/\/www.cnn.com\/ 0.419 Table 3: The top 30 most frequently occurring URL domains. Figure 1: Distribution of entities in claims. one to four entities, and the maximum number of 35 entities occurs in one claim only. Out of the 25,763 entities, 2,767 are unique entities. The top 30 most frequent entities are listed in Table 4.",
            "Figure 1: Distribution of entities in claims. one to four entities, and the maximum number of 35 entities occurs in one claim only. Out of the 25,763 entities, 2,767 are unique entities. The top 30 most frequent entities are listed in Table 4. This clearly shows that most of the claims involve enti- ties related to the United States, which is to be ex- pected, as most of the fact checking websites are US-based. 4 Claim Veracity Prediction We train several models to predict the veracity of claims. Those fall into two categories: those that",
            "Entity Frequency United States 2810 Barack Obama 1598 Republican Party (United States) 783 Texas 665 Democratic Party (United States) 560 Donald Trump 556 Wisconsin 471 United States Congress 354 Hillary Rodham Clinton 306 Bill Clinton 292 California 285 Russia 275 Ohio 239 China 229 George W. Bush 208 Medicare (United States) 206 Australia 186 Iran 183 Brad Pitt 180 Islam 178 Iraq 176 Canada 174 White House 166 New York City 164 Washington, D.C. 164 Jennifer Aniston 163 Mexico 158 Ted Cruz 152 Federal Bureau of Investigation 146 Syria 130 Table 4: Top 30 most frequent entities listed by their Wikipedia URL with pre\ufb01x omitted only consider the claims themselves, and those that encode evidence pages as well. In addition, claim metadata (speaker, checker, linked entities) is optionally encoded for both categories of mod- els, and ablation studies with and without that metadata are shown.",
            "In addition, claim metadata (speaker, checker, linked entities) is optionally encoded for both categories of mod- els, and ablation studies with and without that metadata are shown. We \ufb01rst describe the base model used in Section 4.1, followed by introduc- ing our novel evidence ranking and veracity pre- diction model in Section 4.2, and lastly the meta- data encoding model in Section 4.3. 4.1 Multi-Domain Claim Veracity Prediction with Disparate Label Spaces Since not all fact checking websites use the same claim labels (see Table 6, and Table 10 in the ap- pendix), training a claim veracity prediction model is not entirely straight-forward. One option would be to manually map those labels onto one another. However, since the sheer number of labels is rather large (165), and it is not always clear from the guidelines on fact checking websites how they can be mapped onto one another, we opt to learn how these labels relate to one another as part of the veracity prediction model. To do so, we employ the multi-task learning (MTL) approach inspired by collaborative \ufb01ltering presented in Augenstein et al.",
            "To do so, we employ the multi-task learning (MTL) approach inspired by collaborative \ufb01ltering presented in Augenstein et al. (2018) (MTL with LEL\u2013multitask learning with label embedding layer) that excels on pair- wise sequence classi\ufb01cation tasks with disparate label spaces. More concretely, each domain is modelled as its own task in a MTL architecture, and labels are projected into a \ufb01xed-length label embedding space. Predictions are then made by taking the dot product between the claim-evidence embeddings and the label embeddings. By doing so, the model implicitly learns how semantically close the labels are to one another, and can bene\ufb01t from this knowledge when making predictions for individual tasks, which on their own might only have a small number of instances. When making predictions for individual domains\/tasks, both at training and at test time, as well as when calculat- ing the loss, a mask is applied such that the valid and invalid labels for that task are restricted to the set of known task labels. Note that the setting here slightly differs from Augenstein et al. (2018).",
            "Note that the setting here slightly differs from Augenstein et al. (2018). There, tasks are less strongly related to one another; for example, they consider stance detection, aspect-based sentiment analysis and natural language inference. Here, we have different domains, as opposed to conceptu- ally different tasks, but use their framework, as we have the same underlying problem of disparate la- bel spaces. A more formal problem de\ufb01nition fol- lows next, as our evidence ranking and veracity prediction model in Section 4.2 then builds on it. 4.1.1 Problem De\ufb01nition We frame our problem as a multi-task learning one, where access to labelled datasets for T tasks T1, . . . , TT is given at training time with a target task TT that is of particular interest. The train- ing dataset for task Ti consists of N examples XTi = {xTi 1 , . . . , xTi N} and their labels YTi = {yTi 1 , . . . , yTi N }.",
            "The train- ing dataset for task Ti consists of N examples XTi = {xTi 1 , . . . , xTi N} and their labels YTi = {yTi 1 , . . . , yTi N }. The base model is a classic deep neural network MTL model (Caruana, 1993) that shares its parameters across tasks and has task- speci\ufb01c softmax output layers that output a proba- bility distribution pTi for task Ti: pTi = softmax(WTih + bTi) (1) where softmax(x) = ex\/ P\u2225x\u2225 i=1 exi, WTi \u2208 RLi\u00d7h, bTi \u2208RLi is the weight matrix and bias term of the output layer of task Ti respec- tively, h \u2208Rh is the jointly learned hidden rep-",
            "resentation, Li is the number of labels for task Ti, and h is the dimensionality of h. The MTL model is trained to minimise the sum of individual task losses L1 + . . . + LT using a negative log- likelihood objective. Label Embedding Layer. To learn the relation- ships between labels, a Label Embedding Layer (LEL) embeds labels of all tasks in a joint Eu- clidian space. Instead of training separate softmax output layers as above, a label compatibility func- tion c(\u00b7, \u00b7) measures how similar a label with em- bedding l is to the hidden representation h: c(l, h) = l \u00b7 h (2) where \u00b7 is the dot product. Padding is applied such that l and h have the same dimensionality. Ma- trix multiplication and softmax are used for mak- ing predictions: p = softmax(Lh) (3) where L \u2208R(P i Li)\u00d7l is the label embedding ma- trix for all tasks and l is the dimensionality of the label embeddings.",
            "Ma- trix multiplication and softmax are used for mak- ing predictions: p = softmax(Lh) (3) where L \u2208R(P i Li)\u00d7l is the label embedding ma- trix for all tasks and l is the dimensionality of the label embeddings. We apply a task-speci\ufb01c mask to L in order to obtain a task-speci\ufb01c probabil- ity distribution pTi. The LEL is shared across all tasks, which allows the model to learn the relation- ships between labels in the joint embedding space. 4.2 Joint Evidence Ranking and Claim Veracity Prediction So far, we have ignored the issue of how to obtain claim representation, as the base model described in the previous section is agnostic to how instances are encoded. A very simple approach, which we report as a baseline, is to encode claim texts only. Such a model ignores evidence for and against a claim, and ends up guessing the veracity based on surface patterns observed in the claim texts. We next introduce two variants of evidence- based veracity prediction models that encode 10 pieces of evidence in addition to the claim. Here, we opt to encode search snippets as opposed to whole retrieved pages.",
            "We next introduce two variants of evidence- based veracity prediction models that encode 10 pieces of evidence in addition to the claim. Here, we opt to encode search snippets as opposed to whole retrieved pages. While the latter would also be possible, it comes with a number of ad- ditional challenges, such as encoding large doc- uments, parsing tables or PDF \ufb01les, and encod- ing images or videos on these pages, which we leave to future work. Search snippets also have the bene\ufb01t that they already contain summaries of the part of the page content that is most related to the claim. Figure 2: The Joint Veracity Prediction and Evidence Ranking model, shown for one task. 4.2.1 Problem De\ufb01nition Our problem is to obtain encodings for N exam- ples XTi = {xTi 1 , . . . , xTi N}. For simplicity, we will henceforth drop the task superscript and re- fer to instances as X = {x1, . . . , xN}, as instance encodings are learned in a task-agnostic fashion.",
            ". . , xTi N}. For simplicity, we will henceforth drop the task superscript and re- fer to instances as X = {x1, . . . , xN}, as instance encodings are learned in a task-agnostic fashion. Each example further consists of a claim ai and k = 10 evidence pages Ek = {e10, . . . , eN10}. Each claim and evidence page is encoded with a BiLSTM to obtain a sentence embedding, which is the concatenation of the last state of the forward and backward reading of the sentence, i.e. h = BiLSTM(\u00b7), where h is the sentence embedding. Next, we want to combine claims and evidence sentence embeddings into joint instance represen- tations. In the simplest case, referred to as model variant crawled avg, we mean average the BiL- STM sentence embeddings of all evidence pages (signi\ufb01ed by the overline) and concatenate those with the claim embeddings, i.e. sgi = [hai; hEi] (4) where sgi is the resulting encoding for training example i and [\u00b7; \u00b7] denotes vector concatenation.",
            "However, this has the disadvantage that all evi- dence pages are considered equal. Evidence Ranking The here proposed alterna- tive instance encoding model, crawled ranked, which achieves the highest overall performance as discussed in Section 5, learns the compatibility be- tween an instance\u2019s claim and each evidence page. It ranks evidence pages by their utility for the ve- racity prediction task, and then uses the resulting ranking to obtain a weighted combination of all claim-evidence pairs. No direct labels are avail- able to learn the ranking of individual documents, only for the veracity of the associated claim, so the model has to learn evidence ranks implicitly. To combine claim and evidence representations, we use the matching model proposed for the task of natural language inference by Mou et al. (2016) and adapt it to combine an instance\u2019s claim repre- sentation with each evidence representation, i.e. srij = [hai; heij ; hai \u2212heij ; hai \u00b7 heij ] (5) where srij is the resulting encoding for training example i and evidence page j , [\u00b7; \u00b7] denotes vec- tor concatenation, and \u00b7 denotes the dot product.",
            "srij = [hai; heij ; hai \u2212heij ; hai \u00b7 heij ] (5) where srij is the resulting encoding for training example i and evidence page j , [\u00b7; \u00b7] denotes vec- tor concatenation, and \u00b7 denotes the dot product. All joint claim-evidence representations sri0, . . . , sri10 are then projected into the binary space via a fully connected layer FC, followed by a non-linear activation function f, to obtain a soft ranking of claim-evidence pairs, in practice a 10-dimensional vector, oi = [f(FC(sri0)); . . . ; f(FC(sri10))] (6) where [\u00b7; \u00b7] denotes concatenation. Scores for all labels are obtained as per (6) above, with the same input instance embeddings as for the evidence ranker, i.e. srij . Final predic- tions for all claim-evidence pairs are then obtained by taking the dot product between the label scores and binary evidence ranking scores, i.e.",
            "srij . Final predic- tions for all claim-evidence pairs are then obtained by taking the dot product between the label scores and binary evidence ranking scores, i.e. pi = softmax(c(l, sri) \u00b7 oi) (7) Note that the novelty here is that, unlike for the model described in Mou et al. (2016), we have no direct labels for learning weights for this matching model. Rather, our model has to implicitly learn these weights for each claim-evidence pair in an end-to-end fashion given the veracity labels. Model Micro F1 Macro F1 claim-only 0.469 0.253 claim-only embavg 0.384 0.302 crawled-docavg 0.438 0.248 crawled ranked 0.613 0.441 claim-only + meta 0.494 0.324 claim-only embavg + meta 0.418 0.333 crawled-docavg + meta 0.483 0.286 crawled ranked + meta 0.625 0.492 Table 5: Results with different model variants on the test set, \u2018meta\u2019 means all metadata is used.",
            "4.3 Metadata We experiment with how useful claim metadata is, and encode the following as one-hot vectors: speaker, category, tags and linked entities. We do not encode \u2018Reason\u2019 as it gives away the label, and do not include \u2018Checker\u2019 as there are too many unique checkers for this information to be rele- vant. The claim publication date is potentially rel- evant, but it does not make sense to merely model this as a one-hot feature, so we leave incorporat- ing temporal information to future work. Since all metadata consists of individual words and phrases, a sequence encoder is not necessary, and we opt for a CNN followed by a max pooling operation as used in Wang (2017) to encode metadata for fact checking. The max-pooled metadata representa- tions, denoted hm, are then concatenated with the instance representations, e.g. for the most elabo- rate model, crawled ranked, these would be con- catenated with scrij . 5 Experiments 5.1 Experimental Setup The base sentence embedding model is a BiLSTM over all words in the respective sequences with randomly initialised word embeddings, following Augenstein et al. (2018).",
            "5 Experiments 5.1 Experimental Setup The base sentence embedding model is a BiLSTM over all words in the respective sequences with randomly initialised word embeddings, following Augenstein et al. (2018). We opt for this strong baseline sentence encoding model, as opposed to engineering sentence embeddings that work par- ticularly well for this dataset, to showcase the dataset. We would expect pre-trained contextual encoding models, e.g. ELMO (Peters et al., 2018), ULMFit (Howard and Ruder, 2018), BERT (De- vlin et al., 2018), to offer complementary perfor- mance gains, as has been shown for a few recent papers (Wang et al., 2018a; Rajpurkar et al., 2018). For claim veracity prediction without evidence documents with the MTL with LEL model, we use the following sentence encoding variants: claim-",
            "only, which uses a BiLSTM-based sentence em- bedding as input, and claim-only embavg, which uses a sentence embedding based on mean aver- aged word embeddings as input. We train one multi-task model per task (i.e., one model per domain). We perform a grid search over the following hyperparameters, tuned on the re- spective dev set, and evaluate on the correspod- ing test set (\ufb01nal settings are underlined): word embedding size [64, 128, 256], BiLSTM hidden layer size [64, 128, 256], number of BiLSTM hid- den layers [1, 2, 3], BiLSTM dropout on input and output layers [0.0, 0.1, 0.2, 0.5], word-by-word- attention for BiLSTM with window size 10 (Bah- danau et al., 2014) [True, False], skip-connections for the BiLSTM [True, False], batch size [32, 64, 128], label embedding size [16, 32, 64].",
            "We use ReLU as an activation function for both the BiL- STM and the CNN. For the CNN, the follow- ing hyperparameters are used: number \ufb01lters [32], kernel size [32]. We train using cross-entropy loss and the RMSProp optimiser with initial learning rate of 0.001 and perform early stopping on the dev set with a patience of 3. 5.2 Results For each domain, we compute the Micro as well as Macro F1, then mean average results over all domains. Core results with all vs. no metadata are shown in Table 5. We \ufb01rst experiment with different base model variants and \ufb01nd that label embeddings improve results, and that the best pro- posed models utilising multiple domains outper- form single-task models (see Table 8). This cor- roborates the \ufb01ndings of Augenstein et al. (2018). Per-domain results with the best model are shown in Table 6. Domain names are from hereon af- ter abbreviated for brevity, see Table 11 in the ap- pendix for correspondences to full website names.",
            "(2018). Per-domain results with the best model are shown in Table 6. Domain names are from hereon af- ter abbreviated for brevity, see Table 11 in the ap- pendix for correspondences to full website names. Unsurprisingly, it is hard to achieve a high Macro F1 for domains with many labels, e.g. tron and snes. Further, some domains, surprisingly mostly with small numbers of instances, seem to be very easy \u2013 a perfect Micro and Macro F1 score of 1.0 is achieved on ranz, bove, buca, fani and thal. We \ufb01nd that for those domains, the verdict is often al- ready revealed as part of the claim using explicit wording. Claim-Only vs. Evidence-Based Veracity Pre- diction. Our evidence-based claim veracity pre- diction models outperform claim-only veracity Domain # Insts # Labs Micro F1 Macro F1 ranz 21 2 1.000 1.000 bove 295 2 1.000 1.000 abbc 436 3 0.463 0.",
            "000 1.000 bove 295 2 1.000 1.000 abbc 436 3 0.463 0.453 huca 34 3 1.000 1.000 mpws 47 3 0.667 0.583 peck 65 3 0.667 0.472 faan 111 3 0.682 0.679 clck 38 3 0.833 0.619 fani 20 3 1.000 1.000 chct 355 4 0.550 0.513 obry 59 4 0.417 0.268 vees 504 4 0.721 0.425 faly 111 5 0.278 0.5 goop 2943 6 0.822 0.387 pose 1361 6 0.438 0.328 thet 79 6 0.55 0.37 thal 163 7 1.000 1.000 afck 433 7 0.357 0.259 hoer 1310 7 0.",
            "438 0.328 thet 79 6 0.55 0.37 thal 163 7 1.000 1.000 afck 433 7 0.357 0.259 hoer 1310 7 0.694 0.549 para 222 7 0.375 0.311 wast 201 7 0.344 0.214 vogo 654 8 0.594 0.297 pomt 15390 9 0.321 0.276 snes 6455 12 0.551 0.097 farg 485 11 0.500 0.140 tron 3423 27 0.429 0.046 avg 7.17 0.625 0.492 Table 6: Total number of instances and unique labels per domain, as well as per-domain results with model crawled ranked + meta, sorted by label size Metadata Micro F1 Macro F1 None 0.627 0.441 Speaker 0.602 0.435 + Tags 0.608 0.460 Tags 0.585 0.461 Entity 0.",
            "sorted by label size Metadata Micro F1 Macro F1 None 0.627 0.441 Speaker 0.602 0.435 + Tags 0.608 0.460 Tags 0.585 0.461 Entity 0.569 0.427 + Speaker 0.607 0.477 + Tags 0.625 0.492 Table 7: Ablation results with base model crawled ranked for different types of metadata Model Micro F1 Macro F1 STL 0.527 0.388 MTL 0.556 0.448 MTL + LEL 0.625 0.492 Table 8: Ablation results with crawled ranked + meta encoding for STL vs. MTL vs. MTL + LEL training prediction models by a large margin. Unsur- prisingly, claim-only embavg is outperformed by claim-only. Further, crawled ranked is our best- performing model in terms of Micro F1 and Macro F1, meaning that our model captures that not ev- ery piece of evidence is equally important, and can",
            "Figure 3: Confusion matrix of predicted labels with best-performing model, crawled ranked + meta, on the \u2018pomt\u2019 domain utilise this for veracity prediction. Metadata. We perform an ablation analysis of how metadata impacts results, shown in Table 7. Out of the different types of metadata, topic tags on their own contribute the most. This is likely be- cause they offer highly complementary informa- tion to the claim text of evidence pages. Only us- ing all metadata together achieves a higher Macro F1 at similar Micro F1 than using no metadata at all. To further investigate this, we split the test set into those instances for which no metadata is available vs. those for which metadata is available. We \ufb01nd that encoding metadata within the model hurts performance for domains where no metadata is available, but improves performance where it is. In practice, an ensemble of both types of models would be sensible, as well as exploring more in- volved methods of encoding metadata. 6 Analysis and Discussion An analysis of labels frequently confused with one another, for the largest domain \u2018pomt\u2019 and best-performing model crawled ranked + meta is shown in Figure 3.",
            "6 Analysis and Discussion An analysis of labels frequently confused with one another, for the largest domain \u2018pomt\u2019 and best-performing model crawled ranked + meta is shown in Figure 3. The diagonal represents when gold and predicted labels match, and the num- bers signify the number of test instances. One can observe that the model struggles more to de- tect claims with labels \u2018true\u2019 than those with la- bel \u2018false\u2019. Generally, many confusions occur over close labels, e.g. \u2018half-true\u2019 vs. \u2018mostly true\u2019. We further analyse what properties instances that are predicted correctly vs. incorrectly have, using the model crawled ranked meta. We \ufb01nd that, unsurprisingly, longer claims are harder to classify correctly, and that claims with a high di- rect token overlap with evidence pages lead to a high evidence ranking.",
            "We further analyse what properties instances that are predicted correctly vs. incorrectly have, using the model crawled ranked meta. We \ufb01nd that, unsurprisingly, longer claims are harder to classify correctly, and that claims with a high di- rect token overlap with evidence pages lead to a high evidence ranking. When it comes to fre- quently occurring tags and entities, very general tags such as \u2018government-and-politics\u2019 or \u2018tax\u2019 that do not give away much, frequently co-occur with incorrect predictions, whereas more speci\ufb01c tags such as \u2018brisbane-4000\u2019 or \u2018hong-kong\u2019 tend to co-occur with correct predictions. Similar trends are observed for bigrams. This means that the model has an easy time succeeding for in- stances where the claims are short, where speci\ufb01c topics tend to co-occur with certain veracities, and where evidence documents are highly informative. Instances with longer, more complex claims where evidence is ambiguous remain challenging. 7 Conclusions We present a new, real-world fact checking dataset, currently the largest of its kind.",
            "Instances with longer, more complex claims where evidence is ambiguous remain challenging. 7 Conclusions We present a new, real-world fact checking dataset, currently the largest of its kind. It consists of 34,918 claims collected from 26 fact checking websites, rich metadata and 10 retrieved evidence pages per claim. We \ufb01nd that encoding the meta- data as well evidence pages helps, and introduce a new joint model for ranking evidence pages and predicting veracity. Acknowledgments This research is partially supported by QUARTZ (721321, EU H2020 MSCA-ITN) and DABAI (5153-00004A, Innovation Fund Denmark). References Isabelle Augenstein, Tim Rockt\u00a8aschel, Andreas Vla- chos, and Kalina Bontcheva. 2016a. Stance Detec- tion with Bidirectional Conditional Encoding. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 876\u2013885, Austin, Texas. Association for Computa- tional Linguistics. Isabelle Augenstein, Sebastian Ruder, and Anders S\u00f8gaard. 2018.",
            "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 876\u2013885, Austin, Texas. Association for Computa- tional Linguistics. Isabelle Augenstein, Sebastian Ruder, and Anders S\u00f8gaard. 2018. Multi-Task Learning of Pairwise Sequence Classi\ufb01cation Tasks over Disparate Label Spaces. In NAACL-HLT, pages 1896\u20131906. Associ- ation for Computational Linguistics. Isabelle Augenstein, Andreas Vlachos, and Kalina Bontcheva. 2016b. USFD at SemEval-2016 Task 6: Any-Target Stance Detection on Twitter with Au- toencoders. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 389\u2013393, San Diego, California. Association for Computational Linguistics.",
            "Joan Bachenko, Eileen Fitzpatrick, and Michael Schonwetter. 2008. Veri\ufb01cation and implementa- tion of language-based deception indicators in civil and criminal narratives. In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 41\u201348. Association for Computational Linguistics. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2014. Neural Machine Translation by Jointly Learning to Align and Translate. In Proceedings of ICLR. Ramy Baly, Mitra Mohtarami, James R. Glass, Llu\u00b4\u0131s M`arquez, Alessandro Moschitti, and Preslav Nakov. 2018. Integrating Stance Detection and Fact Check- ing in a Uni\ufb01ed Corpus. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, NAACL-HLT, New Or- leans, Louisiana, USA, June 1-6, 2018, Volume 2 (Short Papers), pages 21\u201327.",
            "Association for Com- putational Linguistics. Alberto Barrn-Cedeo, Tamer Elsayed, Reem Suwaileh, Llus Mrquez, Pepa Atanasova, Wajdi Zaghouani, Spas Kyuchukov, Giovanni Da San Martino, and Preslav Nakov. 2018. Overview of the CLEF-2018 CheckThat! Lab on automatic identi\ufb01cation and veri\ufb01cation of political claims. Task 2: Factuality. In CLEF (Working Notes), volume 2125 of CEUR Workshop Proceedings. CEUR-WS.org. Rich Caruana. 1993. Multitask Learning: A Knowledge-Based Source of Inductive Bias. In Pro- ceedings of ICML. Sihao Chen, Daniel Khashabi, Wenpeng Yin, Chris Callison-Burch, and Dan Roth. 2019. Seeing Things from a Different Angle: Discovering Diverse Per- spectives about Claims. In Proceedings of NAACL.",
            "Sihao Chen, Daniel Khashabi, Wenpeng Yin, Chris Callison-Burch, and Dan Roth. 2019. Seeing Things from a Different Angle: Discovering Diverse Per- spectives about Claims. In Proceedings of NAACL. G L Ciampaglia, P Shiralkar, L M Rocha, J Bollen, F Menczer, and A Flammini. 2015. Computational Fact Checking from Knowledge Networks. PLoS One, 10(6). Leon Derczynski, Kalina Bontcheva, Maria Liakata, Rob Procter, Geraldine Wong Sak Hoi, and Arkaitz Zubiaga. 2017. SemEval-2017 Task 8: Ru- mourEval: Determining rumour veracity and sup- port for rumours. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 69\u201376. Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Un- derstanding.",
            "Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Un- derstanding. CoRR, abs\/1810.04805. Omar Enayet and Samhaa R. El-Beltagy. 2017. NileTMRG at SemEval-2017 Task 8: Determin- ing Rumour and Veracity Support for Rumours on Twitter. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 470\u2013474. Association for Computational Lin- guistics. William Ferreira and Andreas Vlachos. 2016. Emer- gent: a novel data-set for stance classi\ufb01cation. In HLT-NAACL, pages 1163\u20131168. The Association for Computational Linguistics. Andreas Hanselowski, PVS Avinesh, Benjamin Schiller, and Felix Caspelherr. 2017. Team Athene on the Fake News Challenge. Jeremy Howard and Sebastian Ruder. 2018.",
            "The Association for Computational Linguistics. Andreas Hanselowski, PVS Avinesh, Benjamin Schiller, and Felix Caspelherr. 2017. Team Athene on the Fake News Challenge. Jeremy Howard and Sebastian Ruder. 2018. Universal Language Model Fine-tuning for Text Classi\ufb01cation. In ACL (1), pages 328\u2013339. Association for Compu- tational Linguistics. Georgi Karadzhov, Pepa Gencheva, Preslav Nakov, and Ivan Koychev. 2017. We Built a Fake News \/ Click Bait Filter: What Happened Next Will Blow Your Mind! In RANLP 2017, pages 334\u2013343. Hamid Karimi, Proteek Roy, Sari Saba-Sadiya, and Jil- iang Tang. 2018. Multi-Source Multi-Class Fake News Detection. In Proceedings of the 27th Inter- national Conference on Computational Linguistics, pages 1546\u20131557. Elena Kochkina, Maria Liakata, and Isabelle Augen- stein. 2017.",
            "2018. Multi-Source Multi-Class Fake News Detection. In Proceedings of the 27th Inter- national Conference on Computational Linguistics, pages 1546\u20131557. Elena Kochkina, Maria Liakata, and Isabelle Augen- stein. 2017. Turing at SemEval-2017 Task 8: Se- quential Approach to Rumour Stance Classi\ufb01ca- tion with Branch-LSTM. In Proceedings of the 11th International Workshop on Semantic Evalua- tion (SemEval-2017), pages 475\u2013480, Vancouver, Canada. Association for Computational Linguistics. Nikolaos Kolitsas, Octavian-Eugen Ganea, and Thomas Hofmann. 2018. End-to-End Neural Entity Linking. In Proceedings of the 22nd Conference on Computational Natural Language Learning, pages 519\u2013529. Association for Computational Linguistics. Rada Mihalcea and Carlo Strapparava. 2009. The lie detector: Explorations in the automatic recognition of deceptive language.",
            "In Proceedings of the 22nd Conference on Computational Natural Language Learning, pages 519\u2013529. Association for Computational Linguistics. Rada Mihalcea and Carlo Strapparava. 2009. The lie detector: Explorations in the automatic recognition of deceptive language. In Proceedings of the ACL- IJCNLP 2009 Conference Short Papers, pages 309\u2013 312. Association for Computational Linguistics. Tsvetomila Mihaylova, Preslav Nakov, Llu\u00b4\u0131s M`arquez, Alberto Barr\u00b4on-Cede\u02dcno, Mitra Mohtarami, Georgi Karadzhov, and James R. Glass. 2018. Fact Check- ing in Community Forums. In Proceedings of the Thirty-Second AAAI Conference on Arti\ufb01cial Intelli- gence, New Orleans, Louisiana, USA, February 2-7, 2018. AAAI Press. Tanushree Mitra and Eric Gilbert. 2015. Credbank: A large-scale social media corpus with associated credibility annotations. In ICWSM, pages 258\u2013267.",
            "AAAI Press. Tanushree Mitra and Eric Gilbert. 2015. Credbank: A large-scale social media corpus with associated credibility annotations. In ICWSM, pages 258\u2013267. Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui Yan, and Zhi Jin. 2016. Natural Language Inference by Tree-Based Convolution and Heuristic Matching. In ACL (2). The Association for Computer Linguistics.",
            "Ver\u00b4onica P\u00b4erez-Rosas, Bennett Kleinberg, Alexandra Lefevre, and Rada Mihalcea. 2018. Automatic De- tection of Fake News. In Proceedings of the 27th In- ternational Conference on Computational Linguis- tics, pages 3391\u20133401. Association for Computa- tional Linguistics. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized Word Rep- resentations. In NAACL-HLT, pages 2227\u20132237. Association for Computational Linguistics. Dean Pomerleau and Delip Rao. 2017. The Fake News Challenge: Exploring how arti\ufb01cial intelligence technologies could be leveraged to combat fake news. http:\/\/www.fakenewschallenge. org\/. Accessed: 2019-02-14. Kashyap Popat, Subhabrata Mukherjee, Jannik Str\u00a8otgen, and Gerhard Weikum. 2016.",
            "http:\/\/www.fakenewschallenge. org\/. Accessed: 2019-02-14. Kashyap Popat, Subhabrata Mukherjee, Jannik Str\u00a8otgen, and Gerhard Weikum. 2016. Credibility Assessment of Textual Claims on the Web. In CIKM, pages 2173\u20132178. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Don\u2019t Know: Unanswerable Ques- tions for SQuAD. In ACL (2), pages 784\u2013789. As- sociation for Computational Linguistics. Benjamin Riedel, Isabelle Augenstein, Georgios P. Sp- ithourakis, and Sebastian Riedel. 2017. A simple but tough-to-beat baseline for the Fake News Challenge stance detection task. CoRR, abs\/1707.03264. Victoria Rubin, Niall Conroy, Yimin Chen, and Sarah Cornwell. 2016. Fake News or Truth?",
            "2017. A simple but tough-to-beat baseline for the Fake News Challenge stance detection task. CoRR, abs\/1707.03264. Victoria Rubin, Niall Conroy, Yimin Chen, and Sarah Cornwell. 2016. Fake News or Truth? Using Satiri- cal Cues to Detect Potentially Misleading News. In Proceedings of the Second Workshop on Computa- tional Approaches to Deception Detection, pages 7\u2013 17. Association for Computational Linguistics. Giovanni C Santia and Jake Ryland Williams. 2018. BuzzFace: A News Veracity Dataset with Facebook User Commentary and Egos. ICWSM, 531:540. K. Shu, D. Mahudeswaran, S. Wang, D. Lee, and H. Liu. 2018. FakeNewsNet: A Data Repository with News Content, Social Context and Dynamic In- formation for Studying Fake News on Social Media. ArXiv e-prints. Eugenio Tacchini, Gabriele Ballarin, Marco L. Della Vedova, Stefano Moret, and Luca de Alfaro.",
            "ArXiv e-prints. Eugenio Tacchini, Gabriele Ballarin, Marco L. Della Vedova, Stefano Moret, and Luca de Alfaro. 2017. Some Like it Hoax: Automated Fake News Detec- tion in Social Networks. In Proceedings of the Sec- ond Workshop on Data Science for Social Good (So- Good), volume 1960 of CEUR Workshop Proceed- ings. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a Large-scale Dataset for Fact Extraction and VERi\ufb01cation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809\u2013819, New Orleans, Louisiana. Association for Computational Linguistics. Andreas Vlachos and Sebastian Riedel. 2014. Fact Checking: Task de\ufb01nition and dataset construction. In Proceedings of the ACL 2014 Workshop on Lan- guage Technologies and Computational Social Sci- ence, pages 18\u201322.",
            "Andreas Vlachos and Sebastian Riedel. 2014. Fact Checking: Task de\ufb01nition and dataset construction. In Proceedings of the ACL 2014 Workshop on Lan- guage Technologies and Computational Social Sci- ence, pages 18\u201322. Association for Computational Linguistics. Soroush Vosoughi, Deb Roy, and Sinan Aral. 2018. The spread of true and false news online. Science, 359(6380):1146\u20131151. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2018a. GLUE: A Multi-Task Benchmark and Analysis Plat- form for Natural Language Understanding. In BlackboxNLP@EMNLP, pages 353\u2013355. Associa- tion for Computational Linguistics. Dongsheng Wang, Jakob Grue Simonsen, Birger Larsen, and Christina Lioma. 2018b.",
            "In BlackboxNLP@EMNLP, pages 353\u2013355. Associa- tion for Computational Linguistics. Dongsheng Wang, Jakob Grue Simonsen, Birger Larsen, and Christina Lioma. 2018b. The Copen- hagen Team Participation in the Factuality Task of the Competition of Automatic Identi\ufb01cation and Veri\ufb01cation of Claims in Political Debates of the CLEF-2018 Fact Checking Lab. In CLEF (Working Notes), volume 2125 of CEUR Workshop Proceed- ings. CEUR-WS.org. William Yang Wang. 2017. \u201cLiar, Liar Pants on Fire\u201d: A New Benchmark Dataset for Fake News Detec- tion. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol- ume 2: Short Papers), pages 422\u2013426. Association for Computational Linguistics. Wenpeng Yin and Dan Roth. 2018. TwoWingOS: A Two-Wing Optimization Strategy for Evidential Claim Veri\ufb01cation.",
            "Association for Computational Linguistics. Wenpeng Yin and Dan Roth. 2018. TwoWingOS: A Two-Wing Optimization Strategy for Evidential Claim Veri\ufb01cation. In Proceedings of the 2018 Con- ference on Empirical Methods in Natural Language Processing, pages 105\u2013114, Brussels, Belgium. As- sociation for Computational Linguistics. Arkaitz Zubiaga, Elena Kochkina, Maria Liakata, Rob Procter, Michal Lukasik, Kalina Bontcheva, Trevor Cohn, and Isabelle Augenstein. 2018. Discourse- aware rumour stance classi\ufb01cation in social media using sequential classi\ufb01ers. Informatino Processing & Management, 54(2):273\u2013290. Arkaitz Zubiaga, Maria Liakata, Rob Procter, Geral- dine Wong Sak Hoi, and Peter Tolmie. 2016. Analysing how people orient to and spread rumours in social media by looking at conversational threads. PLOS ONE, 11(3):1\u201329.",
            "Websites (Sources) Reason Mediabiasfactcheck Website that checks other news websites CBC No pattern to crawl apnews.com\/APFactCheck No categorical label and no structured claim weeklystandard.com\/tag\/fact-check Mostly no label, and they are placed anywhere ballotpedia.org No categorical label and no structured claim channel3000.com\/news\/politics\/reality-check No categorical label, lack of structure, and no clear claim npr.org\/sections\/politics-fact-check No label and no clear claim (only some titles are claims) dailycaller.com\/buzz\/check-your-fact Is a subset of checkyourfact which has already been crawled sacbee.com6 Contains very few labelled articles, and without clear claims TheGuardian Only a few websites have a pattern for labels. Table 9: The list of websites that we did not crawl and reasons for not crawling them. Domain # Insts # Labels Labels abbc 436 3 in-between, in-the-red, in-the-green afck 433 7 correct, incorrect, mostly-correct, unproven, misleading, understated, exagger- ated bove 295 2 none, rating: false chct 355 4 verdict: true, verdict: false,",
            "in-the-red, in-the-green afck 433 7 correct, incorrect, mostly-correct, unproven, misleading, understated, exagger- ated bove 295 2 none, rating: false chct 355 4 verdict: true, verdict: false, verdict: unsubstantiated, none clck 38 3 incorrect, unsupported, misleading faan 111 3 factscan score: false, factscan score: true, factscan score: misleading faly 71 5 true, none, partly true, unveri\ufb01ed, false fani 20 3 conclusion: accurate, conclusion: false, conclusion: unclear farg 485 11 false, none, distorts the facts, misleading, spins the facts, no evidence, not the whole story, unsupported, cherry picks, exaggerates, out of context goop 2943 6 0, 1, 2, 3, 4, 10 hoer 1310 7 facebook scams, true messages, bogus warning, statirical reports, fake news, unsubstantiated messages, misleading recommendations huca 34 3 a lot of baloney, a little baloney,",
            "1, 2, 3, 4, 10 hoer 1310 7 facebook scams, true messages, bogus warning, statirical reports, fake news, unsubstantiated messages, misleading recommendations huca 34 3 a lot of baloney, a little baloney, some baloney mpws 47 3 accurate, false, misleading obry 59 4 mostly true, veri\ufb01ed, unobservable, mostly false para 222 7 mostly false, mostly true, half-true, false, true, pants on \ufb01re!, half \ufb02ip peck 65 3 false, true, partially true pomt 15390 9 half-true, false, mostly true, mostly false, true, pants on \ufb01re!, full \ufb02op, half \ufb02ip, no \ufb02ip pose 1361 6 promise kept, promise broken, compromise, in the works, not yet rated, stalled ranz 21 2 fact, \ufb01ction snes 6455 12 false, true, mixture, unproven, mostly false, mostly true, miscaptioned, legend, outdated, misattributed, scam,",
            "promise broken, compromise, in the works, not yet rated, stalled ranz 21 2 fact, \ufb01ction snes 6455 12 false, true, mixture, unproven, mostly false, mostly true, miscaptioned, legend, outdated, misattributed, scam, correct attribution thet 79 6 none, mostly false, mostly true, half true, false, true thal 74 2 none, we rate this claim false tron 3423 27 \ufb01ction!, truth!, unproven!, truth! & \ufb01ction!, mostly \ufb01ction!, none, disputed!, truth! & misleading!, authorship con\ufb01rmed!, mostly truth!, incorrect attribu- tion!, scam!, investigation pending!, con\ufb01rmed authorship!, commentary!, pre- viously truth! now resolved!, outdated!, truth! & outdated!, virus!, \ufb01ction! & satire!, truth! & unproven!, misleading!, grass roots movement!, opinion!, cor- rect attribution!, truth! & disputed!, inaccurate attribution!",
            "now resolved!, outdated!, truth! & outdated!, virus!, \ufb01ction! & satire!, truth! & unproven!, misleading!, grass roots movement!, opinion!, cor- rect attribution!, truth! & disputed!, inaccurate attribution! vees 504 4 none, fake, misleading, false vogo 653 8 none, determination: false, determination: true, determination: mostly true, determination: misleading, determination: barely true, determination: huckster propaganda, determination: false, determination: a stretch wast 201 7 4 pinnochios, 3 pinnochios, 2 pinnochios, false, not the whole story, needs context, none Table 10: Number of instances, and labels per domain sorted by number of occurrences",
            "Website Domain Claims Labels Category Speaker Checker Tags Article Claim date Publish date Full text Outlinks abc abbc 436 436 436 - - 436 436 - 436 436 7676 africacheck afck 436 436 - - - - 436 - 436 436 2325 altnews - 496 - - - 496 - 496 - 496 496 6389 boomlive - 302 302 - - - - 302 - 302 302 6054 checkyourfact chht 358 358 - - 358 - - - 358 358 5271 climatefeedback clck 45 45 - - - - 45 - 45 45 489 crikey - 18 18 18 - 18 18 18 - 18 18 212 factcheckni - 36 36 36 - - - 36 - - 36 151 factcheckorg farg 512 512 512 512 512 512 512 512 512 512 8282 factly - 77 77 - - - - 77 - - 77 658 factscan - 115 115 - 115 - - - 115",
            "151 factcheckorg farg 512 512 512 512 512 512 512 512 512 512 8282 factly - 77 77 - - - - 77 - - 77 658 factscan - 115 115 - 115 - - - 115 115 115 1138 fullfact - 336 336 336 - 336 - 336 - 336 336 3838 gossipcop goop 2947 2947 - - 2947 - 2947 - 2947 2947 12583 hoaxslayer hoer 1310 1310 - - 1310 - 1310 - 1310 1310 14499 huf\ufb01ngtonpostca huca 38 38 - 38 38 - 38 38 38 38 78 leadstories - 1547 1547 - - 1547 - 1547 - 1547 1547 12015 mprnews mpws 49 49 - - 49 - 49 - 49 49 319 nytimes - 17 17 - - 17 - 17 - 17 17 271",
            "- - 1547 - 1547 - 1547 1547 12015 mprnews mpws 49 49 - - 49 - 49 - 49 49 319 nytimes - 17 17 - - 17 - 17 - 17 17 271 observatory obry 60 60 - - 60 - 60 - 60 60 592 pandora para 225 225 225 225 225 - 225 - 225 225 114 pesacheck peck 67 67 - - 67 - 67 - 67 67 521 politico - 102 102 - - 102 - 102 - 102 102 150 politifact promise pose 1361 1361 1361 1361 - - 1361 - 1361 1361 6279 politifact stmt pomt 15390 15390 - 15390 - - - 15390 15390 15390 78543 politifact story - 5460 - - - 5460 - - - 5460 5460 24836 radionz ranz 32 32 32 32 - - 32",
            "15390 - 15390 - - - 15390 15390 15390 78543 politifact story - 5460 - - - 5460 - - - 5460 5460 24836 radionz ranz 32 32 32 32 - - 32 32 32 32 44 snopes snes 6457 6457 6457 - 6457 - 6457 - 6457 6457 46735 swissinfo - 20 20 20 20 20 - 20 - 20 20 40 theconversation - 62 62 62 62 62 62 62 - 62 62 723 theferret thet 81 81 81 81) - - 81 - 81(81) 81 885 theguardian - 155 155 155 - 155 - 155 - 155 155 2600 thejournal thal 179 179 - - - - 179 - 179 179 2375 truthor\ufb01ction tron 3674 3674 3674 - - 3674 3674 - 3674 3674",
            "- 155 - 155 155 2600 thejournal thal 179 179 - - - - 179 - 179 179 2375 truthor\ufb01ction tron 3674 3674 3674 - - 3674 3674 - 3674 3674 8268 vera\ufb01les vees 509 509 - - - 509 509 - 509 509 23 voiceofsandiego vogo 660 660 - - - - 660 - 660 660 2352 washingtonpost wast 227 227 - 227 227 - 227 - 227 227 2470 wral - 20 20 - - 20 20 20 - 20 20 355 zimfact - 21 21 21 21 21 - 21 - 21 21 179 Total 43837 43837 43837 43837 43837 43837 43837 43837 43837 43837 260330 Table 11: Summary statistics for claim collection.",
            "21 21 21 21 - 21 - 21 21 179 Total 43837 43837 43837 43837 43837 43837 43837 43837 43837 43837 260330 Table 11: Summary statistics for claim collection. \u2018Domain\u2019 indicates the domain name used for the veracity prediction experiments, \u2018\u2013\u2019 indicates that the website was not used due to missing or insuf\ufb01cient claim labels, see Section 3.2."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1909.03242.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 12967.00048828125,
    "avg_doclen_est": 175.229736328125
}
