{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Syntax-Enhanced Self-Attention-Based Semantic Role Labeling Yue Zhang, Rui Wang, Luo Si Alibaba Group, China {shiyu.zy, masi.wr, luo.si}@alibaba-inc.com Abstract As a fundamental NLP task, semantic role la- beling (SRL) aims to discover the semantic roles for each predicate within one sentence. This paper investigates how to incorporate syntactic knowledge into the SRL task effec- tively. We present different approaches of en- coding the syntactic information derived from dependency trees of different quality and rep- resentations; we propose a syntax-enhanced self-attention model and compare it with other two strong baseline methods; and we con- duct experiments with newly published deep contextualized word representations as well. The experiment results demonstrate that with proper incorporation of the high quality syn- tactic information, our model achieves a new state-of-the-art performance for the Chinese SRL task on the CoNLL-2009 dataset.",
      "The experiment results demonstrate that with proper incorporation of the high quality syn- tactic information, our model achieves a new state-of-the-art performance for the Chinese SRL task on the CoNLL-2009 dataset. 1 Introduction The task of semantic role labeling (SRL) is to rec- ognize arguments for a given predicate in one sen- tence and assign labels to them, including \u201cwho\u201d did \u201cwhat\u201d to \u201cwhom\u201d, \u201cwhen\u201d, \u201cwhere\u201d, etc. Fig- ure 1 is an example sentence with both semantic roles and syntactic dependencies.",
      "Fig- ure 1 is an example sentence with both semantic roles and syntactic dependencies. Since the nature of semantic roles is more abstract than the syntac- tic dependencies, SRL has a wide range of appli- cations in different areas, e.g., text classi\ufb01cation (Sinoara et al., 2016), text summarization (Gen- est and Lapalme, 2011; Khan et al., 2015), recog- nizing textual entailment (Burchardt et al., 2007; Stern and Dagan, 2014), information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007; Yih et al., 2016), and so on. Traditionally, syntax is the bridge to reach se- mantics. However, along with the popularity of the end-to-end models in the NLP community, various recent studies have been discussing the necessity of syntax in the context of SRL.",
      "Traditionally, syntax is the bridge to reach se- mantics. However, along with the popularity of the end-to-end models in the NLP community, various recent studies have been discussing the necessity of syntax in the context of SRL. For instance, He $ \u4e2d\u56fd \u9f13\u52b1 \u5916\u5546 \u6295\u8d44 \u519c\u4e1a $ China encourage foreign invest agriculture merchant SBJ ROOT COMP COMP COMP A0 A1 A2 Figure 1: An example of one sentence with its syntac- tic dependency tree and semantic roles. Arcs above the sentence are semantic role annotations for the predicate \u201c\u9f13\u52b1(encourage)\u201d and below the sentence are syntac- tic dependency annotations of the whole sentence. The meaning of this sentence is \u201cChina encourages foreign merchants to invest in agriculture\u201d. et al. (2017) have observed that only good syntax helps with the SRL performance. Xia et al. (2019) have explored what kind of syntactic information or structure is better suited for the SRL model. Cai et al. (2018) have compared syntax-agnostic and syntax-aware approaches and claim that the syntax-agnostic model surpasses the syntax-aware ones.",
      "(2019) have explored what kind of syntactic information or structure is better suited for the SRL model. Cai et al. (2018) have compared syntax-agnostic and syntax-aware approaches and claim that the syntax-agnostic model surpasses the syntax-aware ones. In this paper, we focus on analyzing the rela- tionship between the syntactic dependency infor- mation and the SRL performance. In particular, we investigate the following four aspects: 1) Qual- ity of the syntactic information: whether the per- formance of the syntactic parser output affects the SRL performance; 2) Representation of the syn- tactic information: how to represent the syntactic dependencies to better preserve the original struc- tural information; 3) Incorporation of the syntac- tic information: at which layer of the SRL model and how to incorporate the syntactic information; and 4) the Relationship with other external re- sources: when we append other external resources into the SRL model, whether their contributions are orthogonal to the syntactic dependencies.",
      "For the main architecture of the SRL model, many neural-network-based models use BiLSTM arXiv:1910.11204v1  [cs.CL]  24 Oct 2019",
      "as the encoder (e.g., Cai et al. (2018); Li et al. (2018); He et al. (2018)), while recently self- attention-based encoder becomes popular due to both the effectiveness and the ef\ufb01ciency (Vaswani et al., 2017; Tan et al., 2017; Strubell et al., 2018). By its nature, the self-attention-based model di- rectly captures the relation between words in the sentence, which is convenient to incorporate syn- tactic dependency information. Strubell et al. (2018) replace one attention head with pre-trained syntactic dependency information, which can be viewed as a hard way to inject syntax into the neural model. Enlightened by the machine trans- lation model proposed by Shaw et al. (2018), we introduce the Relation-Aware method to incorpo- rate syntactic dependencies, which is a softer way to encode richer structural information. Various experiments for the Chinese SRL on the CoNLL-2009 dataset are conducted to evalu- ate our hypotheses.",
      "(2018), we introduce the Relation-Aware method to incorpo- rate syntactic dependencies, which is a softer way to encode richer structural information. Various experiments for the Chinese SRL on the CoNLL-2009 dataset are conducted to evalu- ate our hypotheses. From the empirical results, we observe that: 1) The quality of the syntactic infor- mation is essential when we incorporate structural information into the SRL model; 2) Deeper inte- gration of the syntactic information achieves bet- ter results than the simple concatenation to the in- puts; 3) External pre-trained contextualized word representations help to boost the SRL performance further, which is not entirely overlapping with the syntactic information. In summary, the contributions of our work are: \u2022 We present detailed experiments on differ- ent aspects of incorporating syntactic infor- mation into the SRL model, in what quality, in which representation and how to integrate. \u2022 We introduce the relation-aware approach to employ syntactic dependencies into the self- attention-based SRL model.",
      "\u2022 We introduce the relation-aware approach to employ syntactic dependencies into the self- attention-based SRL model. \u2022 We compare our approach with previous studies, and achieve state-of-the-art results with and without external resources, i.e., in the so-called closed and open settings. 2 Related work Traditional semantic role labeling task (Gildea and Jurafsky, 2002) presumes that the syntactic structure of the sentence is given, either being a constituent tree or a dependency tree, like in the CoNLL shared tasks (Carreras and M`arquez, 2005; Surdeanu et al., 2008; Haji\u02c7c et al., 2009). Recent neural-network-based approaches can be roughly categorized into two classes: 1) making use of the syntactic information (FitzGerald et al., 2015; Roth and Lapata, 2016; Qian et al., 2017; Marcheggiani and Titov, 2017), and 2) pure end- to-end learning from tokens to semantic labels, e.g., Zhou and Xu (2015); Marcheggiani et al. (2017).",
      "(2017). Roth and Lapata (2016) utilize an LSTM model to obtain embeddings from the syntactic depen- dency paths; while Marcheggiani and Titov (2017) construct Graph Convolutional Networks to en- code the dependency structure. Although He et al. (2017)\u2019s approach is a pure end-to-end learning, they have included an analysis of adding syntac- tic dependency information into English SRL in the discussion section. Cai et al. (2018) have compared syntax-agnostic and syntax-aware ap- proaches and Xia et al. (2019) have compared dif- ferent ways to represent and encode the syntactic knowledge. In another line of research, Tan et al. (2017) uti- lize the Transformer network for the encoder in- stead of the BiLSTM. Strubell et al. (2018) present a novel and effective multi-head self-attention model to incorporate syntax, which is called LISA (Linguistically-Informed Self-Attention).",
      "Strubell et al. (2018) present a novel and effective multi-head self-attention model to incorporate syntax, which is called LISA (Linguistically-Informed Self-Attention). We fol- low their approach of replacing one attention head with the dependency head information, but use a softer way to capture the pairwise relationship be- tween input elements (Shaw et al., 2018). For the datasets and annotations of the SRL task, most of the previous research focuses on 1) PropBank (Palmer et al., 2005) and Nom- Bank (Meyers et al., 2004) annotations, i.e., the CoNLL 2005 (Carreras and M`arquez, 2005) and CoNLL 2009 (Haji\u02c7c et al., 2009) shared tasks; 2) OntoNotes annotations (Weischedel et al., 2011), i.e., the CoNLL 2005 and CoNLL 2012 datasets and more; 3) and FrameNet (Baker et al., 1998) annotations.",
      "For the non-English languages, not all of them are widely available. Apart from these, in the broad range of semantic pro- cessing, other formalisms non-exhaustively in- clude abstract meaning representation (Banarescu et al., 2013), universal decompositional seman- tics (White et al., 2016), and semantic dependency parsing (Oepen et al., 2015). Abend and Rap- poport (2017) give a better overview of various se- mantic representations. In this paper, we primarily work on the Chinese and English datasets from the",
      "Figure 2: Architecture of our syntax-enhanced self- attention-based SRL model. Red dotted arrows indi- cate different locations where we incorporate linguis- tic knowledge in different forms. The dotted box on the upper right is the detailed composition of the self- attention block. CoNLL-2009 shared task and focus on the effec- tiveness of incorporating syntax into the Chinese SRL task. 3 Approaches In this section, we \ufb01rst introduce the basic archi- tecture of our self-attention-based SRL model, and then present two different ways to encode the syn- tactic dependency information. Afterwards, we compare three approaches to incorporate the syn- tax into the base model, concatenation to the in- put embedding, LISA, and our proposed relation- aware method. 3.1 The Basic Architecture Our basic model is a multi-head self-attention- based model, which is effective in SRL task as pre- vious work proves (Tan et al., 2018). The model consists of three layers: the input layer, the en- coder layer and the prediction layer as shown in Figure 2.",
      "The model consists of three layers: the input layer, the en- coder layer and the prediction layer as shown in Figure 2. 3.1.1 Input Layer The input layer contains three types of embed- dings: token embedding, predicate embedding, and positional embedding. Token Embedding includes word embedding, part-of-speech (POS) tag embedding. Predicate Embedding has been proposed by He et al. (2017), and its binary embedding is used to indicate the predicates indices in each sentence. Positional Embedding encodes the order of the input word sequence. We follow Vaswani et al. (2017) to use time positional embedding, which is formulated as follows: PE(t, 2i) = sin(t/100002i/d) PE(t, 2i + 1) = cos(t/100002i/d) (1) where t is the position, i means the dimension, and d is the dimension of the model input embedding. 3.1.2 Encoder Layer The self-attention block is almost the same as Transformer encoder proposed by Vaswani et al. (2017).",
      "3.1.2 Encoder Layer The self-attention block is almost the same as Transformer encoder proposed by Vaswani et al. (2017). Speci\ufb01cally the Transformer encoder con- tains a feed-forward network (FFN) and a multi- head attention network. The former is followed by the latter. In this work, we exchange their order, so that the multi-head attention module is moved behind the FFN module1 as Figure 2 shows. FFN The FFN module consists of two af\ufb01ne layers with a ReLU activation in the middle. For- mally, we have the following equation: FFN(x) = max(0, xW1 + b1)W2 + b2 (2) Multi-Head Attention The basic attention mechanism used in the multi-head attention func- tion is called \u201cScaled Dot-Product Attention\u201d, which is formulated as follows: Attention(Q, K, V ) = softmax(QKT \u221adk )V (3) where Q is queries, K is keys, and V is values.",
      "In the multi-head attention setting, it \ufb01rst maps the input matrix X into queries, keys and values matrices by using h different learned linear pro- jections. Taking queries Q as an example: LinearQ i (X) = XW Q i + bQ i (4) where 0 \u2264i < h. Keys and values use similar projections. On each of these projections, we perform the scaled dot-product attention in parallel. These parallel output values are concatenated and once again projected into the \ufb01nal values. Equation 5 depicts the above operations. MultiHead(X) = Concat(head1, . . . , headh)W O (5) 1Changing the order delivers better empirical results (Tan et al., 2018) and our experiments show the same trend. The details of the experiments are not listed in this paper.",
      "where headi = Attention(LinearQ i (X), LinearK i (X), LinearV i (X)) (6) More details about multi-head attention can be found in Vaswani et al. (2017). Add & Norm We employ a residual connection to each module, followed by a layer normalization (Ba et al., 2016) operation. The output of each module is formulated as x = LayerNorm(x + f(x)) (7) where f(x) is implemented by each above mod- ule. 3.2 Representation of the Syntactic Dependencies 3.2.1 Dependency Head & Relation The most intuitive way to represent syntactic in- formation is to use individual dependency rela- tions directly, like dependency head and depen- dency relation label, denoted as DEP and REL for short. Except for LISA, where DEP is a one-hot ma- trix of dependency head word index described in 3.3.2, in other cases, we use the corresponding head word. REL is the dependency relation be- tween the word and its syntactic head.",
      "Except for LISA, where DEP is a one-hot ma- trix of dependency head word index described in 3.3.2, in other cases, we use the corresponding head word. REL is the dependency relation be- tween the word and its syntactic head. We take both DEP and REL as common strings and map them into dense vectors in the similar way of word embedding. 3.2.2 Dependency Path & Relation Path In order to preserve the structural information of dependency trees as much as possible, we take the syntactic path between candidate arguments and predicates in dependency trees as linguistic knowl- edge. Referring to Xia et al. (2019), we use the Tree-based Position Feature (TPF) as Dependency Path (DEPPATH) and use the Shortest Dependency Path (SDP) as Relation Path (RELPATH). To generate DEPPATH & RELPATH between candidate argument and predicate, we \ufb01rstly \ufb01nd their lowest common ancestor. Then we get two sub-paths, one is from the ancestor to the predicate and the other is from the ancestor to the argument.",
      "To generate DEPPATH & RELPATH between candidate argument and predicate, we \ufb01rstly \ufb01nd their lowest common ancestor. Then we get two sub-paths, one is from the ancestor to the predicate and the other is from the ancestor to the argument. For DEPPATH, we compute distance from ancestor to predicate and argument respectively and then concatenate two distances with the separator \u2018,\u2019. \u4e2d\u56fd China (1, 0) SBJ \u9f13\u52b1 (0, 0) encourage \u5916\u5546 foreign merchant (1, 0) COMP \u6295\u8d44 COMP invest (1, 0) \u519c\u4e1a agriculture (2, 0) COMP Figure 3: The syntactic dependency tree of the sentence \u201c\u4e2d\u56fd\u9f13\u52b1\u5916\u5546\u6295\u8d44\u519c\u4e1a\u201d (China encourages foreign merchants to invest in agriculture). Numbers in brack- ets are the DEPPATH for each candidate argument with the predicate \u201c\u9f13\u52b1(encourage)\u201d. Light grey labels on the arcs are the syntactic dependency labels.",
      "Numbers in brack- ets are the DEPPATH for each candidate argument with the predicate \u201c\u9f13\u52b1(encourage)\u201d. Light grey labels on the arcs are the syntactic dependency labels. For RELPATH, we concatenate the labels appear- ing in each sub-path with the separator \u201c \u201d respec- tively to get two label paths, and then concatenate the two label paths with the separator \u2018,\u2019. As shown in Figure 3, the lowest common an- cestor of the predicate \u201c\u9f13\u52b1(encourage)\u201d and the candidate argument \u201c\u519c\u4e1a(agriculture)\u201d is \u201c\u9f13\u52b1 (encourage)\u201d, so their DEPPATH is \u201c2,0\u201d and its RELPATH is \u201cCOMP COMP,\u201d2. We take both DEPPATH and RELPATH as com- mon strings and map them into dense vectors in the similar way of DEP and REL. 3.3 Incorporation Methods 3.3.1 Input Embedding Concatenation To incorporate syntactic knowledge, one simple method is to take it as part of the neural network input, denoted as INPUT.",
      "3.3 Incorporation Methods 3.3.1 Input Embedding Concatenation To incorporate syntactic knowledge, one simple method is to take it as part of the neural network input, denoted as INPUT. We represent the syntac- tic information with dense vectors, and concate- nate it with other information like word embed- ding: input = EW \u2295ES. (8) where \u2295means concatenation; EW means the original inputs of the neural model and ES means the embedding of syntax information, such as DEP/REL or DEPPATH/RELPATH. 3.3.2 LISA Strubell et al. (2018) propose the linguistically- informed self-attention model (LISA for short) to combine SRL and dependency parsing as multi- task learning in a subtle way. Based on the multi- head self-attention model, LISA uses one atten- tion head to predict the dependency results and it 2 When predicate is the ancestor of argument, sub-path from ancestor to predicate is none. We use \u20180\u2019 to represent distance and use empty string to represent the label path, and vice versa.",
      "Figure 4: Attention matrix of the replaced attention head in the LISA model. The left matrix is the original softmax attention, and the right is a one-hot matrix copied from the syntactic dependency head results. can also directly use pre-trained dependency head results to replace the attention matrix during test- ing. Being different from their multi-task learning, we make the replacement of one attention head during both training and testing. Instead of the original softmax attention matrix, we use a one- hot matrix, generated by mapping the dependency head index of each word into a 0-1 vector of the sentence length as Figure 4 shows. We add the dependency relation information with V in the replaced head so that we can make full use of the syntactic knowledge. The replaced attention head is formulated as follows: Attention(Q, K, V ) = MD (V \u2295ER) (9) where MD is the one-hot dependency head matrix and ER means the embedding of dependency rela- tion information, such as REL or RELPATH. 3.3.3 Relation-Aware Self-Attention Relation-aware self-attention model (RELAWE for brevity) incorporates external information into the attention.",
      "3.3.3 Relation-Aware Self-Attention Relation-aware self-attention model (RELAWE for brevity) incorporates external information into the attention. By this way, the model considers the pairwise relationships between input elements, which highly agrees with the task of SRL, i.e., aiming to \ufb01nd the semantic relations between the candidate argument and predicate in one sentence. Compared to the standard attention, in this pa- per, we add the dependency information into Q and V in each attention head, like equation (10) shows: Attention(Q, K, V ) = softmax((Q + ED + ER)KT \u221adk )(V + ED + ER) (10) where ED and ER mean the syntactic dependency head and relation information respectively. For our multi-layer multi-head self-attention model, we make this change to each head of the \ufb01rst N self-attention layers.",
      "For our multi-layer multi-head self-attention model, we make this change to each head of the \ufb01rst N self-attention layers. dev test UAS LAS UAS LAS AUTO 80.50 78.34 80.70 78.46 BIAFFINE 89.00 85.86 89.05 85.60 BIAFFINEBERT 91.76 89.08 92.14 89.23 Table 1: Syntactic dependency performance for differ- ent parsers. AUTO indicates the automatic dependency trees provided by the CoNLL-09 Chinese dataset. BI- AFFINE means the trees are generated by Biaf\ufb01neParser with pre-trained word embedding on the Gigaword corpus while BIAFFINEBERT is the same parser with BERT. We use the labeled accuracy score (LAS) and unlabeled accuracy score (UAS) to measure the quality of syntactic dependency trees. 4 Experiment 4.1 Settings Datasets & Evaluation Metrics Our experiments are conducted on the CoNLL-2009 shared task dataset (Haji\u02c7c et al., 2009).",
      "4 Experiment 4.1 Settings Datasets & Evaluation Metrics Our experiments are conducted on the CoNLL-2009 shared task dataset (Haji\u02c7c et al., 2009). We use the of\ufb01cial evaluation script to compare the output of different system con\ufb01gurations, and report the labeled pre- cision (P), labeled recall (R) and labeled f-score (F1) for the semantic dependencies. Word Representations Most of our experi- ments are conducted in the closed setting without any external word embeddings or data resources than those provided by the CoNLL-2009 datasets. In the closed setting, word embedding is initial- ized by a Gaussian distribution with mean 0 and variance 1 \u221a d, where d is the dimension of embed- ding size of each layer.",
      "In the closed setting, word embedding is initial- ized by a Gaussian distribution with mean 0 and variance 1 \u221a d, where d is the dimension of embed- ding size of each layer. For the experiments with external resources in the open setting, we utilize 1) word embed- dings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese and the published embeddings with 100 dimensions pre- trained on Wikipedia and Gigaword for English; and 2) ELMo3 (Peters et al., 2018) and BERT4 (Devlin et al., 2018), two recently proposed effec- tive deep contextualized word representations5. 3We use the released model on their website: https://github.com/allenai/allennlp/blob/master/tutorials /how to/elmo.md 4We generate our pre-trained BERT embedding with the released model in https://github.com/google-research/bert. The model uses character-based tokenization for Chinese, which require us to maintain alignment between our input text and output text of Bert.",
      "The model uses character-based tokenization for Chinese, which require us to maintain alignment between our input text and output text of Bert. So we take take embedding of the \ufb01rst word piece as the whole word representation. 5In open setting, we use pre-trained word embedding in- stead of random initialized embedding. If using BERT and Elmo additionally, we project them into representation vec- tors of same dimension as word embedding and concatenate",
      "Other embeddings, i.e., POS embedding, lin- guistic knowledge embedding, and so on are ini- tialized in same way as random word embedding no matter in closed or open setting. Syntactic Parsers In Table 1, both AUTO and GOLD syntactic dependencies are provided by the dataset. Since the performance of the AUTO is far behind the state-of-the-art Biaf\ufb01neParser (Dozat and Manning, 2016), we generate more depen- dency results by training Biaf\ufb01neParser6 with dif- ferent external knowledge, including pre-trained word embedding and BERT. Performance for dif- ferent parsers is listed in Table 1. Parameters In this work, we set word embed- ding size dw = 100, POS embedding size dt = 50. The predicate embedding size is set as dp = 100. The syntax-related embedding size varies along with different con\ufb01gurations, so as the feature em- bedding size df. To facilitate residual connections, all sub- layers in the model produce outputs of dimen- sion dmodel = df + dp.",
      "The syntax-related embedding size varies along with different con\ufb01gurations, so as the feature em- bedding size df. To facilitate residual connections, all sub- layers in the model produce outputs of dimen- sion dmodel = df + dp. The hidden dimension dff = 800 is applied for all the experiments. We set the number of shared self-attention blocks N = 10. The number of heads varies with dmodel, but dimension of each head is 25. Besides, LISA incorporates syntax knowledge in the 5-th self- attention layer while RELAWE incorporates in the \ufb01rst 5 layers. We apply the similar dropout strategy as Vaswani et al. (2017), i.e., the attention and resid- ual dropout values are 0.2 and 0.3 respectively. The dropout is also applied in the middle layer of FFN with value 0.2. We also employ label smoothing (Szegedy et al., 2015) of value 0.1 dur- ing training.",
      "The dropout is also applied in the middle layer of FFN with value 0.2. We also employ label smoothing (Szegedy et al., 2015) of value 0.1 dur- ing training. We use softmax-cross-entropy as our loss func- tion, and use the Adadelta optimizer (Zeiler, 2012) with \u03f5 = 10\u22126 and \u03c1 = 0.95. For all experiments, we train the model 200, 000 steps with learning rate lr = 1.0, and each batch has 4096 words. All the hyper-parameters are tuned on the devel- opment set. Con\ufb01gurations We use different abbreviations to represent the parsing results, syntactic depen- dency representations, and incorporation methods. All the system con\ufb01gurations in our experiments are listed in Table 2. them with other input. 6We split train data into 5-fold, and train model with 4- fold to generate automatic trees of the left 1-fold train data. Abbreviation Description NONE No syntactic knowledge Syn.",
      "6We split train data into 5-fold, and train model with 4- fold to generate automatic trees of the left 1-fold train data. Abbreviation Description NONE No syntactic knowledge Syn. Parser AUTO Parsing result from CoNLL-2009 AUTODEL Remove syntax errors of AUTO BIAFFINE Biaf\ufb01neParser result BIAFFINEBERT Biaf\ufb01neParser with BERT GOLD Gold syntax from CoNLL-2009 Syn. Representation DEP Dependency head REL Dependency relation label DEPPATH Tree-based position feature RELPATH Shortest relation label path Incorporation INPUT Add to the input embedding LISA From Strubell et al. (2018) RELAWE Relation-aware self-attention Table 2: A glossary of abbreviations for different sys- tem con\ufb01gurations in our experiments.",
      "(2018) RELAWE Relation-aware self-attention Table 2: A glossary of abbreviations for different sys- tem con\ufb01gurations in our experiments. P R F1 NONE 83.97 82.94 83.45 AUTO 85.92 84.62 85.26 BIAFFINE 86.04 85.43 85.73 BIAFFINEBERT 87.32 87.10 87.21 AUTODEL 88.33 87.67 88.00 GOLD 91.00 91.43 91.22 Table 3: SRL results with dependency trees of differ- ent quality on the Chinese dev set. These experiments are conducted on the RELAWE model with DEP&REL representations. 4.2 Quality of the Syntactic Dependencies We use the above-mentioned dependency trees of different quality for comparison, with DEP&REL representation on our RELAWE model.",
      "These experiments are conducted on the RELAWE model with DEP&REL representations. 4.2 Quality of the Syntactic Dependencies We use the above-mentioned dependency trees of different quality for comparison, with DEP&REL representation on our RELAWE model. In addi- tion, we generate one more data AUTODEL by deleting all the erroneous dependency heads and relations from the provided AUTO data according to the gold heads and relations, and we do not re- place them with any alternative heads and rela- tions7. We take this setting as another reference (along with GOLD) to indicate that erroneous syn- tax information may hurt the performance of the SRL model. We take the GOLD as the upperbound reference of our task setting. Experiment results in Table 3 demonstrate that, incorporating syntactic knowledge into the SRL model can achieve better performance and overall, the better the quality is, the better the SRL model performs. This is con- sistent with the previous study by He et al. (2017) on the English dataset. 7For the AUTODEL data, we cannot guarantee that there exists a syntactic path between the two words.",
      "This is con- sistent with the previous study by He et al. (2017) on the English dataset. 7For the AUTODEL data, we cannot guarantee that there exists a syntactic path between the two words. Therefore, we do not conduct experiments under the DepPath&RelPath setting.",
      "P R F1 AUTO DEPPATH&RELPATH 84.76 81.85 83.28 BIAFFINE DEP 84.33 84.47 84.40 REL 85.84 85.23 85.54 DEP&REL 86.04 85.43 85.73 DEPPATH 85.48 84.17 84.82 RELPATH 86.85 83.92 85.36 DEPPATH&RELPATH 86.40 85.52 85.96 GOLD DEPPATH&RELPATH 92.20 92.53 92.37 Table 4: SRL results with different syntactic represen- tations on the Chinese dev set. Experiments are con- ducted on the RELAWE method. Closer observation reveals two additional inter- esting phenomena. Firstly, SRL performance im- provement is not proportionate to the improve- ment of dependency quality. When switching syntactic dependency trees from AUTO to BI- AFFINE, SRL performance improves 0.5%, al- though syntactic dependency improves about 8%.",
      "Firstly, SRL performance im- provement is not proportionate to the improve- ment of dependency quality. When switching syntactic dependency trees from AUTO to BI- AFFINE, SRL performance improves 0.5%, al- though syntactic dependency improves about 8%. In contrast, the difference between BIAFFINE and BIAFFINEBERT shows more signi\ufb01cant improve- ment of 1.5%. The possible reason is that BI- AFFINEBERT provides key dependency informa- tion which is missing in other con\ufb01gurations. Sec- ondly, the SRL performance gap between AU- TODEL and AUTO is large though they provide the same correct syntactic information. This may indi- cate that incorporating erroneous syntactic knowl- edge hurts the SRL model, and even providing more correct dependencies cannot make up for the harm (cf. BIAFFINEBERT). 4.3 Representation of the Syntactic Dependencies Apart from DEP and REL, we also use DEPPATH and RELPATH to encode the syntactic knowledge. In this subsection, we conduct experiments to compare different syntactic encoding in our SRL model.",
      "BIAFFINEBERT). 4.3 Representation of the Syntactic Dependencies Apart from DEP and REL, we also use DEPPATH and RELPATH to encode the syntactic knowledge. In this subsection, we conduct experiments to compare different syntactic encoding in our SRL model. We base the experiments on our RELAWE model, since it is easier to incorporate different representations for comparison. When generating the RELPATH, we \ufb01lter the paths 1) when the de- pendency distance between the predicate and the candidate argument is more than 4, and 2) when the RELPATH\u2019s frequency is less than 108. No matter in which representation, dependency 8As we know, dependency parsers cannot deal with long distance dependency well and it is unlikely to deliver a reli- able result. And our experiments show that \ufb01ltration achieves empirically better results.",
      "No matter in which representation, dependency 8As we know, dependency parsers cannot deal with long distance dependency well and it is unlikely to deliver a reli- able result. And our experiments show that \ufb01ltration achieves empirically better results. P R F1 INPUT DEP 83.89 83.61 83.75 DEP&REL 86.21 85.00 85.60 DEP&RELPATH 86.01 85.38 85.69 DEPPATH&RELPATH 85.84 85.54 85.69 LISA DEP 84.68 85.38 85.03 DEP&REL 85.56 85.89 85.73 DEP&RELPATH 85.84 85.64 85.74 DEPPATH&RELPATH9 na na na RELAWE DEP 84.33 84.47 84.40 DEP&REL 86.04 85.43 85.73 DEP&RELPATH 86.21 85.01 85.60 DEPPATH&RELPATH 86.40 85.52 85.96 Table 5: SRL results with different incorporation meth- ods of the syntactic information on the Chinese dev set.",
      "Experiments are conducted on the BIAFFINE parsing results. P R F1 BIAFFINE RANDOM 86.40 85.52 85.96 GIGA 86.73 85.58 86.15 ELMO 86.84 86.74 86.79 BERT 88.16 89.57 88.86 BIAFFINEBERT BERT 88.05 89.65 88.84 Table 6: SRL results with different external knowledge on the Chinese dev set. We use the RELAWE model and DEPPATH&RELPATH syntax representation. label information is more important than the head and the combination of the two achieves better performance as our experiment results in Table 4 show. Furthermore, using BIAFFINE dependency trees, DEPPATH and RELPATH perform better than DEP and REL. This is because of the capability of DEPPATH and RELPATH to capture more struc- tural information of the dependency trees. Comparing Table 3 and 4, when using gold dependencies, DEPPATH&RELPATH can achieve much better result than DEP&REL.",
      "This is because of the capability of DEPPATH and RELPATH to capture more struc- tural information of the dependency trees. Comparing Table 3 and 4, when using gold dependencies, DEPPATH&RELPATH can achieve much better result than DEP&REL. But with the AUTO trees, DEPPATH&RELPATH is much worse. Therefore, structural information is much more sensitive to the quality of dependency trees due to error propagation. 4.4 Incorporation Methods This subsection discusses the effectiveness of different incorporation methods of the syntactic knowledge. We take BIAFFINE\u2019s output as our de- 9From the mechanism of LISA, we can \ufb01nd that the re- placed attention head can\u2019t copy the syntactic dependency heads from DEPPATH.",
      "Chinese P R F1 NONE 81.99 80.65 81.31 Closed CoNLL09 SRL Only na na 78.6 INPUT(DEPPATH&RELPATH) 84.19 83.65 83.92 LISA(DEP&RELPATH) 83.84 83.54 83.69 RELAWE(DEPPATH&RELPATH) 84.77 83.68 84.22 Open Marcheggiani and Titov (2017) na na 82.5 Cai et al. (2018) 84.7 84.0 84.3 INPUT(DEPPATH&RELPATH) + BERT 86.89 87.75 87.32 LISA(DEP&RELPATH) + BERT 86.45 87.90 87.17 RELAWE(DEPPATH&RELPATH) + BERT 86.73 87.98 87.35 GOLD 91.93 92.36 92.14 Table 7: SRL results on the Chinese test set. We choose the best settings for each con\ufb01guration of our model.",
      "We choose the best settings for each con\ufb01guration of our model. pendency information for the comparison. Firstly, results in Table 5 show that with lit- tle dependency information (DEP), LISA per- forms better, while incorporating richer syntactic knowledge (DEP&REL or DEP&RELPATH), three methods achieve similar performance. Overall, RELAWE achieves best results given enough syn- tactic knowledge. Secondly, INPUT and LISA achieve much bet- ter performance when we combine the dependency head information and the relation, while Strubell et al. (2018) have not introduced relation infor- mation to the LISA model and Xia et al. (2019) have not combined the head and relation informa- tion either. Our proposed RELAWE method with DEPPATH&RELPATH representation performs the best, which encodes the richest syntactic knowl- edge. Lastly, under the same settings, LISA and RELAWE perform better than INPUT, which in- dicates the importance of the location where the model incorporates the syntax, the input layer vs. the encoder layer.",
      "Lastly, under the same settings, LISA and RELAWE perform better than INPUT, which in- dicates the importance of the location where the model incorporates the syntax, the input layer vs. the encoder layer. 4.5 External Resources Apart from the experiments with syntactic knowl- edge itself, we also compare different external resources to discover their relationship with the syntax, including pre-trained word embeddings, ELMo, and BERT. We conduct experiments with our best setting, the RELAWE model with DEP- PATH & RELPATH and the results are listed in Ta- ble 6. The plain word embedding improves a little in such settings with syntactic information, while for the newly proposed ELMO and BERT, both of them can boost the models further. 4.6 Final Results on the Chinese Test Data Based on the above experiments and analyses, we present the overall results of our model in this subsection. We train the three models (INPUT, LISA, and RELAWE) with their best settings with- out any external knowledge as CLOSED, and we take the same models with BERT as OPEN.",
      "We train the three models (INPUT, LISA, and RELAWE) with their best settings with- out any external knowledge as CLOSED, and we take the same models with BERT as OPEN. The DEPPATH&RELPATH from GOLD without exter- nal knowledge serves as the GOLD for reference. Since we have been focusing on the task of argu- ment identi\ufb01cation and labeling, for both CLOSED and OPEN, we follow Roth and Lapata (2016) to use existing systems\u2019 predicate senses (Johansson and Nugues, 2008) to exclude them from compar- ison. Table 7 shows that our OPEN model achieves more than 3 points of f1-score than the state- of-the-art result, and RELAWE with DEP- PATH&RELPATH achieves the best in both CLOSED and OPEN settings. Notice that our best CLOSED model can almost perform as well as the state-of-the-art model while the latter utilizes pre- trained word embeddings. Besides, performance gap between three models under OPEN setting is very small. It indicates that the representation abil- ity of BERT is so powerful and may contains rich syntactic information.",
      "Besides, performance gap between three models under OPEN setting is very small. It indicates that the representation abil- ity of BERT is so powerful and may contains rich syntactic information. At last, the GOLD result is much higher than the other models, indicating that there is still large space for improvement for this task.",
      "English P R F1 LISA(DEP)10 89.26 85.46 87.32 LISA(DEP&RELPATH) 89.74 85.38 87.51 INPUT(DEPPATH) 89.33 85.60 87.42 INPUT(DEPPATH&RELPATH) 89.70 85.48 87.54 RELAWE(DEPPATH&RELPATH) 89.55 85.92 87.70 Table 8: SRL results on the English test set. We use syntactic dependency results generated by Bi- af\ufb01neParser (On test set, syntactic performance is: UAS = 94.35%, and LAS = 92.54%, which improves about 6% compared to automatic trees in CoNLL- 2009.). 4.7 Results on the English Data We also conduct several experiments on the En- glish dataset to validate the effectiveness of our approaches on other languages than Chinese and the results are in Table 8.",
      "4.7 Results on the English Data We also conduct several experiments on the En- glish dataset to validate the effectiveness of our approaches on other languages than Chinese and the results are in Table 8. Although both con\ufb01g- urations are not exactly the same as their origi- nal papers, we tried our best to reproduce their methods on the CoNLL2009 dataset for our com- parison. Overall, the results are consistent with the Chinese experiments, while the improvement is not as large as the Chinese counterparts. The RELAWE model with DEPPATH&RELPATH still achieves the best performance. Applying our syntax-enhanced model to more languages will be an interesting research direction to work on in the future. 5 Conclusion and Future Work This paper investigates how to incorporate syn- tactic dependency information into semantic role labeling in depth. Firstly, we con\ufb01rm that de- pendency trees of better quality are more helpful for the SRL task. Secondly, we present different ways to encode the trees and the experiments show that keeping more (correct) structural information during encoding improves the SRL performance.",
      "Firstly, we con\ufb01rm that de- pendency trees of better quality are more helpful for the SRL task. Secondly, we present different ways to encode the trees and the experiments show that keeping more (correct) structural information during encoding improves the SRL performance. Thirdly, we compare three incorporation methods and discover that our proposed relation-aware self- attention-based model is the most effective one. Although our experiments are primarily on the Chinese dataset, the approach is largely language independent. Apart from our tentative experi- ments on the English dataset, applying the ap- proach to other languages will be an interesting 10We reimplement LISA in Strubell et al. (2018) as LISA(DEP), and Xia et al. (2019)\u2019s best DEPPATH approach as INPUT(DEPPATH). Therefore, we can compare with their work as fairly as possible. Other settings are the best con\ufb01g- urations for their corresponding methods. research direction to work on in the future. References Omri Abend and Ari Rappoport. 2017. The state of the art in semantic representation.",
      "Other settings are the best con\ufb01g- urations for their corresponding methods. research direction to work on in the future. References Omri Abend and Ari Rappoport. 2017. The state of the art in semantic representation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 77\u201389, Vancou- ver, Canada. Association for Computational Lin- guistics. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin- ton. 2016. Layer normalization. abs/1607.06450. Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The berkeley framenet project. In Proceed- ings of the 36th Annual Meeting of the Associa- tion for Computational Linguistics and 17th Inter- national Conference on Computational Linguistics, pages 86\u201390, Stroudsburg, PA, USA. Association for Computational Linguistics.",
      "In Proceed- ings of the 36th Annual Meeting of the Associa- tion for Computational Linguistics and 17th Inter- national Conference on Computational Linguistics, pages 86\u201390, Stroudsburg, PA, USA. Association for Computational Linguistics. Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Grif\ufb01tt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In Proceedings of the 7th Linguis- tic Annotation Workshop and Interoperability with Discourse, pages 178\u2013186, So\ufb01a, Bulgaria. Associ- ation for Computational Linguistics. Aljoscha Burchardt, Nils Reiter, Stefan Thater, and Anette Frank. 2007. A semantic approach to tex- tual entailment: System evaluation and task analy- sis. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, RTE \u201907, pages 10\u201315, Stroudsburg, PA, USA.",
      "2007. A semantic approach to tex- tual entailment: System evaluation and task analy- sis. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, RTE \u201907, pages 10\u201315, Stroudsburg, PA, USA. Association for Computational Linguistics. Jiaxun Cai, Shexia He, Zuchao Li, and Hai Zhao. 2018. A full end-to-end semantic role labeler, syntax-agnostic over syntax-aware? CoRR, abs/1808.03815. Xavier Carreras and Llu\u00b4\u0131s M`arquez. 2005. Introduc- tion to the conll-2005 shared task: Semantic role la- beling. In Proceedings of the Ninth Conference on Computational Natural Language Learning, pages 152\u2013164, Stroudsburg, PA, USA. Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: pre-training of deep bidirectional transformers for language under- standing. CoRR, abs/1810.04805.",
      "Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: pre-training of deep bidirectional transformers for language under- standing. CoRR, abs/1810.04805. Timothy Dozat and Christopher D. Manning. 2016. Deep biaf\ufb01ne attention for neural dependency pars- ing. CoRR, abs/1611.01734. Nicholas FitzGerald, Oscar T\u00a8ackstr\u00a8om, Kuzman Ganchev, and Dipanjan Das. 2015. Semantic role la- beling with neural network factors. In Proceedings of the 2015 Conference on Empirical Methods in",
      "Natural Language Processing, pages 960\u2013970, Lis- bon, Portugal. Association for Computational Lin- guistics. Pierre-Etienne Genest and Guy Lapalme. 2011. Framework for abstractive summarization using text-to-text generation. In Proceedings of the Workshop on Monolingual Text-To-Text Generation, pages 64\u201373, Portland, Oregon. Association for Computational Linguistics. Daniel Gildea and Daniel Jurafsky. 2002. Auto- matic labeling of semantic roles. Comput. Linguist., 28(3):245\u2013288. Jan Haji\u02c7c, Massimiliano Ciaramita, Richard Johans- son, Daisuke Kawahara, Maria Ant`onia Mart\u00b4\u0131, Llu\u00b4\u0131s M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad\u00b4o, Jan \u02c7St\u02c7ep\u00b4anek, Pavel Stra\u02c7n\u00b4ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages.",
      "2009. The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the Thir- teenth Conference on Computational Natural Lan- guage Learning (CoNLL 2009): Shared Task, pages 1\u201318, Boulder, Colorado. Association for Computa- tional Linguistics. Luheng He, Kenton Lee, Omer Levy, and Luke Zettle- moyer. 2018. Jointly predicting predicates and argu- ments in neural semantic role labeling. In Proceed- ings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 364\u2013369. Luheng He, Kenton Lee, Mike Lewis, and Luke Zettle- moyer. 2017. Deep semantic role labeling: What works and what\u2019s next. In Proceedings of the 55th Annual Meeting of the Association for Computa- tional Linguistics, pages 473\u2013483. Association for Computational Linguistics. Richard Johansson and Pierre Nugues. 2008. The ef- fect of syntactic representation on semantic role la- beling.",
      "Association for Computational Linguistics. Richard Johansson and Pierre Nugues. 2008. The ef- fect of syntactic representation on semantic role la- beling. In COLING 2008, 22nd International Con- ference on Computational Linguistics, Proceedings of the Conference, 18-22 August 2008, Manchester, UK, pages 393\u2013400. Atif Khan, Naomie Salim, and Yogan Jaya Kumar. 2015. A framework for multi-document abstrac- tive summarization based on semantic role labelling. Appl. Soft Comput., 30(C):737\u2013747. Zuchao Li, Shexia He, Jiaxun Cai, Zhuosheng Zhang, Hai Zhao, Gongshen Liu, Linlin Li, and Luo Si. 2018. A uni\ufb01ed syntax-aware framework for seman- tic role labeling. In Proceedings of the 2018 Con- ference on Empirical Methods in Natural Language Processing, pages 2401\u20132411. Diego Marcheggiani, Anton Frolov, and Ivan Titov. 2017.",
      "In Proceedings of the 2018 Con- ference on Empirical Methods in Natural Language Processing, pages 2401\u20132411. Diego Marcheggiani, Anton Frolov, and Ivan Titov. 2017. A simple and accurate syntax-agnostic neural model for dependency-based semantic role labeling. In Proceedings of the 21st Conference on Computa- tional Natural Language Learning (CoNLL 2017), pages 411\u2013420, Vancouver, Canada. Association for Computational Linguistics. Diego Marcheggiani and Ivan Titov. 2017. Encoding sentences with graph convolutional networks for se- mantic role labeling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Lan- guage Processing(EMNLP2017), pages 1506\u20131515, Copenhagen, Denmark. Association for Computa- tional Linguistics. A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska, B. Young, and R. Grishman. 2004. The nombank project: An interim report.",
      "A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska, B. Young, and R. Grishman. 2004. The nombank project: An interim report. In HLT- NAACL 2004 Workshop: Frontiers in Corpus Anno- tation, pages 24\u201331, Boston, Massachusetts, USA. Association for Computational Linguistics. Stephan Oepen, Marco Kuhlmann, Yusuke Miyao, Daniel Zeman, Silvie Cinkova, Dan Flickinger, Jan Hajic, and Zdenka Uresova. 2015. Semeval 2015 task 18: Broad-coverage semantic dependency pars- ing. In Proceedings of the 9th International Work- shop on Semantic Evaluation (SemEval 2015), pages 915\u2013926, Denver, Colorado. Association for Com- putational Linguistics. Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Comput. Linguist., 31(1):71\u2013106.",
      "Association for Com- putational Linguistics. Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Comput. Linguist., 31(1):71\u2013106. Jeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. Glove: Global vectors for word representation. In EMNLP, pages 1532\u20131543. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, pages 2227\u20132237. Feng Qian, Lei Sha, Baobao Chang, LuChen Liu, and Ming Zhang. 2017. Syntax aware lstm model for semantic role labeling.",
      "Feng Qian, Lei Sha, Baobao Chang, LuChen Liu, and Ming Zhang. 2017. Syntax aware lstm model for semantic role labeling. In Proceedings of the 2nd Workshop on Structured Prediction for Natural Lan- guage Processing, pages 27\u201332, Copenhagen, Den- mark. Association for Computational Linguistics. Michael Roth and Mirella Lapata. 2016. Neural se- mantic role labeling with dependency path embed- dings. In Proceedings of the 54th Annual Meet- ing of the Association for Computational Linguis- tics, pages 1192\u20131202, Berlin, Germany. Associa- tion for Computational Linguistics. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position represen- tations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, NAACL-HLT, pages 464\u2013468. Dan Shen and Mirella Lapata. 2007. Using seman- tic roles to improve question answering.",
      "Dan Shen and Mirella Lapata. 2007. Using seman- tic roles to improve question answering. In Pro- ceedings of the 2007 Joint Conference on Empirical",
      "Methods in Natural Language Processing and Com- putational Natural Language Learning (EMNLP- CoNLL), pages 12\u201321, Prague, Czech Republic. As- sociation for Computational Linguistics. R. A. Sinoara, R. G. Rossi, and S. O. Rezende. 2016. Semantic role-based representations in text classi\ufb01- cation. In 2016 23rd International Conference on Pattern Recognition (ICPR), pages 2313\u20132318. Asher Stern and Ido Dagan. 2014. Recognizing im- plied predicate-argument relationships in textual in- ference. In Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguis- tics, pages 739\u2013744, Baltimore, Maryland. Associa- tion for Computational Linguistics. Emma Strubell, Patrick Verga, Daniel Andor, David Weiss, and Andrew McCallum. 2018. Linguistically-informed self-attention for semantic role labeling.",
      "Associa- tion for Computational Linguistics. Emma Strubell, Patrick Verga, Daniel Andor, David Weiss, and Andrew McCallum. 2018. Linguistically-informed self-attention for semantic role labeling. In Proceedings of the 2018 Confer- ence on Empirical Methods in Natural Language Processing, pages 5027\u20135038. Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth. 2003. Using predicate-argument structures for information extraction. In Proceed- ings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 8\u201315, Sap- poro, Japan. Association for Computational Linguis- tics. Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu\u00b4\u0131s M`arquez, and Joakim Nivre. 2008. The conll 2008 shared task on joint parsing of syntactic and se- mantic dependencies.",
      "Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu\u00b4\u0131s M`arquez, and Joakim Nivre. 2008. The conll 2008 shared task on joint parsing of syntactic and se- mantic dependencies. In CoNLL 2008: Proceedings of the Twelfth Conference on Computational Nat- ural Language Learning, pages 159\u2013177, Manch- ester, England. Coling 2008 Organizing Committee. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. 2015. Re- thinking the inception architecture for computer vi- sion. CoRR, abs/1512.00567. Zhixing Tan, Mingxuan Wang, Jun Xie, Yidong Chen, and Xiaodong Shi. 2017. Deep semantic role label- ing with self-attention. Zhixing Tan, Mingxuan Wang, Jun Xie, Yidong Chen, and Xiaodong Shi. 2018. Deep semantic role label- ing with self-attention.",
      "2017. Deep semantic role label- ing with self-attention. Zhixing Tan, Mingxuan Wang, Jun Xie, Yidong Chen, and Xiaodong Shi. 2018. Deep semantic role label- ing with self-attention. In Proceedings of the Thirty- Second AAAI Conference on Arti\ufb01cial Intelligence,, pages 4929\u20134936. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar- nett, editors, Advances in Neural Information Pro- cessing Systems 30, pages 6000\u20136010. Curran As- sociates, Inc.",
      "Curran As- sociates, Inc. Ralph Weischedel, Eduard Hovy, Mitchell Mar- cus, Martha Palmer, Robert Belvin, Sameer Prad- han, Lance Ramshaw, and Nianwen Xue. 2011. Ontonotes: A large training corpus for enhanced processing. Handbook of Natural Language Pro- cessing and Machine Translation. Springer. Aaron Steven White, Drew Reisinger, Keisuke Sak- aguchi, Tim Vieira, Sheng Zhang, Rachel Rudinger, Kyle Rawlins, and Benjamin Van Durme. 2016. Universal decompositional semantics on universal dependencies. In Proceedings of the 2016 Confer- ence on Empirical Methods in Natural Language Processing, pages 1713\u20131723, Austin, Texas. Asso- ciation for Computational Linguistics. Qingrong Xia, Zhenghua Li, Min Zhang, Meishan Zhang, Guohong Fu, Rui Wang, and Luo Si. 2019. Syntax-aware neural semantic role labeling. In Pro- ceedings of the 56th Annual Meeting of the Associa- tion for Computational Linguistics.",
      "2019. Syntax-aware neural semantic role labeling. In Pro- ceedings of the 56th Annual Meeting of the Associa- tion for Computational Linguistics. Wen-tau Yih, Matthew Richardson, Chris Meek, Ming- Wei Chang, and Jina Suh. 2016. The value of se- mantic parse labeling for knowledge base question answering. In Proceedings of the 54th Annual Meet- ing of the Association for Computational Linguis- tics, pages 201\u2013206, Berlin, Germany. Association for Computational Linguistics. Matthew D. Zeiler. 2012. ADADELTA: an adaptive learning rate method. CoRR, abs/1212.5701. Jie Zhou and Wei Xu. 2015. End-to-end learning of semantic role labeling using recurrent neural net- works. In Proceedings of the 53rd Annual Meet- ing of the Association for Computational Linguistics and the 7th International Joint Conference on Nat- ural Language Processing, pages 1127\u20131137, Bei- jing, China. Association for Computational Linguis- tics."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1910.11204.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":11286,
  "avg_doclen":171.0,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1910.11204.pdf"
    }
  }
}