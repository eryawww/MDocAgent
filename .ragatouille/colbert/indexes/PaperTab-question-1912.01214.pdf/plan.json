{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine Translation Baijun Ji\u2021, Zhirui Zhang\u00a7, Xiangyu Duan\u2020\u2021\u2217, Min Zhang\u2020\u2021, Boxing Chen\u00a7 and Weihua Luo\u00a7 \u2020Institute of Arti\ufb01cial Intelligence, Soochow University, Suzhou, China \u2021School of Computer Science and Technology, Soochow University, Suzhou, China \u00a7Alibaba DAMO Academy, Hangzhou, China \u2021bjji@stu.suda.edu.cn \u2020{xiangyuduan, minzhang}@suda.edu.cn \u00a7{zhirui.zzr,boxing.cbx,weihua.luowh}@alibaba-inc.com Abstract Transfer learning between different language pairs has shown its effectiveness for Neural Machine Translation (NMT) in low-resource scenario. However, existing transfer methods involving a common target language are far from success in the extreme scenario of zero-shot translation, due to the lan- guage space mismatch problem between transferor (the par- ent model) and transferee (the child model) on the source side.",
            "However, existing transfer methods involving a common target language are far from success in the extreme scenario of zero-shot translation, due to the lan- guage space mismatch problem between transferor (the par- ent model) and transferee (the child model) on the source side. To address this challenge, we propose an effective trans- fer learning approach based on cross-lingual pre-training. Our key idea is to make all source languages share the same fea- ture space and thus enable a smooth transition for zero-shot translation. To this end, we introduce one monolingual pre- training method and two bilingual pre-training methods to obtain a universal encoder for different languages. Once the universal encoder is constructed, the parent model built on such encoder is trained with large-scale annotated data and then directly applied in zero-shot translation scenario. Exper- iments on two public datasets show that our approach signif- icantly outperforms strong pivot-based baseline and various multilingual NMT approaches. Introduction Although Neural Machine Translation (NMT) has domi- nated recent research on translation tasks (Wu et al. 2016; Vaswani et al. 2017; Hassan et al.",
            "Introduction Although Neural Machine Translation (NMT) has domi- nated recent research on translation tasks (Wu et al. 2016; Vaswani et al. 2017; Hassan et al. 2018), NMT heavily relies on large-scale parallel data, resulting in poor performance on low-resource or zero-resource language pairs (Koehn and Knowles 2017). Translation between these low-resource languages (e.g., Arabic\u2192Spanish) is usually accomplished with pivoting through a rich-resource language (such as En- glish), i.e., Arabic (source) sentence is translated to En- glish (pivot) \ufb01rst which is later translated to Spanish (tar- get) (Kauers et al. 2002; de Gispert and Mari\u00f1o 2006). However, the pivot-based method requires doubled decoding time and suffers from the propagation of translation errors. One common alternative to avoid pivoting in NMT is transfer learning (Zoph et al. 2016; Nguyen and Chiang 2017; Kocmi and Bojar 2018; Kim et al.",
            "One common alternative to avoid pivoting in NMT is transfer learning (Zoph et al. 2016; Nguyen and Chiang 2017; Kocmi and Bojar 2018; Kim et al. 2019) which lever- ages a high-resource pivot\u2192target model (parent) to ini- \u2217Corresponding Author. Copyright c\u20dd2020, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved. Figure 1: The circle and triangle dots represent source sen- tences in different language l1 and l2, and the square dots means target sentences in language l3. A sample of transla- tion pairs is connected by the dashed line. We would like to force each of the translation pairs has the same latent rep- resentation as the right part of the \ufb01gure so as to transfer l1 \u2192l3 model directly to l2 \u2192l3 model. tialize a low-resource source\u2192target model (child) that is further optimized with a small amount of available paral- lel data.",
            "tialize a low-resource source\u2192target model (child) that is further optimized with a small amount of available paral- lel data. Although this approach has achieved success in some low-resource language pairs, it still performs very poorly in extremely low-resource or zero-resource transla- tion scenario. Speci\ufb01cally, Kocmi and Bojar (2018) reports that without any child model training data, the performance of the parent model on the child test set is miserable. In this work, we argue that the language space mis- match problem, also named domain shift problem (Fu et al. 2015), brings about the zero-shot translation failure in trans- fer learning. It is because transfer learning has no explicit training process to guarantee that the source and pivot lan- guages share the same feature distributions, causing that the child model inherited from the parent model fails in such a situation. For instance, as illustrated in the left of Fig- ure 1, the points of the sentence pair with the same seman- tics are not overlapping in source space, resulting in that the shared decoder will generate different translations de- noted by different points in target space.",
            "For instance, as illustrated in the left of Fig- ure 1, the points of the sentence pair with the same seman- tics are not overlapping in source space, resulting in that the shared decoder will generate different translations de- noted by different points in target space. Actually, transfer learning for NMT can be viewed as a multi-domain problem where each source language forms a new domain. Minimiz- ing the discrepancy between the feature distributions of dif- ferent source languages, i.e., different domains, will ensure the smooth transition between the parent and child models, arXiv:1912.01214v1  [cs.CL]  3 Dec 2019",
            "as shown in the right of Figure 1. One way to achieve this goal is the \ufb01ne-tuning technique, which forces the model to forget the speci\ufb01c knowledge from parent data and learn new features from child data. However, the domain shift problem still exists, and the demand of parallel child data for \ufb01ne- tuning heavily hinders transfer learning for NMT towards the zero-resource setting. In this paper, we explore the transfer learning in a common zero-shot scenario where there are a lot of source\u2194pivot and pivot\u2194target parallel data but no source\u2194target parallel data. In this scenario, we propose a simple but effective transfer approach, the key idea of which is to relieve the burden of the domain shift problem by means of cross-lingual pre-training. To this end, we \ufb01rstly investigate the performance of two exist- ing cross-lingual pre-training methods proposed by Lam- ple and Conneau (2019) in zero-shot translation scenario. Besides, a novel pre-training method called BRidge Lan- guage Modeling (BRLM) is designed to make full use of the source\u2194pivot bilingual data to obtain a universal encoder for different languages.",
            "Besides, a novel pre-training method called BRidge Lan- guage Modeling (BRLM) is designed to make full use of the source\u2194pivot bilingual data to obtain a universal encoder for different languages. Once the universal encoder is con- structed, we only need to train the pivot\u2192target model and then test this model in source\u2192target direction directly. The main contributions of this paper are as follows: \u2022 We propose a new transfer learning approach for NMT which uses the cross-lingual language model pre-training to enable a high performance on zero-shot translation. \u2022 We propose a novel pre-training method called BRLM, which can effectively alleviates the distance between dif- ferent source language spaces. \u2022 Our proposed approach signi\ufb01cantly improves zero-shot translation performance, consistently surpassing pivot- ing and multilingual approaches. Meanwhile, the perfor- mance on supervised translation direction remains the same level or even better when using our method. Related Work In recent years, zero-shot translation in NMT has attracted widespread attention in academic research.",
            "Meanwhile, the perfor- mance on supervised translation direction remains the same level or even better when using our method. Related Work In recent years, zero-shot translation in NMT has attracted widespread attention in academic research. Existing meth- ods are mainly divided into four categories: pivot-based method, transfer learning, multilingual NMT, and unsuper- vised NMT. \u2022 Pivot-based Method is a common strategy to obtain a source\u2192target model by introducing a pivot language. This approach is further divided into pivoting and pivot- synthetic. While the former \ufb01rstly translates a source lan- guage into the pivot language which is later translated to the target language (Kauers et al. 2002; de Gispert and Mari\u00f1o 2006; Utiyama and Isahara 2007), the lat- ter trains a source\u2192target model with pseudo data gener- ated from source-pivot or pivot-target parallel data (Chen et al. 2017; Zheng, Cheng, and Liu 2017).",
            "2017; Zheng, Cheng, and Liu 2017). Although the pivot-based methods can achieve not bad performance, it always falls into a computation-expensive and parameter- vast dilemma of quadratic growth in the number of source languages, and suffers from the error propagation prob- lem (Zhu et al. 2013). \u2022 Transfer Learning is \ufb01rstly introduced for NMT by Zoph et al. (2016), which leverages a high-resource par- ent model to initialize the low-resource child model. On this basis, Nguyen and Chiang (2017) and Kocmi and Bojar (2018) use shared vocabularies for source\/target language to improve transfer learning, while Kim, Gao, and Ney (2019) relieve the vocabulary mismatch by mainly using cross-lingual word embedding. Although these methods are successful in the low-resource scene, they have limited effects in zero-shot translation. \u2022 Multilingual NMT (MNMT) enables training a single model that supports translation from multiple source lan- guages into multiple target languages, even those unseen language pairs (Firat, Cho, and Bengio 2016; Firat et al.",
            "\u2022 Multilingual NMT (MNMT) enables training a single model that supports translation from multiple source lan- guages into multiple target languages, even those unseen language pairs (Firat, Cho, and Bengio 2016; Firat et al. 2016; Johnson et al. 2016; Al-Shedivat and Parikh 2019; Aharoni, Johnson, and Firat 2019). Aside from sim- pler deployment, MNMT bene\ufb01ts from transfer learning where low-resource language pairs are trained together with high-resource ones. However, Gu et al. (2019) point out that MNMT for zero-shot translation easily fails, and is sensitive to the hyper-parameter setting. Also, MNMT usually performs worse than the pivot-based method in zero-shot translation setting (Arivazhagan et al. 2018). \u2022 Unsupervised NMT (UNMT) considers a harder setting, in which only large-scale monolingual corpora are avail- able for training.",
            "Also, MNMT usually performs worse than the pivot-based method in zero-shot translation setting (Arivazhagan et al. 2018). \u2022 Unsupervised NMT (UNMT) considers a harder setting, in which only large-scale monolingual corpora are avail- able for training. Recently, many methods have been pro- posed to improve the performance of UNMT, including using denoising auto-encoder, statistic machine transla- tion (SMT) and unsupervised pre-training (Artetxe et al. 2017; Lample et al. 2018; Ren et al. 2019; Lample and Conneau 2019). Since UNMT performs well between similar languages (e.g., English-German translation), its performance between distant languages is still far from expectation. Our proposed method belongs to the transfer learning, but it is different from traditional transfer methods which train a parent model as starting point. Before training a par- ent model, our approach fully leverages cross-lingual pre- training methods to make all source languages share the same feature space and thus enables a smooth transition for zero-shot translation.",
            "Before training a par- ent model, our approach fully leverages cross-lingual pre- training methods to make all source languages share the same feature space and thus enables a smooth transition for zero-shot translation. Approach In this section, we will present a cross-lingual pre- training based transfer approach. This method is designed for a common zero-shot scenario where there are a lot of source\u2194pivot and pivot\u2194target bilingual data but no source\u2194target parallel data, and the whole training process can be summarized as follows step by step: \u2022 Pre-train a universal encoder with source\/pivot monolin- gual or source\u2194pivot bilingual data. \u2022 Train a pivot\u2192target parent model built on the pre-trained universal encoder with the available parallel data. Dur- ing the training process, we freeze several layers of the pre-trained universal encoder to avoid the degeneracy is- sue (Howard and Ruder 2018).",
            "Figure 2: The overview of BRidge Language Modeling (BRLM). The BRLM extends MLM (Lample and Conneau 2019) to pairs of parallel sentences and leverages explicit alignment information obtained by external aligner tool or additional attention layer to encourage word representation alignment across different languages. \u2022 Directly translate source sentences into target sentences with the parent model, which bene\ufb01ts from the availabil- ity of the universal encoder. The key dif\ufb01culty of this method is to ensure the intermedi- ate representations of the universal encoder are language in- variant. In the rest of this section, we \ufb01rst present two exist- ing methods yet to be explored in zero-shot translation, and then propose a straightforward but effective cross-lingual pre-training method. In the end, we present the whole train- ing and inference protocol for transfer. Masked and Translation Language Model Pretraining Two existing cross-lingual pre-training methods, Masked Language Modeling (MLM) and Translation Language Modeling (TLM), have shown their effectiveness on XNLI cross-lingual classi\ufb01cation task (Lample and Conneau 2019; Huang et al.",
            "2019), but these methods have not been well studied on cross-lingual generation tasks in zero-shot condi- tion. We attempt to take advantage of the cross-lingual abil- ity of the two methods for zero-shot translation. Speci\ufb01cally, MLM adopts the Cloze objective of BERT (Devlin et al. 2018) and predicts the masked words that are randomly selected and replaced with [MASK] to- ken on monolingual corpus. In practice, MLM takes dif- ferent language monolingual corpora as input to \ufb01nd fea- tures shared across different languages. With this method, word pieces shared in all languages have been mapped into a shared space, which makes the sentence representations across different languages close (Pires, Schlinger, and Gar- rette 2019). Since MLM objective is unsupervised and only requires monolingual data, TLM is designed to leverage parallel data when it is available. Actually, TLM is a simple extension of MLM, with the difference that TLM concatenates sentence pair into a whole sentence, and then randomly masks words in both the source and target sentences.",
            "Actually, TLM is a simple extension of MLM, with the difference that TLM concatenates sentence pair into a whole sentence, and then randomly masks words in both the source and target sentences. In this way, the model can either attend to surrounding words or to the trans- lation sentence, implicitly encouraging the model to align the source and target language representations. Note that al- though each sentence pair is formed into one sentence, the positions of the target sentence are reset to count form zero. Bridge Language Model Pretraining Aside from MLM and TLM, we propose BRidge Language Modeling (BRLM) to further obtain word-level representa- tion alignment between different languages. This method is inspired by the assumption that if the feature spaces of dif- ferent languages are aligned very well, the masked words in the corrupted sentence can also be guessed by the con- text of the correspondingly aligned words on the other side. To achieve this goal, BRLM is designed to strengthen the ability to infer words across languages based on alignment information, instead of inferring words within monolingual sentence as in MLM or within the pseudo sentence formed by concatenating sentence pair as in TLM.",
            "To achieve this goal, BRLM is designed to strengthen the ability to infer words across languages based on alignment information, instead of inferring words within monolingual sentence as in MLM or within the pseudo sentence formed by concatenating sentence pair as in TLM. As illustrated in Figure 2, BRLM stacks shared encoder over both side sentences separately. In particular, we design two network structures for BRLM, which are divided into",
            "Hard Alignment (BRLM-HA) and Soft Alignment (BRLM- SA) according to the way of generating the alignment infor- mation. These two structures actually extend MLM into a bilingual scenario, with the difference that BRLM leverages external aligner tool or additional attention layer to explic- itly introduce alignment information during model training. \u2022 Hard Alignment (BRLM-HA). We \ufb01rst use exter- nal aligner tool on source\u2194pivot parallel data to ex- tract the alignment information of sentence pair. Dur- ing model training, given source\u2194pivot sentence pair, BRLM-HA randomly masks some words in source sen- tence and leverages alignment information to obtain the aligned words in pivot sentence for masked words. Based on the processed input, BRLM-HA adopts the Trans- former (Vaswani et al. 2017) encoder to gain the hid- den states for source and pivot sentences respectively. Then the training objective of BRLM-HA is to predict the masked words by not only the surrounding words in source sentence but also the encoder outputs of the aligned words.",
            "2017) encoder to gain the hid- den states for source and pivot sentences respectively. Then the training objective of BRLM-HA is to predict the masked words by not only the surrounding words in source sentence but also the encoder outputs of the aligned words. Note that this training process is also car- ried out in a symmetric situation, in which we mask some words in pivot sentence and obtain the aligned words in the source sentence. \u2022 Soft Alignment (BRLM-SA). Instead of using external aligner tool, BRLM-SA introduces an additional atten- tion layer to learn the alignment information together with model training. In this way, BRLM-SA avoids the effect caused by external wrong alignment information and en- ables many-to-one soft alignment during model training. Similar with BRLM-HA, the training objective of BRLM- SA is to predict the masked words by not only the sur- rounding words in source sentence but also the outputs of attention layer. In our implementation, the attention layer is a multi-head attention layer adopted in Transformer, where the queries come from the masked source sentence, the keys and values come from the pivot sentence.",
            "In our implementation, the attention layer is a multi-head attention layer adopted in Transformer, where the queries come from the masked source sentence, the keys and values come from the pivot sentence. In principle, MLM and TLM can learn some implicit align- ment information during model training. However, the align- ment process in MLM is inef\ufb01cient since the shared word pieces only account for a small proportion of the whole cor- pus, resulting in the dif\ufb01culty of expanding the shared in- formation to align the whole corpus. TLM also lacks effort in alignment between the source and target sentences since TLM concatenates the sentence pair into one sequence, mak- ing the explicit alignment between the source and target in- feasible. BRLM fully utilizes the alignment information to obtain better word-level representation alignment between different languages, which better relieves the burden of the domain shift problem. Transfer Protocol We consider the typical zero-shot translation scenario in which a high resource pivot language has parallel data with both source and target languages, while source and target languages has no parallel data between themselves.",
            "Transfer Protocol We consider the typical zero-shot translation scenario in which a high resource pivot language has parallel data with both source and target languages, while source and target languages has no parallel data between themselves. Our pro- posed cross-lingual pretraining based transfer approach for source\u2192target zero-shot translation is mainly divided into two phrases: the pretraining phase and the transfer phase. Corpus Language Train Dev Test Europarl De-En,En-Fr 1M,1M 2,000 2,000 Fr-En,En-Es 1M,1M 2,000 2,000 Ro-En,En-De 0.6M,1.5M 2,000 1,000 MultiUN Ar-En,En-Es En-Ru 9.7M,11.3M 11.6M 4,000 4,000 Table 1: Data Statistics.",
            "In the pretraining phase, we \ufb01rst pretrain MLM on mono- lingual corpora of both source and pivot languages, and con- tinue to pretrain TLM or the proposed BRLM on the avail- able parallel data between source and pivot languages, in order to build a cross-lingual encoder shared by the source and pivot languages. In the transfer phase, we train pivot\u2192target NMT model initialized by the cross-lingually pre-trained encoder, and \ufb01nally transfer the trained NMT model to source\u2192target translation thanks to the shared encoder. Note that during training pivot\u2192target NMT model, we freeze several layers of the cross-lingually pre-trained encoder to avoid the de- generacy issue. For the more complicated scenario that either the source side or the target side has multiple languages, the encoder and the decoder are also shared across each side languages for ef\ufb01cient deployment of translation between multiple lan- guages.",
            "For the more complicated scenario that either the source side or the target side has multiple languages, the encoder and the decoder are also shared across each side languages for ef\ufb01cient deployment of translation between multiple lan- guages. Experiments Setup We evaluate our cross-lingual pre-training based transfer ap- proach against several strong baselines on two public datat- sets, Europarl (Koehn 2005) and MultiUN (Eisele and Chen 2010), which contain multi-parallel evaluation data to assess the zero-shot performance. In all experiments, we use BLEU as the automatic metric for translation evaluation.1 Datasets. The statistics of Europarl and MultiUN cor- pora are summarized in Table 1. For Europarl corpus, we evaluate on French-English-Spanish (Fr-En-Es), German- English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the target language. We remove the multi-parallel sentences between different training corpora to ensure zero-shot settings.",
            "We remove the multi-parallel sentences between different training corpora to ensure zero-shot settings. We use the devtest2006 as the validation set and the test2006 as the test set for Fr\u2192Es and De\u2192Fr. For distant language pair Ro\u2192De, we extract 1,000 overlapping sentences from new- stest2016 as the test set and the 2,000 overlapping sentences split from the training set as the validation set since there is no of\ufb01cial validation and test sets. For vocabulary, we use 60K sub-word tokens based on Byte Pair Encoding (BPE) (Sennrich, Haddow, and Birch 2015). For MultiUN corpus, we use four languages: English (En) is set as the pivot language, which has parallel data 1We calculate BLEU scores with the multi-bleu.perl script.",
            "Europarl Fr \u2192En \u2192Es De \u2192En \u2192Fr Ro \u2192En \u2192De Direction Fr \u2192Es En \u2192Es De \u2192Fr En \u2192Fr Ro \u2192De En \u2192De Baselines Cross-lingual Transfer (Kim, Gao, and Ney 2019) 18.45 34.01 9.86 34.05 2.02 23.61 MNMT(Johnson et al.",
            "2016) 27.12 34.69 21.36 33.87 9.31 24.09 MNMTAgreement (Al-Shedivat and Parikh 2019) 29.91 33.80 24.45 32.55 - - Pivoting 32.25 34.01 27.79 34.05 14.74 23.61 Proposed Cross-lingual Pretraining Based Transfer MLM 35.96 34.83 27.61 35.66 12.64 22.04 MLM+TLM 36.78 34.73 29.45 35.33 14.39 24.96 MLM+BRLM-HA 36.30 34.98 29.91 34.99 14.21 24.26 MLM+BRLM-SA 37.02 34.92 30.66 35.91 15.62 24.95 Table 2: Results on Europarl test sets. Three pivot settings are conducted in our experiments.",
            "Three pivot settings are conducted in our experiments. In each setting, the left column presents the zero-shot performances (source\u2192target), and the right column denotes the performances in the supervised parent model direction (pivot\u2192target). with other three languages which do not have parallel data between each other. The three languages are Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation be- tween themselves constitutes six zero-shot translation di- rection for evaluation. We use 80K BPE splits as the vo- cabulary. Note that all sentences are tokenized by the tok- enize.perl2 script, and we lowercase all data to avoid a large vocabulary for the MultiUN corpus. Experimental Details. We use traditional transfer learn- ing, pivot-based method and multilingual NMT as our base- lines. For the fair comparison, the Transformer-big model with 1024 embedding\/hidden units, 4096 feed-forward \ufb01lter size, 6 layers and 8 heads per layer is adopted for all trans- lation models in our experiments. We set the batch size to 2400 per batch and limit sentence length to 100 BPE tokens.",
            "We set the batch size to 2400 per batch and limit sentence length to 100 BPE tokens. We set the attn_drop = 0 (a dropout rate on each atten- tion head), which is favorable to the zero-shot translation and has no effect on supervised translation directions (Gu et al. 2019). For the model initialization, we use Facebook\u2019s cross-lingual pretrained models released by XLM3 to initial- ize the encoder part, and the rest parameters are initialized with xavier uniform. We employ the Adam optimizer with lr = 0.0001, twarm_up = 4000 and dropout = 0.1. At decod- ing time, we generate greedily with length penalty \u03b1 = 1.0. Regarding MLM, TLM and BRLM, as mentioned in the pre-training phase of transfer protocol, we \ufb01rst pre-train MLM on monolingual data of both source and pivot lan- guages, then leverage the parameters of MLM to initialize TLM and the proposed BRLM, which are continued to be optimized with source-pivot bilingual data.",
            "In our experi- ments, we use MLM+TLM, MLM+BRLM to represent this training process. For the masking strategy during training, following Devlin et al. (2018), 15% of BPE tokens are se- lected to be masked. Among the selected tokens, 80% of them are replaced with [MASK] token, 10% are replaced 2https:\/\/github.com\/moses-smt\/mosesdecoder\/blob\/RELEASE- 3.0\/scripts\/tokenizer\/tokenizer.perl 3https:\/\/github.com\/facebookresearch\/XLM with a random BPE token, and 10% unchanged. The predic- tion accuracy of masked words is used as a stopping cri- terion in the pre-training stage. Besides, we use fastalign tool (Dyer, Chahuneau, and Smith 2013) to extract word alignments for BRLM-HA. Main Results Table 2 and 3 report zero-shot results on Europarl and Multi-UN evaluation sets, respectively. We compare our ap- proaches with related approaches of pivoting, multilingual NMT (MNMT) (Johnson et al.",
            "Main Results Table 2 and 3 report zero-shot results on Europarl and Multi-UN evaluation sets, respectively. We compare our ap- proaches with related approaches of pivoting, multilingual NMT (MNMT) (Johnson et al. 2016), and cross-lingual transfer without pretraining (Kim, Gao, and Ney 2019). The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero- shot scenario that multilingual NMT systems often fail to beat (Johnson et al. 2016; Al-Shedivat and Parikh 2019; Arivazhagan et al. 2018). Pivoting translates source to pivot then to target in two steps, causing inef\ufb01cient trans- lation process. Our approaches use one encoder-decoder model to translate between any zero-shot directions, which is more ef\ufb01cient than pivoting. Regarding the comparison between transfer approaches, our cross-lingual pretraining based transfer outperforms transfer method that does not use pretraining by a large margin. Results on Europarl Dataset.",
            "Regarding the comparison between transfer approaches, our cross-lingual pretraining based transfer outperforms transfer method that does not use pretraining by a large margin. Results on Europarl Dataset. Regarding comparison be- tween the baselines in table 2, we \ufb01nd that pivoting is the strongest baseline that has signi\ufb01cant advantage over other two baselines. Cross-lingual transfer for languages without shared vocabularies (Kim, Gao, and Ney 2019) manifests the worst performance because of not using source\u2194pivot par- allel data, which is utilized as bene\ufb01cial supervised signal for the other two baselines. Our best approach of MLM+BRLM-SA achieves the sig- ni\ufb01cant superior performance to all baselines in the zero- shot directions, improving by 0.9-4.8 BLEU points over the strong pivoting. Meanwhile, in the supervised direction of pivot\u2192target, our approaches performs even better than the original supervised Transformer thanks to the shared en-",
            "MultiUN Ar,Es,Ru \u2194En Direction Ar \u2192Es Es \u2192Ar Ar \u2192Ru Ru \u2192Ar Es \u2192Ru Ru \u2192Es A-ZST A-ST Baselines Cross-lingual Transfer 10.26 12.44 4.58 4.42 13.80 7.93 8.90 44.73 MNMT(Johnson et al.",
            "2016) 27.40 20.18 15.12 16.19 17.88 27.93 20.78 43.95 Pivotingm 42.29 30.15 27.23 26.16 29.57 40.08 32.58 43.95 Proposed Cross-lingual Pretraining Based Transfer MLM 16.50 23.41 9.61 14.23 22.80 23.66 18.36 44.25 MLM+TLM 25.98 26.55 16.84 20.07 25.91 29.52 24.14 43.71 MLM+BRLM-HA 29.05 27.58 18.10 20.42 25.39 30.96 25.25 44.67 MLM+BRLM-SA 36.01 31.08 25.49 25.06 30.47 36.01 30.68 44.54 Adding Back Translation MNMT* (Gu et al.",
            "2019) 39.72 28.05 24.67 24.43 27.41 38.01 30.38 43.98 MLM 40.98 31.53 26.06 26.69 31.28 40.02 32.76 44.28 MLM+TLM 41.15 29.77 27.61 27.74 31.02 40.37 32.39 44.14 MLM+BRLM-HA 41.74 31.89 27.24 27.54 31.29 40.34 33.35 44.52 MLM+BRLM-SA 44.17 33.20 29.01 28.91 32.53 41.93 34.95 45.49 Table 3: Results on MultiUN test sets. The six zero-shot translation directions are evaluated. The column \u201cA-ZST\" reports av- eraged BLEU of zero-shot translation, while the column \u201cA-ST\" reports averaged BLEU of supervised pivot\u2192target direction.",
            "The six zero-shot translation directions are evaluated. The column \u201cA-ZST\" reports av- eraged BLEU of zero-shot translation, while the column \u201cA-ST\" reports averaged BLEU of supervised pivot\u2192target direction. (a) Fr-En (b) De-En (c) Ro-En Figure 3: Cosine similarity between sentence representation of each encoder layer across all source-pivot sentence pairs in the Europarl validation set. coder trained on both large-scale monolingual data and par- allel data between multiple languages. MLM alone that does not use source\u2194pivot parallel data performs much better than the cross-lingual transfer, and achieves comparable results to pivoting. When MLM is combined with TLM or the proposed BRLM, the perfor- mance is further improved. MLM+BRLM-SA performs the best, and is better than MLM+BRLM-HA indicating that soft alignment is helpful than hard alignment for the cross- lingual pretraining. Results on MultiUN Dataset. Like experimental results on Europarl, MLM+BRLM-SA performs the best among all proposed cross-lingual pretraining based transfer ap- proaches as shown in Table 3.",
            "Results on MultiUN Dataset. Like experimental results on Europarl, MLM+BRLM-SA performs the best among all proposed cross-lingual pretraining based transfer ap- proaches as shown in Table 3. When comparing systems consisting of one encoder-decoder model for all zero-shot translation, our approaches performs signi\ufb01cantly better than MNMT (Johnson et al. 2016). Although it is challenging for one model to translate all zero-shot directions between multiple distant language pairs of MultiUN, MLM+BRLM-SA still achieves better perfor- mances on Es \u2192Ar and Es \u2192Ru than strong pivotingm, which uses MNMT to translate source to pivot then to tar- get in two separate steps with each step receiving supervised signal of parallel corpora. Our approaches surpass pivotingm in all zero-shot directions by adding back translation (Sen- nrich, Haddow, and Birch 2015) to generate pseudo parallel sentences for all zero-shot directions based on our pretrained models such as MLM+BRLM-SA, and further training our universal encoder-decoder model with these pseudo data. Gu et al.",
            "Gu et al. (2019) introduces back translation into MNMT, while we adopt it in our transfer approaches. Finally, our best MLM+BRLM-SA with back translation outperforms pivotingm by 2.4 BLEU points averagely, and outperforms MNMT (Gu et al. 2019) by 4.6 BLEU points averagely. Again, in supervised translation directions, MLM+BRLM-",
            "SA with back translation also achieves better performance than the original supervised Transformer. Analysis Sentence Representation. We \ufb01rst evaluate the represen- tational invariance across languages for all cross-lingual pre- training methods. Following Arivazhagan et al. (2018), we adopt max-pooling operation to collect the sentence rep- resentation of each encoder layer for all source-pivot sen- tence pairs in the Europarl validation sets. Then we calcu- late the cosine similarity for each sentence pair and aver- age all cosine scores. As shown in Figure 3, we can ob- serve that, MLM+BRLM-SA has the most stable and similar cross-lingual representations of sentence pairs on all layers, while it achieves the best performance in zero-shot transla- tion. This demonstrates that better cross-lingual representa- tions can bene\ufb01t for the process of transfer learning. Besides, MLM+BRLM-HA is not as superior as MLM+BRLM- SA and even worse than MLM+TLM on Fr-En, since MLM+BRLM-HA may suffer from the wrong alignment knowledge from an external aligner tool.",
            "Besides, MLM+BRLM-HA is not as superior as MLM+BRLM- SA and even worse than MLM+TLM on Fr-En, since MLM+BRLM-HA may suffer from the wrong alignment knowledge from an external aligner tool. We also \ufb01nd an in- teresting phenomenon that as the number of layers increases, the cosine similarity decreases. Contextualized Word Representation. We further sam- ple an English-Russian sentence pair from the MultiUN validation sets and visualize the cosine similarity between hidden states of the top encoder layer to further investi- gate the difference of all cross-lingual pre-training meth- ods. As shown in Figure 4, the hidden states generated by MLM+BRLM-SA have higher similarity for two aligned words. It indicates that MLM+BRLM-SA can gain bet- ter word-level representation alignment between source and pivot languages, which better relieves the burden of the do- main shift problem. The Effect of Freezing Parameters. To freeze parame- ters is a common strategy to avoid catastrophic forgetting in transfer learning (Howard and Ruder 2018).",
            "The Effect of Freezing Parameters. To freeze parame- ters is a common strategy to avoid catastrophic forgetting in transfer learning (Howard and Ruder 2018). Table 4 shows the performance of transfer learning with freezing different layers on MultiUN test set, in which En\u2192Ru denotes the parent model, Ar\u2192Ru and Es\u2192Ru are two child models, and all models are based on MLM+BRLM-SA. We can \ufb01nd that updating all parameters during training will cause a no- table drop on the zero-shot direction due to the catastrophic forgetting. On the contrary, freezing all the parameters leads to the decline on supervised direction because the language features extracted during pre-training is not suf\ufb01cient for MT task. Freezing the \ufb01rst four layers of the transformer shows the best performance and keeps the balance between pre-training and \ufb01ne-tuning.",
            "On the contrary, freezing all the parameters leads to the decline on supervised direction because the language features extracted during pre-training is not suf\ufb01cient for MT task. Freezing the \ufb01rst four layers of the transformer shows the best performance and keeps the balance between pre-training and \ufb01ne-tuning. Conclusion In this paper, we propose a cross-lingual pretraining based transfer approach for the challenging zero-shot translation task, in which source and target languages have no parallel data, while they both have parallel data with a high resource (a) MLM (b) MLM+TLM (c) MLM+BRLM-HA (d) MLM+BRLM-SA Figure 4: Cosine similarity visualization at word level given an English-Russian sentence pair from the MultiUN valida- tion sets. Brighter indicates higher similarity. Freezing Layers En \u2192Ru Ar \u2192Ru Es \u2192Ru None 37.80 16.09 19.80 2 37.79 21.47 28.35 4 37.55 25.49 30.47 6 35.31 22.90 28.22 Table 4: BLEU score of freezing different layers.",
            "The num- ber in Freezing Layers column denotes that the number of encoder layers will not be updated. pivot language. With the aim of building the language in- variant representation between source and pivot languages for smooth transfer of the parent model of pivot\u2192target di- rection to the child model of source\u2192target direction, we in- troduce one monolingual pretraining method and two bilin- gual pretraining methods to construct an universal encoder for the source and pivot languages. Experiments on public datasets show that our approaches signi\ufb01cantly outperforms several strong baseline systems, and manifest the language invariance characteristics in both sentence level and word level neural representations. Acknowledgments We would like to thank the anonymous reviewers for the helpful comments. This work was supported by National Key R&D Program of China (Grant No. 2016YFE0132100), National Natural Science Foundation of China (Grant No. 61525205, 61673289). This work was also partially sup- ported by Alibaba Group through Alibaba Innovative Re- search Program and the Priority Academic Program Devel- opment (PAPD) of Jiangsu Higher Education Institutions.",
            "References Aharoni, R.; Johnson, M.; and Firat, O. 2019. Massively multilingual neural machine translation. In NAACL-HLT. Al-Shedivat, M., and Parikh, A. P. 2019. Consistency by agreement in zero-shot neural machine translation. In NAACL-HLT. Arivazhagan, N.; Bapna, A.; Firat, O.; Aharoni, R.; Johnson, M.; and Macherey, W. 2018. The missing ingredient in zero- shot neural machine translation. ArXiv abs\/1903.07091. Artetxe, M.; Labaka, G.; Agirre, E.; and Cho, K. 2017. Unsupervised neural machine translation. ArXiv abs\/1710.11041. Chen, Y.; Liu, Y. P.; Cheng, Y.; and Li, V. O. K. 2017. A teacher-student framework for zero-resource neural machine translation. In ACL. de Gispert, A., and Mari\u00f1o, J. B. 2006.",
            "2017. A teacher-student framework for zero-resource neural machine translation. In ACL. de Gispert, A., and Mari\u00f1o, J. B. 2006. Catalan-english sta- tistical machine translation without parallel corpus : Bridg- ing through spanish. Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. In NAACL-HLT. Dyer, C.; Chahuneau, V.; and Smith, N. A. 2013. A simple, fast, and effective reparameterization of ibm model 2. In HLT-NAACL. Eisele, A., and Chen, Y. 2010. Multiun: A multilingual corpus from united nation documents. In LREC. Firat, O.; Sankaran, B.; Al-Onaizan, Y.; Yarman-Vural, F. T.; and Cho, K. 2016. Zero-resource translation with multi- lingual neural machine translation.",
            "In LREC. Firat, O.; Sankaran, B.; Al-Onaizan, Y.; Yarman-Vural, F. T.; and Cho, K. 2016. Zero-resource translation with multi- lingual neural machine translation. In EMNLP. Firat, O.; Cho, K.; and Bengio, Y. 2016. Multi-way, mul- tilingual neural machine translation with a shared attention mechanism. In HLT-NAACL. Fu, Y.; Hospedales, T. M.; Xiang, T. Y.; and Gong, S. 2015. Transductive multi-view zero-shot learning. IEEE Transactions on Pattern Analysis and Machine Intelligence 37:2332\u20132345. Gu, J.; Wang, Y.; Cho, K.; and Li, V. O. K. 2019. Improved zero-shot neural machine translation via ignoring spurious correlations. In ACL.",
            "IEEE Transactions on Pattern Analysis and Machine Intelligence 37:2332\u20132345. Gu, J.; Wang, Y.; Cho, K.; and Li, V. O. K. 2019. Improved zero-shot neural machine translation via ignoring spurious correlations. In ACL. Hassan, H.; Aue, A.; Chen, C.; Chowdhary, V.; Clark, J. R.; Federmann, C.; Huang, X.; Junczys-Dowmunt, M.; Lewis, W.; Li, M.; Liu, S.; Liu, T. M.; Luo, R.; Menezes, A.; Qin, T.; Seide, F.; Tan, X.; Tian, F.; Wu, L.; Wu, S.; Xia, Y.; Zhang, D.; Zhang, Z.; and Zhou, M. 2018. Achieving human par- ity on automatic chinese to english news translation. ArXiv abs\/1803.05567. Howard, J., and Ruder, S. 2018. Universal language model \ufb01ne-tuning for text classi\ufb01cation. In ACL.",
            "Achieving human par- ity on automatic chinese to english news translation. ArXiv abs\/1803.05567. Howard, J., and Ruder, S. 2018. Universal language model \ufb01ne-tuning for text classi\ufb01cation. In ACL. Huang, H.; Liang, Y.; Duan, N.; Gong, M.; Shou, L.; Jiang, D.; and Zhou, M. 2019. Unicoder: A universal language encoder by pre-training with multiple cross-lingual tasks. ArXiv abs\/1909.00964. Johnson, M.; Schuster, M.; Le, Q. V.; Krikun, M.; Wu, Y.; Chen, Z.; Thorat, N.; Vi\u00e9gas, F. B.; Wattenberg, M.; Cor- rado, G. S.; Hughes, M.; and Dean, J. 2016. Google\u2019s mul- tilingual neural machine translation system: Enabling zero- shot translation. Transactions of the Association for Com- putational Linguistics 5:339\u2013351.",
            "2016. Google\u2019s mul- tilingual neural machine translation system: Enabling zero- shot translation. Transactions of the Association for Com- putational Linguistics 5:339\u2013351. Kauers, M.; Vogel, S.; F\u00fcgen, C.; and Waibel, A. H. 2002. Interlingua based statistical machine translation. In INTER- SPEECH. Kim, Y.; Petrov, P.; Petrushkov, P.; Khadivi, S.; and Ney, H. 2019. Pivot-based transfer learning for neural ma- chine translation between non-english languages. ArXiv abs\/1909.09524. Kim, Y.; Gao, Y.; and Ney, H. 2019. Effective cross-lingual transfer of neural machine translation models without shared vocabularies. In ACL. Kocmi, T., and Bojar, O. 2018. Trivial transfer learning for low-resource neural machine translation. In WMT. Koehn, P., and Knowles, R. 2017. Six challenges for neural machine translation.",
            "In ACL. Kocmi, T., and Bojar, O. 2018. Trivial transfer learning for low-resource neural machine translation. In WMT. Koehn, P., and Knowles, R. 2017. Six challenges for neural machine translation. In NMT@ACL. Koehn, P. 2005. Europarl: A parallel corpus for statistical machine translation. Lample, G., and Conneau, A. 2019. Cross-lingual language model pretraining. ArXiv abs\/1901.07291. Lample, G.; Ott, M.; Conneau, A.; Denoyer, L.; and Ran- zato, M. 2018. Phrase-based & neural unsupervised ma- chine translation. In EMNLP. Nguyen, T. Q., and Chiang, D. 2017. Transfer learning across low-resource, related languages for neural machine translation. In IJCNLP. Pires, T.; Schlinger, E.; and Garrette, D. 2019.",
            "Nguyen, T. Q., and Chiang, D. 2017. Transfer learning across low-resource, related languages for neural machine translation. In IJCNLP. Pires, T.; Schlinger, E.; and Garrette, D. 2019. How multi- lingual is multilingual bert? In ACL. Ren, S.; Zhang, Z.; Liu, S.; Zhou, M.; and Ma, S. 2019. Un- supervised neural machine translation with smt as posterior regularization. In AAAI. Sennrich, R.; Haddow, B.; and Birch, A. 2015. Neural ma- chine translation of rare words with subword units. In ACL. Utiyama, M., and Isahara, H. 2007. A comparison of pivot methods for phrase-based statistical machine translation. In HLT-NAACL. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017.",
            "In HLT-NAACL. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At- tention is all you need. In NIPS. Wu, Y.; Schuster, M.; Chen, Z.; Le, Q. V.; Norouzi, M.; Macherey, W.; Krikun, M.; Cao, Y.; Gao, Q.; Macherey, K.; et al. 2016. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144. Zheng, H.; Cheng, Y.; and Liu, Y. P. 2017. Maximum ex- pected likelihood estimation for zero-resource neural ma- chine translation. In IJCAI. Zhu, X.; He, Z.; Wu, H.; Wang, H.; Zhu, C.; and Zhao, T. 2013.",
            "2017. Maximum ex- pected likelihood estimation for zero-resource neural ma- chine translation. In IJCAI. Zhu, X.; He, Z.; Wu, H.; Wang, H.; Zhu, C.; and Zhao, T. 2013. Improving pivot-based statistical machine translation using random walk. In EMNLP. Zoph, B.; Yuret, D.; May, J.; and Knight, K. 2016. Trans- fer learning for low-resource neural machine translation. In EMNLP."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1912.01214.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 9233.000061035156,
    "avg_doclen_est": 177.5576934814453
}
