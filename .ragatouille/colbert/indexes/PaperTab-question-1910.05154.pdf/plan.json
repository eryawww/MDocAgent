{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "How Does Language In\ufb02uence Documentation Work\ufb02ow? Unsupervised Word Discovery Using Translations in Multiple Languages Marcely Zanon Boito1 Aline Villavicencio2,3 Laurent Besacier1 (1) Laboratoire d\u2019Informatique de Grenoble (LIG), UGA, G-INP, CNRS, INRIA, France (2) Department of Computer Science, University of Shef\ufb01eld, England (3) Institute of Informatics (INF), UFRGS, Brazil contact: marcely.zanon-boito@univ-grenoble-alpes.fr R\u00c9SUM\u00c9 Comment la langue in\ufb02uence le processus de documentation ? D\u00e9couverte non supervis\u00e9e de mots bas\u00e9e sur des traductions en langues multiples Pour la documentation des langues, la transcription est un processus tr\u00e8s co\u00fbteux : une minute d\u2019enregistrement n\u00e9cessiterait environ une heure et demie de travail pour un linguiste (Austin and Sallabank, 2013).",
            "R\u00e9cemment, la collecte de traductions (dans des langues bien document\u00e9es) align\u00e9es aux enregistrements est devenue une solution populaire pour garantir l\u2019interpr\u00e9tabilit\u00e9 des enregistrements (Adda et al., 2016) et aider \u00e0 leur traitement automatique. Dans cet article, nous \u00e9tudions l\u2019impact de la langue de traduction sur les approches automatiques en documentation des langues. Nous traduisons un corpus parall\u00e8le bilingue Mboshi-Fran\u00e7ais (Godard et al., 2017) dans quatre autres langues, et \u00e9valuons l\u2019impact de la langue de traduction sur une t\u00e2che de segmentation en mots non supervis\u00e9e. Nos r\u00e9sultats sugg\u00e8rent que la langue de traduction peut in\ufb02uencer l\u00e9g\u00e8rement la qualit\u00e9 de segmentation. Cependant, combiner l\u2019information apprise par diff\u00e9rents mod\u00e8les bilingues nous permet d\u2019am\u00e9liorer ces r\u00e9sultats de mani\u00e8re marginale.",
            "Nos r\u00e9sultats sugg\u00e8rent que la langue de traduction peut in\ufb02uencer l\u00e9g\u00e8rement la qualit\u00e9 de segmentation. Cependant, combiner l\u2019information apprise par diff\u00e9rents mod\u00e8les bilingues nous permet d\u2019am\u00e9liorer ces r\u00e9sultats de mani\u00e8re marginale. ABSTRACT For language documentation initiatives, transcription is an expensive resource: one minute of audio is estimated to take one hour and a half on average of a linguist\u2019s work (Austin and Sallabank, 2013). Recently, collecting aligned translations in well-resourced languages became a popular solution for ensuring posterior interpretability of the recordings (Adda et al., 2016). In this paper we investigate language-related impact in automatic approaches for computational language documentation. We translate the bilingual Mboshi-French parallel corpus (Godard et al., 2017) into four other languages, and we perform bilingual-rooted unsupervised word discovery. Our results hint towards an impact of the well-resourced language in the quality of the output. However, by combining the information learned by different bilingual models, we are only able to marginally increase the quality of the segmentation.",
            "Our results hint towards an impact of the well-resourced language in the quality of the output. However, by combining the information learned by different bilingual models, we are only able to marginally increase the quality of the segmentation. MOTS-CL\u00c9S : d\u00e9couverte non supervis\u00e9e du lexique, documentation des langues, approches multilingues. KEYWORDS: unsupervised word discovery, language documentation, multilingual approaches. arXiv:1910.05154v1  [cs.CL]  11 Oct 2019",
            "1 Introduction The Cambridge Handbook of Endangered Languages (Austin and Sallabank, 2011) estimates that at least half of the 7,000 languages currently spoken worldwide will no longer exist by the end of this century. For these endangered languages, data collection campaigns have to accommodate the challenge that many of them are from oral tradition, and producing transcriptions is costly. This transcription bottleneck problem can be handled by translating into a widely spoken language to ensure subsequent interpretability of the collected recordings, and such parallel corpora have been recently created by aligning the collected audio with translations in a well-resourced language (Adda et al., 2016; Godard et al., 2017; Boito et al., 2018). Moreover, some linguists suggested that more than one translation should be collected to capture deeper layers of meaning (Evans and Sasse, 2004). This work is a contribution to the Computational Language Documentation (CLD) research \ufb01eld, that aims to replace part of the manual steps performed by linguists during language documentation initiatives by automatic approaches. Here we investigate the unsupervised word discovery and segmentation task, using the bilingual-rooted approach from Godard et al.",
            "Here we investigate the unsupervised word discovery and segmentation task, using the bilingual-rooted approach from Godard et al. (2018). There, words in the well-resourced language are aligned to unsegmented phonemes in the endangered language in order to identify group of phonemes, and to cluster them into word-like units. We experiment with the Mboshi-French parallel corpus, translating the French text into four other well-resourced languages in order to investigate language impact in this CLD approach. Our results hint that this language impact exists, and that models based on different languages will output different word-like units. 2 Methodology The Multilingual Mboshi Parallel Corpus: In this work we extend the bilingual Mboshi-French parallel corpus (Godard et al., 2017), fruit of the documentation process of Mboshi (Bantu C25), an endangered language spoken in Congo-Brazzaville. The corpus contains 5,130 utterances, for which it provides audio, transcriptions and translations in French. We translate the French into four other well-resourced languages through the use of the DeepL translator.1 The languages added to the dataset are: English, German, Portuguese and Spanish.",
            "The corpus contains 5,130 utterances, for which it provides audio, transcriptions and translations in French. We translate the French into four other well-resourced languages through the use of the DeepL translator.1 The languages added to the dataset are: English, German, Portuguese and Spanish. Table 1 shows some statistics for the produced Multilingual Mboshi parallel corpus.2 Bilingual Unsupervised Word Segmentation\/Discovery Approach: We use the bilingual neural- based Unsupervised Word Segmentation (UWS) approach from Godard et al. (2018) to discover words in Mboshi. In this approach, Neural Machine Translation (NMT) models are trained between language pairs, using as source language the translation (word-level) and as target, the language to document (unsegmented phonemic sequence). Due to the attention mechanism present in these networks (Bahdanau et al., 2014), posterior to training, it is possible to retrieve soft-alignment probability matrices between source and target sequences. These matrices give us sentence-level source-to-target alignment information, and by using it for clustering neighbor phonemes aligned to the same translation word, we are able to create segmentation in the target side.",
            "These matrices give us sentence-level source-to-target alignment information, and by using it for clustering neighbor phonemes aligned to the same translation word, we are able to create segmentation in the target side. The product of this approach is a set of (discovered-units, translation words) pairs. 1Available at https:\/\/www.deepl.com\/translator 2Available at https:\/\/github.com\/mzboito\/mmboshi",
            "Table 1: Statistics for the Multilingual Mboshi parallel corpus. The French text is used for generating translation in the four other languages present in the right side of the table. Table 2: From left to right, results for: bilingual UWS, multilingual leveraging by voting, ANE selection. Multilingual Leveraging: In this work we apply two simple methods for including multilingual information into the bilingual models from Godard et al. (2018). The \ufb01rst one, Multilingual Voting, consists of merging the information learned by models trained with different language pairs by performing a voting over the \ufb01nal discovered boundaries. The voting is performed by applying an agreement threshold T over the output boundaries. This threshold balances between accepting all boundaries from all the bilingual models (zero agreement) and accepting only input boundaries discovered by all these models (total agreement). The second method is ANE Selection. For every language pair and aligned sentence in the dataset, a soft-alignment probability matrix is generated. We use Average Normalized Entropy (ANE) (Boito et al., 2019a) computed over these matrices for selecting the most con\ufb01dent one for segmenting each phoneme sequence.",
            "For every language pair and aligned sentence in the dataset, a soft-alignment probability matrix is generated. We use Average Normalized Entropy (ANE) (Boito et al., 2019a) computed over these matrices for selecting the most con\ufb01dent one for segmenting each phoneme sequence. This exploits the idea that models trained on different language pairs will have language-related behavior, thus differing on the resulting alignment and segmentation over the same phoneme sequence. 3 Experiments The experiment settings from this paper and evaluation protocol for the Mboshi corpus (Boundary F-scores using the ZRC speech reference) are the same from Boito et al. (2019a). Table 2 presents the results for bilingual UWS and multilingual leveraging. For the former, we reach our best result by using as aligned information the French, the original aligned language for this dataset. Languages closely related to French (Spanish and Portuguese) ranked better, while our worst result used German. English also performs notably well in our experiments. We believe this is due to the statistics features of the resulting text. We observe in Table 1 that the English portion of the dataset contains the smallest vocabulary among all languages.",
            "English also performs notably well in our experiments. We believe this is due to the statistics features of the resulting text. We observe in Table 1 that the English portion of the dataset contains the smallest vocabulary among all languages. Since we train our systems in very low-resource settings, vocabulary- related features can impact greatly the system\u2019s capacity to language-model, and consequently the \ufb01nal quality of the produced alignments. Even in high-resource settings, it was already attested that some languages are more dif\ufb01cult to model than others (Cotterell et al., 2018). For the multilingual selection experiments, we experimented combining the languages from top to bottom as they appear Table 2 (ranked by performance; e.g. 1-3 means the combination of FR(1),",
            "Table 3: Top 10 con\ufb01dent (discovered type, translation) pairs for the \ufb01ve bilingual models. The \u201c+\u201d mark means the discovered type is a concatenation of two existing true types. EN(2) and PT(3)). We observe that the performance improvement is smaller than the one observed in previous work (Boito et al., 2019b), which we attribute to the fact that our dataset was arti\ufb01cially augmented. This could result in the available multilingual form of supervision not being as rich as in a manually generated dataset. Finally, the best boundary segmentation result is obtained by performing multilingual voting with all the languages and an agreement of 50%, which indicates that the information learned by different languages will provide additional complementary evidence. Lastly, following the methodology from Boito et al. (2019a), we extract the most con\ufb01dent alignments (in terms of ANE) discovered by the bilingual models.",
            "Lastly, following the methodology from Boito et al. (2019a), we extract the most con\ufb01dent alignments (in terms of ANE) discovered by the bilingual models. Table 3 presents the top 10 most con\ufb01dent (discovered type, translation) pairs.3 Looking at the pairs the bilingual models are most con\ufb01dent about, we observe there are some types discovered by all the bilingual models (e.g. Mboshi word itua, and the concatenation obo\u00e1+ng\u00e1). However, the models still differ for most of their alignments in the table. This hints that while a portion of the lexicon might be captured independently of the language used, other structures might be more dependent of the chosen language. On this note, Haspelmath (2011) suggests the notion of word cannot always be meaningfully de\ufb01ned cross-linguistically. 4 Conclusion In this work we train bilingual UWS models using the endangered language Mboshi as target and different well-resourced languages as aligned information. Results show that similar languages rank better in terms of segmentation performance, and that by combining the information learned by different models, segmentation is further improved.",
            "4 Conclusion In this work we train bilingual UWS models using the endangered language Mboshi as target and different well-resourced languages as aligned information. Results show that similar languages rank better in terms of segmentation performance, and that by combining the information learned by different models, segmentation is further improved. This might be due to the different language- dependent structures that are captured by using more than one language. Lastly, we extend the bilingual Mboshi-French parallel corpus, creating a multilingual corpus for the endangered language Mboshi that we make available to the community. 3The Mboshi phoneme sequences were replaced by their grapheme equivalents to increase readability, but all results were computed using phonemes.",
            "References Adda, G., St\u00fcker, S., Adda-Decker, M., Ambouroue, O., Besacier, L., Blachon, D., Bonneau- Maynard, H., Godard, P., Hamlaoui, F., Idiatov, D., Kouarata, G.-N., Lamel, L., Makasso, E.-M., Rialland, A., de Velde, M. V., Yvon, F., and Zerbian, S. (2016). Breaking the unwritten language barrier: The BULB project. Procedia Computer Science, 81:8\u201314. Austin, P. K. and Sallabank, J. (2011). The Cambridge handbook of endangered languages. Cam- bridge University Press. Austin, P. K. and Sallabank, J. (2013). Endangered languages. Taylor & Francis. Bahdanau, D., Cho, K., and Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate.",
            "(2013). Endangered languages. Taylor & Francis. Bahdanau, D., Cho, K., and Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473. Boito, M. Z., Anastasopoulos, A., Lekakou, M., Villavicencio, A., and Besacier, L. (2018). A small griko-italian speech translation corpus. arXiv preprint arXiv:1807.10740. Boito, M. Z., Villavicencio, A., and Besacier, L. (2019a). Empirical evaluation of sequence-to- sequence models for word discovery in low-resource settings. arXiv preprint arXiv:1907.00184. Boito, M. Z., Villavicencio, A., and Besacier, L. (2019b). Leveraging translations in multiple languages for low-resource unsupervised word segmentation. Unpublished work. Paper under review.",
            "Boito, M. Z., Villavicencio, A., and Besacier, L. (2019b). Leveraging translations in multiple languages for low-resource unsupervised word segmentation. Unpublished work. Paper under review. Cotterell, R., Mielke, S. J., Eisner, J., and Roark, B. (2018). Are all languages equally hard to language-model? arXiv preprint arXiv:1806.03743. Evans, N. and Sasse, H.-J. (2004). Searching for meaning in the library of babel: \ufb01eld semantics and problems of digital archiving. Open Conference Systems, University of Sydney, Faculty of Arts. Godard, P., Adda, G., Adda-Decker, M., Benjumea, J., Besacier, L., Cooper-Leavitt, J., Kouarata, G.-N., Lamel, L., Maynard, H., M\u00fcller, M., et al. (2017). A very low resource language speech corpus for computational language documentation experiments.",
            "(2017). A very low resource language speech corpus for computational language documentation experiments. arXiv preprint arXiv:1710.03501. Godard, P., Zanon Boito, M., Ondel, L., Berard, A., Yvon, F., Villavicencio, A., and Besacier, L. (2018). Unsupervised word segmentation from speech with attention. In Interspeech. Haspelmath, M. (2011). The indeterminacy of word segmentation and the nature of morphology and syntax. Folia linguistica, 45(1):31\u201380."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1910.05154.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 512,
    "num_embeddings_est": 3146.9999084472656,
    "avg_doclen_est": 174.8333282470703
}
