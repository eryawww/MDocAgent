[
  "Causal Explanation Analysis on Social Media Youngseo Son, Nipun Bayas, and H. Andrew Schwartz Stony Brook University Stony Brook, NY {yson,nbayas,has}@cs.stonybrook.edu Abstract Understanding causal explanations \u2014 reasons given for happenings in one\u2019s life \u2014 has been found to be an important psychological factor linked to physical and mental health. Causal explanations are often studied through man- ual identi\ufb01cation of phrases over limited sam- ples of personal writing. Automatic identi\ufb01- cation of causal explanations in social media, while challenging in relying on contextual and sequential cues, offers a larger-scale alterna- tive to expensive manual ratings and opens the door for new applications (e.g. studying pre- vailing beliefs about causes, such as climate change). Here, we explore automating causal explanation analysis, building on discourse parsing, and presenting two novel subtasks: causality detection (determining whether a causal explanation exists at all) and causal ex- planation identi\ufb01cation (identifying the spe- ci\ufb01c phrase that is the explanation).",
  "We achieve strong accuracies for both tasks but \ufb01nd different approaches best: an SVM for causality prediction (F1 = 0.791) and a hier- archy of Bidirectional LSTMs for causal ex- planation identi\ufb01cation (F1 = 0.853). Fi- nally, we explore applications of our complete pipeline (F1 = 0.868), showing demographic differences in mentions of causal explanation and that the association between a word and sentiment can change when it is used within a causal explanation. 1 Introduction Explanations of happenings in one\u2019s life, causal explanations, are an important topic of study in so- cial, psychological, economic, and behavioral sci- ences. For example, psychologists have analyzed people\u2019s causal explanatory style (Peterson et al., 1988) and found strong negative relationships with depression, passivity, and hostility, as well as pos- itive relationships with life satisfaction, quality of Figure 1: A casual relation characterizes the connec- tion between two discourse arguments, one of which is the causal explanation.",
  "life, and length of life (Scheier et al., 1989; Carver and Gaines, 1987; Peterson et al., 1988). To help understand the signi\ufb01cance of causal explanations, consider how they are applied to measuring optimism (and its converse, pes- simism) (Peterson et al., 1988). For example, in \u201cMy parser failed because I always have bugs.\u201d, the emphasized text span is considered a causal explanation which indicates pessimistic personal- ity \u2013 a negative event where the author believes the cause is pervasive. However, in \u201cMy parser failed because I barely worked on the code.\u201d, the expla- nation would be considered a signal of optimistic personality \u2013 a negative event for which the cause is believed to be short-lived. Language-based models which can detect causal explanations from everyday social media language can be used for more than automating optimism detection.",
  "Language-based models which can detect causal explanations from everyday social media language can be used for more than automating optimism detection. Language-based assessments would enable other large-scale downstream tasks: tracking prevailing causal beliefs (e.g., about cli- mate change or autism), better extracting process knowledge from non-\ufb01ction (e.g., gravity causes objects to move toward one another), or detecting attribution of blame or praise in product or service reviews (\u201cI loved this restaurant because the \ufb01sh was cooked to perfection\u201d). arXiv:1809.01202v2  [cs.CL]  18 Oct 2018",
  "In this paper, we introduce causal explanation analysis and its subtasks of detecting the presence of causality (causality prediction) and identifying explanatory phrases (causal explanation identi\ufb01- cation). There are many challenges to achiev- ing these task. First, the ungrammatical texts in social media incur poor syntactic parsing results which drastically affect the performance of dis- course relation parsing pipelines 1. Many causal relations are implicit and do not contain any dis- course markers (e.g., \u2018because\u2019). Further, Explicit causal relations are also more dif\ufb01cult in social media due to the abundance of abbreviations and variations of discourse connectives (e.g., \u2018cuz\u2019 and \u2018bcuz\u2019). Prevailing approaches for social media analy- ses, utilizing traditional linear models or bag of words models (e.g., SVM trained with n-gram, part-of-speech (POS) tags, or lexicon-based fea- tures) alone do not seem appropriate for this task since they simply cannot segment the text into meaningful discourse units or discourse arguments 2 such as clauses or sentences rather than random consecutive token sequences or speci\ufb01c word to- kens.",
  "Even when the discourse units are clear, parsers may still fail to accurately identify dis- course relations since the content of social media is quite different than that of newswire which is typically used for discourse parsing. In order to overcome these dif\ufb01culties of dis- course relation parsing in social media, we sim- plify and minimize the use of syntactic parsing re- sults and capture relations between discourse ar- guments, and investigate the use of a recursive neural network model (RNN). Recent work has shown that RNNs are effective for utilizing dis- course structures for their downstream tasks (Ji and Smith, 2017; Bhatia et al., 2015; Wieting et al., 2015; Paulus et al., 2014), but they have yet to be directly used for discourse relation predic- tion in social media. We evaluated our model by comparing it to off-the-shelf end-to-end discourse relation parsers and traditional models.",
  "We evaluated our model by comparing it to off-the-shelf end-to-end discourse relation parsers and traditional models. We found that the SVM and random forest classi\ufb01ers work better than the LSTM classi\ufb01er for the causality 1Off-the-shelf Penn Discourse Treebank (PDTB) end-to- end parsers perform poorly on our Facebook causal predic- tion dataset (see Table 3) 2Each discourse relation theory uses a different term for minimal discourse text spans: \u2018Elementary Discourse Unit (EDU)\u2019 in RST and \u2018Discourse Argument\u2019 in PDTB. We will call it \u2018Discourse Argument\u2019 in this paper, since we adapted the PDTB text segmentation method. detection, while the LSTM classi\ufb01er outperforms other models for identifying causal explanation.",
  "We will call it \u2018Discourse Argument\u2019 in this paper, since we adapted the PDTB text segmentation method. detection, while the LSTM classi\ufb01er outperforms other models for identifying causal explanation. The contributions of this work include: (1) the proposal of models for both (a) causality predic- tion and (b) causal explanation identi\ufb01cation, (2) the extensive evaluation of a variety of models from social media classi\ufb01cation models and dis- course relation parsers to RNN-based application models, demonstrating that feature-based models work best for causality prediction while RNNs are superior for the more dif\ufb01cult task of causal ex- planation identi\ufb01cation, (3) performance analysis on architectural differences of the pipeline and the classi\ufb01er structures, (4) exploration of the applica- tions of causal explanation to downstream tasks, and (5) release of a novel, anonymized causality Facebook dataset along with our causality predic- tion and causal explanation identi\ufb01cation models. 2 Related Work Identifying causal explanations in documents can be viewed as discourse relation parsing.",
  "2 Related Work Identifying causal explanations in documents can be viewed as discourse relation parsing. The Penn Discourse Treebank (PDTB) (Prasad et al., 2007) has a \u2018Cause\u2019 and \u2018Pragmatic Cause\u2019 dis- course type under a general \u2018Contingency\u2019 class and Rhetorical Structure Theory (RST) (Mann and Thompson, 1987) has a \u2018Relations of Cause\u2019. In most cases, the development of discourse parsers has taken place in-domain, where researchers have used the existing annotations of discourse argu- ments in newswire text (e.g. Wall Street Jour- nal) from the discourse treebank and focused on exploring different features and optimizing vari- ous types of models for predicting relations (Pitler et al., 2009; Park and Cardie, 2012; Zhou et al., 2010). In order to further develop automated sys- tems, researchers have proposed end-to-end dis- course relation parsers, building models which are trained and evaluated on the annotated PDTB and RST Discourse Treebank (RST DT).",
  "In order to further develop automated sys- tems, researchers have proposed end-to-end dis- course relation parsers, building models which are trained and evaluated on the annotated PDTB and RST Discourse Treebank (RST DT). These cor- pora consist of documents from Wall Street Jour- nal (WSJ) which are much more well-organized and grammatical than social media texts (Biran and McKeown, 2015; Lin et al., 2014; Ji and Eisenstein, 2014; Feng and Hirst, 2014). Only a few works have attempted to parse dis- course relations for out-of-domain problems such as text categorizations on social media texts; Ji and Bhatia used models which are pretrained with RST DT for building discourse structures from movie reviews, and Son adapted the PDTB discourse re-",
  "lation parsing approach for capturing counterfac- tual conditionals from tweets (Bhatia et al., 2015; Ji and Smith, 2017; Son et al., 2017). These works had substantial differences to what propose in this paper. First, Ji and Bhatia used a pretrained model (not fully optimal for some parts of the given task) in their pipeline; Ji\u2019s model performed worse than the baseline on the categorization of legisla- tive bills, which is thought to be due to legisla- tive discourse structures differing from those of the training set (WSJ corpus). Bhatia also used a pretrained model \ufb01nding that utilizing discourse relation features did not boost accuracy (Bhatia et al., 2015; Ji and Smith, 2017). Both Bhatia and Son used manual schemes which may limit the coverage of certain types of positive samples\u2013 Bhatia used a hand-crafted schema for weighting discourse structures for the neural network model and Son manually developed seven surface forms of counterfactual thinking for the rule-based sys- tem (Bhatia et al., 2015; Son et al., 2017).",
  "We use social-media-speci\ufb01c features from pretrained models which are directly trained on tweets and we avoid any hand-crafted rules except for those included in the existing discourse argument ex- traction techniques. The automated systems for discourse relation parsing involve multiple subtasks from segment- ing the whole text into discourse arguments to classifying discourse relations between the argu- ments. Past research has found that different types of models and features yield varying performance for each subtask. Some have optimized models for discourse relation classi\ufb01cation (i.e. given a doc- ument indicating if the relation existing) without discourse argument parsing using models such as Naive-Bayes or SVMs, achieve relatively stronger accuracies but a simpler task than that associated with discourse arguments (Park and Cardie, 2012; Zhou et al., 2010; Pitler et al., 2009). Researchers who, instead, tried to build the end-to-end parsing pipelines considered a wider range of approaches including sequence models and RNNs (Biran and McKeown, 2015; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li et al., 2014).",
  "Particularly, when they tried to utilize the discourse struc- tures for out-domain applications, they used RNN- based models and found that those models are advantageous for their downstream tasks (Bhatia et al., 2015; Ji and Smith, 2017). In our case, for identifying causal explana- tions from social media using discourse structure, we build an RNN-based model for its structural effectiveness in this task (see details in section 3.2). However, we also note that simpler models such as SVMs and logistic regression obtained the state-of-the-art performances for text categoriza- tion tasks in social media (Lynn et al., 2017; Mo- hammad et al., 2013), so we build relatively simple models with different properties for each stage of the full pipeline of our parser.",
  "3 Methods We build our model based on PDTB-style dis- course relation parsing since PDTB has a rela- tively simpler text segmentation method;3 for ex- plicit discourse relations, it \ufb01nds the presence of discourse connectives within a document and ex- tracts discourse arguments which parametrize the connective while for implicit relations, it consid- ers all adjacent sentences as candidate discourse arguments. 3.1 Dataset We created our own causal explanation dataset by collecting 3,268 random Facebook status update messages. Three well-trained annotators manually labeled whether or not each message contains the causal explanation and obtained 1,598 causality messages with substantial agreement (\u03ba = 0.61). We used the majority vote for our gold standard. Then, on each causality message, annotators iden- ti\ufb01ed which text spans are causal explanations. For each task, we used 80% of the dataset for training our model and 10% for tuning the hy- perparameters of our models. Finally, we evalu- ated all of our models on the remaining 10% (Ta- ble 1 and Table 2).",
  "For each task, we used 80% of the dataset for training our model and 10% for tuning the hy- perparameters of our models. Finally, we evalu- ated all of our models on the remaining 10% (Ta- ble 1 and Table 2). For causal explanation detec- tion task, we extracted discourse arguments using our parser and selected discourse arguments which most cover the annotated causal explanation text span as our gold standard. 3.2 Model We build two types of models. First, we de- velop feature-based models which utilize features of the successful models in social media analysis and causal relation discourse parsing. Then, we 3RST parsing builds fully hierarchical discourse tree structures out of the whole span of target text which highly depends on syntactic parsing and exact matching of elemen- tary discourse units which are extremely hard to obtain from social media texts",
  "Dataset Causality Non-Causal Total Training 1,284 1,330 2,614 Validation 150 177 327 Test 164 163 327 Total 1,598 1,670 3,268 Table 1: Number of messages containing causality or not in our dataset. Causality messages CE DA Total DA Training 1,278 5,606 Validation 160 652 Test 160 757 Total 1,598 7,015 Table 2: The number of discourse arguments in causal- ity messages. Across 1,598 total causality messages, we found 7,015 discourse arguments (Total DA) and the one which covers annotated causal explanation are used as causal explanation discourse arguments (CE DA) build a recursive neural network model which uses distributed representation of discourse arguments as this approach can even capture latent proper- ties of causal relations which may exist between distant discourse arguments. We speci\ufb01cally se- lected bidirectional LSTM since the model with the discourse distributional structure built in this form outperformed the traditional models in simi- lar NLP downstream tasks (Ji and Smith, 2017).",
  "We speci\ufb01cally se- lected bidirectional LSTM since the model with the discourse distributional structure built in this form outperformed the traditional models in simi- lar NLP downstream tasks (Ji and Smith, 2017). Discourse Argument Extraction As the \ufb01rst step of our pipeline, we use Tweebo parser (Kong et al., 2014) to extract syntactic features from mes- sages. Then, we demarcate sentences using punc- tuation (\u2018,\u2019) tag and periods. Among those sen- tences, we \ufb01nd discourse connectives de\ufb01ned in PDTB annotation along with a Tweet POS tag for conjunction words which can also be a discourse marker. In order to decide whether these connec- tives are really discourse connectives (e.g., I went home, but he stayed) as opposed to simple con- nections of two words (I like apple and banana) we see if verb phrases 4 exist before and after the connective by using dependency parsing results.",
  "Although discourse connective disambiguation is a complicated task which can be much improved by syntactic features (Pitler and Nenkova, 2009), we try to minimize effects of syntactic parsing and simplify it since it is highly error-prone in social 4minimal discourse unit is verb phrases with very few ex- ceptions (Prasad et al., 2007) media. Finally, according to visual inspection, emojis (\u2018E\u2019 tag) are crucial for discourse relation in social media so we take them as separate dis- course arguments (e.g.,in \u201cMy test result... :(\u201d the sad feeling is caused by the test result, but it can- not be captured by plain word tokens). Feature Based Models We trained a linear SVM, an rbf SVM, and a random forest with N- gram, charater N-gram, and tweet POS tags, senti- ment tags, average word lengths and word counts from each message as they have a pivotal role in the models for many NLP downstream tasks in so- cial media (Mohammad et al., 2013; Lynn et al., 2017).",
  "In addition to these features, we also ex- tracted First-Last, First3 features and Word Pairs from every adjacent pair of discourse arguments since these features were most helpful for causal relation prediction (Pitler et al., 2009). First-Last, First3 features are \ufb01rst and last word and \ufb01rst three words of two discourse arguments of the relation, and Word Pairs are the cross product of words of those discourse arguments. These two features en- able our model to capture interaction between two discourse arguments. (Pitler et al., 2009) reported that these two features along with verbs, modal- ity, context, and polarity (which can be captured by N-grams, sentiment tags and POS tags in our previous features) obtained the best performance for predicting Contingency class to which causal- ity belongs. Recursive Neural Network Model We load the GLOVE word embedding (Pennington et al., 2014) trained in Twitter 5 for each token of ex- tracted discourse arguments from messages.",
  "Recursive Neural Network Model We load the GLOVE word embedding (Pennington et al., 2014) trained in Twitter 5 for each token of ex- tracted discourse arguments from messages. For the distributional representation of discourse ar- guments, we run a Word-level LSTM on the words\u2019 embeddings within each discourse argu- ment and concatenate last hidden state vectors of forward LSTM (\u2212\u2192h ) and backward LSTM (\u2190\u2212h ) which is suggested by (Ji and Smith, 2017) (DA = [\u2212\u2192h ; \u2190\u2212h ]). Then, we feed the sequence of the vector representation of discourse arguments to the Discourse-argument-level LSTM (DA-level LSTM) to make a \ufb01nal prediction with log soft- max function. With this structure, the model can learn the representation of interaction of tokens inside each discourse argument, then capture dis- course relations across all of the discourse argu- 5http://nlp.stanford.edu/data/glove. twitter.27B.zip",
  "Figure 2: LSTM classi\ufb01er for causality detection and explanation identi\ufb01cation ments in each message (Figure 2). In order to prevent the over\ufb01tting, we added a dropout layer between the Word-level LSTM and the DA-level LSTM layer. Architectural Variants We also explore subsets of the full RNN architecture, speci\ufb01cally with one of the two LSTM layers removed. In the \ufb01rst model variant, we directly input all word embed- dings of a whole message to a BiLSTM layer and make prediction (Word LSTM) without the help of the distributional vector representations of dis- course arguments. In the second model variant, we take the average of all word embeddings of each discourse argument (DAk = 1 Nk PNk i=1 Wi), and use them as inputs to a BiLSTM layer (DA AVG LSTM) as the average vector of embeddings were quite effective for representing the whole se- quence (Ji and Smith, 2017; Wieting et al., 2015).",
  "As with the full architectures, for CP both of these variants ends with a many-to-one classi\ufb01cation per message, while the CEI model ends with a se- quence of classi\ufb01cations. 3.3 Experiment Feature Based Model We explored three types of models (RBF SVM, Linear SVM, and Ran- dom Forest Classi\ufb01er) which have previously been shown empirically useful for the language analy- sis in social media. We \ufb01ltered out low frequency Word Pairs features as they tend to be noisy and sparse (Pitler et al., 2009). Then, we conducted univariate feature selection to restrict all remain- ing features to those showing at least a small rela- tionship with the outcome. Speci\ufb01cally, we keep all features passing a family-wise error rate of \u03b1 = 60 with the given outcome. After comparing the performance of the optimized version of each model, we also conducted a feature ablation test on the best model in order to see how much each feature contributes to the causality prediction.",
  "After comparing the performance of the optimized version of each model, we also conducted a feature ablation test on the best model in order to see how much each feature contributes to the causality prediction. Neural Network Model We used bidirectional LSTMs for causality classi\ufb01cation and causal ex- planation identi\ufb01cation since the discourse argu- ments for causal explanation can show up either before and after the effected events or results and we want our model to be optimized for both cases. However, there is a risk of over\ufb01tting due to the dataset which is relatively small for the high com- plexity of the model, so we added a dropout layer (p=0.3) between the Word-level LSTM and the DA-level LSTM. For tuning our model, we explore the dimen- sionality of word vector and LSTM hidden state vectors of discourse arguments of 25, 50, 100, and 200 as pretrained GLOVE vectors were trained in this setting. For optimization, we used Stochastic Gradient Descent (SGD) and Adam (Kingma and Ba, 2014) with learning rates 0.01 and 0.001.",
  "For optimization, we used Stochastic Gradient Descent (SGD) and Adam (Kingma and Ba, 2014) with learning rates 0.01 and 0.001. We ignore missing word embeddings because our dataset is quite small for retraining new word embeddings. However, if embeddings are ex- tracted as separate discourse arguments, we used the average of all vectors of all discourse argu- ments in that message. Average embeddings have performed well for representing text sequences in other tasks (Wieting et al., 2015).",
  "Model F1 (Biran and McKeown, 2015) 0.434 (Lin et al., 2014) 0.638 Linear SVM 0.791 RBF SVM 0.777 Random Forest 0.771 LSTM 0.758 Table 3: Causality prediction performance across dif- ferent predictive models. Bold indicates signi\ufb01cant im- provement over the LSTM Model F1 All 0.791 - First-Last, First3 0.788 - Word Pairs 0.787 - POS tags 0.734 - (Char + Word) N-grams 0.769 - Sentiment tags 0.791 Table 4: Feature ablation test of Linear SVM for causality prediction Model Evaluation We \ufb01rst use state-of-the-art PDTB taggers for our baseline (Lin et al., 2014; Biran and McKeown, 2015) for the evaluation of the causality prediction of our models ((Biran and McKeown, 2015) requires sentences extracted from the text as its input, so we used our parser to extract sentences from the message).",
  "Then, we compare how models work for each task and disassembled them to inspect how each part of the models can affect their \ufb01nal prediction perfor- mances. We conducted McNemar\u2019s test to deter- mine whether the performance differences are sta- tistically signi\ufb01cant at p < .05. 4 Results We investigated various models for both causal- ity detection and explanation identi\ufb01cation. Based on their performances on the task, we analyzed the relationships between the types of models and the tasks, and scrutinized further for the best per- forming models. For performance analysis, we re- ported weighted F1 of classes. 4.1 Causality Prediction In order to classify whether a message contains causal relation, we compared off-the-shelf PDTB parsers, linear SVM, RBF SVM, Random forest and LSTM classi\ufb01ers.",
  "For performance analysis, we re- ported weighted F1 of classes. 4.1 Causality Prediction In order to classify whether a message contains causal relation, we compared off-the-shelf PDTB parsers, linear SVM, RBF SVM, Random forest and LSTM classi\ufb01ers. The off-the-shelf parsers achieved the lowest accuracies ((Biran and McK- Model Prec Rec F1 Linear SVM 0.773 0.727 0.743 RBF SVM 0.739 0.771 0.749 Random Forest 0.747 0.790 0.746 LSTM 0.851 0.858 0.853 Table 5: Causal explanation identi\ufb01cation perfor- mance. Bold indicates signi\ufb01cant imrpovement over next best model (p < .05) eown, 2015) and (Lin et al., 2014) in Table 3). This result can be expected since 1) these mod- els were trained with news articles and 2) they are trained for all possible discourse relations in ad- dition to causal relations (e.g., contrast, condition, etc).",
  "This result can be expected since 1) these mod- els were trained with news articles and 2) they are trained for all possible discourse relations in ad- dition to causal relations (e.g., contrast, condition, etc). Among our suggested models, SVM and ran- dom forest classi\ufb01er performed better than LSTM and, in the general trend, the more complex the models were, the worse they performed. This sug- gests that the models with more direct and simpler learning methods with features might classify the causality messages better than the ones more op- timized for capturing distributional information or non-linear relationships of features. Causality Classi\ufb01er Analysis Table 4 shows the results of a feature ablation test to see how each feature contributes to causality classi\ufb01cation per- formance of the linear SVM classi\ufb01er. POS tags caused the largest drop in F1. We suspect POS tags played a unique role because discourse con- nectives can have various surface forms (e.g., be- cause, cuz, bcuz, etc) but still the same POS tag \u2018P\u2019.",
  "POS tags caused the largest drop in F1. We suspect POS tags played a unique role because discourse con- nectives can have various surface forms (e.g., be- cause, cuz, bcuz, etc) but still the same POS tag \u2018P\u2019. Also POS tags can capture the occurrences of modal verbs, a feature previously found to be very useful for detecting similar discourse rela- tions (Pitler et al., 2009). N-gram features caused 0.022 F1 drop while sentiment tags did not af- fect the model when removed. Unlike the previ- ous work where First-Last, First3 and Word pairs tended to gain a large F1 increase for multiclass discourse relation prediction, in our case, they did not affect the prediction performance compared to other feature types such as POS tags or N-grams. 4.2 Causal Explanation Identi\ufb01cation In this task, the model identi\ufb01es causal explana- tions given the discourse arguments of the causal- ity message.",
  "4.2 Causal Explanation Identi\ufb01cation In this task, the model identi\ufb01es causal explana- tions given the discourse arguments of the causal- ity message. We explored over the same mod- els as those we used for causality (sans the out- put layer), and found the almost opposite trend of performances (see Table 5). The Linear SVM ob-",
  "Model CP (F1) CEI (F1) Full LSTM 0.758 0.853 DA AVG LSTM 0.685 0.818 Word LSTM 0.694 0.792 Table 6: The effect of Word-level LSTM (Word LSTM) and discourse argument LSTM (DA AVG LSTM) for causality prediction (CP) and causal expla- nation identi\ufb01cation (CEI). Note that, as described in methods, there are architectural differences for CP and CEI models with the same names, most notably that the output layer is always a single classi\ufb01cation for CP and a sequence of classi\ufb01cations for CEI. tained lowest F1 while the LSTM model made the best identi\ufb01cation performance. As opposed to the simple binary classi\ufb01cation of the causality mes- sages, in order to detect causal explanation, it is more bene\ufb01cial to consider the relation across dis- course arguments of the whole message and im- plicit distributional representation due to the im- plicit causal relations between two distant argu- ments.",
  "4.3 Architectural Variants For causality prediction, we experimented with only word tokens in the whole message without help of Word-level LSTM layer (Word LSTM), and F1 dropped by 0.064 (CP in Table 6). Also, when we used the average of the sequence of word embeddings of each discourse argument as an input to the DA-level LSTM and it caused F1 drop of 0.073. This suggests that the informa- tion gained from both the interaction of words in and in between discourse arguments help when the model utilizes the distributional representation of the texts. For causal explanation identi\ufb01cation, in order to test how the LSTM classi\ufb01er works without its capability of capturing the relations between dis- course arguments, we removed DA-level LSTM layer and ran the LSTM directly on the word em- bedding sequence for each discourse argument for classifying whether the argument is causal expla- nation, and the model had 0.061 F1 drop (Word LSTM in CEI in Table 6). Also, when we ran DA- level LSTM on the average vectors of the word se- quences of each discourse argument of messages, F1 decreased to 0.818.",
  "Also, when we ran DA- level LSTM on the average vectors of the word se- quences of each discourse argument of messages, F1 decreased to 0.818. This follows the similar pattern observed from other types of models per- formances (i.e., SVMs and Random Forest classi- \ufb01ers) that the models with higher complexity for Model Prec Rec F1 CP + CEIcausal 0.864 0.877 0.868 CP + CEIall 0.842 0.864 0.848 CEIcausal Only 0.847 0.788 0.810 CEIall Only 0.836 0.848 0.842 Table 7: The effect of Linear SVM Cauality model (CP) within our pipeline. CEIall: LSTM CEI models trained on all messages; CEIcausal: LSTM CEI mod- els trained only on causality messages (CEIcausal); CP + CEIall|causal: the combination of Linear SVM and each LSTM model.",
  "CEIall: LSTM CEI models trained on all messages; CEIcausal: LSTM CEI mod- els trained only on causality messages (CEIcausal); CP + CEIall|causal: the combination of Linear SVM and each LSTM model. Bold: signi\ufb01cant (p < .05) in- crease in F1 over the next best model, suggesting the two-step approach worked best. capturing the interaction of discourse arguments tend to identify causal explanation with the higher accuracies. For CEI task, we found that when the model ran on the sequence representation of discourse ar- gument (DA AVG LSTM), its performance was higher than the plain sequence of word embed- dings (Word LSTM). Finally, in both subtasks, when the models ran on both Word-level and DA- Level (Full LSTM), they obtained the highest per- formance. 4.4 Complete Pipeline Evaluations thus far zeroed-in on each subtask of causal explanation analysis (i.e. CEI only focused on data already identi\ufb01ed to contain causal expla- nations).",
  "4.4 Complete Pipeline Evaluations thus far zeroed-in on each subtask of causal explanation analysis (i.e. CEI only focused on data already identi\ufb01ed to contain causal expla- nations). Here, we seek to evaluate the complete pipeline of CP and CEI, starting from all of test data (those or without causality) and evaluating the \ufb01nal accuracy of CEI predictions. This is intended to evaluate CEI performance under an applied set- ting where one does not already know whether a document has a causal explanation. There are several approaches we could take to perform CEI starting from unannotated data. We could simply run CEI prediction by itself (CEI Only) or the pipeline of CP \ufb01rst and then only run CEI on documents predicted as causal (CP + CEI). Further, the CEI model could be trained only on those documents annotated causal (as was done in the previous experiments) or on all train- ing documents including many that are not causal. Table 7 show results varying the pipeline and how CEI was trained.",
  "Further, the CEI model could be trained only on those documents annotated causal (as was done in the previous experiments) or on all train- ing documents including many that are not causal. Table 7 show results varying the pipeline and how CEI was trained. Though all setups per- formed decent (F1 > 0.81) we see that the pipelined approach, \ufb01rst predicting causality (with the linear SVM) and then predicting causal expla-",
  "nations only for those with marked causal (CP + CEIcausal) yielded the strongest results. This also utilized the CEI model only trained on those anno- tated causal. Besides performance, an added ben- e\ufb01t from this two step approach is that the CP step is less computational intensive of the CEI step and approximately 2/3 of documents will never need the CEI step applied. Limitations. We had an inevitable limitation on the size of our dataset, since there is no other causality dataset over social media and the anno- tation required an intensive iterative process. This might have limited performances of more com- plex models, but considering the processing time and the computation load, the combination of the linear model and the RNN-based model of our pipeline obtained both the high performance and ef\ufb01ciency for the practical applications to down- stream tasks. In other words, it\u2019s possible the lin- ear model will not perform as well if the training size is increased substantially.",
  "In other words, it\u2019s possible the lin- ear model will not perform as well if the training size is increased substantially. However, a linear model could still be used to do a \ufb01rst-pass, com- putationally ef\ufb01cient labeling, in order to short- list social media posts for further labeling from an LSTM or more complex model. 5 Exploration Here, we explore the use of causal explanation analysis for downstream tasks. First we look at the relationship between use of causal explanation and one\u2019s demographics: age and gender. Then, we consider their use in sentiment analysis for extract- ing the causes of polarity ratings. Research involv- ing human subjects was approved by the Univer- sity of Pennsylvania Institutional Review Board. Demographic differences. We \ufb01rst explored variance in number of causality posts by de- mographics. To do this, we used self-authored posts from a random 300 consenting-users of the MyPersonality dataset (Kosinski et al., 2013).",
  "Demographic differences. We \ufb01rst explored variance in number of causality posts by de- mographics. To do this, we used self-authored posts from a random 300 consenting-users of the MyPersonality dataset (Kosinski et al., 2013). For each user we calculate a cp ratio, de\ufb01ned as the number of causality predicted posts divided by their total number of posts, indicating the percent- age of their posts which include a causal explana- tion. We then correlated this ratio with real-valued age using Pearson correlation and looked the dif- ferences by dichotomous gender using Cohen\u2019s d (the difference in standardized means; only bi- nary gender was available). We found signi\ufb01cant (p < .05) moderate-sized associations for both, CE Non-CE Top Ngrams Top Ngrams 1 worst not 2 was no 3 not \u201d 4 the worst asked 5 horrible she 6 rude told 7 bad said 8 overpriced minutes 9 over ?",
  "10 slow me Table 8: Top words most associated with negative re- views from within causal explanations (CE) and out- side of causal explanation (Non-CE). indicating both older individuals and females were likely to use more causal explanations. Causality in Sentiment Analysis We explored the application of causality explanation identi\ufb01- cation for sentiment analysis using the Yelp po- larity dataset (Zhang et al., 2015). We randomly selected 10,000 of both positive and negative re- views and ran our complete pipeline on them to ex- tract the causal explanations from the reviews. We then analyzed the ngrams from (a) causal expla- nation and (b) all other discourse arguments test- ing for associations with polarity. We used the a Bayesian interpretation of the log odds ratio us- ing an informative dirichlet prior de\ufb01ned by Mon- roe et al. (2008). We found difference in the top ngrams depending on whether the argument the ngram originated from was a causal explanation or not (see Table 8). Top ngrams for causal expla- nations included more content words (e.g.",
  "(2008). We found difference in the top ngrams depending on whether the argument the ngram originated from was a causal explanation or not (see Table 8). Top ngrams for causal expla- nations included more content words (e.g. \u2018rude\u2019, \u2018overpriced\u2019, \u2018slow\u2019) suggesting analyzing causal explanations within reviews can better target the reasons for the negative review. 6 Conclusion We developed a pipeline for causal explanation analysis over social media text, including both causality prediction and causal explanation iden- ti\ufb01cation. We examined a variety of model types and RNN architectures for each part of the pipeline, \ufb01nding an SVM best for causality pre- diction and a hierarchy of BiLSTMs for causal ex- planation identi\ufb01cation, suggesting the later task relies more heavily on sequential information. In fact, we found replacing either layer of the hier-",
  "archical LSTM architecture (the word-level or the DA-level) with a an equivalent \u201cbag of features\u201d approach resulted in reduced accuracy. Results of our whole pipeline of causal explanation analysis were found quite strong, achieving an F1 = 0.868 at identifying discourse arguments that are causal explanations. Finally, we demonstrated use of our models in applications, \ufb01nding associations between demo- graphics and rate of mentioning causal explana- tions, as well as showing differences in the top words predictive of negative ratings in Yelp re- views. Utilization of discourse structure in social media analysis has been a largely untapped area of exploration, perhaps due to its perceived dif\ufb01- culty. We hope the strong results of causal expla- nation identi\ufb01cation here leads to the integration of more syntax and deeper semantics into social media analyses and ultimately enables new appli- cations beyond the current state of the art. Acknowledgments This work was supported, in part, by a grant from the Templeton Religion Trust (ID #TRT0048). The funders had no role in study design, data col- lection and analysis, decision to publish, or prepa- ration of the manuscript.",
  "Acknowledgments This work was supported, in part, by a grant from the Templeton Religion Trust (ID #TRT0048). The funders had no role in study design, data col- lection and analysis, decision to publish, or prepa- ration of the manuscript. We also thank Laura Smith, Yiyi Chen, Greta Jawel and Vanessa Her- nandez for their work in identifying causal expla- nations. References Parminder Bhatia, Yangfeng Ji, and Jacob Eisen- stein. 2015. Better document-level sentiment anal- ysis from rst discourse parsing. arXiv preprint arXiv:1509.01599. Or Biran and Kathleen McKeown. 2015. Pdtb dis- course parsing as a tagging task: The two taggers approach. In Proceedings of the 16th Annual Meet- ing of the Special Interest Group on Discourse and Dialogue, pages 96\u2013104. Charles S Carver and Joan Gollin Gaines. 1987. Opti- mism, pessimism, and postpartum depression.",
  "In Proceedings of the 16th Annual Meet- ing of the Special Interest Group on Discourse and Dialogue, pages 96\u2013104. Charles S Carver and Joan Gollin Gaines. 1987. Opti- mism, pessimism, and postpartum depression. Cog- nitive therapy and Research, 11(4):449\u2013462. Vanessa Wei Feng and Graeme Hirst. 2014. A linear- time bottom-up discourse parser with constraints and post-editing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), volume 1, pages 511\u2013521. Yangfeng Ji and Jacob Eisenstein. 2014. Represen- tation learning for text-level discourse parsing. In ACL (1), pages 13\u201324. Yangfeng Ji and Noah Smith. 2017. Neural discourse structure for text categorization. arXiv preprint arXiv:1702.01829. Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization.",
  "Yangfeng Ji and Noah Smith. 2017. Neural discourse structure for text categorization. arXiv preprint arXiv:1702.01829. Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Lingpeng Kong, Nathan Schneider, Swabha Swayamdipta, Archna Bhatia, Chris Dyer, and Noah A Smith. 2014. A dependency parser for tweets. Michal Kosinski, David Stillwell, and Thore Grae- pel. 2013. Private traits and attributes are pre- dictable from digital records of human behavior. Proceedings of the National Academy of Sciences, 110(15):5802\u20135805. Jiwei Li, Rumeng Li, and Eduard Hovy. 2014. Recur- sive deep models for discourse parsing. In Proceed- ings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2061\u20132069.",
  "2014. Recur- sive deep models for discourse parsing. In Proceed- ings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2061\u20132069. Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A pdtb-styled end-to-end discourse parser. Natural Language Engineering, 20(02):151\u2013184. Veronica Lynn, Youngseo Son, Vivek Kulkarni, Ni- ranjan Balasubramanian, and H Andrew Schwartz. 2017. Human centered nlp with user-factor adap- tation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Process- ing, pages 1146\u20131155. William C Mann and Sandra A Thompson. 1987. Rhetorical structure theory: A theory of text orga- nization. University of Southern California, Infor- mation Sciences Institute. Saif M Mohammad, Svetlana Kiritchenko, and Xiao- dan Zhu. 2013.",
  "1987. Rhetorical structure theory: A theory of text orga- nization. University of Southern California, Infor- mation Sciences Institute. Saif M Mohammad, Svetlana Kiritchenko, and Xiao- dan Zhu. 2013. Nrc-canada: Building the state- of-the-art in sentiment analysis of tweets. arXiv preprint arXiv:1308.6242. Burt L Monroe, Michael P Colaresi, and Kevin M Quinn. 2008. Fightin\u2019words: Lexical feature selec- tion and evaluation for identifying the content of po- litical con\ufb02ict. Political Analysis, 16(4):372\u2013403. Joonsuk Park and Claire Cardie. 2012. Improving im- plicit discourse relation recognition through feature set optimization. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 108\u2013112. Association for Com- putational Linguistics. Romain Paulus, Richard Socher, and Christopher D Manning. 2014. Global belief recursive neural net- works.",
  "Association for Com- putational Linguistics. Romain Paulus, Richard Socher, and Christopher D Manning. 2014. Global belief recursive neural net- works. In Advances in Neural Information Process- ing Systems, pages 2888\u20132896.",
  "Jeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Nat- ural Language Processing (EMNLP), pages 1532\u2013 1543. Christopher Peterson, Martin E Seligman, and George E Vaillant. 1988. Pessimistic explanatory style is a risk factor for physical illness: a thirty- \ufb01ve-year longitudinal study. Journal of personality and social psychology, 55(1):23. Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Automatic sense prediction for implicit discourse re- lations in text. In Proceedings of the Joint Confer- ence of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Lan- guage Processing of the AFNLP: Volume 2-Volume 2, pages 683\u2013691. Association for Computational Linguistics. Emily Pitler and Ani Nenkova. 2009. Using syntax to disambiguate explicit discourse connectives in text.",
  "Association for Computational Linguistics. Emily Pitler and Ani Nenkova. 2009. Using syntax to disambiguate explicit discourse connectives in text. In Proceedings of the ACL-IJCNLP 2009 Confer- ence Short Papers, pages 13\u201316. Association for Computational Linguistics. Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh, Alan Lee, Aravind Joshi, Livio Robaldo, and Bonnie L Webber. 2007. The penn discourse treebank 2.0 an- notation manual. Michael F Scheier, Karen A Matthews, Jane F Owens, George J Magovern, R Craig Lefebvre, R Anne Ab- bott, and Charles S Carver. 1989. Dispositional op- timism and recovery from coronary artery bypass surgery: the bene\ufb01cial effects on physical and psy- chological well-being. Journal of personality and social psychology, 57(6):1024.",
  "1989. Dispositional op- timism and recovery from coronary artery bypass surgery: the bene\ufb01cial effects on physical and psy- chological well-being. Journal of personality and social psychology, 57(6):1024. Youngseo Son, Anneke Buffone, Joe Raso, Allegra Larche, Anthony Janocko, Kevin Zembroski, H An- drew Schwartz, and Lyle Ungar. 2017. Recogniz- ing counterfactual thinking in social media texts. In Proceedings of the 55th Annual Meeting of the As- sociation for Computational Linguistics (Volume 2: Short Papers), volume 2, pages 654\u2013658. John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2015. Towards universal para- phrastic sentence embeddings. arXiv preprint arXiv:1511.08198. Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text clas- si\ufb01cation. In Advances in neural information pro- cessing systems, pages 649\u2013657.",
  "Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text clas- si\ufb01cation. In Advances in neural information pro- cessing systems, pages 649\u2013657. Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian Su, and Chew Lim Tan. 2010. Predicting discourse connectives for implicit discourse relation recog- nition. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 1507\u20131514. Association for Computational Linguistics."
]