{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Bidirectional Long-Short Term Memory for Video Description Yi Bin1, Yang Yang1, Zi Huang2, Fumin Shen1, Xing Xu1 and Heng Tao Shen2,1 1University of Electronic Science and Technology 2The University of Queensland yi.bin@hotmail.com, dlyyang@gmail.com, huang@itee.uq.edu.au fumin.shen@gmail.com, xing.xu@uestc.edu.cn, shenht@itee.uq.edu.au ABSTRACT Video captioning has been attracting broad research atten- tion in multimedia community. However, most existing ap- proaches either ignore temporal information among video frames or just employ local contextual temporal knowledge. In this work, we propose a novel video captioning framework, termed as Bidirectional Long-Short Term Memory (BiLSTM), which deeply captures bidirectional global temporal struc- ture in video. Speci\ufb01cally, we \ufb01rst devise a joint visual mod- elling approach to encode video data by combining a for- ward LSTM pass, a backward LSTM pass, together with vi- sual features from Convolutional Neural Networks (CNNs).",
      "Speci\ufb01cally, we \ufb01rst devise a joint visual mod- elling approach to encode video data by combining a for- ward LSTM pass, a backward LSTM pass, together with vi- sual features from Convolutional Neural Networks (CNNs). Then, we inject the derived video representation into the subsequent language model for initialization. The bene\ufb01ts are in two folds: 1) comprehensively preserving sequential and visual information; and 2) adaptively learning dense vi- sual features and sparse semantic representations for videos and sentences, respectively. We verify the e\ufb00ectiveness of our proposed video captioning framework on a commonly- used benchmark, i.e., Microsoft Video Description (MSVD) corpus, and the experimental results demonstrate that the superiority of the proposed approach as compared to several state-of-the-art methods. CCS Concepts \u2022Computing methodologies \u2192Video summarization; Keywords Video caption; bidirectional long-short term memory 1. INTRODUCTION With the development of digital media technology and popularity of Mobile Internet, online visual content has in- creased rapidly in recent couple of years.",
      "CCS Concepts \u2022Computing methodologies \u2192Video summarization; Keywords Video caption; bidirectional long-short term memory 1. INTRODUCTION With the development of digital media technology and popularity of Mobile Internet, online visual content has in- creased rapidly in recent couple of years. Subsequently, vi- sual content analysis for retrieving [31, 18] and understand- ing becomes a fundamental problem in the area of multi- media research, which has motivated world-wide researchers ACM ISBN 978-1-4503-2138-9. DOI: 10.1145/1235 to develop advanced techniques. Most previous works, how- ever, have focused on classi\ufb01cation task, such as annotating an image [9, 19] or video [5, 17, 21, 30] with given \ufb01xed la- bel sets. With some pioneering methods [4, 14] tackling the challenge of describing images with natural language pro- posed, visual content understanding has attracted more and more attention. State-of-the-art techniques for image cap- tioning have been surpassed by new advanced approaches in succession [2, 3, 8, 25, 29].",
      "State-of-the-art techniques for image cap- tioning have been surpassed by new advanced approaches in succession [2, 3, 8, 25, 29]. Recent researches [15, 24, 32, 11, 23] have been focusing on describing videos with more com- prehensive sentences instead of simple keywords. Di\ufb00erent from image, video is sequential data with temporal struc- ture, which may pose signi\ufb01cant challenge to video caption. Most of the existing works in video description employed max or mean pooling across video frames to obtain video- level representation, which failed to capture temporal knowl- edge. To address this problem, Yao et al. proposed to use 3- D Convolutional Neural Networks to explore local temporal information in video clips, where the most relevant temporal fragments were automatically chosen for generating natural language description with attention mechanism [32]. In [23], Venugopanlan et al. implemented a Long-Short Term Mem- ory (LSTM) network, a variant of Recurrent Neural Net- works (RNNs), to model the global temporal structure in whole video snippet.",
      "In [23], Venugopanlan et al. implemented a Long-Short Term Mem- ory (LSTM) network, a variant of Recurrent Neural Net- works (RNNs), to model the global temporal structure in whole video snippet. However, these methods failed to ex- ploit bidirectional global temporal structure, which could bene\ufb01t from not only previous video frames, but also in- formation in future frames. Also, existing video captioning schemes cannot adaptively learn dense video representation and generate sparse semantic sentences. In this work, we propose to construct a novel bidirec- tional LSTM (BiLSTM) network for video captioning. More speci\ufb01cally, we design a joint visual modelling to compre- hensively explore bidirectional global temporal information in video data by integrating a forward LSTM pass, a back- ward LSTM pass, together with CNNs features. In order to enhance the subsequent sentence generation, the obtained visual representations are then fed into LSTM-based lan- guage model as initialization.",
      "In order to enhance the subsequent sentence generation, the obtained visual representations are then fed into LSTM-based lan- guage model as initialization. We summarize the main con- tributions of this work as follows: (1) To our best knowl- edge, our approach is one of the \ufb01rst to utilize bidirectional recurrent neural networks for exploring bidirectional global temporal structure in video captioning; (2) We construct two sequential processing models for adaptive video repre- sentation learning and language description generation, re- spectively, rather than using the same LSTM for both video frames encoding and text decoding in [23]; and (3) Exten- sive experiments on a real-world video corpus illustrate the arXiv:1606.04631v1  [cs.MM]  15 Jun 2016",
      "CNN (VGG) Visual Model Language Model FU FU FU FU FU BU BU BU BU BU MU MU MU MU MU SU SU SU SU SU <BOS> a is man is man a riding <EOS> \u2026 \u2026 \u2026 FU BU MU SU Forward Unit Backward Unit Merge Unit Sentence Unit time Forward Pass Backward Pass Merge Figure 1: The overall \ufb02owchart of the proposed video captioning framework. We \ufb01rst extract CNNs features of video frames and feed them into forward pass networks (FU, green box) and backward pass networks (BU, yellow box). We then combine the outputs of hidden states together with the original CNNs features, and pass the integrated sequence to another LSTM (MU, blue box) to generate \ufb01nal video representation. We initialize language model (SU, pink box) with video representation and start to generate words sequentially with <BOS> token, and terminate the process until the <EOS> token is emitted. superiority of our proposal as compared to state-of-the-arts. 2.",
      "We initialize language model (SU, pink box) with video representation and start to generate words sequentially with <BOS> token, and terminate the process until the <EOS> token is emitted. superiority of our proposal as compared to state-of-the-arts. 2. THE PROPOSED APPROACH In this section, we elaborate the proposed video captioning framework, including an introduction of the overall \ufb02owchart (as illustrated in Figure 1), a brief review of LSTM-based Sequential Model, the joint visual modelling with bidirec- tional LSTM and CNNs, as well as the sentence generation process. 2.1 LSTM-based Sequential Model With the success in speech recognition and machine trans- lation tasks, recurrent neural structure, especially LSTM and its variants, have dominated sequence processing \ufb01eld. LSTM has been demonstrated to be able to e\ufb00ectively ad- dress the gradients vanishing or explosion problem [6] dur- ing back-propagation through time (BPTT) [26] and to ex- ploit temporal dependencies in very long temporal struc- ture.",
      "LSTM has been demonstrated to be able to e\ufb00ectively ad- dress the gradients vanishing or explosion problem [6] dur- ing back-propagation through time (BPTT) [26] and to ex- ploit temporal dependencies in very long temporal struc- ture. LSTM incorporates several control gates and a con- stant memory cell, the details of which are following: it = \u03c3 (Wixxt + Wihht\u22121) (1) ft = \u03c3 (Wfxxt + Wihht\u22121) (2) ot = \u03c3 (Woxxt + Wohht\u22121) (3) ct = ft \u2299ct\u22121 + it \u2299\u03c6 (Wcxxt + Wchht\u22121) (4) ht = ot \u2299\u03c6 (ct) (5) where Wmn-like matrices are LSTM weight parameters, \u03c3 and \u03c6 are denote the sigmoid and hyperbolic non-linear functions, respectively, and \u2299indicates element-wise mul- tiplication operation. Inspired by the success of LSTM, we devise an LSTM-based network to investigate the video tem- poral structure for video representation.",
      "Inspired by the success of LSTM, we devise an LSTM-based network to investigate the video tem- poral structure for video representation. Then initializing language model with video representation to generate video description. 2.2 Bidirectional Video Modelling Di\ufb00erent from other video description approaches that represent video by implementing pooling across frames [24] or 3-D CNNs with local temporal structure [15], we ap- ply BiLSTM networks to exploit the bidirectional tempo- ral structure of video clips. Convolutional Neural Networks (CNNs) has demonstrated overwhelming performance on im- age recognition, classi\ufb01cation [9] and video content analy- sis [3, 23]. Therefore, we extract ca\ufb00e [7] fc7 layer of each frame through VGG-16 layers [20] ca\ufb00emodel. Following [23, 24], we sample one frame from every ten frames in the video and extract the fc7 layer, the second fully-connected layer, to express selected frames. Then a T-by-4096 feature ma- trix generated to denote given video clip, where T is the number of frames we sampled in the video.",
      "Then a T-by-4096 feature ma- trix generated to denote given video clip, where T is the number of frames we sampled in the video. As in Figure 1, we then implement two LSTMs, forward pass and back- ward pass, to encode CNNs features of video frames, and then merge the output sequences at each time point with a learnt weight matrix. What is interesting is that at each time point in bidirectional structure, we not only \u201csee\u201d the past frames, but also\u201cpeek\u201dat the future frames. In other words, our bidirectional LSTM structure encodes video by scanning the entire video sequence several times (same as the number of time steps at encoding stage), and each scan is relevant to its adjacent scans. To investigate the e\ufb00ect of reinforce- ment of original CNNs feature, we combine the merged hid- den states of BiLSTM structure and fc7 representation time step-wise. We further employ another forward pass LSTM network with incorporated sequence to generate our video representation. In [27, 28], Wu et al.",
      "We further employ another forward pass LSTM network with incorporated sequence to generate our video representation. In [27, 28], Wu et al. had demonstrated that using the output of the last step could perform better than pooling approach across outputs of all the time steps in video classi\ufb01cation task. Similarly, we represent the entire video clip using the state of memory cell and output of the last time point, and feed them into description generator as initialization of memory cell and hidden unit respectively. 2.3 Generating Video Description Existing video captioning approaches usually share com- mon part of visual model and language model as represen- tation [23, 15], which may lead to severe information loss. Besides, they also input the same pooled visual vector of the whole video into every sentence processing unit, thereby ignoring temporal structure. Such methods may easily re- sult in undesirable outputs due to the duplicate inputs in every time point of the new sequence [24]. To address these issues, we generate descriptions for video clips using a se- quential model initialized with visual representation. In- spired by the superior performance of probabilistic sequence generation machine, we generate each word recurrently at each time point.",
      "To address these issues, we generate descriptions for video clips using a se- quential model initialized with visual representation. In- spired by the superior performance of probabilistic sequence generation machine, we generate each word recurrently at each time point. Then the log probability of sentence S can be expressed as below: log p (S|V ) = t=N X t=1 log p (wt|V, w1, ...wt\u22121; \u03b8) (6) where \u03b8 denotes all parameters in sentence generation model and V is the representation of given video, and N indicates the number of words in sentence. We identify the most likely sentence by maximizing the log likelihood in Equation (6),",
      "then our object function can be described as: \u03b8\u2217= argmax \u03b8 t=N X t=1 log p (S|V ; \u03b8) (7) The optimizer updates \u03b8 with \u03b8\u2217across the entire training process applying Stochastic Gradient Descent (SGD). Dur- ing training phrase, the loss is back propagated through time and each LSTM unit learns to derive an appropriate hidden representation ht from input sequence. We then implement the Softmax function to get the probability distribution over the words in the entire vocabulary. At the beginning of the sentence generation, as depicted in Figure 1, an explicit starting token (<BOS>) is needed and we terminate each sentence when the end-of-sentence token (<EOS>) is feeding in. During test phrase, similar to [23], our language model takes the word wt\u22121 with maximum likelihood as input at time t repeatedly until the <EOS> token is emitted. 3. EXPERIMENTS 3.1 Dataset Video Dataset: We evaluate our approach by conduct- ing experiments on the Microsoft Research Video Descrip- tion (MSVD) [1] corpus, which is description for a collection of 1,970 video clips.",
      "3. EXPERIMENTS 3.1 Dataset Video Dataset: We evaluate our approach by conduct- ing experiments on the Microsoft Research Video Descrip- tion (MSVD) [1] corpus, which is description for a collection of 1,970 video clips. Each video clip depicts a single action or a simple event, such as \u201cshooting\u201d, \u201ccutting\u201d, \u201cplaying the piano\u201d and \u201ccooking\u201d, which with the duration between 8 seconds to 25 seconds. There are roughly 43 available sentences per video and 7 words in each sentence at aver- age. Following the majority of prior works [15, 24, 23, 32], we split entire dataset into training, validation and test set with 1200, 100 and 670 snippets, respectively. Image Dataset: Comparing to other LSTM structure and deep networks, the size of video dataset for caption task is small, thereby we apply transferring learning from image description.",
      "Image Dataset: Comparing to other LSTM structure and deep networks, the size of video dataset for caption task is small, thereby we apply transferring learning from image description. COCO 2014 image description dataset [13] has been used to perform experiments frequently [8, 3, 2, 29], which consists of more than 120,000 images, about 82,000 and 40,000 images for training and test respectively. We pre-train our language model on COCO 2014 training set \ufb01rst, then transfer learning on MSVD with integral video description model. 3.2 Experimental Setup 3.2.1 Preprocessing Description Processing: Some minimal preprocessing has been implemented to the descriptions in both MSVD and COCO 2014 datasets. We \ufb01rst employ word tokenize operation in NLTK toolbox1 to obtain individual words, and then convert all words to lower-case. All punctuation are removed, and then we start each sentence with <BOS> and end with <EOS>. Finally, we combine the sets of words in MSVD with COCO 2014, and generate a vocabulary with 12,984 unique words.",
      "All punctuation are removed, and then we start each sentence with <BOS> and end with <EOS>. Finally, we combine the sets of words in MSVD with COCO 2014, and generate a vocabulary with 12,984 unique words. Each word input to our system is represented by one-hot vector. Video Preprocessing: As previous video description works [24, 23, 15] , we sample video frames once in every ten frames, then these frames could represent given video and 1http://www.nltk.org 28.5 frames for each video averagely. We extract frame-wise ca\ufb00e fc7 layer features using VGG-16 layers model, then feed the sequential feature into our video caption system. 3.2.2 Model We employ a bidirectional S2VT [23] and a joint bidi- rectional LSTM structure to investigate the performance of our bidirectional approach. For convenient comparison, we set the size of hidden unit of all LSTMs in our system to 512 as [15, 23], except for the \ufb01rst video encoder in unidi- rectional joint LSTM.",
      "For convenient comparison, we set the size of hidden unit of all LSTMs in our system to 512 as [15, 23], except for the \ufb01rst video encoder in unidi- rectional joint LSTM. During training phrase, we set 80 as maximum number of time steps of LSTM in all our mod- els and a mini-batch with 16 video-sentence pairs. We note that over 99% of the descriptions in MSVD and COCO 2014 contain no more than 40 words, and in [23], Venugopalan et al. pointed out that 94% of the YouTube training videos satisfy our maximum length limit. To ensure su\ufb03cient vi- sual content, we adopt two ways to truncate the videos and sentences adaptively when the sum of the number of frames and words exceed the limit. If the number of words is within 40, we arbitrarily truncate the frames to satisfy the maxi- mum length. When the length of sentence is more than 40, we discard the words that beyond the length and take video frames with a maximum number of 40.",
      "If the number of words is within 40, we arbitrarily truncate the frames to satisfy the maxi- mum length. When the length of sentence is more than 40, we discard the words that beyond the length and take video frames with a maximum number of 40. Bidirectional S2VT: Similar to [23], we implement sev- eral S2VT-based models: S2VT, bidirectional S2VT and re- inforced S2VT with bidirectional LSTM video encoder. We conduct experiment on S2VT using our video features and LSTM structure instead of the end-to-end model in [23], which need original RGB frames as input. For bidirec- tional S2VT model, we \ufb01rst pre-train description generator on COCO 2014 for image caption. We next implement for- ward and backward pass for video encoding and merge the hidden states step-wise with a learnt weight while the lan- guage layer receives merged hidden representation with null padded as words. We also pad the inputs of forward LSTM and backward LSTM with zeros at decoding stage, and con- catenate the merged hidden states to embedded words.",
      "We also pad the inputs of forward LSTM and backward LSTM with zeros at decoding stage, and con- catenate the merged hidden states to embedded words. In the last model, we regard merged bidirectional hidden states as complementary enhancement and concatenate to original fc7 features to obtain a reinforced representation of video, then derive sentence from new feature using the last LSTM. The loss is computed only at decoding stage in all S2VT- based models. Joint-BiLSTM: Di\ufb00erent from S2VT-based models, we employ a joint bidirectional LSTM networks to encode video sequence and decode description applying another LSTM re- spectively rather than sharing the common one. We stack two layers of LSTM networks to encode video and pre-train language model as in S2VT-based models. Similarly, uni- directional LSTM, bidirectional LSTM and reinforced BiL- STM are executed to investigate the performance of each structure. We set 1024 hidden units of the \ufb01rst LSTM in uni- directional encoder so that the output could pass to the sec- ond encoder directly, and the memory cell and hidden state of the last time point are applied to initialize description decoder.",
      "We set 1024 hidden units of the \ufb01rst LSTM in uni- directional encoder so that the output could pass to the sec- ond encoder directly, and the memory cell and hidden state of the last time point are applied to initialize description decoder. Bidirectional structure and reinforced BiLSTM in encoder are implemented similarly to the corresponding type structure in S2VT-based models, respectively, and then feed the video representation into description generator as the unidirectional model aforementioned. 3.3 Results and Analysis",
      "Uni: a person is cutting a potato. Bi: a person is slicing a potato. Re: a person is peeling a potato. Uni: a man is playing. Bi: a man is playing a piano. Re: a man is playing the piano. Uni: a man is playing on a stage. Bi: a group of a man is dancing. Re: a group of people are dancing. Uni: a man is riding a bike. Bi: a man is riding on a motorcycle. Re: a man is riding a motorcycle. Figure 2: Video captioning examples of our pro- posed method. \u201cUni\u201d in color blue, \u201cBi\u201d in color brown and \u201cRe\u201d in color black are unidirectional Joint-LSTM, bidirectional Joint-LSTM and rein- forced Joint-BiLSTM model, respectively. Table 1: Comparison results of unidirectional, bidi- rectional structures and reinforced BiLSTM in both S2VT-based and joint LSTMs structure with ME- TEOR (reported in percentage, higher is better).",
      "Table 1: Comparison results of unidirectional, bidi- rectional structures and reinforced BiLSTM in both S2VT-based and joint LSTMs structure with ME- TEOR (reported in percentage, higher is better). Model METEOR S2VT-unidirectional 28.7 S2VT-bidirectional 28.6 S2VT-BiLSTM reinfored 29.5 Joint-LSTM unidirectional 29.5 Joint-LSTM bidirectional 29.9 Joint-BiLSTM reinforced 30.3 BLEU [16], METEOR [10], ROUGE-L [12] and CIDEr [22] are common evaluation metrics in image and video descrip- tion, the \ufb01rst three were originally proposed to evaluate ma- chine translation at the earliest and CIDEr was proposed to evaluate image description with su\ufb03cient reference sen- tences. To quantitatively evaluate the performance of our bidirectional recurrent based approach, we adopt METEOR metric because of its robust performance.",
      "To quantitatively evaluate the performance of our bidirectional recurrent based approach, we adopt METEOR metric because of its robust performance. Contrasting to the other three metrics, METEOR could capture semantic aspect since it identi\ufb01es all possible matches by extracting exact matcher, stem matcher, paraphrase matcher and syn- onym matcher using WordNet database, and compute sen- tence level similarity scores according to matcher weights. The authors of CIDEr also argued for that METEOR out- performs CIDEr when the reference set is small [22]. We \ufb01rst compare our unidirectional, bidirectional struc- tures and reinforced BiLSTM. As shown in Table 1, in S2VT- based model, bidirectional structure performs very little lower score than unidirectional structure while it shows the oppo- site results in joint LSTM case. It may be caused by the pad at description generating stage in S2VT-based structure.",
      "As shown in Table 1, in S2VT- based model, bidirectional structure performs very little lower score than unidirectional structure while it shows the oppo- site results in joint LSTM case. It may be caused by the pad at description generating stage in S2VT-based structure. We note that BiLSTM reinforced structure gains more than 3% improvement than unidirectional-only model in both S2VT- based and joint LSTMs structures, which means that com- bining bidirectional encoding of video representation is ben- e\ufb01cial to exploit some additional temporal structure in video encoder (Figure 2). On structure level, Table 1 illustrates that our Joint-LSTMs based models outperform all S2VT based models correspondingly. It demonstrates our Joint- LSTMs structure bene\ufb01ts from encoding video and decoding natural language separately. Table 2: Comparing with several state-of-the-art models (reported in percentage, higher is better).",
      "It demonstrates our Joint- LSTMs structure bene\ufb01ts from encoding video and decoding natural language separately. Table 2: Comparing with several state-of-the-art models (reported in percentage, higher is better). Model METEOR LSTM 26.9 Joint-LSTM unidirectinal (ours) 29.5 S2VT [23] -RGB (VGG) 29.2 -RGB (VGG)+Flow (AlexNet) 29.8 LSTM-E (VGG) [15] 29.5 LSTM-E (C3D) [15] 29.9 Yao et al. [32] 29.6 Joint-BiLSTM reinforced (ours) 30.3 We also evaluate our Joint-BiLSTM structure by compar- ing with several other state-of-the-art baseline approaches, which exploit either local or global temporal structure. As shown in Table 2, our Joint-BiLSTM reinforced model out- performs all of the baseline methods.",
      "As shown in Table 2, our Joint-BiLSTM reinforced model out- performs all of the baseline methods. The result of \u201cLSTM\u201d in \ufb01rst row refer from [15] and the last row but one denotes the best model combining local temporal structure using C3D with global temporal structure utilizing temporal at- tention in [32]. From the \ufb01rst two rows, our unidirectional joint LSTM shows rapid improvement, and comparing with S2VT-VGG model in line 3, it also demonstrates some supe- riority. Even LSTM-E jointly models video and descriptions representation by minimizing the distance between video and corresponding sentence, our Joint-BiLSTM reinforced obtains better performance from bidirectional encoding and separated visual and language models. We observed that while our unidirectional S2VT has the same deployment as [23], our model gives a little poorer performance(line 1, Table 1 and line 3, Table 2). As men- tioned in Section 3.2.2, they employed an end-to-end model reading original RGB frames and \ufb01ne-tuning on the VGG ca\ufb00emodel.",
      "As men- tioned in Section 3.2.2, they employed an end-to-end model reading original RGB frames and \ufb01ne-tuning on the VGG ca\ufb00emodel. The features of frames from VGG fc7 layer are more compatible to MSVD dataset and the description task. However, our joint LSTM demonstrates better per- formance with general features rather than speci\ufb01c ones for data, even superior to their model with multiple feature as- pects (RGB + Flow, line 4, Table 2), which means that our Joint-BiLSTM could show more powerful descriptive ability in end-to-end case. Certainly, We would investigate e\ufb00ect of end-to-end type of our Joint-BiLSTM in future works. 4. CONCLUSION AND FUTURE WORKS In this paper, we introduced a sequence to sequence ap- proach to describe video clips with natural language. The core of our method was, we applied two LSTM networks for the visual encoder and natural language generator compo- nent of our model.",
      "4. CONCLUSION AND FUTURE WORKS In this paper, we introduced a sequence to sequence ap- proach to describe video clips with natural language. The core of our method was, we applied two LSTM networks for the visual encoder and natural language generator compo- nent of our model. In particular, we encoded video sequences with a bidirectional Long-Short Term Memory (BiLSTM) network, which could e\ufb00ectively capture the bidirectional global temporal structure in video. Experimental results on MSVD dataset demonstrated the superior performance over many other state-of-the-art methods. We also note some limitations in our model, such as end- to-end framework employed in [23] and distance measured in [15]. In the future we will make more e\ufb00ort to \ufb01x these limitations and exploit the linguistic domain knowledge in visual content understanding.",
      "5. REFERENCES [1] D. L. Chen and W. B. Dolan. Collecting highly parallel data for paraphrase evaluation. In ACL, 2011. [2] X. Chen and C. Lawrence Zitnick. Mind\u2019s eye: A recurrent visual representation for image caption generation. In CVPR, 2015. [3] J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell. Long-term recurrent convolutional networks for visual recognition and description. In CVPR, 2015. [4] A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young, C. Rashtchian, J. Hockenmaier, and D. Forsyth. Every picture tells a story: Generating sentences from images. In ECCV. Springer, 2010. [5] C. Gan, N. Wang, Y. Yang, D.-Y. Yeung, and A. G. Hauptmann.",
      "Every picture tells a story: Generating sentences from images. In ECCV. Springer, 2010. [5] C. Gan, N. Wang, Y. Yang, D.-Y. Yeung, and A. G. Hauptmann. Devnet: A deep event network for multimedia event detection and evidence recounting. In CVPR, 2015. [6] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997. [7] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Ca\ufb00e: Convolutional architecture for fast feature embedding. In ACM Multimedia, 2014. [8] A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In CVPR, 2015.",
      "Ca\ufb00e: Convolutional architecture for fast feature embedding. In ACM Multimedia, 2014. [8] A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In CVPR, 2015. [9] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classi\ufb01cation with deep convolutional neural networks. In NIPS, 2012. [10] M. D. A. Lavie. Meteor universal: language speci\ufb01c translation evaluation for any target language. ACL, 2014. [11] G. Li, S. Ma, and Y. Han. Summarization-based video caption via deep neural networks. In ACM Multimedia, 2015. [12] C.-Y. Lin. Rouge: A package for automatic evaluation of summaries. In ACL, 2004. [13] T.-Y.",
      "Summarization-based video caption via deep neural networks. In ACM Multimedia, 2015. [12] C.-Y. Lin. Rouge: A package for automatic evaluation of summaries. In ACL, 2004. [13] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00b4ar, and C. L. Zitnick. Microsoft coco: Common objects in context. In ECCV. Springer, 2014. [14] V. Ordonez, G. Kulkarni, and T. L. Berg. Im2text: Describing images using 1 million captioned photographs. In NIPS, 2011. [15] Y. Pan, T. Mei, T. Yao, H. Li, and Y. Rui. Jointly modeling embedding and translation to bridge video and language. arXiv preprint arXiv:1505.01861, 2015.",
      "[15] Y. Pan, T. Mei, T. Yao, H. Li, and Y. Rui. Jointly modeling embedding and translation to bridge video and language. arXiv preprint arXiv:1505.01861, 2015. [16] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evaluation of machine translation. In ACL, 2002. [17] V. Ramanathan, P. Liang, and L. Fei-Fei. Video event understanding using natural language descriptions. In ICCV, 2013. [18] F. Shen, W. Liu, S. Zhang, Y. Yang, and H. Tao Shen. Learning binary codes for maximum inner product search. In ICCV, pages 4148\u20134156, December 2015. [19] F. Shen, C. Shen, W. Liu, and H. T. Shen. Supervised discrete hashing. In CVPR, pages 37\u201345, 2015.",
      "In ICCV, pages 4148\u20134156, December 2015. [19] F. Shen, C. Shen, W. Liu, and H. T. Shen. Supervised discrete hashing. In CVPR, pages 37\u201345, 2015. [20] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014. [21] K. Tang, B. Yao, L. Fei-Fei, and D. Koller. Combining the right features for complex event recognition. In ICCV, 2013. [22] R. Vedantam, C. Lawrence Zitnick, and D. Parikh. Cider: Consensus-based image description evaluation. In CVPR, 2015. [23] S. Venugopalan, M. Rohrbach, J. Donahue, R. Mooney, T. Darrell, and K. Saenko. Sequence to sequence-video to text. In ICCV, 2015.",
      "[23] S. Venugopalan, M. Rohrbach, J. Donahue, R. Mooney, T. Darrell, and K. Saenko. Sequence to sequence-video to text. In ICCV, 2015. [24] S. Venugopalan, H. Xu, J. Donahue, M. Rohrbach, R. Mooney, and K. Saenko. Translating videos to natural language using deep recurrent neural networks. arXiv preprint arXiv:1412.4729, 2014. [25] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. In CVPR, 2015. [26] P. J. Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78(10):1550\u20131560, 1990. [27] Z. Wu, Y.-G.",
      "[26] P. J. Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78(10):1550\u20131560, 1990. [27] Z. Wu, Y.-G. Jiang, X. Wang, H. Ye, X. Xue, and J. Wang. Fusing multi-stream deep networks for video classi\ufb01cation. arXiv preprint arXiv:1509.06086, 2015. [28] Z. Wu, X. Wang, Y.-G. Jiang, H. Ye, and X. Xue. Modeling spatial-temporal clues in a hybrid deep learning framework for video classi\ufb01cation. In ACM Multimedia, 2015. [29] K. Xu, J. Ba, R. Kiros, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio. Show, attend and tell: Neural image caption generation with visual attention. arXiv preprint arXiv:1502.03044, 2015.",
      "Show, attend and tell: Neural image caption generation with visual attention. arXiv preprint arXiv:1502.03044, 2015. [30] Y. Yang, Z.-J. Zha, Y. Gao, X. Zhu, and T.-S. Chua. Exploiting web images for semantic video indexing via robust sample-speci\ufb01c loss. TMM, 16(6):1677\u20131689, 2014. [31] Y. Yang, H. Zhang, M. Zhang, F. Shen, and X. Li. Visual coding in a semantic hierarchy. In ACM Multimedia, pages 59\u201368, 2015. [32] L. Yao, A. Torabi, K. Cho, N. Ballas, C. Pal, H. Larochelle, and A. Courville. Describing videos by exploiting temporal structure. In ICCV, 2015."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1606.04631.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":6890,
  "avg_doclen":181.3157894737,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1606.04631.pdf"
    }
  }
}