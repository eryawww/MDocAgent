{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "arXiv:1905.08949v3  [cs.CL]  4 Jun 2019 Recent Advances in Neural Question Generation Liangming Pan, Wenqiang Lei, Tat-Seng Chua and Min-Yen Kan School of Computing National University of Singapore Singapore 117417 {e0272310,wenql,kanmy,chuats}comp.nus.edu.sg Abstract Emerging research in Neural Question Gener- ation (NQG) has started to integrate a larger variety of inputs, and generating questions requiring higher levels of cognition. These trends point to NQG as a bellwether for NLP, about how human intelligence embodies the skills of curiosity and integration. We present a comprehensive survey of neural question generation, examining the corpora, methodologies, and evaluation methods. From this, we elaborate on what we see as emerg- ing on NQG\u2019s trend: in terms of the learn- ing paradigms, input modalities, and cognitive levels considered by NQG. We end by pointing out the potential directions ahead.",
      "From this, we elaborate on what we see as emerg- ing on NQG\u2019s trend: in terms of the learn- ing paradigms, input modalities, and cognitive levels considered by NQG. We end by pointing out the potential directions ahead. 1 Introduction Question Generation (QG) concerns the task of \u201cautomatically generating questions from various inputs such as raw text, database, or semantic rep- resentation\u201d (Rus et al., 2008). People have the ability to ask rich, creative, and revealing ques- tions (Rothe et al., 2017); e.g., asking Why did Gollum betray his master Frodo Baggins? after reading the fantasy novel The Lord of the Rings. How can machines be endowed with the ability to ask relevant and to-the-point questions, given var- ious inputs? This is a challenging, complementary task to Question Answering (QA). Both QA and QG require an in-depth understanding of the in- put source and the ability to reason over relevant contexts.",
      "This is a challenging, complementary task to Question Answering (QA). Both QA and QG require an in-depth understanding of the in- put source and the ability to reason over relevant contexts. But beyond understanding, QG addition- ally integrates the challenges of Natural Language Generation (NLG), i.e., generating grammatically and semantically correct questions. QG is of practical importance: in education, forming good questions are crucial for evaluating students knowledge and stimulating self-learning. QG can generate assessments for course materi- als (Heilman and Smith, 2010) or be used as a component in adaptive, intelligent tutoring sys- tems (Lindberg et al., 2013). In dialog systems, \ufb02uent QG is an important skill for chatbots, e.g., in initiating conversations or obtaining speci\ufb01c in- formation from human users. QA and reading comprehension also bene\ufb01t from QG, by reducing the needed human labor for creating large-scale datasets.",
      "QA and reading comprehension also bene\ufb01t from QG, by reducing the needed human labor for creating large-scale datasets. We can say that traditional QG mainly focused on generating factoid questions from a single sentence or a paragraph, spurred by a series of workshops during 2008\u20132012 (Rus and Lester, 2009; Rus et al., 2010, 2011, 2012). Recently, driven by advances in deep learning, QG research has also begun to utilize \u201cneural\u201d techniques, to develop end-to-end neural models to generate deeper questions (Chen et al., 2018) and to pursue broader applications (Serban et al., 2016; Mostafazadeh et al., 2016). While there have been considerable advances made in NQG, the area lacks a comprehensive sur- vey.",
      "While there have been considerable advances made in NQG, the area lacks a comprehensive sur- vey. This paper \ufb01lls this gap by presenting a sys- tematic survey on recent development of NQG, fo- cusing on three emergent trends that deep learn- ing has brought in QG: (1) the change of learning paradigm, (2) the broadening of the input spec- trum, and (3) the generation of deep questions. 2 Fundamental Aspects of NQG For the sake of clean exposition, we \ufb01rst provide a broad overview of QG by conceptualizing the problem from the perspective of the three intro- duced aspects: (1) its learning paradigm, (2) its input modalities, and (3) the cognitive level it in- volves. This combines past research with recent trends, providing insights on how NQG connects to traditional QG research. 2.1 Learning Paradigm QG research traditionally considers two funda- mental aspects in question asking: \u201cWhat to ask\u201d",
      "and \u201cHow to ask\u201d. A typical QG task considers the identi\ufb01cation of the important aspects to ask about (\u201cwhat to ask\u201d), and learning to realize such iden- ti\ufb01ed aspects as natural language (\u201chow to ask\u201d). Deciding what to ask is a form of machine under- standing: a machine needs to capture important information dependent on the target application, akin to automatic summarization. Learning how to ask, however, focuses on aspects of the language quality such as grammatical correctness, semanti- cally preciseness and language \ufb02exibility. Past research took a reductionist approach, separately considering these two problems of \u201cwhat\u201d and \u201chow\u201d via content selection and question construction. Given a sentence or a paragraph as input, content selection se- lects a particular salient topic worthwhile to ask about and determines the question type (What, When, Who, etc.).",
      "Given a sentence or a paragraph as input, content selection se- lects a particular salient topic worthwhile to ask about and determines the question type (What, When, Who, etc.). Approaches either take a syntactic (Gates, 2008; Liu et al., 2010; Heilman, 2011) or semantic (Yao et al., 2012; Lindberg et al., 2013; Mazidi and Nielsen, 2014; Chali and Hasan, 2015) tack, both starting by ap- plying syntactic or semantic parsing, respectively, to obtain intermediate symbolic representations. Question construction then converts intermediate representations to a natural language question, taking either a tranformation- or template-based approach. The former (Ali et al., 2010; Pal et al., 2010; Heilman, 2011) rearranges the surface form of the input sentence to produce the question; the latter (Chen and Mostow, 2009; Liu et al., 2012; Rokhlenko and Szpektor, 2013) generates ques- tions from pre-de\ufb01ned question templates.",
      "Un- fortunately, such QG architectures are limiting, as their representation is con\ufb01ned to the variety of in- termediate representations, transformation rules or templates. In contrast, neural models motivate an end-to- end architectures. Deep learned frameworks con- trast with the reductionist approach, admitting ap- proaches that jointly optimize for both the \u201cwhat\u201d and \u201chow\u201d in an uni\ufb01ed framework. The major- ity of current NQG models follow the sequence- to-sequence (Seq2Seq) framework that use a uni- \ufb01ed representation and joint learning of content selection (via the encoder) and question construc- tion (via the decoder). In this framework, tradi- tional parsing-based content selection has been re- placed by more \ufb02exible approaches such as atten- tion (Bahdanau et al., 2014) and copying mecha- nism (G\u00a8ulc\u00b8ehre et al., 2016). Question construc- tion has become completely data-driven, requiring far less labor compared to transformation rules, enabling better language \ufb02exibility compared to question templates.",
      "Question construc- tion has become completely data-driven, requiring far less labor compared to transformation rules, enabling better language \ufb02exibility compared to question templates. However, unlike other Seq2Seq learning NLG tasks, such as Machine Translation, Image Cap- tioning, and Abstractive Summarization, which can be loosely regarded as learning a one-to-one mapping, generated questions can differ signi\ufb01- cantly when the intent of asking differs (e.g., the target answer, the target aspect to ask about, and the question\u2019s depth). In Section 5, we summarize different NQG methodologies based on Seq2Seq framework, investigating how some of these QG- speci\ufb01c factors are integrated with neural mod- els, and discussing what could be further explored. The change of learning paradigm in NQG era is also represented by multi-task learning with other NLP tasks, for which we discuss in Section 6.1. 2.2 Input Modality Question generation is an NLG task for which the input has a wealth of possibilities depending on applications.",
      "The change of learning paradigm in NQG era is also represented by multi-task learning with other NLP tasks, for which we discuss in Section 6.1. 2.2 Input Modality Question generation is an NLG task for which the input has a wealth of possibilities depending on applications. While a host of input modalities have been considered in other NLG tasks, such as text summarization (Mani, 1999), image caption- ing (Vinyals et al., 2015) and table-to-text gener- ation (Lebret et al., 2016), traditional QG mainly focused on textual inputs, especially declarative sentences, explained by the original application domains of question answering and education, which also typically featured textual inputs. Recently, with the growth of various QA applications such as Knowledge Base Question Answering (KBQA) (Cui et al., 2017) and Visual Question Answering (VQA) (Antol et al., 2015), NQG research has also widened the spectrum of sources to include knowledge bases (Khapra et al., 2017) and images (Mostafazadeh et al., 2016).",
      "This trend is also spurred by the remark- able success of neural models in feature representation, especially on image fea- tures (Krizhevsky et al., 2012) and knowledge representations (Bordes et al., 2013). We discuss adapting NQG models to other input modalities in Section 6.2. 2.3 Cognitive Levels Finally, we consider the required cognitive pro- cess behind question asking, a distinguishing fac- tor for questions (Anderson et al., 2001). A typical",
      "framework that attempts to categorize the cogni- tive levels involved in question asking comes from Bloom\u2019s taxonomy (Bloom et al., 1984), which has undergone several revisions and currently has six cognitive levels: Remembering, Understand- ing, Applying, Analyzing, Evaluating and Creat- ing (Anderson et al., 2001). Traditional QG focuses on shallow levels of Bloom\u2019s taxonomy: typical QG research is on generating sentence-based factoid questions (e.g., Who, What, Where questions), whose an- swers are simple constituents in the input sen- tence (Heilman and Smith, 2010; Heilman, 2011). However, a QG system achieving human cogni- tive level should be able to generate meaningful questions that cater to higher levels of Bloom\u2019s taxonomy (Desai et al., 2018), such as Why, What- if, and How questions. Traditionally, those \u201cdeep\u201d questions are generated through shallow methods such as handcrafted templates (Liu et al., 2012; Rokhlenko and Szpektor, 2013); however, these methods lack a real understanding and reasoning over the input.",
      "Traditionally, those \u201cdeep\u201d questions are generated through shallow methods such as handcrafted templates (Liu et al., 2012; Rokhlenko and Szpektor, 2013); however, these methods lack a real understanding and reasoning over the input. Although asking deep questions is com- plex, NQG\u2019s ability to generalize over volumi- nous data has enabled recent research to ex- plore the comprehension and reasoning aspects of QG (Labutov et al., 2015; Rothe et al., 2017; Chen et al., 2018; Desai et al., 2018). We inves- tigate this trend in Section 6.3, examining the lim- itations of current Seq2Seq model in generating deep questions, and the efforts made by existing works, indicating further directions ahead. The rest of this paper provides a systematic sur- vey of NQG, covering corpus and evaluation met- rics before examining speci\ufb01c neural models. 3 Corpora As QG can be regarded as a dual task of QA, in principle any QA dataset can be used for QG as well.",
      "3 Corpora As QG can be regarded as a dual task of QA, in principle any QA dataset can be used for QG as well. However, there are at least two corpus-related factors that affect the dif\ufb01culty of question generation. The \ufb01rst is the re- quired cognitive level to answer the question, as we discussed in the previous section. Current NQG has achieved promising results on datasets consisting mainly of shallow factoid questions, such as SQuAD (Rajpurkar et al., 2016) and MS MARCO (Nguyen et al., 2016). However, the performance drops signi\ufb01cantly on deep question datasets, such as LearningQ (Chen et al., 2018), shown in Section 6.3.",
      "However, the performance drops signi\ufb01cantly on deep question datasets, such as LearningQ (Chen et al., 2018), shown in Section 6.3. The second factor is the an- swer type, i.e., the expected form of the answer, typically having four settings: (1) the answer is a text span in the passage, which is usually the case for factoid questions, (2) human-generated, abstractive answer that may not appear in the pas- sage, usually the case for deep questions, (3) mul- tiple choice question where question and its dis- tractors should be jointly generated, and (4) no given answer, which requires the model to auto- matically learn what is worthy to ask. The design of NQG system differs accordingly. Table 1 presents a listing of the NQG corpora grouped by their cognitive level and answer type, along with their statistics. Among them, SQuAD was used by most groups as the benchmark to eval- uate their NQG models. This provides a fair com- parison between different techniques.",
      "Among them, SQuAD was used by most groups as the benchmark to eval- uate their NQG models. This provides a fair com- parison between different techniques. However, it raises the issue that most NQG models work on factoid questions with answer as text span, leaving other types of QG problems less investigated, such as generating deep multi-choice questions. To overcome this, a wider variety of corpora should be benchmarked against in future NQG research. 4 Evaluation Metrics Although the datasets are commonly shared be- tween QG and QA, it is not the case for evalua- tion: it is challenging to de\ufb01ne a gold standard of proper questions to ask. Meaningful, syntactically correct, semantically sound and natural are all use- ful criteria, yet they are hard to quantify. Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated ques- tions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the per- centage of best-ranked questions are reported and used for quality marks.",
      "Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated ques- tions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the per- centage of best-ranked questions are reported and used for quality marks. As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU (Papineni et al., 2002), METEOR (Lavie and Denkowski, 2009), and ROUGE (Lin, 2004), are also widely used. However, some studies (Callison-Burch et al., 2006; Liu et al., 2016) have shown that these metrics do not correlate well with \ufb02uency, ade- quacy, coherence, as they essentially compute the n-gram similarity between the source sentence and the generated question. To overcome this, Nema and Khapra (2018) proposed a new metric to evaluate the \u201canswerability\u201d of a question by calculating the scores for several question-speci\ufb01c",
      "Cognitive Dataset / Contributor Answer Domain Statistics Level Type Documents Questions Q./Doc Shallow SQuAD (Rajpurkar et al., 2016) text span Wikipedia 20,958 97,888 4.67 NewsQA (Trischler et al., 2017) text span News 12,744 119,633 9.39 Medium MS MARCO (Nguyen et al., 2016) human generated Web article 1,010,916 3,563,535 3.53 RACE (Lai et al., 2017) multiple choice Education 27,933 72,547 2.60 Deep LearningQ (Chen et al., 2018) no answer Education 10,841 231,470 21.35 NarrativeQA (Kocisk\u00b4y et al., 2018) human generated Story 1,572 46,765 29.75 Table 1: NQG datasets grouped by their cognitive level and answer type, where the number of documents, the number of questions, and the average number of questions per document (Q./Doc) for each corpus are listed.",
      "factors, including question type, content words, function words, and named entities. However, as it is newly proposed, it has not been applied to evaluate any NQG system yet. To accurately measure what makes a good ques- tion, especially deep questions, improved evalua- tion schemes are required to speci\ufb01cally investi- gate the mechanism of question asking. 5 Methodology Many current NQG models follow the Seq2Seq ar- chitecture.",
      "To accurately measure what makes a good ques- tion, especially deep questions, improved evalua- tion schemes are required to speci\ufb01cally investi- gate the mechanism of question asking. 5 Methodology Many current NQG models follow the Seq2Seq ar- chitecture. Under this framework, given a passage (usually a sentence) X = (x1, \u00b7 \u00b7 \u00b7 , xn) and (pos- sibly) a target answer A (a text span in the pas- sage) as input, an NQG model aims to generate a question Y = (y1, \u00b7 \u00b7 \u00b7 , ym) asking about the tar- get answer A in the passage X, which is de\ufb01ned as \ufb01nding the best question \u00afY that maximizes the conditional likelihood given the passage X and the answer A: \u00afY = arg max Y P(Y |X, A) (1) = arg max Y m X t=1 P(yt|X, A, y<t) (2) Du et al.",
      "(2017) pioneered the \ufb01rst NQG model using an attention Seq2Seq model (Bahdanau et al., 2014), which feeds a sentence into an RNN-based encoder, and generate a question about the sentence through a decoder. The attention mechanism is applied to help decoder pay attention to the most relevant parts of the input sentence while generating a question. Note that this base model does not take the target answer as input. Subsequently, neural models have adopted attention mechanism as a default (Zhou et al., 2017; Duan et al., 2017; Harrison and Walker, 2018). Although these NQG models all share the Seq2Seq framework, they differ in the considera- tion of \u2014 (1) QG-speci\ufb01c factors (e.g., answer en- coding, question word generation, and paragraph- level contexts), and (2) common NLG techniques (e.g., copying mechanism, linguistic features, and reinforcement learning) \u2014 discussed next.",
      "5.1 Encoding Answers The most commonly considered factor by current NQG systems is the target answer, which is typ- ically taken as an additional input to guide the model in deciding which information to focus on when generating; otherwise, the NQG model tend to generate questions without speci\ufb01c target (e.g., \u201cWhat is mentioned?\u201d). Models have solved this by either treating the answer\u2019s position as an ex- tra input feature (Zhou et al., 2017; Zhao et al., 2018), or by encoding the answer with a separate RNN (Duan et al., 2017; Kim et al., 2019). The \ufb01rst type of method augments each in- put word vector with an extra answer indicator feature, indicating whether this word is within the answer span. Zhou et al. (2017) imple- ment this feature using the BIO tagging scheme, while Harrison and Walker (2018) directly use a binary indicator. In addition to the target an- swer, Sun et al.",
      "Zhou et al. (2017) imple- ment this feature using the BIO tagging scheme, while Harrison and Walker (2018) directly use a binary indicator. In addition to the target an- swer, Sun et al. (2018) argued that the context words closer to the answer also deserve more at- tention from the model, since they are usually more relevant. To this end, they incorporate train- able position embeddings (dp1, dp2, \u00b7 \u00b7 \u00b7 , dpn) into the computation of attention distribution, where pi is the relative distance between the i-th word and the answer, and dpi is the embedding of pi. This achieved an extra BLEU-4 gain of 0.89 on SQuAD. To generate answer-related questions, extra an- swer indicators explicitly emphasize the impor- tance of answer; however, it also increases the ten- dency that generated questions include words from the answer, resulting in useless questions, as ob- served by Kim et al. (2019).",
      "To generate answer-related questions, extra an- swer indicators explicitly emphasize the impor- tance of answer; however, it also increases the ten- dency that generated questions include words from the answer, resulting in useless questions, as ob- served by Kim et al. (2019). For example, given the input \u201cJohn Francis OHara was elected presi- dent of Notre Dame in 1934.\u201d, an improperly gen- erated question would be \u201cWho was elected John",
      "Francis?\u201d, which exposes some words in the an- swer. To address this, they propose to replace the answer into a special token for passage encod- ing, and a separate RNN is used to encode the an- swer. The outputs from two encoders are concate- nated as inputs to the decoder. Song et al. (2018) adopted a similar idea that separately encodes pas- sage and answer, but they instead use the multi- perspective matching between two encodings as an extra input to the decoder. We forecast treating the passage and the tar- get answer separately as a future trend, as it re- sults in a more \ufb02exible model, which generalizes to the abstractive case when the answer is not a text span in the input passage. However, this in- evitably increases the model complexity and dif\ufb01- culty in training. 5.2 Question Word Generation Question words (e.g., \u201cwhen\u201d, \u201chow\u201d, and \u201cwhy\u201d) also play a vital role in QG; Sun et al.",
      "However, this in- evitably increases the model complexity and dif\ufb01- culty in training. 5.2 Question Word Generation Question words (e.g., \u201cwhen\u201d, \u201chow\u201d, and \u201cwhy\u201d) also play a vital role in QG; Sun et al. (2018) ob- served that the mismatch between generated ques- tion words and answer type is common for cur- rent NQG systems. For example, a when-question should be triggered for answer \u201cthe end of the Mexican War\u201d while a why-question is generated by the model. A few works (Duan et al., 2017; Sun et al., 2018) considered question word gener- ation separately in model design. Duan et al. (2017) proposed to \ufb01rst generate a question template that contains question word (e.g., \u201chow to #\u201d, where # is the placeholder), be- fore generating the rest of the question. To this end, they train two Seq2Seq models; the former learns to generate question templates for a given text , while the latter learns to \ufb01ll the blank of tem- plate to form a complete question.",
      "To this end, they train two Seq2Seq models; the former learns to generate question templates for a given text , while the latter learns to \ufb01ll the blank of tem- plate to form a complete question. Instead of a two-stage framework, Sun et al. (2018) proposed a more \ufb02exible model by introducing an additional decoding mode that generates the question word. When entering this mode, the decoder produces a question word distribution based on a restricted set of vocabulary using the answer embedding, the de- coder state, and the context vector. The switch be- tween different modes is controlled by a discrete variable produced by a learnable module of the model in each decoding step. Determining the appropriate question word harks back to question type identi\ufb01cation, which is correlated with the question intention, as different intents may yield different questions, even when presented with the same (passage, answer) input pair. This points to the direction of exploring ques- tion pragmatics, where external contextual infor- mation (such as intent) can inform and in\ufb02uence how questions should optimally be generated.",
      "This points to the direction of exploring ques- tion pragmatics, where external contextual infor- mation (such as intent) can inform and in\ufb02uence how questions should optimally be generated. 5.3 Paragraph-level Contexts Leveraging rich paragraph-level contexts around the input text is another natural consideration to produce better questions. According to (Du et al., 2017), around 20% of questions in SQuAD require paragraph-level information to be answered. How- ever, as input texts get longer, Seq2Seq models have a tougher time effectively utilizing relevant contexts, while avoiding irrelevant information. To address this challenge, Zhao et al. (2018) proposed a gated self-attention encoder to re\ufb01ne the encoded context by fusing important informa- tion with the context\u2019s self-representation prop- erly, which has achieved state-of-the-art results on SQuAD. The long passage consisting of in- put texts and its context is \ufb01rst embedded via LSTM with answer position as an extra feature.",
      "The long passage consisting of in- put texts and its context is \ufb01rst embedded via LSTM with answer position as an extra feature. The encoded representation is then fed through a gated self-matching network (Wang et al., 2017b) to aggregate information from the entire passage and embed intra-passage dependencies. Finally, a feature fusion gate (Gong and Bowman, 2018) chooses relevant information between the original and self-matching enhanced representations. Instead of leveraging the whole context, Du and Cardie (2018) performed a pre-\ufb01ltering by running a coreference resolution system on the context passage to obtain coreference clusters for both the input sentence and the answer. The co- referred sentences are then fed into a gating net- work, from which the outputs serve as extra fea- tures to be concatenated with the original input vectors. 5.4 Answer-unaware QG The aforementioned models require the target an- swer as an input, in which the answer essentially serves as the focus of asking. However, in the case that only the input passage is given, a QG system should automatically identify question- worthy parts within the passage.",
      "5.4 Answer-unaware QG The aforementioned models require the target an- swer as an input, in which the answer essentially serves as the focus of asking. However, in the case that only the input passage is given, a QG system should automatically identify question- worthy parts within the passage. This task is syn- onymous with content selection in traditional QG. To date, only two works (Du and Cardie, 2017; Subramanian et al., 2018) have worked in this set- ting. They both follow the traditional decompo- sition of QG into content selection and question construction but implement each task using neural",
      "networks. For content selection, Du and Cardie (2017) learn a sentence selection task to identify question-worthy sentences from the input para- graph using a neural sequence tagging model. Subramanian et al. (2018) train a neural keyphrase extractor to predict keyphrases of the passage. For question construction, they both employed the Seq2Seq model, for which the input is either the selected sentence or the input passage with keyphrases as target answer. However, learning what aspect to ask about is quite challenging when the question requires rea- soning over multiple pieces of information within the passage; cf the Gollum question from the intro- duction. Beyond retrieving question-worthy infor- mation, we believe that studying how different rea- soning patterns (e.g., inductive, deductive, causal and analogical) affects the generation process will be an aspect for future study. 5.5 Technical Considerations Common techniques of NLG have also been con- sidered in NQG model, summarized as 3 tactics: 1. Copying Mechanism.",
      "5.5 Technical Considerations Common techniques of NLG have also been con- sidered in NQG model, summarized as 3 tactics: 1. Copying Mechanism. Most NQG models (Zhou et al., 2017; Yuan et al., 2017; Wang et al., 2018; Harrison and Walker, 2018; Kumar et al., 2018a) employ the copying mech- anism of G\u00a8ulc\u00b8ehre et al. (2016), which directly copies relevant words from the source sentence to the question during decoding. This idea is widely accepted as it is common to refer back to phrases and entities appearing in the text when formulating factoid questions, and dif\ufb01cult for a RNN decoder to generate such rare words on its own. 2. Linguistic Features.",
      "This idea is widely accepted as it is common to refer back to phrases and entities appearing in the text when formulating factoid questions, and dif\ufb01cult for a RNN decoder to generate such rare words on its own. 2. Linguistic Features. Approaches also seek to leverage additional linguistic features that complements word embeddings, including word case, POS and NER tags (Zhou et al., 2017; Wang et al., 2018) as well as corefer- ence (Harrison and Walker, 2018) and depen- dency information (Kumar et al., 2018a). These categorical features are vectorized and concate- nated with word embeddings. The feature vectors can be either one-hot or trainable and serve as in- put to the encoder. 3. Policy Gradient. Optimizing for just ground- truth log likelihood ignores the many equiva- lent ways of asking a question.",
      "The feature vectors can be either one-hot or trainable and serve as in- put to the encoder. 3. Policy Gradient. Optimizing for just ground- truth log likelihood ignores the many equiva- lent ways of asking a question. Relevant QG work (Yuan et al., 2017; Kumar et al., 2018b) have adopted policy gradient methods to add task- speci\ufb01c rewards (such as BLEU or ROUGE) to the original objective. This helps to diversify the questions generated, as the model learns to dis- tribute probability mass among equivalent expres- sions rather than the single ground truth question. 5.6 The State of the Art In Table 2, we summarize existing NQG mod- els with their employed techniques and their best- reported performance on SQuAD. These meth- ods achieve comparable results; as of this writing, Zhao et al. (2018) is the state-of-the-art. Two points deserve mention. First, while the copying mechanism has shown marked improve- ments, there exist shortcomings. Kim et al.",
      "These meth- ods achieve comparable results; as of this writing, Zhao et al. (2018) is the state-of-the-art. Two points deserve mention. First, while the copying mechanism has shown marked improve- ments, there exist shortcomings. Kim et al. (2019) observed many invalid answer-revealing questions attributed to the use of the copying mechanism; cf the John Francis example in Section 5.1. They abandoned copying but still achieved a perfor- mance rivaling other systems. In parallel ap- plication areas such as machine translation, the copy mechanism has been to a large extent re- placed with self-attention (Lin et al., 2017) or transformer (Vaswani et al., 2017). The future prospect of the copying mechanism requires fur- ther investigation. Second, recent approaches that employ paragraph-level contexts have shown promising results: not only boosting performance, but also constituting a step towards deep question generation, which requires reasoning over rich contexts.",
      "The future prospect of the copying mechanism requires fur- ther investigation. Second, recent approaches that employ paragraph-level contexts have shown promising results: not only boosting performance, but also constituting a step towards deep question generation, which requires reasoning over rich contexts. 6 Emerging Trends We discuss three trends that we wish to call prac- titioners\u2019 attention to as NQG evolves to take the center stage in QG: Multi-task Learning, Wider In- put Modalities and Deep Question Generation. 6.1 Multi-task Learning As QG has become more mature, work has started to investigate how QG can assist in other NLP tasks, and vice versa. Some NLP tasks bene\ufb01t from enriching training samples by QG to alleviate the data shortage problem. This idea has been suc- cessfully applied to semantic parsing (Guo et al., 2018a) and QA (Sachan and Xing, 2018). In the semantic parsing task that maps a natural lan- guage question to a SQL query, Guo et al.",
      "This idea has been suc- cessfully applied to semantic parsing (Guo et al., 2018a) and QA (Sachan and Xing, 2018). In the semantic parsing task that maps a natural lan- guage question to a SQL query, Guo et al. (2018a) achieved a 3% performance gain with an en- larged training set that contains pseudo-labeled (SQL, question) pairs generated by a Seq2Seq QG model. In QA, Sachan and Xing (2018) em- ployed the idea of self-training (Nigam and Ghani,",
      "Models Answer Encoding Features Performance QW PC CP LF PG BLEU-4 METEOR ROUGEL Du et al. (2017) not used 12.28 16.62 39.75 Duan et al. (2017) not used \u2022 12.28 \u2212 \u2212 Zhou et al. (2017) answer position \u2022 \u2022 13.29 \u2212 \u2212 Yuan et al. (2017) answer position \u2022 \u2022 10.50 \u2212 \u2212 Wang et al. (2018) answer position \u2022 \u2022 13.86 18.38 44.37 Harrsion et al. (2018) answer position \u2022 \u2022 14.39 19.54 43.00 Kumar et al. (2018b) not used \u2022 \u2022 \u2022 16.17 19.85 43.90 Sun et al. (2018) answer+context position \u2022 \u2022 15.64 \u2212 \u2212 Zhao et al. (2018) answer position \u2022 \u2022 16.38 20.25 44.48 Du and Cardie (2018) answer position \u2022 \u2022 15.16 19.12 \u2212 Song et al.",
      "(2018) answer position \u2022 \u2022 16.38 20.25 44.48 Du and Cardie (2018) answer position \u2022 \u2022 15.16 19.12 \u2212 Song et al. (2018) separate encoder \u2022 13.98 18.77 42.72 Kim et al. (2019) separate encoder 16.20 19.92 43.96 Table 2: Existing NQG models with their best-reported performance on SQuAD. Legend: QW: question word generation, PC: paragraph-level context, CP: copying mechanism, LF: linguistic features, PG: policy gradient. 2000) to jointly learn QA and QG. The QA and QG models are \ufb01rst trained on a labeled corpus. Then, the QG model is used to create more ques- tions from an unlabeled text corpus and the QA model is used to answer these newly-created ques- tions. The newly-generated question\u2013answer pairs form an enlarged dataset to iteratively retrain the two models. The process is repeated while perfor- mance of both models improve.",
      "The newly-generated question\u2013answer pairs form an enlarged dataset to iteratively retrain the two models. The process is repeated while perfor- mance of both models improve. Investigating the core aspect of QG, we say that a well-trained QG system should have the abil- ity to: (1) \ufb01nd the most salient information in the passage to ask questions about, and (2) given this salient information as target answer, to generate an answer related question. Guo et al. (2018b) lever- aged the \ufb01rst characteristic to improve text sum- marization by performing multi-task learning of summarization with QG, as both these two tasks require the ability to search for salient informa- tion in the passage. Duan et al. (2017) applied the second characteristic to improve QA. For an input question q and a candidate answer \u02c6a, they generate a question \u02c6q for \u02c6a by way of QG system.",
      "Duan et al. (2017) applied the second characteristic to improve QA. For an input question q and a candidate answer \u02c6a, they generate a question \u02c6q for \u02c6a by way of QG system. Since the generated question \u02c6q is closely related to \u02c6a, the similarity between q and \u02c6q helps to evaluate whether \u02c6a is the correct answer. Other works focus on jointly training to combine QG and QA. Wang et al. (2017a) simultaneously train the QG and QA mod- els in the same Seq2Seq model by alternat- ing input data between QA and QG examples. Tang et al. (2018) proposed a training algorithm that generalizes Generative Adversarial Network (GANs) (Goodfellow et al., 2014) under the ques- tion answering scenario. The model improves QG by incorporating an additional QA-speci\ufb01c loss, and improving QA performance by adding arti\ufb01- cially generated training instances from QG.",
      "The model improves QG by incorporating an additional QA-speci\ufb01c loss, and improving QA performance by adding arti\ufb01- cially generated training instances from QG. How- ever, while joint training has shown some effec- tiveness, due to the mixed objectives, its perfor- mance on QG are lower than the state-of-the-art results, which leaves room for future exploration. 6.2 Wider Input Modalities QG work now has incorporated input from knowl- edge bases (KBQG) and images (VQG). Inspired by the use of SQuAD as a question benchmark, Serban et al. (2016) created a 30M large-scale dataset of (KB triple, question) pairs to spur KBQG work. They baselined an attention seq2seq model to generate the target factoid ques- tion. Due to KB sparsity, many entities and pred- icates are unseen or rarely seen at training time. ElSahar et al.",
      "They baselined an attention seq2seq model to generate the target factoid ques- tion. Due to KB sparsity, many entities and pred- icates are unseen or rarely seen at training time. ElSahar et al. (2018) address these few-/zero-shot issues by applying the copying mechanism and in- corporating textual contexts to enrich the informa- tion for rare entities and relations. Since a sin- gle KB triple provides only limited information, KB-generated questions also overgeneralize \u2014 a model asks \u201cWho was born in New York?\u201d when given the triple (Donald Trump, Place of birth, New York). To solve this, Khapra et al. (2017) enrich the input with a sequence of keywords col- lected from its related triples. Visual Question Generation (VQG) is another emerging topic which aims to ask questions given an image. We categorize VQG into grounded- and open-ended VQG by the level of cognition. Grounded VQG generates visually grounded ques- tions, i.e., all relevant information for the answer can be found in the input image (Zhang et al., 2017).",
      "We categorize VQG into grounded- and open-ended VQG by the level of cognition. Grounded VQG generates visually grounded ques- tions, i.e., all relevant information for the answer can be found in the input image (Zhang et al., 2017). A key purpose of grounded VQG is to sup- port the dataset construction for VQA. To ensure the questions are grounded, existing systems rely",
      "on image captions to varying degrees. Ren et al. (2015) and Zhu et al. (2016) simply convert im- age captions into questions using rule-based meth- ods with textual patterns. Zhang et al. (2017) pro- posed a neural model that can generate questions with diverse types for a single image, using sep- arate networks to construct dense image captions and to select question types. In contrast to grounded QG, humans ask higher cognitive level questions about what can be in- ferred rather than what can be seen from an image. Motivated by this, Mostafazadeh et al. (2016) proposed open-ended VQG that aims to gener- ate natural and engaging questions about an im- age. These are deep questions that require high cognition such as analyzing and creation.",
      "Motivated by this, Mostafazadeh et al. (2016) proposed open-ended VQG that aims to gener- ate natural and engaging questions about an im- age. These are deep questions that require high cognition such as analyzing and creation. With signi\ufb01cant progress in deep generative models, marked by variational auto-encoders (VAEs) and GANs, such models are also used in open-ended VQG to bring \u201ccreativity\u201d into generated ques- tions (Jain et al., 2017; Fan et al., 2018), showing promising results. This also brings hope to address deep QG from text, as applied in NLG: e.g., Seq- GAN (Yu et al., 2017) and LeakGAN (Guo et al., 2018c). 6.3 Generation of Deep Questions Endowing a QG system with the ability to ask deep questions will help us build curious ma- chines that can interact with humans in a bet- ter manner. However, Rus et al. (2007) pointed out that asking high-quality deep questions is dif\ufb01cult, even for humans.",
      "However, Rus et al. (2007) pointed out that asking high-quality deep questions is dif\ufb01cult, even for humans. Citing the study from Graesser and Person (1994) to show that stu- dents in college asked only about 6 deep-reasoning questions per hour in a question\u2013encouraging tu- toring session. These deep questions are often about events, evaluation, opinions, syntheses or reasons, corresponding to higher-order cognitive levels. To verify the effectiveness of existing NQG models in generating deep questions, Chen et al. (2018) conducted an empirical study that applies the attention Seq2Seq model on LearningQ, a deep-question centric dataset containing over 60% questions that require reasoning over multiple sen- tences or external knowledge to answer. However, the results were poor; the model achieved minis- cule BLEU-4 scores of < 4 and METEOR scores of < 9, compared with > 12 (BLEU-4) and > 16 (METEOR) on SQuAD.",
      "However, the results were poor; the model achieved minis- cule BLEU-4 scores of < 4 and METEOR scores of < 9, compared with > 12 (BLEU-4) and > 16 (METEOR) on SQuAD. Despite further in-depth analysis are needed to explore the reasons behind, we believe there are two plausible explanations: (1) Seq2Seq models handle long inputs ineffec- tively, and (2) Seq2Seq models lack the ability to reason over multiple pieces of information. Despite still having a long way to go, some works have set out a path forward. A few early QG works attempted to solve this through building deep semantic representations of the entire text, using concept maps over key- words (Olney et al., 2012) or minimal recursion semantics (Yao and Zhang, 2010) to reason over concepts in the text. Labutov et al. (2015) pro- posed a crowdsourcing-based work\ufb02ow that in- volves building an intermediate ontology for the input text, soliciting question templates through crowdsourcing, and generating deep questions based on template retrieval and ranking.",
      "Labutov et al. (2015) pro- posed a crowdsourcing-based work\ufb02ow that in- volves building an intermediate ontology for the input text, soliciting question templates through crowdsourcing, and generating deep questions based on template retrieval and ranking. Although this process is semi-automatic, it provides a prac- tical and ef\ufb01cient way towards deep QG. In a separate line of work, Rothe et al. (2017) pro- posed a framework that simulates how people ask deep questions by treating questions as formal pro- grams that execute on the state of the world, out- putting an answer. Based on our survey, we believe the roadmap towards deep NGQ points towards research that will (1) enhance the NGQ model with the ability to consider relationships among multiple source sen- tences, (2) explicitly model typical reasoning pat- terns, and (3) understand and simulate the mecha- nism behind human question asking. 7 Conclusion \u2013 What\u2019s the Outlook?",
      "7 Conclusion \u2013 What\u2019s the Outlook? We have presented a comprehensive survey of NQG, categorizing current NQG models based on different QG-speci\ufb01c and common technical vari- ations, and summarizing three emerging trends in NQG: multi-task learning, wider input modalities, and deep question generation. What\u2019s next for NGQ? We end with future potential directions by applying past insights to current NQG models; the \u201cunknown unknown\u201d, promising directions yet explored. When to Ask: Besides learning what and how to ask, in many real-world applications that ques- tion plays an important role, such as automated tu- toring and conversational systems, learning when to ask become an important issue. In contrast to general dialog management (Lee et al., 2010), no research has explored when machine should ask an engaging question in dialog. Modeling ques- tion asking as an interactive and dynamic process",
      "may become an interesting topic ahead. Personalized QG: Question asking is quite per- sonalized: people with different characters and knowledge background ask different questions. However, integrating QG with user modeling in dialog management or recommendation system has not yet been explored. Explicitly modeling user state and awareness leads us towards person- alized QG, which dovetails deep, end-to-end QG with deep user modeling and pairs the dual of generation\u2013comprehension much in the same vein as in the vision\u2013image generation area. References Husam Ali, Yllias Chali, and Sadid A Hasan. 2010. Automation of question generation from sentences. In Proceedings of QG2010: The Third Workshop on Question Generation, pages 58\u201367. Lorin W Anderson, David R Krathwohl, Peter W Airasian, Kathleen A Cruikshank, Richard E Mayer, Paul R Pintrich, James Raths, and Merlin C Wit- trock. 2001. A taxonomy for learning, teaching, and assessing: A revision of blooms taxonomy of edu- cational objectives, abridged edition. White Plains, NY: Longman.",
      "2001. A taxonomy for learning, teaching, and assessing: A revision of blooms taxonomy of edu- cational objectives, abridged edition. White Plains, NY: Longman. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar- garet Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. 2015. VQA: visual question an- swering. In IEEE International Conference on Com- puter Vision (ICCV), pages 2425\u20132433. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473. Benjamin Samuel Bloom, Max D Engelhart, Edward J Furst, Walker H Hill, and David R Krathwohl. 1984. Taxonomy of educational objectives: Handbook 1: Cognitive domain. Antoine Bordes, Nicolas Usunier, Alberto Garc\u00b4\u0131a- Dur\u00b4an, Jason Weston, and Oksana Yakhnenko. 2013.",
      "1984. Taxonomy of educational objectives: Handbook 1: Cognitive domain. Antoine Bordes, Nicolas Usunier, Alberto Garc\u00b4\u0131a- Dur\u00b4an, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi- relational data. In Annual Conference on Neural In- formation Processing Systems (NIPS), pages 2787\u2013 2795. Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluation the role of bleu in ma- chine translation research. In Conference of the European Chapter of the Association for Computa- tional Linguistics (EACL). Yllias Chali and Sadid A. Hasan. 2015. Towards topic- to-question generation. Computational Linguistics (CL), 41(1):1\u201320. Guanliang Chen, Jie Yang, Claudia Hauff, and Geert- Jan Houben. 2018. Learningq: A large-scale dataset for educational question generation. In In- ternational Conference on Web and Social Media (ICWSM), pages 481\u2013490.",
      "Guanliang Chen, Jie Yang, Claudia Hauff, and Geert- Jan Houben. 2018. Learningq: A large-scale dataset for educational question generation. In In- ternational Conference on Web and Social Media (ICWSM), pages 481\u2013490. Wei Chen and Jack Mostow. 2009. Generating ques- tions automatically from informational text. In In- ternational Conference on Arti\ufb01cial Intelligence in Education (AIED), pages 17\u201324. Wanyun Cui, Yanghua Xiao, Haixun Wang, Yangqiu Song, Seung-won Hwang, and Wei Wang. 2017. KBQA: learning question answering over QA cor- pora and knowledge bases. The Proceedings of the VLDB Endowment (PVLDB), 10(5):565\u2013576. Takshak Desai, Parag Dakle, and Dan Moldovan. 2018. Generating questions for reading comprehension us- ing coherence relations.",
      "The Proceedings of the VLDB Endowment (PVLDB), 10(5):565\u2013576. Takshak Desai, Parag Dakle, and Dan Moldovan. 2018. Generating questions for reading comprehension us- ing coherence relations. In The 5th Workshop on Natural Language Processing Techniques for Edu- cational Applications (NLP-TEA@ACL), pages 1\u2013 10. Xinya Du and Claire Cardie. 2017. Identifying where to focus in reading comprehension for neural ques- tion generation. In Conference on Empirical Meth- ods in Natural Language Processing (EMNLP), pages 2067\u20132073. Xinya Du and Claire Cardie. 2018. Harvest- ing paragraph-level question-answer pairs from wikipedia. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 1907\u2013 1917. Xinya Du, Junru Shao, and Claire Cardie. 2017. Learn- ing to ask: Neural question generation for reading comprehension. In Annual Meeting of the Associ- ation for Computational Linguistics (ACL), pages 1342\u20131352.",
      "Xinya Du, Junru Shao, and Claire Cardie. 2017. Learn- ing to ask: Neural question generation for reading comprehension. In Annual Meeting of the Associ- ation for Computational Linguistics (ACL), pages 1342\u20131352. Nan Duan, Duyu Tang, Peng Chen, and Ming Zhou. 2017. Question generation for question answer- ing. In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 866\u2013874. Hady ElSahar, Christophe Gravier, and Fr\u00b4ed\u00b4erique Laforest. 2018. Zero-shot question generation from knowledge graphs for unseen predicates and entity types. In Annual Conference of the North American Chapter of the Association for Computational Lin- guistics (NAACL-HLT), pages 218\u2013228. Zhihao Fan, Zhongyu Wei, Siyuan Wang, Yang Liu, and Xuanjing Huang. 2018. A reinforcement learn- ing framework for natural question generation using bi-discriminators. In International Conference on Computational Linguistics (COLING), pages 1763\u2013 1774. D Gates.",
      "2018. A reinforcement learn- ing framework for natural question generation using bi-discriminators. In International Conference on Computational Linguistics (COLING), pages 1763\u2013 1774. D Gates. 2008. Generating look-back strategy ques- tions from expository texts. In The Workshop on the Question Generation Shared Task and Evalua- tion Challenge.",
      "Yichen Gong and Samuel R. Bowman. 2018. Ruminat- ing reader: Reasoning with gated multi-hop atten- tion. In Workshop on Machine Reading for Question Answering@ACL, pages 1\u201311. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. 2014. Gen- erative adversarial nets. In Annual Conference on Neural Information Processing Systems (NIPS), pages 2672\u20132680. Arthur C Graesser and Natalie K Person. 1994. Ques- tion asking during tutoring. American Educational Research Journal, 31(1):104\u2013137. C\u00b8 aglar G\u00a8ulc\u00b8ehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio. 2016. Pointing the unknown words. In Annual Meeting of the Asso- ciation for Computational Linguistics (ACL).",
      "2016. Pointing the unknown words. In Annual Meeting of the Asso- ciation for Computational Linguistics (ACL). Daya Guo, Yibo Sun, Duyu Tang, Nan Duan, Jian Yin, Hong Chi, James Cao, Peng Chen, and Ming Zhou. 2018a. Question generation from SQL queries im- proves neural semantic parsing. In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1597\u20131607. Han Guo, Ramakanth Pasunuru, and Mohit Bansal. 2018b. Soft layer-speci\ufb01c multi-task summarization with entailment and question generation. In Annual Meeting of the Association for Computational Lin- guistics (ACL), pages 687\u2013697. Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and Jun Wang. 2018c. Long text generation via adversarial training with leaked information. In AAAI Conference on Arti\ufb01cial Intelligence (AAAI), pages 5141\u20135148. Vrindavan Harrison and Marilyn A. Walker. 2018.",
      "2018c. Long text generation via adversarial training with leaked information. In AAAI Conference on Arti\ufb01cial Intelligence (AAAI), pages 5141\u20135148. Vrindavan Harrison and Marilyn A. Walker. 2018. Neural generation of diverse questions using answer focus, contextual and linguistic features. In Interna- tional Conference on Natural Language Generation (INLG), pages 296\u2013306. Michael Heilman. 2011. Automatic factual question generation from text. Language Technologies Insti- tute School of Computer Science Carnegie Mellon University, 195. Michael Heilman and Noah A. Smith. 2010. Good question! statistical ranking for question generation. In Annual Conference of the North American Chap- ter of the Association for Computational Linguistics (NAACL-HLT), pages 609\u2013617. Unnat Jain, Ziyu Zhang, and Alexander G. Schwing. 2017. Creativity: Generating diverse questions using variational autoencoders. In IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR), pages 5415\u20135424.",
      "Unnat Jain, Ziyu Zhang, and Alexander G. Schwing. 2017. Creativity: Generating diverse questions using variational autoencoders. In IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR), pages 5415\u20135424. Mitesh M. Khapra, Dinesh Raghu, Sachindra Joshi, and Sathish Reddy. 2017. Generating natural lan- guage question-answer pairs from a knowledge graph using a RNN based question generation model. In Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 376\u2013385. Yanghoon Kim, Hwanhee Lee, Joongbo Shin, and Ky- omin Jung. 2019. Improving neural question gener- ation using answer separation. In AAAI Conference on Arti\ufb01cial Intelligence (AAAI). Tom\u00b4as Kocisk\u00b4y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00b4abor Melis, and Edward Grefenstette. 2018. The narrativeqa read- ing comprehension challenge.",
      "Tom\u00b4as Kocisk\u00b4y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00b4abor Melis, and Edward Grefenstette. 2018. The narrativeqa read- ing comprehension challenge. Transactions of the Association for Computational Linguistics (TACL), 6:317\u2013328. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hin- ton. 2012. Imagenet classi\ufb01cation with deep con- volutional neural networks. In Annual Conference on Neural Information Processing Systems (NIPS), pages 1106\u20131114. Vishwajeet Kumar, Kireeti Boorla, Yogesh Meena, Ganesh Ramakrishnan, and Yuan-Fang Li. 2018a. Automating reading comprehension by generating question and answer pairs. In The Paci\ufb01c-Asia Con- ference on Knowledge Discovery and Data Mining (PAKDD), pages 335\u2013348. Vishwajeet Kumar, Ganesh Ramakrishnan, and Yuan- Fang Li. 2018b.",
      "In The Paci\ufb01c-Asia Con- ference on Knowledge Discovery and Data Mining (PAKDD), pages 335\u2013348. Vishwajeet Kumar, Ganesh Ramakrishnan, and Yuan- Fang Li. 2018b. A framework for automatic ques- tion generation from text using deep reinforcement learning. CoRR, abs/1808.04961. Igor Labutov, Sumit Basu, and Lucy Vanderwende. 2015. Deep questions without deep understanding. In Annual Meeting of the Association for Computa- tional Linguistics (ACL), pages 889\u2013898. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard H. Hovy. 2017. RACE: large-scale read- ing comprehension dataset from examinations. In Conference on Empirical Methods in Natural Lan- guage Processing (EMNLP), pages 785\u2013794. Alon Lavie and Michael J. Denkowski. 2009. The meteor metric for automatic evaluation of machine translation. Machine Translation, 23(2-3):105\u2013115.",
      "Alon Lavie and Michael J. Denkowski. 2009. The meteor metric for automatic evaluation of machine translation. Machine Translation, 23(2-3):105\u2013115. R\u00b4emi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with ap- plication to the biography domain. In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1203\u20131213. Cheongjae Lee, Sangkeun Jung, Kyungduk Kim, Donghyeon Lee, and Gary Geunbae Lee. 2010. Re- cent approaches to dialog management for spoken dialog systems. Journal of Computing Science and Engineering (JCSE), 4(1):1\u201322. Chin-Yew Lin. 2004. Rouge: A package for auto- matic evaluation of summaries. Text Summarization Branches Out.",
      "Zhouhan Lin, Minwei Feng, C\u00b4\u0131cero Nogueira dos San- tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. 2017. A structured self-attentive sentence embedding. CoRR, abs/1703.03130. David Lindberg, Fred Popowich, John C. Nesbit, and Philip H. Winne. 2013. Generating natural lan- guage questions to support learning on-line. In Eu- ropean Workshop on Natural Language Generation (ENLG), pages 105\u2013114. Chia-Wei Liu, Ryan Lowe, Iulian Serban, Michael Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How NOT to evaluate your dialogue sys- tem: An empirical study of unsupervised evaluation metrics for dialogue response generation. In Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 2122\u20132132. Ming Liu, Rafael A. Calvo, and Vasile Rus. 2010. Automatic question generation for literature review writing support.",
      "In Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 2122\u20132132. Ming Liu, Rafael A. Calvo, and Vasile Rus. 2010. Automatic question generation for literature review writing support. In International Conference on In- telligent Tutoring Systems (ITS), pages 45\u201354. Ming Liu, Rafael A. Calvo, and Vasile Rus. 2012. G- asks: An intelligent automatic question generation system for academic writing support. Dialogue and Discourse (D&D), 3(2):101\u2013124. Inderjeet Mani. 1999. Advances in automatic text sum- marization. MIT press. Karen Mazidi and Rodney D. Nielsen. 2014. Linguis- tic considerations in automatic question generation. In Annual Meeting of the Association for Computa- tional Linguistics (ACL), pages 321\u2013326. Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Mar- garet Mitchell, Xiaodong He, and Lucy Vander- wende. 2016.",
      "Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Mar- garet Mitchell, Xiaodong He, and Lucy Vander- wende. 2016. Generating natural questions about an image. In Annual Meeting of the Association for Computational Linguistics (ACL). Preksha Nema and Mitesh M. Khapra. 2018. Towards a better metric for evaluating question generation sys- tems. In Conference on Empirical Methods in Nat- ural Language Processing (EMNLP), pages 3950\u2013 3959. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A human generated machine reading comprehension dataset. In Proceedings of the NIPS Workshop on Cognitive Computation: In- tegrating neural and symbolic approaches. Kamal Nigam and Rayid Ghani. 2000. Analyzing the effectiveness and applicability of co-training. In In- ternational Conference on Information and Knowl- edge Management (CIKM), pages 86\u201393.",
      "Kamal Nigam and Rayid Ghani. 2000. Analyzing the effectiveness and applicability of co-training. In In- ternational Conference on Information and Knowl- edge Management (CIKM), pages 86\u201393. Andrew McGregor Olney, Arthur C. Graesser, and Na- talie K. Person. 2012. Question generation from concept maps. Dialogue and Discourse (D&D), 3(2):75\u201399. Santanu Pal, Tapabrata Mondal, Partha Pakray, Di- pankar Das, and Sivaji Bandyopadhyay. 2010. Qg- stec system description\u2013juqgg: A rule based ap- proach. Proceedings of QG2010: The Third Work- shop on Question Generation, pages 76\u201379. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Annual Meet- ing of the Association for Computational Linguistics (ACL), pages 311\u2013318.",
      "Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Annual Meet- ing of the Association for Computational Linguistics (ACL), pages 311\u2013318. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100, 000+ questions for machine comprehension of text. In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2383\u20132392. Mengye Ren, Ryan Kiros, and Richard S. Zemel. 2015. Exploring models and data for image question an- swering. In Annual Conference on Neural Informa- tion Processing Systems (NIPS), pages 2953\u20132961. Oleg Rokhlenko and Idan Szpektor. 2013. Generat- ing synthetic comparable questions for news articles. In Annual Meeting of the Association for Computa- tional Linguistics (ACL), pages 742\u2013751.",
      "Oleg Rokhlenko and Idan Szpektor. 2013. Generat- ing synthetic comparable questions for news articles. In Annual Meeting of the Association for Computa- tional Linguistics (ACL), pages 742\u2013751. Anselm Rothe, Brenden M. Lake, and Todd M. Gureckis. 2017. Question asking as program gener- ation. In Annual Conference on Neural Information Processing Systems (NIPS), pages 1046\u20131055. Vasile Rus, Zhiqiang Cai, and Art Graesser. 2008. Question generation: Example of a multi-year evalu- ation campaign. Online Proceedings of 1st Question Generation Workshop, NSF, Arlington, VA. Vasile Rus, Zhiqiang Cai, and Arthur C. Graesser. 2007. Experiments on generating questions about facts. In Computational Linguistics and Intelligent Text Processing (CICLing), pages 444\u2013455. Vasile Rus and James C. Lester. 2009. The 2nd workshop on question generation.",
      "2007. Experiments on generating questions about facts. In Computational Linguistics and Intelligent Text Processing (CICLing), pages 444\u2013455. Vasile Rus and James C. Lester. 2009. The 2nd workshop on question generation. In International Conference on Arti\ufb01cial Intelligence in Education (AIED), page 808. Vasile Rus, Brendan Wyse, Paul Piwek, Mihai Lintean, Svetlana Stoyanchev, and Cristian Moldovan. 2010. Overview of the \ufb01rst question generation shared task evaluation challenge. In Proceedings of QG2010: The Third Workshop on Question Generation, pages 45\u201357. Vasile Rus, Brendan Wyse, Paul Piwek, Mihai C. Lin- tean, Svetlana Stoyanchev, and Cristian Moldovan. 2011. Question generation shared task and evalu- ation challenge - status report. In European Work- shop on Natural Language Generation (ENLG), pages 318\u2013320.",
      "2011. Question generation shared task and evalu- ation challenge - status report. In European Work- shop on Natural Language Generation (ENLG), pages 318\u2013320. Vasile Rus, Brendan Wyse, Paul Piwek, Mihai C. Lin- tean, Svetlana Stoyanchev, and Cristian Moldovan. 2012. A detailed account of the \ufb01rst question gen- eration shared task evaluation challenge. Dialogue and Discourse (D&D), 3(2):177\u2013204.",
      "Mrinmaya Sachan and Eric P. Xing. 2018. Self- training for jointly learning to ask and answer ques- tions. In Annual Conference of the North American Chapter of the Association for Computational Lin- guistics (NAACL-HLT), pages 629\u2013640. Iulian Vlad Serban, Alberto Garc\u00b4\u0131a-Dur\u00b4an, C\u00b8 aglar G\u00a8ulc\u00b8ehre, Sungjin Ahn, Sarath Chandar, Aaron C. Courville, and Yoshua Bengio. 2016. Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus. In Annual Meeting of the Association for Computational Lin- guistics (ACL). Linfeng Song, Zhiguo Wang, Wael Hamza, Yue Zhang, and Daniel Gildea. 2018. Leveraging context infor- mation for natural question generation. In Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL- HLT), pages 569\u2013574.",
      "2018. Leveraging context infor- mation for natural question generation. In Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL- HLT), pages 569\u2013574. Sandeep Subramanian, Tong Wang, Xingdi Yuan, Saizheng Zhang, Adam Trischler, and Yoshua Ben- gio. 2018. Neural models for key phrase extraction and question generation. In Workshop on Machine Reading for Question Answering@ACL, pages 78\u2013 88. Xingwu Sun, Jing Liu, Yajuan Lyu, Wei He, Yan- jun Ma, and Shi Wang. 2018. Answer-focused and position-aware neural question generation. In Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 3930\u20133939. Duyu Tang, Nan Duan, Zhao Yan, Zhirui Zhang, Yibo Sun, Shujie Liu, Yuanhua Lv, and Ming Zhou. 2018. Learning to collaborate for question answering and asking.",
      "Duyu Tang, Nan Duan, Zhao Yan, Zhirui Zhang, Yibo Sun, Shujie Liu, Yuanhua Lv, and Ming Zhou. 2018. Learning to collaborate for question answering and asking. In Annual Conference of the North Amer- ican Chapter of the Association for Computational Linguistics (NAACL-HLT), pages 1564\u20131574. Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har- ris, Alessandro Sordoni, Philip Bachman, and Ka- heer Suleman. 2017. Newsqa: A machine com- prehension dataset. In Workshop on Representation Learning for NLP (Rep4NLP@ACL), pages 191\u2013 200. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Annual Conference on Neural Informa- tion Processing Systems (NIPS), pages 6000\u20136010.",
      "2017. Attention is all you need. In Annual Conference on Neural Informa- tion Processing Systems (NIPS), pages 6000\u20136010. Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show and tell: A neural im- age caption generator. In IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), pages 3156\u20133164. Tong Wang, Xingdi Yuan, and Adam Trischler. 2017a. A joint model for question answering and question generation. CoRR, abs/1706.01450. Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and Ming Zhou. 2017b. Gated self-matching net- works for reading comprehension and question an- swering. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 189\u2013198. Zichao Wang, Andrew S. Lan, Weili Nie, Andrew E. Waters, Phillip J. Grimaldi, and Richard G. Bara- niuk. 2018.",
      "In Annual Meeting of the Association for Computational Linguistics (ACL), pages 189\u2013198. Zichao Wang, Andrew S. Lan, Weili Nie, Andrew E. Waters, Phillip J. Grimaldi, and Richard G. Bara- niuk. 2018. Qg-net: a data-driven question genera- tion model for educational content. In Annual ACM Conference on Learning at Scale (L@S), pages 7:1\u2013 7:10. Xuchen Yao, Gosse Bouma, and Yi Zhang. 2012. Semantics-based question generation and imple- mentation. Dialogue and Discourse (D&D), 3(2):11\u201342. Xuchen Yao and Yi Zhang. 2010. Question generation with minimal recursion semantics. In Proceedings of QG2010: The Third Workshop on Question Gen- eration, pages 68\u201375. Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. Seqgan: Sequence generative adversarial nets with policy gradient.",
      "In Proceedings of QG2010: The Third Workshop on Question Gen- eration, pages 68\u201375. Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. Seqgan: Sequence generative adversarial nets with policy gradient. In AAAI Conference on Arti\ufb01- cial Intelligence (AAAI), pages 2852\u20132858. Xingdi Yuan, Tong Wang, C\u00b8 aglar G\u00a8ulc\u00b8ehre, Alessan- dro Sordoni, Philip Bachman, Saizheng Zhang, Sandeep Subramanian, and Adam Trischler. 2017. Machine comprehension by text-to-text neural ques- tion generation. In The 2nd Workshop on Represen- tation Learning for NLP (Rep4NLP@ACL), pages 15\u201325. Shijie Zhang, Lizhen Qu, Shaodi You, Zhenglu Yang, and Jiawan Zhang. 2017. Automatic generation of grounded visual questions. In International Joint Conference on Arti\ufb01cial Intelligence (IJCAI), pages 4235\u20134243.",
      "Shijie Zhang, Lizhen Qu, Shaodi You, Zhenglu Yang, and Jiawan Zhang. 2017. Automatic generation of grounded visual questions. In International Joint Conference on Arti\ufb01cial Intelligence (IJCAI), pages 4235\u20134243. Yao Zhao, Xiaochuan Ni, Yuanyuan Ding, and Qifa Ke. 2018. Paragraph-level neural question generation with maxout pointer and gated self-attention net- works. In Conference on Empirical Methods in Nat- ural Language Processing (EMNLP), pages 3901\u2013 3910. Qingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan, Hangbo Bao, and Ming Zhou. 2017. Neu- ral question generation from text: A preliminary study. In CCF International Conference of Natu- ral Language Processing and Chinese Computing (NLPCC), pages 662\u2013671. Yuke Zhu, Oliver Groth, Michael S. Bernstein, and Li Fei-Fei. 2016. Visual7w: Grounded question an- swering in images.",
      "Yuke Zhu, Oliver Groth, Michael S. Bernstein, and Li Fei-Fei. 2016. Visual7w: Grounded question an- swering in images. In IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), pages 4995\u20135004."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1905.08949.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":13749,
  "avg_doclen":171.8625,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1905.08949.pdf"
    }
  }
}