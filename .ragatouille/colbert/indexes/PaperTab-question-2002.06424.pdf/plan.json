{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Deeper Task-Speci\ufb01city Improves Joint Entity and Relation Extraction Phil Crone Ancestry.com pcrone@ancestry.com Abstract Multi-task learning (MTL) is an effective method for learning related tasks, but designing MTL mod- els necessitates deciding which and how many pa- rameters should be task-speci\ufb01c, as opposed to shared between tasks. We investigate this issue for the problem of jointly learning named entity recognition (NER) and relation extraction (RE) and propose a novel neural architecture that allows for deeper task-speci\ufb01city than does prior work. In par- ticular, we introduce additional task-speci\ufb01c bidi- rectional RNN layers for both the NER and RE tasks and tune the number of shared and task- speci\ufb01c layers separately for different datasets. We achieve state-of-the-art (SOTA) results for both tasks on the ADE dataset; on the CoNLL04 dataset, we achieve SOTA results on the NER task and com- petitive results on the RE task while using an order of magnitude fewer trainable parameters than the current SOTA architecture.",
            "An ablation study con- \ufb01rms the importance of the additional task-speci\ufb01c layers for achieving these results. Our work sug- gests that previous solutions to joint NER and RE undervalue task-speci\ufb01city and demonstrates the importance of correctly balancing the number of shared and task-speci\ufb01c parameters for MTL ap- proaches in general. 1 Introduction Multi-task learning (MTL) refers to machine learning ap- proaches in which information and representations are shared to solve multiple, related tasks. Relative to single-task learn- ing approaches, MTL often shows improved performance on some or all sub-tasks and can be more computationally ef\ufb01- cient [Caruana, 1997; Cipolla et al., 2018; Vandenhende et al., 2019; Li et al., 2019a]. We focus here on a form of MTL known as hard parameter sharing. Hard parameter sharing refers to the use of deep learning models in which inputs to models \ufb01rst pass through a number of shared layers.",
            "We focus here on a form of MTL known as hard parameter sharing. Hard parameter sharing refers to the use of deep learning models in which inputs to models \ufb01rst pass through a number of shared layers. The hid- den representations produced by these shared layers are then fed as inputs to a number of task-speci\ufb01c layers. Within the domain of natural language processing (NLP), MTL approaches have been applied to a wide range of prob- In 1809, author Edgar Allan PoePEOP was born in BostonLOC Figure 1: Example of a sentence containing named entities and rela- tions from the CoNLL04 dataset. This sentence expresses a Lives-In relation between Edgar Allan Poe and Boston. lems [Li et al., 2019a]. In recent years, one particularly fruitful application of MTL to NLP has been joint solving of named entity recognition (NER) and relation extraction (RE), two important information extraction tasks with applications in search, question answering, and knowledge base construc- tion [Jiang, 2012].",
            "In recent years, one particularly fruitful application of MTL to NLP has been joint solving of named entity recognition (NER) and relation extraction (RE), two important information extraction tasks with applications in search, question answering, and knowledge base construc- tion [Jiang, 2012]. NER consists in the identi\ufb01cation of spans of text as corresponding to named entities and the classi\ufb01ca- tion of each span\u2019s entity type. RE consists in the identi\ufb01ca- tion of all triples (ei, ej, r), where ei and ej are named enti- ties and r is a relation that holds between ei and ej according to the text. For example, in Figure 1, Edgar Allan Poe and Boston are named entities of the types People and Location, respectively. In addition, the text indicates that the Lives-In relation obtains between Edgar Allan Poe and Boston. One option for solving these two problems is a pipeline ap- proach using two independent models, each designed to solve a single task, with the output of the NER model serving as an input to the RE model. However, MTL approaches offer a number of advantages over the pipeline approach.",
            "One option for solving these two problems is a pipeline ap- proach using two independent models, each designed to solve a single task, with the output of the NER model serving as an input to the RE model. However, MTL approaches offer a number of advantages over the pipeline approach. First, the pipeline approach is more susceptible to error proroga- tion wherein prediction errors from the NER model enter the RE model as inputs that the latter model cannot correct. Sec- ond, the pipeline approach only allows solutions to the NER task to inform the RE task, but not vice versa. In contrast, the joint approach allows for solutions to either task to inform the other. For example, learning that there is a Lives-In relation between Edgar Allan Poe and Boston can be useful for deter- mining the types of these entities. Finally, the joint approach can be computationally more ef\ufb01cient than the pipeline ap- proach. As mentioned above, MTL approaches are generally more ef\ufb01cient than single-task learning alternatives.",
            "Finally, the joint approach can be computationally more ef\ufb01cient than the pipeline ap- proach. As mentioned above, MTL approaches are generally more ef\ufb01cient than single-task learning alternatives. This is due to the fact that solutions to related tasks often rely on similar information, which in an MTL setting only needs to be represented in one model in order to solve all tasks. For ex- ample, the fact that Edgar Allan Poe is followed by was born can help a model determine both that Edgar Allan Poe is an instance of a People entity and that the sentence expresses a Lives-In relation. arXiv:2002.06424v1  [cs.CL]  15 Feb 2020",
            "While the choice as to which and how many layers to share between tasks is known to be an important factor rel- evant to the performance of MTL models [Zhao et al., 2018; Vandenhende et al., 2019], this issue has received relatively little attention within the context of joint NER and RE. As we show below in Section 2, prior proposals for jointly solv- ing NER and RE have typically made use of very few task- speci\ufb01c parameters or have mostly used task-speci\ufb01c param- eters only for the RE task. We seek to correct for this over- sight by proposing a novel neural architecture for joint NER and RE. In particular, we make the following contributions: 1. We allow for deeper task-speci\ufb01city than does previous work via the use of additional task-speci\ufb01c bidirectional recurrent neural networks (BiRNNs) for both tasks. 2.",
            "In particular, we make the following contributions: 1. We allow for deeper task-speci\ufb01city than does previous work via the use of additional task-speci\ufb01c bidirectional recurrent neural networks (BiRNNs) for both tasks. 2. Because the relatedness between the NER and RE tasks is not constant across all textual domains, we take the number of shared and task-speci\ufb01c layers to be an ex- plicit hyperparameter of the model that can be tuned sep- arately for different datasets. We evaluate the proposed architecture on two publicly available datasets: the Adverse Drug Events (ADE) dataset [Gurulingappa et al., 2012] and the CoNLL04 dataset [Roth and Yih, 2004]. We show that our architecture is able to out- perform the current state-of-the-art (SOTA) results on both the NER and RE tasks in the case of ADE. In the case of CoNLL04, our proposed architecture achieves SOTA perfor- mance on the NER task and achieves near SOTA performance on the RE task.",
            "In the case of CoNLL04, our proposed architecture achieves SOTA perfor- mance on the NER task and achieves near SOTA performance on the RE task. On both datasets, our results are SOTA when averaging performance across both tasks. Moreover, we achieve these results using an order of magnitude fewer trainable parameters than the current SOTA architecture. 2 Related Work We focus in this section on previous deep learning approaches to solving the tasks of NER and RE, as this work is most directly comparable to our proposal. Most work on joint NER and RE has adopted a BIO or BILOU scheme for the NER task, where each token is labeled to indicate whether it is the (B)eginning of an entity, (I)nside an entity, or (O)utside an entity. The BILOU scheme extends these labels to indicate if a token is the (L)ast token of an entity or is a (U)nit, i.e. the only token within an entity span. Several approaches treat the NER and RE tasks as if they were a single task. For example, Gupta et al.",
            "the only token within an entity span. Several approaches treat the NER and RE tasks as if they were a single task. For example, Gupta et al. [2016], follow- ing Miwa and Sasaki [2014], treat the two tasks as a table- \ufb01lling problem where each cell in the table corresponds to a pair of tokens (ti, tj) in the input text. For the diagonal of the table, the cell label is the BILOU tag for ti. All other cells are labeled with the relation r, if it exists, such that (ei, ej, r), where ei is the entity whose span\u2019s \ufb01nal token is ti, is in the set of true relations. A BiRNN is trained to \ufb01ll the cells of the table. Zheng et al. [2017] introduce a BILOU tagging scheme that incorporates relation information into the tags, allowing them to treat both tasks as if they were a single NER task. A series of two bidirectional LSTM (BiLSTM) layers and a \ufb01nal softmax layer are used to produce output tags. Li et al.",
            "A series of two bidirectional LSTM (BiLSTM) layers and a \ufb01nal softmax layer are used to produce output tags. Li et al. [2019b] solve both tasks as a form of multi-turn question answering in which the input text is queried with question templates \ufb01rst to detect entities and then, given the detected entities, to detect any relations between these entities. Li et al. use BERT [Devlin et al., 2019] as the backbone of their question-answering model and produce answers by tagging the input text with BILOU tags to identify the span corre- sponding to the answer(s). The above approaches allow for very little task-speci\ufb01city, since both the NER task and the RE task are coerced into a single task. Other approaches incorporate greater task- speci\ufb01city in one of two ways. First, several models share the majority of model parameters between the NER and RE tasks, but also have separate scoring and\/or output layers used to produce separate outputs for each task. For example, Katiyar and Cardie [2017] and Bekoulis et al.",
            "First, several models share the majority of model parameters between the NER and RE tasks, but also have separate scoring and\/or output layers used to produce separate outputs for each task. For example, Katiyar and Cardie [2017] and Bekoulis et al. [2018] propose mod- els in which token representations \ufb01rst pass through one or more shared BiLSTM layers. Katiyar and Cardie use a soft- max layer to tag tokens with BILOU tags to solve the NER task and use an attention layer to detect relations between each pair of entities. Bekoulis et al., following Lample et al. [2016], use a conditional random \ufb01eld (CRF) layer to pro- duce BIO tags for the NER task. The output from the shared BiLSTM layer for every pair of tokens is passed through re- lation scoring and sigmoid layers to predict relations. A second method of incorporating greater task-speci\ufb01city into these models is via deeper layers for solving the RE task. Miwa and Bansal [2016] and Li et al.",
            "A second method of incorporating greater task-speci\ufb01city into these models is via deeper layers for solving the RE task. Miwa and Bansal [2016] and Li et al. [2017] pass token rep- resentations through a BiLSTM layer and then use a softmax layer to label each token with the appropriate BILOU label. Both proposals then use a type of tree-structured bidirectional LSTM layer stacked on top of the shared BiLSTM to solve the RE task. Nguyen and Verspoor [2019] use BiLSTM and CRF layers to perform the NER task. Label embeddings are created from predicted NER labels, concatenated with token representations, and then passed through a RE-speci\ufb01c BiL- STM. A biaf\ufb01ne attention layer [Dozat and Manning, 2016] operates on the output of this BiLSTM to predict relations. An alternative to the BIO\/BILOU scheme is the span-based approach, wherein spans of the input text are directly labeled as to whether they correspond to any entity and, if so, their entity types. Luan et al.",
            "An alternative to the BIO\/BILOU scheme is the span-based approach, wherein spans of the input text are directly labeled as to whether they correspond to any entity and, if so, their entity types. Luan et al. [2018] adopt a span-based approach in which token representations are \ufb01rst passed through a BiL- STM layer. The output from the BiLSTM is used to con- struct representations of candidate entity spans, which are then scored for both the NER and RE tasks via feed for- ward layers. Luan et al. [2019] follow a similar approach, but construct coreference and relation graphs between entities to propagate information between entities connected in these graphs. The resulting entity representations are then classi- \ufb01ed for NER and RE via feed forward layers. To the best of our knowledge, the current SOTA model for joint NER and RE is the span-based proposal of Eberts and Ulges [2019]. In this architecture, token representations are obtained using a pre-trained BERT model that is \ufb01ne-tuned during training. Representations for candidate entity spans are obtained by max pooling over all tokens in each span.",
            "In this architecture, token representations are obtained using a pre-trained BERT model that is \ufb01ne-tuned during training. Representations for candidate entity spans are obtained by max pooling over all tokens in each span. Span representa- tions are passed through an entity classi\ufb01cation layer to solve the NER task. Representations of all pairs of spans that are predicted to be entities and representations of the contexts",
            "B-LOC O B-PEOP I-PEOP Token Representations Shared BiRNN Layer(s) NER-Speci\ufb01c BiRNN Layer(s) NER Scoring Layer CRF NER Output RE-Speci\ufb01c BiRNN Layer(s) Label Embedding  Concatenation &  Filtering RE Scoring Layer RE Output Figure 2: Illustration of our proposed architecture. Token representations are derived from a pre-trained ELMo model, pre-trained GloVe embeddings, learned character-based embeddings, and one-hot encoded casing vectors. The number of shared and task-speci\ufb01c BiRNN layers is treated as a hyperparameter of the model architecture. Only the \ufb01nal token in each entity span is used for predictions for the RE task; grey boxes indicate tokens that are not used for relation predictions. The output for the RE task is a vector of size |R| for all pairs of entities, where R is the set of all possible relations. between these pairs are then passed through a \ufb01nal layer with sigmoid activation to predict relations between entities.",
            "The output for the RE task is a vector of size |R| for all pairs of entities, where R is the set of all possible relations. between these pairs are then passed through a \ufb01nal layer with sigmoid activation to predict relations between entities. With respect to their degrees of task-speci\ufb01city, these span-based approaches resemble the BIO\/BILOU approaches in which the majority of model parameters are shared, but each task possesses independent scoring and\/or output layers. Overall, previous approaches to joint NER and RE have experimented little with deep task-speci\ufb01city, with the excep- tion of those models that include additional layers for the RE task. To our knowledge, no work has considered including additional NER-speci\ufb01c layers beyond scoring and\/or output layers. This may re\ufb02ect a residual in\ufb02uence of the pipeline approach in which the NER task must be solved \ufb01rst before additional layers are used to solve the RE task. However, there is no a priori reason to think that the RE task would ben- e\ufb01t more from additional task-speci\ufb01c layers than the NER task.",
            "However, there is no a priori reason to think that the RE task would ben- e\ufb01t more from additional task-speci\ufb01c layers than the NER task. We also note that while previous work has tackled joint NER and RE in variety of textual domains, in all cases the number of shared and task-speci\ufb01c parameters is held con- stant across these domains. 3 Model The architecture proposed here is inspired by several previous proposals [Katiyar and Cardie, 2017; Bekoulis et al., 2018; Nguyen and Verspoor, 2019]. We treat the NER task as a sequence labeling problem using BIO labels. Token repre- sentations are \ufb01rst passed through a series of shared, BiRNN layers. Stacked on top of these shared BiRNN layers is a se- quence of task-speci\ufb01c BiRNN layers for both the NER and RE tasks. We take the number of shared and task-speci\ufb01c layers to be a hyperparameter of the model.",
            "Stacked on top of these shared BiRNN layers is a se- quence of task-speci\ufb01c BiRNN layers for both the NER and RE tasks. We take the number of shared and task-speci\ufb01c layers to be a hyperparameter of the model. Both sets of task- speci\ufb01c BiRNN layers are followed by task-speci\ufb01c scoring and output layers. Figure 2 illustrates this architecture. Be- low, we use superscript e for NER-speci\ufb01c variables and lay- ers and superscript r for RE-speci\ufb01c variables and layers. 3.1 Shared Layers We obtain contextual token embeddings using the pre-trained ELMo 5.5B model [Peters et al., 2018].1 For each token in the input text ti, this model returns three vectors, which we combine via a weighted averaging layer.",
            "3.1 Shared Layers We obtain contextual token embeddings using the pre-trained ELMo 5.5B model [Peters et al., 2018].1 For each token in the input text ti, this model returns three vectors, which we combine via a weighted averaging layer. Each token ti\u2019s weighted ELMo embedding telmo i is concatenated to a pre- trained GloVe embedding [Pennington et al., 2014] tglove i , a character-level word embedding tchar i learned via a single BiRNN layer [Lample et al., 2016] and a one-hot encoded casing vector tcasing i . The full representation of ti is given by vi (where \u25e6denotes concatenation): vi = telmo i \u25e6tglove i \u25e6tchar i \u25e6tcasing i (1) For an input text with n tokens, v1:n are fed as input to a sequence of one or more shared BiRNN layers, with the out- put sequence from the ith shared BiRNN layer serving as the input sequence to the i + 1st shared BiRNN layer.",
            "3.2 NER-Speci\ufb01c Layers The \ufb01nal shared BiRNN layer is followed by a sequence of zero or more NER-speci\ufb01c BiRNN layers; the output of the \ufb01nal shared BiRNN layer serves as input to the \ufb01rst NER- speci\ufb01c BiRNN layer, if such a layer exists, and the output from from the ith NER-speci\ufb01c BiRNN layer serves as input to the i + 1st NER-speci\ufb01c BiRNN layer. For every token ti, let he i denote an NER-speci\ufb01c hidden representation for ti corresponding to the ith element of the output sequence from the \ufb01nal NER-speci\ufb01c BiRNN layer or the \ufb01nal shared BiRNN layer if there are zero NER-speci\ufb01c BiRNN layers. 1We also experimented with using a pre-trained BERT model rather than ELMo, but performance when using ELMo was slightly higher than when using BERT.",
            "1We also experimented with using a pre-trained BERT model rather than ELMo, but performance when using ELMo was slightly higher than when using BERT. In order to help minimize the total number of trainable parameters in our model, we did not experiment with \ufb01ne-tuning ELMo or BERT.",
            "An NER score for token ti, se i, is obtained by passing he i through a series of two feed forward layers: se i = FFNN(e2) \u0010 (FFNN(e1)(he i)) \u0011 (2) The activation function of FFNN(e1) and its output size are treated as hyperparameters. FFNN(e2) uses linear activation and its output size is |E|, where E is the set of possible entity types. The sequence of NER scores for all tokens, se 1:n, is then passed as input to a linear-chain CRF layer to produce the \ufb01nal BIO tag predictions, \u02c6ye 1:n. During inference, Viterbi decoding is used to determine the most likely sequence \u02c6ye 1:n. 3.3 RE-Speci\ufb01c Layers Similar to the NER-speci\ufb01c layers, the output sequence from the \ufb01nal shared BiRNN layer is fed through zero or more RE- speci\ufb01c BiRNN layers.",
            "3.3 RE-Speci\ufb01c Layers Similar to the NER-speci\ufb01c layers, the output sequence from the \ufb01nal shared BiRNN layer is fed through zero or more RE- speci\ufb01c BiRNN layers. Let hr i denote the ith output from the \ufb01nal RE-speci\ufb01c BiRNN layer or the \ufb01nal shared BiRNN layer if there are no RE-speci\ufb01c BiRNN layers. Following previous work [Gupta et al., 2016; Bekoulis et al., 2018; Nguyen and Verspoor, 2019], we predict relations between entities ei and ej using learned representations from the \ufb01nal tokens of the spans corresponding to ei and ej. To this end, we \ufb01lter the sequence hr 1:n to include only elements hr i such that token ti is the \ufb01nal token in an entity span. Dur- ing training, ground truth entity spans are used for \ufb01ltering. During inference, predicted entity spans derived from \u02c6ye 1:n are used.",
            "Dur- ing training, ground truth entity spans are used for \ufb01ltering. During inference, predicted entity spans derived from \u02c6ye 1:n are used. Each hr i is concatenated to a learned NER label embedding for ti, le i: gr i = hr i \u25e6le i (3) Ground truth NER labels are used to obtain le 1:n during train- ing, and predicted NER labels are used during inference.2 Next, RE scores are computed for every pair (gr i , gr j). If R is the set of possible relations, we calculate the DISTMULT score [Yang et al., 2014] for every relation rk \u2208R and every pair (gr i , gr j) as follows: DISTMULTrk(gr i , gr j) = (gr i )T M rkgr j (4) M rk is a diagonal matrix such that M rk \u2208Rp\u00d7p, where p is the dimensionality of gr i .",
            "We also pass each RE-speci\ufb01c hidden representation gr i through a single feed forward layer: f r i = FFNN(r1)(gr i ) (5) As in the case of FFNN(e1), the activation function of FFNN(r1) and its output size are treated as hyperparameters. Let DISTMULTr i,j denote the concatenation of DISTMULTrk(gr i , gr j) for all rk \u2208R and let cosi,j de- note the cosine distance between vectors f r i and f r j . We obtain RE scores for (ti, tj) via a feed forward layer: sr i,j = FFNN(r2) \u0000f r i \u25e6f r j \u25e6cosi,j \u25e6DISTMULTr i,j \u0001 (6) 2Because ground truth NER labels are used to generate label em- beddings during training, the output from the BiRNN layer(s) de- scribed in Section 3.2 is opaque to the RE-speci\ufb01c portion of the model during training.",
            "If predicted NER labels were used during training instead, as in [Nguyen and Verspoor, 2019], these layers would be shared rather than NER-speci\ufb01c. Hyperparameter ADE CoNLL04 BiRNN Type GRU GRU Character BiRNN Size 32 32 Non-Character BiRNN Size 128 256 # Shared BiRNN Layers 1 1 # NER-Speci\ufb01c BiRNN Layers 1 1 # RE-Speci\ufb01c BiRNN Layers 1 2 FFNN(e1) Activation ReLU tanh FFNN(e1) Output Size 64 64 FFNN(r1) Activation ReLU ReLU FFNN(r1) Output Size 128 128 Label Embedding Size 25 25 Pre-BiRNN Dropout 0.5 0.35 Pre-RE Scoring Dropout 0.5 0.5 RE Threshold \u03b8r 0.9 0.9 Table 1: Optimal hyperparameters used for \ufb01nal training on the ADE and CoNLL04 datasets. FFNN(r2) uses linear activation, and its output size is |R|.",
            "FFNN(r2) uses linear activation, and its output size is |R|. Final relation predictions for a pair of tokens (ti, tj), \u02c6yr i,j, are obtained by passing sr i,j through an elementwise sigmoid layer. A relation is predicted for all outputs from this sigmoid layer exceeding \u03b8r, which we treat as a hyperparameter. 3.4 Training During training, character embeddings, label embeddings, and weights for the weighted average layer, all BiRNN weights, all feed forward networks, and M rk for all rk \u2208R are trained in a supervised manner. As mentioned above, BIO tags for all tokens are used as labels for the NER task. For the the RE task, binary outputs are used. For every relation rk \u2208R and for every pair of tokens (ti, tj) such that ti is the \ufb01nal token of entity ei and tj is the \ufb01nal token of entity ej, the RE label yrk i,j = 1 if (ei, ej, rk) is a true relation. Otherwise, we have yrk i,j = 0. For both output layers, we compute the cross-entropy loss.",
            "Otherwise, we have yrk i,j = 0. For both output layers, we compute the cross-entropy loss. If LNER and LRE denote the cross-entropy loss for the NER and RE outputs, respectively, then the total model loss is given by L = LNER + \u03bbrLRE. The weight \u03bbr is treated as a hyperparameter and allows for tuning the relative impor- tance of the NER and RE tasks during training. Final training for both datasets used a value of 5 for \u03bbr. For the ADE dataset, we trained using the Adam optimizer with a mini-batch size of 16. For the CoNLL04 dataset, we used the Nesterov Adam optimizer with and a mini-batch size of 2. For both datasets, we used a learning rate of 5 \u00d7 10\u22124, During training, dropout was applied before each BiRNN layer, other than the character BiRNN layer, and before the RE scoring layer. 4 Experiments We evaluate the architecture described above using the fol- lowing two publicly available datasets.",
            "Dataset Model Metric Type NER RE Avg. Precision Recall F1 Precision Recall F1 F1 ADE Bekoulis et al. [2018] Macro 84.72 88.16 86.40 72.10 77.24 74.58 80.49 Eberts and Ulges [2019] Macro 89.26 89.26 89.25 78.09 80.43 79.24 84.25 Ours Macro 89.06 89.63 89.48 80.51 86.81 83.74 86.61 CoNLL04 Nguyen and Verspoor [2019] Macro \u2013 \u2013 86.20 \u2013 \u2013 64.40 75.30 Li et al.",
            "[2019b] Micro 89.00 86.60 87.80 69.20 68.20 68.90 78.35 Eberts and Ulges [2019] Macro 85.78 86.84 86.25 74.75 71.52 72.87 79.56 Eberts and Ulges [2019] Micro 88.25 89.64 88.94 73.04 70.00 71.47 80.21 Ours Macro 87.92 86.42 87.00 77.73 68.38 72.63 79.82 Ours Micro 89.84 89.73 89.78 78.69 64.84 71.08 80.43 Table 2: Precision, Recall, and F1 scores for our model and other recent models on the ADE and CoNLL04 datasets. Because our scores are averaged across multiple trials, F1 scores shown here cannot be directly calculated from the precision and recall scores shown here. Note that Nguyen and Verspoor do not report precision and recall scores.",
            "Because our scores are averaged across multiple trials, F1 scores shown here cannot be directly calculated from the precision and recall scores shown here. Note that Nguyen and Verspoor do not report precision and recall scores. ADE The Adverse Drug Events (ADE) dataset [Gurulingappa et al., 2012] consists of 4,272 sentences describing adverse ef- fects from the use of particular drugs. The text is annotated using two entity types (Adverse-Effect and Drug) and a single relation type (Adverse-Effect). Of the entity instances in the dataset, 120 overlap with other entities. Similar to prior work using BIO\/BILOU tagging, we remove overlapping entities. We preserve the entity with the longer span and remove any relations involving a removed entity. There are no of\ufb01cial training, dev, and test splits for the ADE dataset, leading previous researchers to use some form of cross-validation when evaluating their models on this dataset. We split out 10% of the data to use as a held-out dev set.",
            "There are no of\ufb01cial training, dev, and test splits for the ADE dataset, leading previous researchers to use some form of cross-validation when evaluating their models on this dataset. We split out 10% of the data to use as a held-out dev set. Final results are obtained via 10-fold cross-validation using the remaining 90% of the data and the hyperparame- ters obtained from tuning on the dev set. Following previous work, we report macro-averaged performance metrics aver- aged across each of the 10 folds. CoNLL04 The CoNLL04 dataset [Roth and Yih, 2004] consists of 1,441 sentences from news articles annotated with four entity types (Location, Organization, People, and Other) and \ufb01ve relation types (Works-For, Kill, Organization-Based-In, Lives-In, and Located-In). This dataset contains no overlapping entities. We use the three-way split of [Gupta et al., 2016], which contains 910 training, 243 dev, and 288 test sentences. All hyperparameters are tuned against the dev set.",
            "This dataset contains no overlapping entities. We use the three-way split of [Gupta et al., 2016], which contains 910 training, 243 dev, and 288 test sentences. All hyperparameters are tuned against the dev set. Final results are obtained by averaging results from \ufb01ve trials with ran- dom weight initializations in which we trained on the com- bined training and dev sets and evaluated on the test set. As previous work using the CoNLL04 dataset has reported both micro- and macro-averages, we report both sets of metrics. In evaluating NER performance on these datasets, a pre- dicted entity is only considered a true positive if both the en- tity\u2019s span and span type are correctly predicted. In evaluat- ing RE performance, we follow previous work in adopting a strict evaluation method wherein a predicted relation is only considered correct if the spans corresponding to the two argu- ments of this relation and the entity types of these spans are also predicted correctly.",
            "In evaluat- ing RE performance, we follow previous work in adopting a strict evaluation method wherein a predicted relation is only considered correct if the spans corresponding to the two argu- ments of this relation and the entity types of these spans are also predicted correctly. We experimented with LSTMs and GRUs for all BiRNN layers in the model and experimented with using 1\u22123 shared BiRNN layers and 0\u22123 task-speci\ufb01c BiRNN layers for each task. Hyperparameters used for \ufb01nal training are listed in Table 1. 4.1 Results Full results for the performance of our model, as well as other recent work, are shown in Table 2. In addition to precision, recall, and F1 scores for both tasks, we show the average of the F1 scores across both tasks. On the ADE dataset, we achieve SOTA results for both the NER and RE tasks. On the CoNLL04 dataset, we achieve SOTA results on the NER task, while our performance on the RE task is competitive with other recent models.",
            "On the ADE dataset, we achieve SOTA results for both the NER and RE tasks. On the CoNLL04 dataset, we achieve SOTA results on the NER task, while our performance on the RE task is competitive with other recent models. On both datasets, we achieve SOTA re- sults when considering the average F1 score across both tasks. The largest gain relative to the previous SOTA performance is on the RE task of the ADE dataset, where we see an absolute improvement of 4.5 on the macro-average F1 score. While the model of Eberts and Ulges [2019] outperforms our proposed architecture on the CoNLL04 RE task, their results come at the cost of greater model complexity. As mentioned above, Eberts and Ulges \ufb01ne-tune the BERTBASE model, which has 110 million trainable parameters. In con- trast, given the hyperparameters used for \ufb01nal training on the CoNLL04 dataset, our proposed architecture has approx- imately 6 million trainable parameters.",
            "In con- trast, given the hyperparameters used for \ufb01nal training on the CoNLL04 dataset, our proposed architecture has approx- imately 6 million trainable parameters. The fact that the optimal number of task-speci\ufb01c layers dif- fered between the two datasets demonstrates the value of tak- ing the number of shared and task-speci\ufb01c layers to be a hy- perparameter of our model architecture. As shown in Table 1, the \ufb01nal hyperparameters used for the CoNLL04 dataset included an additional RE-speci\ufb01c BiRNN layer than did the \ufb01nal hyperparameters used for the ADE dataset. We suspect that this is due to the limited number of relations and enti- ties in the ADE dataset. For most examples in this dataset, it is suf\ufb01cient to correctly identify a single Drug entity, a sin- gle Adverse-Effect entity, and an Adverse-Effect relation be- tween the two entities. Thus, the NER and RE tasks for this dataset are more closely related than they are in the case of",
            "Model Type Metric Type NER F1 RE F1 Baseline Macro 87.00 72.63 Micro 89.78 71.08 No NER-Speci\ufb01c Macro 86.19 68.45 BiRRNs Micro 89.24 67.05 No RE-Speci\ufb01c Macro 85.84 59.57 BiRRNs Micro 89.06 57.82 No Task-Speci\ufb01c Macro 86.16 57.45 BiRRNs Micro 89.46 55.47 Table 3: Results from an ablation study using the CoNLL04 dataset. All models have the same number of total parameters. the CoNLL04 dataset. Intuitively, cases in which the NER and RE problems can be solved by relying on more shared information should require fewer task-speci\ufb01c layers. 4.2 Ablation Study To further demonstrate the effectiveness of the additional task-speci\ufb01c BiRNN layers in our architecture, we conducted an ablation study using the CoNLL04 dataset.",
            "4.2 Ablation Study To further demonstrate the effectiveness of the additional task-speci\ufb01c BiRNN layers in our architecture, we conducted an ablation study using the CoNLL04 dataset. We trained and evaluated in the same manner described above, using the same hyperparameters, with the following exceptions: 1. We used either (i) zero NER-speci\ufb01c BiRNN layers, (ii) zero RE-speci\ufb01c BiRNN layers, or (iii) zero task- speci\ufb01c BiRNN layers of any kind. 2. We increased the number of shared BiRNN layers to keep the total number of model parameters consistent with the number of parameters in the baseline model. 3. We average the results for each set of hyperparameter across three trials with random weight initializations. Table 3 contains the results from the ablation study. These results show that the proposed architecture bene\ufb01ts from the inclusion of both NER- and RE-speci\ufb01c layers. However, the RE task bene\ufb01ts much more from the inclusion of these task- speci\ufb01c layers than does the NER task.",
            "These results show that the proposed architecture bene\ufb01ts from the inclusion of both NER- and RE-speci\ufb01c layers. However, the RE task bene\ufb01ts much more from the inclusion of these task- speci\ufb01c layers than does the NER task. We take this to re- \ufb02ect the fact that the RE task is more dif\ufb01cult than the NER task for the CoNLL04 dataset, and therefore bene\ufb01ts the most from its own task-speci\ufb01c layers. This is consistent with the fact that the hyperparameter setting that performs best on the RE task is that with no NER-speci\ufb01c BiRNN layers, i.e. the setting that retained RE-speci\ufb01c BiRNN layers. In contrast, the inclusion of task-speci\ufb01c BiRNN layers of any kind had relatively little impact on the performance on the NER task. Note that the setting with no NER-speci\ufb01c layers is some- what similar to the setup of Nguyen and Verspoor\u2019s [2019] model, but includes an additional shared and an additional RE-speci\ufb01c layer.",
            "Note that the setting with no NER-speci\ufb01c layers is some- what similar to the setup of Nguyen and Verspoor\u2019s [2019] model, but includes an additional shared and an additional RE-speci\ufb01c layer. That this setting outperforms Nguyen et al.\u2019s model re\ufb02ects the contribution of having deeper shared and RE-speci\ufb01c layers, separate from the contribution of NER-speci\ufb01c layers. 5 Conclusion Our results demonstrate the utility of using deeper task- speci\ufb01city in models for joint NER and RE and of tuning the level of task-speci\ufb01city separately for different datasets. We conclude that prior work on joint NER and RE underval- ues the importance of task-speci\ufb01city. More generally, these results underscore the importance of correctly balancing the number of shared and task-speci\ufb01c parameters in MTL. We note that other approaches that employ a single model architecture across different datasets are laudable insofar as we should prefer models that can generalize well across do- mains with little domain-speci\ufb01c hyperparameter tuning.",
            "We note that other approaches that employ a single model architecture across different datasets are laudable insofar as we should prefer models that can generalize well across do- mains with little domain-speci\ufb01c hyperparameter tuning. On the other hand, the similarity between the NER and RE tasks varies across domains, and improved performance can be achieved on these tasks by tuning the number of shared and task-speci\ufb01c parameters. In our work, we treated the number of shared and task-speci\ufb01c layers as a hyperparameter to be tuned for each dataset, but future work may explore ways to select this aspect of the model architecture in a more princi- pled way. For example, Vandenhende et al. [2019] propose using a measure of af\ufb01nity between tasks to determine how many layers to share in MTL networks. Task af\ufb01nity scores of NER and RE could be computed for different textual domains or datasets, which could then guide the decision regarding the number of shared and task-speci\ufb01c layers to employ for joint NER and RE models deployed on these domains.",
            "Task af\ufb01nity scores of NER and RE could be computed for different textual domains or datasets, which could then guide the decision regarding the number of shared and task-speci\ufb01c layers to employ for joint NER and RE models deployed on these domains. Other extensions to the present work could include \ufb01ne- tuning the model used to obtain contextual word embeddings, e.g. ELMo or BERT, during training. In order to minimize the number of trainable parameters, we did not employ such \ufb01ne- tuning in our model, but we suspect a \ufb01ne-tuning approach could lead to improved performance relative to our results. An additional opportunity for future work would be an ex- tension of this work to other related NLP tasks, such as co- reference resolution and cross-sentential relation extraction. References [Bekoulis et al., 2018] Giannis Bekoulis, Johannes Deleu, Thomas Demeester, and Chris Develder. Joint entity recognition and relation extraction as a multi-head selec- tion problem. Expert Systems with Applications, 114:34\u2013 45, 2018.",
            "Joint entity recognition and relation extraction as a multi-head selec- tion problem. Expert Systems with Applications, 114:34\u2013 45, 2018. [Caruana, 1997] Rich Caruana. Multitask learning. Machine Learning, 28(1):41\u201375, 1997. [Cipolla et al., 2018] Roberto Cipolla, Yarin Gal, and Alex Kendall. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. 2018 IEEE\/CVF Conference on Computer Vision and Pattern Recognition, Jun 2018. [Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Ken- ton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understand- ing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapo- lis, Minnesota, June 2019. Association for Computational Linguistics.",
            "[Dozat and Manning, 2016] Timothy Dozat and Christo- pher D Manning. Deep biaf\ufb01ne attention for neural depen- dency parsing. arXiv preprint arXiv:1611.01734, 2016. [Eberts and Ulges, 2019] Markus Eberts and Adrian Ulges. Span-based joint entity and relation extraction with trans- former pre-training. arXiv preprint arXiv:1909.07755, 2019. [Gupta et al., 2016] Pankaj Gupta, Hinrich Sch\u00a8utze, and Bernt Andrassy. Table \ufb01lling multi-task recurrent neural network for joint entity and relation extraction. In Pro- ceedings of COLING 2016, the 26th International Con- ference on Computational Linguistics: Technical Papers, pages 2537\u20132547, Osaka, Japan, December 2016. The COLING 2016 Organizing Committee.",
            "In Pro- ceedings of COLING 2016, the 26th International Con- ference on Computational Linguistics: Technical Papers, pages 2537\u20132547, Osaka, Japan, December 2016. The COLING 2016 Organizing Committee. [Gurulingappa et al., 2012] H Gurulingappa, AM Rajput, A Roberts, J Fluck, M Hofmann-Apitius, and L Toldo. Development of a benchmark corpus to support the au- tomatic extraction of drug-related adverse effects from medical case reports. Journal of Biomedical Informatics, 45(5):885, 2012. [Jiang, 2012] Jing Jiang. Information extraction from text. In Mining Text Data, pages 11\u201341. Springer, 2012. [Katiyar and Cardie, 2017] Arzoo Katiyar and Claire Cardie. Going out on a limb: Joint extraction of entity mentions and relations without dependency trees.",
            "In Mining Text Data, pages 11\u201341. Springer, 2012. [Katiyar and Cardie, 2017] Arzoo Katiyar and Claire Cardie. Going out on a limb: Joint extraction of entity mentions and relations without dependency trees. In Proceedings of the 55th Annual Meeting of the Association for Computa- tional Linguistics (Volume 1: Long Papers), pages 917\u2013 928, Vancouver, Canada, July 2017. Association for Com- putational Linguistics. [Lample et al., 2016] Guillaume Lample, Miguel Balles- teros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. Neural architectures for named entity recog- nition. Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2016. [Li et al., 2017] Fei Li, Meishan Zhang, Guohong Fu, and Donghong Ji. A neural joint model for entity and rela- tion extraction from biomedical text.",
            "[Li et al., 2017] Fei Li, Meishan Zhang, Guohong Fu, and Donghong Ji. A neural joint model for entity and rela- tion extraction from biomedical text. BMC Bioinformatics, 18(1):198, 2017. [Li et al., 2019a] Jianquan Li, Xiaokang Liu, Wenpeng Yin, Min Yang, and Liqun Ma. An empirical evaluation of multi-task learning in deep neural networks for natural lan- guage processing. arXiv preprint arXiv:1908.07820, 2019. [Li et al., 2019b] Xiaoya Li, Fan Yin, Zijun Sun, Xiayu Li, Arianna Yuan, Duo Chai, Mingxin Zhou, and Jiwei Li. Entity-relation extraction as multi-turn question answer- ing. arXiv preprint arXiv:1905.05529, 2019. [Luan et al., 2018] Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi.",
            "arXiv preprint arXiv:1905.05529, 2019. [Luan et al., 2018] Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. Multi-task identi\ufb01cation of en- tities, relations, and coreference for scienti\ufb01c knowledge graph construction. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018. [Luan et al., 2019] Yi Luan, Dave Wadden, Luheng He, Amy Shah, Mari Ostendorf, and Hannaneh Hajishirzi. A general framework for information extraction using dy- namic span graphs. Proceedings of the 2019 Conference of the North, 2019. [Miwa and Bansal, 2016] Makoto Miwa and Mohit Bansal. End-to-end relation extraction using LSTMs on sequences and tree structures.",
            "Proceedings of the 2019 Conference of the North, 2019. [Miwa and Bansal, 2016] Makoto Miwa and Mohit Bansal. End-to-end relation extraction using LSTMs on sequences and tree structures. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1105\u20131116, Berlin, Ger- many, August 2016. Association for Computational Lin- guistics. [Miwa and Sasaki, 2014] Makoto Miwa and Yutaka Sasaki. Modeling joint entity and relation extraction with table representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1858\u20131869, Doha, Qatar, October 2014. Association for Computational Linguistics. [Nguyen and Verspoor, 2019] Dat Quoc Nguyen and Karin Verspoor. End-to-end neural relation extraction using deep biaf\ufb01ne attention. In European Conference on Information Retrieval, pages 729\u2013738. Springer, 2019.",
            "[Nguyen and Verspoor, 2019] Dat Quoc Nguyen and Karin Verspoor. End-to-end neural relation extraction using deep biaf\ufb01ne attention. In European Conference on Information Retrieval, pages 729\u2013738. Springer, 2019. [Pennington et al., 2014] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vec- tors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, 2014. [Peters et al., 2018] Matthew Peters, Mark Neumann, Mo- hit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word repre- sentations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Compu- tational Linguistics: Human Language Technologies, Vol- ume 1 (Long Papers), pages 2227\u20132237, 2018.",
            "Deep contextualized word repre- sentations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Compu- tational Linguistics: Human Language Technologies, Vol- ume 1 (Long Papers), pages 2227\u20132237, 2018. [Roth and Yih, 2004] Dan Roth and Wen-tau Yih. A linear programming formulation for global inference in natural language tasks. In Proceedings of the Eighth Conference on Computational Natural Language Learning (CoNLL- 2004) at HLT-NAACL 2004, pages 1\u20138, 2004. [Vandenhende et al., 2019] Simon Vandenhende, Bert De Brabandere, and Luc Van Gool. Branched multi-task networks: Deciding what layers to share. arXiv preprint arXiv:1904.02920, 2019. [Yang et al., 2014] Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and relations for learning and inference in knowledge bases.",
            "[Yang et al., 2014] Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and relations for learning and inference in knowledge bases. arXiv preprint arXiv:1412.6575, 2014. [Zhao et al., 2018] Xiangyun Zhao, Haoxiang Li, Xiaohui Shen, Xiaodan Liang, and Ying Wu. A modulation module for multi-task learning with applications in image retrieval. Lecture Notes in Computer Science, page 415\u2013432, 2018. [Zheng et al., 2017] Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing Hao, Peng Zhou, and Bo Xu. Joint ex- traction of entities and relations based on a novel tag- ging scheme. Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2017."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-2002.06424.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 9882.99983215332,
    "avg_doclen_est": 186.47169494628906
}
