{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for Sentence Classi\ufb01cation Ye Zhang1 Stephen Roller1 Byron C. Wallace2 1Department of Computer Science 2iSchool The University of Texas at Austin The University of Texas at Austin {yezhang,roller}@cs.utexas.edu byron.wallace@utexas.edu Abstract We introduce a novel, simple convolution neu- ral network (CNN) architecture \u2013 multi-group norm constraint CNN (MGNC-CNN) \u2013 that capitalizes on multiple sets of word embed- dings for sentence classi\ufb01cation. MGNC- CNN extracts features from input embedding sets independently and then joins these at the penultimate layer in the network to form a \ufb01- nal feature vector. We then adopt a group regularization strategy that differentially pe- nalizes weights associated with the subcom- ponents generated from the respective embed- ding sets. This model is much simpler than comparable alternative architectures and re- quires substantially less training time. Fur- thermore, it is \ufb02exible in that it does not re- quire input word embeddings to be of the same dimensionality.",
            "This model is much simpler than comparable alternative architectures and re- quires substantially less training time. Fur- thermore, it is \ufb02exible in that it does not re- quire input word embeddings to be of the same dimensionality. We show that MGNC-CNN consistently outperforms baseline models. 1 Introduction Neural models have recently gained popularity for Natural Language Processing (NLP) tasks (Gold- berg, 2015; Collobert and Weston, 2008; Cho, 2015). For sentence classi\ufb01cation, in particular, Convolution Neural Networks (CNN) have realized impressive performance (Kim, 2014; Zhang and Wallace, 2015). These models operate over word embeddings, i.e., dense, low dimensional vector rep- resentations of words that aim to capture salient se- mantic and syntactic properties (Collobert and We- ston, 2008). An important consideration for such models is the speci\ufb01cation of the word embeddings. Several op- tions exist. For example, Kalchbrenner et al.",
            "An important consideration for such models is the speci\ufb01cation of the word embeddings. Several op- tions exist. For example, Kalchbrenner et al. (2014) initialize word vectors to random low-dimensional vectors to be \ufb01t during training, while Johnson and Zhang (2014) use \ufb01xed, one-hot encodings for each word. By contrast, Kim (2014) initializes word vectors to those estimated via the word2vec model trained on 100 billion words of Google News (Mikolov et al., 2013); these are then up- dated during training. Initializing embeddings to pre-trained word vectors is intuitively appealing be- cause it allows transfer of learned distributional se- mantics. This has allowed a relatively simple CNN architecture to achieve remarkably strong results. Many pre-trained word embeddings are now read- ily available on the web, induced using different models, corpora, and processing steps.",
            "This has allowed a relatively simple CNN architecture to achieve remarkably strong results. Many pre-trained word embeddings are now read- ily available on the web, induced using different models, corpora, and processing steps. Different embeddings may encode different aspects of lan- guage (Pad\u00b4o and Lapata, 2007; Erk and Pad\u00b4o, 2008; Levy and Goldberg, 2014): those based on bag-of- words (BoW) statistics tend to capture associations (doctor and hospital), while embeddings based on dependency-parses encode similarity in terms of use (doctor and surgeon). It is natural to consider how these embeddings might be combined to improve NLP models in general and CNNs in particular. Contributions. We propose MGNC-CNN, a novel, simple, scalable CNN architecture that can accom- modate multiple off-the-shelf embeddings of vari- able sizes. Our model treats different word em- beddings as distinct groups, and applies CNNs in- dependently to each, thus generating corresponding feature vectors (one per embedding) which are then arXiv:1603.00968v2  [cs.CL]  27 Mar 2016",
            "concatenated at the classi\ufb01cation layer. Inspired by prior work exploiting regularization to encode struc- ture for NLP tasks (Yogatama and Smith, 2014; Wal- lace et al., 2015), we impose different regularization penalties on weights for features generated from the respective word embedding sets. Our approach enjoys the following advantages com- pared to the only existing comparable model (Yin and Sch\u00a8utze, 2015): (i) It can leverage diverse, read- ily available word embeddings with different dimen- sions, thus providing \ufb02exibility. (ii) It is compar- atively simple, and does not, for example, require mutual learning or pre-training. (iii) It is an order of magnitude more ef\ufb01cient in terms of training time. 2 Related Work Prior work has considered combining latent repre- sentations of words that capture syntactic and se- mantic properties (Van de Cruys et al., 2011), and in- ducing multi-modal embeddings (Bruni et al., 2012) for general NLP tasks. And recently, Luo et al.",
            "And recently, Luo et al. (2014) proposed a framework that combines mul- tiple word embeddings to measure text similarity, however their focus was not on classi\ufb01cation. More similar to our work, Yin and Sch\u00a8utze (2015) proposed MVCNN for sentence classi\ufb01cation. This CNN-based architecture accepts multiple word em- beddings as inputs. These are then treated as sepa- rate \u2018channels\u2019, analogous to RGB channels in im- ages. Filters consider all channels simultaneously. MVCNN achieved state-of-the-art performance on multiple sentence classi\ufb01cation tasks. However, this model has practical drawbacks. (i) MVCNN re- quires that input word embeddings have the same dimensionality. Thus to incorporate a second set of word vectors trained on a corpus (or using a model) of interest, one needs to either \ufb01nd embeddings that happen to have a set number of dimensions or to es- timate embeddings from scratch. (ii) The model is complex, both in terms of implementation and run- time.",
            "(ii) The model is complex, both in terms of implementation and run- time. Indeed, this model requires pre-training and mutual-learning and requires days of training time, whereas the simple architecture we propose requires on the order of an hour (and is easy to implement). 3 Model Description We \ufb01rst review standard one-layer CNN (which ex- ploits a single set of embeddings) for sentence clas- si\ufb01cation (Kim, 2014), and then propose our aug- mentations, which exploit multiple embedding sets. Basic CNN. In this model we \ufb01rst replace each word in a sentence with its vector representation, result- ing in a sentence matrix A \u2208Rs\u00d7d, where s is the (zero-padded) sentence length, and d is the dimen- sionality of the embeddings. We apply a convolu- tion operation between linear \ufb01lters with parameters w1, w2, ..., wk and the sentence matrix. For each wi \u2208Rh\u00d7d, where h denotes \u2018height\u2019, we slide \ufb01l- ter i across A, considering \u2018local regions\u2019 of h ad- jacent rows at a time.",
            "For each wi \u2208Rh\u00d7d, where h denotes \u2018height\u2019, we slide \ufb01l- ter i across A, considering \u2018local regions\u2019 of h ad- jacent rows at a time. At each local region, we per- form element-wise multiplication and then take the element-wise sum between the \ufb01lter and the (\ufb02at- tened) sub-matrix of A, producing a scalar. We do this for each sub-region of A that the \ufb01lter spans, re- sulting in a feature map vector ci \u2208R(s\u2212h+1)\u00d71. We can use multiple \ufb01lter sizes with different heights, and for each \ufb01lter size we can have multiple \ufb01l- ters. Thus the model comprises k weight vectors w1, w2, ...wk, each of which is associated with an instantiation of a speci\ufb01c \ufb01lter size. These in turn generate corresponding feature maps c1, c2, ...ck, with dimensions varying with \ufb01lter size.",
            "These in turn generate corresponding feature maps c1, c2, ...ck, with dimensions varying with \ufb01lter size. A 1-max pooling operation is applied to each feature map, extracting the largest number oi from each feature map i. Finally, we combine all oi together to form a feature vector o \u2208Rk to be fed through a softmax function for classi\ufb01cation. We regularize weights at this level in two ways. (1) Dropout, in which we randomly set elements in o to zero during the train- ing phase with probability p, and multiply p with the parameters trained in o at test time. (2) An l2 norm penalty, for which we set a threshold \u03bb for the l2 norm of o during training; if this is exceeded, we rescale the vector accordingly. For more details, see (Zhang and Wallace, 2015). MG-CNN. Assuming we have m word embeddings with corresponding dimensions d1, d2, ...dm, we can simply treat each word embedding independently.",
            "For more details, see (Zhang and Wallace, 2015). MG-CNN. Assuming we have m word embeddings with corresponding dimensions d1, d2, ...dm, we can simply treat each word embedding independently. In this case, the input to the CNN comprises mul- tiple sentence matrices A1, A2, ...Am, where each Al \u2208Rs\u00d7dl may have its own width dl. We then ap-",
            "ply different groups of \ufb01lters {w1}, {w2}, ...{wm} independently to each Al, where {wl} denotes the set of \ufb01lters for Al. As in basic CNN, {wl} may have multiple \ufb01lter sizes, and multiple \ufb01lters of each size may be introduced. At the classi\ufb01cation layer we then obtain a feature vector ol for each embed- ding set, and we can simply concatenate these to- gether to form the \ufb01nal feature vector o to feed into the softmax function, where o = o1 \u2295o2... \u2295om. This representation contains feature vectors gener- ated from all sets of embeddings under considera- tion. We call this method multiple group CNN (MG- CNN). Here groups refer to the features generated from different embeddings. Note that this differs from \u2018multi-channel\u2019 models because at the convo- lution layer we use different \ufb01lters on each word em- bedding matrix independently, whereas in a standard multi-channel approach each \ufb01lter would consider all channels simultaneously and generate a scalar from all channels at each local region.",
            "Note that this differs from \u2018multi-channel\u2019 models because at the convo- lution layer we use different \ufb01lters on each word em- bedding matrix independently, whereas in a standard multi-channel approach each \ufb01lter would consider all channels simultaneously and generate a scalar from all channels at each local region. As above, we impose a max l2 norm constraint on the \ufb01nal feature vector o for regularization. Figure 1 illustrates this approach. MGNC-CNN. We propose an augmentation of MG- CNN, Multi-Group Norm Constraint CNN (MGNC- CNN), which differs in its regularization strategy. Speci\ufb01cally, in this variant we impose grouped reg- ularization constraints, independently regularizing subcomponents ol derived from the respective em- beddings, i.e., we impose separate max norm con- straints \u03bbl for each ol (where l again indexes em- bedding sets); these \u03bbl hyper-parameters are to be tuned on a validation set.",
            "Intuitively, this method aims to better capitalize on features derived from word embeddings that capture discriminative prop- erties of text for the task at hand by penalizing larger weight estimates for features derived from less dis- criminative embeddings. 4 Experiments 4.1 Datasets Stanford Sentiment Treebank Stanford Sentiment Treebank (SST) (Socher et al., 2013). This concerns predicting movie review sentiment. Two datasets are derived from this corpus: (1) SST-1, containing I  hate  this movie Convolution  Layer 1 max pooling softmax function embedding 1 embedding 2 o1 o2 o   Two \ufb01lters  height 2 Two \ufb01lters  height 3 Two \ufb01lters  height 2 Two \ufb01lters  height 3 8 feature  maps Figure 1: Illustration of MG-CNN and MGNC-CNN. The \ufb01l- ters applied to the respective embeddings are completely inde- pendent. MG-CNN applies a max norm constraint to o, while MGNC-CNN applies max norm constraints on o1 and o2 in- dependently (group regularization).",
            "The \ufb01l- ters applied to the respective embeddings are completely inde- pendent. MG-CNN applies a max norm constraint to o, while MGNC-CNN applies max norm constraints on o1 and o2 in- dependently (group regularization). Note that one may easily extend the approach to handle more than two embeddings at once. \ufb01ve classes: very negative, negative, neutral, posi- tive, and very positive. (2) SST-2, which has only two classes: negative and positive. For both, we re- move phrases of length less than 4 from the training set.1 Subj (Pang and Lee, 2004). The aim here is to classify sentences as either subjective or objective. This comprises 5000 instances of each. TREC (Li and Roth, 2002). A question classi\ufb01cation dataset containing six classes: abbreviation, entity, descrip- tion, human, location and numeric. There are 5500 training and 500 test instances. Irony (Wallace et al., 2014). This dataset contains 16,006 sentences from reddit labeled as ironic (or not).",
            "There are 5500 training and 500 test instances. Irony (Wallace et al., 2014). This dataset contains 16,006 sentences from reddit labeled as ironic (or not). The dataset is imbalanced (relatively few sentences are ironic). Thus before training, we under-sampled negative in- stances to make classes sizes equal. Note that for this dataset we report the Area Under Curve (AUC), rather than accuracy, because it is imbalanced. 1As in (Kim, 2014).",
            "Model Subj SST-1 SST-2 TREC Irony CNN(w2v) 93.14 (92.92,93.39) 46.99 (46.11,48.28) 87.03 (86.16,88.08) 93.32 (92.40,94.60) 67.15 (66.53,68.11) CNN(Glv) 93.41(93.20,93.51) 46.58 (46.11,47.06) 87.36 (87.20,87.64) 93.36 (93.30,93.60) 67.84 (67.29,68.38) CNN(Syn) 93.24(93.01,93.45) 45.48(44.67,46.24) 86.04 (85.28,86.77) 94.68 (94.00,95.00) 67.93 (67.30,68.38) MVCNN (Yin and Sch\u00a8utze, 2015) 93.9 49.6 89.",
            "04 (85.28,86.77) 94.68 (94.00,95.00) 67.93 (67.30,68.38) MVCNN (Yin and Sch\u00a8utze, 2015) 93.9 49.6 89.4 - - C-CNN(w2v+Glv) 93.72 (93.68,93.76) 47.02(46.24,47.69) 87.42(86.88,87.81) 93.80 (93.40,94.20) 67.70 (66.97,68.35) C-CNN(w2v+Syn) 93.48 (93.43,93.52) 46.91(45.97,47.81) 87.17 (86.55,87.42) 94.66 (94.00,95.20) 68.08 (67.33,68.57) C-CNN(w2v+Syn+Glv) 93.61 (93.47,93.77) 46.52 (45.02,47.47) 87.",
            "66 (94.00,95.20) 68.08 (67.33,68.57) C-CNN(w2v+Syn+Glv) 93.61 (93.47,93.77) 46.52 (45.02,47.47) 87.55 (86.77,88.58) 95.20 (94.80,65.60) 68.38 (67.66,69.23) MG-CNN(w2v+Glv) 93.84 (93.66,94.35) 48.24 (47.60,49.05) 87.90 (87.48,88.30) 94.09 (93.60,94.80) 69.40 (66.35,72.30) MG-CNN(w2v+Syn) 93.78 (93.62,93.98) 48.48(47.78,49.19) 87.47(87.10,87.70) 94.87 (94.00,95.60) 68.28 (66.44,69.",
            "78 (93.62,93.98) 48.48(47.78,49.19) 87.47(87.10,87.70) 94.87 (94.00,95.60) 68.28 (66.44,69.97) MG-CNN(w2v+Syn+Glv) 94.11 (94.04,94.17) 48.01 (47.65,48.37) 87.63(87.04,88.36) 94.68 (93.80,95.40) 69.19(67.06,72.30) MGNC-CNN(w2v+Glv) 93.93 (93.79,94.14) 48.53 (47.92,49.37) 88.35(87.86,88.74) 94.40 (94.00,94.80) 69.15 (67.25,71.70) MGNC-CNN(w2v+Syn) 93.95 (93.75,94.21) 48.51 (47.60,49.41) 87.",
            "40 (94.00,94.80) 69.15 (67.25,71.70) MGNC-CNN(w2v+Syn) 93.95 (93.75,94.21) 48.51 (47.60,49.41) 87.88(87.64,88.19) 95.12 (94.60,95.60) 69.35 (67.40,70.86) MGNC-CNN(w2v+Syn+Glv) 94.09 (93.98,94.18) 48.65 (46.92,49.19) 88.30 (87.83,88.65) 95.52 (94.60,96.60) 71.53 (69.74,73.06) Table 1: Results mean (min, max) achieved with each method. w2v:word2vec. Glv:GloVe. Syn: Syntactic embedding. Note that we experiment with using two and three sets of embeddings jointly, e.g., w2v+Syn+Glv indicates that we use all three of these.",
            "w2v:word2vec. Glv:GloVe. Syn: Syntactic embedding. Note that we experiment with using two and three sets of embeddings jointly, e.g., w2v+Syn+Glv indicates that we use all three of these. Model Subj SST-1 SST-2 TREC Irony CNN(w2v) 9 81 81 9 243 CNN(Glv) 3 9 1 9 81 CNN(Syn) 3 81 9 81 1 C-CNN(w2v+Glv) 9 9 3 3 1 C-CNN(w2v+Syn) 3 81 9 9 1 C-CNN(w2v+Syn+Glv) 9 9 1 81 81 MG-CNN(w2v+Glv) 3 9 3 81 9 MG-CNN(w2v+Syn) 9 81 3 81 3 MG-CNN(w2v+Syn+Glv) 9 1 9 243 9 MGNC-CNN(w2v+Glv) (9,",
            "3) (81,9) (1,1) (9,81) (243,243) MGNC-CNN(w2v+Syn) (3,3) (81,81) (81,9) (81,81) (81,3) MGNC-CNN(w2v+Syn+Glv) (81,81,81) (81,81,1) (9,9,9) (1,81,81) (243,243,3) Table 2: Best \u03bb2 value on the validation set for each method w2v:word2vec. Glv:GloVe. Syn: Syntactic embedding. 4.2 Pre-trained Word Embeddings We consider three sets of word embeddings for our experiments: (i) word2vec2 is trained on 100 billion tokens of Google News dataset; (ii) GloVe (Pen- nington et al., 2014)3 is trained on aggregated global word-word co-occurrence statistics from Common Crawl (840B tokens); and (iii) syntactic word embedding trained on dependency-parsed corpora.",
            "These three embedding sets happen to all be 300- dimensional, but our model could accommodate ar- bitrary and variable sizes. We pre-trained our own syntactic embeddings fol- lowing (Levy and Goldberg, 2014). We parsed the ukWaC corpus (Baroni et al., 2009) using the Stanford Dependency Parser v3.5.2 with Stanford Dependencies (Chen and Manning, 2014) and ex- tracted (word, relation+context) pairs from parse trees. We \u201ccollapsed\u201d nodes with prepositions and notated inverse relations separately, e.g., \u201cdog 2https:\/\/code.google.com\/p\/word2vec\/ 3http:\/\/nlp.stanford.edu\/projects\/glove\/ barks\u201d emits two tuples: (barks, nsubj dog) and (dog, nsubj\u22121 barks). We \ufb01lter words and contexts that appear fewer than 100 times, resulting in \u223c173k words and 1M contexts. We trained 300d vectors us- ing word2vecf4 with default parameters.",
            "We \ufb01lter words and contexts that appear fewer than 100 times, resulting in \u223c173k words and 1M contexts. We trained 300d vectors us- ing word2vecf4 with default parameters. 4.3 Setup We compared our proposed approaches to a stan- dard CNN that exploits a single set of word em- beddings (Kim, 2014). We also compared to a baseline of simply concatenating embeddings for each word to form long vector inputs. We re- fer to this as Concatenation-CNN C-CNN. For all multiple embedding approaches (C-CNN, MG- CNN and MGNC-CNN), we explored two com- bined sets of embedding: word2vec+Glove, and word2vec+syntactic, and one three sets of embed- ding: word2vec+Glove+syntactic. For all models, we tuned the l2 norm constraint \u03bb over the range {1 3, 1, 3, 9, 81, 243} on a validation set. For instan- 4https:\/\/bitbucket.org\/yoavgo\/word2vecf\/",
            "tiations of MGNC-CNN in which we exploited two embeddings, we tuned both \u03bb1, and \u03bb2; where we used three embedding sets, we tuned \u03bb1, \u03bb2 and \u03bb3. We used standard train\/test splits for those datasets that had them. Otherwise, we performed 10-fold cross validation, creating nested development sets with which to tune hyperparameters. For all exper- iments we used \ufb01lters sizes of 3, 4 and 5 and we created 100 feature maps for each \ufb01lter size. We applied 1 max-pooling and dropout (rate: 0.5) at the classi\ufb01cation layer. For training we used back- propagation in mini-batches and used AdaDelta as the stochastic gradient descent (SGD) update rule, and set mini-batch size as 50. In this work, we treat word embeddings as part of the parameters of the model, and update them as well during training. In all our experiments, we only tuned the max norm constraint(s), \ufb01xing all other hyperparameters. 4.4 Results and Discussion We repeated each experiment 10 times and report the mean and ranges across these.",
            "In all our experiments, we only tuned the max norm constraint(s), \ufb01xing all other hyperparameters. 4.4 Results and Discussion We repeated each experiment 10 times and report the mean and ranges across these. This replication is important because training is stochastic and thus introduces variance in performance (Zhang and Wal- lace, 2015). Results are shown in Table 1, and the corresponding best norm constraint value is shown in Table 2. We also show results on Subj, SST-1 and SST-2 achieved by the more complex model of (Yin and Sch\u00a8utze, 2015) for comparison; this represents the state-of-the-art on the three datasets other than TREC. We can see that MGNC-CNN and MG-CNN al- ways outperform baseline methods (including C- CNN), and MGNC-CNN is usually better than MG-CNN. And on the Subj dataset, MG-CNN ac- tually achieves slightly better results than (Yin and Sch\u00a8utze, 2015), with far less complexity and re- quired training time (MGNC-CNN performs com- parably, although no better, here).",
            "And on the Subj dataset, MG-CNN ac- tually achieves slightly better results than (Yin and Sch\u00a8utze, 2015), with far less complexity and re- quired training time (MGNC-CNN performs com- parably, although no better, here). On the TREC dataset, the best-ever accuracy we are aware of is 96.0% (Mou et al., 2015), which falls within the range of the result of our MGNC-CNN model with three word embeddings. On the irony dataset, our model with three embeddings achieves 4% improve- ment (in terms of AUC) compared to the baseline model. On SST-1 and SST-2, our model performs slightly worse than (Yin and Sch\u00a8utze, 2015). However, we again note that their performance is achieved using a much more complex model which involves pre- training and mutual-learning steps. This model takes days to train, whereas our model requires on the or- der of an hour. We note that the method proposed by Astudillo et al.",
            "However, we again note that their performance is achieved using a much more complex model which involves pre- training and mutual-learning steps. This model takes days to train, whereas our model requires on the or- der of an hour. We note that the method proposed by Astudillo et al. (2015) is able to accommodate multiple embed- ding sets with different dimensions by projecting the original word embeddings into a lower-dimensional space. However, this work requires training the op- timal projection matrix on laebled data \ufb01rst, which again incurs large overhead. Of course, our model also has its own limitations: in MGNC-CNN, we need to tune the norm constraint hyperparameter for all the word embeddings. As the number of word embedding increases, this will in- crease the running time. However, this tuning pro- cedure is embarrassingly parallel. 5 Conclusions We have proposed MGNC-CNN: a simple, \ufb02exible CNN architecture for sentence classi\ufb01cation that can exploit multiple, variable sized word embeddings.",
            "However, this tuning pro- cedure is embarrassingly parallel. 5 Conclusions We have proposed MGNC-CNN: a simple, \ufb02exible CNN architecture for sentence classi\ufb01cation that can exploit multiple, variable sized word embeddings. We demonstrated that this consistently achieves bet- ter results than a baseline architecture that exploits only a single set of word embeddings, and also a naive concatenation approach to capitalizing on multiple embeddings. Furthermore, our results are comparable to those achieved with a recently pro- posed model (Yin and Sch\u00a8utze, 2015) that is much more complex. However, our simple model is easy to implement and requires an order of magnitude less training time. Furthermore, our model is much more \ufb02exible than previous approaches, because it can accommodate variable-size word embeddings. Acknowledgments This work was supported in part by the Army Re- search Of\ufb01ce (grant W911NF-14-1-0442) and by The Foundation for Science and Technology, Portu- gal (grant UTAP-EXPL\/EEIESS\/0031\/2014).",
            "Acknowledgments This work was supported in part by the Army Re- search Of\ufb01ce (grant W911NF-14-1-0442) and by The Foundation for Science and Technology, Portu- gal (grant UTAP-EXPL\/EEIESS\/0031\/2014). This work was also made possible by the support of the Texas Advanced Computer Center (TACC) at UT Austin.",
            "References [Astudillo et al.2015] Ramon F Astudillo, Silvio Amir, Wang Lin, M\u00b4ario Silva, and Isabel Trancoso. 2015. Learning word representations from scarce and noisy data with embedding sub-spaces. In Proceedings of Associa- tion for Computational Linguistics, pages 1074\u20131084. [Baroni et al.2009] Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collection of very large linguistically pro- cessed web-crawled corpora. Language Resources and Evaluation, 43(3):209\u2013226. [Bruni et al.2012] Elia Bruni, Gemma Boleda, Marco Ba- roni, and Nam-Khanh Tran. 2012. Distributional se- mantics in technicolor. In Proceedings of Association for Computational Linguistics, pages 136\u2013145. [Chen and Manning2014] Danqi Chen and Christopher Manning. 2014. A fast and accurate dependency parser using neural networks.",
            "2012. Distributional se- mantics in technicolor. In Proceedings of Association for Computational Linguistics, pages 136\u2013145. [Chen and Manning2014] Danqi Chen and Christopher Manning. 2014. A fast and accurate dependency parser using neural networks. In Proceedings of Empirical Methods in Natural Language Processing, pages 740\u2013 750. [Cho2015] Kyunghyun Cho. 2015. Natural language understanding with distributed representation. arXiv preprint arXiv:1511.07916. [Collobert and Weston2008] Ronan Collobert and Jason Weston. 2008. A uni\ufb01ed architecture for natural lan- guage processing: Deep neural networks with multitask learning. In Proceedings of the International Conference on Machine learning, pages 160\u2013167. [Erk and Pad\u00b4o2008] Katrin Erk and Sebastian Pad\u00b4o. 2008. A structured vector space model for word mean- ing in context. In Proceedings of Empirical Methods in Natural Language Processing, pages 897\u2013906. [Goldberg2015] Yoav Goldberg. 2015.",
            "2008. A structured vector space model for word mean- ing in context. In Proceedings of Empirical Methods in Natural Language Processing, pages 897\u2013906. [Goldberg2015] Yoav Goldberg. 2015. A primer on neural network models for natural language processing. arXiv preprint arXiv:1510.00726. [Johnson and Zhang2014] Rie Johnson and Tong Zhang. 2014. Effective use of word order for text categoriza- tion with convolutional neural networks. arXiv preprint arXiv:1412.1058. [Kalchbrenner et al.2014] Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. arXiv preprint arXiv:1404.2188. [Kim2014] Yoon Kim. 2014. Convolutional neural networks for sentence classi\ufb01cation. arXiv preprint arXiv:1408.5882.",
            "arXiv preprint arXiv:1404.2188. [Kim2014] Yoon Kim. 2014. Convolutional neural networks for sentence classi\ufb01cation. arXiv preprint arXiv:1408.5882. [Levy and Goldberg2014] Omer Levy and Yoav Gold- berg. 2014. Dependency-based word embeddings. In Proceedings of Association for Computational Linguis- tics, pages 302\u2013308. [Li and Roth2002] Xin Li and Dan Roth. 2002. Learning question classi\ufb01ers. In Proceedings of the International Conference on Computational Linguistics, pages 1\u20137. [Luo et al.2014] Yong Luo, Jian Tang, Jun Yan, Chao Xu, and Zheng Chen. 2014. Pre-trained multi-view word em- bedding using two-side neural network. In Conference on Arti\ufb01cial Intelligence. [Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.",
            "In Conference on Arti\ufb01cial Intelligence. [Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their composi- tionality. In Advances in Neural Information Processing Systems, pages 3111\u20133119. [Mou et al.2015] Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. 2015. Discriminative neural sen- tence modeling by tree-based convolution. In Proceed- ings of Empirical Methods in Natural Language Process- ing, pages 2315\u20132325. [Pad\u00b4o and Lapata2007] Sebastian Pad\u00b4o and Mirella Lap- ata. 2007. Dependency-based construction of seman- tic space models. Computational Linguistics, 33(2):161\u2013 199. [Pang and Lee2004] Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjec- tivity summarization based on minimum cuts.",
            "Computational Linguistics, 33(2):161\u2013 199. [Pang and Lee2004] Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjec- tivity summarization based on minimum cuts. In Pro- ceedings of Association for Computational Linguistics, page 271. [Pennington et al.2014] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In Proceed- ings of the Empirical Methods in Natural Language Processing, pages 1532\u20131543. [Socher et al.2013] Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a senti- ment treebank. In Proceedings of Empirical Methods in Natural Language Processing, pages 1631\u20131642. [Van de Cruys et al.2011] Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen. 2011.",
            "In Proceedings of Empirical Methods in Natural Language Processing, pages 1631\u20131642. [Van de Cruys et al.2011] Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen. 2011. Latent vector weighting for word meaning in context. In Proceedings of Empirical Methods in Natural Language Processing, pages 1012\u20131022. [Wallace et al.2014] Byron C. Wallace, Do Kook Choe, Laura Kertz, and Eugene Charniak. 2014. Humans re- quire context to infer ironic intent (so computers proba- bly do, too). In Proceedings of Association for Computa- tional Linguistics, pages 512\u2013516. [Wallace et al.2015] Byron C. Wallace, Do Kook Choe, and Eugene Charniak. 2015. Sparse, contextually in- formed models for irony detection: Exploiting user com- munities, entities and sentiment. In Proceedings of Asso- ciation for Computational Linguistics, pages 1035\u20131044.",
            "2015. Sparse, contextually in- formed models for irony detection: Exploiting user com- munities, entities and sentiment. In Proceedings of Asso- ciation for Computational Linguistics, pages 1035\u20131044. [Yin and Sch\u00a8utze2015] Wenpeng Yin and Hinrich Sch\u00a8utze. 2015. Multichannel variable-size convolution for sentence classi\ufb01cation. In Proceedings of the Con- ference on Computational Natural Language Learning, pages 204\u2013214.",
            "[Yogatama and Smith2014] Dani Yogatama and Noah Smith. 2014. Making the most of bag of words: Sen- tence regularization with alternating direction method of multipliers. In Proceedings of the International Confer- ence on Machine Learning, pages 656\u2013664. [Zhang and Wallace2015] Ye Zhang and Byron C. Wal- lace. 2015. A sensitivity analysis of (and practitioners\u2019 guide to) convolutional neural networks for sentence clas- si\ufb01cation. arXiv preprint arXiv:1510.03820."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1603.00968.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 6725.999771118164,
    "avg_doclen_est": 172.46153259277344
}
