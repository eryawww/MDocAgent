{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "arXiv:1703.07090v1  [cs.CL]  21 Mar 2017 DEEP LSTM FOR LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION Xu Tian, Jun Zhang, Zejun Ma, Yi He, Juan Wei, Peihao Wu, Wenchang Situ, Shuai Li, Yang Zhang Alibaba Shenma Search, Beijing, China {xu.tian, zj102217, zejun.mamzj, heyi.hy, wj80290, peihao.wph, wenchang.situwc, voolc.li, zy80232}@alibaba-inc.com ABSTRACT Recurrent neural networks (RNNs), especially long short- term memory (LSTM) RNNs, are effective network for se- quential task like speech recognition. Deeper LSTM models perform well on large vocabulary continuous speech recogni- tion, because of their impressive learning ability. However, it is more dif\ufb01cult to train a deeper network. We introduce a training framework with layer-wise training and exponen- tial moving average methods for deeper LSTM models.",
            "However, it is more dif\ufb01cult to train a deeper network. We introduce a training framework with layer-wise training and exponen- tial moving average methods for deeper LSTM models. It is a competitive framework that LSTM models of more than 7 layers are successfully trained on Shenma voice search data in Mandarin and they outperform the deep LSTM models trained by conventional approach. Moreover, in order for on- line streaming speech recognition applications, the shallow model with low real time factor is distilled from the very deep model. The recognition accuracy have little loss in the dis- tillation process. Therefore, the model trained with the pro- posed training framework reduces relative 14% character er- ror rate, compared to original model which has the similar real-time capability. Furthermore, the novel transfer learning strategy with segmental Minimum Bayes-Risk is also intro- duced in the framework. The strategy makes it possible that training with only a small part of dataset could outperform full dataset training from the beginning. 1. INTRODUCTION Recently, deep neural network has been widely employed in various recognition tasks.",
            "The strategy makes it possible that training with only a small part of dataset could outperform full dataset training from the beginning. 1. INTRODUCTION Recently, deep neural network has been widely employed in various recognition tasks. Increasing the depth of neural network is a effective way to improve the performance, and convolutional neural network (CNN) has bene\ufb01ted from it in visual recognition task[1]. Deeper long short-term mem- ory (LSTM) recurrent neural networks (RNNs) are also applied in large vocabulary continuous speech recognition (LVCSR) task, because LSTM networks have shown better performance than Fully-connected feed-forward deep neural network[2, 3, 4, 5]. Training neural network becomes more challenge when it goes deep. A conceptual tool called linear classi\ufb01er probe is introduced to better understand the dynamics inside a neural network [6]. The discriminating features of linear classi\ufb01er is the hidden units of a intermediate layer. For deep neural networks, it is observed that deeper layer\u2019s accuracy is lower than that of shallower layers. Therefore, the tool shows the dif\ufb01culty of deep neural model training visually.",
            "The discriminating features of linear classi\ufb01er is the hidden units of a intermediate layer. For deep neural networks, it is observed that deeper layer\u2019s accuracy is lower than that of shallower layers. Therefore, the tool shows the dif\ufb01culty of deep neural model training visually. Layer-wise pre-training is a successful method to train very deep neural networks [7]. The convergence becomes harder with increasing the number of layers, even though the model is initialized with Xavier or its variants [8, 9]. But the deeper network which is initialized with a shallower trained network could converge well. The size of LVCSR training dataset goes larger and train- ing with only one GPU becomes high time consumption in- evitably. Therefore, parallel training with multi-GPUs is more suitable for LVCSR system. Mini-batch based stochastic gra- dient descent (SGD) is the most popular method in neural net- work training procedure. Asynchronous SGD is a successful effort for parallel training based on it [10, 11]. It can many times speed up the training time without decreasing the accu- racy.",
            "Asynchronous SGD is a successful effort for parallel training based on it [10, 11]. It can many times speed up the training time without decreasing the accu- racy. Besides, synchronous SGD is another effective effort, where the parameter server waits for every works to \ufb01nish their computation and sent their local models to it, and then it sends updated model back to all workers [12]. Synchronous SGD converges well in parallel training with data parallelism, and is also easy to implement. In order to further improve the performance of deep neu- ral network with parallel training, several methods are pro- posed. Model averaging method achieves linear speedup, as the \ufb01nal model is averaged from all parameters of local mod- els in different workers [13, 14], but the accuracy decreases compared with single GPU training. Moreover, blockwise model-updating \ufb01lter (BMUF) provides another almost lin- ear speedup approach with multi-GPUs on the basis of model averaging. It can achieve improvement or no-degradation of recognition performance compared with mini-batch SGD on single GPU [15]. Moving averaged (MA) approaches are also proposed for parallel training.",
            "It can achieve improvement or no-degradation of recognition performance compared with mini-batch SGD on single GPU [15]. Moving averaged (MA) approaches are also proposed for parallel training. It is demonstrated that the moving average of the parameters obtained by SGD performs as well as the parameters that minimize the empirical cost, and moving av- erage parameters can be used as the estimator of them, if the size of training data is large enough [16]. One pass learn- ing is then proposed, which is the combination of learning rate schedule and averaged SGD using moving average [17]. Exponential moving average (EMA) is proposed as a non- interference method[18]. EMA model is not broadcasted to",
            "workers to update their local models, and it is applied as the \ufb01nal model of entire training process. EMA method is uti- lized with model averaging and BMUF to further decrease the character error rate (CER). It is also easy to implement in existing parallel training systems. Frame stacking can also speed up the training time [19]. The super frame is stacked by several regular frames, and it contains the information of them. Thus, the network can see multiple frames at a time, as the super frame is new input. Frame stacking can also lead to faster decoding. For streaming voice search service, it needs to display in- termediate recognition results while users are still speaking. As a result, the system needs to ful\ufb01ll high real-time require- ment, and we prefer unidirectional LSTM network rather than bidirectional one. High real-time requirement means low real time factor (RTF), but the RTF of deep LSTM model is higher inevitably. The dilemma of recognition accuracy and real- time requirement is an obstacle to the employment of deep LSTM network. Deep model outperforms because it contains more knowledge, but it is also cumbersome.",
            "The dilemma of recognition accuracy and real- time requirement is an obstacle to the employment of deep LSTM network. Deep model outperforms because it contains more knowledge, but it is also cumbersome. As a result, the knowledge of deep model can be distilled to a shallow model [20]. It provided a effective way to employ the deep model to the real-time system. In this paper, we explore a entire deep LSTM RNN train- ing framework, and employ it to real-time application. The deep learning systems bene\ufb01t highly from a large quantity of labeled training data. Our \ufb01rst and basic speech recogni- tion system is trained on 17000 hours of Shenma voice search dataset. It is a generic dataset sampled from diverse aspects of search queries. The requirement of speech recognition sys- tem also addressed by speci\ufb01c scenario, such as map and nav- igation task. The labeled dataset is too expensive, and train- ing a new model with new large dataset from the beginning costs lots of time. Thus, it is natural to think of transferring the knowledge from basic model to new scenario\u2019s model. Transfer learning expends less data and less training time than full training.",
            "The labeled dataset is too expensive, and train- ing a new model with new large dataset from the beginning costs lots of time. Thus, it is natural to think of transferring the knowledge from basic model to new scenario\u2019s model. Transfer learning expends less data and less training time than full training. In this paper, we also introduce a novel trans- fer learning strategy with segmental Minimum Bayes-Risk (sMBR). As a result, transfer training with only 1000 hours data can match equivalent performance for full training with 7300 hours data. Our deep LSTM training framework for LVCSR is pre- sented in Section 2. Section 3 describes how the very deep models does apply in real world applications, and how to transfer the model to another task. The framework is analyzed and discussed in Section 4, and followed by the conclusion in Section 5. 2. OUR TRAINING FRAMEWORK 2.1. Layer-wise Training with Soft Target and Hard Tar- get Gradient-based optimization of deep LSTM network with random initialization get stuck in poor solution easily.",
            "2. OUR TRAINING FRAMEWORK 2.1. Layer-wise Training with Soft Target and Hard Tar- get Gradient-based optimization of deep LSTM network with random initialization get stuck in poor solution easily. Xavier initialization can partially solve this problem [8], so this method is the regular initialization method of all training pro- cedure. However, it does not work well when it is utilized to initialize very deep model directly, because of vanish- ing or exploding gradients. Instead, layer-wise pre-training method is a effective way to train the weights of very deep architecture[7, 21]. In layer-wise pre-training procedure, a one-layer LSTM model is \ufb01rstly trained with normalized ini- tialization. Sequentially, two-layers LSTM model\u2019s \ufb01rst layer is initialized by trained one-layer model, and its second layer is regularly initialized. In this way, a deep architecture is layer-by-layer trained, and it can converge well. In conventional layer-wise pre-training, only parameters of shallower network are transfered to deeper one, and the learning targets are still the alignments generated by HMM- GMM system.",
            "In this way, a deep architecture is layer-by-layer trained, and it can converge well. In conventional layer-wise pre-training, only parameters of shallower network are transfered to deeper one, and the learning targets are still the alignments generated by HMM- GMM system. The targets are vectors that only one state\u2019s probability is one, and the others\u2019 are zeros. They are known as hard targets, and they carry limited knowledge as only one state is active. In contrast, the knowledge of shallower net- work should be also transfered to deeper one. It is obtained by the softmax layer of existing model typically, so each state has a probability rather than only zero or one, and called as soft target. As a result, the deeper network which is student network learns the parameters and knowledge from shallower one which is called teacher network. When training the stu- dent network from the teacher network, the \ufb01nal alignment is the combination of hard target and soft target in our layer- wise training phase. The \ufb01nal alignment provides various knowledge which transfered from teacher network and ex- tracted from true labels.",
            "When training the stu- dent network from the teacher network, the \ufb01nal alignment is the combination of hard target and soft target in our layer- wise training phase. The \ufb01nal alignment provides various knowledge which transfered from teacher network and ex- tracted from true labels. If only soft target is learned, student network perform no better than teacher network, but it could outperform teacher network as it also learns true labels. The deeper network spends less time to getting the same level of original network than the network trained from the beginning, as a period of low performance is skipped. Therefore, training with hard and soft target is a time saving method. For large training dataset, training with the whole dataset still spends too much time. A network \ufb01rstly trained with only a small part of dataset could go deeper as well, and so the training time reducing rapidly. When the network is deep enough, it then trained on the entire dataset to get further improvement. There is no gap of accuracy between these two approaches, but latter one saves much time. 2.2. Differential Saturation Check The objects of conventionalsaturation check are gradients and the cell activations [5].",
            "There is no gap of accuracy between these two approaches, but latter one saves much time. 2.2. Differential Saturation Check The objects of conventionalsaturation check are gradients and the cell activations [5]. Gradients are clipped to range [-5, 5],",
            "while the cell activations clipped to range [-50, 50]. Apart from them, the differentials of recurrent layers is also limited. If the differentials go beyond the range, corresponding back propagation is skipped, while if the gradients and cell acti- vations go beyond the bound, values are set as the boundary values. The differentials which are too large or too small will lead to the gradients easily vanishing, and it demonstrates the failure of this propagation. As a result, the parameters are not updated, and next propagation . 2.3. Sequence Discriminative Training Cross-entropy (CE) is widely used in speech recognition training system as a frame-wise discriminative training cri- terion. However, it is not well suited to speech recognition, because speech recognition training is a sequential learning problem. In contrast, sequence discriminative training cri- terion has shown to further improve performance of neural network \ufb01rst trained with cross-entropy [22, 23, 24].",
            "However, it is not well suited to speech recognition, because speech recognition training is a sequential learning problem. In contrast, sequence discriminative training cri- terion has shown to further improve performance of neural network \ufb01rst trained with cross-entropy [22, 23, 24]. We choose state-level minimum bayes risk (sMBR)[22] among a number of sequence discriminative criterion is proposed, such as maximum mutual information (MMI) [25] and mini- mum phone error (MPE) [26]. MPE and sMBR are designed to minimize the expected error of different granularity of la- bels, while CE aims to minimizes expected frame error, and MMI aims to minimizes expected sentence error. State-level information is focused on by sMBR. a frame-level accurate model is \ufb01rstly trained by CE loss function, and then sMBR loss function is utilized for further training to get sequence-level accuracy. Only a part of train- ing dataset is needed in sMBR training phase on the basis of whole dataset CE training. 2.4. Parallel Training It is demonstrated that training with larger dataset can im- prove recognition accuracy.",
            "Only a part of train- ing dataset is needed in sMBR training phase on the basis of whole dataset CE training. 2.4. Parallel Training It is demonstrated that training with larger dataset can im- prove recognition accuracy. However, larger dataset means more training samples and more model parameters. There- fore, parallel training with multiple GPUs is essential, and it makes use of data parallelism [10]. The entire training data is partitioned into several split without overlapping and they are distributed to different GPUs. Each GPU trains with one split of training dataset locally. All GPUs synchronize their local models with model average method after a mini-batch optimization [13, 14]. 2.4.1. Block-wise Model Updating Filter Model average method achieves linear speedup in train- ing phase, but the recognition accuracy decreases compared with single GPU training. Block-wise model updating \ufb01l- ter (BMUF) is another successful effort in parallel training with linear speedup as well. It can achieve no-degradation of recognition accuracy with multi-GPUs [15]. In the model average method, aggregated model \u00af\u03b8(t) is computed and broadcasted to GPUs.",
            "It can achieve no-degradation of recognition accuracy with multi-GPUs [15]. In the model average method, aggregated model \u00af\u03b8(t) is computed and broadcasted to GPUs. On the basis of it, BMUF proposed a novel model updating strategy: \u00af\u03b8(t) = 1 N N X i=1 \u03b8i G(t) = \u00af\u03b8(t) \u2212\u03b8g(t \u22121) \u2206(t) = \u03b7t\u2206(t \u22121) + \u03b6tG(t) Where G(t) denotes model update, and \u2206(t) is the global- model update. There are two parameters in BMUF, block momentum \u03b7, and block learning rate \u03b6. Then, the global model is updated as \u03b8g(t) = \u03b8g(t \u22121) + \u2206(t) Consequently, \u03b8g(t) is broadcasted to all GPUs to initial their local models, instead of \u00af\u03b8(t) in model average method. 2.4.2. Exponential Moving Average Model Averaged SGD is proposed to further accelerate the conver- gence speed of SGD.",
            "2.4.2. Exponential Moving Average Model Averaged SGD is proposed to further accelerate the conver- gence speed of SGD. Averaged SGD leverages the moving average (MA) \u00af\u03b8 as the estimator of \u03b8\u2217[16]: \u00af\u03b8t = 1 t t X \u03c4=1 \u03b8\u03c4 Where \u03b8\u03c4 is computed by model averaging or BMUF. It is shown that \u00af\u03b8t can well converge to \u03b8\u2217, with the large enough training dataset in single GPU training. It can be considered as a non-interference strategy that \u00af\u03b8t does not participate the main optimization process, and only takes effect after the end of entire optimization. However, for the parallel training im- plementation, each \u03b8\u03c4 is computed by model averaging and BMUF with multiple models, and moving average model \u00af\u03b8t does not well converge, compared with single GPU training. Model averaging based methods are employed in paral- lel training of large scale dataset, because of their faster con- vergence, and especially no-degradation implementation of BMUF.",
            "Model averaging based methods are employed in paral- lel training of large scale dataset, because of their faster con- vergence, and especially no-degradation implementation of BMUF. But combination of model averaged based methods and moving average does not match the expectation of further enhance performance and it is presented as \u00af\u03b8gt = 1 t t X \u03c4=1 \u03b8g\u03c4 The weight of each \u03b8gt is equal in moving average method regardless the effect of temporal order. But t closer to the end of training achieve higher accuracy in the model averaging based approach, and thus it should be with more proportion in \ufb01nal \u00af\u03b8g. As a result, exponential moving average(EMA) is appropriate, which the weight for each older parameters de- crease exponentially, and never reaching zero. After moving",
            "average based methods, the EMA parameters are updated re- cursively as \u02c6\u03b8gt = \u03b1\u02c6\u03b8gt\u22121 + (1 \u2212\u03b1)\u03b8gt Here \u03b1 represents the degree of weight decrease, and called exponential updating rate. EMA is also a non-interference training strategy that is implemented easily, as the updated model is not broadcasted. Therefore, there is no need to add extra learning rate updating approach, as it can be appended to existing training procedure directly. 3. DEPLOYMENT There is a high real time requirement in real world applica- tion, especially in online voice search system. Shenma voice search is one of the most popular mobile search engines in China, and it is a streaming service that intermediate recog- nition results displayed while users are still speaking. Unidi- rectional LSTM network is applied, rather than bidirectional one, because it is well suited to real-time streaming speech recognition. 3.1. Distillation It is demonstrated that deep neural network architecture can achieve improvement in LVCSR. However, it also leads to much more computation and higher RTF, so that the recogni- tion result can not be displayed in real time.",
            "3.1. Distillation It is demonstrated that deep neural network architecture can achieve improvement in LVCSR. However, it also leads to much more computation and higher RTF, so that the recogni- tion result can not be displayed in real time. It should be noted that deeper neural network contains more knowledge, but it is also cumbersome. the knowledge is key to improve the per- formance. If it can be transfered from cumbersome model to a small model, the recognition ability can also be transfered to the small model. Knowledge transferring to small model is called distillation [20]. The small model can perform as well as cumbersome model, after distilling. It provide a way to utilize high-performance but high RTF model in real time system. The class probability produced by the cumbersome model is regarded as soft target, and the generalization abil- ity of cumbersome model is transfered to small model with it. Distillation is model\u2019s knowledge transferring approach, so there is no need to use the hard target, which is different with the layer-wise training method. 3.2.",
            "Distillation is model\u2019s knowledge transferring approach, so there is no need to use the hard target, which is different with the layer-wise training method. 3.2. Transfer Learning with sMBR For a certain speci\ufb01c scenario, the model trained with the data recorded from it has better adaptation than the model trained with generic scenario. But it spends too much time training a model from the beginning, if there is a well-trained model for generic scenarios. Moreover, labeling a large quantity of training data in new scenario is both costly and time con- suming. If a model transfer trained with smaller dataset can obtained the similar recognition accuracy compared with the model directly trained with larger dataset, it is no doubt that transfer learning is more practical. Since speci\ufb01c scenario is Dataset Shenma Voice Search Amap Training set 16150 6935 Validation set 850 365 Test set 10 8 Total 17010 7308 Table 1. The time summation of different sets of Shenma voice search and Amap. a subset of generic scenario, some knowledge can be shared between them. Besides, generic scenario consists of various conditions, so its model has greater robustness.",
            "The time summation of different sets of Shenma voice search and Amap. a subset of generic scenario, some knowledge can be shared between them. Besides, generic scenario consists of various conditions, so its model has greater robustness. As a result, not only shared knowledge but also robustness can be trans- fered from the model of generic scenario to the model of spe- ci\ufb01c one. As the model well trained from generic scenario achieves good performance in frame level classi\ufb01cation, sequence dis- criminative training is required to adapt new model to speci\ufb01c scenario additionally. Moreover, it does not need alignment from HMM-GMM system, and it also saves amount of time to prepare alignment. 4. EXPERIMENTS 4.1. Training Data A large quantity of labeled data is needed for training a more accurate acoustic model. We collect the 17000 hours labeled data from Shenma voice search, which is one of the most pop- ular mobile search engines in China. The dataset is created from anonymous online users\u2019 search queries in Mandarin, and all audio \ufb01le\u2019s sampling rate is 16kHz, recorded by mo- bile phones.",
            "The dataset is created from anonymous online users\u2019 search queries in Mandarin, and all audio \ufb01le\u2019s sampling rate is 16kHz, recorded by mo- bile phones. This dataset consists of many different condi- tions, such as diverse noise even low signal-to-noise, babble, dialects, accents, hesitation and so on. In the Amap, which is one of the most popular web map- ping and navigation services in China, users can search loca- tions and navigate to locations they want though voice search. To present the performance of transfer learning with sequence discriminative training, the model trained from Shenma voice search which is greneric scenario transfer its knowledge to the model of Amap voice search. 7300 hours labeled data is collected in the similar way of Shenma voice search data collection. Two dataset is divided into training set, validation set and test set separately, and the quantity of them is shown in Ta- ble 1. The three sets are split according to speakers, in or- der to avoid utterances of same speaker appearing in three sets simultaneously. The test sets of Shenma and Amap voice search are called Shenma Test and Amap Test.",
            "4.2. Experimental setup LSTM RNNs outperform conventional RNNs for speech recognition system, especially deep LSTM RNNs, because of its long-range dependencies more accurately for tempo- ral sequence conditions [27, 24]. Shenma and Amap voice search is a streaming service that intermediate recognition results displayed while users are still speaking. So as for on- line recognition in real time, we prefer unidirectional LSTM model rather than bidirectional one. Thus, the training system is unidirectional LSTM-based. A 26-dimensional \ufb01lter bank and 2-dimensional pitch fea- ture is extracted for each frame, and is concatenated with \ufb01rst and second order difference as the \ufb01nal input of the net- work. The super frame are stacked by 3 frames without over- lapping. The architecture we trained consists of two LSTM layers with sigmoid activation function, followed by a full- connection layer. The out layer is a softmax layer with 11088 hidden markov model (HMM) tied-states as output classes, the loss function is cross-entropy (CE).",
            "The architecture we trained consists of two LSTM layers with sigmoid activation function, followed by a full- connection layer. The out layer is a softmax layer with 11088 hidden markov model (HMM) tied-states as output classes, the loss function is cross-entropy (CE). The performance met- ric of the system in Mandarin is reported with character error rate (CER). The alignment of frame-level ground truth is ob- tained by GMM-HMM system. Mini-batched SGD is utilized with momentum trick and the network is trained for a total of 4 epochs. The block learning rate and block momentum of BMUF are set as 1 and 0.9. 5-gram language model is leveraged in decoder, and the vocabulary size is as large as 760000. Differentials of recurrent layers is limited to range [-10000,10000], while gradients are clipped to range [-5, 5] and cell activations clipped to range [-50, 50]. After training with CE loss, sMBR loss is employed to further improve the performance. It has shown that BMUF outperforms traditional model averaging method, and it is utilized at the synchronization phase.",
            "After training with CE loss, sMBR loss is employed to further improve the performance. It has shown that BMUF outperforms traditional model averaging method, and it is utilized at the synchronization phase. After synchronizing with BMUF, EMA method fur- ther updates the model in non-interference way. The training system is deployed on the MPI-based HPC cluster where 8 GPUs. Each GPU processes non-overlap subset split from the entire large scale dataset in parallel. Local models from distributed workers synchronize with each other in decentralized way. In the traditional model averaging and BMUF method, a parameter server waits for all workers to send their local models, aggregate them, and send the updated model to all workers. Computing resource of workers is wasted until aggregation of the parameter server done. Decentralized method makes full use of computing resource, and we employ the MPI-based Mesh AllReduce method. It is mesh topology as shown in Figure 1. There is no centralized parameter server, and peer to peer communica- tion is used to transmit local models between workers.",
            "Decentralized method makes full use of computing resource, and we employ the MPI-based Mesh AllReduce method. It is mesh topology as shown in Figure 1. There is no centralized parameter server, and peer to peer communica- tion is used to transmit local models between workers. Local model \u03b8i of i-th worker in N workers cluster is split to N pieces \u03b8i,j j = 1 \u00b7 \u00b7 \u00b7 N, and send to corresponding worker. In the aggregation phase, j-th worker computed N splits of model \u03b8i,j i = 1 \u00b7 \u00b7 \u00b7 N and send updated model \u00af\u03b8gj back to workers. As a result, all workers participate in aggregation and no computing resource is dissipated. It is signi\ufb01cant to promote training ef\ufb01ciency, when the size of neural net- work model is too large. The EMA model is also updated additionally, but not broadcasting it. Fig. 1. Mesh AllReduce of model averaging and BMUF. 5. RESULTS In order to evaluate our system, several sets of experiments are performed.",
            "The EMA model is also updated additionally, but not broadcasting it. Fig. 1. Mesh AllReduce of model averaging and BMUF. 5. RESULTS In order to evaluate our system, several sets of experiments are performed. The Shenma test set including about 9000 samples and Amap test set including about 7000 samples con- tain various real world conditions. It simulates the major- ity of user scenarios, and can well evaluates the performance of a trained model. Firstly, we show the results of models trained with EMA method. Secondly, for real world appli- cations, very deep LSTM is distilled to a shallow one, so as for lower RTF. The model of Amap is also needed to train for map and navigation scenarios. The performance of transfer learning from Shenma voice search to Amap voice search is also presented. 5.1. Layer-wise Training In layer-wise training, the deeper model learns both pa- rameters and knowledge from the shallower model. The deeper model is initialized by the shallower one, and its alignment is the combination of hard target and soft target of shallower one.",
            "5.1. Layer-wise Training In layer-wise training, the deeper model learns both pa- rameters and knowledge from the shallower model. The deeper model is initialized by the shallower one, and its alignment is the combination of hard target and soft target of shallower one. Two targets have the same weights in our framework. The teacher model is trained with CE. For each layer-wise trained CE model, corresponding sMBR model is also trained, as sMBR could achieve additional improvement. In our framework, 1000 hours data is randomly selected from",
            "Layer CER (%) Xavier Init CE Layer-wise CE CE+sMBR 6 3.72 - 2.85 7 3.93 3.68 2.81 8 3.81 3.60 2.77 9 3.87 2.82 2.49 Table 2. The CER of 6 to 9-layers models trained by regular Xavier Initialization, layer-wise training with CE criterion and CE + sMBR criteria. The teacher of 9-layer model is 8-layers sMBR model, while the others\u2019 teacher is CE model. the total dataset for sMBR training. There is no obvious performance enhancement when the size of sMBR training dataset increases. For very deep unidirectional LSTM initialized with Xavier initialization algorithm, 6-layers model converges well, but there is no further improvement with increasing the number of layers. Therefore, the \ufb01rst 6 layers of 7-layers model is initialized by 6-layers model, and soft target is pro- vided by 6-layers model. Consequently, deeper LSTM is also trained in the same way.",
            "Therefore, the \ufb01rst 6 layers of 7-layers model is initialized by 6-layers model, and soft target is pro- vided by 6-layers model. Consequently, deeper LSTM is also trained in the same way. It should be noticed that the teacher model of 9-layers model is the 8-layers model trained by sMBR, while the other teacher model is CE model. As shown in Table 2, the layer-wise trained models always performs better than the models with Xavier initialization, as the model is deep. Therefore, for the last layer training, we choose 8-layers sMBR model as the teacher model instead of CE model. A comparison between 6-layers and 9-layers sMBR models shows that 3 additional layers of layer-wise training brings relative 12.6% decreasing of CER. It is also signi\ufb01cant that the averaged CER of sMBR models with different lay- ers decreases absolute 0.73% approximately compared with CE models, so the improvement of sequence discriminative learning is promising. 5.2.",
            "It is also signi\ufb01cant that the averaged CER of sMBR models with different lay- ers decreases absolute 0.73% approximately compared with CE models, so the improvement of sequence discriminative learning is promising. 5.2. Distillation 9-layers unidirectional LSTM model achieves outstanding performance, but meanwhile it is too computationally expen- sive to allow deployment in real time recognition system. In order to ensure real-time of the system, the number of lay- ers needs to be reduced. The shallower network can learn the knowledge of deeper network with distillation. It is found that RTF of 2-layers network is acceptable, so the knowledge is distilled from 9-layers well-trained model to 2-layers model. Table 3 shows that distillation from 9-layers to 2-layers brings RTF decrease of relative 53%, while CER only increases 5%. The knowledge of deep network is almost transfered with distillation, Distillation brings promising RTF reduction, but only little knowledge of deep network is lost.",
            "The knowledge of deep network is almost transfered with distillation, Distillation brings promising RTF reduction, but only little knowledge of deep network is lost. Moreover, CER of 2-layers distilled LSTM decreases relative 14%, compared with 2-layers regular-trained LSTM. Models CER (%) RTF 9-layers LSTM 2.49 0.74 2-layers regular-trained LSTM 3.06 0.36 2-laryers distilled LSTM 2.63 0.35 Table 3. The CER and RTF of 9-layers, 2-layers regular-trained and 2-laryers distilled LSTM. Training methods CER (%) Shenma model 7.87 Amap CE + sMBR 6.81 Shenma model + Amap sMBR 6.26 Table 4. The CER of different 2-layers models, which are Shenma distilled model, Amap model further trained with Amap dataset, and Shenma model trained with sMBR on Amap dataset. 5.3.",
            "The CER of different 2-layers models, which are Shenma distilled model, Amap model further trained with Amap dataset, and Shenma model trained with sMBR on Amap dataset. 5.3. Transfer Learning 2-layers distilled model of Shenma voice search has shown a impressive performance on Shenma Test, and we call it Shenma model. It is trained for generic search scenario, but it has less adaptation for speci\ufb01c scenario like Amap voice search. Training with very large dataset using CE loss is re- garded as improvement of frame level recognition accuracy, and sMBR with less dataset further improves accuracy as se- quence discriminative training. If robust model of generic scenario is trained, there is no need to train a model with very large dataset, and sequence discriminative training with less dataset is enough. Therefore, on the basis of Shenma model, it is suf\ufb01cient to train a new Amap model with small dataset us- ing sMBR. As shown in Table 4, Shenma model presents the worst performance among three methods, since it does not trained for Amap scenario.",
            "Therefore, on the basis of Shenma model, it is suf\ufb01cient to train a new Amap model with small dataset us- ing sMBR. As shown in Table 4, Shenma model presents the worst performance among three methods, since it does not trained for Amap scenario. 2-layers Shenma model further trained with sMBR achieves about 8.1% relative reduction, compared with 2-layers regular-trained Amap model. Both training sMBR datasets contain the same 1000 hours data. As a result, with the Shenma model, only about 14% data usage achieves lower CER, and it leads to great time and cost saving with less labeled data. Besides, transfer learning with sMBR does not use the alignment from the HMM-GMM system, so it also saves huge amount of time. 6. CONCLUSION We have presented a whole deep unidirectional LSTM paral- lel training system for LVCSR. The recognition performance improves when the network goes deep. Distillation makes it possible that deep LSTM model transfer its knowledge to shallow model with little loss.",
            "6. CONCLUSION We have presented a whole deep unidirectional LSTM paral- lel training system for LVCSR. The recognition performance improves when the network goes deep. Distillation makes it possible that deep LSTM model transfer its knowledge to shallow model with little loss. The model could be distilled to 2-layers model with very low RTF, so that it can display the immediate recognition results. As a result, its CER decrease relatively 14%, compared with the 2-layers regular trained model. In addition, transfer learning with sMBR is also pro-",
            "posed. If a great model has well trained from generic sce- nario, only 14% of the size of training dataset is needed to train a more accuracy acoustic model for speci\ufb01c scenario. Our future work includes 1) \ufb01nding more effective methods to reduce the CER by increasing the number of layers; 2) applying this training framework to Connectionist Temporal Classi\ufb01cation (CTC) and attention-based neural networks. 7. REFERENCES [1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, \u201cDeep residual learning for image recognition,\u201d arXiv preprint arXiv:1512.03385, 2015. [2] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Se- nior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al., \u201cDeep neural networks for acoustic mod- eling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.",
            "29, no. 6, pp. 82\u201397, 2012. [3] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton, \u201cSpeech recognition with deep recurrent neu- ral networks,\u201d in 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013, pp. 6645\u20136649. [4] Alex Graves, Navdeep Jaitly, and Abdel-rahman Mo- hamed, \u201cHybrid speech recognition with deep bidirec- tional lstm,\u201d in Automatic Speech Recognition and Un- derstanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 273\u2013278. [5] Hasim Sak, Andrew W Senior, and Franc\u00b8oise Beaufays, \u201cLong short-term memory recurrent neural network ar- chitectures for large scale acoustic modeling.,\u201d in IN- TERSPEECH, 2014, pp. 338\u2013342.",
            "[5] Hasim Sak, Andrew W Senior, and Franc\u00b8oise Beaufays, \u201cLong short-term memory recurrent neural network ar- chitectures for large scale acoustic modeling.,\u201d in IN- TERSPEECH, 2014, pp. 338\u2013342. [6] Guillaume Alain and Yoshua Bengio, \u201cUnderstanding intermediate layers using linear classi\ufb01er probes,\u201d arXiv preprint arXiv:1610.01644, 2016. [7] Geoffrey E Hinton and Ruslan R Salakhutdinov, \u201cRe- ducing the dimensionality of data with neural networks,\u201d Science, vol. 313, no. 5786, pp. 504\u2013507, 2006. [8] Xavier Glorot and Yoshua Bengio, \u201cUnderstanding the dif\ufb01culty of training deep feedforward neural net- works.,\u201d in Aistats, 2010, vol. 9, pp. 249\u2013256.",
            "504\u2013507, 2006. [8] Xavier Glorot and Yoshua Bengio, \u201cUnderstanding the dif\ufb01culty of training deep feedforward neural net- works.,\u201d in Aistats, 2010, vol. 9, pp. 249\u2013256. [9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, \u201cDelving deep into recti\ufb01ers: Surpassing human- level performance on imagenet classi\ufb01cation,\u201d in Pro- ceedings of the IEEE international conference on com- puter vision, 2015, pp. 1026\u20131034. [10] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior, Paul Tucker, Ke Yang, Quoc V Le, et al., \u201cLarge scale dis- tributed deep networks,\u201d in Advances in neural informa- tion processing systems, 2012, pp. 1223\u20131231.",
            "1223\u20131231. [11] Shanshan Zhang, Ce Zhang, Zhao You, Rong Zheng, and Bo Xu, \u201cAsynchronous stochastic gradient descent for dnn training,\u201d in 2013 IEEE International Confer- ence on Acoustics, Speech and Signal Processing. IEEE, 2013, pp. 6660\u20136663. [12] Jianmin Chen, Rajat Monga, Samy Bengio, and Rafal Jozefowicz, \u201cRevisiting distributed synchronous sgd,\u201d arXiv preprint arXiv:1604.00981, 2016. [13] Ryan McDonald, Keith Hall, and Gideon Mann, \u201cDis- tributed training strategies for the structured percep- tron,\u201d in Human Language Technologies: The 2010 An- nual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, 2010, pp. 456\u2013464.",
            "Association for Computational Linguistics, 2010, pp. 456\u2013464. [14] Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J Smola, \u201cParallelized stochastic gradient de- scent,\u201d in Advances in neural information processing systems, 2010, pp. 2595\u20132603. [15] Kai Chen and Qiang Huo, \u201cScalable training of deep learning machines by incremental block training with intra-block parallel optimization and blockwise model- update \ufb01ltering,\u201d in 2016 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5880\u20135884. [16] Boris T Polyak and Anatoli B Juditsky, \u201cAcceleration of stochastic approximation by averaging,\u201d SIAM Journal on Control and Optimization, vol. 30, no. 4, pp. 838\u2013 855, 1992. [17] Wei Xu, \u201cTowards optimal one pass large scale learn- ing with averaged stochastic gradient descent,\u201d arXiv preprint arXiv:1107.2490, 2011.",
            "30, no. 4, pp. 838\u2013 855, 1992. [17] Wei Xu, \u201cTowards optimal one pass large scale learn- ing with averaged stochastic gradient descent,\u201d arXiv preprint arXiv:1107.2490, 2011. [18] Tian Xu, Zhang Jun, Ma Zejun, He Yi, and Wei Juan, \u201cExponential moving average model in parallel speech recognition training,\u201d arXiv preprint arXiv:1703.01024, 2017. [19] Has\u00b8im Sak, Andrew Senior, Kanishka Rao, and Franc\u00b8oise Beaufays, \u201cFast and accurate recurrent neural network acoustic models for speech recognition,\u201d arXiv preprint arXiv:1507.06947, 2015. [20] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean, \u201cDistill- ing the knowledge in a neural network,\u201d arXiv preprint arXiv:1503.02531, 2015.",
            "[21] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh, \u201cA fast learning algorithm for deep belief nets,\u201d Neural computation, vol. 18, no. 7, pp. 1527\u20131554, 2006. [22] Brian Kingsbury, \u201cLattice-based optimization of se- quence classi\ufb01cation criteria for neural-network acous- tic modeling,\u201d in Acoustics, Speech and Signal Pro- cessing, 2009. ICASSP 2009. IEEE International Con- ference on. IEEE, 2009, pp. 3761\u20133764. [23] Has\u00b8im Sak, Andrew Senior, Kanishka Rao, Ozan Irsoy, Alex Graves, Franc\u00b8oise Beaufays, and Johan Schalk- wyk, \u201cLearning acoustic frame labeling for speech recognition with recurrent neural networks,\u201d in Acous- tics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4280\u20134284.",
            "IEEE, 2015, pp. 4280\u20134284. [24] Has\u00b8im Sak, F\u00b4elix de Chaumont Quitry, Tara Sainath, Kanishka Rao, et al., \u201cAcoustic modelling with cd-ctc- smbr lstm rnns,\u201d in Automatic Speech Recognition and Understanding (ASRU), 2015 IEEE Workshop on. IEEE, 2015, pp. 604\u2013609. [25] Yves Normandin, Hidden Markov models, maximum mutual information estimation, and the speech recogni- tion problem, Ph.D. thesis, McGill University, Montreal, 1991. [26] Daniel Povey, Discriminative training for large vocab- ulary speech recognition, Ph.D. thesis, University of Cambridge, 2005. [27] Michiel Hermans and Benjamin Schrauwen, \u201cTraining and analysing deep recurrent neural networks,\u201d in Ad- vances in Neural Information Processing Systems, 2013, pp. 190\u2013198."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1703.07090.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 8709.999908447266,
    "avg_doclen_est": 189.3478240966797
}
