{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Stacked DeBERT: All Attention in Incomplete Data for Text Classi\ufb01cation Gwenaelle Cunha Sergio1 and Minho Lee2 1 School of Electronics and Electrical Engineering 2 Department of Arti\ufb01cial Intelligence Kyungpook National University, Daegu, 41566, South Korea Abstract In this paper, we propose Stacked DeBERT, short for Stacked Denoising Bidirectional Encoder Representations from Transformers. This novel model improves robustness in incomplete data, when compared to existing systems, by designing a novel encoding scheme in BERT, a powerful language representation model solely based on attention mechanisms. Incomplete data in natural language processing refer to text with missing or incorrect words, and its presence can hinder the performance of current models that were not implemented to withstand such noises, but must still perform well even under duress. This is due to the fact that current approaches are built for and trained with clean and complete data, and thus are not able to extract features that can adequately represent incomplete data. Our proposed approach consists of obtaining intermediate input representations by applying an embedding layer to the input tokens followed by vanilla transformers.",
            "This is due to the fact that current approaches are built for and trained with clean and complete data, and thus are not able to extract features that can adequately represent incomplete data. Our proposed approach consists of obtaining intermediate input representations by applying an embedding layer to the input tokens followed by vanilla transformers. These intermediate features are given as input to novel denoising transformers which are responsible for obtaining richer input representations. The proposed approach takes advantage of stacks of multilayer perceptrons for the reconstruction of missing words\u2019 embeddings by extracting more abstract and meaningful hidden feature vectors, and bidirectional transformers for improved embedding representation. We consider two datasets for training and evaluation: the Chatbot Natural Language Understanding Evaluation Corpus and Kaggle\u2019s Twitter Sentiment Corpus. Our model shows improved F1-scores and better robustness in informal\/incorrect texts present Preprint submitted to Neural Networks (Accepted) December 14, 2021 arXiv:2001.00137v2  [cs.CL]  14 Jan 2021",
            "in tweets and in texts with Speech-to-Text error in the sentiment and intent classi\ufb01cation tasks. 1 Keywords: Incomplete Text Classi\ufb01cation, Incomplete Data, Speech-to-Text Error, BERT, Transformers, Denoising 1. Introduction Understanding a user\u2019s intent and sentiment is of utmost importance for current intelligent chatbots to respond appropriately to human requests. However, current systems are not able to perform to their best capacity when presented with incomplete data, meaning sentences with missing or incorrect words. This scenario is likely to happen when one considers human error done in writing. In fact, it is rather naive to assume that users will always type fully grammatically correct sentences [1]. This has been aggravated with the advent of the internet and social networks, which allowed language and modern communication to be been rapidly transformed [2, 3]. Take Twitter for instance, where information is expected to be readily communicated in short and concise sentences with little to no regard to correct sentence grammar or word spelling [4]. Further motivation can be found in Automatic Speech Recognition (ASR) applications, where high error rates prevail and pose an enormous hurdle in the broad adoption of speech technology by users worldwide [5].",
            "Further motivation can be found in Automatic Speech Recognition (ASR) applications, where high error rates prevail and pose an enormous hurdle in the broad adoption of speech technology by users worldwide [5]. This is an important issue to tackle because, in addition to more widespread user adoption, improving Speech-to-Text (STT) accuracy diminishes error propagation to modules using the recognized text. With that in mind, in order for current systems to improve the quality of their services, there is a need for development of robust intelligent systems that are able to understand a user even when faced with incomplete representation in language. The advancement of deep neural networks have immensely aided in the development of the Natural Language Processing (NLP) domain. Tasks such 1https:\/\/github.com\/gcunhase\/StackedDeBERT 2",
            "as text generation, sentence correction, image captioning and text classi\ufb01ca- tion, have been possible via models such as Convolutional Neural Networks and Recurrent Neural Networks [6, 7, 8]. More recently, state-of-the-art results have been achieved with attention models, more speci\ufb01cally Transformers [9]. Current approaches for Text Classi\ufb01cation tasks focus on e\ufb03cient embedding representations. Kim et al. [10] use semantically enriched word embeddings to make synonym and antonym word vectors respectively more and less similar in order to improve intent classi\ufb01cation performance. Devlin et al. [11] propose Bidirectional Encoder Representations from Transformers (BERT), a powerful bidirectional language representation model based on Transformers, achieving state-of-the-art results on eleven NLP tasks [12], including sentiment text classi\ufb01- cation. Concurrently, Shridhar et al. [13] also reach state of the art in the intent recognition task using Semantic Hashing for feature representation followed by a neural classi\ufb01er. All aforementioned approaches are, however, applied to datasets based solely on complete data.",
            "Concurrently, Shridhar et al. [13] also reach state of the art in the intent recognition task using Semantic Hashing for feature representation followed by a neural classi\ufb01er. All aforementioned approaches are, however, applied to datasets based solely on complete data. Incompleteness in data can refer to noise in either the input [14, 15, 16, 17, 18, 19, 20] or in the labels [21, 22, 23]. In this work, we focus on the former. More speci\ufb01cally, we focus on noisy text [24] obtained from social networks [14, 15, 16, 17] and through ASR processing techniques [18, 19, 20]. Studies on social media text classi\ufb01cation mainly focus on normalizing the data into a clean or standard form before classi\ufb01cation for improved perfor- mance, instead of considering the data incompleteness as it is.",
            "Studies on social media text classi\ufb01cation mainly focus on normalizing the data into a clean or standard form before classi\ufb01cation for improved perfor- mance, instead of considering the data incompleteness as it is. Vateekul and Koomsubha [14] apply pre-processing techniques on Thai Twitter data and evaluate sentiment classi\ufb01cation performance on two Deep Learning models: Long Short-Term Memory and Dynamic Convolutional Neural Network. Joshi and Deshpande [16] also apply extensive pre-processing steps to noisy social media text and extract their n-gram features in order to evaluate their sentiment on probabilistic classi\ufb01ers, namely Naive Bayes and Maximum Entropy. More recently, researchers have proposed using encoder-decoder frameworks to perform social media text normalization. Tiwari and Naskar [15] achieve good results by training their model with synthetic data and later using transfer learning on the 3",
            "WNUT 2015 shared task dataset. Lourentzou et al. [17] also achieve comparable results by proposing a hybrid word-character attention-based encoder-decoder model. Some researchers have additionally attempted at improving noisy text embeddings. Barbosa and Feng [25] propose a two-step model for sentiment classi\ufb01cation in tweets. The authors \ufb01rst classify the tweets into subjective or objective and then further classify the subjective tweets as positive or negative. They obtain a feature vector from tweets using additional information, namely meta-information from words and written characteristics from tweets, and com- pare it with other raw word representations such as n-grams. The authors then show improved classi\ufb01cation results using Support Vector Machine (SVM) as a classi\ufb01er. Other researchers, such as Vinciarelli [18] and Agarwal et al. [19], focus on the simulation of ASR noise in text and the e\ufb00ect that their introduction has in the classi\ufb01cation performance of Naive Bayes and SVM. Shrestha et al. [20] expand that research by investigating the decay in performance in logistic regression and neural networks.",
            "Shrestha et al. [20] expand that research by investigating the decay in performance in logistic regression and neural networks. These works focus on investigating the impact in the performance of an existing model given incomplete data. More recently, with the introduction of the transformer [9] based model BERT [11], researchers [26, 27] have changed the focus to improving a model\u2019s performance by extracting more robust input representation. Hrinchuk et al. [26] and Liao et al. [27] propose to use a transformer encoder-decoder architecture for the task of ASR correction. Realizing the need for further research in the area of noisy text classi\ufb01cation, we make it the focus of this paper. In this task, the model aims to identify the user\u2019s intent or sentiment by analyzing a sentence with missing and\/or incorrect words. In the sentiment classi\ufb01cation task, the model aims to identify the user\u2019s sentiment given a tweet, written in informal language and without regards for sentence correctness. As for the incomplete data problem, we approach it as a reconstruction or imputation task [28]. Vincent et al.",
            "In the sentiment classi\ufb01cation task, the model aims to identify the user\u2019s sentiment given a tweet, written in informal language and without regards for sentence correctness. As for the incomplete data problem, we approach it as a reconstruction or imputation task [28]. Vincent et al. [29, 30] and Glorot et al. [31] propose to reconstruct clean data from their noisy version by mapping the input to meaningful representations. This approach has also been shown to outperform other models, such as predictive mean matching, random forest, 4",
            "SVM and Multiple imputation by Chained Equations (MICE), at missing data imputation tasks [32, 33]. Researchers in the noisy text and missing data imputation areas have shown that meaningful feature representation of data is of utter importance for high performance achieving methods. We propose a model that combines the power of BERT in the NLP domain and the strength of denoising strategies in incom- plete data reconstruction to tackle the tasks of incomplete intent and sentiment classi\ufb01cation. This enables the implementation of a novel encoding scheme, more robust to incomplete data, called Stacked Denoising BERT or Stacked DeBERT. Our approach consists of obtaining richer input representations from input tokens by stacking denoising transformers on an embedding layer with vanilla transformers. The embedding layer and vanilla transformers extract inter- mediate input features from the input tokens, and the denoising transformers are responsible for obtaining richer input representations from them. By improving BERT with stronger denoising abilities, we are able to reconstruct missing and incorrect words\u2019 embeddings and improve classi\ufb01cation accuracy. To summarize, our contribution is three-fold: \u2022 Implementation of a novel encoding scheme to obtain richer embedding representations.",
            "By improving BERT with stronger denoising abilities, we are able to reconstruct missing and incorrect words\u2019 embeddings and improve classi\ufb01cation accuracy. To summarize, our contribution is three-fold: \u2022 Implementation of a novel encoding scheme to obtain richer embedding representations. This is done by reconstructing hidden embeddings from noisy input and using information from both incomplete and complete data during training. \u2022 Improvement on the robustness and performance of BERT when applied to incomplete data, in particular noisy user-generated texts obtained from Twitter and noisy ASR text. \u2022 Open-source code and release of corpora used in the tasks of incomplete intent and sentiment classi\ufb01cation from incorrect sentences. The remainder of this paper is organized in four sections, with Section 2 explaining the proposed model. This is followed by Section 3 which includes a detailed description of the dataset used for training and evaluation purposes and 5",
            "how it was obtained. Section 4 covers the baseline models used for comparison, training speci\ufb01cations and experimental results and Section 5 has further analysis and discussion on the results. Finally, Section 6 wraps up this paper with conclusion and future works. 2. Proposed model We propose Stacked Denoising BERT (DeBERT) as a novel encoding scheming for the task of incomplete intent classi\ufb01cation and sentiment classi\ufb01cation from incorrect sentences, such as tweets and text with STT error. The proposed model, illustrated in Fig. 1, is structured as a stacking of embedding layers and vanilla transformer layers, similarly to the conventional BERT [11], followed by layers of novel denoising transformers. It is important to note that part of the reason why BERT is such a powerful language model is due to its pre-training objectives: next sentence prediction (NSP) and masked language model (MLM). The \ufb01rst is used to help BERT understand sentence continuity by forcing it to predict whether two given sentences are in the correct order.",
            "The \ufb01rst is used to help BERT understand sentence continuity by forcing it to predict whether two given sentences are in the correct order. This is very important to model multiple-sentences inputs, but in our case, we only deal with single sentences inputs. The second pre-training objective, on the other hand, forces the model to predict random masked words, meaning that it indirectly models data incompleteness to a certain degree. However, BERT does not directly handle incorrect words in the input, resulting in a reduction in performance when faced with noisy user-generated text such as Twitter or text obtained through ASR processing techniques. We aim to ameliorate that limitation and improve the robustness and e\ufb03ciency of BERT when applied to incomplete data by reconstructing hidden embeddings from sentences with missing words. By reconstructing these hidden embeddings, we are able to improve the encoding scheme in BERT. The initial part of the model is the conventional BERT, a multi-layer bidi- rectional Transformer encoder and a powerful language model. During training, BERT is \ufb01ne-tuned on the incomplete text classi\ufb01cation corpus (see Section 3). 6",
            "h rec \u2026 E[CLS] E1 E[SEP] \u201cu kno who am i talkin bout??\u201d [CLS] Tok1 TokN \u2026 Tok2 h inc [SEP] Transformer Layers Denoising Transformers Layer E2 EN T[CLS] T1 T[SEP] \u2026 T2 TN T[CLS] T1 T[SEP] \u2026 T2 TN Text Classification Feedforward Network Softmax \u2026 E[CLS] E1 E[SEP] \u201cDo you know who I am talking about?",
            "[CLS] Tok1 TokN \u2026 Tok2 h comp [SEP] E2 EN T[CLS] T1 T[SEP] \u2026 T2 TN Incomplete text Complete text During training (a) Model architecture ETOKEN ESEGMENT EPOSITION + + = = = EINPUT EN+1 \u2026 E0 E1 EB \u2026 EA EA E[SEP] \u2026 E[CLS] ETok1 = E[SEP] \u2026 E[CLS] E1 (b) Embedding layers h rec h\u2019rec1 z1 z z2 h\u2019rec2 h\u2019rec \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 Trm Trm Trm \u2026 Trm Trm Trm \u2026 \u2026 \u2026 l1 l2 l3 l4 l5 l6 l\u20191 l\u20192 l\u20193 l\u20194 l\u20195 l\u20196 h comp \u2026 h inc \u2026 T[CLS] \u2026 T1 T[SEP] (c) Denoising transformers layer Figure 1: The proposed model Stacked DeBERT (a) is organized in three modules: embedding (b), conventional bidirectional transformers, and denoising bidirectional transformers (c).",
            "During training, our model obtains the intermediate embeddings for both incomplete and complete data, represented by hinc and hcomp respectively, with conventional BERT and trains stacks of multilayer perceptrons to partially reconstruct the intermediate embedding hinc into h\u2032 rec. This embedding is then given to bidirectional transformers to generate the \ufb01nal reconstructed embedding hrec. In the end, a more robust [CLS] token is obtained for text classi\ufb01cation. During testing, only the incomplete text is used. 7",
            "The \ufb01rst layer pre-processes the input sentence by making it lower-case and by tokenizing it. It also pre\ufb01xes the sequence of tokens with a special character [CLS] and su\ufb01xes each sentence with a [SEP] character. It is followed by an embedding layer used for input representation, with the \ufb01nal input embedding being a sum of token embedddings, segmentation embeddings and position em- beddings. The \ufb01rst one, token embedding layer, uses a vocabulary dictionary to convert each token into a more representative embedding. The segmentation embedding layer indicates which tokens constitute a sentence by signaling either 1 or 0. In our case, since our data are formed of single sentences, the segment is 1 until the \ufb01rst [SEP] character appears (indicating segment A) and then it becomes 0 (segment B). The position embedding layer, as the name indicates, adds information related to the token\u2019s position in the sentence. This prepares the data to be considered by the layers of vanilla bidirectional transformers, which outputs a hidden embedding that can be used by our novel layers of denoising transformers.",
            "The position embedding layer, as the name indicates, adds information related to the token\u2019s position in the sentence. This prepares the data to be considered by the layers of vanilla bidirectional transformers, which outputs a hidden embedding that can be used by our novel layers of denoising transformers. Although BERT has shown to perform better than other baseline models when handling incomplete data, it is still not enough to e\ufb03ciently handle such data, as mentioned in the beginning of this section. Because of that, there is a need for further improvement of the hidden feature vectors obtained from sentences with missing words. With this purpose in mind, we implement a novel encoding scheme consisting of denoising transformers, which uses information from both incomplete and complete data during training and only incomplete data during testing. During training, the conventional BERT, composed of embedding layers and vanilla transformers (see Fig. 1), is used to obtain the intermediate embeddings from both the incomplete and the complete data. These intermediate embeddings, hinc and hcomp respectively, are then used to train the denoising transformers layer in a two-step process.",
            "1), is used to obtain the intermediate embeddings from both the incomplete and the complete data. These intermediate embeddings, hinc and hcomp respectively, are then used to train the denoising transformers layer in a two-step process. The \ufb01rst step is to train stacks of multilayer perceptrons to partially reconstruct the incomplete embedding hinc into the partially, and more meaningful, recovered embedding h\u2032 rec through a comparison loss with the complete embedding hcomp. The second step improves the partial embedding representation by giving h\u2032 rec to bidirectional transformers 8",
            "to generate the \ufb01nal reconstructed embedding hrec. In the end, a more robust [CLS] token is obtained for text classi\ufb01cation. Note that the intermediate embeddings hinc and hcomp both have shape (Nbs, 768, 128), where Nbs is the batch size, 768 is the original BERT embedding size for a single token, and 128 is the maximum sequence length in a sentence. The stacks of multilayer perceptrons are structured as two sets of three layers with two hidden layers each. The \ufb01rst set is responsible for compressing the hinc into a latent-space representation, extracting more abstract features into lower dimension vectors z1, z2 and z with shape (Nbs, 128, 128), (Nbs, 32, 128), and (Nbs, 12, 128), respectively. This process is shown in Eq.",
            "This process is shown in Eq. (1): z1 = Wl2 \u0000Wl1hinc + bl1 \u0001 + bl2 = f{W,b} \u0000hinc \u0001 z2 = Wl4 \u0000Wl3z1 + bl3 \u0001 + bl4 = f{Wz1,bz1} \u0000z1 \u0001 z = Wl6 \u0000Wl5z2 + bl5 \u0001 + bl6 = f{Wz2,bz2} \u0000z2 \u0001 (1) where f(\u00b7) is the parameterized function mapping hinc to the hidden state z. The weight matrices and bias are represented by W and b respectively, with the index li\u2208{1...6} indicating its corresponding layer. The second set then respectively reconstructs z1, z2 and z into h\u2032 rec1, h\u2032 rec2 and h\u2032 rec. This process is shown in Eq.",
            "The second set then respectively reconstructs z1, z2 and z into h\u2032 rec1, h\u2032 rec2 and h\u2032 rec. This process is shown in Eq. (2): h\u2032 rec2 = Wl\u2032 2 \u0000Wl\u2032 1z + bl\u2032 1 \u0001 + bl\u2032 2 = g{W \u2032z2,b\u2032z2} \u0000z \u0001 h\u2032 rec1 = Wl\u2032 4 \u0000Wl\u2032 3h\u2032 rec2 + bl\u2032 3 \u0001 + bl\u2032 4 = g{W \u2032z1,b\u2032z1} \u0000h\u2032 rec2 \u0001 h\u2032 rec = Wl\u2032 6 \u0000Wl\u2032 5h\u2032 rec1 + bl\u2032 5 \u0001 + bl\u2032 6 = g{W \u2032,b\u2032} \u0000h\u2032 rec1 \u0001 (2) where g(\u00b7) is the parameterized function that reconstructs z as h\u2032 rec. In other words, h\u2032 rec is the intermediate reconstructed embeddings, obtained by recon- structing the latent-space representation z through a set of stacks of multilayer perceptrons.",
            "In other words, h\u2032 rec is the intermediate reconstructed embeddings, obtained by recon- structing the latent-space representation z through a set of stacks of multilayer perceptrons. The weight matrices and bias are represented by W and b respec- tively, with the index l\u2032 i\u2208{1...6} indicating its corresponding layer. The intermediate reconstructed embedding h\u2032 rec is compared with the com- plete embedding hcomp through a mean square error loss function, as shown in Eq. (3): 9",
            "L \u0000h\u2032 rec, hcomp \u0001 = 1 Nbs Nbs X i=1 \u0010 h\u2032 rec(i) \u2212hcomp(i) \u00112 (3) After reconstructing the correct hidden embeddings from the incomplete sentences, the correct hidden embeddings are given to bidirectional transformers to generate the \ufb01nal reconstructed input representations hrec. The model is then \ufb01ne-tuned in an end-to-end manner on the incomplete text classi\ufb01cation corpus. Classi\ufb01cation is done with a feedforward network and softmax activation function. Softmax \u03c3 is a discrete probability distribution function for NC classes, with the sum of the classes probability being 1 and the maximum value being the predicted class. The predicted class can be mathematically calculated as in Eq. (4): \u02c6y = arg max i\u22081...NC pi = arg max i\u22081...NC \u03c3(oi) = arg max i\u22081...NC eoi PNC k=1 eoi (4) where o = Wt + b, the output of the feedforward layer used for classi\ufb01cation. 3. Dataset 3.1.",
            "3. Dataset 3.1. Twitter Sentiment Classi\ufb01cation In order to evaluate the performance of our model, we need access to a naturally noisy dataset with real human errors. Poor quality texts obtained from Twitter, called tweets, are then ideal for our task. For this reason, we choose Kaggle\u2019s two-class Sentiment140 dataset [34]2, which consists of spoken text being used in writing and without strong consideration for grammar or sentence correctness. Thus, it has many mistakes, as speci\ufb01ed in Table 1 [17]. Even though this corpus has incorrect sentences and their emotional labels, they lack their respective corrected sentences, necessary for the training of our model. In order to obtain this missing information, we use Amazon Mechanical Turk (MTurk) [35], a paid marketplace for Human Intelligence Tasks (HITs) which allows for anonymity between \u201crequesters\u201d and \u201cworkers\u201d. This ensures 2https:\/\/www.kaggle.com\/kazanova\/sentiment140 10",
            "Table 1: Types of mistakes on the Twitter dataset. Mistake type Examples Spelling \u201cteh\u201d (the), \u201ccorreclty\u201d (correctly), \u201cteusday\u201d (Tuesday) Casual pronunciation \u201cwanna\u201d (want to), \u201cdunno\u201d (don\u2019t know) Abbreviation \u201cLit\u201d (Literature), \u201cpls\u201d (please), \u201cu\u201d (you), \u201cidk\u201d (I don\u2019t know) Repeteated letters \u201cthursdayyyyyy\u201d, \u201csleeeeeeeeeep\u201d Onomatopoeia \u201cWoohoo\u201d, \u201chmmm\u201d, \u201cyaay\u201d Others \u201cim\u201d (I\u2019m), \u201cyour\/ur\u201d (you\u2019re), \u201cryt\u201d (right) that the requester is not able to in\ufb02uence the worker into answering a survey the way they want, reducing bias and allowing for believable results to be obtained. In this work, we simply use MTurk to outsource native English speakers to obtain the correct sentences from the original tweets.",
            "In this work, we simply use MTurk to outsource native English speakers to obtain the correct sentences from the original tweets. More speci\ufb01cally, human annotators are given a list of original tweets and they are asked to correct them with as little change as possible and without inserting any extra punctuation marks unless absolutely necessary, following guidelines for noisy text normalization [17]. After getting the data back, we manually check if the corrections are acceptable, otherwise we post another HIT. We claim this is unbiased because this is only dependent on the English language, not on the sentiment or corrector\u2019s background. Some examples are shown in Table 2. Table 2: Examples of original tweets and their corrected version. Original tweet Corrected tweet \u201cgoonite sweet dreamz\u201d \u201cGood night, sweet dreams.\u201d \u201cwell i dunno..i didnt give him an ans yet\u201d \u201cWell I don\u2019t know, I didn\u2019t give him an answer yet.\u201d \u201cu kno who am i talkin bout??\u201d \u201cDo you know who I am talking about?\u201d 11",
            "After obtaining the correct sentences, our two-class dataset 3 has class distribution as shown in Table 3. There are 200 sentences used in the training stage, with 100 belonging to the positive sentiment class and 100 to the negative class, and 50 samples being used in the evaluation stage, with 25 negative and 25 positive. This totals in 250 samples, with incorrect and correct sentences combined. Since our goal is to evaluate the model\u2019s performance and robustness in the presence of noise, we only consider incorrect data in the testing phase. Note that BERT is a pre-trained model, meaning that small amounts of data are enough for appropriate \ufb01ne-tuning. Table 3: Details about our Twitter Sentiment Classi\ufb01cation dataset, composed of incorrect and correct data. Dataset Sentiment Train Test Total Sentiment140 Negative (0) 100 25 125 Positive (1) 100 25 125 Total 200 50 250 3.2.",
            "Dataset Sentiment Train Test Total Sentiment140 Negative (0) 100 25 125 Positive (1) 100 25 125 Total 200 50 250 3.2. Intent Classi\ufb01cation from Text with STT Error In the intent classi\ufb01cation task, we are presented with a corpus that su\ufb00ers from the opposite problem of the Twitter sentiment classi\ufb01cation corpus. In the intent classi\ufb01cation corpus, we have the complete sentences and intent labels but lack their corresponding incomplete sentences, and since our task revolves around text classi\ufb01cation in incomplete or incorrect data, it is essential that we obtain this information. To remedy this issue, we apply a Text-to-Speech (TTS) module followed by a Speech-to-Text (STT) module to the complete sentences in order to obtain incomplete sentences with STT error. Due to TTS and STT modules available being imperfect, the resulting sentences have a reasonable 3Available at https:\/\/github.com\/gcunhase\/StackedDeBERT 12",
            "level of noise in the form of missing or incorrectly transcribed words. Analysis on this dataset 4 adds value to our work by enabling evaluation of our model\u2019s robustness to di\ufb00erent rates of data incompleteness. The dataset used to evaluate the models\u2019 performance is the Chatbot Natural Language Understanding (NLU) Evaluation Corpus, introduced by Braun et al. [36] to test NLU services. It is a publicly available 5 benchmark and is composed of sentences obtained from a German Telegram chatbot used to answer questions about public transport connections. The dataset has two intents, namely Departure Time and Find Connection with 100 train and 106 test samples, shown in Table 4. Even though English is the main language of the benchmark, this dataset contains a few German station and street names. Table 4: Details about our Incomplete Intent Classi\ufb01cation dataset based on the Chatbot NLU Evaluation Corpus.",
            "Even though English is the main language of the benchmark, this dataset contains a few German station and street names. Table 4: Details about our Incomplete Intent Classi\ufb01cation dataset based on the Chatbot NLU Evaluation Corpus. Dataset Intent Train Test Total Chatbot NLU Departure Time (0) 43 35 98 Find Connection (1) 57 71 128 Total 100 106 206 The incomplete dataset used for training is composed of lower-cased in- complete data obtained by manipulating the original corpora. The incomplete sentences with STT error are obtained in a 2-step process shown in Fig. 2. The \ufb01rst step is to apply a TTS module to the available complete sentence. Here, we apply gtts 6, a Google Text-to-Speech python library, and macsay 7, a terminal command available in Mac OS as say.",
            "2. The \ufb01rst step is to apply a TTS module to the available complete sentence. Here, we apply gtts 6, a Google Text-to-Speech python library, and macsay 7, a terminal command available in Mac OS as say. The second step consists of applying an STT module to the obtained audio \ufb01les in order to obtain text containing STT 4Available at https:\/\/github.com\/gcunhase\/StackedDeBERT 5https:\/\/github.com\/sebischair\/NLU-Evaluation-Corpora 6https:\/\/pypi.org\/project\/gTTS\/ 7https:\/\/ss64.com\/osx\/say.html 13",
            "Original text (complete) Obtained text (incomplete) TTS Intermediate audio STT Figure 2: Diagram of 2-step process to obtain dataset with STT error in text. errors. The STT module used here was witai 8, freely available and maintained by Wit.ai. The mentioned TTS and STT modules were chosen according to code availability and whether it\u2019s freely available or has high daily usage limitations. Table 5 exempli\ufb01es a complete and its respective incomplete sentences with di\ufb00erent TTS-STT combinations, thus varying rates of missing and incorrect words. The level of noise is denoted by two metrics: inverted BLEU (iBLEU) and Word Error Rate (WER) score. The inverted BLEU score ranges from 0 to 1 and is denoted by Eq. (5): iBLEU = 1 \u2212BLEU (5) where BLEU is a common metric usually used in machine translation tasks [37]. We decide to showcase that instead of regular BLEU because it is more indicative to the amount of noise in the incomplete text, where the higher the iBLEU, the higher the noise.",
            "We decide to showcase that instead of regular BLEU because it is more indicative to the amount of noise in the incomplete text, where the higher the iBLEU, the higher the noise. We also provide noise-level measurements in terms of the WER metric, widely used to evaluate automatic speech recognition systems. This metric indicates the amount of e\ufb00ort needed to revert a given sentence into its golden form. In other words, it calculates the number of words deletion D, insertion I, and substitution S, in relation to the total number of words N in the reference, as shown in Eq. (6): WER = S + D + I N (6) 8https:\/\/wit.ai 14",
            "Similarly to iBLEU, the lower the WER score, the lower the level of noise in a sentence. Table 5: Example of sentence from Chatbot NLU Corpus with di\ufb00erent TTS-STT combinations and their respective inverted BLEU and WER scores, which denote the level of noise in the text. TTS-STT (iBLEU\/WER) Original sentence With STT error gtts-witai (0.44\/2.39) \u201chow can i get from garching to mil- bertshofen?\u201d \u201chow can i get from garching to melbourne open.\u201d \u201chow to get from bonner platz to freimann?\u201d \u201chow to get from bonner platz to fry.\u201d \u201cprinzregentenplatz to rotkreuzplatz\u201d \u201cprince richard replies to recruit plants.\u201d \u201cwhen does the next u-bahn depart at garch- ing?\u201d \u201cwhen does the next bus depart at garching.\u201d macsay-witai (0.50\/3.11) \u201chow can i get from garching to mil- bertshofen?\u201d \u201chow can i get from garching to meal prep.",
            "\u201d macsay-witai (0.50\/3.11) \u201chow can i get from garching to mil- bertshofen?\u201d \u201chow can i get from garching to meal prep.\u201d \u201chow to get from bonner platz to freimann?\u201d \u201chow to get from bonner platz to fry.\u201d \u201cprinzregentenplatz to rotkreuzplatz\u201d \u201cbrandon regional \ufb02ats to rent.\u201d \u201cwhen does the next u-bahn depart at garch- ing?\u201d \u201cwhen does the next oakland airport or city.\u201d 4. Experimental Results 4.1. Baseline models Besides the already mentioned BERT, the following baseline models are also used for comparison. NLU service platforms. We focus on the three following services, where the \ufb01rst two are commercial services and last one is open source with two separate backends: Google Dialog\ufb02ow (formerly Api.ai) 9, SAP Conversational AI (for- merly Recast.ai) 10 and Rasa (spacy and tensor\ufb02ow backend) 11.",
            "To the best 9https:\/\/dialogflow.com 10https:\/\/cai.tools.sap 11https:\/\/rasa.com 15",
            "of our knowledge, these platforms don\u2019t have any special measures to handle incomplete data. Semantic hashing with classi\ufb01er. Shridhar et al. [13] proposed a word embedding method that doesn\u2019t su\ufb00er from out-of-vocabulary issues. The authors achieve this by using hash tokens in the alphabet instead of a single word, making it vocabulary independent. For classi\ufb01cation, classi\ufb01ers such as Multilayer Perceptron (MLP), Support Vector Machine (SVM) and Random Forest are used. A complete list of classi\ufb01ers and training speci\ufb01cations are given in Section 4.2. 4.2. Training speci\ufb01cations The baseline and proposed models are each trained separately on the incom- plete intent classi\ufb01cation and Twitter sentiment classi\ufb01cation tasks. The intent classi\ufb01cation data include three di\ufb00erent datasets: complete data and two TTS- STT variants (gtts-witai and macsay-witai).",
            "The intent classi\ufb01cation data include three di\ufb00erent datasets: complete data and two TTS- STT variants (gtts-witai and macsay-witai). The sentiment classi\ufb01cation data also include three di\ufb00erent datasets: original text, corrected text and incorrect with correct texts. The reported F1 scores are the best accuracies obtained from 10 runs using each dataset individually as training data. NLU service platforms. No settable training con\ufb01gurations available in the online platforms. These frameworks simply take a given training dataset, without distinction between clean or incorrect data, and train and test it on their own platform. The only di\ufb00erence is in the data format it accepts, which changes depending on the platform or code. For example, Rasa uses json format, whereas Dialog\ufb02ow and SAP uses csv format. Semantic hashing with classi\ufb01er.",
            "The only di\ufb00erence is in the data format it accepts, which changes depending on the platform or code. For example, Rasa uses json format, whereas Dialog\ufb02ow and SAP uses csv format. Semantic hashing with classi\ufb01er. Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classi\ufb01ers with parameters set as speci\ufb01ed in the authors\u2019 paper so as to allow comparison: MLP with 3 hidden layers of sizes [300, 100, 50] respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classi\ufb01er and estimator ([50, 60, 70]; 16",
            "Linear Support Vector Classi\ufb01er with L1 and L2 penalty and tolerance of 10\u22123; Regularized linear classi\ufb01er with Stochastic Gradient Descent (SGD) learning with regularization term alpha = 10\u22124 and L1, L2 and Elastic-Net penalty; Nearest Centroid with Euclidian metric, where classi\ufb01cation is done by representing each class with a centroid; Bernoulli Naive Bayes with smoothing parameter alpha = 10\u22122; K-means clustering with 2 clusters and L2 penalty; and Logistic Regression classi\ufb01er with L2 penalty, tolerance of 10\u22124 and regularization term of 1.0. Most often, the best performing classi\ufb01er was MLP. BERT.",
            "Most often, the best performing classi\ufb01er was MLP. BERT. Conventional BERT is a BERT-Base Uncased model, meaning that it has 12 transformer blocks L, hidden size H of 768, and 12 self-attention heads A. The model is \ufb01ne-tuned with our dataset on 2 Titan X GPUs for 3 epochs with Adam Optimizer, learning rate of 2 \u221710\u22125, maximum sequence length of 128, and warm up proportion of 0.1. The train batch size is 4 for the Twitter Sentiment Corpus and 8 for the Chatbot Intent Classi\ufb01cation Corpus. Stacked DeBERT. Our proposed model is trained in an end-to-end manner on 2 Titan X GPUs with the same hyperparameters as BERT and training time depending on the size of the dataset and train batch size.",
            "Stacked DeBERT. Our proposed model is trained in an end-to-end manner on 2 Titan X GPUs with the same hyperparameters as BERT and training time depending on the size of the dataset and train batch size. The stack of multilayer perceptrons, in the denoising block of our model, are trained for 100 and 1,000 epochs with Adam Optimizer, weight decay of 10\u22125, MSE loss criterion and batch size the same as BERT (4 for the Twitter Sentiment Corpus and 8 for the Chatbot Intent Classi\ufb01cation Corpus). In this step, we increase the learning rate to 10\u22123 for faster training. 4.3. Results on Sentiment Classi\ufb01cation from Incorrect Text Experimental results for the Twitter Sentiment Classi\ufb01cation task on Kaggle\u2019s Sentiment140 Corpus dataset, displayed in Table 6, show that our model has better F1-micros scores, outperforming the baseline models by 6% to 8%. We evaluate our model and baseline models on three versions of the dataset.",
            "We evaluate our model and baseline models on three versions of the dataset. The \ufb01rst one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80% against BERT\u2019s 72%. The second version 17",
            "(Corr) considers the corrected tweets, and shows higher accuracy given that it is less noisy. In that version, Stacked DeBERT achieves 82% accuracy against BERT\u2019s 76%, an improvement of 6%. In the last case (Inc+Corr), we consider both incorrect and correct tweets as input to the models in hopes of improving performance. However, the accuracy was similar to the \ufb01rst aforementioned version, 80% for our model and 74% for the second highest performing model. Since the \ufb01rst and last corpus gave similar performances with our model, we conclude that the Twitter dataset does not require complete sentences to be given as training input, in addition to the original naturally incorrect tweets, in order to better model the noisy sentences. Table 6: F1-micro scores for the Twitter Sentiment Classi\ufb01cation task on Kaggle\u2019s Sentiment140 Corpus. Note that: (Inc) is the original dataset, with naturally incorrect tweets, (Corr) is the corrected version of the dataset and (Inc+Corr) contains both. The noise level is represented by the iBLEU score.",
            "Note that: (Inc) is the original dataset, with naturally incorrect tweets, (Corr) is the corrected version of the dataset and (Inc+Corr) contains both. The noise level is represented by the iBLEU score. F1-score (micro, %) Model Inc Corr Inc+Corr iBLEU score 0.63 0.00 0.63 Rasa (spacy) 44.00 54.00 54.00 Rasa (tensor\ufb02ow) 53.06 60.00 59.18 Dialog\ufb02ow 30.00 40.00 42.00 SAP Conversational AI 59.18 65.31 59.18 Semantic Hashing 72.00 70.00 72.00 BERT 72.00 76.00 74.00 Stacked DeBERT (ours) 80.00 82.00 80.00 In addition to the overall F1-score, we also present a confusion matrix, in Fig. 3, with the per-class F1-scores for BERT and Stacked DeBERT.",
            "3, with the per-class F1-scores for BERT and Stacked DeBERT. The normalized confusion matrix plots the predicted labels versus the target\/target labels. Similarly to Table 6, we evaluate our model with the original Twitter dataset, the corrected version and both original and corrected tweets. It can be seen that our model is able to improve the overall performance by improving 18",
            "Corr 0.88 0.12 0.64 0.36 0.84 0.20 0.80 0.16 Inc Inc+Corr 0.92 0.08 0.52 0.48 0.88 0.12 0.72 0.28 0.88 0.12 0.60 0.40 0.84 0.16 0.76 0.24 BERT Stacked DeBERT True label Predicted label 0 1 0 1 True label Predicted label 0 1 0 1 True label Predicted label 0 1 0 1 True label Predicted label 0 1 0 1 True label Predicted label 0 1 0 1 True label Predicted label 0 1 0 1 Figure 3: Normalized confusion matrix for the Twitter Sentiment Classi\ufb01cation dataset. The \ufb01rst row has the confusion matrices for BERT in the original Twitter dataset (Inc), the corrected version (Corr) and both original and corrected tweets (Inc+Corr) respectively.",
            "The \ufb01rst row has the confusion matrices for BERT in the original Twitter dataset (Inc), the corrected version (Corr) and both original and corrected tweets (Inc+Corr) respectively. The second row contains the confusion matrices for Stacked DeBERT in the same order. the accuracy of the lower performing classes. In the Inc dataset, the true class 1 in BERT performs with approximately 50%. However, Stacked DeBERT is able to improve that to 72%, although to a cost of a small decrease in performance of class 0. A similar situation happens in the remaining two datasets, with improved accuracy in class 0 from 64% to 84% and 60% to 76% respectively. 4.4. Results on Intent Classi\ufb01cation from Text with STT Error Experimental results for the Intent Classi\ufb01cation task on the Chatbot NLU Corpus with STT error can be seen in Table 7.",
            "4.4. Results on Intent Classi\ufb01cation from Text with STT Error Experimental results for the Intent Classi\ufb01cation task on the Chatbot NLU Corpus with STT error can be seen in Table 7. When presented with data containing STT error, our model outperforms all baseline models in both combi- nations of TTS-STT: gtts-witai outperforms the second placing baseline model by 0.94% with F1-score of 97.17%, and macsay-witai outperforms the next highest achieving model by 1.89% with F1-score of 96.23%. Additionally, we also present Fig. 4 with the normalized confusion matrices for BERT and Stacked DeBERT for sentences containing STT error. Analogously to the Twitter Sentiment Classi\ufb01cation task, the per-class F1-scores show that our model is able to improve the overall performance by improving the accuracy 19",
            "Table 7: F1-micro scores for the Chatbot Intent Classi\ufb01cation Corpus. Note that we include results with the original sentences (complete data) and sentences imbued with TTS-STT error (gtts-witai and macsay-witai), with the noise level being represented by the iBLEU and WER scores.",
            "Note that we include results with the original sentences (complete data) and sentences imbued with TTS-STT error (gtts-witai and macsay-witai), with the noise level being represented by the iBLEU and WER scores. F1-score (micro, %) Model Complete gtts-witai macsay-witai iBLEU score 0.00 0.44 0.50 WER score 0.00 2.39 3.11 Rasa (spacy) 92.45 91.51 86.79 Rasa (tensor\ufb02ow) 99.06 92.89 91.51 Dialog\ufb02ow 96.23 87.74 81.13 SAP Conversational AI 95.24 94.29 94.29 Semantic Hashing 99.06 95.28 91.51 BERT 98.11 96.23 94.34 Stacked DeBERT (ours) 99.06 97.17 96.23 of one class while maintaining the high-achieving accuracy of the second one.",
            "gtts-witai Complete macsay-witai BERT Stacked DeBERT True label Predicted label 0 1 0 1 True label Predicted label 0 1 0 1 True label Predicted label 0 1 0 1 True label Predicted label 0 1 0 1 True label Predicted label 0 1 0 1 True label Predicted label 0 1 0 1 0.94 0.06 1.00 0.00 0.97 0.03 0.96 0.04 0.97 0.03 1.00 0.00 0.97 0.03 0.96 0.04 1.00 0.00 0.96 0.04 0.91 0.09 0.96 0.04 Figure 4: Normalized confusion matrix for the Chatbot NLU Intent Classi\ufb01cation dataset for complete data and data with STT error. The \ufb01rst column has the confusion matrices for BERT and the second for Stacked DeBERT. 20",
            "5. Result Analysis and Discussion 5.1. Model Robustness Here we analyze the robustness of our model given varying levels of noise in the Chatbot Intent Classi\ufb01cation Corpora. Table 7 indicates the level of noise in each TTS-STT noisy dataset with their respective iBLEU and WER scores, where 0 means no noise and higher values mean higher quantity of noise. As expected, the models\u2019 accuracy degrade with the increase in noise, thus F1-scores of gtts-witai are higher than macsay-witai. However, our model does not only outperform the baseline models but does so with a wider margin. This is shown with the increasing robustness plot in Fig. 5 and can be demonstrated by macsay-witai outperforming the baseline models by twice the gap achieved by gtts-witai. Figure 5: Robustness bar plot for the Chatbot NLU Corpus with STT error. Further analysis of the results in Table 7 show that, BERT decay is almost constant with the addition of noise, with the di\ufb00erence between the complete data 21",
            "and gtts-witai being 1.88 and gtts-witai and macsay-witai being 1.89. Whereas in Stacked DeBERT, that di\ufb00erence is 1.89 and 0.94 respectively. This is stronger indication of our model\u2019s robustness in the presence of noise. 5.2. Performance Comparison with Macro-Averaged Scores In this section, we evaluate the performance of our model in more detail by adding a more extensive investigation of the results which include: clearer explanation of the confusion matrix and macro-average precision, recall, and F1 scores. Previously, in Sections 4.3 and 4.4, we only included the micro-averaged F1 score since it is considered a good measure of overall e\ufb00ectiveness of classi\ufb01ers, and it is thus the conventional evaluation metric [38]. To ensure that our following explanation is clear, please consider the confusion matrix [38], in Fig. 6, for our sentiment classi\ufb01cation task.",
            "To ensure that our following explanation is clear, please consider the confusion matrix [38], in Fig. 6, for our sentiment classi\ufb01cation task. Since our problem is a multi-class problem, where each data sample is assigned to exactly one class, the micro-averaged measures are as in Eq. (7): F1 = P = R = TP + TN TP + TN + FP + FN (7) where P is precision, R is recall, and TP, TN, FP, and FN are as indicated in Fig 6. Since the micro-averaged scores are the same, we simply display the micro-F1 score as per usual in the literature. We also further analyze the confusion matrices obtained from both datasets in regards to Type I and II errors. In the Twitter sentiment classi\ufb01cation task (Fig. 6), our model achieves better overall performance by trading-o\ufb00accuracy with the best performing class 0, meaning that our model performs slightly worse in the TN and Type I error and signi\ufb01cantly better in the Type II and TP errors when compared to the baseline model. In the Chatbot intent classi\ufb01cation task (Fig.",
            "In the Chatbot intent classi\ufb01cation task (Fig. 7), our model performs better or similarly in all instances. When considering the macro average, however, we are able to separate the in\ufb02uence of each wrong and correct prediction into the precision, recall, and F1 metrics. This allows for a more rigorous performance investigation of our model and the baseline model BERT. Once again, consider the twitter sentiment 22",
            "True Negative  (TN) False Positive  (FP, Type I) False Negative  (FN, Type II) True Positive  (TP) True label 0 (neg) 1 (pos) 0  (neg) 1  (pos) Predicted label Figure 6: Confusion matrix format for the sentiment classi\ufb01cation task, with rows indicating true labels and columns indicating predicted labels, and 0 representing the negative class and 1 representing the positive class. classi\ufb01cation task, where our goal is to classify an input data into positive (1) or negative (0) class. The macro-average measures are the average between all per-class measures. The macro-precision Pmacro indicates what proportion of positive (or negative) predictions was actually correct. In other words, how precise or accurate the model is. Mathematically, we have Eq. (8): P1 = TP TP + FP , P0 = TN TN + FN , Pmacro = P1 + P0 2 (8) The macro-recall Rmacro indicates what proportion of true positives (or true negatives) was correctly predicted. In other words, how reliable the model is in its predictions.",
            "In other words, how reliable the model is in its predictions. Mathematically, we have Eq. (9): R1 = TP TP + FN , R0 = TN TN + FP , Rmacro = R1 + R0 2 (9) Finally, the macro-F1 score combines precision and recall by calculating their harmonic mean and it is thus the preferred metric to indicate the overall performance of a model. Mathematically, we have Eq. (10): F1macro = 2 \u2217Pmacro \u2217Rmacro Pmacro + Rmacro (10) We calculate the macro-average precision, recall, and F1 scores with each dataset and show that our model outperforms the baseline in all cases. Note that, following the same pattern as the micro-F1 scores, our model signi\ufb01cantly 23",
            "outperforms the baseline model in the Twitter sentiment classi\ufb01cation task and it outperforms the baseline model in the Chatbot intent classi\ufb01cation task with a smaller margin. These results are shown in Tables 8 and 9. Table 8: Macro-average precision (P), recall (R), and F1 scores (%) for the Twitter Sentiment Classi\ufb01cation Corpus. Note that: (Inc) is the original dataset, with naturally incorrect tweets, (Corr) is the corrected version of the dataset and (Inc+Corr) contains both.",
            "Note that: (Inc) is the original dataset, with naturally incorrect tweets, (Corr) is the corrected version of the dataset and (Inc+Corr) contains both. Inc Corr Inc+Corr Model P R F1 P R F1 P R F1 BERT 76.19 72.00 70.83 77.59 76.00 75.65 76.04 74.00 73.48 Stacked DeBERT (ours) 80.79 80.00 79.87 82.05 82.00 81.99 80.19 80.00 79.97 Table 9: Macro-average precision (P), recall (R), and F1 scores (%) for the Chatbot Intent Classi\ufb01cation Corpus with the original sentences (complete data) and sentences imbued with TTS-STT error (gtts-witai and macsay-witai).",
            "Complete gtts-witai macsay-witai Model P R F1 P R F1 P R F1 BERT 98.63 97.14 97.83 95.22 96.46 95.79 93.60 93.60 93.60 Stacked DeBERT (ours) 99.31 98.57 98.93 96.05 97.89 96.87 95.22 96.46 95.79 5.3. Performance Improvement Comparison Between Datasets When comparing the improvement in performance in the Twitter and TTS- STT Chatbot datasets, we notice that the former shows major improvements whilst the latter shows only minor improvements. We investigate if this is due to lower noise levels in the Twitter dataset. However, as can be seen in Table 10, our model\u2019s better performance in the Twitter dataset is not related to it having lower noise levels when compared to the TTS-STT Chatbot corpus. A possible reason as to why our model is able to improve its performance by a larger margin in the Twitter dataset can be due to BERT being trained on the Wikipedia and Book Corpus [11].",
            "A possible reason as to why our model is able to improve its performance by a larger margin in the Twitter dataset can be due to BERT being trained on the Wikipedia and Book Corpus [11]. Twitter has arguably more noise when compared to BERT\u2019s original training data due to its highly informal setting and character limitation. However, studies suggest that social media text has relatively small grammatical 24",
            "disparity when compared to edited text such as Wikipedia [39], and since they both contain user generated texts, their basic sentence structure is still much more similar than when compared to sentences with STT error. This di\ufb00erent word composition in sentences a\ufb00ected with STT error, makes it harder for the model to perform as well as its counterpart trained with user-generated text. Table 10: Comparison of performance improvement in relation to varying levels of noise between the TTS-STT Chatbot datasets, namely gtts-witai and macsay-witai, and the Twitter Sentiment Dataset with incorrect text (Inc). Note that, for fair comparison, the WER score for the Twitter dataset has also been included here, even though that score is usually only used to measure levels of noise in text with STT error, which is only present in the Chatbot corpus.",
            "Note that, for fair comparison, the WER score for the Twitter dataset has also been included here, even though that score is usually only used to measure levels of noise in text with STT error, which is only present in the Chatbot corpus. Chatbot Twitter gtts-witai macsay-witai Inc iBLEU 0.44 0.50 0.63 WER 2.39 3.11 6.36 Improvement +0.94 +1.89 +8.00 Another interesting observation in the Twitter dataset is that our proposed model more signi\ufb01cantly improves the performance of class 1 (positive) with a small decrease of performance of class 0 (negative). However, the same pattern is not observed when we compare performance in the Chatbot dataset, where the proposed model shows minor improvements in class 0 (Departure Time intent) while maintaining the performance of class 1 (Find Connection intent).",
            "However, the same pattern is not observed when we compare performance in the Chatbot dataset, where the proposed model shows minor improvements in class 0 (Departure Time intent) while maintaining the performance of class 1 (Find Connection intent). We believe that the in-class improvement of class 1 noticed in the Twitter dataset is due to the existence of a larger gap in performance between classes 0 and 1 in the baseline BERT model, which allows for our model to achieve better overall accuracy with a trade-o\ufb00of performances. After an investigation of possible reasons why that gap in performance exists in the Twitter dataset, we conclude that that is due to di\ufb00ering average number of words in sentences from positive\/negative classes and from train\/test sets. Table 11 shows a comparison between the average number of words in the Twitter dataset and both TTS-STT Chatbot copora. As can be seen, the Chatbot corpus consistently maintains an 25",
            "average number of 9 to 10 words per sentence for both classes in both the train and test sets. Whereas the Twitter dataset has an average of 16 words for both classes during training and an average of 24 words for the negative class versus 9 words for the positive during testing, which explains the better performance in the positive class. A larger test set might also be in\ufb02uencing the smaller performance improvement detected in the Chatbot corpus (see Tables 3 and 4). Table 11: Average number of words per sentence in the TTS-STT Chatbot datasets, namely gtts-witai and macsay-witai, and the Twitter Sentiment Dataset with incorrect text (Inc). Chatbot Twitter gtts-witai macsay-witai Inc Class Train Test Train Test Train Test Avg. # 0 9 9 9 9 16 24 words 1 9 10 10 10 16 9 6. Conclusion In this work, we proposed a novel deep neural network, robust to noisy text in the form of sentences with missing and\/or incorrect words, called Stacked DeBERT.",
            "Conclusion In this work, we proposed a novel deep neural network, robust to noisy text in the form of sentences with missing and\/or incorrect words, called Stacked DeBERT. The idea was to improve the accuracy performance by improving the representation ability of the model with the implementation of novel denoising transformers. More speci\ufb01cally, our model was able to reconstruct hidden em- beddings from their respective incomplete hidden embeddings. Stacked DeBERT was compared against three NLU service platforms and two other machine learn- ing methods, namely BERT and Semantic Hashing with neural classi\ufb01er. Our model showed better performance when evaluated on F1 scores in both Twitter sentiment and intent text with STT error classi\ufb01cation tasks. The per-class F1 score was also evaluated in the form of normalized confusion matrices, showing that our model was able to improve the overall performance by better balancing the accuracy of each class, trading-o\ufb00small decreases in high achieving class for signi\ufb01cant improvements in lower performing ones. In the Chatbot dataset, 26",
            "accuracy improvement was achieved even without trade-o\ufb00, with the highest achieving classes maintaining their accuracy while the lower achieving class saw improvement. Further evaluation on the F1-scores decay in the presence of noise demonstrated that our model is more robust than the baseline models when considering noisy data, be that in the form of incorrect sentences or sentences with STT error. Not only that, experiments on the Twitter dataset also showed improved accuracy in clean data, with complete sentences. We infer that this is due to our model being able to extract richer data representations from the input data regardless of the completeness of the sentence. For future works, we plan on evaluating the robustness of our model against other types of noise, such as word reordering, word insertion, and spelling mistakes in sentences. In order to improve the performance of our model, further experiments will be done in search for more appropriate hyperparameters and more complex neural classi\ufb01ers to substitute the last feedforward network layer.",
            "In order to improve the performance of our model, further experiments will be done in search for more appropriate hyperparameters and more complex neural classi\ufb01ers to substitute the last feedforward network layer. Acknowledgments This work was partly supported by Institute of Information & Communi- cations Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (2016-0-00564, Development of Intelligent Interaction Tech- nology Based on Context Awareness and Human Intention Understanding) and Korea Evaluation Institute of Industrial Technology (KEIT) grant funded by the Korea government (MOTIE) (50%) and the Technology Innovation Program: Industrial Strategic Technology Development Program (No: 10073162) funded By the Ministry of Trade, Industry & Energy (MOTIE, Korea) (50%). References [1] Raymond R Panko. Thinking is bad: Implications of human error research for spreadsheet research and practice. arXiv preprint arXiv:0801.3114, 2008. [2] Abdu M Talib Al-Kadi and Rashad Ali Ahmed. Evolution of english in the internet age.",
            "arXiv preprint arXiv:0801.3114, 2008. [2] Abdu M Talib Al-Kadi and Rashad Ali Ahmed. Evolution of english in the internet age. Indonesian Journal of Applied Linguistics, 7(3):727\u2013736, 2018. 27",
            "[3] Jack Grieve, Andrea Nini, and Diansheng Guo. Mapping lexical innovation on american social media. Journal of English Linguistics, 46(4):293\u2013319, 2018. [4] S Sirucek. Twitter, where grammar comes to die. Hu\ufb03ng- ton Post. http:\/\/www. hu\ufb03ngtonpost. com\/stefan-sirucek\/twitter-where- grammarcom b 379191. html, 2010. [5] Rahhal Errattahi, Asmaa El Hannani, and Hassan Ouahmane. Automatic speech recognition errors detection and correction: A review. Procedia Computer Science, 128:32\u201337, 2018. [6] Gwenaelle Cunha Sergio, Dennis Singh Moirangthem, and Minho Lee. Temporal hierarchies in sequence to sequence for sentence correction. In 2018 International Joint Conference on Neural Networks (IJCNN), pages 1\u20137. IEEE, 2018.",
            "Temporal hierarchies in sequence to sequence for sentence correction. In 2018 International Joint Conference on Neural Networks (IJCNN), pages 1\u20137. IEEE, 2018. [7] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3156\u20133164, 2015. [8] Yoon Kim. Convolutional neural networks for sentence classi\ufb01cation. arXiv preprint arXiv:1408.5882, 2014. [9] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,  Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998\u20136008, 2017.",
            "Attention is all you need. In Advances in neural information processing systems, pages 5998\u20136008, 2017. [10] Joo-Kyung Kim, Gokhan Tur, Asli Celikyilmaz, Bin Cao, and Ye-Yi Wang. Intent detection using semantically enriched word embeddings. In 2016 IEEE Spoken Language Technology Workshop (SLT), pages 414\u2013419. IEEE, 2016. 28",
            "[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [12] Alex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018. [13] Kumar Shridhar, Amit Sahu, Ayushman Dash, Pedro Alonso, Gustav Pihlgren, Vinay Pondeknath, Fotini Simistira, and Marcus Liwicki. Subword semantic hashing for intent classi\ufb01cation on small datasets. arXiv preprint arXiv:1810.07150, 2018. [14] Peerapon Vateekul and Thanabhat Koomsubha. A study of sentiment analysis using deep learning techniques on thai twitter data.",
            "arXiv preprint arXiv:1810.07150, 2018. [14] Peerapon Vateekul and Thanabhat Koomsubha. A study of sentiment analysis using deep learning techniques on thai twitter data. In 2016 13th International Joint Conference on Computer Science and Software Engineering (JCSSE), pages 1\u20136. IEEE, 2016. [15] Ajay Shankar Tiwari and Sudip Kumar Naskar. Normalization of social media text using deep neural networks. In Proceedings of the 14th Inter- national Conference on Natural Language Processing (ICON-2017), pages 312\u2013321, 2017. [16] Shaunak Joshi and Deepali Deshpande. Twitter sentiment analysis system. arXiv preprint arXiv:1807.07752, 2018. [17] Ismini Lourentzou, Kabir Manghnani, and ChengXiang Zhai. Adapting sequence to sequence models for text normalization in social media.",
            "arXiv preprint arXiv:1807.07752, 2018. [17] Ismini Lourentzou, Kabir Manghnani, and ChengXiang Zhai. Adapting sequence to sequence models for text normalization in social media. In Proceedings of the International AAAI Conference on Web and Social Media, volume 13, pages 335\u2013345, 2019. [18] Alessandro Vinciarelli. Noisy text categorization. IEEE transactions on pattern analysis and machine intelligence, 27(12):1882\u20131895, 2005. [19] Sumeet Agarwal, Shantanu Godbole, Diwakar Punjani, and Shourya Roy. How much noise is too much: A study in automatic text classi\ufb01cation. In 29",
            "Seventh IEEE International Conference on Data Mining (ICDM 2007), pages 3\u201312. IEEE, 2007. [20] Niraj Shrestha, Elias Moons, and Marie-Francine Moens. Using related text sources to improve classi\ufb01cation of transcribed speech data. In International Conference on Advanced Machine Learning Technologies and Applications, pages 507\u2013517. Springer, 2019. [21] Kamal Nigam, Andrew Kachites McCallum, Sebastian Thrun, and Tom Mitchell. Text classi\ufb01cation from labeled and unlabeled documents using em. Machine learning, 39(2-3):103\u2013134, 2000. [22] Ganesh Ramakrishnan, Krishna Prasad Chitrapura, Raghu Krishnapuram, and Pushpak Bhattacharyya. A model for handling approximate, noisy or incomplete labeling in text classi\ufb01cation. In Proceedings of the 22nd international conference on Machine learning, pages 681\u2013688, 2005.",
            "A model for handling approximate, noisy or incomplete labeling in text classi\ufb01cation. In Proceedings of the 22nd international conference on Machine learning, pages 681\u2013688, 2005. [23] Yuta Tsuboi, Hisashi Kashima, Shinsuke Mori, Hiroki Oda, and Yuji Mat- sumoto. Training conditional random \ufb01elds using incomplete annotations. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 897\u2013904, 2008. [24] L Venkata Subramaniam. Noisy text analytics. In NAACL (Tutorial Abstracts), pages 5\u20136, 2010. [25] Luciano Barbosa and Junlan Feng. Robust sentiment detection on twitter from biased and noisy data. In Coling 2010: Posters, pages 36\u201344, 2010. [26] Oleksii Hrinchuk, Mariya Popova, and Boris Ginsburg. Correction of automatic speech recognition with transformer sequence-to-sequence model.",
            "In Coling 2010: Posters, pages 36\u201344, 2010. [26] Oleksii Hrinchuk, Mariya Popova, and Boris Ginsburg. Correction of automatic speech recognition with transformer sequence-to-sequence model. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7074\u20137078. IEEE, 2020. [27] Junwei Liao, Se\ufb01k Emre Eskimez, Liyang Lu, Yu Shi, Ming Gong, Linjun Shou, Hong Qu, and Michael Zeng. Improving readability for automatic speech recognition transcription. arXiv preprint arXiv:2004.04438, 2020. 30",
            "[28] Irfan Pratama, Adhistya Erna Permanasari, Igi Ardiyanto, and Rini In- drayani. A review of missing values handling methods on time-series data. In 2016 International Conference on Information Technology Systems and Innovation (ICITSI), pages 1\u20136. IEEE, 2016. [29] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Man- zagol. Extracting and composing robust features with denoising autoen- coders. In Proceedings of the 25th international conference on Machine learning, pages 1096\u20131103. ACM, 2008. [30] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of machine learning research, 11(Dec):3371\u20133408, 2010. [31] Xavier Glorot, Antoine Bordes, and Yoshua Bengio.",
            "Journal of machine learning research, 11(Dec):3371\u20133408, 2010. [31] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse recti\ufb01er neural networks. In Proceedings of the fourteenth international conference on arti\ufb01cial intelligence and statistics, pages 315\u2013323, 2011. [32] Lovedeep Gondara and Ke Wang. Multiple imputation using deep denoising autoencoders. arXiv preprint arXiv:1705.02737, 2017. [33] Adriana Fonseca Costa, Miriam Seoane Santos, Jastin Pompeu Soares, and Pedro Henriques Abreu. Missing data imputation via denoising autoencoders: the untold story. In International Symposium on Intelligent Data Analysis, pages 87\u201398. Springer, 2018. [34] Alec Go, Richa Bhayani, and Lei Huang. Twitter sentiment classi\ufb01cation using distant supervision. CS224N Project Report, Stanford, 1(12):2009, 2009.",
            "Springer, 2018. [34] Alec Go, Richa Bhayani, and Lei Huang. Twitter sentiment classi\ufb01cation using distant supervision. CS224N Project Report, Stanford, 1(12):2009, 2009. [35] Michael Buhrmester, Tracy Kwang, and Samuel D Gosling. Amazon\u2019s mechanical turk: A new source of inexpensive, yet high-quality, data? Perspectives on psychological science, 6(1):3\u20135, 2011. 31",
            "[36] Daniel Braun, Adrian Hernandez-Mendez, Florian Matthes, and Manfred Langen. Evaluating natural language understanding services for conver- sational question answering systems. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 174\u2013185, 2017. [37] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for Computational Linguistics, 2002. [38] Marina Sokolova and Guy Lapalme. A systematic analysis of performance measures for classi\ufb01cation tasks. Information processing & management, 45(4):427\u2013437, 2009. [39] Timothy Baldwin, Paul Cook, Marco Lui, Andrew MacKinlay, and Li Wang. How noisy social media text, how di\ufb00rnt social media sources? In Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 356\u2013364, 2013. 32"
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-2001.00137.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 13785.999755859375,
    "avg_doclen_est": 162.188232421875
}
