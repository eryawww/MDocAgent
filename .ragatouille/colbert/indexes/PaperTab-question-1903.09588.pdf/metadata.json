{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "arXiv:1903.09588v1  [cs.CL]  22 Mar 2019 Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence Chi Sun, Luyao Huang, Xipeng Qiu\u2217 Shanghai Key Laboratory of Intelligent Information Processing, Fudan University School of Computer Science, Fudan University 825 Zhangheng Road, Shanghai, China {sunc17,lyhuang18,xpqiu}@fudan.edu.cn Abstract Aspect-based sentiment analysis (ABSA), which aims to identify \ufb01ne-grained opinion polarity towards a speci\ufb01c aspect, is a chal- lenging subtask of sentiment analysis (SA). In this paper, we construct an auxiliary sen- tence from the aspect and convert ABSA to a sentence-pair classi\ufb01cation task, such as ques- tion answering (QA) and natural language in- ference (NLI).",
      "In this paper, we construct an auxiliary sen- tence from the aspect and convert ABSA to a sentence-pair classi\ufb01cation task, such as ques- tion answering (QA) and natural language in- ference (NLI). We \ufb01ne-tune the pre-trained model from BERT and achieve new state-of- the-art results on SentiHood and SemEval- 2014 Task 4 datasets.1 1 Introduction Sentiment analysis (SA) is an important task in natural language processing. It solves the com- putational processing of opinions, emotions, and subjectivity - sentiment is collected, analyzed and summarized. It has received much attention not only in academia but also in industry, provid- ing real-time feedback through online reviews on websites such as Amazon, which can take advan- tage of customers\u2019 opinions on speci\ufb01c products or services. The underlying assumption of this task is that the entire text has an overall polarity.",
      "The underlying assumption of this task is that the entire text has an overall polarity. However, the users\u2019 comments may contain dif- ferent aspects, such as: \u201cThis book is a hardcover version, but the price is a bit high.\u201d The polarity in \u2018appearance\u2019 is positive, and the polarity regarding \u2018price\u2019 is negative. Aspect-based sentiment analy- sis (ABSA) (Jo and Oh, 2011; Pontiki et al., 2014, 2015, 2016) aims to identify \ufb01ne-grained polarity towards a speci\ufb01c aspect. This task allows users to evaluate aggregated sentiments for each aspect of a given product or service and gain a more granu- lar understanding of their quality. \u2217Corresponding author. 1The source codes are available at https://github.com/HSLCY/ABSA-BERT-pair Both SA and ABSA are sentence-level or document-level tasks, but one comment may re- fer to more than one object, and sentence-level tasks cannot handle sentences with multiple tar- gets. Therefore, Saeidi et al.",
      "Therefore, Saeidi et al. (2016) introduce the task of targeted aspect-based sentiment analy- sis (TABSA), which aims to identify \ufb01ne-grained opinion polarity towards a speci\ufb01c aspect associ- ated with a given target. The task can be divided into two steps: (1) the \ufb01rst step is to determine the aspects associated with each target; (2) the second step is to resolve the polarity of aspects to a given target. The earliest work on (T)ABSA relied heav- ily on feature engineering (Wagner et al., 2014; Kiritchenko et al., 2014), and subsequent neu- ral network-based methods (Nguyen and Shirai, 2015; Wang et al., 2016; Tang et al., 2015, 2016; Wang et al., 2017) achieved higher accuracy. Re- cently, Ma et al. (2018) incorporate useful com- monsense knowledge into a deep neural net- work to further enhance the result of the model. Liu et al.",
      "Re- cently, Ma et al. (2018) incorporate useful com- monsense knowledge into a deep neural net- work to further enhance the result of the model. Liu et al. (2018) optimize the memory network and apply it to their model to better capture lin- guistic structure. More recently, the pre-trained language models, such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2018), have shown their effectiveness to allevi- ate the effort of feature engineering. Especially, BERT has achieved excellent results in QA and NLI. However, there is not much improvement in (T)ABSA task with the direct use of the pre- trained BERT model (see Table 3). We think this is due to the inappropriate use of the pre-trained BERT model. Since the input representation of BERT can rep- resent both a single text sentence and a pair of text sentences, we can convert (T)ABSA into a sentence-pair classi\ufb01cation task and \ufb01ne-tune the",
      "pre-trained BERT. In this paper, we investigate several methods of constructing an auxiliary sentence and trans- form (T)ABSA into a sentence-pair classi\ufb01cation task. We \ufb01ne-tune the pre-trained model from BERT and achieve new state-of-the-art results on (T)ABSA task. We also conduct a comparative ex- periment to verify that the classi\ufb01cation based on a sentence-pair is better than the single-sentence classi\ufb01cation with \ufb01ne-tuned BERT, which means that the improvement is not only from BERT but also from our method. In particular, our contribu- tion is two-fold: 1. We propose a new solution of (T)ABSA by converting it to a sentence-pair classi\ufb01cation task. 2. We \ufb01ne-tune the pre-trained BERT model and achieve new state-of-the-art results on Senti- Hood and SemEval-2014 Task 4 datasets. 2 Methodology In this section, we describe our method in detail.",
      "2. We \ufb01ne-tune the pre-trained BERT model and achieve new state-of-the-art results on Senti- Hood and SemEval-2014 Task 4 datasets. 2 Methodology In this section, we describe our method in detail. 2.1 Task description TABSA In TABSA, a sentence s usually con- sists of a series of words: {w1, \u00b7 \u00b7 \u00b7 , wm}, and some of the words {wi1, \u00b7 \u00b7 \u00b7 , wik} are pre-identi\ufb01ed targets {t1, \u00b7 \u00b7 \u00b7 , tk}, following Saeidi et al. (2016), we set the task as a 3- class classi\ufb01cation problem: given the sen- tence s, a set of target entities T and a \ufb01xed aspect set A = {general, price, transit- location, safety}, predict the sentiment polarity y \u2208{positive, negative, none} over the full set of the target-aspect pairs {(t, a) : t \u2208T, a \u2208A}.",
      "As we can see in Table 1, the gold standard polar- ity of (LOCATION2, price) is negative, while the polarity of (LOCATION1, price) is none. ABSA In ABSA, the target-aspect pairs {t, a} become only aspects a. This setting is equiva- lent to learning subtasks 3 (Aspect Category De- tection) and subtask 4 (Aspect Category Polarity) of SemEval-2014 Task 42 at the same time. 2.2 Construction of the auxiliary sentence For simplicity, we mainly describe our method with TABSA as an example. We consider the following four methods to con- vert the TABSA task into a sentence pair classi\ufb01- cation task: 2http://alt.qcri.org/semeval2014/task4/ Example: LOCATION2 is central London so extremely ex- pensive, LOCATION1 is often considered the coolest area of London. Target Aspect Sentiment LOC1 general Positive LOC1 price None LOC1 safety None LOC1 transit-location None LOC2 general None LOC2 price Negative LOC2 safety None LOC2 transit-location Positive Table 1: An example of SentiHood dataset.",
      "Target Aspect Sentiment LOC1 general Positive LOC1 price None LOC1 safety None LOC1 transit-location None LOC2 general None LOC2 price Negative LOC2 safety None LOC2 transit-location Positive Table 1: An example of SentiHood dataset. Methods Output Auxiliary Sentence QA-M S.P. Question w/o S.P. NLI-M S.P. Pseudo-sentence w/o S.P. QA-B {yes,no} Question w/ S.P. NLI-B {yes,no} Pseudo-sentence w/ S.P. Table 2: The construction methods. Due to limited space, we use the following abbreviations: S.P. for sen- timent polarity, w/o for without, and w/ for with. Sentences for QA-M The sentence we want to generate from the target-aspect pair is a question, and the format needs to be the same.",
      "for sen- timent polarity, w/o for without, and w/ for with. Sentences for QA-M The sentence we want to generate from the target-aspect pair is a question, and the format needs to be the same. For example, for the set of a target-aspect pair (LOCATION1, safety), the sentence we generate is \u201cwhat do you think of the safety of location - 1 ?\u201d Sentences for NLI-M For the NLI task, the con- ditions we set when generating sentences are less strict, and the form is much simpler. The sen- tence created at this time is not a standard sen- tence, but a simple pseudo-sentence, with (LOCA- TION1, safety) pair as an example: the auxiliary sentence is: \u201clocation - 1 - safety\u201d. Sentences for QA-B For QA-B, we add the la- bel information and temporarily convert TABSA into a binary classi\ufb01cation problem (label \u2208 {yes, no}) to obtain the probability distribution.",
      "Sentences for QA-B For QA-B, we add the la- bel information and temporarily convert TABSA into a binary classi\ufb01cation problem (label \u2208 {yes, no}) to obtain the probability distribution. At this time, each target-aspect pair will gener- ate three sequences such as \u201cthe polarity of the aspect safety of location - 1 is positive\u201d, \u201cthe polarity of the aspect safety of location - 1 is negative\u201d, \u201cthe polarity of the aspect safety of location - 1 is none\u201d. We use the probabil-",
      "ity value of yes as the matching score. For a target-aspect pair which generates three sequences (positive, negative, none), we take the class of the sequence with the highest matching score for the predicted category. Sentences for NLI-B The difference between NLI-B and QA-B is that the auxiliary sentence changes from a question to a pseudo-sentence. The auxiliary sentences are: \u201clocation - 1 - safety - positive\u201d, \u201clocation - 1 - safety - negative\u201d, and \u201clocation - 1 - safety - none\u201d. After we construct the auxiliary sentence, we can transform the TABSA task from a single sen- tence classi\ufb01cation task to a sentence pair classi- \ufb01cation task. As shown in Table 3, this is a nec- essary operation that can signi\ufb01cantly improve the experimental results of the TABSA task. 2.3 Fine-tuning pre-trained BERT BERT (Devlin et al., 2018) is a new language rep- resentation model, which uses bidirectional trans- formers to pre-train a large corpus, and \ufb01ne-tunes the pre-trained model on other tasks.",
      "2.3 Fine-tuning pre-trained BERT BERT (Devlin et al., 2018) is a new language rep- resentation model, which uses bidirectional trans- formers to pre-train a large corpus, and \ufb01ne-tunes the pre-trained model on other tasks. We \ufb01ne- tune the pre-trained BERT model on TABSA task. Let\u2019s take a brief look at the input representation and the \ufb01ne-tuning procedure. 2.3.1 Input representation The input representation of the BERT can explic- itly represent a pair of text sentences in a sequence of tokens. For a given token, its input represen- tation is constructed by summing the correspond- ing token, segment, and position embeddings. For classi\ufb01cation tasks, the \ufb01rst word of each sequence is a unique classi\ufb01cation embedding ([CLS]). 2.3.2 Fine-tuning procedure BERT \ufb01ne-tuning is straightforward.",
      "For classi\ufb01cation tasks, the \ufb01rst word of each sequence is a unique classi\ufb01cation embedding ([CLS]). 2.3.2 Fine-tuning procedure BERT \ufb01ne-tuning is straightforward. To obtain a \ufb01xed-dimensional pooled representation of the in- put sequence, we use the \ufb01nal hidden state (i.e., the output of the transformer) of the \ufb01rst token as the input. We denote the vector as C \u2208RH. Then we add a classi\ufb01cation layer whose param- eter matrix is W \u2208RK\u00d7H, where K is the num- ber of categories. Finally, the probability of each category P is calculated by the softmax function P = softmax(CW T). 2.3.3 BERT-single and BERT-pair BERT-single for (T)ABSA BERT for single sentence classi\ufb01cation tasks. Suppose the number of target categories are nt and aspect categories are na.",
      "2.3.3 BERT-single and BERT-pair BERT-single for (T)ABSA BERT for single sentence classi\ufb01cation tasks. Suppose the number of target categories are nt and aspect categories are na. We consider TABSA as a combination of nt \u00b7 na target-aspect-related sentiment classi\ufb01- cation problems, \ufb01rst classifying each sentiment classi\ufb01cation problem, and then summarizing the results obtained. For ABSA, We \ufb01ne-tune pre- trained BERT model to train na classi\ufb01ers for all aspects and then summarize the results. BERT-pair for (T)ABSA BERT for sentence pair classi\ufb01cation tasks. Based on the auxil- iary sentence constructed in Section 2.2, we use the sentence-pair classi\ufb01cation approach to solve (T)ABSA. Corresponding to the four ways of con- structing sentences, we name the models: BERT- pair-QA-M, BERT-pair-NLI-M, BERT-pair-QA- B, and BERT-pair-NLI-B.",
      "Corresponding to the four ways of con- structing sentences, we name the models: BERT- pair-QA-M, BERT-pair-NLI-M, BERT-pair-QA- B, and BERT-pair-NLI-B. 3 Experiments 3.1 Datasets We evaluate our method on the SentiHood (Saeidi et al., 2016) dataset3, which consists of 5,215 sentences, 3,862 of which contain a single target, and the remainder multiple targets. Each sentence contains a list of target-aspect pairs {t, a} with the sentiment polarity y. Ultimately, given a sentence s and the target t in the sentence, we need to: (1) detect the mention of an aspect a for the tar- get t; (2) determine the positive or negative sentiment polarity y for detected target-aspect pairs. We also evaluate our method on SemEval-2014 Task 4 (Pontiki et al., 2014) dataset4 for aspect- based sentiment analysis.",
      "We also evaluate our method on SemEval-2014 Task 4 (Pontiki et al., 2014) dataset4 for aspect- based sentiment analysis. The only difference from the SentiHood is that the target-aspect pairs {t, a} become only aspects a. This setting allows us to jointly evaluate subtask 3 (Aspect Category Detection) and subtask 4 (Aspect Category Polar- ity). 3.2 Hyperparameters We use the pre-trained uncased BERT-base model5 for \ufb01ne-tuning. The number of Trans- former blocks is 12, the hidden layer size is 768, the number of self-attention heads is 12, and the total number of parameters for the pre- trained model is 110M. When \ufb01ne-tuning, we keep 3Dataset mirror: https://github.com/uclmr/jack/tree/master /data/sentihood 4http://alt.qcri.org/semeval2014/task4/ 5https://storage.googleapis.com/bert models/2018 10 18/ uncased L-12 H-768 A-12.zip",
      "Model Aspect Sentiment Acc. F1 AUC Acc. AUC LR (Saeidi et al., 2016) - 39.3 92.4 87.5 90.5 LSTM-Final (Saeidi et al., 2016) - 68.9 89.8 82.0 85.4 LSTM-Loc (Saeidi et al., 2016) - 69.3 89.7 81.9 83.9 LSTM+TA+SA (Ma et al., 2018) 66.4 76.7 - 86.8 - SenticLSTM (Ma et al., 2018) 67.4 78.2 - 89.3 - Dmu-Entnet (Liu et al., 2018) 73.5 78.5 94.4 91.0 94.8 BERT-single 73.7 81.0 96.4 85.5 84.2 BERT-pair-QA-M 79.4 86.4 97.0 93.6 96.",
      "5 94.4 91.0 94.8 BERT-single 73.7 81.0 96.4 85.5 84.2 BERT-pair-QA-M 79.4 86.4 97.0 93.6 96.4 BERT-pair-NLI-M 78.3 87.0 97.5 92.1 96.5 BERT-pair-QA-B 79.2 87.9 97.1 93.3 97.0 BERT-pair-NLI-B 79.8 87.5 96.6 92.8 96.9 Table 3: Performance on SentiHood dataset. We boldface the score with the best performance across all models. We use the results reported in Saeidi et al. (2016), Ma et al. (2018) and Liu et al. (2018). \u201c-\u201d means not reported. the dropout probability at 0.1, set the number of epochs to 4. The initial learning rate is 2e-5, and the batch size is 24.",
      "(2018) and Liu et al. (2018). \u201c-\u201d means not reported. the dropout probability at 0.1, set the number of epochs to 4. The initial learning rate is 2e-5, and the batch size is 24. 3.3 Exp-I: TABSA We compare our model with the following models: \u2022 LR (Saeidi et al., 2016): a logistic regression classi\ufb01er with n-gram and pos-tag features. \u2022 LSTM-Final (Saeidi et al., 2016): a biLSTM model with the \ufb01nal state as a representation. \u2022 LSTM-Loc (Saeidi et al., 2016): a biLSTM model with the state associated with the tar- get position as a representation. \u2022 LSTM+TA+SA (Ma et al., 2018): a biLSTM model which introduces complex target-level and sentence-level attention mechanisms. \u2022 SenticLSTM (Ma et al., 2018): an upgraded version of the LSTM+TA+SA model which introduces external information from Sentic- Net (Cambria et al., 2016).",
      "\u2022 SenticLSTM (Ma et al., 2018): an upgraded version of the LSTM+TA+SA model which introduces external information from Sentic- Net (Cambria et al., 2016). \u2022 Dmu-Entnet (Liu et al., 2018): a bi- directional EntNet (Henaff et al., 2016) with external \u201cmemory chains\u201d with a delayed memory update mechanism to track entities. During the evaluation of SentiHood, following Saeidi et al. (2016), we only consider the four most frequently seen aspects (general, price, transit-location, safety). When evaluating the as- pect detection, following Ma et al. (2018), we use strict accuracy and Macro-F1, and we also report AUC. In sentiment classi\ufb01cation, we use accuracy and macro-average AUC as the evaluation indices. 3.3.1 Results Results on SentiHood are presented in Table 3.",
      "(2018), we use strict accuracy and Macro-F1, and we also report AUC. In sentiment classi\ufb01cation, we use accuracy and macro-average AUC as the evaluation indices. 3.3.1 Results Results on SentiHood are presented in Table 3. The results of the BERT-single model on aspect detection are better than Dmu-Entnet, but the ac- curacy of sentiment classi\ufb01cation is much lower than that of both SenticLstm and Dmu-Entnet, with a difference of 3.8 and 5.5 respectively. However, BERT-pair outperforms other models on aspect detection and sentiment analysis by a substantial margin, obtaining 9.4 macro-average F1 and 2.6 accuracies improvement over Dmu- Entnet. Overall, the performance of the four BERT-pair models is close. It is worth noting that BERT-pair-NLI models perform relatively better on aspect detection, while BERT-pair-QA models perform better on sentiment classi\ufb01cation.",
      "Overall, the performance of the four BERT-pair models is close. It is worth noting that BERT-pair-NLI models perform relatively better on aspect detection, while BERT-pair-QA models perform better on sentiment classi\ufb01cation. Also, the BERT-pair-QA-B and BERT-pair-NLI-B mod- els can achieve better AUC values on sentiment classi\ufb01cation than the other models. 3.4 Exp-II: ABSA The benchmarks for SemEval-2014 Task 4 are the two best performing systems in Pontiki et al. (2014) and ATAE-LSTM (Wang et al., 2016). When evaluating SemEval-2014 Task 4 subtask 3 and subtask 4, following Pontiki et al. (2014), we use Micro-F1 and accuracy respectively.",
      "Models P R F1 XRCE 83.23 81.37 82.29 NRC-Canada 91.04 86.24 88.58 BERT-single 92.78 89.07 90.89 BERT-pair-QA-M 92.87 90.24 91.54 BERT-pair-NLI-M 93.15 90.24 91.67 BERT-pair-QA-B 93.04 89.95 91.47 BERT-pair-NLI-B 93.57 90.83 92.18 Table 4: Test set results for Semeval-2014 task 4 Sub- task 3: Aspect Category Detection. We use the results reported in XRCE (Brun et al., 2014) and NRC-Canada (Kiritchenko et al., 2014).",
      "We use the results reported in XRCE (Brun et al., 2014) and NRC-Canada (Kiritchenko et al., 2014). Models 4-way 3-way Binary XRCE 78.1 - - NRC-Canada 82.9 - - LSTM - 82.0 88.3 ATAE-LSTM - 84.0 89.9 BERT-single 83.7 86.9 93.3 BERT-pair-QA-M 85.2 89.3 95.4 BERT-pair-NLI-M 85.1 88.7 94.4 BERT-pair-QA-B 85.9 89.9 95.6 BERT-pair-NLI-B 84.6 88.7 95.1 Table 5: Test set accuracy (%) for Semeval-2014 task 4 Subtask 4: Aspect Category Polarity. We use the results reported in XRCE (Brun et al., 2014), NRC- Canada (Kiritchenko et al., 2014) and ATAE-LSTM (Wang et al., 2016).",
      "We use the results reported in XRCE (Brun et al., 2014), NRC- Canada (Kiritchenko et al., 2014) and ATAE-LSTM (Wang et al., 2016). \u201c-\u201d means not reported. 3.4.1 Results Results on SemEval-2014 are presented in Ta- ble 4 and Table 5. We \ufb01nd that BERT-single has achieved better results on these two subtasks, and BERT-pair has achieved further improvements over BERT-single. The BERT-pair-NLI-B model achieves the best performance for aspect category detection. For aspect category polarity, BERT- pair-QA-B performs best on all 4-way, 3-way, and binary settings. 4 Discussion Why is the experimental result of the BERT-pair model so much better? On the one hand, we convert the target and aspect information into an auxiliary sentence, which is equivalent to expo- nentially expanding the corpus.",
      "4 Discussion Why is the experimental result of the BERT-pair model so much better? On the one hand, we convert the target and aspect information into an auxiliary sentence, which is equivalent to expo- nentially expanding the corpus. A sentence si in the original data set will be expanded into (si, t1, a1), \u00b7 \u00b7 \u00b7 , (si, t1, ana), \u00b7 \u00b7 \u00b7 , (si, tnt, ana) in the sentence pair classi\ufb01cation task. On the other hand, it can be seen from the amazing improve- ment of the BERT model on the QA and NLI tasks (Devlin et al., 2018) that the BERT model has an advantage in dealing with sentence pair classi\ufb01ca- tion tasks. This advantage comes from both un- supervised masked language model and next sen- tence prediction tasks. TABSA is more complicated than SA due to ad- ditional target and aspect information. Directly \ufb01ne-tuning the pre-trained BERT on TABSA does not achieve performance growth.",
      "This advantage comes from both un- supervised masked language model and next sen- tence prediction tasks. TABSA is more complicated than SA due to ad- ditional target and aspect information. Directly \ufb01ne-tuning the pre-trained BERT on TABSA does not achieve performance growth. However, when we separate the target and the aspect to form an auxiliary sentence and transform the TABSA into a sentence pair classi\ufb01cation task, the scenario is similar to QA and NLI, and then the advantage of the pre-trained BERT model can be fully utilized. Our approach is not limited to TABSA, and this construction method can be used for other similar tasks. For ABSA, we can use the same approach to construct the auxiliary sentence with only aspects. In BERT-pair models, BERT-pair-QA-B and BERT-pair-NLI-B achieve better AUC values on sentiment classi\ufb01cation, probably because of the modeling of label information. 5 Conclusion In this paper, we constructed an auxiliary sen- tence to transform (T)ABSA from a single sen- tence classi\ufb01cation task to a sentence pair clas- si\ufb01cation task.",
      "5 Conclusion In this paper, we constructed an auxiliary sen- tence to transform (T)ABSA from a single sen- tence classi\ufb01cation task to a sentence pair clas- si\ufb01cation task. We \ufb01ne-tuned the pre-trained BERT model on the sentence pair classi\ufb01cation task and obtained the new state-of-the-art results. We compared the experimental results of single sentence classi\ufb01cation and sentence pair classi\ufb01- cation based on BERT \ufb01ne-tuning, analyzed the advantages of sentence pair classi\ufb01cation, and ver- i\ufb01ed the validity of our conversion method. In the future, we will apply this conversion method to other similar tasks. Acknowledgments We would like to thank the anonymous re- viewers for their valuable comments. The re- search work is supported by Shanghai Munic- ipal Science and Technology Commission (No. 16JC1420401 and 17JC1404100), National Key Research and Development Program of China (No. 2017YFB1002104), and National Natural Science Foundation of China (No. 61672162 and 61751201).",
      "References Caroline Brun, Diana Nicoleta Popa, and Claude Roux. 2014. Xrce: Hybrid classi\ufb01cation for aspect-based sentiment analysis. In Proceedings of the 8th In- ternational Workshop on Semantic Evaluation (Se- mEval 2014), pages 838\u2013842. Erik Cambria, Soujanya Poria, Rajiv Bajpai, and Bj\u00a8orn Schuller. 2016. Senticnet 4: A semantic resource for sentiment analysis based on conceptual primitives. In Proceedings of COLING 2016, the 26th Inter- national Conference on Computational Linguistics: Technical Papers, pages 2666\u20132677. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805. Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, and Yann LeCun. 2016. Tracking the world state with recurrent entity networks.",
      "arXiv preprint arXiv:1810.04805. Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, and Yann LeCun. 2016. Tracking the world state with recurrent entity networks. arXiv preprint arXiv:1612.03969. Yohan Jo and Alice H Oh. 2011. Aspect and senti- ment uni\ufb01cation model for online review analysis. In Proceedings of the fourth ACM international con- ference on Web search and data mining, pages 815\u2013 824. ACM. Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and Saif Mohammad. 2014. Nrc-canada-2014: Detect- ing aspects and sentiment in customer reviews. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 437\u2013 442. Fei Liu, Trevor Cohn, and Timothy Baldwin. 2018. Re- current entity networks with delayed memory update for targeted aspect-based sentiment analysis. arXiv preprint arXiv:1804.11019.",
      "Fei Liu, Trevor Cohn, and Timothy Baldwin. 2018. Re- current entity networks with delayed memory update for targeted aspect-based sentiment analysis. arXiv preprint arXiv:1804.11019. Yukun Ma, Haiyun Peng, and Erik Cambria. 2018. Targeted aspect-based sentiment analysis via em- bedding commonsense knowledge into an attentive lstm. In Proceedings of AAAI. Thien Hai Nguyen and Kiyoaki Shirai. 2015. Phrasernn: Phrase recursive neural network for aspect-based sentiment analysis. In Proceedings of the 2015 Conference on Empirical Methods in Nat- ural Language Processing, pages 2509\u20132514. Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. arXiv preprint arXiv:1802.05365.",
      "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. arXiv preprint arXiv:1802.05365. Maria Pontiki, Dimitris Galanis, Haris Papageor- giou, Ion Androutsopoulos, Suresh Manandhar, AL- Smadi Mohammad, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing Qin, Orph\u00b4ee De Clercq, et al. 2016. Semeval-2016 task 5: Aspect based sentiment anal- ysis. In Proceedings of the 10th international work- shop on semantic evaluation (SemEval-2016), pages 19\u201330. Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Suresh Manandhar, and Ion Androutsopoulos. 2015. Semeval-2015 task 12: Aspect based sentiment anal- ysis.",
      "Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Suresh Manandhar, and Ion Androutsopoulos. 2015. Semeval-2015 task 12: Aspect based sentiment anal- ysis. In Proceedings of the 9th International Work- shop on Semantic Evaluation (SemEval 2015), pages 486\u2013495. Maria Pontiki, Dimitris Galanis, John Pavlopou- los, Harris Papageorgiou, Ion Androut- sopoulos, and Suresh Manandhar. 2014. Semeval-2014 task 4: Aspect based sentiment analysis. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27\u201335. Association for Computational Linguistics. Alec Radford, Karthik Narasimhan, Tim Sali- mans, and Ilya Sutskever. 2018. Improv- ing language understanding by generative pre- training. URL https://s3-us-west-2. amazon- aws. com/openai-assets/research-covers/language- unsupervised/language understanding paper.",
      "2018. Improv- ing language understanding by generative pre- training. URL https://s3-us-west-2. amazon- aws. com/openai-assets/research-covers/language- unsupervised/language understanding paper. pdf. Marzieh Saeidi, Guillaume Bouchard, Maria Liakata, and Sebastian Riedel. 2016. Sentihood: targeted aspect based sentiment analysis dataset for urban neighbourhoods. arXiv preprint arXiv:1610.03771. Duyu Tang, Bing Qin, Xiaocheng Feng, and Ting Liu. 2015. Effective lstms for target- dependent sentiment classi\ufb01cation. arXiv preprint arXiv:1512.01100. Duyu Tang, Bing Qin, and Ting Liu. 2016. Aspect level sentiment classi\ufb01cation with deep memory net- work. arXiv preprint arXiv:1605.08900. Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab Barman, Dasha Bogdanova, Jennifer Foster, and Lamia Tounsi.",
      "arXiv preprint arXiv:1605.08900. Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab Barman, Dasha Bogdanova, Jennifer Foster, and Lamia Tounsi. 2014. Dcu: Aspect-based polarity classi\ufb01cation for semeval task 4. In Proceedings of the 8th international workshop on semantic evalua- tion (SemEval 2014), pages 223\u2013229. Bo Wang, Maria Liakata, Arkaitz Zubiaga, and Rob Procter. 2017. Tdparse: Multi-target-speci\ufb01c sen- timent recognition on twitter. In Proceedings of the 15th Conference of the European Chapter of the As- sociation for Computational Linguistics: Volume 1, Long Papers, volume 1, pages 483\u2013493. Yequan Wang, Minlie Huang, Li Zhao, et al. 2016. Attention-based lstm for aspect-level sentiment clas- si\ufb01cation. In Proceedings of the 2016 conference on empirical methods in natural language processing, pages 606\u2013615."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1903.09588.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":6294,
  "avg_doclen":174.8333333333,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1903.09588.pdf"
    }
  }
}