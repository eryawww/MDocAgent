{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Learning High-order Structural and Attribute information by Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding Wenqiang Liu, Hongyun cai, Xu Cheng, Sifa Xie ,Yipeng Yu,Hanyu Zhang Interactive Entertainment Group, Tencent Inc, Shenzhen, China. liuwenqiangcs@gmail.com, {laineycai,alexcheng,sifaxie,dukehyzhang} @tencent.com, yypzju@163.com Abstract The goal of representation learning of knowledge graph is to encode both entities and relations into a low-dimensional embedding spaces. Many recent works have demonstrated the bene\ufb01ts of knowledge graph embedding on knowledge graph completion task, such as relation extraction. However, we observe that: 1) existing method just take direct rela- tions between entities into consideration and fails to express high-order structural relationship between entities; 2) these methods just leverage relation triples of KGs while ignor- ing a large number of attribute triples that encoding rich se- mantic information. To overcome these limitations, this pa- per propose a novel knowledge graph embedding method, named (KANE), which is inspired by the recent develop- ments of graph convolutional networks (GCN).",
            "To overcome these limitations, this pa- per propose a novel knowledge graph embedding method, named (KANE), which is inspired by the recent develop- ments of graph convolutional networks (GCN). KANE can capture both high-order structural and attribute information of KGs in an ef\ufb01cient, explicit and uni\ufb01ed manner under the graph convolutional networks framework. Empirical results on three datasets show that KANE signi\ufb01cantly outperforms seven state-of-arts methods. Further analysis verify the ef\ufb01- ciency of our method and the bene\ufb01ts brought by the attention mechanism. Introduction In the past decade, many large-scale Knowledge Graphs (KGs), such as Freebase (Bollacker et al. 2008), DBpe- dia (Auer et al. 2007) and YAGO (Suchanek, Kasneci, and Weikum 2007) have been built to represent human complex knowledge about the real-world in the machine-readable for- mat.",
            "2008), DBpe- dia (Auer et al. 2007) and YAGO (Suchanek, Kasneci, and Weikum 2007) have been built to represent human complex knowledge about the real-world in the machine-readable for- mat. The facts in KGs are usually encoded in the form of triples (head entity, relation, tail entity) (denoted (h, r, t) in this study) through the Resource Description Frame- work1, e.g.,(Donald Trump, BornIn, New York City). Fig- ure 2 shows the subgraph of knowledge graph about the family of Donald Trump. In many KGs, we can observe that some relations indicate attributes of entities, such as the Born and Abstract in Figure 2, and others indicates the relations between entities (the head entity and tail en- tity are real world entity). Hence, the relationship in KG Copyright c\u20dd2020, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved.",
            "Hence, the relationship in KG Copyright c\u20dd2020, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved. 1https:\/\/www.w3.org\/TR\/rdf11-concepts\/ Figure 1: Subgraph of a knowledge graph contains entities, relations and attributes. can be divided into relations and attributes, and corre- spondingly two types of triples, namely relation triples and attribute triples (Sun, Hu, and Li 2017). A rela- tion triples in KGs represents relationship between enti- ties, e.g.,(Donald Trump, Fatherof, Ivanka Trump), while attribute triples denote a literal attribute value of an entity, e.g.,(Donald Trump, Born, \u201dJune 14, 1946\u201d). Knowledge graphs have became important basis for many arti\ufb01cial intelligence applications, such as recommenda- tion system (Wang et al. 2018), question answering (Hao et al. 2017) and information retrieval (Xiong, Power, and Callan 2017), which is attracting growing interests in both academia and industry communities.",
            "2018), question answering (Hao et al. 2017) and information retrieval (Xiong, Power, and Callan 2017), which is attracting growing interests in both academia and industry communities. A common approach to apply KGs in these arti\ufb01cial intelligence applications is through embedding, which provide a simple method to encode both entities and relations into a continuous low- dimensional embedding spaces. Hence, learning distribu- tional representation of knowledge graph has attracted many research attentions in recent years. TransE (Bordes et al. 2013) is a seminal work in representation learning low- dimensional vectors for both entities and relations. The ba- sic idea behind TransE is that the embedding t of tail entity should be close to the head entity\u2019s embedding r plus the re- lation vector t if (h, r, t) holds, which indicates h + r \u2248t. This model provide a \ufb02exible way to improve the ability in completing the KGs, such as predicating the missing items in knowledge graph. Since then, several methods like TransH (Wang et al. 2014) and TransR (Lin et al.",
            "This model provide a \ufb02exible way to improve the ability in completing the KGs, such as predicating the missing items in knowledge graph. Since then, several methods like TransH (Wang et al. 2014) and TransR (Lin et al. 2015b), which represent the relational translation in other effective forms, have been proposed. Recent attempts focused on ei- ther incorporating extra information beyond KG triples (Xie, arXiv:1910.03891v2  [cs.AI]  10 Oct 2019",
            "Liu, and Sun 2016; Fatemi, Ravanbakhsh, and Poole 2019; An et al. 2018; Guan, Song, and Liao 2019), or designing more complicated strategies (Ding et al. 2018; Vilnis et al. 2018; Cai and Wang 2018). While these methods have achieved promising results in KG completion and link predication, existing knowledge graph embedding methods still have room for improvement. First, TransE and its most extensions only take direct re- lations between entities into consideration. We argue that the high-order structural relationship between entities also contain rich semantic relationships and incorporating these information can improve model performance. For exam- ple the fact Donald Trump F atherof \u2212\u2192 Ivanka Trump Spouse \u2212\u2192 Jared Kushner indicates the relationship between entity Donald Trump and entity Jared Kushner. Several path-based methods have attempted to take multiple-step relation paths into consideration for learning high-order structural infor- mation of KGs (Lin et al. 2015a; Toutanova et al. 2016). But note that huge number of paths posed a critical complex- ity challenge on these methods.",
            "2015a; Toutanova et al. 2016). But note that huge number of paths posed a critical complex- ity challenge on these methods. In order to enable ef\ufb01cient path modeling, these methods have to make approximations by sampling or applying path selection algorithm. We argue that making approximations has a large impact on the \ufb01nal performance. Second, to the best of our knowledge, most existing knowledge graph embedding methods just leverage relation triples of KGs while ignoring a large number of attribute triples. Therefore, these methods easily suffer from sparse- ness and incompleteness of knowledge graph. Even worse, structure information usually cannot distinguish the differ- ent meanings of relations and entities in different triples. We believe that these rich information encoded in attribute triples can help explore rich semantic information and fur- ther improve the performance of knowledge graph. For ex- ample, we can learn date of birth and abstraction from val- ues of Born and Abstract about Donald Trump in Figure 2.",
            "We believe that these rich information encoded in attribute triples can help explore rich semantic information and fur- ther improve the performance of knowledge graph. For ex- ample, we can learn date of birth and abstraction from val- ues of Born and Abstract about Donald Trump in Figure 2. There are a huge number of attribute triples in real KGs, for example the statistical results in (Sun, Hu, and Li 2017) shows attribute triples are three times as many as relation- ship triples in English DBpedia (2016-04)2. Recent a few attempts try to incorporate attribute triples (Fatemi, Ravan- bakhsh, and Poole 2019; An et al. 2018). However, these are two limitations existing in these methods. One is that only a part of attribute triples are used in the existing methods, such as only entity description is used in (An et al. 2018). The other is some attempts try to jointly model the attribute triples and relation triples in one uni\ufb01ed optimization prob- lem. The loss of two kinds triples has to be carefully bal- anced during optimization.",
            "2018). The other is some attempts try to jointly model the attribute triples and relation triples in one uni\ufb01ed optimization prob- lem. The loss of two kinds triples has to be carefully bal- anced during optimization. For example, (Sun, Hu, and Li 2017) use hyper-parameters to weight the loss of two kinds triples in their models. Considering limitations of existing knowledge graph em- bedding methods, we believe it is of critical importance to develop a model that can capture both high-order struc- tural and attribute information of KGs in an ef\ufb01cient, ex- plicit and uni\ufb01ed manner. Towards this end, inspired by the recent developments of graph convolutional networks 2http:\/\/wiki.dbpedia.org\/downloads-2016-04 (GCN) (Kipf and Welling 2017), which have the poten- tial of achieving the goal but have not been explored much for knowledge graph embedding, we propose Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding (KANE). The key ideal of KANE is to aggre- gate all attribute triples with bias and perform embedding propagation based on relation triples when calculating the representations of given entity.",
            "The key ideal of KANE is to aggre- gate all attribute triples with bias and perform embedding propagation based on relation triples when calculating the representations of given entity. Speci\ufb01cally, two carefully designs are equipped in KANE to correspondingly address the above two challenges: 1) recursive embedding propaga- tion based on relation triples, which updates a entity em- bedding. Through performing such recursively embedding propagation, the high-order structural information of kGs can be successfully captured in a linear time complexity; and 2) multi-head attention-based aggregation. The weight of each attribute triples can be learned through applying the neural attention mechanism (Veli\u02c7ckovi\u00b4c et al. 2018). In experiments, we evaluate our model on two KGs tasks including knowledge graph completion and entity classi\ufb01- cation. Experimental results on three datasets shows that our method can signi\ufb01cantly outperforms state-of-arts methods. The main contributions of this study are as follows: 1) We highlight the importance of explicitly modeling the high-order structural and attribution information of KGs to provide better knowledge graph embedding.",
            "Experimental results on three datasets shows that our method can signi\ufb01cantly outperforms state-of-arts methods. The main contributions of this study are as follows: 1) We highlight the importance of explicitly modeling the high-order structural and attribution information of KGs to provide better knowledge graph embedding. 2) We proposed a new method KANE, which achieves can capture both high-order structural and attribute information of KGs in an ef\ufb01cient, explicit and uni\ufb01ed manner under the graph convolutional networks framework. 3) We conduct experiments on three datasets, demonstrat- ing the effectiveness of KANE and its interpretability in un- derstanding the importance of high-order relations. Related Work In recent years, there are many efforts in Knowledge Graph Embeddings for KGs aiming to encode entities and rela- tions into a continuous low-dimensional embedding spaces. Knowledge Graph Embedding provides a very simply and effective methods to apply KGs in various arti\ufb01cial intelli- gence applications. Hence, Knowledge Graph Embeddings has attracted many research attentions in recent years.",
            "Knowledge Graph Embedding provides a very simply and effective methods to apply KGs in various arti\ufb01cial intelli- gence applications. Hence, Knowledge Graph Embeddings has attracted many research attentions in recent years. The general methodology is to de\ufb01ne a score function for the triples and \ufb01nally learn the representations of entities and relations by minimizing the loss function fr(h, t), which implies some types of transformations on h and t. TransE (Bordes et al. 2013) is a seminal work in knowledge graph embedding, which assumes the embedding t of tail entity should be close to the head entity\u2019s embedding r plus the relation vector t when (h, r, t) holds as mentioned in sec- tion \u201cIntroduction\u201d. Hence, TransE de\ufb01nes the following loss function: fr(h, t) = ||h + r \u2212t||l1\/l2. (1) TransE regarding the relation as a translation between head entity and tail entity is inspired by the word2vec (Mikolov et al. 2013), where relationships between words often cor- respond to translations in latent feature space.",
            "(1) TransE regarding the relation as a translation between head entity and tail entity is inspired by the word2vec (Mikolov et al. 2013), where relationships between words often cor- respond to translations in latent feature space. This model achieves a good trade-off between computational ef\ufb01ciency and accuracy in KGs with thousands of relations. but this",
            "model has \ufb02aws in dealing with one-to-many, many-to-one and many-to-many relations. In order to address this issue, TransH (Wang et al. 2014) models a relation as a relation-speci\ufb01c hyperplane together with a translation on it, allowing entities to have distinct rep- resentation in different relations. TransR (Lin et al. 2015b) models entities and relations in separate spaces, i.e., entity space and relation spaces, and performs translation from en- tity spaces to relation spaces. TransD (Ji et al. 2015) cap- tures the diversity of relations and entities simultaneously by de\ufb01ning dynamic mapping matrix. Recent attempts can be divided into two categories: (i) those which tries to incor- porate additional information to further improve the perfor- mance of knowledge graph embedding, e.g., entity types or concepts (Guan, Song, and Liao 2019), relations paths (Lin et al. 2015a), textual descriptions (Fatemi, Ravanbakhsh, and Poole 2019; An et al. 2018) and logical rules (Guo et al.",
            "2015a), textual descriptions (Fatemi, Ravanbakhsh, and Poole 2019; An et al. 2018) and logical rules (Guo et al. 2016); (ii) those which tries to design more complicated strategies, e.g., deep neural network models (Schlichtkrull et al. 2018). Except for TransE and its extensions, some efforts mea- sure plausibility by matching latent semantics of entities and relations. The basic idea behind these models is that the plausible triples of a KG is assigned low energies. For exam- ples, Distant Model (Bordes et al. 2011) de\ufb01nes two differ- ent projections for head and tail entity in a speci\ufb01c relation, i.e., Mr,1 and Mr,2. It represents the vectors of head and tail entity can be transformed by these two projections. The loss function is fr(h, t) = ||Mr,1h \u2212Mr,2t||1.",
            "It represents the vectors of head and tail entity can be transformed by these two projections. The loss function is fr(h, t) = ||Mr,1h \u2212Mr,2t||1. Our KANE is conceptually advantageous to existing methods in that: 1) it directly factors high-order relations into the predictive model in linear time which avoids the la- bor intensive process of materializing paths, thus is more ef\ufb01cient and convenient to use; 2) it directly encodes all at- tribute triples in learning representation of entities which can capture rich semantic information and further improve the performance of knowledge graph embedding, and 3) KANE can directly factors high-order relations and attribute infor- mation into the predictive model in an ef\ufb01cient, explicit and uni\ufb01ed manner, thus all related parameters are tailored for optimizing the embedding objective. Problem Formulation In this study, wo consider two kinds of triples existing in KGs: relation triples and attribute triples. Relation triples de- note the relation between entities, while attribute triples de- scribe attributes of entities.",
            "Problem Formulation In this study, wo consider two kinds of triples existing in KGs: relation triples and attribute triples. Relation triples de- note the relation between entities, while attribute triples de- scribe attributes of entities. Both relation and attribute triples denotes important information about entity, we will take both of them into consideration in the task of learning repre- sentation of entities. We let I denote the set of IRIs (Interna- tionalized Resource Identi\ufb01er), B are the set of blank nodes, and L are the set of literals (denoted by quoted strings). The relation triples and attribute triples can be formalized as fol- lows: De\ufb01nition 1. Relation and Attribute Triples: A set of Relation triples TR can be represented by TR \u2282E \u00d7R\u00d7E, where E \u2282I \u222aB is set of entities, R \u2282I is set of relations between entities. Similarly, TA \u2282E \u00d7 R \u00d7 A is the set of attribute triples, where A \u2282I \u222aB \u222aL is the set of attribute values. De\ufb01nition 2.",
            "Similarly, TA \u2282E \u00d7 R \u00d7 A is the set of attribute triples, where A \u2282I \u222aB \u222aL is the set of attribute values. De\ufb01nition 2. Knowledge Graph: A KG consists of a combination of relation triples in the form of (h, r, t) \u2208 TR, and attribute triples in form of (h, r, a) \u2208TA. For- mally, we represent a KG as G = (E, R, A, TR, TA), where E = {h, t|(h, r, t) \u2208TR \u222a(h, r, a) \u2208TA} is set of entities, R = {r|(h, r, t) \u2208TR \u222a(h, r, a) \u2208TA} is set of relations, A = {a|(h, r, a) \u2208TA}, respectively.",
            "The purpose of this study is try to use embedding-based model which can capture both high-order structural and at- tribute information of KGs that assigns a continuous repre- sentations for each element of triples in the form (h, r, t) and (h, r, a), where Boldfaced h \u2208Rk, r \u2208Rk, t \u2208Rk and a \u2208Rk denote the embedding vector of head entity h, relation r, tail entity t and attribute a respectively. Next, we detail our proposed model which models both high-order structural and attribute information of KGs in an ef\ufb01cient, explicit and uni\ufb01ed manner under the graph convo- lutional networks framework. Proposed Model In this section, we present the proposed model in detail. We \ufb01rst introduce the overall framework of KANE, then discuss the input embedding of entities, relations and values in KGs, the design of embedding propagation layers based on graph attention network and the loss functions for link predication and entity classi\ufb01cation task, respectively. Overall Architecture The process of KANE is illustrated in Figure 2. We intro- duce the architecture of KANE from left to right.",
            "Overall Architecture The process of KANE is illustrated in Figure 2. We intro- duce the architecture of KANE from left to right. As shown in Figure 2, the whole triples of knowledge graph as input. The task of attribute embedding lays is embedding every value in attribute triples into a continuous vector space while preserving the semantic information. To capture both high- order structural information of KGs, we used an attention- based embedding propagation method. This method can re- cursively propagate the embeddings of entities from an en- tity\u2019s neighbors, and aggregate the neighbors with different weights. The \ufb01nal embedding of entities, relations and val- ues are feed into two different deep neural network for two different tasks including link predication and entity classi\ufb01- cation. Attribute Embedding Layer The value in attribute triples usually is sentence or a word. To encode the representation of value from its sentence or word, we need to encode the variable-length sentences to a \ufb01xed-length vector. In this study, we adopt two different encoders to model the attribute value. Bag-of-Words Encoder.",
            "To encode the representation of value from its sentence or word, we need to encode the variable-length sentences to a \ufb01xed-length vector. In this study, we adopt two different encoders to model the attribute value. Bag-of-Words Encoder. The representation of attribute value can be generated by a summation of all words embed- dings of values. We denote the attribute value a as a word sequence a = w1, ..., wn, where wi is the word at position i. The embedding of a can be de\ufb01ned as follows. a = n X i=1 wi, (2)",
            "Donald  Trump New York Donald John Trump (born June 14, 1946) is an American businessman, author, television producer... June 14,  1946 Jared  Kushner Judaism January 10,  1981 Ivanka  Trump \u2026 Word Embedding n n Embedding  Propagation  Layer Embedding  Propagation  Layer Concatenate  Aggregator Averaging  Aggregator Linear h r v h W v \ud835\udf0b(\u210e, (\ud835\udc63+ \ud835\udc5f)) Leaky ReLu h r Linear t r W \ud835\udf0b(\u210e, (\ud835\udc61+ \ud835\udc5f)) Leaky ReLu Attribute Embedding Layer  Embedding Propagation Layers Output Layer Embedding Propagation Layer Link Prediction \u2248  \uff0b  h r t Conv+Relu Conv+Relu y1 y2 ym \u2026 Entity   classification Figure 2: Illustration of the KANE architecture. where wi \u2208Rk is the word embedding of wi. Bag-of-Words Encoder is a simple and intuitive method, which can capture the relative importance of words. But this method suffers in that two strings that contains the same words with different order will have the same representa- tion.",
            "where wi \u2208Rk is the word embedding of wi. Bag-of-Words Encoder is a simple and intuitive method, which can capture the relative importance of words. But this method suffers in that two strings that contains the same words with different order will have the same representa- tion. LSTM Encoder. In order to overcome the limitation of Bag-of-Word encoder, we consider using LSTM networks to encoder a sequence of words in attribute value into a sin- gle vector. The \ufb01nal hidden state of the LSTM networks is selected as a representation of the attribute value. a = flstm(w1, w2, w3, ..., wn), (3) where flstm is the LSTM network. Embedding Propagation Layer Next we describe the details of recursively embedding prop- agation method building upon the architecture of graph con- volution network. Moreover, by exploiting the idea of graph attention network, out method learn to assign varying levels of importance to entity in every entity\u2019s neighborhood and can generate attentive weights of cascaded embedding prop- agation. In this study, embedding propagation layer consists of two mainly components: attentive embedding propaga- tion and embedding aggregation.",
            "In this study, embedding propagation layer consists of two mainly components: attentive embedding propaga- tion and embedding aggregation. Here, we start by describ- ing the attentive embedding propagation. Attentive Embedding Propagation: Considering an KG G, the input to our layer is a set of entities, relations and attribute values embedding. We use h \u2208Rk to denote the embedding of entity h. The neighborhood of entity h can be described by Nh = {t, a|(h, r, t) \u2208TR \u222a(h, r, a) \u2208TA}. The purpose of attentive embedding propagation is encode Nh and output a vector \u20d7h as the new embedding of entity h. In order to obtain suf\ufb01cient expressive power, one learn- able linear transformation W \u2208Rk \u2032\u00d7k is adopted to trans- form the input embeddings into higher level feature space.",
            "In this study, we take a triple (h, r, t) as example and the output a vector \u20d7h can be formulated as follows: \u20d7h = X t\u2208Nh \u03c0(h, r, t)W(r + t), (4) where \u03c0(h, r, t) is attention coef\ufb01cients which indicates the importance of entity\u2019s t to entities h . In this study, the attention coef\ufb01cients also control how many information being propagated from its neighborhood through the relation. To make attention coef\ufb01cients easily comparable between different entities, the attention coef\ufb01- cient \u03c0(h, r, t) can be computed using a softmax function over all the triples connected with h. The softmax function can be formulated as follows: \u03c0(h, r, t) = exp(\u03c0(h, r, t)) P t\u2032\u2208Nh exp(\u03c0(h, r\u2032, t\u2032)).",
            "(5) Hereafter, we implement the attention coef\ufb01cients \u03c0(h, r, t) through a single-layer feedforward neural net- work, which is formulated as follows: \u03c0(h, r, t) = LeakyRelu((Wr)T W(r + t)), (6) where the leakyRelu is selected as activation function. As shown in Equation 6, the attention coef\ufb01cient score is depend on the distance head entity h and the tail entity t plus the relation r, which follows the idea behind TransE that the embedding t of head entity should be close to the tail entity\u2019s embedding r plus the relation vector t if (h, r, t) holds. Embedding Aggregation. To stabilize the learning pro- cess of attention, we perform multi-head attention on \ufb01nal layer. Speci\ufb01cally, we use m attention mechanism to execute the transformation of Equation 4. A aggregators is needed to combine all embeddings of multi-head graph attention layer.",
            "To stabilize the learning pro- cess of attention, we perform multi-head attention on \ufb01nal layer. Speci\ufb01cally, we use m attention mechanism to execute the transformation of Equation 4. A aggregators is needed to combine all embeddings of multi-head graph attention layer. In this study, we adapt two types of aggregators: \u2022 Concatenation Aggregator concatenates all embeddings of multi-head graph attention, followed by a nonlinear transformation: \u20d7h\u2019 = LeakyReLu(W( m\f\f\f \f\f\f i=1 X t\u2208Nh \u03c0(h, r, t)iWi(r + t))), (7) where \f\f\f \f\f\f represents concatenation, \u03c0(h, r, t)i are normal- ized attention coef\ufb01cient computed by the i-th attentive embedding propagation, and Wi denotes the linear trans- formation of input embedding. \u2022 Averaging Aggregator sums all embeddings of multi-head graph attention and the output embedding in the \ufb01nal is",
            "calculated applying averaging: \u20d7h\u2019 = LeakyReLu( 1 m( m X i=1 X t\u2208Nh \u03c0(h, r, t)iWi(r + t))). (8) In order to encode the high-order connectivity information in KGs, we use multiple embedding propagation layers to gathering the deep information propagated from the neigh- bors. More formally, the embedding of entity h in l-th layers can be de\ufb01ned as follows: \u20d7h (l) = X t\u2208Nh \u03c0(h, r, t)W(r(l\u22121) + t(l\u22121)). (9) After performing L embedding propagation layers, we can get the \ufb01nal embedding of entities, relations and attribute values, which include both high-order structural and at- tribute information of KGs. Next, we discuss the loss func- tions of KANE for two different tasks and introduce the learning and optimization detail. Output Layer and Training Details Here, we introduce the learning and optimization details for our method. Two different loss functions are carefully de- signed fro two different tasks of KG, which include knowl- edge graph completion and entity classi\ufb01cation.",
            "Output Layer and Training Details Here, we introduce the learning and optimization details for our method. Two different loss functions are carefully de- signed fro two different tasks of KG, which include knowl- edge graph completion and entity classi\ufb01cation. Next details of these two loss functions are discussed. knowledge graph completion. This task is a classical task in knowledge graph representation learning community. Speci\ufb01cally, two subtasks are included in knowledge graph completion: entity predication and link predication. Entity predication aims to infer the impossible head\/tail entities in testing datasets when one of them is missing, while the link predication focus on complete a triple when relation is missing. In this study, we borrow the idea of translational scoring function from TransE, which the embedding t of tail entity should be close to the head entity\u2019s embedding r plus the relation vector t if (h, r, t) holds, which indicates d(h + r, t) = ||h + r \u2212t||.",
            "Speci\ufb01cally, we train our model using hinge-loss function, given formally as L = X (h,r,e)\u2208T X (h\u2032,r,e\u2032)\u2208T \u2032 [\u03b3 +d(h+r, e)\u2212d(h\u2032 +r \u2212e\u2032)]+, (10) where \u03b3 > 0 is a margin hyper-parameter, [x]+ denotes the positive part of x, T = TR \u222aTA is the set of valid triples, and T \u2032 is set of corrupted triples which can be formulated as: T \u2032 = {(h\u2032, r, e)|h\u2032 \u2208E} \u222a{(h, r, e\u2032)|e\u2032 \u2208E}. (11) Entity Classi\ufb01cation. For the task of entity classi\ufb01cation, we simple uses a fully connected layers and binary cross- entropy loss (BCE) over sigmoid activation on the output of last layer.",
            "(11) Entity Classi\ufb01cation. For the task of entity classi\ufb01cation, we simple uses a fully connected layers and binary cross- entropy loss (BCE) over sigmoid activation on the output of last layer. We minimize the binary cross-entropy on all labeled entities, given formally as: L = \u2212 1 |ED| X e\u2208ED C X j=1 [yej log(\u03c3(fej))+(1\u2212yej) log(1\u2212\u03c3(fej))] (12) where ED is the set of entities indicates have labels, C is the dimension of the output features, which is equal to the number of classes, yej is the label indicator of entity e for j-th class, and \u03c3(x) is sigmoid function \u03c3(x) = 1 1+e\u2212x . We optimize these two loss functions using mini-batch stochastic gradient decent (SGD) over the possible h, r, t, with the chin rule that applying to update all parameters. At each step, we update the parameter h\u03c4+1 \u2190h\u03c4 \u2212\u03bb\u2207hL, where \u03c4 labels the iteration step and \u03bb is the learning rate.",
            "At each step, we update the parameter h\u03c4+1 \u2190h\u03c4 \u2212\u03bb\u2207hL, where \u03c4 labels the iteration step and \u03bb is the learning rate. Experiments Date sets In this study, we evaluate our model on three real KG in- cluding two typical large-scale knowledge graph: Freebase (Bollacker et al. 2008), DBpedia (Auer et al. 2007) and a self-construction game knowledge graph. First, we adapt a dataset extracted from Freebase, i.e., FB24K, which used by (Lin, Liu, and Sun 2016). Then, we collect extra entities and relations that from DBpedia which that they should have at least 100 mentions (Bordes et al. 2013) and they could link to the entities in the FB24K by the sameAs triples. Finally, we build a datasets named as DBP24K. In addition, we build a game datasets from our game knowledge graph, named as Game30K. The statistics of datasets are listed in Table 1. Table 1: The statistics of datasets.",
            "Finally, we build a datasets named as DBP24K. In addition, we build a game datasets from our game knowledge graph, named as Game30K. The statistics of datasets are listed in Table 1. Table 1: The statistics of datasets. Datasets FB24K DBP24K Game30K #Entities 23,643 22,951 30,845 #Relations 673 2,561 1,318 #Attributes 314 1,561 2,760 #Total Triples 423,560 437,561 370,140 Experiments Setting In evaluation, we compare our method with three types of models: 1) Typical Methods. Three typical knowledge graph em- bedding methods includes TransE, TransR and TransH are selected as baselines. For TransE, the dissimilarity measure is implemented with L1-norm, and relation as well as entity are replaced during negative sampling. For TransR, we di- rectly use the source codes released in (Lin et al. 2015b). In order for better performance, the replacement of relation in negative sampling is utilized according to the suggestion of author. 2) Path-based Methods.",
            "For TransR, we di- rectly use the source codes released in (Lin et al. 2015b). In order for better performance, the replacement of relation in negative sampling is utilized according to the suggestion of author. 2) Path-based Methods. We compare our method with two typical path-based model include PTransE, and ALL- PATHS (Toutanova et al. 2016). PTransE is the \ufb01rst method to model relation path in KG embedding task, and ALL- PATHS improve the PTransE through a dynamic program- ming algorithm which can incorporate all relation paths of bounded length.",
            "Table 2: Entity classi\ufb01cation results in accuracy. We run all models 10 times and report mean \u00b1 standard deviation. KANE signi\ufb01cantly outperforms baselines on FB24K, DBP24K and Game30K. Types Methods FB24K DBP24K Game30K Typical TransE + LR 0.5819 \u00b10.0015 0.6124 \u00b10.0001 0.6315 \u00b10.0018 TransR + LR 0.6012 \u00b10.0017 0.6516 \u00b10.0018 0.6731 \u00b10.0026 TransH + LR 0.6129 \u00b10.0005 0.6439 \u00b10.0054 0.6821 \u00b10.0052 Path-based PTransE + LR 0.7564 \u00b10.0031 0.8119 \u00b10.0031 0.8041 \u00b10.0000 ALL-PATHS + LR 0.7625 \u00b10.0037 0.8155 \u00b10.0041 0.8172 \u00b10.",
            "7564 \u00b10.0031 0.8119 \u00b10.0031 0.8041 \u00b10.0000 ALL-PATHS + LR 0.7625 \u00b10.0037 0.8155 \u00b10.0041 0.8172 \u00b10.0049 Attribute-incorporated KR-EAR + LR 0.7319 \u00b10.0004 0.7962 \u00b10.0005 0.8092 \u00b10.0062 R-GCN + LR 0.7721 \u00b10.0022 0.8193 \u00b10.0041 0.8229 \u00b10.0054 Our Methods KANE (BOW+Concatenation) 0.7852 \u00b10.0013 0.8205 \u00b10.0012 0.8312 \u00b10.0008 KANE (BOW+Average) 0.7745 \u00b10.0015 0.8221 \u00b10.0095 0.8293 \u00b10.0037 KANE (LSTM+Concatenation) 0.8011 \u00b10.0011 0.8592 \u00b10.0062 0.8605 \u00b10.",
            "0015 0.8221 \u00b10.0095 0.8293 \u00b10.0037 KANE (LSTM+Concatenation) 0.8011 \u00b10.0011 0.8592 \u00b10.0062 0.8605 \u00b10.0033 KANE (LSTM+Average) 0.7929 \u00b10.0018 0.8236 \u00b10.0021 0.8523 \u00b10.0031 3) Attribute-incorporated Methods. Several state-of- art attribute-incorporated methods including R-GCN (Schlichtkrull et al. 2018) and KR-EAR (Lin, Liu, and Sun 2016) are used to compare with our methods on three real datasets. In addition, four variants of KANE which each of which correspondingly de\ufb01nes its speci\ufb01c way of computing the attribute value embedding and embedding aggregation are used as baseline in evaluation. In this study, we name four three variants as KANE (BOW+Concatenation), KANE (BOW+Average), and KANE (LSTM+Concatenation), KANE (LSTM+Average).",
            "In this study, we name four three variants as KANE (BOW+Concatenation), KANE (BOW+Average), and KANE (LSTM+Concatenation), KANE (LSTM+Average). Our method is learned with mini-batch SGD. As for hyper-parameters, we select batch size among {16, 32, 64, 128}, learning rate \u03bb for SGD among {0.1, 0.01, 0.001}. For a fair comparison, we also set the vector dimensions of all entity and relation to the same k \u2208{128, 258, 512, 1024}, the same dissimilarity measure l1 or l2 distance in loss function, and the same number of negative examples n among {1, 10, 20, 40}. The training time on both data sets is limited to at most 400 epochs. The best models are selected by a grid search and early stopping on validation sets. Entity Classi\ufb01cation Evaluation Protocol. In entity classi\ufb01cation, the aim is to predicate the type of entity.",
            "The training time on both data sets is limited to at most 400 epochs. The best models are selected by a grid search and early stopping on validation sets. Entity Classi\ufb01cation Evaluation Protocol. In entity classi\ufb01cation, the aim is to predicate the type of entity. For all baseline models, we \ufb01rst get the entity embedding in different datasets through default parameter settings as in their original papers or implementa- tions.Then, Logistic Regression is used as classi\ufb01er, which regards the entity\u2019s embeddings as feature of classi\ufb01er. In evaluation, we random selected 10% of training set as vali- dation set and accuracy as evaluation metric. Test Performance. Experimental results of entity clas- si\ufb01cation on the test sets of all the datasets is shown in Table 2. The results is clearly demonstrate that our pro- posed method signi\ufb01cantly outperforms state-of-art results on accuracy for three datasets. For more in-depth perfor- mance analysis, we note: (1) Among all baselines, Path- based methods and Attribute-incorporated methods outper- form three typical methods.",
            "For more in-depth perfor- mance analysis, we note: (1) Among all baselines, Path- based methods and Attribute-incorporated methods outper- form three typical methods. This indicates that incorporating extra information can improve the knowledge graph embed- 5 10 15 20 25 30 35 40 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 KANE (LSTM+Concatenation) R-GCN KR-EAR PTransE ALL-PATHS TransE (a) DBP24K 5 10 15 20 25 30 35 40 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 KANE (LSTM+Concatenation) R-GCN KR-EAR PTransE ALL-PATHS TransE (b) Game24K Figure 3: Test accuracy with increasing epoch. ding performance; (2) Four variants of KANE always out- perform baseline methods.",
            "ding performance; (2) Four variants of KANE always out- perform baseline methods. The main reasons why KANE works well are two fold: 1) KANE can capture high-order structural information of KGs in an ef\ufb01cient, explicit manner and passe these information to their neighboring; 2) KANE leverages rich information encoded in attribute triples. These rich semantic information can further improve the perfor- mance of knowledge graph; (3) The variant of KANE that use LSTM Encoder and Concatenation aggregator outper- form other variants. The main reasons is that LSTM encoder can distinguish the word order and concatenation aggregator combine all embedding of multi-head attention in a higher leaver feature space, which can obtain suf\ufb01cient expressive power. Ef\ufb01ciency Evaluation. Figure 3 shows the test accuracy with increasing epoch on DBP24K and Game30K. We can see that test accuracy \ufb01rst rapidly increased in the \ufb01rst ten iterations, but reaches a stable stages when epoch is larger than 40. Figure 4 shows test accuracy with different embed- ding size and training data proportions.",
            "We can see that test accuracy \ufb01rst rapidly increased in the \ufb01rst ten iterations, but reaches a stable stages when epoch is larger than 40. Figure 4 shows test accuracy with different embed- ding size and training data proportions. We can note that too small embedding size or training data proportions can not generate suf\ufb01cient global information. In order to further analysis the embeddings learned by our method, we use t- SNE tool (Maaten and Hinton 2008) to visualize the learned embedding. Figure 5 shows the visualization of 256 dimen- sional entity\u2019s embedding on Game30K learned by KANE, R-GCN, PransE and TransE. We observe that our method",
            "Table 3: Results of knowledge graph completion (FB24K) Metric Entity Predication Relation Predication Hits@10 Mean Rank Hits@1 Mean Rank Raw Filter Raw Filter Raw Filter Raw Filter TransE 35.8 53.0 259 200 65.9 83.8 3.1 2.8 TransR 37.0 56.1 260 200 65.2 84.5 3.4 3.1 TransH 33.9 50.2 282 224 64.9 84.1 3.4 3.1 PTransE 37.3 56.1 249 131 67.3 86.1 2.4 2.1 ALL-PATHS 38.6 59.9 208 121 67.8 86.4 2.1 1.9 KR-EAR 38.5 54.5 186 133 67.9 86.2 2.4 2.1 R-GCN 42.3 59.1 151 119 68.8 87.2 2.1 2.",
            "9 KR-EAR 38.5 54.5 186 133 67.9 86.2 2.4 2.1 R-GCN 42.3 59.1 151 119 68.8 87.2 2.1 2.0 KANE (BOW+Concatenation) 37.4 57.4 201 123 68.9 80.1 2.2 2.1 KANE (BOW+Average) 34.3 55.8 189 131 68.1 87.3 2.4 2.2 KANE (LSTM+Concatenation) 41.5 61.2 162 103 69.4 88.1 1.9 1.8 KANE (LSTM+Average) 34.3 59.8 173 108 69.1 87.3 2.2 2.1 200 400 600 800 1000 0.700 0.725 0.750 0.775 0.800 0.825 0.850 0.",
            "8 173 108 69.1 87.3 2.2 2.1 200 400 600 800 1000 0.700 0.725 0.750 0.775 0.800 0.825 0.850 0.875 0.900 DBP24K Game30K (a) Embedding size. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.3 0.4 0.5 0.6 0.7 0.8 DBP24K Game30K (b) Training data proportions Figure 4: Test accuracy by varying parameter.",
            "0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (a) R-GCN 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (b) KANE 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (c) TransE 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (d) PransE Figure 5: The t-SNE visualization of entity embeddings in Game30K. can learn more discriminative entity\u2019s embedding than other other methods.",
            "can learn more discriminative entity\u2019s embedding than other other methods. Knowledge Graph Completion The purpose of knowledge graph completion is to complete a triple (h, r, t) when one of h, r, t is missing, which is used many literature (Bordes et al. 2013). Two measures are con- sidered as our evaluation metrics: (1) the mean rank of cor- rect entities or relations (Mean Rank); (2) the proportion of correct entities or relations ranked in top1 (Hits@1, for rela- tions) or top 10 (Hits@10, for entities). Following the setting in (Bordes et al. 2013), we also adopt the two evaluation set- tings named \u201draw\u201d and \u201d\ufb01lter\u201d in order to avoid misleading behavior. The results of entity and relation predication on FB24K are shown in the Table 3. This results indicates that KANE still outperforms other baselines signi\ufb01cantly and consis- tently. This also veri\ufb01es the necessity of modeling high- order structural and attribute information of KGs in Knowl- edge graph embedding models.",
            "This results indicates that KANE still outperforms other baselines signi\ufb01cantly and consis- tently. This also veri\ufb01es the necessity of modeling high- order structural and attribute information of KGs in Knowl- edge graph embedding models. Conclusion and Future Work Many recent works have demonstrated the bene\ufb01ts of knowl- edge graph embedding in knowledge graph completion, such as relation extraction. However, We argue that knowl- edge graph embedding method still have room for improve- ment. First, TransE and its most extensions only take direct relations between entities into consideration. Second, most existing knowledge graph embedding methods just leverage relation triples of KGs while ignoring a large number of attribute triples. In order to overcome these limitation, in- spired by the recent developments of graph convolutional networks, we propose a new knowledge graph embedding methods, named KANE. The key ideal of KANE is to ag- gregate all attribute triples with bias and perform embed- ding propagation based on relation triples when calculating the representations of given entity. Empirical results on three datasets show that KANE signi\ufb01cantly outperforms seven state-of-arts methods.",
            "References An, B.; Chen, B.; Han, X.; and Sun, L. 2018. Accurate text-enhanced knowledge graph representation learning. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics, 745\u2013755. Auer, S.; Bizer, C.; Kobilarov, G.; Lehmann, J.; Cyganiak, R.; and Ives, Z. 2007. Dbpedia: A nucleus for a web of open data. The semantic web 722\u2013735. Bollacker, K.; Evans, C.; Paritosh, P.; Sturge, T.; and Taylor, J. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Manage- ment of data, 1247\u20131250. ACM. Bordes, A.; Weston, J.; Collobert, R.; Bengio, Y.; et al. 2011. Learning structured embeddings of knowledge bases. In AAAI, volume 6, 6.",
            "ACM. Bordes, A.; Weston, J.; Collobert, R.; Bengio, Y.; et al. 2011. Learning structured embeddings of knowledge bases. In AAAI, volume 6, 6. Bordes, A.; Usunier, N.; Garcia-Duran, A.; Weston, J.; and Yakhnenko, O. 2013. Translating embeddings for model- ing multi-relational data. In Advances in neural information processing systems, 2787\u20132795. Cai, L., and Wang, W. Y. 2018. Kbgan: Adversarial learn- ing for knowledge graph embeddings. In Proceedings of the 2018 Conference of the North American Chapter of the As- sociation for Computational Linguistics, 1470\u20131480. Ding, B.; Wang, Q.; Wang, B.; and Guo, L. 2018. Improving knowledge graph embedding using simple constraints. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, 110\u2013121.",
            "Ding, B.; Wang, Q.; Wang, B.; and Guo, L. 2018. Improving knowledge graph embedding using simple constraints. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, 110\u2013121. Fatemi, B.; Ravanbakhsh, S.; and Poole, D. 2019. Improved knowledge graph embedding using background taxonomic information. In Proceedings of the AAAI Conference on Ar- ti\ufb01cial Intelligence, volume 33, 3526\u20133533. Guan, N.; Song, D.; and Liao, L. 2019. Knowledge graph embedding with concepts. Knowledge-Based Systems 164:38\u201344. Guo, S.; Wang, Q.; Wang, L.; Wang, B.; and Guo, L. 2016. Jointly embedding knowledge graphs and logical rules. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 192\u2013202. Hao, Y.; Zhang, Y.; Liu, K.; He, S.; Liu, Z.; Wu, H.; and Zhao, J. 2017.",
            "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 192\u2013202. Hao, Y.; Zhang, Y.; Liu, K.; He, S.; Liu, Z.; Wu, H.; and Zhao, J. 2017. An end-to-end model for question answering over knowledge base with cross-attention combining global knowledge. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, 221\u2013231. Ji, G.; He, S.; Xu, L.; Liu, K.; and Zhao, J. 2015. Knowl- edge graph embedding via dynamic mapping matrix. In Pro- ceedings of the 53th Annual Meeting of the Association for Computational Linguistics, 687\u2013696. Kipf, T. N., and Welling, M. 2017. Semi-supervised classi- \ufb01cation with graph convolutional networks. In International Conference on Learning Representations. Lin, Y.; Liu, Z.; Luan, H.; Sun, M.; Rao, S.; and Liu, S. 2015a. Modeling relation paths for representation learning of knowledge bases.",
            "In International Conference on Learning Representations. Lin, Y.; Liu, Z.; Luan, H.; Sun, M.; Rao, S.; and Liu, S. 2015a. Modeling relation paths for representation learning of knowledge bases. In Proceedings of Conference on Em- pirical Methods in Natural Language Processing. Lin, Y.; Liu, Z.; Sun, M.; Liu, Y.; and Zhu, X. 2015b. Learn- ing entity and relation embeddings for knowledge graph completion. In Twenty-Fourth AAAI Conference on Arti\ufb01- cial Intelligence, 2181\u20132187. Lin, Y.; Liu, Z.; and Sun, M. 2016. Knowledge representa- tion learning with entities, attributes and relations. In Pro- ceedings of the Twenty-Fifth International Joint Conference on Arti\ufb01cial Intelligence, 2866\u20132872. AAAI Press. Maaten, L. v. d., and Hinton, G. 2008. Visualizing data using t-sne. Journal of machine learning research 9(Nov):2579\u2013 2605.",
            "AAAI Press. Maaten, L. v. d., and Hinton, G. 2008. Visualizing data using t-sne. Journal of machine learning research 9(Nov):2579\u2013 2605. Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, 3111\u20133119. Schlichtkrull, M.; Kipf, T. N.; Bloem, P.; Van Den Berg, R.; Titov, I.; and Welling, M. 2018. Modeling relational data with graph convolutional networks. In European Semantic Web Conference, 593\u2013607. Springer. Suchanek, F. M.; Kasneci, G.; and Weikum, G. 2007. Yago: a core of semantic knowledge. In Proceedings of the 16th in- ternational conference on World Wide Web, 697\u2013706. ACM.",
            "Suchanek, F. M.; Kasneci, G.; and Weikum, G. 2007. Yago: a core of semantic knowledge. In Proceedings of the 16th in- ternational conference on World Wide Web, 697\u2013706. ACM. Sun, Z.; Hu, W.; and Li, C. 2017. Cross-lingual entity align- ment via joint attribute-preserving embedding. In Interna- tional Semantic Web Conference, 628\u2013644. Springer. Toutanova, K.; Lin, V.; Yih, W.-t.; Poon, H.; and Quirk, C. 2016. Compositional learning of embeddings for relation paths in knowledge base and text. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, 1434\u20131444. Veli\u02c7ckovi\u00b4c, P.; Cucurull, G.; Casanova, A.; Romero, A.; Lio, P.; and Bengio, Y. 2018. Graph attention networks. In In- ternational Conference on Learning Representations.",
            "Veli\u02c7ckovi\u00b4c, P.; Cucurull, G.; Casanova, A.; Romero, A.; Lio, P.; and Bengio, Y. 2018. Graph attention networks. In In- ternational Conference on Learning Representations. Vilnis, L.; Li, X.; Murty, S.; and McCallum, A. 2018. Prob- abilistic embedding of knowledge graphs with box lattice measures. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, 263\u2013272. Wang, Z.; Zhang, J.; Feng, J.; and Chen, Z. 2014. Knowl- edge graph embedding by translating on hyperplanes. In Twenty-Eighth AAAI Conference on Arti\ufb01cial Intelligence. Wang, H.; Zhang, F.; Wang, J.; Zhao, M.; Li, W.; Xie, X.; and Guo, M. 2018. Ripplenet: Propagating user preferences on the knowledge graph for recommender systems.",
            "Wang, H.; Zhang, F.; Wang, J.; Zhao, M.; Li, W.; Xie, X.; and Guo, M. 2018. Ripplenet: Propagating user preferences on the knowledge graph for recommender systems. In Pro- ceedings of the 27th ACM International Conference on In- formation and Knowledge Management, 417\u2013426. ACM. Xie, R.; Liu, Z.; and Sun, M. 2016. Representation learning of knowledge graphs with hierarchical types. In Proceed- ings of the Twenty-Fifth International Joint Conference on Arti\ufb01cial Intelligence, 2965\u20132971. Xiong, C.; Power, R.; and Callan, J. 2017. Explicit semantic ranking for academic search via knowledge graph embed- ding. In Proceedings of the 26th international conference on world wide web, 1271\u20131279. International World Wide Web Conferences Steering Committee."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1910.03891.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 10342.000213623047,
    "avg_doclen_est": 178.3103485107422
}
