{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "arXiv:1704.05907v1  [cs.CL]  19 Apr 2017 End-to-End Multi-View Networks for Text Classi\ufb01cation Hongyu Guo and Colin Cherry and Jiang Su National Research Council Canada 1200 Montreal Road, Ottawa, Ontario, K1A 0R6, Canada firstname.lastname@nrc-cnrc.gc.ca Abstract We propose a multi-view network for text classi\ufb01cation. Our method automatically creates various views of its input text, each taking the form of soft attention weights that distribute the classi\ufb01er\u2019s focus among a set of base features. For a bag-of-words representation, each view focuses on a dif- ferent subset of the text\u2019s words. Ag- gregating many such views results in a more discriminative and robust represen- tation. Through a novel architecture that both stacks and concatenates views, we produce a network that emphasizes both depth and width, allowing training to con- verge quickly. Using our multi-view ar- chitecture, we establish new state-of-the- art accuracies on two benchmark tasks.",
      "Through a novel architecture that both stacks and concatenates views, we produce a network that emphasizes both depth and width, allowing training to con- verge quickly. Using our multi-view ar- chitecture, we establish new state-of-the- art accuracies on two benchmark tasks. 1 Introduction State-of-the-art deep neural networks leverage task-speci\ufb01c architectures to develop hierarchi- cal representations of their input, with each layer building a re\ufb01ned abstraction of the layer that came before it (Conneau et al., 2016). For text classi\ufb01cation, one can think of this as a single reader building up an increasingly re\ufb01ned under- standing of the content. In a departure from this philosophy, we propose a divide-and-conquer ap- proach, where a team of readers each focus on dif- ferent aspects of the text, and then combine their representations to make a joint decision. More precisely, the proposed Multi-View Net- work (MVN) for text classi\ufb01cation learns to gen- erate several views of its input text.",
      "More precisely, the proposed Multi-View Net- work (MVN) for text classi\ufb01cation learns to gen- erate several views of its input text. Each view is formed by focusing on different sets of words through a view-speci\ufb01c attention mecha- nism. These views are arranged sequentially, so each subsequent view can build upon or deviate from previous views as appropriate. The \ufb01nal rep- resentation that concatenates these diverse views should be more robust to noise than any one of its components. Furthermore, different sentences may look similar under one view but different un- der another, allowing the network to devote partic- ular views to distinguishing between subtle differ- ences in sentences, resulting in more discrimina- tive representations. Unlike existing multi-view neural network ap- proaches for image processing (Zhu et al., 2014; Su et al., 2015), where multiple views are pro- vided as part of the input, our MVN learns to automatically create views from its input text by focusing on different sets of words.",
      "Com- pared to deep Convolutional Networks (CNN) for text (Zhang et al., 2015; Conneau et al., 2016), the MVN strategy emphasizes network width over depth. Shorter connections between each view and the loss function enable better gradient \ufb02ow in the networks, which makes the system eas- ier to train. Our use of multiple views is sim- ilar in spirit to the weak learners used in en- semble methods (Breiman, 1996; Friedman et al., 1998; Wolpert, 1992), but our views produce vector-valued intermediate representations instead of classi\ufb01cation scores, and all our views are trained jointly with feedback from the \ufb01nal clas- si\ufb01er.",
      "Experiments on two benchmark data sets, the Stanford Sentiment Treebank (Socher et al., 2013) and the AG English news corpus (Zhang et al., 2015), show that 1) our method achieves very competitive accuracy, 2) some views distinguish themselves from others by better categorizing spe- ci\ufb01c classes, and 3) when our base bag-of-words feature set is augmented with convolutional fea- tures, the method establishes a new state-of-the-art for both data sets.",
      "v2  v3  v1  concatenation  softmax  input text  S+  S+  S+  fully connected  v4  S+  Figure 1: A MVN architecture with four views. 2 Multi-View Networks for Text The MVN architecture is depicted in Figure 1. First, individual selection vectors s+ are created, each formed by a distinct softmax weighted sum over the word vectors of the input text. Next, these selections are sequentially transformed into views v, with each view in\ufb02uencing the views that come after it. Finally, all views are concatenated and fed into a two-layer perceptron for classi\ufb01cation. 2.1 Multiple Attentions for Selection Each selection s+ is constructed by focusing on a different subset of words from the origi- nal text, as determined by a softmax weighted sum (Bahdanau et al., 2014). Given a piece of text with H words, we represent it as a bag-of-words feature matrix B \u2208IRH\u00d7d. Each row of the matrix corresponds to one word, which is represented by a d-dimensional vector, as provided by a learned word embedding table.",
      "Given a piece of text with H words, we represent it as a bag-of-words feature matrix B \u2208IRH\u00d7d. Each row of the matrix corresponds to one word, which is represented by a d-dimensional vector, as provided by a learned word embedding table. The selection s+ i for the ith view is the softmax weighted sum of features: s+ i = H X h=1 di,hB[h : h] (1) where the weight di,h is computed by: di,h = exp(mi,h) PH h=1 exp(mi,h) (2) mi,h = ws i tanh (W s i B[h : h]) (3) here, ws i (a vector) and W s i (a matrix) are learned selection parameters. By varying the weights di,h, the selection for each view can focus on differ- ent words from B, as illustrated by different color curves connecting to s+ in Figure 1.",
      "By varying the weights di,h, the selection for each view can focus on differ- ent words from B, as illustrated by different color curves connecting to s+ in Figure 1. 2.2 Aggregating Selections into Views Having built one s+ for each of our V views, the actual views are then created as follows: v1 =s+ 1 ; vV = s+ V (4) vi = tanh(W v i ([v1; v2; ...; vi\u22121; s+ i ])) (5) for i = 2 . . . V \u22121 where W v i are learned parameter matrices, and [. . . ; . . .] represents concatenation. The \ufb01rst and last views are formed by solely s+; however, they play very different roles in our network. vV is completely disconnected from the others, an in- dependent attempt at good feature selection, in- tended to increase view diversity (Muslea et al., 2002; Guo and Viktor, 2006, 2008; Wang et al., 2015).",
      "vV is completely disconnected from the others, an in- dependent attempt at good feature selection, in- tended to increase view diversity (Muslea et al., 2002; Guo and Viktor, 2006, 2008; Wang et al., 2015). Conversely, v1 forms the base of a struc- ture similar to a multi-layer perceptron with short- cutting, as de\ufb01ned by the recurrence in Equation 5. Here, the concatenation of all previous views im- plements short-cutting, while the recursive de\ufb01ni- tion of each view implements stacking, forming a deep network depicted by horizontal arrows in Figure 1. This structure makes each view aware of the information in those previous to it, allow- ing them to build upon each other. Note that the W v matrices are view-speci\ufb01c and grow with each view, making the overall parameter count quadratic in the number of views. 2.3 Classi\ufb01cation with Views The \ufb01nal step is to transform our views into a classi\ufb01cation of the input text.",
      "2.3 Classi\ufb01cation with Views The \ufb01nal step is to transform our views into a classi\ufb01cation of the input text. The MVN does so by concatenating its view vectors, which are then fed into a fully connected projection fol- lowed by a softmax function to produce a distri- bution over the possible classes. Dropout regular- ization (Hinton et al., 2012) can be applied at this softmax layer, as in (Kim, 2014). 2.4 Beyond Bags of Words The MVN\u2019s selection layer operates on a matrix of feature vectors B, which has thus far corre- sponded to a bag of word vectors. Each view\u2019s selection makes intuitive sense when features cor- respond to words, as it is easy to imagine differ- ent readers of a text focusing on different words, with each reader arriving at a useful interpretation. However, there is a wealth of knowledge on how to construct powerful feature representations for text, such as those used by convolutional neural networks (CNNs). To demonstrate the utility of",
      "having views that weight arbitrary feature vectors, we augment our bag-of-words representation with vectors built by n-gram \ufb01lters max-pooled over the entire text (Kim, 2014), with one feature vec- tor for each n-gram order, n = 2 . . . 5. The aug- mented B matrix has H+4 rows. Unlike our word vectors, the 4 CNN vectors each provide represen- tations of the entire text. Returning to our reader analogy, one could imagine these to correspond to quick (n = 2) or careful (n = 5) skims of the text. Regardless of whether a feature vector is built by embedding table or by max-pooled n-gram \ufb01lters, we always back-propagate through all feature con- struction layers, so they become specialized to our end task. 3 Experiments 3.1 Stanford Sentiment Treebank The Stanford Sentiment Treebank contains 11,855 sentences from movie reviews.",
      "3 Experiments 3.1 Stanford Sentiment Treebank The Stanford Sentiment Treebank contains 11,855 sentences from movie reviews. We use the same splits for training, dev, and test data as in (Socher et al., 2013) to predict the \ufb01ne-grained 5-class sentiment categories of the sentences. For comparison purposes, following (Kim, 2014; Kalchbrenner et al., 2014; Lei et al., 2015), we train the models using both phrases and sentences, but only evaluate sentences at test time. We initialized all of the word embed- dings (Cherry and Guo, 2015; Cherry et al., 2015) using the publicly available 300 dimensional pre-trained vectors from GloVe (Pennington et al., 2014). We learned 8 views with 200 dimensions each, which requires us to project the 300 dimen- sional word vectors, which we implemented using a linear transformation, whose weight matrix and bias term are shared across all words, followed by a tanh activation.",
      "We learned 8 views with 200 dimensions each, which requires us to project the 300 dimen- sional word vectors, which we implemented using a linear transformation, whose weight matrix and bias term are shared across all words, followed by a tanh activation. For optimization, we used Adadelta (Zeiler, 2012), with a starting learning rate of 0.0005 and a mini-batch of size 50. Also, we used dropout (with a rate of 0.2) to avoid over\ufb01tting. All of these MVN hyperparameters were determined through experiments measuring validation-set accuracy. The test-set accuracies obtained by different learning methods, including the current state-of- the-art results, are presented in Table 1. The re- sults indicate that the bag-of-words MVN outper- forms most methods, but obtains lower accuracy than the state-of-the-art results achieved by the tree-LSTM (Tai et al., 2015; Zhu et al., 2015) and the high-order CNN (Lei et al., 2015).",
      "However, MVN (with convolutional features) 51.5 MVN 49.6 high-order CNN 51.2 tree-LSTM 51.0 DRNN 49.8 DCNN 48.5 CNN-MC 47.4 NBoW 44.5 SVM 38.3 Table 1: Accuracies on the Stanford Sentiment Treebank 5-class classi\ufb01cation task; except for the MVN, all results are drawn from (Lei et al., 2015). 49.0 49.5 50.0 50.5 51.0 51.5 Accuray Achieved (%) Number of Views Used 1 3 6 8 10 Figure 2: Accuracies obtained by varying the number of views. when augmented with 4 convolutional features as described in Section 2.4, the MVN strategy sur- passes both of these, establishing a new state-of- the-art on this benchmark. In Figure 2, we present the test-set accuracies obtained while varying the number of views in our MVN with convolutional features.",
      "In Figure 2, we present the test-set accuracies obtained while varying the number of views in our MVN with convolutional features. These re- sults indicate that better predictive accuracy can be achieved while increasing the number of views up to eight. After eight, the accuracy starts to drop. The number of MVN views should be tuned for each new application, but it is good to see that not too many views are required to achieve opti- mal performance on this task. To better understand the bene\ufb01ts of the MVN method, we further analyzed the eight views con- structed by our best model. After training, we ob- tained the view representation vectors for both the training and testing data, and then independently trained a very simple, but fast and stable Na\u00a8\u0131ve Bayes classi\ufb01er (McCallum and Nigam, 1998) for each view. We report class-speci\ufb01c F-measures for",
      "0 0.1 0.2 0.3 0.4 0.5 0.6 0 1 2 3 4 v1 v2 v3 v4 v5 v6 v7 v8 F-measure  class  category  Figure 3: Class-speci\ufb01c F-measures obtained by Na\u00a8\u0131ve Bayes classi\ufb01ers built over different views. Full MVN 51.5 Voting by 8 independent, 1-view MVNs 50.2 (weak-learner: 49.5 \u00b1 0.20) MVN w/ no horizontal links 49.0 MVN w/ length-1 horizontal links 50.5 Table 2: Ablation experiments on the Stanford Sentiment Treebank test set each view in Figure 3. From this \ufb01gure, we can ob- serve that different views focus on different target classes. For example, the \ufb01rst two views perform poorly on the 0 (very negative) and 1 (negative) classes, but achieve the highest F-measures on the 2 (neutral) class.",
      "For example, the \ufb01rst two views perform poorly on the 0 (very negative) and 1 (negative) classes, but achieve the highest F-measures on the 2 (neutral) class. Meanwhile, the non-neutral classes each have a different view that achieves the highest F-measure. This suggests that some views have specialized in order to better separate subsets of the training data. We provide an ablation study in Table 2. First, we construct a traditional ensemble model. We in- dependently train eight MVN models, each with a single view, to serve as weak learners. We have them vote with equal weight for the \ufb01nal clas- si\ufb01cation, obtaining a test-set accuracy of 50.2. Next, we restrict the views in the MVN to be un- aware of each other. That is, we replace Equa- tion 5 with vi = s+ i , which removes all horizon- tal links in Figure 1. This drops performance to 49.0.",
      "Next, we restrict the views in the MVN to be un- aware of each other. That is, we replace Equa- tion 5 with vi = s+ i , which removes all horizon- tal links in Figure 1. This drops performance to 49.0. Finally, we experiment with a variant of MVN, where each view is only connected to the most recent previous view, replacing Equation 5 with vi = tanh(W v i ([vi\u22121; s+ i ])), leading to a ver- sion where the parameter count grows linearly in the number of views. This drops the test-set per- formance to 50.5. These experiments suggest that enabling the views to build upon each other is cru- cial for achieving the best performance. MVN (with convolutional features) 7.13 MVN 7.49 n-grams TFIDF 7.64 n-grams 7.96 Lg. Lk. Convolution 8.55 29 layers Convolution with KMaxPooling 8.67 Lg.",
      "Lk. Convolution 8.55 29 layers Convolution with KMaxPooling 8.67 Lg. Full Convolution 9.85 BoW 11.19 LSTM 13.94 Bag-of-means 16.91 Table 3: Error rates on the AG News test set. All results except for the MVN are drawn from (Conneau et al., 2016) 3.2 AG\u2019s English News Categorization The AG corpus (Zhang et al., 2015; Conneau et al., 2016) contains categorized news articles from more than 2,000 news outlets on the web. The task has four classes, and for each class there are 30,000 training documents and 1,900 test documents. A random sample of the training set was used for hyper-parameter tuning. The training and testing settings of this task are exactly the same as those presented for the Stanford Sentiment Treebank task in Section 3.1, except that the mini-batch size is reduced to 23, and each view has a dimension of 100. The test errors obtained by various methods are presented in Table 3.",
      "The test errors obtained by various methods are presented in Table 3. These results show that the bag-of-words MVN outperforms the state-of-the- art accuracy obtained by the non-neural n-gram TFIDF approach (Zhang et al., 2015), as well as several very deep CNNs (Conneau et al., 2016). Accuracy was further improved when the MVN was augmented with 4 convolutional features. In Figure 4, we show how accuracy and loss evolve on the validation set during MVN train- ing. These curves show that training is quite sta- ble. The MVN achieves its best results in just a few thousand iterations. 4 Conclusion and Future Work We have presented a novel multi-view neural net- work for text classi\ufb01cation, which creates multi- ple views of the input text, each represented as a weighted sum of a base set of feature vectors. These views work together to produce a discrimi- native feature representation for text classi\ufb01cation. Unlike many neural approaches to classi\ufb01cation,",
      "Figure 4: Accuracies and cost on the validation set during training on the AG News data set. 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 82% 84% 86% 88% 90% 92% 94% 100 1500 2900 4300 5700 7100 8500 9900 11300 12700 14100 15500 16900 18300 19700 21100 accuracy loss iterations  accuracy  cost  our architecture emphasizes network width in ad- dition to depth, enhancing gradient \ufb02ow during training. We have used the multi-view network ar- chitecture to establish new state-of-the-art results on two benchmark text classi\ufb01cation tasks. In the future, we wish to better understand the bene\ufb01ts of generating multiple views, explore new sources of base features, and apply this technique to other NLP problems such as translation or tagging.",
      "In the future, we wish to better understand the bene\ufb01ts of generating multiple views, explore new sources of base features, and apply this technique to other NLP problems such as translation or tagging. References Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR abs/1409.0473. Leo Breiman. 1996. Bagging predictors. Mach. Learn. 24(2). Colin Cherry and Hongyu Guo. 2015. The unreason- able effectiveness of word representations for twit- ter named entity recognition. In HLT-NAACL. pages 735\u2013745. Colin Cherry, Hongyu Guo, and Chengbi Dai. 2015. Nrc: Infused phrase vectors for named entity recog- nition in twitter. ACL-IJCNLP 2015:54\u201360. Alexis Conneau, Holger Schwenk, Lo\u00a8\u0131c Barrault, and Yann LeCun. 2016.",
      "Nrc: Infused phrase vectors for named entity recog- nition in twitter. ACL-IJCNLP 2015:54\u201360. Alexis Conneau, Holger Schwenk, Lo\u00a8\u0131c Barrault, and Yann LeCun. 2016. Very deep convolutional net- works for natural language processing. CoRR abs/1606.01781. Jerome Friedman, Trevor Hastie, and Robert Tibshi- rani. 1998. Additive logistic regression: a statistical view of boosting. Annals of Statistics 28:2000. Hongyu Guo and Herna L Viktor. 2006. Mining rela- tional data through correlation-based multiple view validation. In KDD\u201906. ACM, pages 567\u2013573. Hongyu Guo and Herna L Viktor. 2008. Multire- lational classi\ufb01cation: a multiple view approach. Knowledge and Information Systems 17(3):287\u2013 312.",
      "In KDD\u201906. ACM, pages 567\u2013573. Hongyu Guo and Herna L Viktor. 2008. Multire- lational classi\ufb01cation: a multiple view approach. Knowledge and Information Systems 17(3):287\u2013 312. Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhut- dinov. 2012. Improving neural networks by preventing co-adaptation of feature detectors. CoRR abs/1207.0580. Nal Kalchbrenner, Edward Grefenstette, and Phil Blun- som. 2014. A convolutional neural network for modelling sentences. CoRR abs/1404.2188. Yoon Kim. 2014. Convolutional neural networks for sentence classi\ufb01cation. CoRR abs/1408.5882. Tao Lei, Regina Barzilay, and Tommi S. Jaakkola. 2015. Molding cnns for text: non-linear, non- consecutive convolutions.",
      "CoRR abs/1408.5882. Tao Lei, Regina Barzilay, and Tommi S. Jaakkola. 2015. Molding cnns for text: non-linear, non- consecutive convolutions. CoRR abs/1508.04112. Andrew McCallum and Kamal Nigam. 1998. A com- parison of event models for naive bayes text classi\ufb01- cation. AAAI Workshop . Ion Muslea, Steven Minton, and Craig A. Knoblock. 2002. Active + semi-supervised learning = robust multi-view learning. In Proceedings of the Nine- teenth International Conference on Machine Learn- ing. ICML \u201902, pages 435\u2013442. Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In EMNLP. volume 14, pages 1532\u2013 43. Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013.",
      "In EMNLP. volume 14, pages 1532\u2013 43. Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep mod- els for semantic compositionality over a sentiment treebank. In EMNLP \u201913. Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. 2015. Multi-view convolu- tional neural networks for 3d shape recognition. In Proceedings of the 2015 IEEE International Confer- ence on Computer Vision (ICCV). ICCV \u201915, pages 945\u2013953. Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved semantic representations from tree-structured long short-term memory net- works. CoRR abs/1503.00075. Weiran Wang, Raman Arora, Karen Livescu, and Jeff Bilmes. 2015. On deep multi-view representation learning. In International Conference on Machine Learning (ICML). Lille, France.",
      "CoRR abs/1503.00075. Weiran Wang, Raman Arora, Karen Livescu, and Jeff Bilmes. 2015. On deep multi-view representation learning. In International Conference on Machine Learning (ICML). Lille, France. David H. Wolpert. 1992. Stacked generalization. Neu- ral Networks 5:241\u2013259. Matthew D. Zeiler. 2012. ADADELTA: an adaptive learning rate method. CoRR abs/1212.5701.",
      "Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text clas- si\ufb01cation. In NIPS\u201915. pages 649\u2013657. Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo. 2015. Long short-term memory over recursive structures. In ICML. pages 1604\u20131612. Zhenyao Zhu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. 2014. Multi-view perceptron: a deep model for learning face identity and view representations. In NIPS. pages 217\u2013225."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1704.05907.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":4823,
  "avg_doclen":172.25,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1704.05907.pdf"
    }
  }
}