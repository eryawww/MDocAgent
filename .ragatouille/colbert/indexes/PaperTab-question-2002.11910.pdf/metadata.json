{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Integrating Boundary Assembling into a DNN Framework for Named Entity Recognition in Chinese Social Media Text Zhaoheng Gong Harvard Business School zgong@hbs.edu Ping Chen University of Massachusetts Boston Ping.Chen@umb.edu Jiang Zhou AI Strike jay.zhou@aistrike.us Abstract Named entity recognition is a challenging task in Natural Language Processing, especially for informal and noisy social media text. Chi- nese word boundaries are also entity bound- aries, therefore, named entity recognition for Chinese text can bene\ufb01t from word boundary detection, outputted by Chinese word segmen- tation. Yet Chinese word segmentation poses its own dif\ufb01culty because it is in\ufb02uenced by several factors, e.g., segmentation criteria, em- ployed algorithm, etc. Dealt improperly, it may generate a cascading failure to the qual- ity of named entity recognition followed. In this paper we integrate a boundary assembling method with the state-of-the-art deep neural network model, and incorporate the updated word boundary information into a conditional random \ufb01eld model for named entity recogni- tion.",
      "In this paper we integrate a boundary assembling method with the state-of-the-art deep neural network model, and incorporate the updated word boundary information into a conditional random \ufb01eld model for named entity recogni- tion. Our method shows a 2% absolute im- provement over previous state-of-the-art re- sults. 1 Introduction Named entity recognition (NER) is a challenging problem in Natural Language Processing, and of- ten serves as an important step for many popular applications, such as information extraction and question answering. NER requires phrases refer- ring to entities in text be identi\ufb01ed and assigned to particular entity types, thus can be naturally mod- eled as a sequence labeling task. In recent years, a lot of progress has been made on NER by ap- plying sequential models such as conditional ran- dom \ufb01eld (CRF) or neural network models such as long short-term memory (LSTM) (e.g., McCal- lum and Li, 2003; Nadeau and Sekine, 2007; Chiu and Nichols, 2016; Lample et al., 2016).",
      "Yet this task still remains a challenging one, especially in social media domain such as tweets, partially be- cause of informality and noise of such text and low frequencies of distinctive named entities (Rit- ter et al., 2011). Chinese is a language that consists of sequen- tial Chinese characters without capitalization in- formation and delimitation between words. Rather than words as in English or Romance languages, the equivalence of an English word in Chinese may contain one or several Chinese characters. Thus Chinese word segmentation is needed as a \ufb01rst step before entity mentions can be recognized. The outputs of Chinese word segmentation are of- ten used as features to support named entity recog- nition. In the neural network based models, the boundary information can be extracted from hid- den layers of a Chinese word segmentation model (e.g., Peng and Dredze, 2016; Cao et al., 2018). Relying on outputs of Chinese word segmenta- tion has its own challenge, because Chinese word segmentation is often in\ufb02uenced by the following four factors.",
      "Relying on outputs of Chinese word segmenta- tion has its own challenge, because Chinese word segmentation is often in\ufb02uenced by the following four factors. First, models trained from corpora in other languages, even if not language-speci\ufb01c such as in Lample et al. (2016), cannot be di- rectly applied to Chinese text. Some efforts have been made on Chinese word segmentation recently (Chen et al., 2015a,b; Cai and Zhao, 2016; Xu and Sun, 2016). Second, differences in segmen- tation criteria exist among labeled corpora. For instance, in the PKUs Peoples Daily corpus, a per- son\u2019s family name and given name is segmented into two tokens, while in the Penn Chinese Tree- bank corpus they are segmented into one (Chen et al., 2017). Third, for entity mentions that are compound words, models may separate them into fragmented words (Chen et al., 2015c). Many Chi- nese words are also morphemes, and separated en- tity mention fragments may be falsely combined with adjacent words.",
      "Third, for entity mentions that are compound words, models may separate them into fragmented words (Chen et al., 2015c). Many Chi- nese words are also morphemes, and separated en- tity mention fragments may be falsely combined with adjacent words. Fourth, current sequential models perform poorly in identifying named en- tities with long dependency. Yet many named en- tities expand over a long range, especially organi- zation names and location names. Errors caused by Chinese word segmentation can generate a cascading failure, which hurt the arXiv:2002.11910v1  [cs.CL]  27 Feb 2020",
      "Figure 1: The model for named entity recognition. The LSTM module is trained twice, with inputs \ufb01rst for CWS then for NER. Boundary assembling method is added in between LSTM and CRF for CWS, so that a better representation of segmentation can be obtained. Dashed-line arrows indicate parameter adjustment based on CRF\u2019s loss function between each training epoch. CRF for NER takes directly the hidden vectors in LSTM as dynamic features. Abbreviations: CRF: conditional random \ufb01eld; CWS: Chinese word segmentation; NER: named entity recognition; LSTM: long short-term memory. performance of downstream tasks such as NER. Latest developments in Chinese NER (e.g., Peng and Dredze, 2015; He and Sun, 2016; Peng and Dredze, 2016; He and Sun, 2017; Zhang and Yang, 2018; Zhao et al., 2018) have yet shown little fo- cus on this issue. Chen et al.",
      "Chen et al. (2015c) found that by assembling these single words back together, in- formation groups and sentence structure are better reserved, which could bene\ufb01t downstream tasks such as NER. Inspired by Chen et al. (2015c), we integrate in this paper a boundary assembling step into the state-of-the-art LSTM model for Chinese word segmentation, and feed the output into a CRF model for NER, resulting in a 2% absolute im- provement on the overall F1 score over current state-of-the-art methods. This paper is organized as follows. In Section 2 we discuss our model, which consists of an LSTM module for Chinese word segmentation, a bound- ary assembling step for more accurate word seg- mentation, and a CRF module for NER. We show the experiment results and discuss the model per- formance in Section 3. We conclude in Section 4. 2 Model Our model consists of three modules. A dia- gram of the model is shown in Figure 1. Char- acters in the input text for Chinese word segmen- tation are converted to vectors that are used to train the LSTM module.",
      "We conclude in Section 4. 2 Model Our model consists of three modules. A dia- gram of the model is shown in Figure 1. Char- acters in the input text for Chinese word segmen- tation are converted to vectors that are used to train the LSTM module. Output of the LSTM module are transformed by a biased-linear trans- formation to get likelihood scores of segmenta- tion labeling, then passed through the boundary assembling module. The updated boundary in- formation is used as feature input into the CRF for Chinese word segmentation (CWS), together with character-vector sequences. In each training",
      "epoch, CRF for CWS provides feedback into the LSTM hidden layer and the biased-linear transfor- mation to update the hyper-parameters. Another corpus for NER is then used to train the LSTM again, the hidden vector of which (now contains segmentation information updated by the bound- ary assembling method) is taken as feature input to CRF for NER. Lexical features extracted from the input text for NER, as well as the word embed- ding sequence, are also taken by the CRF module as input to generate NER labels. This section pro- vides descriptions for each module. 2.1 LSTM for Word Segmentation We choose an LSTM module for the CWS task. Raw input Chinese text is converted from char- acters to vectors with character-positional in- put embeddings pre-trained by Peng and Dredze (2016) over 112,971,734 Weibo messages using word2vec (Mikolov et al., 2013). Detailed pa- rameter settings can be found in Peng and Dredze (2015). The embeddings contain 52,057 unique characters in a 100-dimension space.",
      "Detailed pa- rameter settings can be found in Peng and Dredze (2015). The embeddings contain 52,057 unique characters in a 100-dimension space. The LSTM module takes these vectors into a single layer that contains 150 nodes, and modi\ufb01es them into likelihood scores for each segmentation label. A biased-linear transformation is carried out on these likelihood scores, generating predicted la- bels for segmentation. These labels are then mod- i\ufb01ed by the Boundary Assembling module, which we will discuss in detail in the next section. Be- cause labels are in sequence, and dependency may exist among adjacent labels, a transition probabil- ity matrix is introduced. The likelihood score to- gether with the transition score are taken by a CRF module with a maximum-likelihood training ob- jective. Feedbacks based on the loss function of the CRF are then given to the LSTM\u2019s hidden layer and the biased-linear transformation\u2019s parameters for update. 2.2 Boundary Assembling Method In each sentence, Chinese characters are labeled as either Begin, Inside, End, or Singleton (BIES labeling).",
      "Feedbacks based on the loss function of the CRF are then given to the LSTM\u2019s hidden layer and the biased-linear transformation\u2019s parameters for update. 2.2 Boundary Assembling Method In each sentence, Chinese characters are labeled as either Begin, Inside, End, or Singleton (BIES labeling). The likelihood of individual Chinese characters being labeled as each type is calculated by the LSTM module described in the previous section. Chen et al. (2015c) found in a Chinese corpus that the word label \u201dEnd\u201d has a better per- formance than \u201dBegin\u201d. This motivates us to carry out a backward greedy search over each sentence\u2019s label sequence to identify word boundaries. If two words segmented in a sentence are identi\ufb01ed as nouns, and one word is immediately before the other, we assemble their boundaries, creating a new word candidate for entity recognition. This strategy has the advantage to \ufb01nd named entities with long word length. It also reduces the in\ufb02u- ence caused by different segmentation criteria. 2.3 CRF for Named Entity Recognition A log-bilinear CRF module is used for the NER task, and takes three inputs.",
      "It also reduces the in\ufb02u- ence caused by different segmentation criteria. 2.3 CRF for Named Entity Recognition A log-bilinear CRF module is used for the NER task, and takes three inputs. The \ufb01rst is the sequential character-positional embeddings men- tioned above. The second is the hidden vector from LSTM as dynamic feature inputs. The third is lexical features extracted from the input text. These lexical features are the likelihood of a char- acter being at a speci\ufb01c position of a noun (\ufb01rst character of a noun, second character of a noun, etc.), and is achieved by comparing the character with a pre-de\ufb01ned dictionary trained by Peng and Dredze (2015). 3 Experiments 3.1 Datasets Datasets used in this study for training, valida- tion, and test are the same as used in Peng et al. (2016) for both word segmentation and named en- tity recognition.",
      "3 Experiments 3.1 Datasets Datasets used in this study for training, valida- tion, and test are the same as used in Peng et al. (2016) for both word segmentation and named en- tity recognition. Speci\ufb01cally, dataset for word seg- mentation is taken from the SIGHAN 2005 bake- off PKU corpus (Emerson, 2005), which includes 123,530 sentences for training and 11,697 sen- tences for testing. Dataset for named entity recog- nition is a corpus composed of 1,890 Sina Weibo (a Chinese social media website) messages, with 1,350 messages split into training set, 270 into val- idation set, and 270 into test set (Peng and Dredze, 2016). Both named entity and nominal mention are annotated, each with four entity types: person, organization, location, and geo-political entity. A major cleanup and revision of annotations of this corpus has been performed by He and Sun (2016). In this study, all results for comparisons are based on this updated corpus.",
      "A major cleanup and revision of annotations of this corpus has been performed by He and Sun (2016). In this study, all results for comparisons are based on this updated corpus. 3.2 Training Settings The weights and hyper-parameters in the LSTM module are adjusted in each iteration using stochastic gradient descent for two stages in tan- dem. The \ufb01rst stage is based on the CWS in- put text, and the second stage on NER input text. Since the corpus for CWS is much larger than",
      "Models Named Entity Nominal Mention Overall Prec Recall F1 Prec Recall F1 F1 He and Sun (2017) 61.68 48.82 54.50 74.13 53.54 62.17 58.23 Peng and Dredze (2017) 66.67 47.22 55.28 74.48 54.55 62.97 58.99 Xu et al. (2018) 59.48 54.97 57.14 72.41 53.03 61.22 59.11 Our method 71.33 47.22 56.82 73.89 58.59 65.35 61.06 Table 1: The results of two previous models, and results of this study, in which we apply a boundary assembling method. Precision, recall, and F1 scores are shown for both named entity and nominal mention. For both tasks and their overall performance, we outperform the other two models.",
      "Precision, recall, and F1 scores are shown for both named entity and nominal mention. For both tasks and their overall performance, we outperform the other two models. the corpus for NER, the former corpus is ran- domly sub-sampled during each training epoch, with each sample containing 13,500 sentences for training step one, and 1,350 sentences for training step two. Models are trained until the F1 score of NER model converges on the validation dataset, or up to 30 epochs. Models are trained on a Windows PC with a 2.7 GHz Intel Core i7 CPU. For the best performed model, the average training time for the LSTM module is 1897.6 seconds per iteration. Time for data loading, pre-processing, or model evaluation is not included. 3.3 Results and Discussion Our best model performance with its Precision, Recall, and F1 scores on named entity and nomi- nal mention are shown in Table 1. This best model performance is achieved with a dropout rate of 0.1, and a learning rate of 0.05.",
      "3.3 Results and Discussion Our best model performance with its Precision, Recall, and F1 scores on named entity and nomi- nal mention are shown in Table 1. This best model performance is achieved with a dropout rate of 0.1, and a learning rate of 0.05. Our results are com- pared with state-of-the-art models (He and Sun, 2017; Peng and Dredze, 2017; Xu et al., 2018) on the same Sina Weibo training and test datasets. Our model shows an absolute improvement of 2% for the overall F1 score. This signi\ufb01cant improvement validates our method of applying boundary assembling to the segmented sentences, which results in more ac- curate word segmentation and better semantic un- derstanding. Sentences in the PKU corpus are of- ten segmented into the smallest word units. This results in too fragmented information and incor- rect lexical units when input into a named entity recognition model, although this may bene\ufb01t some other natural language process tasks. By applying the boundary assembling method, sense group in a sentence is better preserved.",
      "This results in too fragmented information and incor- rect lexical units when input into a named entity recognition model, although this may bene\ufb01t some other natural language process tasks. By applying the boundary assembling method, sense group in a sentence is better preserved. The downstream NER model can then take advantage of this, im- proving its result. The PKU corpus used for CWS module consists of mainly news articles, which are quite different from the social media Weibo corpus. Performance of an NLP task often drops when tested on a dif- ferent domain or a corpus of different characteris- tics. Our improvement indicates that the boundary assembling method is not sensitive to the speci\ufb01c domain, and is a robust method for cross-domain scenarios. The identi\ufb01cation of two nouns that are next to each other depends on the pre-trained lexi- cal features. If our model is tested over out-of- vocabulary dataset, it may not perform well due to the lack of this lexical information. 4 Conclusion In this paper we integrate a boundary assembling step with an LSTM module and a CRF module for Named Entity Recognition in Chinese social me- dia text.",
      "If our model is tested over out-of- vocabulary dataset, it may not perform well due to the lack of this lexical information. 4 Conclusion In this paper we integrate a boundary assembling step with an LSTM module and a CRF module for Named Entity Recognition in Chinese social me- dia text. With the abundance of social media in- formation, our work is timely and desirable. The improvement in experiment results over existing methods clearly shows the effectiveness of our ap- proach. Acknowledgments This research was partially funded by the Engi- neering Directorate of the National Science Foun- dation (1820118).",
      "References Deng Cai and Hai Zhao. 2016. Neural word seg- mentation learning for chinese. arXiv preprint arXiv:1606.04300. Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, and Shengping Liu. 2018. Adversarial transfer learn- ing for chinese named entity recognition with self- attention mechanism. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing, pages 182\u2013192. Xinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xuanjing Huang. 2015a. Gated recursive neural network for chinese word segmentation. In Proceedings of the 53rd Annual Meeting of the Association for Compu- tational Linguistics and the 7th International Joint Conference on Natural Language Processing (Vol- ume 1: Long Papers), volume 1, pages 1744\u20131753. Xinchi Chen, Xipeng Qiu, Chenxi Zhu, Pengfei Liu, and Xuanjing Huang. 2015b. Long short-term mem- ory neural networks for chinese word segmentation.",
      "Xinchi Chen, Xipeng Qiu, Chenxi Zhu, Pengfei Liu, and Xuanjing Huang. 2015b. Long short-term mem- ory neural networks for chinese word segmentation. In Proceedings of the 2015 Conference on Empiri- cal Methods in Natural Language Processing, pages 1197\u20131206. Xinchi Chen, Zhan Shi, Xipeng Qiu, and Xuanjing Huang. 2017. Adversarial multi-criteria learning for chinese word segmentation. arXiv preprint arXiv:1704.07556. Yanping Chen, Qinghua Zheng, and Ping Chen. 2015c. A boundary assembling method for chinese entity- mention recognition. IEEE Intelligent Systems, 30(6):50\u201358. Jason PC Chiu and Eric Nichols. 2016. Named entity recognition with bidirectional lstm-cnns. Transac- tions of the Association for Computational Linguis- tics, 4:357\u2013370. Thomas Emerson. 2005. The second international chi- nese word segmentation bakeoff.",
      "2016. Named entity recognition with bidirectional lstm-cnns. Transac- tions of the Association for Computational Linguis- tics, 4:357\u2013370. Thomas Emerson. 2005. The second international chi- nese word segmentation bakeoff. In Proceedings of the fourth SIGHAN workshop on Chinese language Processing. Hangfeng He and Xu Sun. 2016. F-score driven max margin neural network for named entity recognition in chinese social media. CoRR, abs/1611.04234. Hangfeng He and Xu Sun. 2017. A uni\ufb01ed model for cross-domain and semi-supervised named entity recognition in chinese social media. In Thirty-First AAAI Conference on Arti\ufb01cial Intelligence. Guillaume Lample, Miguel Ballesteros, Sandeep Sub- ramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. arXiv preprint arXiv:1603.01360. Andrew McCallum and Wei Li. 2003.",
      "2016. Neural architectures for named entity recognition. arXiv preprint arXiv:1603.01360. Andrew McCallum and Wei Li. 2003. Early results for named entity recognition with conditional random \ufb01elds, feature induction and web-enhanced lexicons. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4, pages 188\u2013191. Association for Computational Lin- guistics. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositional- ity. In Advances in neural information processing systems, pages 3111\u20133119. David Nadeau and Satoshi Sekine. 2007. A sur- vey of named entity recognition and classi\ufb01cation. Lingvisticae Investigationes, 30(1):3\u201326. Nanyun Peng and Mark Dredze. 2015. Named en- tity recognition for chinese social media with jointly trained embeddings.",
      "Lingvisticae Investigationes, 30(1):3\u201326. Nanyun Peng and Mark Dredze. 2015. Named en- tity recognition for chinese social media with jointly trained embeddings. In Processings of the Confer- ence on Empirical Methods in Natural Language Processing (EMNLP), pages 548\u2013554. Nanyun Peng and Mark Dredze. 2016. Improving named entity recognition for chinese social media with word segmentation representation learning. In Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (ACL), vol- ume 2, pages 149\u2013155. Nanyun Peng and Mark Dredze. 2017. Supplementary results for named entity recognition on chinese so- cial media with an updated dataset. Computing Re- search Repository, arXiv:1603.00786. Version 2. Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011. Named entity recognition in tweets: an experimental study.",
      "Computing Re- search Repository, arXiv:1603.00786. Version 2. Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011. Named entity recognition in tweets: an experimental study. In Proceedings of the conference on empiri- cal methods in natural language processing, pages 1524\u20131534. Association for Computational Linguis- tics. Jingjing Xu, Hangfeng He, Xu Sun, Xuancheng Ren, and Sujian Li. 2018. Cross-domain and semisuper- vised named entity recognition in chinese social me- dia: A uni\ufb01ed model. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 26:2142\u2013 2152. Jingjing Xu and Xu Sun. 2016. Dependency-based gated recursive neural network for chinese word seg- mentation. In Proceedings of the 54th Annual Meet- ing of the Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pages 567\u2013572. Yue Zhang and Jie Yang. 2018.",
      "In Proceedings of the 54th Annual Meet- ing of the Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pages 567\u2013572. Yue Zhang and Jie Yang. 2018. Chinese ner using lat- tice lstm. In ACL. Dongyang Zhao, Jiuming Huang, and Yan Jia. 2018. Chinese name entity recognition using highway- lstm-crf. In ACAI 2018."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-2002.11910.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":4654,
  "avg_doclen":179.0,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-2002.11910.pdf"
    }
  }
}