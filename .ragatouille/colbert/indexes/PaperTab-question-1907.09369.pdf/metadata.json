{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Emotion Detection in Text: Focusing on Latent Representation Armin Seyeditabari UNC Charlotte sseyedi1@uncc.edu Narges Tabari University of Virginia ns5kn@virginia.edu She\ufb01e Gholizadeh UNC Charlotte sgholiza@uncc.eduu Wlodek Zadrozny UNC Charlotte wzadrozn@uncc.edu Abstract In recent years, emotion detection in text has become more popular due to its vast potential applications in marketing, political science, psychology, human- computer interaction, arti\ufb01cial intelligence, etc. In this work, we argue that current methods which are based on conventional machine learning models cannot grasp the intricacy of emotional language by ignoring the se- quential nature of the text, and the context. These meth- ods, therefore, are not suf\ufb01cient to create an applica- ble and generalizable emotion detection methodology. Understanding these limitations, we present a new net- work based on a bidirectional GRU model to show that capturing more meaningful information from text can signi\ufb01cantly improve the performance of these models.",
      "Understanding these limitations, we present a new net- work based on a bidirectional GRU model to show that capturing more meaningful information from text can signi\ufb01cantly improve the performance of these models. The results show signi\ufb01cant improvement with an aver- age of 26.8 point increase in F-measure on our test data and 38.6 increase on the totally new dataset. Introduction There have been many advances in machine learning meth- ods which help machines understand human behavior bet- ter than ever. One of the most important aspects of human behavior is emotion. If machines could detect human emo- tional expressions, it could be used to improve on verity of applications such as marketing (Bagozzi, Gopinath, and Nyer 1999), human-computer interactions (Brave and Nass 2003), political science (Druckman and McDermott 2008) etc. Emotion in humans is complex and hard to distinguish.",
      "Emotion in humans is complex and hard to distinguish. There have been many emotional models in psychology which tried to classify and point out basic human emotions such as Ekman\u2019s 6 basic emotions (Ekman 1992), Plutchik\u2019s wheel of emotions (Plutchik 1991), or Parrott\u2019s three-level categorization of emotions (Parrott 2001). These varieties show that emotions are hard to de\ufb01ne, distinguish, and cate- gorize even for human experts. By adding the complexity of language and the fact that emotion expressions are very complex and context depen- dant (Ben-Ze\u2019ev 2000; Bazzanella 2004; Oatley, Keltner, and Jenkins 2006), we can see why detecting emotions in textual data is a challenging task. This dif\ufb01culty can be seen Copyright c\u20dd2022, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved.",
      "This dif\ufb01culty can be seen Copyright c\u20dd2022, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved. when human annotators try to assign emotional labels to the text, but using various techniques the annotation task can be accomplished with desirable agreement among the annota- tors (Tafreshi and Diab 2018). Related Work A lot of work has been done on detecting emotion in speech or visual data (Han, Yu, and Tashev 2014; Lee and Ta- shev 2015; Wang et al. 2018; Zhang et al. 2016). But de- tecting emotions in textual data is a relatively new area that demands more research.",
      "2018; Zhang et al. 2016). But de- tecting emotions in textual data is a relatively new area that demands more research. There have been many at- tempts to detect emotions in text using conventional ma- chine learning techniques and handcrafted features in which given the dataset, the authors try to \ufb01nd the best feature set that represents the most and the best information about the text, then passing the converted text as feature vectors to the classi\ufb01er for training (Suttles and Ide 2013; Purver and Battersby 2012; Mohammad 2012; Daum\u00b4e III 2009; Roberts et al. 2012; Hasan, Rundensteiner, and Agu 2014; Hasan, Rundensteiner, and Agu 2018; Wang et al. 2012; Balabantaray, Mohammad, and Sharma 2012; Wen and Wan 2014; Li and Xu 2014; Li et al. 2015; Seyeditabari et al. 2018).",
      "2012; Balabantaray, Mohammad, and Sharma 2012; Wen and Wan 2014; Li and Xu 2014; Li et al. 2015; Seyeditabari et al. 2018). During the process of creating the feature set, in these methods, some of the most important information in the text such as the sequential nature of the data, and the context will be lost. Considering the complexity of the task, and the fact that these models lose a lot of information by using simpler mod- els such as the bag of words model (BOW) or lexicon fea- tures, these attempts lead to methods which are not reusable and generalizable. Further improvement in classi\ufb01cation al- gorithms, and trying out new paths is necessary in order to improve the performance of emotion detection methods. Some suggestions that were less present in the literature, are to develop methods that go above lexical representations and consider the \ufb02ow of the language.",
      "Further improvement in classi\ufb01cation al- gorithms, and trying out new paths is necessary in order to improve the performance of emotion detection methods. Some suggestions that were less present in the literature, are to develop methods that go above lexical representations and consider the \ufb02ow of the language. Due to this sequential nature, recurrent and convolutional neural networks have been used in many NLP tasks and were able to improve the performance in a variety of classi\ufb01ca- tion tasks (Lai et al. 2015; Zhang, Zhao, and LeCun 2015; Zhou et al. 2015; Lee and Dernoncourt 2016). There have been very few works in using deep neural network for emotion detection in text (Abdul-Mageed and Ungar 2017; Mundra et al. 2017). These models can capture the complex- arXiv:1907.09369v1  [cs.CL]  22 Jul 2019",
      "ity an context of the language better not only by keeping the sequential information but also by creating hidden rep- resentation for the text as a whole and by learning the impor- tant features without any additional (and often incomplete) human-designed features. In this work, we argue that creating a model that can bet- ter capture the context and sequential nature of text , can signi\ufb01cantly improve the performance in the hard task of emotion detection. We show this by using a recurrent neural network-based classi\ufb01er that can learn to create a more in- formative latent representation of the target text as a whole, and we show that this can improve the \ufb01nal performance sig- ni\ufb01cantly. Based on that, we suggest focusing on methodolo- gies that increase the quality of these latent representations both contextually and emotionally, can improve the perfor- mance of these models. Based on this assumption we pro- pose a deep recurrent neural network architecture to detect discrete emotions in a tweet dataset. The code can be ac- cessed at GitHub [https://github.com/armintabari/Emotion- Detection-RNN].",
      "Based on this assumption we pro- pose a deep recurrent neural network architecture to detect discrete emotions in a tweet dataset. The code can be ac- cessed at GitHub [https://github.com/armintabari/Emotion- Detection-RNN]. Experiment Baseline Approaches We compare our approach to two other, the \ufb01rst one uses almost the same tweet data as we use for training, and the second one is the CrowdFlower dataset annotated for emo- tions. In the \ufb01rst one Wang et al. (Wang et al. 2012) down- loaded over 5M tweets which included one of 131 emo- tional hashtags based on Parrott\u2019s three-level categorization of emotions in seven categories: joy, sadness, anger, love, fear, thankfulness, surprise. To assess the quality of using hashtags as labels, the sampled 400 tweets randomly and after comparing human annotations by hashtag labels they came up with simple heuristics to increase the quality of labeling by ignoring tweets with quotations and URLs and only keeping tweets with 5 terms or more that have the emo- tional hashtags at the end of the tweets. Using these rules they extracted around 2.5M tweets.",
      "Using these rules they extracted around 2.5M tweets. After sampling another 400 random tweets and comparing it to human annotation the saw that hashtags can classify the tweets with 95% pre- cision. They did some pre-processing by making all words lower-case, replaced user mentions with @user, replaced let- ters/punctuation that is repeated more than twice with the same two letters/punctuation (e.g., ooooh \u2192ooh, !!!!! \u2192 !!); normalized some frequently used informal expressions (e.g., ll will, dnt \u2192 do not); and stripped hash symbols. They used a sub-sample of their dataset to \ufb01gure out the best approaches for classi\ufb01cation, and after trying two differ- ent classi\ufb01ers (multinomial Naive Bayes and LIBLINEAR) and 12 different feature sets, they got their best results using logistic regression branch for LIBLINEAR classi\ufb01er and a feature set consist of n-gram(n=1,2), LIWC and MPQA lex- icons, WordNet-Affect and POS tags.",
      "In the second one, the reported results are from a paper by (Bostan and Klinger 2018) in which they used maximum en- tropy classi\ufb01er with bag of words model to classify various emotional datasets. Here we only report part of their result for CrowdFlower dataset that can be mapped to one of our seven labels. Data and preparation There are not many free datasets available for emotion clas- si\ufb01cation. Most datasets are subject-speci\ufb01c (i.e. news head- lines, fairy tails, etc.) and not big enough to train deep neural networks. Here we use the tweet dataset created by Wang et al. As mentioned in the previous section, they have collected over 2 million tweets by using hashtags for labeling their data. They created a list of words associated with 7 emo- tions (six emotions from (Shaver et al. 1987) love, joy, sur- prise, anger, sadness fear plus thankfulness (See Table 1), and used the list as their guide to label the sampled tweets with acceptable quality.",
      "1987) love, joy, sur- prise, anger, sadness fear plus thankfulness (See Table 1), and used the list as their guide to label the sampled tweets with acceptable quality. Emotion Hashtags Number of Tweets joy 36 706,182 sadness 36 616,471 anger 23 574,170 love 7 301,759 fear 22 135,154 thankfulness 2 131,340 surprise 5 23,906 Total 131 2,488,982 Table 1: Statistics in the original dataset from Wang et al. After pre-processing, they have used 250k tweets as the test set, around 250k as development test and the rest of the data (around 2M) as training data. their best results us- ing LIBLINEAR classi\ufb01er and a feature set containing n- gram(n=1,2), LIWC and MPQA lexicons, WordNet-Affect and POS tags can be seen in Table 2.",
      "their best results us- ing LIBLINEAR classi\ufb01er and a feature set containing n- gram(n=1,2), LIWC and MPQA lexicons, WordNet-Affect and POS tags can be seen in Table 2. It can be seen that their best results were for high count emotions like joy and sad- ness as high as 72.1 in F-measure and worst result was for a low count emotion surprise with F-measure of 13.9. Emotion F-measure% joy (28.5%) 72.1 sadness (24.6%) 64.7 anger (23.0%) 71.5 love (12.1%) 51.5 fear (5.6%) 43.9 thankfulness (5.3%) 57.1 surprise (1.0%) 13.9 Table 2: Results of \ufb01nal classi\ufb01cation in Wang et al. As Twitter is against polishing this many tweets, Wang et al. provided the tweet ids along with their label. For our experiment, we retrieved the tweets in Wang et al.\u2019s dataset by tweet IDs.",
      "As Twitter is against polishing this many tweets, Wang et al. provided the tweet ids along with their label. For our experiment, we retrieved the tweets in Wang et al.\u2019s dataset by tweet IDs. As the dataset is from 7 years ago We could only download over 1.3 million tweets from around 2.5M tweet IDs in the dataset. The distribution of the data can be seen in Table 3. In our experiment, we used simpler pre-processing steps which will be explained later on in the \u201dExperiment\u201d sec- tion.",
      "Figure 1: Bidirectional GRU architecture used in our experiment. Emotion Number of Tweets joy 393,631 sadness 338,015 anger 298,480 love 169,267 fear 73,575 thankfulness 79,341 surprise 13,535 Total 1,387,787 Table 3: Statistics in the downloaded dataset from Wang et al (2012). This is the main dataset used for training the model. Model In this section, we introduce the deep neural network ar- chitecture that we used to classify emotions in the tweets dataset. Emotional expressions are more complex and context-dependent even compared to other forms of expres- sions based mostly on the complexity and ambiguity of hu- man emotions and emotional expressions and the huge im- pact of context on the understanding of the expressed emo- tion. These complexities are what led us to believe lexicon- based features like is normally used in conventional machine learning approaches are unable to capture the intricacy of emotional expressions. Our architecture was designed to show that using a model that captures better information about the context and se- quential nature of the text can outperform lexicon-based methods commonly used in the literature.",
      "Our architecture was designed to show that using a model that captures better information about the context and se- quential nature of the text can outperform lexicon-based methods commonly used in the literature. As mentioned in the Introduction, Recurrent Neural Networks (RNNs) have been shown to perform well for the verity of tasks in NLP, especially classi\ufb01cation tasks. And as our goal was to cap- ture more information about the context and sequential na- ture of the text, we decided to use a model based on bidi- rectional RNN, speci\ufb01cally a bidirectional GRU network to analyze the tweets. For building the emotion classi\ufb01er, we have decided to use 7 binary classi\ufb01ers-one for each emotion- each of which uses the same architecture for detecting a speci\ufb01c emotion. You can see the plot diagram of the model in Figure 1. The \ufb01rst layer consists of an embedding lookup layer that will not change during training and will be used to convert each term to its corresponding embedding vector. In our experiments, we tried various word embedding models but saw little dif- ference in their performance.",
      "The \ufb01rst layer consists of an embedding lookup layer that will not change during training and will be used to convert each term to its corresponding embedding vector. In our experiments, we tried various word embedding models but saw little dif- ference in their performance. Here we report the results for two which had the best performance among all, ConceptNet Numberbatch (Speer, Chin, and Havasi 2017) and fastText (Mikolov et al. 2018) both had 300 dimensions. As none of our tweets had more than 35 terms, we set the size of the embedding layer to 35 and added padding to shorter tweets. The output of this layer goes to a bidi- rectional GRU layer selected to capture the entirety of each tweet before passing its output forward. The goal is to cre- ate an intermediate representation for the tweets that capture the sequential nature of the data. For the next step, we use a concatenation of global max-pooling and average-pooling layers (with a window size of two).",
      "The goal is to cre- ate an intermediate representation for the tweets that capture the sequential nature of the data. For the next step, we use a concatenation of global max-pooling and average-pooling layers (with a window size of two). Then a max-pooling was used to extract the most important features form the GRU output and an average-pooling layer was used to considers all features to create a representation for the text as a whole.",
      "These partial representations are then were concatenated to create out \ufb01nal hidden representation. For classi\ufb01cation, the output of the concatenation is passed to a dense classi\ufb01ca- tion layer with 70 nodes along with a dropout layer with a rate of 50% to prevent over-\ufb01tting. The \ufb01nal layer is a sig- moid layer that generates the \ufb01nal output of the classi\ufb01er returning the class probability. Experiment Minimal pre-processing was done by converting text to lower case, removing the hashtags at the end of tweets and separating each punctuation from the connected token (e.g., awesome!! \u2192awesome !!) and replacing comma and new- line characters with white space. The text, then, was tok- enized using TensorFlow-Keras tokenizer. Top N terms were selected and added to our dictionary where N=100k for higher count emotions joy, sadness, anger, love and N=50k for thankfulness and fear and N=25k for surprise. Seven binary classi\ufb01ers were trained for the seven emotions with a batch size of 250 and for 20 epochs with binary cross- entropy as the objective function and Adam optimizer.",
      "Seven binary classi\ufb01ers were trained for the seven emotions with a batch size of 250 and for 20 epochs with binary cross- entropy as the objective function and Adam optimizer. The architecture of the model can be seen in Figure 1. For train- ing each classi\ufb01er, a balanced dataset was created with se- lecting all tweets from the target set as class 1 and a random sample of the same size from other classes as class 0. For each classi\ufb01er, 80% of the data was randomly selected as the training set, and 10% for the validation set, and 10% as the test set. As mentioned before we used the two embed- ding models, ConceptNet Numberbatch and fastText as the two more modern pre-trained word vector spaces to see how changing the embedding layer can affect the performance. The result of comparison among different embeddings can be seen in Table 5. It can be seen that the best performance was divided between the two embedding models with minor performance variations. The comparison of our result with Wang et al.",
      "The result of comparison among different embeddings can be seen in Table 5. It can be seen that the best performance was divided between the two embedding models with minor performance variations. The comparison of our result with Wang et al. can be seen in Table 4. as shown, the results from our model shows sig- ni\ufb01cant improvement from 10% increase in F-measure for a high count emotion joy up to 61.7 point increase in F- measure for a low count emotion surprise. on average we showed 26.8 point increase in F-measure for all categories and more interestingly our result shows very little variance between different emotions compare to results reported by Wang et al. Emotion Wang et al%.",
      "on average we showed 26.8 point increase in F-measure for all categories and more interestingly our result shows very little variance between different emotions compare to results reported by Wang et al. Emotion Wang et al%. Ours% Difference% joy 72.1 82.1 10.0 sadness 64.7 79.2 14.5 anger 71.5 83.7 12.2 love 51.5 80.3 28.8 fear 43.9 78.1 34.2 thankfulness 57.1 83.6 26.5 surprise 13.9 75.6 61.7 Average 53.5 80.4 26.8 Table 4: Results of classi\ufb01cation using bidirectional GRU. Reported numbers are F1-measures.",
      "Reported numbers are F1-measures. Emotion Numberbatch fastText joy 82.11 81.90 sadness 79.17 78.71 anger 83.44 83.74 love 79.83 80.29 fear 77.61 78.11 thankfulness 83.64 83.58 surprise 75.40 75.58 Table 5: Results of classi\ufb01cation using two embedding mod- els and bidirectional GRU. No meaningful differences was seen between the two models. Reported numbers are F1- measures. Model Performances on New Dataset To asses the performance of these models on a totally unseen data, we tried to classify the CrowdFlower emotional tweets dataset. The CrowdFlower dataset consists of 40k tweets an- notated via crowd-sourcing each with a single emotional la- bel. This dataset is considered a hard dataset to classify with a lot of noise. The distribution of the dataset can be seen in Table 6.",
      "The CrowdFlower dataset consists of 40k tweets an- notated via crowd-sourcing each with a single emotional la- bel. This dataset is considered a hard dataset to classify with a lot of noise. The distribution of the dataset can be seen in Table 6. The labeling on this dataset is non-standard, so we used the following mapping for labels: \u2022 sadness \u2192sadness \u2022 worry \u2192fear \u2022 happiness \u2192joy \u2022 love \u2192love \u2022 surprise \u2192surprise \u2022 anger \u2192anger We then classi\ufb01ed emotions using the pre-trained models and emotionally \ufb01tted fastText embedding. The result can be seen in Table 7. The baseline results are from (Bostan and Klinger 2018) done using BOW model and maximum en- tropy classi\ufb01er. We saw a huge improvement from 26 point increase in F-measure for the emotion joy (happiness) up to 57 point increase for surprise with total average increase of 38.6 points. Bostan and Klinger did not report classi\ufb01ca- tion results for the emotion love so we did not include it in the average.",
      "Bostan and Klinger did not report classi\ufb01ca- tion results for the emotion love so we did not include it in the average. These results show that our trained models per- form exceptionally on a totally new dataset with a different method of annotation. Conclusion and Future Work In this paper, we have shown that using the designed RNN based network we could increase the performance of clas- si\ufb01cation dramatically. We showed that keeping the sequen- tial nature of the data can be hugely bene\ufb01cial when work- ing with textual data especially faced with the hard task of detecting more complex phenomena like emotions. We ac- complished that by using a recurrent network in the process of generating our hidden representation. We have also used a max-pooling layer to capture the most relevant features and an average pooling layer to capture the text as a whole prov- ing that we can achieve better performance by focusing on creating a more informative hidden representation. In future",
      "Emotion Number of Tweets neutral 8638 worry 8459 happiness 5209 sadness 5165 love 3842 surprise 2187 fun 1776 relief 1526 hate 1323 enthusiasm 759 boredom 179 anger 110 empty 827 Total 40000 Table 6: Distribution of labels in CrowdFlower dataset. Emotion Baseline% Our Model% Difference joy (happiness) 38 64 26 sadness 27 65 38 anger 24 62 38 love - 66 - fear (worry) 31 65 34 surprise 9 66 57 Average 25.8 63.2 38.6 Table 7: Results from classifying CrowdFlower data using pre-trained model. Reported numbers are F1-measure. we can focus on improving these representations for exam- ple by using attention networks (Bahdanau, Cho, and Bengio 2014; Yang et al. 2016) to capture a more contextual repre- sentation or using language model based methods like BERT (Devlin et al. 2018) that has been shown very successful in various NLP tasks.",
      "2016) to capture a more contextual repre- sentation or using language model based methods like BERT (Devlin et al. 2018) that has been shown very successful in various NLP tasks. References [Abdul-Mageed and Ungar 2017] Abdul-Mageed, M., and Ungar, L. 2017. Emonet: Fine-grained emotion detection with gated recurrent neural networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 718\u2013728. [Bagozzi, Gopinath, and Nyer 1999] Bagozzi, R. P.; Gopinath, M.; and Nyer, P. U. 1999. The role of emotions in marketing. Journal of the academy of marketing science 27(2):184\u2013206. [Bahdanau, Cho, and Bengio 2014] Bahdanau, D.; Cho, K.; and Bengio, Y. 2014. Neural machine translation by jointly learning to align and translate.",
      "[Bahdanau, Cho, and Bengio 2014] Bahdanau, D.; Cho, K.; and Bengio, Y. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473. [Balabantaray, Mohammad, and Sharma 2012] Balabantaray, R. C.; Mohammad, M.; and Sharma, N. 2012. Multi-class twitter emotion classi\ufb01cation: A new approach. International Journal of Applied Information Systems 4(1):48\u201353. [Bazzanella 2004] Bazzanella, C. 2004. Emotions, language and context. Emotion in dialogic interaction: Advances in the Complex 55\u201372. [Ben-Ze\u2019ev 2000] Ben-Ze\u2019ev, A. 2000. The subtlety of emo- tions. MIT Press. [Bostan and Klinger 2018] Bostan, L. A. M., and Klinger, R. 2018.",
      "2000. The subtlety of emo- tions. MIT Press. [Bostan and Klinger 2018] Bostan, L. A. M., and Klinger, R. 2018. An analysis of annotated corpora for emotion clas- si\ufb01cation in text. In Proceedings of the 27th International Conference on Computational Linguistics, 2104\u20132119. [Brave and Nass 2003] Brave, S., and Nass, C. 2003. Emo- tion in human\u2013computer interaction. Human-Computer In- teraction 53. [Daum\u00b4e III 2009] Daum\u00b4e III, H. 2009. Frustratingly easy domain adaptation. arXiv preprint arXiv:0907.1815. [Devlin et al. 2018] Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.",
      "2018] Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. [Druckman and McDermott 2008] Druckman, J. N., and McDermott, R. 2008. Emotion and the framing of risky choice. Political Behavior 30(3):297\u2013321. [Ekman 1992] Ekman, P. 1992. An argument for basic emo- tions. Cognition & emotion 6(3-4):169\u2013200. [Han, Yu, and Tashev 2014] Han, K.; Yu, D.; and Tashev, I. 2014. Speech emotion recognition using deep neural net- work and extreme learning machine. In Fifteenth annual conference of the international speech communication asso- ciation. [Hasan, Rundensteiner, and Agu 2014] Hasan, M.; Runden- steiner, E.; and Agu, E.",
      "In Fifteenth annual conference of the international speech communication asso- ciation. [Hasan, Rundensteiner, and Agu 2014] Hasan, M.; Runden- steiner, E.; and Agu, E. 2014. Emotex: Detecting emotions in twitter messages. [Hasan, Rundensteiner, and Agu 2018] Hasan, M.; Runden- steiner, E.; and Agu, E. 2018. Automatic emotion detection in text streams by analyzing twitter data. International Jour- nal of Data Science and Analytics. [Lai et al. 2015] Lai, S.; Xu, L.; Liu, K.; and Zhao, J. 2015. Recurrent convolutional neural networks for text classi\ufb01ca- tion. In Twenty-ninth AAAI conference on arti\ufb01cial intelli- gence. [Lee and Dernoncourt 2016] Lee, J. Y., and Dernoncourt, F. 2016. Sequential short-text classi\ufb01cation with recur- rent and convolutional neural networks.",
      "[Lee and Dernoncourt 2016] Lee, J. Y., and Dernoncourt, F. 2016. Sequential short-text classi\ufb01cation with recur- rent and convolutional neural networks. arXiv preprint arXiv:1603.03827. [Lee and Tashev 2015] Lee, J., and Tashev, I. 2015. High- level feature representation using recurrent neural network for speech emotion recognition. In Sixteenth Annual Con- ference of the International Speech Communication Associ- ation. [Li and Xu 2014] Li, W., and Xu, H. 2014. Text-based emo- tion classi\ufb01cation using emotion cause extraction. Expert Systems with Applications 41(4):1742\u20131749. [Li et al. 2015] Li, S.; Huang, L.; Wang, R.; and Zhou, G. 2015. Sentence-level emotion classi\ufb01cation with label and context dependence. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Lan-",
      "guage Processing (Volume 1: Long Papers), volume 1, 1045\u20131053. [Mikolov et al. 2018] Mikolov, T.; Grave, E.; Bojanowski, P.; Puhrsch, C.; and Joulin, A. 2018. Advances in pre-training distributed word representations. In Proceedings of the In- ternational Conference on Language Resources and Evalu- ation (LREC 2018). [Mohammad 2012] Mohammad, S. M. 2012. # emotional tweets. In Proceedings of the First Joint Conference on Lexi- cal and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Pro- ceedings of the Sixth International Workshop on Semantic Evaluation, 246\u2013255. Association for Computational Lin- guistics. [Mundra et al. 2017] Mundra, S.; Sen, A.; Sinha, M.; Man- narswamy, S.; Dandapat, S.; and Roy, S. 2017. Fine-grained emotion detection in contact center chat utterances.",
      "[Mundra et al. 2017] Mundra, S.; Sen, A.; Sinha, M.; Man- narswamy, S.; Dandapat, S.; and Roy, S. 2017. Fine-grained emotion detection in contact center chat utterances. In Paci\ufb01c-Asia Conference on Knowledge Discovery and Data Mining, 337\u2013349. Springer. [Oatley, Keltner, and Jenkins 2006] Oatley, K.; Keltner, D.; and Jenkins, J. M. 2006. Understanding emotions. Black- well publishing. [Parrott 2001] Parrott, W. G. 2001. Emotions in social psy- chology: Essential readings. Psychology Press. [Plutchik 1991] Plutchik, R. 1991. The emotions. University Press of America. [Purver and Battersby 2012] Purver, M., and Battersby, S. 2012. Experimenting with distant supervision for emotion classi\ufb01cation.",
      "1991. The emotions. University Press of America. [Purver and Battersby 2012] Purver, M., and Battersby, S. 2012. Experimenting with distant supervision for emotion classi\ufb01cation. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, 482\u2013491. Association for Computational Lin- guistics. [Roberts et al. 2012] Roberts, K.; Roach, M. A.; Johnson, J.; Guthrie, J.; and Harabagiu, S. M. 2012. Empatweet: An- notating and detecting emotions on twitter. In LREC, vol- ume 12, 3806\u20133813. Citeseer. [Seyeditabari et al. 2018] Seyeditabari, A.; Levens, S.; Maestas, C. D.; Shaikh, S.; Walsh, J. I.; Zadrozny, W.; Danis, C.; and Thompson, O. P. 2018. Cross corpus emotion classi\ufb01cation using survey data.",
      "2018. Cross corpus emotion classi\ufb01cation using survey data. [Shaver et al. 1987] Shaver, P.; Schwartz, J.; Kirson, D.; and O\u2019connor, C. 1987. Emotion knowledge: Further explo- ration of a prototype approach. Journal of personality and social psychology 52(6):1061. [Speer, Chin, and Havasi 2017] Speer, R.; Chin, J.; and Havasi, C. 2017. ConceptNet 5.5: An open multilingual graph of general knowledge. 4444\u20134451. [Suttles and Ide 2013] Suttles, J., and Ide, N. 2013. Distant supervision for emotion classi\ufb01cation with discrete binary values. In International Conference on Intelligent Text Pro- cessing and Computational Linguistics, 121\u2013136. Springer. [Tafreshi and Diab 2018] Tafreshi, S., and Diab, M. 2018.",
      "In International Conference on Intelligent Text Pro- cessing and Computational Linguistics, 121\u2013136. Springer. [Tafreshi and Diab 2018] Tafreshi, S., and Diab, M. 2018. Sentence and clause level emotion annotation, detection, and classi\ufb01cation in a multi-genre corpus. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC-2018). [Wang et al. 2012] Wang, W.; Chen, L.; Thirunarayan, K.; and Sheth, A. P. 2012. Harnessing twitter\u201d big data\u201d for automatic emotion identi\ufb01cation. In Privacy, Security, Risk and Trust (PASSAT), 2012 International Conference on and 2012 International Confernece on Social Computing (So- cialCom), 587\u2013592. IEEE. [Wang et al. 2018] Wang, S.-H.; Phillips, P.; Dong, Z.-C.; and Zhang, Y.-D. 2018.",
      "IEEE. [Wang et al. 2018] Wang, S.-H.; Phillips, P.; Dong, Z.-C.; and Zhang, Y.-D. 2018. Intelligent facial emotion recogni- tion based on stationary wavelet entropy and jaya algorithm. Neurocomputing 272:668\u2013676. [Wen and Wan 2014] Wen, S., and Wan, X. 2014. Emotion classi\ufb01cation in microblog texts using class sequential rules. In AAAI, 187\u2013193. [Yang et al. 2016] Yang, Z.; Yang, D.; Dyer, C.; He, X.; Smola, A.; and Hovy, E. 2016. Hierarchical attention net- works for document classi\ufb01cation. In Proceedings of the 2016 conference of the North American chapter of the asso- ciation for computational linguistics: human language tech- nologies, 1480\u20131489. [Zhang et al. 2016] Zhang, Y.-D.; Yang, Z.-J.; Lu, H.-M.; Zhou, X.-X.",
      "[Zhang et al. 2016] Zhang, Y.-D.; Yang, Z.-J.; Lu, H.-M.; Zhou, X.-X.; Phillips, P.; Liu, Q.-M.; and Wang, S.-H. 2016. Facial emotion recognition based on biorthogonal wavelet entropy, fuzzy support vector machine, and strati\ufb01ed cross validation. IEEE Access 4:8375\u20138385. [Zhang, Zhao, and LeCun 2015] Zhang, X.; Zhao, J.; and LeCun, Y. 2015. Character-level convolutional networks for text classi\ufb01cation. In Advances in neural information processing systems, 649\u2013657. [Zhou et al. 2015] Zhou, C.; Sun, C.; Liu, Z.; and Lau, F. 2015. A c-lstm neural network for text classi\ufb01cation. arXiv preprint arXiv:1511.08630."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1907.09369.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":6818,
  "avg_doclen":174.8205128205,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1907.09369.pdf"
    }
  }
}