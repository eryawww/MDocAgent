{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Non-Projective Dependency Parsing with Non-Local Transitions Daniel Fern\u00b4andez-Gonz\u00b4alez and Carlos G\u00b4omez-Rodr\u00b4\u0131guez Universidade da Coru\u02dcna FASTPARSE Lab, LyS Research Group, Departamento de Computaci\u00b4on Campus de Elvi\u02dcna, s\/n, 15071 A Coru\u02dcna, Spain d.fgonzalez@udc.es, carlos.gomez@udc.es Abstract We present a novel transition system, based on the Covington non-projective parser, in- troducing non-local transitions that can dir- ectly create arcs involving nodes to the left of the current focus positions. This avoids the need for long sequences of No-Arc trans- itions to create long-distance arcs, thus allevi- ating error propagation. The resulting parser outperforms the original version and achieves the best accuracy on the Stanford Dependen- cies conversion of the Penn Treebank among greedy transition-based parsers. 1 Introduction Greedy transition-based parsers are popular in NLP, as they provide competitive accuracy with high ef\ufb01ciency.",
            "The resulting parser outperforms the original version and achieves the best accuracy on the Stanford Dependen- cies conversion of the Penn Treebank among greedy transition-based parsers. 1 Introduction Greedy transition-based parsers are popular in NLP, as they provide competitive accuracy with high ef\ufb01ciency. They syntactically analyze a sen- tence by greedily applying transitions, which read it from left to right and produce a dependency tree. However, this greedy process is prone to er- ror propagation: one wrong choice of transition can lead the parser to an erroneous state, causing more incorrect decisions. This is especially cru- cial for long attachments requiring a larger number of transitions. In addition, transition-based pars- ers traditionally focus on only two words of the sentence and their local context to choose the next transition. The lack of a global perspective favors the presence of errors when creating arcs involving multiple transitions. As expected, transition-based parsers build short arcs more accurately than long ones (McDonald and Nivre, 2007).",
            "The lack of a global perspective favors the presence of errors when creating arcs involving multiple transitions. As expected, transition-based parsers build short arcs more accurately than long ones (McDonald and Nivre, 2007). Previous research such as (Fern\u00b4andez-Gonz\u00b4alez and G\u00b4omez-Rodr\u00b4\u0131guez, 2012) and (Qi and Man- ning, 2017) proves that the widely-used projective arc-eager transition-based parser of Nivre (2003) bene\ufb01ts from shortening the length of transition sequences by creating non-local attachments. In particular, they augmented the original transition system with new actions whose behavior en- tails more than one arc-eager transition and in- volves a context beyond the traditional two focus words. Attardi (2006) and Sartorio et al. (2013) also extended the arc-standard transition-based al- gorithm (Nivre, 2004) with the same success.",
            "Attardi (2006) and Sartorio et al. (2013) also extended the arc-standard transition-based al- gorithm (Nivre, 2004) with the same success. In the same vein, we present a novel unrestric- ted non-projective transition system based on the well-known algorithm by Covington (2001) that shortens the transition sequence necessary to parse a given sentence by the original algorithm, which becomes linear instead of quadratic with respect to sentence length. To achieve that, we propose new transitions that affect non-local words and are equivalent to one or more Covington actions, in a similar way to the transitions de\ufb01ned by Qi and Manning (2017) based on the arc-eager parser. Experiments show that this novel variant signi\ufb01c- antly outperforms the original one in all datasets tested, and achieves the best reported accuracy for a greedy dependency parser on the Stanford De- pendencies conversion of the WSJ Penn Treebank.",
            "Experiments show that this novel variant signi\ufb01c- antly outperforms the original one in all datasets tested, and achieves the best reported accuracy for a greedy dependency parser on the Stanford De- pendencies conversion of the WSJ Penn Treebank. 2 Non-Projective Covington Parser The original non-projective parser de\ufb01ned by Cov- ington (2001) was modelled under the transition- based parsing framework by Nivre (2008). We only sketch this transition system brie\ufb02y for space reasons, and refer to (Nivre, 2008) for details. Parser con\ufb01gurations have the form c = \u27e8\u03bb1, \u03bb2, B, A\u27e9, where \u03bb1 and \u03bb2 are lists of par- tially processed words, B a list (called buffer) of unprocessed words, and A the set of depend- ency arcs built so far. Given an input string w1 \u00b7 \u00b7 \u00b7 wn, the parser starts at the initial con\ufb01gura- tion cs(w1 . . . wn) = \u27e8[], [], [1 . . .",
            "Given an input string w1 \u00b7 \u00b7 \u00b7 wn, the parser starts at the initial con\ufb01gura- tion cs(w1 . . . wn) = \u27e8[], [], [1 . . . n], \u2205\u27e9and runs transitions until a terminal con\ufb01guration of the form \u27e8\u03bb1, \u03bb2, [], A\u27e9is reached: at that point, A arXiv:1710.09340v3  [cs.CL]  15 May 2018",
            "Covington: Shift: \u27e8\u03bb1, \u03bb2, j|B, A\u27e9\u21d2\u27e8\u03bb1 \u00b7 \u03bb2|j, [], B, A\u27e9 No-Arc: \u27e8\u03bb1|i, \u03bb2, B, A\u27e9\u21d2\u27e8\u03bb1, i|\u03bb2, B, A\u27e9 Left-Arc: \u27e8\u03bb1|i, \u03bb2, j|B, A\u27e9\u21d2\u27e8\u03bb1, i|\u03bb2, j|B, A \u222a{j \u2192i}\u27e9 only if \u2204x | x \u2192i \u2208A (single-head) and i \u2192\u2217j \u0338\u2208A (acyclicity). Right-Arc: \u27e8\u03bb1|i, \u03bb2, j|B, A\u27e9\u21d2\u27e8\u03bb1, i|\u03bb2, j|B, A \u222a{i \u2192j}\u27e9 only if \u2204x | x \u2192j \u2208A (single-head) and j \u2192\u2217i \u0338\u2208A (acyclicity).",
            "NL-Covington: Shift: \u27e8\u03bb1, \u03bb2, j|B, A\u27e9\u21d2\u27e8\u03bb1 \u00b7 \u03bb2|j, [], B, A\u27e9 Left-Arck: \u27e8\u03bb1|ik|...|i1, \u03bb2, j|B, A\u27e9\u21d2\u27e8\u03bb1, ik|...|i1|\u03bb2, j|B, A \u222a{j \u2192ik}\u27e9 only if \u2204x | x \u2192ik \u2208A (single-head) and ik \u2192\u2217j \u0338\u2208A (acyclicity). Right-Arck: \u27e8\u03bb1|ik|...|i1, \u03bb2, j|B, A\u27e9\u21d2\u27e8\u03bb1, ik|...|i1|\u03bb2, j|B, A \u222a{ik \u2192j}\u27e9 only if \u2204x | x \u2192j \u2208A (single-head) and j \u2192\u2217ik \u0338\u2208A (acyclicity). Figure 1: Transitions of the non-projective Covington (top) and NL-Covington (bottom) dependency parsers.",
            "Figure 1: Transitions of the non-projective Covington (top) and NL-Covington (bottom) dependency parsers. The notation i \u2192\u2217j \u2208A means that there is a (possibly empty) directed path from i to j in A. contains the dependency graph for the input.1 The set of transitions is shown in the top half of Figure 1. Their logic can be summarized as follows: when in a con\ufb01guration of the form \u27e8\u03bb1|i, \u03bb2, j|B, A\u27e9, the parser has the chance to cre- ate a dependency involving words i and j, which we will call left and right focus words of that con- \ufb01guration. The Left-Arc and Right-Arc transitions are used to create a leftward (i \u2190j) or rightward arc (i \u2192j), respectively, between these words, and also move i from \u03bb1 to the \ufb01rst position of \u03bb2, effectively moving the focus to i \u22121 and j. If no dependency is desired between the focus words, the No-Arc transition makes the same modi\ufb01ca- tion of \u03bb1 and \u03bb2, but without building any arc.",
            "Finally, the Shift transition moves the whole con- tent of the list \u03bb2 plus j to \u03bb1 when no more at- tachments are pending between j and the words of \u03bb1, thus reading a new input word and placing the focus on j and j + 1. Transitions that create arcs are disallowed in con\ufb01gurations where this would violate the single-head or acyclicity con- straints (cycles and nodes with multiple heads are not allowed in the dependency graph). Figure 3 shows the transition sequence in the Covington transition system which derives the dependency graph in Figure 2. The resulting parser can generate arbitrary non- projective trees, and its complexity is O(n2). 3 Non-Projective NL-Covington Parser The original logic described by Covington (2001) parses a sentence by systematically traversing every pair of words. The Shift transition, intro- 1Note that, in general, A is a forest, but it can be conver- ted to a tree by linking headless nodes as dependents of an arti\ufb01cial root node at position 0.",
            "The Shift transition, intro- 1Note that, in general, A is a forest, but it can be conver- ted to a tree by linking headless nodes as dependents of an arti\ufb01cial root node at position 0. 1 2 3 4 5 Figure 2: Dependency tree for an input sentence. Tran. \u03bb1 \u03bb2 Buffer Arc [ ] [ ] [ 1, 2, 3, 4, 5 ] SH [ 1 ] [ ] [ 2, 3, 4, 5 ] RA [ ] [ 1 ] [ 2, 3, 4, 5 ] 1 \u21922 SH [ 1, 2 ] [ ] [ 3, 4, 5 ] NA [ 1 ] [ 2 ] [ 3, 4, 5 ] RA [ ] [ 1, 2 ] [ 3, 4, 5 ] 1 \u21923 SH [ 1, 2, 3 ] [ ] [ 4, 5 ] SH [ 1, 2, 3, 4 ] [ ] [ 5 ] LA [ 1,",
            "2 ] [ 3, 4, 5 ] 1 \u21923 SH [ 1, 2, 3 ] [ ] [ 4, 5 ] SH [ 1, 2, 3, 4 ] [ ] [ 5 ] LA [ 1, 2, 3 ] [ 4 ] [ 5 ] 4 \u21905 NA [ 1, 2] [ 3, 4 ] [ 5 ] NA [ 1 ] [ 2, 3, 4 ] [ 5 ] RA [ ] [ 1, 2, 3, 4 ] [ 5 ] 1 \u21925 SH [ 1, 2, 3, 4, 5 ] [ ] [ ] Figure 3: Transition sequence for parsing the sen- tence in Figure 2 using the Covington parser (LA=LEFT-ARC, RA=RIGHT-ARC, NA=NO- ARC, SH=SHIFT).",
            "2, 3, 4, 5 ] [ ] [ ] Figure 3: Transition sequence for parsing the sen- tence in Figure 2 using the Covington parser (LA=LEFT-ARC, RA=RIGHT-ARC, NA=NO- ARC, SH=SHIFT). duced by Nivre (2008) in the transition-based ver- sion, is an optimization that avoids the need to ap- ply a sequence of No-Arc transitions to empty the list \u03bb1 before reading a new input word. However, there are still situations where se- quences of No-Arc transitions are needed. For ex- ample, if we are in a con\ufb01guration C with focus words i and j and the next arc we need to create goes from j to i \u2212k (k > 1), then we will need",
            "k \u22121 consecutive No-Arc transitions to move the left focus word to i and then apply Left-Arc. This could be avoided if a non-local Left-Arc transition could be undertaken directly at C, creating the re- quired arc and moving k words to \u03bb2 at once. The advantage of such approach would be twofold: (1) less risk of making a mistake at C due to consid- ering a limited local context, and (2) shorter trans- ition sequence, alleviating error propagation. We present a novel transition system called NL- Covington (for \u201cnon-local Covington\u201d), described in the bottom half of Figure 1. It consists in a modi\ufb01cation of the non-projective Covington al- gorithm where: (1) the Left-Arc and Right-Arc transitions are parameterized with k, allowing the immediate creation of any attachment between j and the kth leftmost word in \u03bb1 and moving k words to \u03bb2 at once, and (2) the No-Arc transition is removed since it is no longer necessary.",
            "This new transition system can use some re- stricted global information to build non-local de- pendencies and, consequently, reduce the number of transitions needed to parse the input. For in- stance, as presented in Figure 4, the NL-Covington parser will need 9 transitions, instead of 12 tradi- tional Covington actions, to analyze the sentence in Figure 2. In fact, while in the standard Covington al- gorithm a transition sequence for a sentence of length n has length O(n2) in the worst case (if all nodes are connected to the \ufb01rst node, then we need to traverse every node to the left of each right fo- cus word); for NL-Covington the sequence length is always O(n): one Shift transition for each of the n words, plus one arc-building transition for each of the n \u22121 arcs in the dependency tree. Note, however, that this does not affect the parser\u2019s time complexity, which is still quadratic as in the ori- ginal Covington parser.",
            "Note, however, that this does not affect the parser\u2019s time complexity, which is still quadratic as in the ori- ginal Covington parser. This is because the al- gorithm has O(n) possible transitions to be scored at each con\ufb01guration, while the original Coving- ton has O(1) transitions due to being limited to creating local leftward\/rightward arcs between the focus words. The completeness and soundness of NL- Covington can easily be proved as there is a map- ping between transition sequences of both parsers, where a sequence of k \u22121 No-Arc and one arc transition in Covington is equivalent to a Left-Arck or Right-Arck in NL-Covington. Tran. \u03bb1 \u03bb2 Buffer Arc [ ] [ ] [ 1, 2, 3, 4, 5 ] SH [ 1 ] [ ] [ 2, 3, 4 , 5 ] RA1 [ ] [ 1 ] [ 2, 3, 4 , 5 ] 1 \u21922 SH [ 1, 2 ] [ ] [ 3, 4, 5 ] RA2 [ ] [ 1,",
            "3, 4 , 5 ] RA1 [ ] [ 1 ] [ 2, 3, 4 , 5 ] 1 \u21922 SH [ 1, 2 ] [ ] [ 3, 4, 5 ] RA2 [ ] [ 1, 2 ] [ 3, 4, 5 ] 1 \u21923 SH [ 1, 2, 3 ] [ ] [ 4, 5 ] SH [ 1, 2, 3, 4 ] [ ] [ 5 ] LA1 [ 1, 2, 3 ] [ 4 ] [ 5 ] 4 \u21905 RA3 [ ] [ 1, 2, 3, 4 ] [ 5 ] 1 \u21925 SH [ 1, 2, 3, 4, 5 ] [ ] [ ] Figure 4: Transition sequence for parsing the sen- tence in Figure 2 using the NL-Covington parser (LA=LEFT-ARC, RA=RIGHT-ARC, SH=SHIFT).",
            "2, 3, 4, 5 ] [ ] [ ] Figure 4: Transition sequence for parsing the sen- tence in Figure 2 using the NL-Covington parser (LA=LEFT-ARC, RA=RIGHT-ARC, SH=SHIFT). 4 Experiments 4.1 Data and Evaluation We use 9 datasets2 from the CoNLL-X (Buch- holz and Marsi, 2006) and all datasets from the CoNLL-XI shared task (Nivre et al., 2007). To compare our system to the current state-of-the- art transition-based parsers, we also evaluate it on the Stanford Dependencies (de Marneffe and Manning, 2008) conversion (using the Stanford parser v3.3.0)3 of the WSJ Penn Treebank (Mar- cus et al., 1993), hereinafter PT-SD, with stand- ard splits. Labelled and Unlabelled Attachment Scores (LAS and UAS) are computed excluding punctuation only on the PT-SD, for comparability. We repeat each experiment with three independ- ent random initializations and report the average accuracy.",
            "Labelled and Unlabelled Attachment Scores (LAS and UAS) are computed excluding punctuation only on the PT-SD, for comparability. We repeat each experiment with three independ- ent random initializations and report the average accuracy. Statistical signi\ufb01cance is assessed by a paired test with 10,000 bootstrap samples. 4.2 Model To implement our approach we take advantage of the model architecture described in Qi and Man- ning (2017) for the arc-swift parser, which ex- tends the architecture of Kiperwasser and Gold- berg (2016) by applying a biaf\ufb01ne combination during the featurization process. We implement both the Covington and NL-Covington parsers un- der this architecture, adapt the featurization pro- cess with biaf\ufb01ne combination of Qi and Manning (2017) to these parsers, and use their same training 2We excluded the languages from CoNLL-X that also ap- peared in CoNLL-XI, i.e., if a language was present in both shared tasks, we used the latest version. 3https:\/\/nlp.stanford.edu\/software\/ lex-parser.shtml",
            "Covington NL-Covington Language UAS LAS UAS LAS Arabic 66.67 53.24 68.69 54.59 Basque 74.31 66.18 75.45 67.61 Catalan 91.93 86.12 92.60 86.99 Chinese 83.87 76.19 85.25 77.56 Czech 84.27 77.91 86.26 79.95 English 89.94 88.74 91.51 90.47 Greek 79.91 72.65 80.61 73.41 Hungarian 76.80 65.21 78.57 67.51 Italian 82.03 75.87 83.63 78.03 Turkish 80.29 70.68 81.30 71.28 Bulgarian 81.78 76.23 83.65 78.40 Danish 86.56 81.18 88.40 82.77 Dutch 86.19 82.24 87.45 83.76 German 85.72 82.28 87.24 83.",
            "78 76.23 83.65 78.40 Danish 86.56 81.18 88.40 82.77 Dutch 86.19 82.24 87.45 83.76 German 85.72 82.28 87.24 83.92 Japanese 92.20 90.41 93.63 91.65 Portuguese 86.69 82.19 87.89 83.69 Slovene 76.07 66.81 77.83 69.74 Spanish 74.67 69.41 76.58 71.60 Swedish 74.65 64.67 75.62 65.95 Average 81.82 75.17 83.27 76.78 Table 1: Parsing accuracy (UAS and LAS, in- cluding punctuation) of the Covington and NL- Covington non-projective parsers on CoNLL-XI (\ufb01rst block) and CoNLL-X (second block) data- sets. Best results for each language are shown in bold. All improvements in this table are statistic- ally signi\ufb01cant (\u03b1 = .05).",
            "Best results for each language are shown in bold. All improvements in this table are statistic- ally signi\ufb01cant (\u03b1 = .05). setup. More details about these model parameters are provided in Appendix A. Since this architecture uses batch training, we train with a static oracle. The NL-Covington al- gorithm has no spurious ambiguity at all, so there is only one possible static oracle: canonical trans- ition sequences are generated by choosing the transition that builds the shortest pending gold arc involving the current right focus word j, or Shift if there are no unbuilt gold arcs involving j. We note that a dynamic oracle can be obtained for the NL-Covington parser by adapting the one for standard Covington of G\u00b4omez-Rodr\u00b4\u0131guez and Fern\u00b4andez-Gonz\u00b4alez (2015). As NL-Covington transitions are concatenations of Covington ones, their loss calculation algorithm is compatible with NL-Covington. Apart from error exploration, this also opens the way to incorporating non- monotonicity (Fern\u00b4andez-Gonz\u00b4alez and G\u00b4omez- Rodr\u00b4\u0131guez, 2017).",
            "Apart from error exploration, this also opens the way to incorporating non- monotonicity (Fern\u00b4andez-Gonz\u00b4alez and G\u00b4omez- Rodr\u00b4\u0131guez, 2017). While these approaches have shown to improve accuracy under online training settings, here we prioritize homogeneous compar- ability to (Qi and Manning, 2017), so we use batch training and a static oracle, and still obtain state- of-the-art accuracy for a greedy parser.",
            "While these approaches have shown to improve accuracy under online training settings, here we prioritize homogeneous compar- ability to (Qi and Manning, 2017), so we use batch training and a static oracle, and still obtain state- of-the-art accuracy for a greedy parser. Parser Type UAS LAS (Chen and Manning, 2014) gs 91.8 89.6 (Dyer et al., 2015) gs 93.1 90.9 (Weiss et al., 2015) greedy gs 93.2 91.2 (Ballesteros et al., 2016) gd 93.5 91.4 (Kiperwasser and Goldberg, 2016) gd 93.9 91.9 (Qi and Manning, 2017) gs 94.3 92.2 This work gs 94.5 92.4 (Weiss et al., 2015) beam b(8) 94.0 92.1 (Alberti et al., 2015) b(32) 94.2 92.4 (Andor et al., 2016) b(32) 94.6 92.8 (Shi et al., 2017) dp 94.5 - (Kuncoro et al., 2017) (constit.)",
            "c 95.8 94.6 Table 2: Accuracy comparison of state-of-the-art transition-based dependency parsers on PT-SD. The \u201cType\u201d column shows the type of parser: gs is a greedy parser trained with a static oracle, gd a greedy parser trained with a dynamic oracle, b(n) a beam search parser with beam size n, dp a parser that employs global training with dynamic pro- gramming, and c a constituent parser with conver- sion to dependencies. 4.3 Results Table 1 presents a comparison between the Cov- ington parser and the novel variant developed here. The NL-Covington parser outperforms the ori- ginal version in all datasets tested, with all im- provements statistically signi\ufb01cant (\u03b1 = .05). Table 2 compares our novel system with other state-of-the-art transition-based dependency pars- ers on the PT-SD. Greedy parsers are in the \ufb01rst block, beam-search and dynamic programming parsers in the second block. The third block shows the best result on this benchmark, obtained with constituent parsing with generative re-ranking and conversion to dependencies.",
            "Greedy parsers are in the \ufb01rst block, beam-search and dynamic programming parsers in the second block. The third block shows the best result on this benchmark, obtained with constituent parsing with generative re-ranking and conversion to dependencies. Despite being the only non-projective parser tested on a practically projective dataset,4 our parser achieves the highest score among greedy transition-based models (even above those trained with a dynamic oracle). We even slightly outperform the arc-swift sys- tem of Qi and Manning (2017), with the same model architecture, implementation and train- ing setup, but based on the projective arc-eager transition-based parser instead. This may be be- cause our system takes into consideration any per- missible attachment between the focus word j and any word in \u03bb1 at each con\ufb01guration, while their approach is limited by the arc-eager logic: it al- 4Only 41 out of 39,832 sentences of the PT-SD training dataset present some kind of non-projectivity.",
            "Arc-swift NL-Covington Language UAS LAS UAS LAS Arabic 67.54 53.65 68.69\u2217 54.59\u2217 Basque 74.88 67.44 75.45 67.61 Catalan 92.98 87.51\u2217 92.60 86.99 Chinese 84.96 77.34 85.25 77.56 Czech 85.92 79.82 86.26 79.95 English 91.41 90.43 91.51 90.47 Greek 81.64\u2217 74.56\u2217 80.61 73.41 Hungarian 78.70 69.27\u2217 78.57 67.51 Italian 83.29 78.60\u2217 83.63 78.03 Turkish 79.56 70.22 81.30\u2217 71.28\u2217 Bulgarian 83.28 78.19 83.65 78.40 Danish 87.86 82.58 88.40\u2217 82.77 Dutch 83.27 80.14 87.",
            "22 81.30\u2217 71.28\u2217 Bulgarian 83.28 78.19 83.65 78.40 Danish 87.86 82.58 88.40\u2217 82.77 Dutch 83.27 80.14 87.45\u2217 83.76\u2217 German 86.28 82.97 87.24\u2217 83.92\u2217 Japanese 93.64 91.92 93.63 91.65 Portuguese 87.01 83.09 87.89\u2217 83.69\u2217 Slovene 77.89 69.37 77.83 69.74 Spanish 75.55 70.62 76.58\u2217 71.60\u2217 Swedish 75.00 65.66 75.62 65.95 Average 82.67 76.49 83.27 76.78 Table 3: Parsing accuracy (UAS and LAS, with punctuation) of the arc-swift and NL-Covington parsers on CoNLL-XI (1st block) and CoNLL-X (2nd block) datasets.",
            "67 76.49 83.27 76.78 Table 3: Parsing accuracy (UAS and LAS, with punctuation) of the arc-swift and NL-Covington parsers on CoNLL-XI (1st block) and CoNLL-X (2nd block) datasets. Best results for each lan- guage are in bold. * indicates statistically signi- \ufb01cant improvements (\u03b1 = .05). lows all possible rightward arcs (possibly fewer than our approach as the arc-eager stack usually contains a small number of words), but only one leftward arc is permitted per parser state. It is also worth noting that the arc-swift and NL-Covington parsers have the same worst-case time complex- ity, (O(n2)), as adding non-local arc transitions to the arc-eager parser increases its complexity from linear to quadratic, but it does not affect the com- plexity of the Covington algorithm. Thus, it can be argued that this technique is better suited to Cov- ington than to arc-eager parsing. We also compare NL-Covington to the arc- swift parser on the CoNLL datasets (Table 3).",
            "Thus, it can be argued that this technique is better suited to Cov- ington than to arc-eager parsing. We also compare NL-Covington to the arc- swift parser on the CoNLL datasets (Table 3). For fairness of comparison, we projectivize (via maltparser5) all training datasets, instead of \ufb01lter- ing non-projective sentences, as some of the lan- guages are signi\ufb01cantly non-projective. Even do- ing that, the NL-Covington parser improves over the arc-swift system in terms of UAS in 14 out of 19 datasets, obtaining statistically signi\ufb01cant im- provements in accuracy on 7 of them, and statist- ically signi\ufb01cant decreases in just one. Finally, we analyze how our approach reduces the length of the transition sequence consumed by 5http:\/\/www.maltparser.org\/ Covington NL-Covington Language trans.\/sent. trans.\/sent.",
            "Finally, we analyze how our approach reduces the length of the transition sequence consumed by 5http:\/\/www.maltparser.org\/ Covington NL-Covington Language trans.\/sent. trans.\/sent. Arabic 194.80 78.22 Basque 46.74 30.13 Catalan 117.35 60.07 Chinese 19.12 14.95 Czech 60.62 33.03 English 78.01 46.75 Greek 89.23 48.77 Hungarian 68.54 37.66 Italian 63.67 40.93 Turkish 53.53 30.08 Bulgarian 51.35 29.81 Danish 66.77 36.34 Dutch 42.78 28.93 German 61.16 31.89 Japanese 24.30 16.11 Portuguese 76.14 40.74 Slovene 56.15 31.79 Spanish 109.70 55.28 Swedish 48.59 29.07 PTB-SD 81.65 46.92 Average 70.51 38.37 Table 4: Average transitions executed per sentence (trans.\/sent.)",
            "when analyzing each dataset by the original Covington and NL-Covington algorithms. the original Covington parser. In Table 4 we re- port the transition sequence length per sentence used by the Covington and the NL-Covington al- gorithms to analyze each dataset from the same benchmark used for evaluating parsing accuracy. As seen in the table, NL-Covington produces not- ably shorter transition sequences than Covington, with a reduction close to 50% on average. 5 Conclusion We present a novel variant of the non-projective Covington transition-based parser by incorporat- ing non-local transitions, reducing the length of transition sequences from O(n2) to O(n). This system clearly outperforms the original Coving- ton parser and achieves the highest accuracy on the WSJ Penn Treebank (Stanford Dependencies) obtained to date with greedy dependency parsing.",
            "This system clearly outperforms the original Coving- ton parser and achieves the highest accuracy on the WSJ Penn Treebank (Stanford Dependencies) obtained to date with greedy dependency parsing. Acknowledgments This work has received funding from the European Research Council (ERC), under the European Union\u2019s Horizon 2020 research and innovation programme (FASTPARSE, grant agreement No 714150), from the TELEPARES-UDC (FFI2014- 51978-C2-2-R) and ANSWER-ASAP (TIN2017- 85160-C2-1-R) projects from MINECO, and from Xunta de Galicia (ED431B 2017\/01).",
            "References Chris Alberti, David Weiss, Greg Coppola, and Slav Petrov. 2015. Improved transition-based parsing and tagging with neural networks. In Proceedings of the 2015 Conference on Empirical Methods in Nat- ural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015. pages 1354\u20131359. http:\/\/aclweb.org\/anthology\/D\/D15\/D15-1159.pdf. Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman Ganchev, Slav Petrov, and Michael Collins. 2016. Globally nor- malized transition-based neural networks. In Pro- ceedings of the 54th Annual Meeting of the Associ- ation for Computational Linguistics, ACL 2016, Au- gust 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. http:\/\/aclweb.org\/anthology\/P\/P16\/P16- 1231.pdf. Giuseppe Attardi. 2006.",
            "http:\/\/aclweb.org\/anthology\/P\/P16\/P16- 1231.pdf. Giuseppe Attardi. 2006. Experiments with a multil- anguage non-projective dependency parser. In Pro- ceedings of the 10th Conference on Computational Natural Language Learning (CoNLL). pages 166\u2013 170. Miguel Ballesteros, Yoav Goldberg, Chris Dyer, and Noah A. Smith. 2016. Training with explora- tion improves a greedy stack-lstm parser. CoRR abs\/1603.03793. http:\/\/arxiv.org\/abs\/1603.03793. Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2016. Enriching word vec- tors with subword information. arXiv preprint arXiv:1607.04606 . Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of the 10th Conference on Computa- tional Natural Language Learning (CoNLL).",
            "Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of the 10th Conference on Computa- tional Natural Language Learning (CoNLL). pages 149\u2013164. http:\/\/www.aclweb.org\/anthology\/W06- 2920. Danqi Chen and Christopher Manning. 2014. A fast and accurate dependency parser using neural net- works. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Pro- cessing (EMNLP). Doha, Qatar, pages 740\u2013750. http:\/\/www.aclweb.org\/anthology\/D14-1082. Michael A. Covington. 2001. A fundamental algorithm for dependency parsing. In Proceedings of the 39th Annual ACM Southeast Conference. ACM, New York, NY, USA, pages 95\u2013102. Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The stanford typed depend- encies representation.",
            "In Proceedings of the 39th Annual ACM Southeast Conference. ACM, New York, NY, USA, pages 95\u2013102. Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The stanford typed depend- encies representation. In Coling 2008: Pro- ceedings of the Workshop on Cross-Framework and Cross-Domain Parser Evaluation. Associ- ation for Computational Linguistics, Strouds- burg, PA, USA, CrossParser \u201908, pages 1\u20138. http:\/\/dl.acm.org\/citation.cfm?id=1608858.1608859. Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and Noah A. Smith. 2015. Transition- based dependency parsing with stack long short- term memory. In Proceedings of the 53rd An- nual Meeting of the Association for Computa- tional Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Pro- cessing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers.",
            "pages 334\u2013343. http:\/\/aclweb.org\/anthology\/P\/P15\/P15-1033.pdf. Daniel Fern\u00b4andez-Gonz\u00b4alez and Carlos G\u00b4omez- Rodr\u00b4\u0131guez. 2012. Improving transition-based dependency parsing with buffer transitions. In Pro- ceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Com- putational Natural Language Learning. Association for Computational Linguistics, pages 308\u2013319. http:\/\/aclweb.org\/anthology\/D\/D12\/D12-1029.pdf. Daniel Fern\u00b4andez-Gonz\u00b4alez and Carlos G\u00b4omez- Rodr\u00b4\u0131guez. 2017. A full non-monotonic transition system for unrestricted non-projective parsing. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Vancouver, Canada, pages 288\u2013298. http:\/\/aclweb.org\/anthology\/P17-1027. Carlos G\u00b4omez-Rodr\u00b4\u0131guez and Daniel Fern\u00b4andez- Gonz\u00b4alez. 2015.",
            "Association for Computational Linguistics, Vancouver, Canada, pages 288\u2013298. http:\/\/aclweb.org\/anthology\/P17-1027. Carlos G\u00b4omez-Rodr\u00b4\u0131guez and Daniel Fern\u00b4andez- Gonz\u00b4alez. 2015. An ef\ufb01cient dynamic oracle for unrestricted non-projective parsing. In Pro- ceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Nat- ural Language Processing (ACL-IJCNLP 2015). Volume 2: Short Papers. Association for Computa- tional Linguistics, Beijing, China, pages 256\u2013261. http:\/\/www.aclweb.org\/anthology\/P15-2042. Alex Graves and J\u00a8urgen Schmidhuber. 2005. Frame- wise phoneme classi\ufb01cation with bidirectional lstm and other neural network architectures. Neural Net- works pages 5\u20136. Eliyahu Kiperwasser and Yoav Goldberg. 2016. Simple and accurate dependency parsing using bidirectional LSTM feature representations. TACL 4:313\u2013327.",
            "Neural Net- works pages 5\u20136. Eliyahu Kiperwasser and Yoav Goldberg. 2016. Simple and accurate dependency parsing using bidirectional LSTM feature representations. TACL 4:313\u2013327. ht- tps:\/\/transacl.org\/ojs\/index.php\/tacl\/article\/view\/885. Adhiguna Kuncoro, Miguel Ballesteros, Ling- peng Kong, Chris Dyer, Graham Neubig, and Noah A. Smith. 2017. What do recurrent neural network grammars learn about syn- tax? In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Pa- pers. Association for Computational Linguistics, pages 1249\u20131258. http:\/\/aclanthology.coli.uni- saarland.de\/pdf\/E\/E17\/E17-1117.pdf. Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated",
            "corpus of English: The Penn Treebank. Computa- tional Linguistics 19:313\u2013330. Ryan McDonald and Joakim Nivre. 2007. Character- izing the errors of data-driven dependency parsing models. In Proceedings of the 2007 Joint Confer- ence on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL). pages 122\u2013131. Joakim Nivre. 2003. An ef\ufb01cient algorithm for pro- jective dependency parsing. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT 03). ACL\/SIGPARSE, pages 149\u2013160. Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Proceedings of the Work- shop on Incremental Parsing: Bringing Engineering and Cognition Together (ACL). pages 50\u201357. Joakim Nivre. 2008. Algorithms for Determ- inistic Incremental Dependency Parsing. Computational Linguistics 34(4):513\u2013553.",
            "pages 50\u201357. Joakim Nivre. 2008. Algorithms for Determ- inistic Incremental Dependency Parsing. Computational Linguistics 34(4):513\u2013553. https:\/\/doi.org\/10.1162\/coli.07-056-R1-07-027. Joakim Nivre, Johan Hall, Sandra K\u00a8ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Pro- ceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007. pages 915\u2013932. http:\/\/www.aclweb.org\/anthology\/D\/D07\/D07- 1096.pdf. Jeffrey Pennington, Richard Socher, and Chris- topher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Nat- ural Language Processing (EMNLP). pages 1532\u2013 1543. http:\/\/www.aclweb.org\/anthology\/D14-1162.",
            "2014. Glove: Global vectors for word representation. In Empirical Methods in Nat- ural Language Processing (EMNLP). pages 1532\u2013 1543. http:\/\/www.aclweb.org\/anthology\/D14-1162. Peng Qi and Christopher D. Manning. 2017. Arc- swift: A novel transition system for dependency parsing. In Proceedings of the 55th Annual Meet- ing of the Association for Computational Linguist- ics, ACL 2017, Vancouver, Canada, July 30 - Au- gust 4, Volume 2: Short Papers. pages 110\u2013117. https:\/\/doi.org\/10.18653\/v1\/P17-2018. Francesco Sartorio, Giorgio Satta, and Joakim Nivre. 2013. A transition-based dependency parser us- ing a dynamic parsing strategy. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguist- ics, pages 135\u2013144.",
            "2013. A transition-based dependency parser us- ing a dynamic parsing strategy. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguist- ics, pages 135\u2013144. http:\/\/aclanthology.coli.uni- saarland.de\/pdf\/P\/P13\/P13-1014.pdf. Tianze Shi, Liang Huang, and Lillian Lee. 2017. Fast(er) exact decoding and global training for transition-based dependency parsing via a minimal feature set. CoRR abs\/1708.09403. http:\/\/arxiv.org\/abs\/1708.09403. David Weiss, Chris Alberti, Michael Collins, and Slav Petrov. 2015. Structured training for neural net- work transition-based parsing. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Pro- cessing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers. pages 323\u2013333.",
            "pages 323\u2013333. http:\/\/aclweb.org\/anthology\/P\/P15\/P15-1032.pdf. A Model Details We provide more details of the neural network ar- chitecture used in this paper, which is taken from Qi and Manning (2017). The model consists of two blocks of 2-layered bidirectional long short-term memory (BiLSTM) networks (Graves and Schmidhuber, 2005) with 400 hidden units in each direction. The \ufb01rst block is used for POS tagging and the second one, for parsing. As the input of the tagging block, we use words represented as word embeddings, and BiL- STMs are employed to perform feature extraction. The resulting output is fed into a multi-layer per- ceptron (MLP), with a hidden layer of 100 recti- \ufb01ed linear units (ReLU), that provides a POS tag for each input token in a 32-dimensional repres- entation. Word embeddings concatenated to these POS tag embeddings serve as input of the second block of BiLSTMs to undertake the parsing stage.",
            "Word embeddings concatenated to these POS tag embeddings serve as input of the second block of BiLSTMs to undertake the parsing stage. Then, the output of the parsing block is fed into a MLP with two separate ReLU hidden layers (one for deriving the representation of the head, and the other for the dependency label) that, after be- ing merged and by means of a softmax function, score all the feasible transitions, allowing to greed- ily choose and apply the highest-scoring one. Moreover, we adapt the featurization process with biaf\ufb01ne combination described in Qi and Manning (2017) for the arc-swift system to be used on the original Covington and NL-Covington parsers. In particular, arc transitions are featurized by the concatenation of the representation of the head and dependent words of the arc to be created, the No-Arc transition is featurized by the right- most word in \u03bb1 and the leftmost word in the buf- fer B and, \ufb01nally, for the Shift transition only the leftmost word in B is used.",
            "Unlike Qi and Man- ning (2017) do for baseline parsers, we do not use the featurization method detailed in Kiperwasser and Goldberg (2016)6 for the original Covington parser, as we observed that this results in lower 6For instance, Kiperwasser and Goldberg (2016) featurize all transitions of the arc-eager parser in the same way by con- catenating the representations of the top 3 words on the stack and the leftmost word in the buffer.",
            "scores and then the comparison would be unfair in our case. We implement both systems under the same framework, with the original Covington parser represented as the NL-Covington system plus the No-Arc transition and with k limited to 1. A thorough description of the model architec- ture and featurization mechanism can be found in Qi and Manning (2017). Our training setup is exactly the same used by Qi and Manning (2017), training the models dur- ing 10 epochs for large datasets and 30 for small ones. In addition, we initialize word embeddings with 100-dimensional GloVe vectors (Pennington et al., 2014) for English and use 300-dimensional Facebook vectors (Bojanowski et al., 2016) for other languages. The other parameters of the neural network keep the same values. The parser\u2019s source code is freely avail- able at https:\/\/github.com\/danifg\/ Non-Local-Covington."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1710.09340.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 8148.999755859375,
    "avg_doclen_est": 169.7708282470703
}
