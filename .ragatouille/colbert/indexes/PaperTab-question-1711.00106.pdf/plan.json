{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "DCN+: MIXED OBJECTIVE AND DEEP RESIDUAL COATTENTION FOR QUESTION ANSWERING Caiming Xiong\u2217, Victor Zhong\u2217, Richard Socher Salesforce Research Palo Alto, CA 94301, USA {cxiong, vzhong, rsocher}@salesforce.com ABSTRACT Traditional models for question answering optimize using cross entropy loss, which encourages exact answers at the cost of penalizing nearby or overlapping answers that are sometimes equally accurate. We propose a mixed objective that combines cross entropy loss with self-critical policy learning. The objective uses rewards derived from word overlap to solve the misalignment between evaluation metric and optimization objective. In addition to the mixed objective, we improve dynamic coattention networks (DCN) with a deep residual coattention encoder that is inspired by recent work in deep self-attention and residual networks. Our proposals improve model performance across question types and input lengths, especially for long questions that requires the ability to capture long-term de- pendencies.",
            "Our proposals improve model performance across question types and input lengths, especially for long questions that requires the ability to capture long-term de- pendencies. On the Stanford Question Answering Dataset, our model achieves state-of-the-art results with 75.1% exact match accuracy and 83.1% F1, while the ensemble obtains 78.9% exact match accuracy and 86.0% F1. 1 INTRODUCTION Existing state-of-the-art question answering models are trained to produce exact answer spans for a question and a document. In this setting, a ground truth answer used to supervise the model is de\ufb01ned as a start and an end position within the document. Existing training approaches optimize using cross entropy loss over the two positions. However, this suffers from a fundamental disconnect between the optimization, which is tied to the position of a particular ground truth answer span, and the evaluation, which is based on the textual content of the answer. This disconnect is especially harmful in cases where answers that are textually similar to, but distinct in positions from, the ground truth are penalized in the same fashion as answers that are textually dissimilar.",
            "This disconnect is especially harmful in cases where answers that are textually similar to, but distinct in positions from, the ground truth are penalized in the same fashion as answers that are textually dissimilar. For example, suppose we are given the sentence \u201cSome believe that the Golden State Warriors team of 2017 is one of the greatest teams in NBA history\u201d, the question \u201cwhich team is considered to be one of the greatest teams in NBA history\u201d, and a ground truth answer of \u201cthe Golden State Warriors team of 2017\u201d. The span \u201cWarriors\u201d is also a correct answer, but from the perspective of traditional cross entropy based training it is no better than the span \u201chistory\u201d. To address this problem, we propose a mixed objective that combines traditional cross entropy loss over positions with a measure of word overlap trained with reinforcement learning. We obtain the latter objective using self-critical policy learning in which the reward is based on word overlap be- tween the proposed answer and the ground truth answer.",
            "To address this problem, we propose a mixed objective that combines traditional cross entropy loss over positions with a measure of word overlap trained with reinforcement learning. We obtain the latter objective using self-critical policy learning in which the reward is based on word overlap be- tween the proposed answer and the ground truth answer. Our mixed objective brings two bene\ufb01ts: (i) the reinforcement learning objective encourages answers that are textually similar to the ground truth answer and discourages those that are not; (ii) the cross entropy objective signi\ufb01cantly facili- tates policy learning by encouraging trajectories that are known to be correct. The resulting objective is one that is both faithful to the evaluation metric and converges quickly in practice. In addition to our mixed training objective, we extend the Dynamic Coattention Network (DCN) by Xiong et al. (2017) with a deep residual coattention encoder. This allows the network to build richer representations of the input by enabling each input sequence to attend to previous attention contexts. Vaswani et al.",
            "(2017) with a deep residual coattention encoder. This allows the network to build richer representations of the input by enabling each input sequence to attend to previous attention contexts. Vaswani et al. (2017) show that the stacking of attention layers helps model long-range \u2217Equal contribution 1 arXiv:1711.00106v2  [cs.CL]  10 Nov 2017",
            "BiLSTM1 BiLSTM1 Coattention1 Coattention2 BiLSTM2 BiLSTM2 Output BiLSTM Question Document LD 1 LQ 1 EQ 1 ED 1 SD 1 SQ 1 CD 1 CD 2 SD 2 EQ 2 ED 2 Figure 1: Deep residual coattention encoder. dependencies. We merge coattention outputs from each layer by means of residual connections to reduce the length of signal paths. He et al. (2016) show that skip layer connections facilitate signal propagation and alleviate gradient degradation. The combination of the deep residual coattention encoder and the mixed objective leads to higher performance across question types, question lengths, and answer lengths on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) compared to our DCN baseline. The im- provement is especially apparent on long questions, which require the model to capture long-range dependencies between the document and the question. Our model, which we call DCN+, achieves state-of-the-art results on SQuAD, with 75.1% exact match accuracy and 83.1% F1.",
            "Our model, which we call DCN+, achieves state-of-the-art results on SQuAD, with 75.1% exact match accuracy and 83.1% F1. When ensem- bled, the DCN+ obtains 78.9% exact match accuracy and 86.0% F1. 2 DCN+ We consider the question answering task in which we are given a document and a question, and are asked to \ufb01nd the answer in the document. Our model is based on the DCN by Xiong et al. (2017), which consists of a coattention encoder and a dynamic decoder. The encoder \ufb01rst encodes the ques- tion and the document separately, then builds a codependent representation through coattention. The decoder then produces a start and end point estimate given the coattention. The DCN decoder is dy- namic in the sense that it iteratively estimates the start and end positions, stopping when estimates between iterations converge to the same positions or when a prede\ufb01ned maximum number of it- erations is reached.",
            "The DCN decoder is dy- namic in the sense that it iteratively estimates the start and end positions, stopping when estimates between iterations converge to the same positions or when a prede\ufb01ned maximum number of it- erations is reached. We make two signi\ufb01cant changes to the DCN by introducing a deep residual coattention encoder and a mixed training objective that combines cross entropy loss from maximum likelihood estimation and reinforcement learning rewards from self-critical policy learning. 2.1 DEEP RESIDUAL COATTENTION ENCODER Because it only has a single-layer coattention encoder, the DCN is limited in its ability to compose complex input representations. Vaswani et al. (2017) proposed stacked self-attention modules to facilitate signal traversal. They also showed that the network\u2019s ability to model long-range depen- dencies can be improved by reducing the length of signal paths. We propose two modi\ufb01cations to the coattention encoder to leverage these \ufb01ndings. First, we extend the coattention encoder with self-attention by stacking coattention layers. This allows the network to build richer representations over the input.",
            "We propose two modi\ufb01cations to the coattention encoder to leverage these \ufb01ndings. First, we extend the coattention encoder with self-attention by stacking coattention layers. This allows the network to build richer representations over the input. Second, we merge coattention outputs from each layer with residual connections. This reduces the length of signal paths. Our encoder is shown in Figure 1. Suppose we are given a document of m words and a question of n words. Let LD \u2208Re\u00d7m and LQ \u2208Re\u00d7n respectively denote the word embeddings for the document and the question, where e is the dimension of the word embeddings. We obtain document encodings ED 1 and question 2",
            "encodings EQ 1 through a bidirectional Long Short-Term Memory Network (LSTM) (Hochreiter & Schmidhuber, 1997), where we use integer subscripts to denote the coattention layer number. ED 1 = biLSTM1 \u0000LD\u0001 \u2208Rh\u00d7(m+1) (1) EQ 1 = tanh \u0000W biLSTM1 \u0000LQ\u0001 + b \u0001 \u2208Rh\u00d7(n+1) (2) Here, h denotes the hidden state size and the +1 indicates the presence of an additional sentinel word which allows the coattention to not focus on any part of the input. Like the original DCN, we add a non-linear transform to the question encoding. We compute the af\ufb01nity matrix between the document and the question as A = \u0010 EQ 1 \u0011\u22ba ED 1 \u2208 R(m+1)\u00d7(n+1). Let softmax (X) denote the softmax operation over the matrix X that normalizes X column-wise.",
            "Let softmax (X) denote the softmax operation over the matrix X that normalizes X column-wise. The document summary vectors and question summary vectors are computed as SD 1 = EQ 1 softmax (A\u22ba) \u2208Rh\u00d7(m+1) (3) SQ 1 = ED 1 softmax (A) \u2208Rh\u00d7(n+1) (4) We de\ufb01ne the document coattention context as follows. Note that we drop the dimension corre- sponding to the sentinel vector \u2013 it has already been used during the summary computation and is not a potential position candidate for the decoder. CD 1 = SQ 1 softmax (A\u22ba) \u2208Rh\u00d7m (5) We further encode the summaries using another bidirectional LSTM. ED 2 = biLSTM2 \u0000SD 1 \u0001 \u2208R2h\u00d7m (6) EQ 2 = biLSTM2 \u0010 SQ 1 \u0011 \u2208R2h\u00d7n (7) Equation 3 to equation 5 describe a single coattention layer. We compute the second coattention layer in a similar fashion.",
            "We compute the second coattention layer in a similar fashion. Namely, let coattn denote a multi-valued mapping whose inputs are the two input sequences ED 1 and EQ 1 . We have coattn1 \u0010 ED 1 , EQ 1 \u0011 \u2192 SD 1 , SQ 1 , CD 1 (8) coattn2 \u0010 ED 2 , EQ 2 \u0011 \u2192 SD 2 , SQ 2 , CD 2 (9) The output of our encoder is then obtained as U = biLSTM \u0000concat \u0000ED 1 ; ED 2 ; SD 1 ; SD 2 ; CD 1 ; CD 2 \u0001\u0001 \u2208R2h\u00d7m (10) where concat (A, B) denotes the concatenation between the matrices A and B along the \ufb01rst di- mension. This encoder is different than the original DCN in its depth and its use of residual connections.",
            "This encoder is different than the original DCN in its depth and its use of residual connections. We use not only the output of the deep coattention network CD 2 as input to the \ufb01nal bidirectional LSTM, but add skip connections to initial encodings ED 1 , ED 2 , summary vectors SD 1 , SD 2 , and coattention context CD 1 . This is akin to transformer networks (Vaswani et al., 2017), which achieved state- of-the-art results on machine translation using deep self-attention layers to help model long-range dependencies, and residual networks (He et al., 2016), which achieved state-of-the-art results in image classi\ufb01cation through the addition of skip layer connections to facilitate signal propagation and alleviate gradient degradation. 3",
            "2.2 MIXED OBJECTIVE USING SELF-CRITICAL POLICY LEARNING The DCN produces a distribution over the start position of the answer and a distribution over the end position of the answer. Let s and e denote the respective start and end points of the ground truth answer. Because the decoder of the DCN is dynamic, we denote the start and end distributions produced at the tth decoding step by pstart t \u2208Rm and pend t \u2208Rm. For convenience, we denote the greedy estimate of the start and end positions by the model at the tth decoding step by st and et. Moreover, let \u0398 denote the parameters of the model. Similar to other question answering models, the DCN is supervised using the cross entropy loss on the start position distribution and the end position distribution: lce(\u0398) = \u2212 X t \u0000log pstart t (s | st\u22121, et\u22121; \u0398) + log pend t (e | st\u22121, et\u22121; \u0398) \u0001 (11) Equation 11 states that the model accumulates a cross entropy loss over each position during each decoding step given previous estimates of the start and end positions.",
            "The question answering task consists of two evaluation metrics. The \ufb01rst, exact match, is a binary score that denotes whether the answer span produced by the model has exact string match with the ground truth answer span. The second, F1, computes the degree of word overlap between the answer span produced by the model and the ground truth answer span. We note that there is a disconnect between the cross entropy optimization objective and the evaluation metrics. For example, suppose we are given the answer estimates A and B, neither of which match the ground truth positions. However, A has an exact string match with the ground truth answer whereas B does not. The cross entropy objective penalizes A and B equally, despite the former being correct under both evaluation metrics. In the less extreme case where A does not have exact match but has some degree of word overlap with the ground truth, the F1 metric still prefers A over B despite its wrongly predicted positions. We encode this preference using reinforcement learning, using the F1 score as the reward function.",
            "In the less extreme case where A does not have exact match but has some degree of word overlap with the ground truth, the F1 metric still prefers A over B despite its wrongly predicted positions. We encode this preference using reinforcement learning, using the F1 score as the reward function. Let \u02c6st \u223cpstart t and \u02c6et \u223cpstart t denote the sampled start and end positions from the estimated distributions at decoding step t. We de\ufb01ne a trajectory \u02c6\u03c4 as a sequence of sampled start and end points \u02c6st and \u02c6et through all T decoder time steps. The reinforcement learning objective is then the negative expected rewards R over trajectories.",
            "The reinforcement learning objective is then the negative expected rewards R over trajectories. lrl (\u0398) = \u2212E\u02c6\u03c4\u223cp\u03c4 [R (s, e, \u02c6sT , \u02c6eT ; \u0398)] (12) \u2248 \u2212E\u02c6\u03c4\u223cp\u03c4 [F1 (ans (\u02c6sT,\u02c6eT) , ans (s, e)) \u2212F1 (ans (sT, eT) , ans (s, e))] (13) We use F1 to denote the F1 scoring function and ans (s, e) to denote the answer span retrieved using the start point s and end point e. In equation 13, instead of using only the F1 word overlap as the reward, we subtract from it a baseline. Greensmith et al. (2001) show that a good baseline reduces the variance of gradient estimates and facilitates convergence. In our case, we employ a self-critic (Konda & Tsitsiklis, 1999) that uses the F1 score produced by the current model during greedy inference without teacher forcing.",
            "(2001) show that a good baseline reduces the variance of gradient estimates and facilitates convergence. In our case, we employ a self-critic (Konda & Tsitsiklis, 1999) that uses the F1 score produced by the current model during greedy inference without teacher forcing. For ease of notation, we abbreviate R (s, e, \u02c6sT , \u02c6eT ; \u0398) as R. As per Sutton et al. (1999) and Schul- man et al.",
            "For ease of notation, we abbreviate R (s, e, \u02c6sT , \u02c6eT ; \u0398) as R. As per Sutton et al. (1999) and Schul- man et al. (2015), the expected gradient of a non-differentiable reward function can be computed as \u2207\u0398lrl (\u0398) = \u2212\u2207\u0398 (E\u02c6\u03c4\u223cp\u03c4 [R]) (14) = \u2212E\u02c6\u03c4\u223cp\u03c4 [R\u2207\u0398 log p\u03c4 (\u03c4; \u0398)] (15) = \u2212E\u02c6\u03c4\u223cp\u03c4 \" R\u2207\u0398  T X t \u0000log pstart t (\u02c6st|\u02c6st\u22121, \u02c6et\u22121; \u0398) + log pend t (\u02c6et|\u02c6st\u22121, \u02c6et\u22121; \u0398) \u0001 !# \u2248 \u2212R\u2207\u0398  T X t \u0000log pstart t (\u02c6st|\u02c6st\u22121, \u02c6et\u22121; \u0398) + log pend t (\u02c6et|\u02c6st\u22121, \u02c6et\u22121; \u0398) \u0001 !",
            "(16) 4",
            "Greedy  prediction Evaluator Sampled policy prediction Evaluator Self critic Baseline F1 Policy F1 lossrl sT , eT \u02c6sT , \u02c6eT DCN+ Maximum  likelihood  estimation lossce Task combination loss Figure 2: Computation of the mixed objective. In equation 16, we approximate the expected gradient using a single Monte-Carlo sample \u03c4 drawn from p\u03c4. This sample trajectory \u03c4 contains the start and end positions \u02c6st and \u02c6et sampled during all decoding steps. One of the key problems in applying RL to natural language processing is the discontinuous and discrete space the agent must explore in order to \ufb01nd a good policy. For problems with large ex- ploration space, RL approaches tend to be applied as \ufb01ne-tuning steps after a maximum likelihood model has already been trained (Paulus et al., 2017; Wu et al., 2016). The resulting model is con- strained in its exploration during \ufb01ne-tuning because it is biased by heavy pretraining. We instead treat the optimization problem as a multi-task learning problem.",
            "The resulting model is con- strained in its exploration during \ufb01ne-tuning because it is biased by heavy pretraining. We instead treat the optimization problem as a multi-task learning problem. The \ufb01rst task is to optimize for po- sitional match with the ground truth answer using the the cross entropy objective. The second task is to optimize for word overlap with the ground truth answer with the self-critical reinforcement learning objective. In a similar fashion to Kendall et al. (2017), we combine the two losses using homoscedastic uncertainty as task-dependent weightings. l = 1 2\u03c32ce lce (\u0398) + 1 2\u03c32 rl lrl (\u0398) + log \u03c32 ce + log \u03c32 rl (17) Here, \u03c3ce and \u03c3rl are learned parameters. The gradient of the cross entropy objective can be derived using straight-forward backpropagation. The gradient of the self-critical reinforcement learning objective is shown in equation 16. Figure 2 illustrates how the mixed objective is computed.",
            "The gradient of the cross entropy objective can be derived using straight-forward backpropagation. The gradient of the self-critical reinforcement learning objective is shown in equation 16. Figure 2 illustrates how the mixed objective is computed. In practice, we \ufb01nd that adding the cross entropy task signi\ufb01cantly facilitates policy learning by pruning the space of candidate trajectories - without the former, it is very dif\ufb01cult for policy learning to converge due to the large space of potential answers, documents, and questions. 3 EXPERIMENTS We train and evaluate our model on the Stanford Question Answering Dataset (SQuAD). We show our test performance of our model against other published models, and demonstrate the importance of our proposals via ablation studies on the development set. To preprocess the corpus, we use the reversible tokenizer from Stanford CoreNLP (Manning et al., 2014). For word embeddings, we use GloVe embeddings pretrained on the 840B Common Crawl corpus (Pennington et al., 2014) as well as character ngram embeddings by Hashimoto et al. (2017).",
            "For word embeddings, we use GloVe embeddings pretrained on the 840B Common Crawl corpus (Pennington et al., 2014) as well as character ngram embeddings by Hashimoto et al. (2017). In addition, we concatenate these embeddings with context vectors (CoVe) trained on WMT (McCann et al., 2017). For out of vocabulary words, we set the embeddings and context vectors to zero. We perform word dropout on the document which zeros a word embedding with probability 0.075. In addition, we swap the \ufb01rst maxout layer of the highway maxout network in the DCN decoder with a sparse mixture of experts layer (Shazeer et al., 2017). This layer is similar to the maxout layer, except instead of taking the top scoring expert, we take the top k = 2 expert. The model is trained using ADAM (Kingma & Ba, 5",
            "2014) with default hyperparameters. Hyperparameters of our model are identical to the DCN. We implement our model using PyTorch. 3.1 RESULTS Single Model Dev Single Model Test Ensemble Test Model EM F1 EM F1 EM F1 DCN+ (ours) 74.5% 83.1% 75.1% 83.1% 78.9% 86.0% rnet 72.3% 80.6% 72.3% 80.7% 76.9% 84.0% DCN w\/ CoVe (baseline) 71.3% 79.9% \u2013 \u2013 \u2013 \u2013 Mnemonic Reader 70.1% 79.6% 69.9% 79.2% 73.7% 81.7% Document Reader 69.5% 78.8% 70.0% 79.0% \u2013 \u2013 FastQA 70.3% 78.5% 70.8% 78.9% \u2013 \u2013 ReasoNet \u2013 \u2013 69.1% 78.9% 73.4% 81.",
            "8% 70.0% 79.0% \u2013 \u2013 FastQA 70.3% 78.5% 70.8% 78.9% \u2013 \u2013 ReasoNet \u2013 \u2013 69.1% 78.9% 73.4% 81.8% SEDT 67.9% 77.4% 68.5% 78.0% 73.0% 80.8% BiDAF 67.7% 77.3% 68.0% 77.3% 73.7% 81.5% DCN 65.4% 75.6% 66.2% 75.9% 71.6% 80.4% Table 1: Test performance on SQuAD.",
            "0% 77.3% 73.7% 81.5% DCN 65.4% 75.6% 66.2% 75.9% 71.6% 80.4% Table 1: Test performance on SQuAD. The papers are as follows: rnet (Microsoft Asia Natural Language Computing Group, 2017), SEDT (Liu et al., 2017), BiDAF (Seo et al., 2017), DCN w\/ CoVe (McCann et al., 2017), ReasoNet (Shen et al., 2017), Document Reader (Chen et al., 2017), FastQA (Weissenborn et al., 2017), DCN (Xiong et al., 2017). The CoVe authors did not submit their model, which we use as our baseline, for SQuAD test evaluation. The performance of our model is shown in Table 1. Our model achieves state-of-the-art results on SQuAD dataset with 75.1% exact match accuracy and 83.1% F1.",
            "The performance of our model is shown in Table 1. Our model achieves state-of-the-art results on SQuAD dataset with 75.1% exact match accuracy and 83.1% F1. When ensembled, our model obtains 78.9% exact match accuracy and 86.0% F1. To illustrate the effectiveness of our proposals, we use the DCN with context vectors as a baseline (McCann et al., 2017). This model is identical to the DCN by Xiong et al. (2017), except that it augments the word representations with context vectors trained on WMT16. Comparison to baseline DCN with CoVe. DCN+ outperforms the baseline by 3.2% exact match accuracy and 3.2% F1 on the SQuAD development set. Figure 3 shows the consistent performance gain of DCN+ over the baseline across question types, question lengths, and answer lengths. In particular, DCN+ provides a signi\ufb01cant advantage for long questions.",
            "Figure 3 shows the consistent performance gain of DCN+ over the baseline across question types, question lengths, and answer lengths. In particular, DCN+ provides a signi\ufb01cant advantage for long questions. how other what when where which who why 0.4 0.6 0.8 1.0 F1 baseline DCN w\/ CoVe DCN+ 0 5 10 15 20 25 30 # words in question baseline DCN w\/ CoVe DCN+ 0 4 8 12 16 20 25 # words in answer baseline DCN w\/ CoVe DCN+ Figure 3: Performance comparison between DCN+ and the baseline DCN with CoVe on the SQuAD development set. Ablation study. The contributions of each part of our model are shown in Table 2. We note that the deep residual coattention yielded the highest contribution to model performance, followed by the mixed objective. The sparse mixture of experts layer in the decoder added minor improvements to the model performance. 6",
            "Model EM \u2206EM F1 \u2206F1 DCN+ (ours) 74.5% \u2013 83.1% \u2013 - Deep residual coattention 73.1% -1.4% 81.5% -1.6% - Mixed objective 73.8% -0.7% 82.1% -1.0% - Mixture of experts 74.0% -0.5% 82.4% -0.7% DCN w\/ CoVe (baseline) 71.3% -3.2% 79.9% -3.2% Table 2: Ablation study on the development set of SQuAD. 0 20000 40000 60000 80000 100000120000140000 iterations 0.2 0.4 0.6 0.8 F1 RL train RL dev No RL train No RL dev (a) Entirety of the training curve.",
            "0 20000 40000 60000 80000 100000120000140000 iterations 0.2 0.4 0.6 0.8 F1 RL train RL dev No RL train No RL dev (a) Entirety of the training curve. 0 2000 4000 6000 8000 iterations 0.2 0.4 0.6 F1 RL train RL dev No RL train No RL dev (b) A closeup of the early stages of training. Figure 4: Training curve of DCN+ with and without reinforcement learning. In the latter case, only the cross entropy objective is used. The mixed objective initially performs worse as it begins policy learning from scratch, but quickly outperforms the cross entropy model. Mixed objective convergence. The training curves for DCN+ with reinforcement learning and DCN+ without reinforcement learning are shown in Figure 4 to illustrate the effectiveness of our proposed mixed objective. In particular, we note that without mixing in the cross entropy loss, it is extremely dif\ufb01cult to learn the policy.",
            "The training curves for DCN+ with reinforcement learning and DCN+ without reinforcement learning are shown in Figure 4 to illustrate the effectiveness of our proposed mixed objective. In particular, we note that without mixing in the cross entropy loss, it is extremely dif\ufb01cult to learn the policy. When we combine the cross entropy loss with the rein- forcement learning objective, we \ufb01nd that the model initially performs worse early on as it begins policy learning from scratch (shown in Figure 4b). However, with the addition of cross entropy loss, the model quickly learns a reasonable policy and subsequently outperforms the purely cross entropy model (shown in Figure 4a). Sample predictions. Figure 5 compares predictions by DCN+ and by the baseline on the devel- opment set. Both models retrieve answers that have sensible entity types. For example, the second example asks for \u201cwhat game\u201d and both models retrieve an American football game; the third exam- ple asks for \u201ctype of Turing machine\u201d and both models retrieve a type of turing machine. We \ufb01nd, however, that DCN+ consistently make less mistakes on \ufb01nding the correct entity.",
            "We \ufb01nd, however, that DCN+ consistently make less mistakes on \ufb01nding the correct entity. This is especially apparent in the examples we show, which contain several entities or candidate answers of the correct type. In the \ufb01rst example, Gasquet wrote about the plague and called it \u201cGreat Pestilence\u201d. While he likely did think of the plague as a \u201cgreat pestilence\u201d, the phrase \u201csuggested that it would appear to be some form of ordinary Eastern or bubonic plague\u201d provides evidence for the correct answer \u2013 \u201csome form of ordinary Eastern or bubonic plague\u201d. Similarly, the second example states that Thomas Davis was injured in the \u201cNFC Championship Game\u201d, but the game he insisted on playing in is the \u201cSuper Bowl\u201d. Finally, \u201cmulti-tape\u201d and \u201csingle-tape\u201d both appear in the sentence that provides provenance for the answer to the question. However, it is the \u201csingle-tape\u201d Turing machine that implies quadratic time. In these examples, DCN+ \ufb01nds the correct entity out of ones that have the right type whereas the baseline does not. 4 RELATED WORK Neural models for question answering.",
            "However, it is the \u201csingle-tape\u201d Turing machine that implies quadratic time. In these examples, DCN+ \ufb01nds the correct entity out of ones that have the right type whereas the baseline does not. 4 RELATED WORK Neural models for question answering. Current state-of-the-art approaches for question answer- ing over unstructured text tend to be neural approaches. Wang & Jiang (2017) proposed one of the \ufb01rst conditional attention mechanisms in the Match-LSTM encoder. Coattention (Xiong et al., 2017), bidirectional attention \ufb02ow (Seo et al., 2017), and self-matching attention (Microsoft Asia Natural Language Computing Group, 2017) all build codependent representations of the question and the document. These approaches of conditionally encoding two sequences are widely used in 7",
            "The historian Francis Aidan Gasquet wrote about the 'Great Pestilence' in 1893 and suggested that \"it would appear to  be some form of the ordinary Eastern or bubonic plague\". He was able to adopt the epidemiology of the bubonic plague  for the Black Death for the second edition in 1908, implicating rats and \ufb02eas in the process, and his interpretation was  widely accepted for other ancient and medieval epidemics, such as the Justinian plague that was prevalent in the  Eastern Roman Empire from 541 to 700 CE. What did Gasquet think the plague was? Carolina su\ufb00ered a major setback when Thomas Davis, an 11-year veteran who had already overcome three ACL tears in  his career, went down with a broken arm in the NFC Championship Game. Despite this, he insisted he would still \ufb01nd a  way to play in the Super Bowl. His prediction turned out to be accurate. What game did Thomas Davis say he would play in, despite breaking a bone earlier on? But bounding the computation time above by some concrete function f(n) often yields complexity classes that depend  on the chosen machine model.",
            "His prediction turned out to be accurate. What game did Thomas Davis say he would play in, despite breaking a bone earlier on? But bounding the computation time above by some concrete function f(n) often yields complexity classes that depend  on the chosen machine model. For instance, the language {xx | x is any binary string} can be solved in linear time on a  multi-tape Turing machine, but necessarily requires quadratic time in the model of single-tape Turing machines. If we  allow polynomial variations in running time, Cobham-Edmonds thesis states that \"the time complexities in any two  reasonable and general models of computation are polynomially related\" (Goldreich 2008, Chapter 1.2). This forms the  basis for the complexity class P, which is the set of decision problems solvable by a deterministic Turing machine within  polynomial time. The corresponding set of function problems is FP. A language solved in quadratic time implies the use of what type of Turing machine? Figure 5: Predictions by DCN+ (red) and DCN with CoVe (blue) on the SQuAD development set. question answering.",
            "The corresponding set of function problems is FP. A language solved in quadratic time implies the use of what type of Turing machine? Figure 5: Predictions by DCN+ (red) and DCN with CoVe (blue) on the SQuAD development set. question answering. After building codependent encodings, most models predict the answer by generating the start position and the end position corresponding to the estimated answer span. The generation process utilizes a pointer network (Vinyals et al., 2015) over the positions in the docu- ment. Xiong et al. (2017) also introduced the dynamic decoder, which iteratively proposes answers by alternating between start position and end position estimates, and in some cases is able to recover from initially erroneous predictions. Neural attention models. Neural attention models saw early adoption in machine transla- tion (Bahdanau et al., 2015) and has since become to de-facto architecture for neural machine trans- lation models.",
            "Neural attention models. Neural attention models saw early adoption in machine transla- tion (Bahdanau et al., 2015) and has since become to de-facto architecture for neural machine trans- lation models. Self-attention, or intra-attention, has been applied to language modeling, sentiment analysis, natural language inference, and abstractive text summarization (Chen et al., 2017; Paulus et al., 2017). Vaswani et al. (2017) extended this idea to a deep self-attentional network which ob- tained state-of-the-art results in machine translation. Coattention, which builds codependent repre- sentations of multiple inputs, has been applied to visual question answering (Lu et al., 2016). Xiong et al. (2017) introduced coattention for question answering. Bidirectional attention \ufb02ow (Seo et al., 2017) and self-matching attention (Microsoft Asia Natural Language Computing Group, 2017) also build codependent representations between the question and the document. Reinforcement learning in NLP.",
            "Bidirectional attention \ufb02ow (Seo et al., 2017) and self-matching attention (Microsoft Asia Natural Language Computing Group, 2017) also build codependent representations between the question and the document. Reinforcement learning in NLP. Many tasks in natural language processing have evaluation met- rics that are not differentiable. Dethlefs & Cuay\u00b4ahuitl (2011) proposed a hierarchical reinforcement learning technique for generating text in a simulated way-\ufb01nding domain. Narasimhan et al. (2015) applied deep Q-networks to learn policies for text-based games using game rewards as feedback. Li et al. (2016) introduced a neural conversational model trained using policy gradient methods, whose reward function consisted of heuristics for ease of answering, information \ufb02ow, and semantic coherence. Bahdanau et al. (2017) proposed a general actor-critic temporal-difference method for sequence prediction, performing metric optimization on language modeling and machine transla- tion.",
            "Bahdanau et al. (2017) proposed a general actor-critic temporal-difference method for sequence prediction, performing metric optimization on language modeling and machine transla- tion. Direct word overlap metric optimization has also been applied to summarization (Paulus et al., 2017), and machine translation (Wu et al., 2016). 5 CONCLUSION We introduced DCN+, an state-of-the-art question answering model with deep residual coattention trained using a mixed objective that combines cross entropy supervision with self-critical policy learning. We showed that our proposals improve model performance across question types, question lengths, and answer lengths on the Stanford Question Answering Dataset ( SQuAD). On SQuAD, the DCN+ achieves 75.1% exact match accuracy and 83.1% F1. When ensembled, the DCN+ ob- tains 78.9% exact match accuracy and 86.0% F1. 8",
            "REFERENCES Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In ICLR, 2015. Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron C. Courville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. In ICLR, 2017. Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open- domain questions. In ACL, 2017. Nina Dethlefs and Heriberto Cuay\u00b4ahuitl. Combining hierarchical reinforcement learning and bayesian networks for natural language generation in situated dialogue. In Proceedings of the 13th European Workshop on Natural Language Generation, pp. 110\u2013120. Association for Com- putational Linguistics, 2011. Evan Greensmith, Peter L. Bartlett, and Jonathan Baxter. Variance reduction techniques for gradient estimates in reinforcement learning.",
            "In Proceedings of the 13th European Workshop on Natural Language Generation, pp. 110\u2013120. Association for Com- putational Linguistics, 2011. Evan Greensmith, Peter L. Bartlett, and Jonathan Baxter. Variance reduction techniques for gradient estimates in reinforcement learning. Journal of Machine Learning Research, 5:1471\u20131530, 2001. Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. A joint many-task model: Growing a neural network for multiple NLP tasks. In EMNLP, 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770\u2013 778, 2016. Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9 8: 1735\u201380, 1997. Alex Kendall, Yarin Gal, and Roberto Cipolla.",
            "770\u2013 778, 2016. Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9 8: 1735\u201380, 1997. Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. CoRR, abs\/1705.07115, 2017. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs\/1412.6980, 2014. Vijay R. Konda and John N. Tsitsiklis. Actor-critic algorithms. In NIPS, 1999. Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky. Deep reinforce- ment learning for dialogue generation. In EMNLP, 2016. Rui Liu, Junjie Hu, Wei Wei, Zi Yang, and Eric Nyberg. Structural embedding of syntactic trees for machine comprehension. In ACL, 2017.",
            "Deep reinforce- ment learning for dialogue generation. In EMNLP, 2016. Rui Liu, Junjie Hu, Wei Wei, Zi Yang, and Eric Nyberg. Structural embedding of syntactic trees for machine comprehension. In ACL, 2017. Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical question-image co-attention for visual question answering. In NIPS, 2016. Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David McClosky. The stanford corenlp natural language processing toolkit. In ACL, 2014. Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Contextualized word vectors. In NIPS, 2017. Microsoft Asia Natural Language Computing Group. R-net: Machine reading comprehension with self-matching networks. 2017. Karthik Narasimhan, Tejas D. Kulkarni, and Regina Barzilay.",
            "In NIPS, 2017. Microsoft Asia Natural Language Computing Group. R-net: Machine reading comprehension with self-matching networks. 2017. Karthik Narasimhan, Tejas D. Kulkarni, and Regina Barzilay. Language understanding for text- based games using deep reinforcement learning. In EMNLP, 2015. Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. CoRR, abs\/1705.04304, 2017. Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In EMNLP, 2014. 9",
            "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine comprehension of text. In EMNLP, 2016. John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel. Gradient estimation using stochastic computation graphs. In NIPS, 2015. Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. Bidirectional attention \ufb02ow for machine comprehension. In ICLR, 2017. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In ICLR, 2017. Yelong Shen, Po-Sen Huang, Jianfeng Gao, and Weizhu Chen. Reasonet: Learning to stop reading in machine comprehension.",
            "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In ICLR, 2017. Yelong Shen, Po-Sen Huang, Jianfeng Gao, and Weizhu Chen. Reasonet: Learning to stop reading in machine comprehension. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1047\u20131055. ACM, 2017. Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In NIPS, 1999. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017. Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In NIPS, 2015. Shuohang Wang and Jing Jiang.",
            "Attention is all you need. In NIPS, 2017. Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In NIPS, 2015. Shuohang Wang and Jing Jiang. Machine comprehension using match-lstm and answer pointer. In ICLR, 2017. Dirk Weissenborn, Georg Wiese, and Laura Seiffe. Making neural qa as simple as possible but not simpler. In CoNLL, 2017. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine trans- lation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016. Caiming Xiong, Victor Zhong, and Richard Socher. Dynamic coattention networks for question answering. In ICLR, 2017. 10"
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1711.00106.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 7959.999755859375,
    "avg_doclen_est": 165.8333282470703
}
