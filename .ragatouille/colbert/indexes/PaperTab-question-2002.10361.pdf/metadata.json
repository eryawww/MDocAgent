{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Multilingual Twitter Corpus and Baselines for Evaluating Demographic Bias in Hate Speech Recognition Xiaolei Huang1\u2217, Linzi Xing2, Franck Dernoncourt3, Michael J. Paul1 1.University of Colorado Boulder, 2. University of British Columbia 3. Adobe Research 1. {xiaolei.huang, mpaul}@colorado.edu, 2. lzxing@cs.ubc.ca 3. dernonco@adobe.com Abstract Existing research on fairness evaluation of document classi\ufb01cation models mainly uses synthetic monolingual data without ground truth for author demographic attributes. In this work, we assemble and publish a multilingual Twitter corpus for the task of hate speech detection with inferred four author demographic factors: age, country, gender and race/ethnicity. The corpus covers \ufb01ve languages: English, Italian, Polish, Portuguese and Spanish. We evaluate the inferred demographic labels with a crowdsourcing platform, Figure Eight. To examine factors that can cause biases, we take an empirical analysis of demographic predictability on the English corpus.",
      "The corpus covers \ufb01ve languages: English, Italian, Polish, Portuguese and Spanish. We evaluate the inferred demographic labels with a crowdsourcing platform, Figure Eight. To examine factors that can cause biases, we take an empirical analysis of demographic predictability on the English corpus. We measure the performance of four popular document classi\ufb01ers and evaluate the fairness and bias of the baseline classi\ufb01ers on the author-level demographic attributes. Keywords: demographic bias, fairness, multilingual, document classi\ufb01cation, hate speech 1. Introduction While document classi\ufb01cation models should be objective and independent from human biases in documents, research have shown that the models can learn human biases and therefore be discriminatory towards particular demographic groups (Dixon et al., 2018; Borkan et al., 2019; Sun et al., 2019b). The goal of fairness-aware document classi\ufb01ers is to train and build non-discriminatory models towards peo- ple no matter what their demographic attributes are, such as gender and ethnicity.",
      "The goal of fairness-aware document classi\ufb01ers is to train and build non-discriminatory models towards peo- ple no matter what their demographic attributes are, such as gender and ethnicity. Existing research (Dixon et al., 2018; Kiritchenko and Mohammad, 2018; Park et al., 2018; Garg et al., 2019; Borkan et al., 2019) in evaluating fairness of document classi\ufb01ers focus on the group fairness (Choulde- chova and Roth, 2018), which refers to every demographic group has equal probability of being assigned to the posi- tive predicted document category. However, the lack of original author demographic attributes and multilingual corpora bring challenges towards the fair- ness evaluation of document classi\ufb01ers. First, the datasets commonly used to build and evaluate the fairness of doc- ument classi\ufb01ers obtain derived synthetic author demo- graphic attributes instead of the original author information.",
      "First, the datasets commonly used to build and evaluate the fairness of doc- ument classi\ufb01ers obtain derived synthetic author demo- graphic attributes instead of the original author information. The common data sources either derive from Wikipedia toxic comments (Dixon et al., 2018; Park et al., 2018; Garg et al., 2019) or synthetic document templates (Kir- itchenko and Mohammad, 2018; Park et al., 2018). The Wikipedia Talk corpus1 (Wulczyn et al., 2017) provides de- mographic information of annotators instead of the authors, Equity Evaluation Corpus2 (Kiritchenko and Mohammad, 2018) are created by sentence templates and combinations of racial names and gender coreferences. While existing work (Davidson et al., 2019; Diaz et al., 2018) infers user demographic information (white/black, young/old) from the text, such inference is still likely to cause confound- The work was partially done when the \ufb01rst author worked as an intern at Adobe Research.",
      "1 https://figshare.com/articles/Wikipedia_ Detox_Data/4054689 2 http://saifmohammad.com/WebPages/ Biases-SA.html ing errors that impact and break the independence between demographic factors and the fairness evaluation of text clas- si\ufb01ers. Second, existing research in the fairness evalua- tion mainly focus on only English resources, such as age biases in blog posts (Diaz et al., 2018), gender biases in Wikipedia comments (Dixon et al., 2018) and racial biases in hate speech detection (Davidson et al., 2019). Differ- ent languages have shown different patterns of linguistic variations across the demographic attributes (Johannsen et al., 2015; Huang and Paul, 2019), methods (Zhao et al., 2017; Park et al., 2018) to reduce and evaluate the demo- graphic bias in English corpora may not apply to other lan- guages.",
      "For example, Spanish has gender-dependent nouns, but this does not exist in English (Sun et al., 2019b); and Portuguese varies across Brazil and Portugal in both word usage and grammar (Maier and G\u00f3mez-Rodr\u00edguez, 2014). The rich variations have not been explored under the fair- ness evaluation due to lack of multilingual corpora. Ad- ditionally, while we have hate speech detection datasets in multiple languages (Waseem and Hovy, 2016; Sanguinetti et al., 2018; Ptaszynski et al., 2019; Basile et al., 2019; For- tuna et al., 2019), there is still no integrated multilingual corpora that contain author demographic attributes which can be used to measure group fairness. The lack of author demographic attributes and multilingual datasets limits re- search for evaluating classi\ufb01er fairness and developing un- biased classi\ufb01ers.",
      "The lack of author demographic attributes and multilingual datasets limits re- search for evaluating classi\ufb01er fairness and developing un- biased classi\ufb01ers. In this study, we combine previously published cor- pora labeled for Twitter hate speech recognition in En- glish (Waseem and Hovy, 2016; Waseem, 2016; Founta et al., 2018), Italian (Sanguinetti et al., 2018), Pol- ish (Ptaszynski et al., 2019), Portuguese (Fortuna et al., 2019), and Spanish (Basile et al., 2019), and publish this multilingual data augmented with author-level demo- graphic information for four attributes: race, gender, age and country. The demographic factors are inferred from user pro\ufb01les, which are independent from text documents, the tweets. To our best knowledge, this is the \ufb01rst multilin- gual hate speech corpus annotated with author attributes aiming for fairness evaluation. We start with presenting col- arXiv:2002.10361v2  [cs.CL]  3 Mar 2020",
      "lection and inference steps of the datasets. Next, we take an exploratory study on the language variations across demo- graphic groups on the English dataset. We then experiment with four multiple classi\ufb01cation models to establish base- line levels of this corpus. Finally, we evaluate the fairness performance of those document classi\ufb01ers. 2. Data We assemble the annotated datasets for hate speech clas- si\ufb01cation. To narrow down the data sources, we limit our dataset sources to the unique online social media site, Twit- ter. We have requested 16 published Twitter hate speech datasets, and \ufb01nally obtained 7 of them in \ufb01ve languages.",
      "To narrow down the data sources, we limit our dataset sources to the unique online social media site, Twit- ter. We have requested 16 published Twitter hate speech datasets, and \ufb01nally obtained 7 of them in \ufb01ve languages. By using the Twitter streaming API3, we collected the tweets annotated by hate speech labels and their corre- sponding user pro\ufb01les in English (Waseem and Hovy, 2016; Waseem, 2016; Founta et al., 2018), Italian (Sanguinetti et al., 2018), Polish (Ptaszynski et al., 2019), Portuguese (For- tuna et al., 2019), and Spanish (Basile et al., 2019). We binarize all tweets\u2019 labels (indicating whether a tweet has indications of hate speech), allowing to merge the different label sets and reduce the data sparsity. Whether a tweet is considered hate speech heavily depends on who the speaker is; for example, whether a racial slur is intended as hate speech depends in part on the speaker\u2019s race (Waseem and Hovy, 2016).",
      "Whether a tweet is considered hate speech heavily depends on who the speaker is; for example, whether a racial slur is intended as hate speech depends in part on the speaker\u2019s race (Waseem and Hovy, 2016). Therefore, hate speech classi\ufb01ers may not generalize well across all groups of peo- ple, and disparities in the detection offensive speech could lead to bias in content moderation (Shen et al., 2018). Our contribution is to further annotate the data with user demo- graphic attributes inferred from their public pro\ufb01les, thus creating a corpus suitable for evaluating author-level fair- ness for this hate speech recognition task across multiple languages. 2.1. User Attribute Inference We consider four user factors of age, race, gender and geo- graphic location. For location, we inference two granular- ities, country and US region, but only experiment with the country attribute. While the demographic attributes can be inferred through tweets (Volkova et al., 2015; Davidson et al., 2019), we intentionally exclude the contents from the tweets if they infer these user attributes, in order to make the evaluation of fairness more reliable and independent.",
      "While the demographic attributes can be inferred through tweets (Volkova et al., 2015; Davidson et al., 2019), we intentionally exclude the contents from the tweets if they infer these user attributes, in order to make the evaluation of fairness more reliable and independent. If users were grouped based on attributes inferred from their text, then any differences in text classi\ufb01cation across those groups could be related to the same text. Instead, we infer attributes from public user pro\ufb01le information (i.e., descrip- tion, name and photo). Age, Race, Gender. We infer these attributes from each user\u2019s pro\ufb01le image by using Face++ (https://www. faceplusplus.com/), a computer vision API that pro- vides estimates of demographic characteristics. Empiri- cal comparisons of facial recognition APIs have found that Face++ is the most accurate tool on Twitter data (Jung et al., 2018) and works comparatively better for darker skins (Buolamwini and Gebru, 2018). For the gender, we choose the binary categories (male/female) by the predicted 3 https://developer.twitter.com/ probabilities.",
      "For the gender, we choose the binary categories (male/female) by the predicted 3 https://developer.twitter.com/ probabilities. We map the racial outputs into four cate- gories: Asian, Black, Latino and White. We only keep users that appear to be at least 13 years old, and we save the \ufb01rst result from the API if multiple faces are identi\ufb01ed. We experiment and evaluate with binarization of race and age with roughly balanced distributions (white and nonwhite, \u2264 median vs. elder age) to consider a simpli\ufb01ed setting across different languages, since race is harder to infer accurately. Country. The country-level language variations can bring challenges that are worth to explore. We extract ge- olocation information from users whose pro\ufb01les contained either numerical location coordinates or a well-formatted (matching a regular expression) location name. We fed the extracted values to the Google Maps API (https: //maps.googleapis.com) to obtain structured loca- tion information (city, state, country).",
      "We fed the extracted values to the Google Maps API (https: //maps.googleapis.com) to obtain structured loca- tion information (city, state, country). We \ufb01rst count the main country source and then binarize the country to indi- cate if a user is in the main country or not. For example, the majority of users in the English are from the United States (US), therefore, we can binarize the country attributes to indicate if the users are in the US or not. 2.2. Corpus Summary We show the corpus statistics in Table 1 and summarize the full demographic distributions in Table 2. The binary de- mographic attributes (age, country, gender, race) can bring several bene\ufb01ts. First, we can create comparatively bal- anced label distributions. We can observe that there are differences in the race and gender among Italian and Polish data, while other attributes across the other languages show comparably balanced demographic distributions. Second, we can reduce errors inferred from the Face++ on coarse la- bels. Third, it is more convenient for us to analyze, conduct experiments and evaluate the group fairness of document classi\ufb01ers.",
      "Second, we can reduce errors inferred from the Face++ on coarse la- bels. Third, it is more convenient for us to analyze, conduct experiments and evaluate the group fairness of document classi\ufb01ers. Language Users Docs Tokens HS Ratio English 64,067 83,077 20.066 .370 Italian 3,810 5,671 19.721 .195 Polish 86 10,919 14.285 .089 Portuguese 600 1,852 18.494 .205 Spanish 4,600 4,831 19.199 .397 Table 1: Statistical summary of multilingual corpora across English, Italian, Polish, Portuguese and Spanish. We present number of users (Users), documents (Docs), and average tokens per document (Tokens) in the corpus, plus the label distribution (HS Ratio, percent of documents la- beled positive for hate speech). Table 1 presents different patterns of the corpus. The Pol- ish data has the smallest users. This is because the data focuses on the people who own the most popular accounts in the Polish data (Ptaszynski et al., 2019), the other data collected tweets randomly.",
      "Table 1 presents different patterns of the corpus. The Pol- ish data has the smallest users. This is because the data focuses on the people who own the most popular accounts in the Polish data (Ptaszynski et al., 2019), the other data collected tweets randomly. And the dataset shows a much more sparse distribution of the hate speech label than the other languages. Table 2 presents different patterns of the user attributes. En- glish, Portuguese and Spanish users are younger than the Italian and Polish users in the collected data. And both Ital- ian and Polish show more skewed demographic distribu-",
      "Language Age Country Gender Race English Mean Median US non-US Female Male White non-White 32.041 29 .599 .401 .499 .501 .505 .495 Italian Mean Median Italy non-Italy Female Male White non-White 44.518 43 .778 .222 .307 .692 .981 .018 Polish Mean Median Poland non-Poland Female Male White non-White 39.245 38 .795 .205 .324 .676 .895 .105 Portuguese Mean Median Brazil non-Brazil Female Male White non-White 29.635 26 .437 .563 .569 .431 .508 .492 Spanish Mean Median Spain non-Spain Female Male White non-White 31.911 27 .339 .661 .463 .537 .549 .451 Table 2: Statistical summary of user attributes in age, country, gender and race. For the age, we present both mean and median values in case of outliers. For the other attributes, we show binary distributions. tions in country, gender and race, while the other datasets show more balanced distributions. 2.3.",
      "For the age, we present both mean and median values in case of outliers. For the other attributes, we show binary distributions. tions in country, gender and race, while the other datasets show more balanced distributions. 2.3. Demographic Inference Accuracy Image-based approaches will have inaccuracies, as a per- son\u2019s demographic attributes cannot be conclusively deter- mined merely from their appearance. However, given the dif\ufb01culty in obtaining ground truth values, we argue that automatically inferred attributes can still be informative for studying classi\ufb01er fairness. If a classi\ufb01er performs signi\ufb01- cantly differently across different groups of users, then this shows that the classi\ufb01er is biased along certain groupings, even if those groupings are not perfectly aligned with the actual attributes they are named after. This subsection tries to quantify how reliably these groupings correspond to the demographic variables.",
      "This subsection tries to quantify how reliably these groupings correspond to the demographic variables. Age Race Gender Annotator Agreement Face++ .80 .80 .98 Accuracy English .86 .90 .94 Italian .82 .96 .98 Polish .88 .96 .98 Portuguese .82 .78 .92 Spanish .76 .82 .90 Overall .828 .884 .944 Table 3: Annotator agreement (percentage overlap) and evaluation accuracy for Face++. Prior research found that Face++ achieves 93.0% and 92.0% accuracy on gender and ethnicity evaluations (Jung et al., 2018). We further conduct a small evaluation on the hate speech corpus by a small sample of annotated user pro- \ufb01le photos providing a rough estimate of accuracy while acknowledging that our annotations are not ground truth. We obtained the annotations from the crowdsourcing web- site, Figure Eight (https://figure-eight.com/). We randomly sampled 50 users whose attributes came from Face++ in each language. We anonymize the user pro- \ufb01les and feed the information to the crowdsourcing website. Three annotators annotated each user photo with the binary demographic categories.",
      "We randomly sampled 50 users whose attributes came from Face++ in each language. We anonymize the user pro- \ufb01les and feed the information to the crowdsourcing website. Three annotators annotated each user photo with the binary demographic categories. To select quali\ufb01ed annotators and ensure quality of the evaluations, we set up 5 golden stan- dard annotation questions for each language. The annota- tors can join the evaluation task only by passing the golden standard questions. We decide demographic attributes by majority votes and present evaluation results in Table 3. Our \ufb01nal evaluations show that overall the Face++ achieves averaged accuracy scores of 82.8%, 88.4% and 94.4% for age, race and gender respectively. 2.4. Privacy Considerations To facilitate the study of classi\ufb01cation fairness, we will publicly distribute this anonymized corpus with the inferred demographic attributes including both original and bina- rized versions.",
      "2.4. Privacy Considerations To facilitate the study of classi\ufb01cation fairness, we will publicly distribute this anonymized corpus with the inferred demographic attributes including both original and bina- rized versions. To preserve user privacy, we will not pub- licize the personal pro\ufb01le information, including user ids, photos, geocoordinates as well as other user pro\ufb01le in- formation, which were used to infer the demographic at- tributes. We will, however, provide inferred demographic attributes in their original formats from the Face++ and Google Maps based on per request to allow wider re- searchers and communities to replicate the methodology and probe more depth of fairness in document classi\ufb01ca- tion. 3. Language Variations across Demographic Groups Demographic factors can improve the performances of doc- ument classi\ufb01ers (Hovy, 2015), and demographic variations root in language, especially in social media data (Volkova et al., 2013; Hovy, 2015).",
      "Language Variations across Demographic Groups Demographic factors can improve the performances of doc- ument classi\ufb01ers (Hovy, 2015), and demographic variations root in language, especially in social media data (Volkova et al., 2013; Hovy, 2015). For example, language styles are highly correlated with authors\u2019 demographic attributes, such as age, race, gender and location (Coulmas, 2017; Preo\u00b8tiuc-Pietro and Ungar, 2018). Research (Bolukbasi et al., 2016; Zhao et al., 2017; Garg et al., 2018) \ufb01nd that biases and stereotypes exist in word embeddings, which is widely used in document classi\ufb01cation tasks. For example, \u201creceptionist\u201d is closer to females while \u201cprogrammer\u201d is closer to males, and \u201cprofessor\u201d is closer to Asian Ameri- cans while \u201chousekeeper\u201d is closer to Hispanic Americans. This motivates us to explore and test if the language varia- tions hold in our particular dataset, how strong the effects are.",
      "This motivates us to explore and test if the language varia- tions hold in our particular dataset, how strong the effects are. We conduct the empirical analysis of demographic pre- dictability on the English dataset. 3.1. Are Demographic Factors Predictable in Documents? We examine how accurately the documents can predict au- thor demographic attributes from three different levels:",
      "Demographic Attributes Top 10 Features of Demographic Attribute Prediction Race White nigga, fucking, ass, bro, damn, niggas, sir, moive, melon, bitches Other abuse, gg, feminism, wadhwa, feminists, uh, freebsd, feminist, ve, blocked Gender Female rent, driving, tho, adorable, met, presented, yoga, stressed, awareness, me Male idiot, the, players, match, idiots, sir, fucking, nigga, bro, trump Table 4: Top 10 predictable features of race and gender in the English dataset. 1. Word-level. We extract TF-IDF-weighted 1-, 2-grams features. 2. POS-level. We use Tweebo parser (Kong et al., 2014) to tag and extract POS features. We count the POS tag and then normalize the counts for each document. 3. Topic-level. We train a Latent Dirichlet Alloca- tion (Blei et al., 2003) model with 20 topics using Gensim (Rehurek and Sojka, 2010) with default pa- rameters.",
      "3. Topic-level. We train a Latent Dirichlet Alloca- tion (Blei et al., 2003) model with 20 topics using Gensim (Rehurek and Sojka, 2010) with default pa- rameters. Then a document can be represented as a probabilistic distribution over the 20 topics. We shuf\ufb02e and split data into training (70%) and test (30%) sets. Three logistic classi\ufb01ers are trained by the three levels of features separately. We measure the prediction accuracy and show the absolute improvements in Figure 1. age country gender race Demographic Factors pos topic word Features 11.5 5.0 8.1 6.4 13.7 11.9 9.4 9.6 16.1 16.7 11.7 11.7 Figure 1: Predictability of demographic attributes from the English data. We show the absolute percentage im- provements in accuracy over majority-class baselines. The majority-class baselines of accuracy are .500 for the binary predictions.",
      "We show the absolute percentage im- provements in accuracy over majority-class baselines. The majority-class baselines of accuracy are .500 for the binary predictions. The darker color indicates higher improve- ments and vice versa. The improved prediction accuracy scores over majority baselines suggest that language variations across demo- graphic groups are encoded in the text documents. The re- sults show that documents are the most predictable to the age attribute. We can also observe that the word is the most predictable feature to demographic factors, while the POS feature is least predictable towards the country fac- tor. These suggest there might be a connection between language variations and demographic groups. This moti- vates us to further explore the language variations based on word features. We rank the word features by mutual infor- mation classi\ufb01cation (Pedregosa et al., 2011) and present the top 10 unigram features in Table 4. The qualitative re- sults show the most predictable word features towards the demographic groups and suggest such variations may im- pact extracted feature representations and further training fair document classi\ufb01ers.",
      "The qualitative re- sults show the most predictable word features towards the demographic groups and suggest such variations may im- pact extracted feature representations and further training fair document classi\ufb01ers. The Table 4 shows that when classifying hate speech tweets, the n-words and b-words are more signi\ufb01cant cor- related with the white instead of the other racial groups. However, this shows an opposite view than the existing work (Davidson et al., 2019), which presents the two types of words are more signi\ufb01cantly correlated with the black. This can highlight the values of our approach that to avoid confounding errors, we obtain author demographic infor- mation independently from the user generated documents. 4. Experiments Demographic variations root in documents, especially in social media data (Volkova et al., 2013; Hovy, 2015; Jo- hannsen et al., 2015). Such variations could further impact the performance and fairness of document classi\ufb01ers.",
      "Experiments Demographic variations root in documents, especially in social media data (Volkova et al., 2013; Hovy, 2015; Jo- hannsen et al., 2015). Such variations could further impact the performance and fairness of document classi\ufb01ers. In this study, we experiment four different classi\ufb01cation mod- els including logistic regression (LR), recurrent neural net- work (RNN) (Chung et al., 2014), convolutional neural net- work (CNN) (Kim, 2014) and Google BERT (Devlin et al., 2019). We present the baseline results of both performance and fairness evaluations across the multilingual corpus. 4.1. Data Preprocessing To anonymize user information, we hash user and tweet ids and then replace hyperlinks, usernames, and hashtags with generic symbols (URL, USER, HASHTAG). Docu- ments are lowercased and tokenized using NLTK (Bird and Loper, 2004). The corpus is randomly split into training (70%), development (15%), and test (15%) sets.",
      "Docu- ments are lowercased and tokenized using NLTK (Bird and Loper, 2004). The corpus is randomly split into training (70%), development (15%), and test (15%) sets. We train the models on the training set and \ufb01nd the optimal hyper- parameters on the development set before \ufb01nal evaluations on the test set. We randomly shuf\ufb02e the training data at the beginning of each training epoch. 4.2. Baseline Models We implement and experiment four baseline classi\ufb01cation models. To compare fairly, we keep the feature size up to 15K for each classi\ufb01er across all \ufb01ve languages. We cal- culate the weight for each document category by N Nl (King and Zeng, 2001), where N is the number of documents in each language and Nl is the number of documents labeled by the category. Particularly, for training BERT model, we append two additional tokens, \u201c[CLS]\u201d and \u201c[SEP]\u201d, at the start and end of each document respectively.",
      "Particularly, for training BERT model, we append two additional tokens, \u201c[CLS]\u201d and \u201c[SEP]\u201d, at the start and end of each document respectively. For the neu- ral models, we pad each document or drop rest of words up to 40 tokens. We use \u201cunknown\u201d as a replacement for un- known tokens. We initialize CNN and RNN classi\ufb01ers by pre-trained word embeddings (Mikolov et al., 2013; Godin et al., 2015; Bojanowski et al., 2017; Deriu et al., 2017) and train the networks up to 10 epochs. LR. We \ufb01rst extract TF-IDF-weighted features of uni- , bi-, and tri-grams on the corpora, using the most fre-",
      "Language Method Acc F1-w F1-m AUC English LR .874 .874 .841 .920 CNN .878 .877 .845 .927 RNN .898 .896 .867 .938 BERT .705 .635 .579 .581 Language Method Acc F1-w F1-m AUC Italian LR .660 .679 .631 .725 CNN .687 .702 .651 .745 RNN .729 .731 .666 .763 BERT .697 .629 .468 .498 Language Method Acc F1-w F1-m AUC Polish LR .864 .846 .653 .804 CNN .855 .851 .688 .813 RNN .857 .854 .696 .822 BERT .824 .782 .478 .474 Language Method Acc F1-w F1-m AUC Portuguese LR .660 .598 .551 .648 CNN .681 .674 .653 .719 RNN .607 .586 .553 .633 BERT .613 .568 .525 .524 Language Method Acc F1-w F1-m AUC Spanish LR .704 .707 .698 .761 CNN .650 .654 .645 .710 RNN .674 .674 .658 .720 BERT .605 .573 .502 .",
      "586 .553 .633 BERT .613 .568 .525 .524 Language Method Acc F1-w F1-m AUC Spanish LR .704 .707 .698 .761 CNN .650 .654 .645 .710 RNN .674 .674 .658 .720 BERT .605 .573 .502 .505 Table 5: Overall performance evaluation of baseline classi\ufb01ers. We evaluate overall performance by four metrics including accuracy (Acc), weighted F1 score (F1-w), macro F1 score (F1-m) and area under the ROC curve (AUC). The higher score indicates better performance. We highlight models achieve the best performance in each column. quent 15K features with the minimum feature frequency as 2. We then train a LogisticRegression from scikit- learn (Pedregosa et al., 2011). We use \u201cliblinear\u201d as the solver function and leave the other parameters as default. CNN. We implement the Convolutional Neural Network (CNN) classi\ufb01er described in (Kim, 2014; Zimmerman et al., 2018) by Keras (Chollet and others, 2015).",
      "CNN. We implement the Convolutional Neural Network (CNN) classi\ufb01er described in (Kim, 2014; Zimmerman et al., 2018) by Keras (Chollet and others, 2015). We \ufb01rst ap- ply 100 \ufb01lters with three different kernel sizes, 3, 4 and 5. After the convolution operations, we feed the concatenated features to a fully connected layer and output document representations with 100 dimensions. We apply \u201csoftplus\u201d function with a l2 regularization with .03 and a dropout rate with .3 in the dense layer. The model feeds the document representation to \ufb01nal prediction. We train the model with batch size 64, set model optimizer as Adam (Kingma and Ba, 2014) and calculate loss values by the cross entropy function. We keep all other parameter settings as described in the paper (Kim, 2014). RNN. We build a recurrent neural network (RNN) clas- si\ufb01er by using bi-directional Gated Recurrent Unit (bi- GRU) (Chung et al., 2014; Park et al., 2018).",
      "RNN. We build a recurrent neural network (RNN) clas- si\ufb01er by using bi-directional Gated Recurrent Unit (bi- GRU) (Chung et al., 2014; Park et al., 2018). We set the output dimension of GRU as 200 and apply a dropout on the output with rate .2. We optimize the RNN with RM- Sprop (Tieleman and Hinton, 2012) and use the same loss function and batch size as the CNN model. We leave the other parameters as default in the Keras (Chollet and oth- ers, 2015). BERT BERT is a transformer-based pre-trained language model which was well trained on multi-billion sentences publicly available on the web (Devlin et al., 2019), which can effectively generate the precise text semantics and use- ful signals. We implement a BERT-based classi\ufb01cation model by HuggingFace\u2019s Transformers (Wolf et al., 2019). The model encodes each document into a \ufb01xed size (768) of representation and feed to a linear prediction layer.",
      "We implement a BERT-based classi\ufb01cation model by HuggingFace\u2019s Transformers (Wolf et al., 2019). The model encodes each document into a \ufb01xed size (768) of representation and feed to a linear prediction layer. The model is optimized by AdamW with a warmup and learning rate as .1 and 2e\u22125 respectively. We leave parameters as their default, conduct \ufb01ne-tuning steps with 4 epochs and set batch size as 32 (Sun et al., 2019a). The classi\ufb01cation model loads \u201cbert-base-uncased\u201d pre-trained BERT model for English and \u201cbert-base-multilingual-uncased\u201d multilin- gual BERT model (Gertner et al., 2019) for the other lan- guages. The multilingual BERT model follows the same method of BERT by using Wikipedia text from the top 104 languages. Due to the label imbalance shown in Table 1, we balance training instances by randomly oversampling the minority during the training process. 4.3. Evaluation Metrics Performance Evaluation.",
      "The multilingual BERT model follows the same method of BERT by using Wikipedia text from the top 104 languages. Due to the label imbalance shown in Table 1, we balance training instances by randomly oversampling the minority during the training process. 4.3. Evaluation Metrics Performance Evaluation. To measure overall perfor- mance, we evaluate models by four metrics: accuracy (Acc), weighted F1 score (F1-w), macro F1 score (F1- m) and area under the ROC curve (AUC). The F1 score coherently combines both precision and recall by 2 \u2217 precision\u2217recall precision+recall. We report F1-m considering that the datasets are imbalanced. Fairness Evaluation. To evaluate group fairness, we measure the equality differences (ED) of true posi- tive/negative and false positive/negative rates for each de- mographic factor. ED is a standard metric to evaluate fair- ness and bias of document classi\ufb01ers (Dixon et al., 2018; Park et al., 2018; Garg et al., 2019).",
      "ED is a standard metric to evaluate fair- ness and bias of document classi\ufb01ers (Dixon et al., 2018; Park et al., 2018; Garg et al., 2019). This metric sums the differences between the rates within speci\ufb01c user groups and the overall rates. Taking the false positive rate (FPR) as an example, we calculate the equality difference by: FPED = X d\u2208D |FPRd \u2212FPR| , where D is a demographic factor (e.g., race) and d is a demographic group (e.g., white or nonwhite). 5. Results We have presented our evaluation results of performance and fairness in Table 5 and Table 6 respectively. Country",
      "and race have very skewed distributions in the Italian and Polish corpora, therefore, we omit fairness evaluation on the two factors. Overall performance evaluation. Table 5 demonstrates the performances of the baseline classi\ufb01ers for hate speech classi\ufb01cation on the corpus we proposed. Results are obtained from the \ufb01ve languages covered in our corpus respectively. Among the four baseline classi\ufb01ers, LR, CNN and RNN consistently perform well on all languages. Moreover, neural-based models (CNN and RNN) substan- tially outperform LR on four out of \ufb01ve languages (except Spanish). However, the results obtained by BERT are rela- tively lower than the other baselines, and show more signif- icant gap in the English dataset. One possible explanation is BERT was pre-trained on Wikipedia documents, which are signi\ufb01cantly different from the Twitter corpus in docu- ment length, word usage and grammars. For example, each tweet is a short document with 20 tokens, but the BERT is trained on long documents up to 512 tokens.",
      "For example, each tweet is a short document with 20 tokens, but the BERT is trained on long documents up to 512 tokens. Existing re- search suggests that \ufb01ne-tuning on the multilingual corpus can further improve performance of BERT models (Sun et al., 2019a). Group fairness evaluation. We have measured the group fairness in Table 6. Generally, the RNN classi\ufb01er achieves better and more stable performance across major fairness evaluation tasks. By comparing the different baseline clas- si\ufb01ers, we can \ufb01nd out that the LR usually show stronger biases than the neural classi\ufb01cation models among major- ity of the tasks. While the BERT classi\ufb01er performs com- paratively lower accuracy and F1 scores, the classi\ufb01er has less biases on the most of the datasets. However, biases can signi\ufb01cantly increases for the Portuguese dataset when the BERT classi\ufb01er achieves better performance.",
      "However, biases can signi\ufb01cantly increases for the Portuguese dataset when the BERT classi\ufb01er achieves better performance. We examine the relationship by building linear model between two dif- ferences: the performance differences between the RNN and other classi\ufb01ers, the SUM-ED differences between RNN and other classi\ufb01ers. We \ufb01nd that the classi\ufb01cation performance does not have signi\ufb01cantly (p \u2212value > .05) correlation with fairness and bias. The signi\ufb01cant biases of classi\ufb01ers varies across tasks and languages: the classi\ufb01ers trained on Polish and Italian are biased the most by Age and Gender, the classi\ufb01ers trained on Spanish and Portuguese are most biased the most by Country, and the classi\ufb01ers trained on English tweets are the most unbiased through- out all the attributes. Classi\ufb01ers usually have very high bias scores on both gender and age in Italian and Polish data. We \ufb01nd that the age and gender both have very skewed dis- tributions in the Italian and Polish datasets.",
      "Classi\ufb01ers usually have very high bias scores on both gender and age in Italian and Polish data. We \ufb01nd that the age and gender both have very skewed dis- tributions in the Italian and Polish datasets. Overall, our baselines provide a promising start for evaluating future new methods of reducing demographic biases for document classi\ufb01cation under the multilingual setting. 6. Conclusion In this paper, we propose a new multilingual dataset cover- ing four author demographic annotations (age, gender, race and country) for the hate speech detection task. We show the experimental results of several popular classi\ufb01cation models in both overall and fairness performance evalua- tions. Our empirical exploration indicates that language variations across demographic groups can lead to biased classi\ufb01ers. This dataset can be used for measuring fairness of document classi\ufb01ers along author-level attributes and exploring bias factors across multilingual settings and mul- tiple user factors. The proposed framework for inferring the author demographic attributes can be used to generate more large-scale datasets or even applied to other social media sites (e.g., Amazon and Yelp).",
      "The proposed framework for inferring the author demographic attributes can be used to generate more large-scale datasets or even applied to other social media sites (e.g., Amazon and Yelp). While we encode the demographic attributes into categories in this work, we will provide inferred probabilities of the demographic attributes from Face++ to allow for broader research exploration. Our code, anonymized data and data state- ment (Bender and Friedman, 2018) will be publicly avail- able at https://github.com/xiaoleihuang/ Multilingual_Fairness_LREC. 6.1. Limitations While our dataset provides new information on author de- mographic attributes, and our analysis suggest directions toward reducing bias, a number of limitations must be ac- knowledged in order to appropriately interpret our \ufb01ndings. First, inferring user demographic attributes by pro\ufb01le in- formation can be risky due to the accuracy of the infer- ence toolkit. In this work, we present multiple strategies to reduce the errors bringing by the inference toolkits, such as human evaluation, manually screening and using exter- nal public pro\ufb01le information (Instagram).",
      "In this work, we present multiple strategies to reduce the errors bringing by the inference toolkits, such as human evaluation, manually screening and using exter- nal public pro\ufb01le information (Instagram). However, we cannot guarantee perfect accuracy of the demographic at- tributes, and, errors in the attributes may themselves be \u201cunfair\u201d or unevenly distributed due to bias in the infer- ence tools (Buolamwini and Gebru, 2018). Still, obtaining individual-level attributes is an important step toward un- derstanding classi\ufb01er fairness, and our results found biases across these groupings of users, even if some of the group- ings contained errors. Second, because methods for inferring demographic at- tributes are not accurate enough to provide \ufb01ne-grained information, our attribute categories are still too coarse- grained (binary age groups and gender, and only four race categories). Using coarse-grained attributes would hide the identities of speci\ufb01c demographic groups, including other racial minorities and people with non-binary gender. Broadening our analyses and evaluations to include more attribute values may require better methods of user attribute inference or different sources of data.",
      "Using coarse-grained attributes would hide the identities of speci\ufb01c demographic groups, including other racial minorities and people with non-binary gender. Broadening our analyses and evaluations to include more attribute values may require better methods of user attribute inference or different sources of data. Third, language variations across demographic groups might introduce annotation biases. Existing research (Sap et al., 2019) shows that annotators are more likely to an- notate tweets containing African American English words as hate speech. Additionally, the nationality and educa- tional level might also impact on the quality of annota- tions (Founta et al., 2018). Similarly, different annotation sources of our dataset (which merged two different corpora) might have variations in annotating schema. To reduce an- notation biases due to the different annotating schema, we merge the annotations into the two most compatible docu- ment categories: normal and hate speech. Annotation bi- ases might still exist, therefore, we will release our original anonymized multilingual dataset for research communities.",
      "Age Language Method FNED FPED SUM-ED English LR .059 .104 .163 CNN .052 .083 .135 RNN .041 .118 .159 BERT .004 .012 .016 Gender Language Method FNED FPED SUM-ED English LR .023 .056 .079 CNN .018 .056 .074 RNN .013 .055 .068 BERT .007 .009 .016 Language Method FNED FPED SUM-ED Italian LR .076 .194 .270 CNN .003 .211 .214 RNN .042 .185 .227 BERT .029 .034 .063 Language Method FNED FPED SUM-ED Italian LR .145 .020 .165 CNN .064 .094 .158 RNN .088 .075 .163 BERT .041 .056 .097 Language Method FNED FPED SUM-ED Polish LR .256 .059 .315 CNN .389 .138 .527 RNN .335 .089 .424 BERT .027 .027 .054 Language Method FNED FPED SUM-ED Polish LR .266 .045 .309 CNN .411 .048 .459 RNN .340 .034 .374 BERT .042 .013 .055 Language Method FNED FPED SUM-ED Portuguese LR .061 .",
      "424 BERT .027 .027 .054 Language Method FNED FPED SUM-ED Polish LR .266 .045 .309 CNN .411 .048 .459 RNN .340 .034 .374 BERT .042 .013 .055 Language Method FNED FPED SUM-ED Portuguese LR .061 .044 .105 CNN .033 .096 .129 RNN .079 .045 .124 BERT .090 .097 .187 Language Method FNED FPED SUM-ED Portuguese LR .052 .007 .059 CNN .018 .013 .031 RNN .099 .083 .182 BERT .055 .125 .180 Language Method FNED FPED SUM-ED Spanish LR .089 .013 .102 CNN .117 .139 .256 RNN .078 .083 .161 BERT .052 .015 .067 Language Method FNED FPED SUM-ED Spanish LR .131 .061 .292 CNN .032 .108 .140 RNN .030 .039 .069 BERT .021 .016 .037 Country Language Method FNED FPED SUM-ED English LR .026 .053 .079 CNN .027 .063 .090 RNN .024 .061 .085 BERT .006 .001 .",
      "292 CNN .032 .108 .140 RNN .030 .039 .069 BERT .021 .016 .037 Country Language Method FNED FPED SUM-ED English LR .026 .053 .079 CNN .027 .063 .090 RNN .024 .061 .085 BERT .006 .001 .007 Race Language Method FNED FPED SUM-ED English LR .019 .056 .075 CNN .007 .029 .036 RNN .008 .063 .071 BERT .003 .009 .012 Language Method FNED FPED Portuguese LR .093 .026 .119 CNN .110 .122 .232 RNN .022 .004 .026 BERT .073 .071 .144 Language Method FNED FPED SUM-ED Portuguese LR .068 .005 .073 CNN .056 .033 .089 RNN .074 .054 .128 BERT .045 .186 .231 Language Method FNED FPED SUM-ED Spanish LR .152 .154 .306 CNN .089 .089 .178 RNN .071 .113 .184 BERT .017 .017 .034 Language Method FNED FPED SUM-ED Spanish LR .095 .030 .125 CNN .072 .054 .126 RNN .011 .004 .",
      "152 .154 .306 CNN .089 .089 .178 RNN .071 .113 .184 BERT .017 .017 .034 Language Method FNED FPED SUM-ED Spanish LR .095 .030 .125 CNN .072 .054 .126 RNN .011 .004 .015 BERT .046 .005 .051 Table 6: Fairness evaluation of baseline classi\ufb01ers across the \ufb01ve languages on the four demographic factors. We measure fairness and bias of document classi\ufb01ers by equality differences of false negative rate (FNED), false positive rate (FPED) and sum of FNED and FPED (SUM-ED). The higher score indicates lower fairness and higher bias and vice versa.",
      "7. Acknowledgement The authors thank the anonymous reviews for their insight- ful comments and suggestions. This work was supported in part by the National Science Foundation under award num- ber IIS-1657338. This work was also supported in part by a research gift from Adobe. 8. Bibliographical References Basile, V., Bosco, C., Fersini, E., Nozza, D., Patti, V., Rangel Pardo, F. M., Rosso, P., and Sanguinetti, M. (2019). SemEval-2019 task 5: Multilingual detection of hate speech against immigrants and women in twitter. In Proceedings of the 13th International Workshop on Semantic Evaluation, pages 54\u201363, Minneapolis, Min- nesota, USA, June. ACL. Bender, E. M. and Friedman, B. (2018). Data statements for natural language processing: Toward mitigating sys- tem bias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587\u2013604. Bird, S. and Loper, E. (2004).",
      "(2018). Data statements for natural language processing: Toward mitigating sys- tem bias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587\u2013604. Bird, S. and Loper, E. (2004). Nltk: the natural language toolkit. In Proceedings of the ACL 2004 on Interactive poster and demonstration sessions, page 31. Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent dirichlet allocation. Journal of Machine Learning Re- search, 3(Jan):993\u20131022. Bojanowski, P., Grave, E., Joulin, A., and Mikolov, T. (2017). Enriching word vectors with subword informa- tion. Transactions of the Association for Computational Linguistics, 5:135\u2013146. Bolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V., and Kalai, A. T. (2016). Man is to computer programmer as woman is to homemaker?",
      "Bolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V., and Kalai, A. T. (2016). Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In NIPS, pages 4349\u20134357. Borkan, D., Dixon, L., Sorensen, J., Thain, N., and Vasser- man, L. (2019). Nuanced metrics for measuring un- intended bias with real data for text classi\ufb01cation. In WWW. Buolamwini, J. and Gebru, T. (2018). Gender shades: In- tersectional accuracy disparities in commercial gender classi\ufb01cation. In Conference on Fairness, Accountabil- ity and Transparency, pages 77\u201391. Chollet, F. et al. (2015). Keras. https://keras.io. Chouldechova, A. and Roth, A. (2018). The fron- tiers of fairness in machine learning.",
      "Chollet, F. et al. (2015). Keras. https://keras.io. Chouldechova, A. and Roth, A. (2018). The fron- tiers of fairness in machine learning. arXiv preprint arXiv:1810.08810. Chung, J., Gulcehre, C., Cho, K., and Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. In NIPS 2014 Workshop on Deep Learning. Coulmas, F. (2017). Sociolinguistics: the study of speakers choice; second edition. Cambridge University Press. Davidson, T., Bhattacharya, D., and Weber, I. (2019). Racial bias in hate speech and abusive language detec- tion datasets. In Proceedings of the Third Workshop on Abusive Language Online, pages 25\u201335. ACL.",
      "Davidson, T., Bhattacharya, D., and Weber, I. (2019). Racial bias in hate speech and abusive language detec- tion datasets. In Proceedings of the Third Workshop on Abusive Language Online, pages 25\u201335. ACL. Deriu, J., Lucchi, A., De Luca, V., Severyn, A., M\u00fcller, S., Cieliebak, M., Hofmann, T., and Jaggi, M. (2017). Leveraging large amounts of weakly supervised data for multi-language sentiment classi\ufb01cation. In WWW, WWW \u201917, pages 1045\u20131052, Republic and Canton of Geneva, Switzerland. International World Wide Web Conferences Steering Committee. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of deep bidirectional trans- formers for language understanding. In NAACL, pages 4171\u20134186, Minneapolis, Minnesota, June. ACL.",
      "(2019). BERT: Pre-training of deep bidirectional trans- formers for language understanding. In NAACL, pages 4171\u20134186, Minneapolis, Minnesota, June. ACL. Diaz, M., Johnson, I., Lazar, A., Piper, A. M., and Ger- gle, D. (2018). Addressing age-related bias in sentiment analysis. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, pages 412:1\u2013 412:14. Dixon, L., Li, J., Sorensen, J., Thain, N., and Vasserman, L. (2018). Measuring and mitigating unintended bias in text classi\ufb01cation. In AIES, pages 67\u201373. Fortuna, P., Rocha da Silva, J., Soler-Company, J., Wanner, L., and Nunes, S. (2019). A hierarchically-labeled Por- tuguese hate speech dataset. In Proceedings of the Third Workshop on Abusive Language Online, pages 94\u2013104.",
      "(2019). A hierarchically-labeled Por- tuguese hate speech dataset. In Proceedings of the Third Workshop on Abusive Language Online, pages 94\u2013104. Founta, A. M., Djouvas, C., Chatzakou, D., Leontiadis, I., Blackburn, J., Stringhini, G., Vakali, A., Sirivianos, M., and Kourtellis, N. (2018). Large scale crowdsourc- ing and characterization of twitter abusive behavior. In ICWSM. Garg, N., Schiebinger, L., Jurafsky, D., and Zou, J. (2018). Word embeddings quantify 100 years of gender and ethnic stereotypes. National Academy of Sciences, 115:E3635\u2013E3644. Garg, S., Perot, V., Limtiaco, N., Taly, A., Chi, E. H., and Beutel, A. (2019). Counterfactual fairness in text classi- \ufb01cation through robustness. In AIES.",
      "Garg, S., Perot, V., Limtiaco, N., Taly, A., Chi, E. H., and Beutel, A. (2019). Counterfactual fairness in text classi- \ufb01cation through robustness. In AIES. Gertner, A., Henderson, J., Merkhofer, E., Marsh, A., Well- ner, B., and Zarrella, G. (2019). MITRE at SemEval- 2019 task 5: Transfer learning for multilingual hate speech detection. In Proceedings of the 13th Interna- tional Workshop on Semantic Evaluation, pages 453\u2013 459, Minneapolis, Minnesota, USA, June. ACL. Godin, F., Vandersmissen, B., De Neve, W., and Van de Walle, R. (2015). Multimedia lab @ ACL WNUT NER shared task: Named entity recognition for twitter mi- croposts using distributed word representations. In Pro- ceedings of the Workshop on Noisy User-generated Text, pages 146\u2013153, Beijing, China, July. ACL.",
      "Multimedia lab @ ACL WNUT NER shared task: Named entity recognition for twitter mi- croposts using distributed word representations. In Pro- ceedings of the Workshop on Noisy User-generated Text, pages 146\u2013153, Beijing, China, July. ACL. Hovy, D. (2015). Demographic factors improve classi\ufb01- cation performance. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis- tics, pages 752\u2013762. Huang, X. and Paul, M. J. (2019). Neural user factor adaptation for text classi\ufb01cation: Learning to general- ize across author demographics. In Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics, pages 46\u201356. Johannsen, A., Hovy, D., and S\u00f8gaard, A. (2015). Cross- lingual syntactic variation over age and gender. In Pro- ceedings of the Nineteenth Conference on Computa- tional Natural Language Learning, pages 103\u2013112, Bei- jing, China, July. ACL.",
      "(2015). Cross- lingual syntactic variation over age and gender. In Pro- ceedings of the Nineteenth Conference on Computa- tional Natural Language Learning, pages 103\u2013112, Bei- jing, China, July. ACL. Jung, S.-G., An, J., Kwak, H., Salminen, J., and Jansen, B. J. (2018). Assessing the accuracy of four popular face",
      "recognition tools for inferring gender, age, and race. In ICWSM. Kim, Y. (2014). Convolutional neural networks for sen- tence classi\ufb01cation. In Proceedings of the 2014 Confer- ence on Empirical Methods in Natural Language Pro- cessing (EMNLP), pages 1746\u20131751, Doha, Qatar, Oc- tober. ACL. King, G. and Zeng, L. (2001). Logistic regression in rare events data. Political analysis, 9(2):137\u2013163. Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Kiritchenko, S. and Mohammad, S. (2018). Examining gender and race bias in two hundred sentiment analysis systems. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages 43\u201353.",
      "Kiritchenko, S. and Mohammad, S. (2018). Examining gender and race bias in two hundred sentiment analysis systems. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages 43\u201353. Kong, L., Schneider, N., Swayamdipta, S., Bhatia, A., Dyer, C., and Smith, N. A. (2014). A dependency parser for tweets. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1001\u20131012. Maier, W. and G\u00f3mez-Rodr\u00edguez, C. (2014). Language va- riety identi\ufb01cation in Spanish tweets. In Proceedings of the EMNLP\u20192014 Workshop on Language Technology for Closely Related Languages and Language Variants, pages 25\u201335, Doha, Qatar, October. ACL. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. (2013). Distributed representations of words and phrases and their compositionality.",
      "ACL. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111\u20133119. Park, J. H., Shin, J., and Fung, P. (2018). Reducing gender bias in abusive language detection. In Proceedings of the 2018 Conference on EMNLP, pages 2799\u20132804. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., et al. (2011). Scikit-learn: Ma- chine learning in python. Journal of machine learning research, 12(Oct):2825\u20132830. Preo\u00b8tiuc-Pietro, D. and Ungar, L. (2018). User-level race and ethnicity predictors from twitter text.",
      "Journal of machine learning research, 12(Oct):2825\u20132830. Preo\u00b8tiuc-Pietro, D. and Ungar, L. (2018). User-level race and ethnicity predictors from twitter text. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1534\u20131545. Ptaszynski, M., Pieciukiewicz, A., and Dyba\u0142a, P. (2019). Results of the poleval 2019 shared task 6: First dataset and open shared task for automatic cyberbullying detec- tion in polish twitter. In Proceedings of the PolEval2019 Workshop, page 89. Rehurek, R. and Sojka, P. (2010). Software framework for topic modelling with large corpora. In In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks. Sanguinetti, M., Poletto, F., Bosco, C., Patti, V., and Stranisci, M. (2018). An Italian twitter corpus of hate speech against immigrants.",
      "Sanguinetti, M., Poletto, F., Bosco, C., Patti, V., and Stranisci, M. (2018). An Italian twitter corpus of hate speech against immigrants. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan, May. European Language Resources Association (ELRA). Sap, M., Card, D., Gabriel, S., Choi, Y., and Smith, N. A. (2019). The risk of racial bias in hate speech detection. In Proceedings of the 57th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 1668\u20131678, July. Shen, Q., Yoder, M., Jo, Y., and Rose, C. (2018). Percep- tions of censorship and moderation bias in political de- bate forums. In ICWSM. Sun, C., Qiu, X., Xu, Y., and Huang, X. (2019a). How to \ufb01ne-tune bert for text classi\ufb01cation?",
      "In ICWSM. Sun, C., Qiu, X., Xu, Y., and Huang, X. (2019a). How to \ufb01ne-tune bert for text classi\ufb01cation? arXiv preprint arXiv:1905.05583. Sun, T., Gaut, A., Tang, S., Huang, Y., ElSherief, M., Zhao, J., Mirza, D., Belding, E., Chang, K.-W., and Wang, W. Y. (2019b). Mitigating gender bias in natural lan- guage processing: Literature review. In Proceedings of the 57th Annual Meeting of the Association for Compu- tational Linguistics, pages 1630\u20131640. Tieleman, T. and Hinton, G. (2012). Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26\u201331. Volkova, S., Wilson, T., and Yarowsky, D. (2013).",
      "COURSERA: Neural networks for machine learning, 4(2):26\u201331. Volkova, S., Wilson, T., and Yarowsky, D. (2013). Explor- ing demographic language variations to improve multi- lingual sentiment analysis in social media. In Proceed- ings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1815\u20131827. Volkova, S., Bachrach, Y., Armstrong, M., and Sharma, V. (2015). Inferring latent user properties from texts pub- lished in social media. In AAAI. Waseem, Z. and Hovy, D. (2016). Hateful symbols or hate- ful people? predictive features for hate speech detec- tion on twitter. In Proceedings of the NAACL student research workshop, pages 88\u201393. Waseem, Z. (2016). Are you a racist or am i seeing things? annotator in\ufb02uence on hate speech detection on twitter.",
      "In Proceedings of the NAACL student research workshop, pages 88\u201393. Waseem, Z. (2016). Are you a racist or am i seeing things? annotator in\ufb02uence on hate speech detection on twitter. In Proceedings of the \ufb01rst workshop on NLP and com- putational social science, pages 138\u2013142. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtow- icz, M., and Brew, J. (2019). Huggingface\u2019s transform- ers: State-of-the-art natural language processing. ArXiv, abs/1910.03771. Wulczyn, E., Thain, N., and Dixon, L. (2017). Ex machina: Personal attacks seen at scale. In WWW, pages 1391\u20131399. Zhao, J., Wang, T., Yatskar, M., Ordonez, V., and Chang, K.-W.",
      "(2017). Ex machina: Personal attacks seen at scale. In WWW, pages 1391\u20131399. Zhao, J., Wang, T., Yatskar, M., Ordonez, V., and Chang, K.-W. (2017). Men also like shopping: Reducing gen- der bias ampli\ufb01cation using corpus-level constraints. In Proceedings of the 2017 Conference on Empirical Meth- ods in Natural Language Processing. Zimmerman, S., Kruschwitz, U., and Fox, C. (2018). Im- proving hate speech detection with deep learning ensem- bles. In Proceedings of the Eleventh International Con- ference on Language Resources and Evaluation (LREC 2018)."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-2002.10361.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":11557,
  "avg_doclen":175.1060606061,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-2002.10361.pdf"
    }
  }
}