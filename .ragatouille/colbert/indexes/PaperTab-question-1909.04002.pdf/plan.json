{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "The Trumpiest Trump? Identifying a Subject\u2019s Most Characteristic Tweets Charuta Pethe Department of Computer Science, Stony Brook University, NY, USA cpethe@cs.stonybrook.edu Steven Skiena Department of Computer Science, Stony Brook University, NY, USA skiena@cs.stonybrook.edu Abstract The sequence of documents produced by any given author varies in style and content, but some documents are more typical or represen- tative of the source than others. We quantify the extent to which a given short text is char- acteristic of a speci\ufb01c person, using a dataset of tweets from \ufb01fteen celebrities. Such analy- sis is useful for generating excerpts of high- volume Twitter pro\ufb01les, and understanding how representativeness relates to tweet popu- larity. We \ufb01rst consider the related task of bi- nary author detection (is x the author of text T?), and report a test accuracy of 90.37% for the best of \ufb01ve approaches to this problem. We then use these models to compute characteri- zation scores among all of an author\u2019s texts.",
            "), and report a test accuracy of 90.37% for the best of \ufb01ve approaches to this problem. We then use these models to compute characteri- zation scores among all of an author\u2019s texts. A user study shows human evaluators agree with our characterization model for all 15 celebri- ties in our dataset, each with p-value < 0.05. We use these classi\ufb01ers to show surprisingly strong correlations between characterization scores and the popularity of the associated texts. Indeed, we demonstrate a statistically signi\ufb01cant correlation between this score and tweet popularity (likes\/replies\/retweets) for 13 of the 15 celebrities in our study. 1 Introduction Social media platforms, particularly microblog- ging services such as Twitter, have become in- creasingly popular (Statista, 2019) as a means to express thoughts and opinions. Twitter users emit tweets about a wide variety of topics, which vary in the extent to which they re\ufb02ect a user\u2019s person- ality, brand and interests.",
            "Twitter users emit tweets about a wide variety of topics, which vary in the extent to which they re\ufb02ect a user\u2019s person- ality, brand and interests. This observation mo- tivates the question we consider here, of how to quantify the degree to which tweets are character- istic of their author? People who are familiar with a given author ap- pear to be able to make such judgments con\ufb01- dently. For example, consider the following pair of tweets written by US President Donald Trump, at the extreme sides of our characterization scores (0.9996 vs. 0.0013) for him: Tweet 1: Thank you for joining us at the Lin- coln Memorial tonight- a very special evening! Together, we are going to MAKE AMERICA GREAT AGAIN! Tweet 2: \u201cThe bend in the road is not the end of the road unless you refuse to take the turn.\u201d - Anonymous Although both these tweets are from the same account, we assert that Tweet 1 sounds more char- acteristic of Donald Trump than Tweet 2. We might also guess that the \ufb01rst is more popular than second.",
            "We might also guess that the \ufb01rst is more popular than second. Indeed, Tweet 1 received 155,000 likes as opposed to only 234 for Tweet 2. Such an author characterization score has many possible applications. With the ability to identify the most\/least characteristic tweets from a person, we can generate reduced excerpts for high-volume Twitter pro\ufb01les. Similarly, identifying the least characteristic tweets can highlight unusual content or suspicious activity. A run of suf\ufb01ciently unrep- resentative tweets might be indicative that a hacker has taken control of a user\u2019s account. But more fundamentally, our work provides the necessary tool to study the question of how \u201ccharacteristic-ness\u201d or novelty are related to tweet popularity. Do tweets that are more char- acteristic of the user get more likes, replies and retweets? Is such a relationship universal, or does it depend upon the personality or domain of the author? Twitter users with a large follower base can employ our methods to understand how char- acteristic a new potential tweet sounds, and obtain an estimate of how popular it is likely to become.",
            "Is such a relationship universal, or does it depend upon the personality or domain of the author? Twitter users with a large follower base can employ our methods to understand how char- acteristic a new potential tweet sounds, and obtain an estimate of how popular it is likely to become. To answer these questions, we formally de\ufb01ne the problem of author representativeness testing, arXiv:1909.04002v1  [cs.CL]  9 Sep 2019",
            "and model the task as a binary classi\ufb01cation prob- lem. Our primary contributions in this paper in- clude: \u2022 Five approaches to authorship veri\ufb01ca- tion: As a proxy for the question of repre- sentativeness testing (which has no convinc- ing source of ground truth without extensive human annotation), we consider the task of distinguishing tweets written by a given au- thor from others they did not write. We com- pare \ufb01ve distinct computational approaches to such binary tweet classi\ufb01cation (user vs. non-user). Our best model achieves a test ac- curacy of 90.37% over a dataset of 15 Twit- ter celebrities. We use the best performing model to compute a score (the probability of authorship), which quanti\ufb01es how character- istic of the user a given tweet is. \u2022 Human evaluation study: To verify that our results are in agreement with human judgment of how \u2018characteristic\u2019 a tweet is, we ask human evaluators which of a pair of tweets sounds more characteristic of the given celebrity.",
            "\u2022 Human evaluation study: To verify that our results are in agreement with human judgment of how \u2018characteristic\u2019 a tweet is, we ask human evaluators which of a pair of tweets sounds more characteristic of the given celebrity. The human evaluators are in agreement with our model 70.40% of the time, signi\ufb01cant above the 0.05 level for each of our 15 celebrities. \u2022 Correlation analysis for popularity: Our characterization score exhibits strikingly high absolute correlation with popularity (likes, replies and retweets), despite the fact that tweet text is the only feature used to train the classi\ufb01er which yields these scores. Figure 1: Plot of log mean number of likes against tweet score percentile for Donald Trump and Justin Bieber. Node color denotes the year for which the max- imum number of tweets are present in each percentile bucket, demonstrating that this is not merely a temporal correlation. For 13 of the 15 celebrities in our dataset, we observe a statistically signi\ufb01cant correla- tion between characterization score and pop- ularity. Figure 1 shows the relation between tweet score and tweet popularity for Donald Trump and Justin Bieber respectively.",
            "For 13 of the 15 celebrities in our dataset, we observe a statistically signi\ufb01cant correla- tion between characterization score and pop- ularity. Figure 1 shows the relation between tweet score and tweet popularity for Donald Trump and Justin Bieber respectively. The \ufb01gure shows that the sign of this associa- tion differs for various celebrities, re\ufb02ecting whether their audience seeks novelty or rein- forcement. \u2022 Iterative sampling for class imbalance: Our task requires distinguishing a user\u2019s tweets (perhaps 1,000 positive training exam- ples) from the sea of all other user\u2019s tweets (implying billions of possible negative train- ing examples). We present an iterative sam- pling technique to exploit this class imbal- ance, which improves the test accuracy for negative examples by 2.62%. 2 Problem Formulation We formally de\ufb01ne the author representativeness problem as follows: Input: A Twitter author U and the collection of their tweets, and a new tweet T. Problem: Compute score(T, U), the probability that T was written by U. This score quanti\ufb01es how characteristic of writer U, tweet T is.",
            "2.1 Methodology In order to obtain this representativeness score, we model our task as a classi\ufb01cation problem, where we seek to distinguish tweets from U against tweets from all other users. By modeling this as a binary classi\ufb01cation prob- lem, it becomes possible to quantify how charac- teristic of a writer a tweet is, as a probability im- plied by its distance from the decision boundary. Thus, we obtain a characterization score between 0 and 1 for each tweet. Challenges: In training a classi\ufb01er to distinguish between user and non-user tweets, we should ide- ally have an equal amount of examples of both classes. User tweets are simply all the tweets from that user\u2019s Twitter account, and measure perhaps in the thousands. Indeed, the number of tweets per user per day is limited to 2400 per day by current Twitter policy (https:\/\/help.twitter.com\/en\/rules- and-policies\/twitter-limits). The negative exam- ples consist of all tweets written by other Twitter",
            "users, a total of approximately 500 million per day (https:\/\/business.twitter.com). Thus there is an ex- treme class imbalance between user and non-user tweets. Moreover, the nature of language used on Twitter does not conform to formal syntactic or se- mantic rules. The sentences tend to be highly un- structured, and the vocabulary is not restricted to a particular dictionary. 2.2 Data For the binary classi\ufb01cation task described in Sec- tion 2.1, we term tweets from U as positive ex- amples, and tweets from other users as negative examples. \u2022 Positive examples: We take tweets written by 15 celebrities from various domains, from 01-Jan-2008 to 01-Dec-2018, as positive ex- amples. Properties of these Twitter celebri- ties are provided in Table 1. \u2022 Negative examples: We have col- lected 1% of tweets from Twitter\u2019s daily feed using the Twitter API (https:\/\/developer.twitter.com\/en\/docs.html) to use as negative examples. User Tweet count Domain Foll.",
            "\u2022 Negative examples: We have col- lected 1% of tweets from Twitter\u2019s daily feed using the Twitter API (https:\/\/developer.twitter.com\/en\/docs.html) to use as negative examples. User Tweet count Domain Foll. (Before) (After) Amitabh Bachchan 49437 10342 Acting 37.0 Ariana Grande 37738 13657 Music 62.3 Barack Obama 11350 6772 Politics 106.0 Bill Gates 2699 1754 Business 47.1 Donald Trump 35549 18295 Politics 59.9 Ellen DeGeneres 17317 9616 TV 77.6 J K Rowling 6037 2634 Author 14.7 Jimmy Fallon 10698 3596 TV 51.1 Justin Bieber 18044 5193 Acting 105.0 Kevin Durant 22532 4146 Sports 17.5 Kim Kardashian 24541 7943 Modeling 60.5 Lady Gaga 7239 3767 Music 78.5 LeBron James 5145 2102 Sports 42.6 Narendra Modi 17613 6672 Politics 47.0 Oprah Winfrey 11685 4588 TV 42.2 Table 1: Twitter celebrities in our dataset, with tweet counts before and after \ufb01ltering (Foll.",
            "denotes follow- ers in millions) Preprocessing and Filtering: We have prepro- cessed and \ufb01ltered the data to remove tweets that are unrepresentative or too short for analysis. All text has been converted to lowercase, and stripped of punctuation marks and URLs. This is because our approaches are centered around word usage. However, in future models, punctuation may prove effective as a feature. Further, we restrict analy- sis to English language tweets containing no at- tached images. We select only tweets which are more than 10 words long, and contain at least 5 le- gitimate (dictionary) English words. We de\ufb01ne an unedited transfer of an original tweet as a retweet, and remove these from our dataset. Since com- ments on retweets are written by the user them- selves, we retain these in our dataset. We note that celebrity Twitter accounts can be handled by PR agencies, in addition to the owner themselves. Because our aim is to characterize Twitter pro\ufb01les as entities, we have not attempted to distinguish between user-written and agency- written tweets. However, this is an interesting di- rection for future research.",
            "Because our aim is to characterize Twitter pro\ufb01les as entities, we have not attempted to distinguish between user-written and agency- written tweets. However, this is an interesting di- rection for future research. We use a train-test split of 70-30% on the posi- tive examples, and generate negative training and test sets of the same sizes for each user, by ran- domly sampling from the large set of negative ex- amples. 3 Related work 3.1 Author identi\ufb01cation and veri\ufb01cation The challenge of author identi\ufb01cation has a long history in NLP. PAN 2013 (Juola and Stamatatos, 2013) introduced the question: \u201cGiven a set of documents by the same author, is an additional (out-of-set) document also by that author?\u201d The corpus is comprised of text pieces from text- books, newspaper articles, and \ufb01ction. Submis- sions to PAN 2014 (Stamatatos et al., 2014) also model authorship veri\ufb01cation as binary classi\ufb01- cation, by using non-author documents as nega- tive examples.",
            "Submis- sions to PAN 2014 (Stamatatos et al., 2014) also model authorship veri\ufb01cation as binary classi\ufb01- cation, by using non-author documents as nega- tive examples. The best submission (Seidman, 2013) in PAN 2013 uses the General Impostors (GI) method, which is a modi\ufb01cation of the Im- postors Method (Koppel et al., 2012). The best submission (Khonji and Iraqi, 2014) in PAN 2014 presents a modi\ufb01cation of the GI method. These methods are based on the impostors framework (Koppel and Winter, 2014). Veenman and Li (2013) used compression dis- tance as a document representation, for authorship veri\ufb01cation in PAN 2013. HB et al. (2015) present a global feature extraction approach and achieve state-of-the-art accuracy for the PAN 2014 corpus.",
            "HB et al. (2015) present a global feature extraction approach and achieve state-of-the-art accuracy for the PAN 2014 corpus. The best submission (Bagnall, 2015) in PAN 2015 (Stamatatos et al., 2015) uses a character-level RNN model for author identi\ufb01cation, in which each author is represented as a sub-model, and the",
            "recurrent layer is shared by all sub-models. This is useful if the number of authors is \ufb01xed, and the problem is modeled as multi-class classi\ufb01cation. Mohsen et al. (2016) also approach multi-class au- thor identi\ufb01cation, using deep learning for feature extraction, and Nirkhi et al. (2016) using hierar- chical clustering. Potha and Stamatatos (2018) propose an intrin- sic pro\ufb01le-based veri\ufb01cation method that uses la- tent semantic indexing (LSI), which is effective for longer texts. Koppel and Schler (2004) and Luyckx and Daelemans (2008) explore methods for authorship veri\ufb01cation for larger documents such as essays and novels. Nizamani and Memon (2013) and Brocardo et al. (2013) explore author identi\ufb01cation for emails, and Chen and Sun (2017) for scienti\ufb01c papers. Azarbonyad et al.",
            "Nizamani and Memon (2013) and Brocardo et al. (2013) explore author identi\ufb01cation for emails, and Chen and Sun (2017) for scienti\ufb01c papers. Azarbonyad et al. (2015) make use of temporal changes in word usage to identify authors of tweets and emails. Fissette (2010), Green and Sheppard (2013), and Zhang et al. (2014) evaluate the utility of various features for this task. Stamatatos (2008) proposes text sam- pling to address the lack of text samples of undis- puted authorship, to produce a desirable distribu- tion over classes. Koppel et al. (2009) compare methods for vari- ants of the authorship attribution problem. Bhar- gava et al. (2013) apply stylometric analysis to tweets to determine the author. L\u00b4opez-Monroy et al. (2015) propose a document representation capturing discriminative and subpro\ufb01le-speci\ufb01c information of terms. Rocha et al.",
            "(2013) apply stylometric analysis to tweets to determine the author. L\u00b4opez-Monroy et al. (2015) propose a document representation capturing discriminative and subpro\ufb01le-speci\ufb01c information of terms. Rocha et al. (2016) review methods for authorship attribution for social me- dia forensics. Peng et al. (2016a) use bit-level n- grams for determining authorship for online news. Peng et al. (2016b) apply this method to detect as- trotur\ufb01ng on social media. The\u00b4ophilo et al. (2019) employ deep learning speci\ufb01cally for authorship attribution of short messages. 3.2 Predicting tweet popularity Suh et al. (2010) leverages features such as URL, number of hashtags, number of followers and fol- lowees etc. in a generalized linear model, to predict the number of retweets. Naveed et al. (2011) extend this approach to perform content- based retweet prediction using several features in- cluding sentiments, emoticons, punctuations etc.",
            "in a generalized linear model, to predict the number of retweets. Naveed et al. (2011) extend this approach to perform content- based retweet prediction using several features in- cluding sentiments, emoticons, punctuations etc. Bandari et al. (2012) apply the same approach for regression as well as classi\ufb01cation, to predict the number of retweets speci\ufb01cally for news ar- ticles. Zaman et al. (2014) present a Bayesian model for retweet prediction using early retweet times, retweets of other tweets, and the user\u2019s fol- lower graph. Tan et al. (2014) analyze whether different wording of a tweet by the same author af- fects its popularity. SEISMIC (Zhao et al., 2015) and PSEISMIC (Chen and Li, 2017) are statistical methods to predict the \ufb01nal number of retweets. Zhang et al.",
            "SEISMIC (Zhao et al., 2015) and PSEISMIC (Chen and Li, 2017) are statistical methods to predict the \ufb01nal number of retweets. Zhang et al. (2018) approach retweet prediction as a multi-class classi\ufb01cation problem, and present a feature-weighted model, where weights are com- puted using information gain. 3.3 Training with imbalanced datasets Various methods to handle imbalanced datasets have been described by Kotsiantis et al. (2006). These include undersampling (Kotsiantis and Pin- telas, 2003), oversampling, and feature selection (Zheng et al., 2004) at the data level. However, due to random undersampling, potentially useful samples can be discarded, while random oversam- pling poses the risk of over\ufb01tting. This problem can be handled at the algorithmic level as well: the threshold method (Weiss, 2004) produces sev- eral classi\ufb01ers by varying the threshold of the clas- si\ufb01er score.",
            "This problem can be handled at the algorithmic level as well: the threshold method (Weiss, 2004) produces sev- eral classi\ufb01ers by varying the threshold of the clas- si\ufb01er score. One-class classi\ufb01cation can be per- formed using a divide-and-conquer approach, to iteratively build rules to cover new training in- stances (Cohen, 1995). Cost-sensitive learning (Domingos, 1999) uses unequal misclassi\ufb01cation costs to address the class imbalance problem. 4 Approaches to authorship veri\ufb01cation As described in Section 2.1, we build classi\ufb01cation models to distinguish between user and non-user tweets. We have explored \ufb01ve distinct approaches to build such models. 4.1 Approach 1: Compression This approach is inspired from Kolmogorov com- plexity (Li and Vit\u00b4anyi, 2013), which argues that the compressibility of a text re\ufb02ects the quality of the underlying model.",
            "4.1 Approach 1: Compression This approach is inspired from Kolmogorov com- plexity (Li and Vit\u00b4anyi, 2013), which argues that the compressibility of a text re\ufb02ects the quality of the underlying model. We use the Lempel- Ziv-Welch (LZW) compression algorithm (Welch, 1984) to approximate Kolmogorov complexity by dynamically building a dictionary to encode word patterns from the training corpus. The longest oc- curring pattern match present in the dictionary is used to encode the text. We hypothesize that the length of a tweet T from user U, compressed using a dictionary built",
            "from positive examples, will be less than the length of the same tweet compressed using a dic- tionary built from negative examples. We use the following setup to classify test tweets for each Twitter user in our dataset: 1. Build an encoding dictionary using positive examples (trainpos), and an encoding dictio- nary using negative examples (trainneg). 2. Encode the new tweet T using both these dic- tionaries, to obtain Tpos = encodepos(T) and Tneg = encodeneg(T) respectively. 3. If the length of Tpos is less than that of Tneg, classify T as positive; else, classify it as neg- ative. This gives us the class label for each new tweet T. In addition, we compute the characterization score of tweet T with respect to user U, as de- scribed in Equation 1. score(T, U) = 1 \u2212len(Tpos) len(T) (1) Thus the shorter the length of the encoded tweet, the more characteristic of the user T is.",
            "4.2 Approach 2: Topic modeling We hypothesize that each user writes about topics with a particular probability distribution, and that each tweet re\ufb02ects the probability distribution over these topics. We train a topic model using Latent Dirichlet Allocation (LDA) (Blei et al., 2003) on a large corpus of tweets, and use this topic model to compute topic distributions for individual tweets. We then use these values as features. We exper- iment with two types of classi\ufb01ers: Logistic Re- gression (LR), and Multi Linear Perceptron (MLP) of size (5, 5, 5). We represent each tweet as a dis- tribution over n = 500 topics. The characterization score of a tweet T is given by the classi\ufb01er\u2019s con\ufb01dence that T belongs to the positive class. 4.3 Approach 3: n-gram probability We hypothesize that a Twitter user can be charac- terized by usage of words and their frequencies in tweets, and model this using n-gram frequencies.",
            "4.3 Approach 3: n-gram probability We hypothesize that a Twitter user can be charac- terized by usage of words and their frequencies in tweets, and model this using n-gram frequencies. We use the following setup to classify test tweets for each Twitter user in our dataset: 1. Build a frequency dictionary of all n-grams in positive examples (trainpos), and a frequency dictionary of all n-grams in negative exam- ples (trainneg). 2. Compute the average probability of all n- gram sequences in the new tweet T using both these dictionaries, to obtain probpos(T) and probneg(T) respectively. Here, we use add-one smoothing and conditional backoff to compute these probability values. 3. If probpos(T) is greater than probneg(T), clas- sify T as positive; else, classify it as negative. The characterization score of tweet T is given by the average n-gram probability computed using the frequency dictionary of trainpos. We experi- ment with n = 1 (unigrams) and n = 2 (bigrams).",
            "The characterization score of tweet T is given by the average n-gram probability computed using the frequency dictionary of trainpos. We experi- ment with n = 1 (unigrams) and n = 2 (bigrams). 4.4 Approach 4: Document embeddings We hypothesize that if we obtain latent represen- tations of tweets as documents, tweets from the same author will cluster together, and will be dif- ferentiable from tweets from others. To that end, we use the following setup: 1. We obtain representations of tweets as doc- ument embeddings. We experiment with two types of document embeddings: Fast- Text (Facebook-Research, 2016) (embedding size = 100) and BERT-Base, uncased (Devlin et al., 2018) (embedding size = 768). 2. We then use these embeddings as features to train a classi\ufb01cation model. We experiment with two types of classi\ufb01ers: Logistic Re- gression (LR) and Multi Linear Perceptron (MLP) of size (5, 5, 5).",
            "2. We then use these embeddings as features to train a classi\ufb01cation model. We experiment with two types of classi\ufb01ers: Logistic Re- gression (LR) and Multi Linear Perceptron (MLP) of size (5, 5, 5). The characterization score of tweet T is given by the classi\ufb01er\u2019s con\ufb01dence that T belongs to the positive class. Iterative sampling: As described in Section 2.1, there exists an extreme class imbalance for this binary classi\ufb01cation task, in that the number of negative examples is far more than the number of positive examples. Here, we explore an itera- tive sampling technique to address this problem. We train our classi\ufb01er for multiple iterations, coupling the same trainpos with a new randomly sampled trainneg set in each iteration.",
            "Figure 2: Mean accuracy of the BERT + MLP classi\ufb01er for all users over 40 iterations We conduct this experiment for all users with the best performing model for this approach, i.e. we use BERT embeddings as features, and MLP for classi\ufb01cation. We train this classi\ufb01er for 40 iterations, and compare the model\u2019s performance when we use the same set of negative examples vs. when we randomly sample new negative examples in each iteration. Figure 2 shows the mean train and test accuracy for all users over 40 iterations. As expected, the training accuracy is higher if we do not sample, as the model gets trained on the same data repeatedly in each iteration. However, if we perform random sampling, the model is exposed to a larger number of negative examples, which results in a higher test accuracy (+ 1.08%), speci\ufb01cally for negative test examples (+ 2.62%). 4.5 Approach 5: Token embeddings and sequential modeling In this approach, we tokenize each tweet, and ob- tain embeddings for each token. We then sequen- tially give these embeddings as input to a classi\ufb01er.",
            "4.5 Approach 5: Token embeddings and sequential modeling In this approach, we tokenize each tweet, and ob- tain embeddings for each token. We then sequen- tially give these embeddings as input to a classi\ufb01er. We use a pretrained model (BERT-Base, Un- cased: 12-layer, 768-hidden, 12-heads, 110M pa- rameters) to generate token embeddings of size 768, and pass these to a Long Short Term Mem- ory (LSTM) (Hochreiter and Schmidhuber, 1997) classi\ufb01er. We use an LSTM layer with 768 units with dropout and recurrent dropout ratio 0.2, fol- lowed by a dense layer with sigmoid activation. User (1) Compression (2) LDA + LR (2) LDA + MLP (3) Bigram (3) Unigram (4) FT + LR (4) FT + MLP (4) BERT + LR (4) BERT + MLP (5) BERT + LSTM Amitabh Bachchan 72.93 69.47 74.91 84.45 90.",
            "93 69.47 74.91 84.45 90.16 84.58 87.13 93.59 93.73 96.32 Ariana Grande 71.98 76.76 80.57 73.89 85.62 84.00 85.28 87.36 87.98 90.20 Barack Obama 78.85 80.09 87.20 82.47 92.58 91.75 93.06 95.28 95.57 96.57 Bill Gates 74.29 70.78 81.78 81.21 87.10 86.34 83.97 91.56 92.41 92.41 Donald Trump 72.11 77.38 81.91 77.68 89.70 87.72 88.88 90.94 91.62 93.40 Ellen DeGeneres 70.75 69.39 74.53 71.90 84.40 81.27 83.11 87.38 88.60 91.12 J K Rowling 63.",
            "88 90.94 91.62 93.40 Ellen DeGeneres 70.75 69.39 74.53 71.90 84.40 81.27 83.11 87.38 88.60 91.12 J K Rowling 63.39 64.12 70.84 71.27 77.08 79.40 80.75 79.34 80.68 79.71 Jimmy Fallon 67.39 72.04 73.89 78.41 85.59 82.25 83.06 85.82 86.95 88.62 Justin Bieber 73.16 70.99 79.84 75.06 85.72 85.50 86.76 89.53 89.92 92.89 Kevin Durant 68.78 76.56 80.24 74.61 86.21 85.76 86.93 84.75 85.84 88.62 Kim Kardashian 69.73 72.10 76.39 71.49 83.89 80.92 82.",
            "56 80.24 74.61 86.21 85.76 86.93 84.75 85.84 88.62 Kim Kardashian 69.73 72.10 76.39 71.49 83.89 80.92 82.49 84.12 85.25 88.35 Lady Gaga 66.01 67.07 72.40 71.44 81.14 76.91 79.90 81.46 83.21 84.54 LeBron James 67.95 66.21 73.33 74.17 82.42 81.97 77.05 83.48 84.77 85.53 Narendra Modi 82.78 84.40 89.51 90.75 94.39 94.69 95.71 97.21 97.41 97.33 Oprah Winfrey 68.61 61.74 70.47 75.37 83.88 83.69 83.51 86.37 87.07 90.01 Mean 71.25 71.94 77.",
            "21 97.41 97.33 Oprah Winfrey 68.61 61.74 70.47 75.37 83.88 83.69 83.51 86.37 87.07 90.01 Mean 71.25 71.94 77.86 76.95 85.99 84.45 85.17 87.88 88.73 90.37 Table 2: Test accuracy (%) of \ufb01ve approaches to classify user vs. non-user tweets (The best performing approach is shown in bold for each user) [Note that for each user, the test set contains an equal number of positive and negative examples.]",
            "We train this model using the Adam optimizer (Kingma and Ba, 2014) and binary cross-entropy loss, with accuracy as the training metric. 4.6 Results and Comparison Table 2 presents the user-wise test accuracy of the \ufb01ve approaches under the speci\ufb01ed con\ufb01gurations. Note that the test set contains an equal number of positive and negative examples for each author. Other baselines that we attempted to compare against include the best submissions to the PAN 2013 and 2014 author veri\ufb01cation challenge: Sei- dman (2013) and Khonji and Iraqi (2014), which are variants of the Impostors Method. This chal- lenge employed signi\ufb01cantly longer documents (with an average of 1039, 845, and 4393 words per document for articles, essays and novels respec- tively, as opposed to an average of 19 words per tweet) and signi\ufb01cantly fewer documents per au- thor (an average of 3.2, 2.6 and 1 document\/s per author, as opposed to an average of 6738 tweets per user).",
            "Our experiments with the authorship veri\ufb01cation classi\ufb01er (Eder et al., 2016) showed that the Impostors Method is prohibitively expen- sive on larger corpora, and also performed too in- accurately on short texts to provide a meaningful baseline. For 13 of the 15 users in our dataset, Ap- proach 4.5 (token embeddings followed by se- quential modeling) has the highest accuracy. This model correctly identi\ufb01es the author of 90.37% of all tweets in our study, and will be used to de\ufb01ne the characterization score for our subsequent stud- ies. 5 User study To verify whether human evaluators are in agree- ment with our characterization model, we con- ducted a user study using MTurk (Amazon, 2005). 5.1 Setup For each user in our dataset, we build a set of 20 tweet pairs, with one tweet each from the 50 top-scoring and bottom-scoring tweets written by the user. We ask the human evaluator to choose which tweet sounds more characteristic of the user.",
            "5.1 Setup For each user in our dataset, we build a set of 20 tweet pairs, with one tweet each from the 50 top-scoring and bottom-scoring tweets written by the user. We ask the human evaluator to choose which tweet sounds more characteristic of the user. To validate that the MTurk worker knows enough about the Twitter user to pick a character- istic tweet, we use a quali\ufb01cation test containing a basic set of questions about the Twitter user. We were unable to \ufb01nd equal numbers of Turkers fa- miliar with each subject, so our number of evalua- tors n differs according to author. 5.2 Results Table 3 describes the results obtained in the user study: the mean and standard deviation of percent- age of answers in agreement with our model, the p-value, and the number of MTurk workers who completed each task. We \ufb01nd that the average agreement of human evaluators with our model is 70.40% over all 15 users in our dataset. User Mean(%) \u03c3(%) p-value n Amitabh Bachchan 67.08 16.44 1.",
            "We \ufb01nd that the average agreement of human evaluators with our model is 70.40% over all 15 users in our dataset. User Mean(%) \u03c3(%) p-value n Amitabh Bachchan 67.08 16.44 1.30e-07 12 Ariana Grande 67.19 24.01 7.60e-10 16 Barack Obama 55.75 17.04 2.43e-02 20 Bill Gates 70.26 14.19 1.72e-15 19 Donald Trump 83.85 8.87 2.52e-58 26 Ellen DeGeneres 73.75 14.22 5.44e-22 20 J K Rowling 65.79 10.04 7.51e-10 19 Jimmy Fallon 80.00 21.93 5.76e-25 14 Justin Bieber 71.94 22.57 3.97e-17 18 Kevin Durant 64.38 15.04 3.04e-07 16 Kim Kardashian 71.25 14.95 9.",
            "93 5.76e-25 14 Justin Bieber 71.94 22.57 3.97e-17 18 Kevin Durant 64.38 15.04 3.04e-07 16 Kim Kardashian 71.25 14.95 9.27e-18 20 Lady Gaga 85.00 10.31 1.45e-22 9 LeBron James 63.50 12.15 7.38e-08 20 Narendra Modi 60.45 13.68 2.34e-03 11 Oprah Winfrey 75.79 18.12 1.25e-24 19 Table 3: MTurk user study results: For each of these 15 celebrities, human evaluators support our represen- tativeness scores with a signi\ufb01cance level above 0.05. (p-values < 10\u22125 are shown in bold.) For each of the 15 celebrities, the human eval- uators agree with our model above a signi\ufb01cance level of 0.05, and in 13 of 15 cases above a level of 10\u22125.",
            "(p-values < 10\u22125 are shown in bold.) For each of the 15 celebrities, the human eval- uators agree with our model above a signi\ufb01cance level of 0.05, and in 13 of 15 cases above a level of 10\u22125. This makes clear our scores are measuring what we intend to be measuring. 6 Mapping with popularity 6.1 Correlation We now explore the relationship between charac- terization score and tweet popularity for each of the users in our dataset. To analyze this relation- ship, we perform the following procedure for each author U: 1. Sort all tweets written by U in ascending or- der of characterization score. 2. Bucket the sorted tweets by percentile score (1 to 100). 3. For each bucket, calculate the mean number of likes, replies, and retweets.",
            "4. Compute the correlation of this mean and the percentile score. User Likes Replies Retweets Donald Trump 0.64 0.63 0.55 Amitabh Bachchan 0.58 0.81 0.69 Narendra Modi 0.46 0.01 0.22 Jimmy Fallon 0.29 0.54 0.41 J K Rowling 0.21 0.32 0.14 Lady Gaga 0.05 0.12 -0.01 Bill Gates -0.05 -0.11 -0.21 LeBron James -0.22 -0.27 -0.24 Oprah Winfrey -0.30 -0.41 -0.17 Ellen DeGeneres -0.34 -0.29 -0.40 Barack Obama -0.45 -0.46 -0.45 Kevin Durant -0.57 -0.67 -0.53 Kim Kardashian -0.71 -0.72 -0.70 Justin Bieber -0.73 -0.50 -0.71 Ariana Grande -0.74 -0.77 -0.75 Table 4: Pearson correlation coef\ufb01cients between mean popularity measure and percentile,",
            "53 Kim Kardashian -0.71 -0.72 -0.70 Justin Bieber -0.73 -0.50 -0.71 Ariana Grande -0.74 -0.77 -0.75 Table 4: Pearson correlation coef\ufb01cients between mean popularity measure and percentile, for each user (Coef- \ufb01cients with p-value < 0.01 are shown in bold color). Green values exhibit signi\ufb01cant positive correlation, and red values signi\ufb01cant negative correlation. The Pearson correlation coef\ufb01cients (r-values) are listed in Table 4. The users at the top (Trump, Bachchan, Modi) all display very strong positive correlation. We name this group UPC (Users with Positive Correlation), and the group of users at the bottom (Grande, Bieber, Kardashian) as UNC (Users with Negative Correlation). 6.2 Interpretation For users with positive correlation, the higher the tweet\u2019s characterization score, the more popular it becomes, i.e. the more likes, replies, and retweets it receives.",
            "6.2 Interpretation For users with positive correlation, the higher the tweet\u2019s characterization score, the more popular it becomes, i.e. the more likes, replies, and retweets it receives. In contrast, for users with negative cor- relation, the higher the tweet score, the less popu- lar it becomes. Figure 3 shows the plot of log mean number of likes per bucket vs. tweet score percentile, for users with the highest positive correlation. Simi- larly, Figure 4 shows the plot of log mean number of likes per bucket vs. tweet score percentile, for users with the highest negative correlation. Figure 3: Log mean likes vs. percentile for users of positive correlation (The color denotes the year for which maximum tweets are present in the percentile bucket). Figure 4: Log mean likes vs. percentile for users of negative correlation (The color denotes the year for which maximum tweets are present in the percentile bucket). One may question whether these results are due to temporal effects: user\u2019s popularity vary with time, and perhaps the model\u2019s more char- acteristic tweets simply re\ufb02ect periods of author- ship.",
            "One may question whether these results are due to temporal effects: user\u2019s popularity vary with time, and perhaps the model\u2019s more char- acteristic tweets simply re\ufb02ect periods of author- ship. Figures 3 and 4 disprove this hypothesis.",
            "Here the color of each point denotes the year for which most tweets are present in the correspond- ing bucket. Since the distribution of colors over time is not clustered, we infer that the observed result is not an artifact of temporal effects. In both cases, there is a strong trend in tweet popularity based on tweet score. We note that the plots are presented on the log scale, meaning the trends here are exponential. 6.3 Qualitative Analysis We present examples of the most and least char- acteristic tweets for celebrities from three cate- gories, along with their corresponding character- ization scores computed using Approach 4.5. 6.3.1 Users with Positive Correlation (UPC) Donald Trump Tweet Score Prior to the election it was well known that I have interests in properties all over the world. Only the crooked media makes this a big deal! 0.9998 Today is the \ufb01rst day of the rest of your life - make the most of it! 0.0001 Amitabh Bachchan Tweet Score T 2843 - The work is demanding .. the crew binding .. the city exciting .. and the dialogues expanding .. \u2018BADLA\u2019 is grinding .. !!",
            "0.0001 Amitabh Bachchan Tweet Score T 2843 - The work is demanding .. the crew binding .. the city exciting .. and the dialogues expanding .. \u2018BADLA\u2019 is grinding .. !! 0.9996 hahaha .. now i dont have a HD .. but ya a car ride is on .. 0.0002 The characterization score appears to have cor- rectly captured aspects of the user\u2019s personality from their corpus of tweets. For these celebrities, high scoring tweets generally prove more popular (In this example - Donald Trump: 70.5K vs. 693 likes; Amitabh Bachchan: 7.1K vs. 9 likes), as re\ufb02ected in their positive correlation coef\ufb01cients. 6.3.2 Users with Negative Correlation (UNC) Ariana Grande Tweet Score Finalizing the set list for Fresno! Getting so excited.. Can\u2019t believe the show is already almost sold out, you guys are amazing. Xoxo! 0.9997 The \ufb01rst thing I do when I get to a new city is look up how close the nearest Whole Foods is.",
            "Getting so excited.. Can\u2019t believe the show is already almost sold out, you guys are amazing. Xoxo! 0.9997 The \ufb01rst thing I do when I get to a new city is look up how close the nearest Whole Foods is. 0.0002 Justin Bieber Tweet Score grateful to everyone who came out and to my band, dancers, and whole crew. The energy last night was incredible and cant wait to tour 0.9999 Less cantaloupe, more berries. I\u2019m talking to you, pre-packaged fruit salads. Don\u2019t play me like that. 0.00002 Again, high scoring tweets appear more character- istic of their respective users. But here, low scor- ing tweets are generally more popular (In this ex- ample - Ariana Grande: 622 vs. 2.4K likes; Justin Bieber: 454 vs. 13.8K likes), as re\ufb02ected in their negative correlation coef\ufb01cients.",
            "2.4K likes; Justin Bieber: 454 vs. 13.8K likes), as re\ufb02ected in their negative correlation coef\ufb01cients. 6.3.3 Users with no signi\ufb01cant correlation Bill Gates Tweet Score I recently visited a lab doing super-cool energy work-a good reminder of why governments should sponsor R&D 0.9986 There\u2019s a lot of green on this map-which is good-but still not enough. 0.0027 Here, tweets from extreme ends of the spectrum have similar content, so little variation can be ex- pected in their popularity. For this celebrity, there is no signi\ufb01cant correlation between characteriza- tion score and popularity. 7 Conclusions We have presented and evaluated measures of bi- nary author classi\ufb01cation, to obtain a user-speci\ufb01c characterization score for each tweet. We demon- strate that sequential modeling on word embed- dings yields the best result of 90.37% mean test accuracy, and that human evaluators are in agree- ment with our model 70.40% of the time.",
            "We demon- strate that sequential modeling on word embed- dings yields the best result of 90.37% mean test accuracy, and that human evaluators are in agree- ment with our model 70.40% of the time. Our work demonstrates that representativeness scores correlate with popularity, and opens new research directions concerning virality on social media. Acknowledgments We are grateful to the anonymous reviewers for their helpful feedback. We also thank Niranjan Balasubramanian and H. Andrew Schwartz for their comments and suggestions. This work was partially supported by NSF grants IIS-1546113 and IIS-1927227.",
            "References Amazon. 2005. MTurk. (https:\/\/www.mturk.com\/). Hosein Azarbonyad, Mostafa Dehghani, Maarten Marx, and Jaap Kamps. 2015. Time-aware Author- ship Attribution for Short Text Streams. In Proceed- ings of the 38th International ACM SIGIR Confer- ence on Research and Development in Information Retrieval, pages 727\u2013730. ACM. Douglas Bagnall. 2015. Author identi\ufb01cation us- ing multi-headed recurrent neural networks. arXiv preprint arXiv:1506.04891. Roja Bandari, Sitaram Asur, and Bernardo A Huber- man. 2012. The Pulse of News in Social Media: Forecasting Popularity. In Sixth International AAAI Conference on Weblogs and Social Media. Mudit Bhargava, Pulkit Mehndiratta, and Krishna Asawa. 2013. Stylometric Analysis for Authorship Attribution on Twitter. In International Conference on Big Data Analytics, pages 37\u201347. Springer.",
            "Mudit Bhargava, Pulkit Mehndiratta, and Krishna Asawa. 2013. Stylometric Analysis for Authorship Attribution on Twitter. In International Conference on Big Data Analytics, pages 37\u201347. Springer. David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent Dirichlet Allocation. Journal of Ma- chine Learning Research, 3(Jan):993\u20131022. Marcelo Luiz Brocardo, Issa Traore, Sherif Saad, and Isaac Woungang. 2013. Authorship veri\ufb01cation for short messages using stylometry. In 2013 In- ternational Conference on Computer, Information and Telecommunication Systems (CITS), pages 1\u20136. IEEE. Hsin-Yu Chen and Cheng-Te Li. 2017. PSEISMIC: A Personalized Self-Exciting Point Process Model for Predicting Tweet Popularity. In 2017 IEEE Interna- tional Conference on Big Data, pages 2710\u20132713. IEEE. Ting Chen and Yizhou Sun.",
            "2017. PSEISMIC: A Personalized Self-Exciting Point Process Model for Predicting Tweet Popularity. In 2017 IEEE Interna- tional Conference on Big Data, pages 2710\u20132713. IEEE. Ting Chen and Yizhou Sun. 2017. Task-Guided and Path-Augmented Heterogeneous Network Embed- ding for Author Identi\ufb01cation. In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, pages 295\u2013304. ACM. William W Cohen. 1995. Fast effective rule induction. In Machine Learning Proceedings 1995, pages 115\u2013 123. Elsevier. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Un- derstanding. arXiv preprint arXiv:1810.04805. Pedro Domingos. 1999. MetaCost: A General Method for Making Classi\ufb01ers Cost-Sensitive.",
            "BERT: Pre-training of Deep Bidirectional Transformers for Language Un- derstanding. arXiv preprint arXiv:1810.04805. Pedro Domingos. 1999. MetaCost: A General Method for Making Classi\ufb01ers Cost-Sensitive. In KDD, vol- ume 99, pages 155\u2013164. Maciej Eder, Jan Rybicki, and Mike Kestemont. 2016. Stylometry with R: a Package for Computational Text Analysis. R journal, 8(1). Facebook-Research. 2016. FastText. (https:\/\/research.fb.com\/fasttext\/). MVM Fissette. 2010. Author Identi\ufb01cation in Short Texts. Rachel M Green and John W Sheppard. 2013. Com- paring frequency-and style-based features for twitter author identi\ufb01cation. In FLAIRS Conference. Barathi Ganesh HB, U Reshma, et al. 2015. Author identi\ufb01cation based on word distribution in word space.",
            "2013. Com- paring frequency-and style-based features for twitter author identi\ufb01cation. In FLAIRS Conference. Barathi Ganesh HB, U Reshma, et al. 2015. Author identi\ufb01cation based on word distribution in word space. In 2015 ICACCI, pages 1519\u20131523. IEEE. Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997. Long Short-term Memory. Neural computation, 9(8):1735\u20131780. Patrick Juola and Efstathios Stamatatos. 2013. Overview of the Author Identi\ufb01cation Task at PAN 2013. In CLEF (Working Notes). Mahmoud Khonji and Youssef Iraqi. 2014. A Slightly- modi\ufb01ed GI-based Author-veri\ufb01er with Lots of Features (ASGALF). CLEF (Working Notes), 1180:977\u2013983. Diederik P Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization.",
            "CLEF (Working Notes), 1180:977\u2013983. Diederik P Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization. arXiv preprint arXiv:1412.6980. Moshe Koppel and Jonathan Schler. 2004. Authorship veri\ufb01cation as a one-class classi\ufb01cation problem. In Proceedings of the Twenty-First International Con- ference on Machine learning, page 62. ACM. Moshe Koppel, Jonathan Schler, and Shlomo Arga- mon. 2009. Computational Methods in Authorship Attribution. Journal of the American Society for in- formation Science and Technology, 60(1):9\u201326. Moshe Koppel, Jonathan Schler, Shlomo Argamon, and Yaron Winter. 2012. The \u201cFundamental Prob- lem\u201d of Authorship Attribution. English Studies, 93(3):284\u2013291. Moshe Koppel and Yaron Winter. 2014. Determining if Two Documents Are Written by the Same Author.",
            "2012. The \u201cFundamental Prob- lem\u201d of Authorship Attribution. English Studies, 93(3):284\u2013291. Moshe Koppel and Yaron Winter. 2014. Determining if Two Documents Are Written by the Same Author. Journal of the Association for Information Science and Technology, 65(1):178\u2013187. SB Kotsiantis and PE Pintelas. 2003. Mixture of Ex- pert Agents for Handling Imbalanced Data Sets. An- nals of Mathematics, Computing & Teleinformatics, 1(1):46\u201355. Sotiris Kotsiantis, Dimitris Kanellopoulos, Panayiotis Pintelas, et al. 2006. Handling imbalanced datasets: A review. GESTS International Transactions on Computer Science and Engineering, 30(1):25\u201336. Ming Li and Paul Vit\u00b4anyi. 2013. An introduc- tion to Kolmogorov complexity and its applications. Springer Science & Business Media.",
            "GESTS International Transactions on Computer Science and Engineering, 30(1):25\u201336. Ming Li and Paul Vit\u00b4anyi. 2013. An introduc- tion to Kolmogorov complexity and its applications. Springer Science & Business Media. A Pastor L\u00b4opez-Monroy, Manuel Montes-y G\u00b4omez, Hugo Jair Escalante, Luis Villase\u02dcnor-Pineda, and Efstathios Stamatatos. 2015. Discriminative subpro\ufb01le-speci\ufb01c representations for author pro\ufb01l- ing in social media. Knowledge-Based Systems, 89:134\u2013147.",
            "Kim Luyckx and Walter Daelemans. 2008. Author- ship Attribution and Veri\ufb01cation with Many Authors and Limited Data. In Proceedings of the 22nd Inter- national Conference on Computational Linguistics- Volume 1, pages 513\u2013520. ACL. Ahmed M Mohsen, Nagwa M El-Makky, and Na- gia Ghanem. 2016. Author Identi\ufb01cation using Deep Learning. In 2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA), pages 898\u2013903. IEEE. Nasir Naveed, Thomas Gottron, J\u00b4er\u02c6ome Kunegis, and Arifah Che Alhadi. 2011. Bad News Travel Fast: A Content-based Analysis of Interestingness on Twit- ter. In Proceedings of the 3rd International Web Sci- ence Conference, page 8. ACM. Smita Nirkhi, RV Dharaskar, and VM Thakare. 2016. Authorship Veri\ufb01cation of Online Messages for Forensic Investigation.",
            "In Proceedings of the 3rd International Web Sci- ence Conference, page 8. ACM. Smita Nirkhi, RV Dharaskar, and VM Thakare. 2016. Authorship Veri\ufb01cation of Online Messages for Forensic Investigation. Procedia Computer Science, 78:640\u2013645. Sarwat Nizamani and Nasrullah Memon. 2013. CEAI: CCM-based email authorship identi\ufb01cation model. Egyptian Informatics Journal, 14(3):239\u2013249. Jian Peng, Kim-Kwang Raymond Choo, and Helen Ashman. 2016a. Bit-level n-gram based forensic au- thorship analysis on social media: Identifying indi- viduals from linguistic pro\ufb01les. Journal of Network and Computer Applications, 70:171\u2013182. Jian Peng, Raymond Kim-Kwang Choo, and Helen Ashman. 2016b. Astrotur\ufb01ng Detection in So- cial Media: Using Binary n-Gram Analysis for Authorship Attribution.",
            "Journal of Network and Computer Applications, 70:171\u2013182. Jian Peng, Raymond Kim-Kwang Choo, and Helen Ashman. 2016b. Astrotur\ufb01ng Detection in So- cial Media: Using Binary n-Gram Analysis for Authorship Attribution. In 2016 IEEE Trust- com\/BigDataSE\/ISPA, pages 121\u2013128. IEEE. Nektaria Potha and Efstathios Stamatatos. 2018. In- trinsic Author Veri\ufb01cation Using Topic Modeling. In Proceedings of the 10th Hellenic Conference on Arti\ufb01cial Intelligence, page 20. ACM. Anderson Rocha, Walter J Scheirer, Christopher W Forstall, Thiago Cavalcante, Antonio Theophilo, Bingyu Shen, Ariadne RB Carvalho, and Efstathios Stamatatos. 2016. Authorship Attribution for Social Media Forensics. IEEE Transactions on Informa- tion Forensics and Security, 12(1):5\u201333. Shachar Seidman. 2013.",
            "2016. Authorship Attribution for Social Media Forensics. IEEE Transactions on Informa- tion Forensics and Security, 12(1):5\u201333. Shachar Seidman. 2013. Authorship Veri\ufb01cation Using the Impostors Method. In CLEF 2013 Evaluation Labs and Workshop-Online Working Notes. Citeseer. Efstathios Stamatatos. 2008. Author identi\ufb01cation: Using text sampling to handle the class imbalance problem. Information Processing & Management, 44(2):790\u2013799. Efstathios Stamatatos, Walter Daelemans, Ben Ver- hoeven, Patrick Juola, Aurelio L\u00b4opez-L\u00b4opez, Mar- tin Potthast, and Benno Stein. 2015. Overview of the Author Identi\ufb01cation Task at PAN 2015. In CLEF 2015 Evaluation Labs and Workshop \u2013 Work- ing Notes Papers, Toulouse, France. CEUR.",
            "2015. Overview of the Author Identi\ufb01cation Task at PAN 2015. In CLEF 2015 Evaluation Labs and Workshop \u2013 Work- ing Notes Papers, Toulouse, France. CEUR. Efstathios Stamatatos, Walter Daelemans, Ben Verho- even, Martin Potthast, Benno Stein, Patrick Juola, Miguel A Sanchez-Perez, and Alberto Barr\u00b4on- Cede\u02dcno. 2014. Overview of the Author Identi\ufb01- cation Task at PAN 2014. In CLEF 2014 Eval- uation Labs and Workshop Working Notes Papers, Shef\ufb01eld, UK, 2014, pages 1\u201321. Statista. 2019. Twitter: Num- ber of active users 2010-2018. (https:\/\/www.statista.com\/statistics\/282087\/number- of-monthly-active-twitter-users\/). Bongwon Suh, Lichan Hong, Peter Pirolli, and Ed H Chi. 2010. Want to be Retweeted? Large Scale Analytics on Factors Impacting Retweet in Twitter Network.",
            "Bongwon Suh, Lichan Hong, Peter Pirolli, and Ed H Chi. 2010. Want to be Retweeted? Large Scale Analytics on Factors Impacting Retweet in Twitter Network. In 2010 IEEE Second International Con- ference on Social Computing, pages 177\u2013184. IEEE. Chenhao Tan, Lillian Lee, and Bo Pang. 2014. The effect of wording on message propagation: Topic- and author-controlled natural experiments on Twit- ter. arXiv preprint arXiv:1405.1438. Ant\u02c6onio The\u00b4ophilo, Lu\u00b4\u0131s AM Pereira, and Anderson Rocha. 2019. A Needle in a Haystack? Harnessing Onomatopoeia and User-speci\ufb01c Stylometrics for Authorship Attribution of Micro-messages. pages 2692\u20132696. Cor J Veenman and Zhenshi Li. 2013. Authorship Veri\ufb01cation with Compression Features. In CLEF (working notes). Gary M Weiss. 2004. Mining with Rarity: a Unifying Framework.",
            "Cor J Veenman and Zhenshi Li. 2013. Authorship Veri\ufb01cation with Compression Features. In CLEF (working notes). Gary M Weiss. 2004. Mining with Rarity: a Unifying Framework. ACM SIGKDD Explorations Newslet- ter, 6(1):7\u201319. Terry A. Welch. 1984. Technique for High- Performance Data Compression. Computer, 17. Tauhid Zaman, Emily B Fox, Eric T Bradlow, et al. 2014. A Bayesian approach for predicting the pop- ularity of tweets. The Annals of Applied Statistics, 8(3):1583\u20131611. Chunxia Zhang, Xindong Wu, Zhendong Niu, and Wei Ding. 2014. Authorship identi\ufb01cation from unstruc- tured texts. Knowledge-Based Systems, 66:99\u2013111. Yang Zhang, Zhiheng Xu, and Qing Yang. 2018. Predicting Popularity of Messages in Twitter using a Feature-weighted Model.",
            "Authorship identi\ufb01cation from unstruc- tured texts. Knowledge-Based Systems, 66:99\u2013111. Yang Zhang, Zhiheng Xu, and Qing Yang. 2018. Predicting Popularity of Messages in Twitter using a Feature-weighted Model. http:\/\/ww.nlp.ia.ac.cn\/2012papers\/gjhy\/gh154.pdf, 20. Qingyuan Zhao, Murat A Erdogdu, Hera Y He, Anand Rajaraman, and Jure Leskovec. 2015. SEISMIC: A Self-Exciting Point Process Model for Predicting Tweet Popularity. In Proceedings of the 21th ACM SIGKDD, pages 1513\u20131522. ACM. Zhaohui Zheng, Xiaoyun Wu, and Rohini Srihari. 2004. Feature Selection for Text Categorization on Imbalanced Data. ACM SIGKDD Explorations Newsletter, 6(1):80\u201389."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1909.04002.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 11573.0,
    "avg_doclen_est": 180.828125
}
