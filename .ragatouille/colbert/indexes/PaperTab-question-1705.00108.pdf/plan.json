{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Semi-supervised sequence tagging with bidirectional language models Matthew E. Peters, Waleed Ammar, Chandra Bhagavatula, Russell Power Allen Institute for Arti\ufb01cial Intelligence {matthewp,waleeda,chandrab,russellp}@allenai.org Abstract Pre-trained word embeddings learned from unlabeled text have become a stan- dard component of neural network archi- tectures for NLP tasks. However, in most cases, the recurrent network that oper- ates on word-level representations to pro- duce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pre- trained context embeddings from bidi- rectional language models to NLP sys- tems and apply it to sequence labeling tasks. We evaluate our model on two stan- dard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task speci\ufb01c gazetteers.",
            "We evaluate our model on two stan- dard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task speci\ufb01c gazetteers. 1 Introduction Due to their simplicity and ef\ufb01cacy, pre-trained word embedding have become ubiquitous in NLP systems. Many prior studies have shown that they capture useful semantic and syntactic information (Mikolov et al., 2013; Pennington et al., 2014) and including them in NLP systems has been shown to be enormously helpful for a variety of downstream tasks (Collobert et al., 2011). However, in many NLP tasks it is essential to represent not just the meaning of a word, but also the word in context. For example, in the two phrases \u201cA Central Bank spokesman\u201d and \u201cThe Central African Republic\u201d, the word \u2018Central\u2019 is used as part of both an Organization and Location.",
            "For example, in the two phrases \u201cA Central Bank spokesman\u201d and \u201cThe Central African Republic\u201d, the word \u2018Central\u2019 is used as part of both an Organization and Location. Accordingly, current state of the art sequence tag- ging models typically include a bidirectional re- current neural network (RNN) that encodes token sequences into a context sensitive representation before making token speci\ufb01c predictions (Yang et al., 2017; Ma and Hovy, 2016; Lample et al., 2016; Hashimoto et al., 2016). Although the token representation is initialized with pre-trained embeddings, the parameters of the bidirectional RNN are typically learned only on labeled data. Previous work has explored meth- ods for jointly learning the bidirectional RNN with supplemental labeled data from other tasks (e.g., S\u00f8gaard and Goldberg, 2016; Yang et al., 2017). In this paper, we explore an alternate semi- supervised approach which does not require ad- ditional labeled data.",
            "In this paper, we explore an alternate semi- supervised approach which does not require ad- ditional labeled data. We use a neural language model (LM), pre-trained on a large, unlabeled cor- pus to compute an encoding of the context at each position in the sequence (hereafter an LM embed- ding) and use it in the supervised sequence tag- ging model. Since the LM embeddings are used to compute the probability of future words in a neu- ral LM, they are likely to encode both the semantic and syntactic roles of words in context. Our main contribution is to show that the con- text sensitive representation captured in the LM embeddings is useful in the supervised sequence tagging setting. When we include the LM embed- dings in our system overall performance increases from 90.87% to 91.93% F1 for the CoNLL 2003 NER task, a more then 1% absolute F1 increase, and a substantial improvement over the previous state of the art. We also establish a new state of the art result (96.37% F1) for the CoNLL 2000 Chunking task.",
            "We also establish a new state of the art result (96.37% F1) for the CoNLL 2000 Chunking task. As a secondary contribution, we show that us- ing both forward and backward LM embeddings boosts performance over a forward only LM. We also demonstrate that domain speci\ufb01c pre-training is not necessary by applying a LM trained in the news domain to scienti\ufb01c papers. arXiv:1705.00108v1  [cs.CL]  29 Apr 2017",
            "2 Language model augmented sequence taggers (TagLM) 2.1 Overview The main components in our language-model- augmented sequence tagger (TagLM) are illus- trated in Fig. 1. After pre-training word embed- dings and a neural LM on large, unlabeled corpora (Step 1), we extract the word and LM embeddings for every token in a given input sequence (Step 2) and use them in the supervised sequence tagging model (Step 3). 2.2 Baseline sequence tagging model Our baseline sequence tagging model is a hierar- chical neural tagging model, closely following a number of recent studies (Ma and Hovy, 2016; Lample et al., 2016; Yang et al., 2017; Chiu and Nichols, 2016) (left side of Figure 2). Given a sentence of tokens (t1, t2, . . .",
            "Given a sentence of tokens (t1, t2, . . . , tN) it \ufb01rst forms a representation, xk, for each token by concatenating a character based representation ck with a token embedding wk: ck = C(tk; \u03b8c) wk = E(tk; \u03b8w) xk = [ck; wk] (1) The character representation ck captures morpho- logical information and is either a convolutional neural network (CNN) (Ma and Hovy, 2016; Chiu and Nichols, 2016) or RNN (Yang et al., 2017; Lample et al., 2016). It is parameterized by C(\u00b7, \u03b8c) with parameters \u03b8c. The token embed- dings, wk, are obtained as a lookup E(\u00b7, \u03b8w), ini- tialized using pre-trained word embeddings, and \ufb01ne tuned during training (Collobert et al., 2011). To learn a context sensitive representation, we employ multiple layers of bidirectional RNNs.",
            "To learn a context sensitive representation, we employ multiple layers of bidirectional RNNs. For each token position, k, the hidden state hk,i of RNN layer i is formed by concatenating the hid- den states from the forward (\u2212\u2192 h k,i) and backward (\u2190\u2212 h k,i) RNNs. As a result, the bidirectional RNN is able to use both past and future information to make a prediction at token k. More formally, for the \ufb01rst RNN layer that operates on xk to output hk,1: \u2212\u2192 h k,1 = \u2212\u2192 R 1(xk, \u2212\u2192 h k\u22121,1; \u03b8\u2212 \u2192 R 1) \u2190\u2212 h k,1 = \u2190\u2212 R 1(xk, \u2190\u2212 h k+1,1; \u03b8\u2190 \u2212 R 1) hk,1 = [\u2212\u2192 h k,1; \u2190\u2212 h k,1] (2) Step 2: Prepare word  embedding and LM  embedding for each  token in the input  sequence.",
            "The need to capture future context in the LM embeddings suggests  itis beneficial to also consider a \\textit{backward} LM in additionalto the traditional \\textit{forward} LM.  A backward LM predicts the  previous token given the future context.  Given a sentence with $N$  tokens, it computes\\[P(t_{k-1} | t_k, t_{k+1}, ..., t_N).\\]A backward  LM can be implemThe need to capture future context in the LM  embeddings suggests itis beneficial to also consider a  \\textit{backward} LM in additionalto the traditional \\textit{forward}  LM.  A backward LM predicts the previous token given the future  context.  Given a sentence with $N$ tokens, it computes\\[P(t_{k-1} |  t_k, t_{k+1}, ..., t_N).\\]A backward LM can be implemented in an  analogous way to a forward LM and produces an embedding  $\\overleftarrow{\\mathbf{h}}^{LM}_k$, for the sequence $(t_k,  t_{k+1}, ..., t_N)$, the output embeddings of the top layer LSTM.",
            "The  need to capture future context in the LM embeddings suggests itis  beneficial to also consider a \\textit{backward} LM in additionalto the  traditional \\textit{forward} LM.  A backward LM predicts the  previous token given the future context.  Given a sentence with $N$  tokens, it computes\\[P(t_{k-1} | t_k, t_{k+1}, ..., t_N).\\]A backward  LM can be implemented in an analogous way to a forward LM and  produces an embedding $\\overleftarrow{\\mathbf{h}}^{LM}_k$, for  the sequence $(t_k, t_{k+1}, ..., t_N)$, the output embeddings of the  top layer LSTM.ented in an analogous way to a forward LM and  produces an embedding $\\overleftarrow{\\mathbf{h}}^{LM}_k$, for  the sequence $(t_k, t_{k+1}, ..., t_N)$, the output embeddings of the  top layer LSTM. The need to capture future context in the LM embeddings suggests  itis beneficial to also consider a \\textit{backward} LM in additionalto the traditional \\textit{forward} LM.",
            "The need to capture future context in the LM embeddings suggests  itis beneficial to also consider a \\textit{backward} LM in additionalto the traditional \\textit{forward} LM.  A backward LM predicts the  previous token given the future context.  Given a sentence with $N$  tokens, it computes\\[P(t_{k-1} | t_k, t_{k+1}, ..., t_N).\\]A backward  LM can be implemThe need to capture future context in the LM  embeddings suggests itis beneficial to also consider a  \\textit{backward} LM in additionalto the traditional \\textit{forward}  LM.  A backward LM predicts the previous token given the future  context.  Given a sentence with $N$ tokens, it computes\\[P(t_{k-1} |  t_k, t_{k+1}, ..., t_N).\\]A backward LM can be implemented in an  analogous way to a forward LM and produces an embedding  $\\overleftarrow{\\mathbf{h}}^{LM}_k$, for the sequence $(t_k,  t_{k+1}, ..., t_N)$, the output embeddings of the top layer LSTM.",
            "The  need to capture future context in the LM embeddings suggests itis  beneficial to also consider a \\textit{backward} LM in additionalto the  traditional \\textit{forward} LM.  A backward LM predicts the  previous token given the future context.  Given a sentence with $N$  tokens, it computes\\[P(t_{k-1} | t_k, t_{k+1}, ..., t_N).\\]A backward  LM can be implemented in an analogous way to a forward LM and  produces an embedding $\\overleftarrow{\\mathbf{h}}^{LM}_k$, for  the sequence $(t_k, t_{k+1}, ..., t_N)$, the output embeddings of the  top layer LSTM.ented in an analogous way to a forward LM and  produces an embedding $\\overleftarrow{\\mathbf{h}}^{LM}_k$, for  the sequence $(t_k, t_{k+1}, ..., t_N)$, the output embeddings of the  top layer LSTM. The need to capture future context in the LM embeddings suggests  itis beneficial to also consider a \\textit{backward} LM in additionalto the traditional \\textit{forward} LM.",
            "The need to capture future context in the LM embeddings suggests  itis beneficial to also consider a \\textit{backward} LM in additionalto the traditional \\textit{forward} LM.  A backward LM predicts the  previous token given the future context.  Given a sentence with $N$  tokens, it computes\\[P(t_{k-1} | t_k, t_{k+1}, ..., t_N).\\]A backward  LM can be implemThe need to capture future context in the LM  embeddings suggests itis beneficial to also consider a  \\textit{backward} LM in additionalto the traditional \\textit{forward}  LM.  A backward LM predicts the previous token given the future  context.  Given a sentence with $N$ tokens, it computes\\[P(t_{k-1} |  t_k, t_{k+1}, ..., t_N).\\]A backward LM can be implemented in an  analogous way to a forward LM and produces an embedding  $\\overleftarrow{\\mathbf{h}}^{LM}_k$, for the sequence $(t_k,  t_{k+1}, ..., t_N)$, the output embeddings of the top layer LSTM.",
            "The  need to capture future context in the LM embeddings suggests itis  beneficial to also consider a \\textit{backward} LM in additionalto the  traditional \\textit{forward} LM.  A backward LM predicts the  previous token given the future context.  Given a sentence with $N$  tokens, it computes\\[P(t_{k-1} | t_k, t_{k+1}, ..., t_N).\\]A backward  LM can be implemented in an analogous way to a forward LM and  produces an embedding $\\overleftarrow{\\mathbf{h}}^{LM}_k$, for  the sequence $(t_k, t_{k+1}, ..., t_N)$, the output embeddings of the  top layer LSTM.ented in an analogous way to a forward LM and  produces an embedding $\\overleftarrow{\\mathbf{h}}^{LM}_k$, for  the sequence $(t_k, t_{k+1}, ..., t_N)$, the output embeddings of the  top layer LSTM. The need to capture future context in the LM embeddings suggests  itis beneficial to also consider a \\textit{backward} LM in additionalto the traditional \\textit{forward} LM.",
            "The need to capture future context in the LM embeddings suggests  itis beneficial to also consider a \\textit{backward} LM in additionalto the traditional \\textit{forward} LM.  A backward LM predicts the  previous token given the future context.  Given a sentence with $N$  tokens, it computes\\[P(t_{k-1} | t_k, t_{k+1}, ..., t_N).\\]A backward  LM can be implemThe need to capture future context in the LM  embeddings suggests itis beneficial to also consider a  \\textit{backward} LM in additionalto the traditional \\textit{forward}  LM.  A backward LM predicts the previous token given the future  context.  Given a sentence with $N$ tokens, it computes\\[P(t_{k-1} |  t_k, t_{k+1}, ..., t_N).\\]A backward LM can be implemented in an  analogous way to a forward LM and produces an embedding  $\\overleftarrow{\\mathbf{h}}^{LM}_k$, for the sequence $(t_k,  t_{k+1}, ..., t_N)$, the output embeddings of the top layer LSTM.",
            "The  need to capture future context in the LM embeddings suggests itis  beneficial to also consider a \\textit{backward} LM in additionalto the  traditional \\textit{forward} LM.  A backward LM predicts the  previous token given the future context.  Given a sentence with $N$  tokens, it computes\\[P(t_{k-1} | t_k, t_{k+1}, ..., t_N).\\]A backward  LM can be implemented in an analogous way to a forward LM and  produces an embedding $\\overleftarrow{\\mathbf{h}}^{LM}_k$, for  the sequence $(t_k, t_{k+1}, ..., t_N)$, the output embeddings of the  top layer LSTM.ented in an analogous way to a forward LM and  produces an embedding $\\overleftarrow{\\mathbf{h}}^{LM}_k$, for  the sequence $(t_k, t_{k+1}, ..., t_N)$, the output embeddings of the  top layer LSTM. unlabeled data Recurrent language  model Word embedding  model Step 1: Pretrain word  embeddings and  language model.",
            "unlabeled data Recurrent language  model Word embedding  model Step 1: Pretrain word  embeddings and  language model. New    York   is     located   \u2026 Sequence tagging model B-LOC   E-LOC     O         O       \u2026 input  sequence output   sequence Word  embedding LM  embedding Two representations of the word \u201cYork\u201d Step 3:  Use both word  embeddings and LM  embeddings in the  sequence tagging  model. New    York   is     located   \u2026 Figure 1: The main components in TagLM, our language-model-augmented sequence tagging system. The language model component (in or- ange) is used to augment the input token represen- tation in a traditional sequence tagging models (in grey). The second RNN layer is similar and uses hk,1 to output hk,2.",
            "The language model component (in or- ange) is used to augment the input token represen- tation in a traditional sequence tagging models (in grey). The second RNN layer is similar and uses hk,1 to output hk,2. In this paper, we use L = 2 lay- ers of RNNs in all experiments and parameterize Ri as either Gated Recurrent Units (GRU) (Cho et al., 2014) or Long Short-Term Memory units (LSTM) (Hochreiter and Schmidhuber, 1997) de- pending on the task. Finally, the output of the \ufb01nal RNN layer hk,L is used to predict a score for each possible tag us- ing a single dense layer. Due to the dependencies between successive tags in our sequence label- ing tasks (e.g. using the BIOES labeling scheme, it is not possible for I-PER to follow B-LOC), it is bene\ufb01cial to model and decode each sen- tence jointly instead of independently predicting the label for each token.",
            "using the BIOES labeling scheme, it is not possible for I-PER to follow B-LOC), it is bene\ufb01cial to model and decode each sen- tence jointly instead of independently predicting the label for each token. Accordingly, we add another layer with parameters for each label bi- gram, computing the sentence conditional random \ufb01eld (CRF) loss (Lafferty et al., 2001) using the forward-backward algorithm at training time, and using the Viterbi algorithm to \ufb01nd the most likely tag sequence at test time, similar to Collobert et al. (2011).",
            "New            York          is        located      ... Neural net Char  CNN\/  RNN Embedding Token  embedding RNN Dense E-LOC B-LOC CRF bi-RNN  (R2) Token  representation New     York   is  located   ... Forward LM Backward LM h1 LM Concat LM   embedding Sequence  tagging Pre-trained bi-LM bi-RNN (R1) Sequence  representation Concatenation Token  representation New     York   is  located   ... Token  representation h1,1 h2 LM h2,1 h1,2 h2,2 Figure 2: Overview of TagLM, our language model augmented sequence tagging architecture. The top level embeddings from a pre-trained bidirectional LM are inserted in a stacked bidirectional RNN sequence tagging model. See text for details. 2.3 Bidirectional LM A language model computes the probability of a token sequence (t1, t2, . . . , tN) p(t1, t2, . . . , tN) = N Y k=1 p(tk | t1, t2, . . . , tk\u22121).",
            ". . , tN) p(t1, t2, . . . , tN) = N Y k=1 p(tk | t1, t2, . . . , tk\u22121). Recent state of the art neural language models (J\u00b4ozefowicz et al., 2016) use a similar architec- ture to our baseline sequence tagger where they pass a token representation (either from a CNN over characters or as token embeddings) through multiple layers of LSTMs to embed the history (t1, t2, . . . , tk) into a \ufb01xed dimensional vector \u2212\u2192 h LM k . This is the forward LM embedding of the token at position k and is the output of the top LSTM layer in the language model. Finally, the language model predicts the probability of token tk+1 using a softmax layer over words in the vo- cabulary. The need to capture future context in the LM embeddings suggests it is bene\ufb01cial to also con- sider a backward LM in additional to the tradi- tional forward LM. A backward LM predicts the previous token given the future context.",
            "The need to capture future context in the LM embeddings suggests it is bene\ufb01cial to also con- sider a backward LM in additional to the tradi- tional forward LM. A backward LM predicts the previous token given the future context. Given a sentence with N tokens, it computes p(t1, t2, . . . , tN) = N Y k=1 p(tk | tk+1, tk+2, . . . , tN). A backward LM can be implemented in an anal- ogous way to a forward LM and produces the backward LM embedding \u2190\u2212 h LM k , for the sequence (tk, tk+1, . . . , tN), the output embeddings of the top layer LSTM. In our \ufb01nal system, after pre-training the for- ward and backward LMs separately, we remove the top layer softmax and concatenate the for- ward and backward LM embeddings to form bidirectional LM embeddings, i.e., hLM k = [\u2212\u2192 h LM k ; \u2190\u2212 h LM k ]. Note that in our formulation, the forward and backward LMs are independent, with- out any shared parameters.",
            "Note that in our formulation, the forward and backward LMs are independent, with- out any shared parameters. 2.4 Combining LM with sequence model Our combined system, TagLM, uses the LM em- beddings as additional inputs to the sequence tag- ging model. In particular, we concatenate the LM embeddings hLM with the output from one of the bidirectional RNN layers in the sequence model. In our experiments, we found that introducing the LM embeddings at the output of the \ufb01rst layer per- formed the best. More formally, we simply replace (2) with hk,1 = [\u2212\u2192 h k,1; \u2190\u2212 h k,1; hLM k ]. (3) There are alternate possibilities for adding the LM embeddings to the sequence model. One pos-",
            "sibility adds a non-linear mapping after the con- catenation and before the second RNN (e.g. re- placing (3) with f([\u2212\u2192 h k,1; \u2190\u2212 h k,1; hLM k ]) where f is a non-linear function). Another possibility in- troduces an attention-like mechanism that weights the all LM embeddings in a sentence before in- cluding them in the sequence model. Our ini- tial results with the simple concatenation were en- couraging so we did not explore these alternatives in this study, preferring to leave them for future work. 3 Experiments We evaluate our approach on two well bench- marked sequence tagging tasks, the CoNLL 2003 NER task (Sang and Meulder, 2003) and the CoNLL 2000 Chunking task (Sang and Buch- holz, 2000). We report the of\ufb01cial evaluation met- ric (micro-averaged F1). In both cases, we use the BIOES labeling scheme for the output tags, following previous work which showed it out- performs other options (e.g., Ratinov and Roth, 2009).",
            "We report the of\ufb01cial evaluation met- ric (micro-averaged F1). In both cases, we use the BIOES labeling scheme for the output tags, following previous work which showed it out- performs other options (e.g., Ratinov and Roth, 2009). Following Chiu and Nichols (2016), we use the Senna word embeddings (Collobert et al., 2011) and pre-processed the text by lowercasing all tokens and replacing all digits with 0. CoNLL 2003 NER. The CoNLL 2003 NER task consists of newswire from the Reuters RCV1 corpus tagged with four different entity types (PER, LOC, ORG, MISC). It includes standard train, development and test sets. Following pre- vious work (Yang et al., 2017; Chiu and Nichols, 2016) we trained on both the train and develop- ment sets after tuning hyperparameters on the de- velopment set. The hyperparameters for our baseline model are similar to Yang et al. (2017).",
            "The hyperparameters for our baseline model are similar to Yang et al. (2017). We use two bidirec- tional GRUs with 80 hidden units and 25 dimen- sional character embeddings for the token charac- ter encoder. The sequence layer uses two bidirec- tional GRUs with 300 hidden units each. For reg- ularization, we add 25% dropout to the input of each GRU, but not to the recurrent connections. CoNLL 2000 chunking. The CoNLL 2000 chunking task uses sections 15-18 from the Wall Street Journal corpus for training and section 20 for testing. It de\ufb01nes 11 syntactic chunk types (e.g., NP, VP, ADJP) in addition to other. We randomly sampled 1000 sentences from the train- ing set as a held-out development set. The baseline sequence tagger uses 30 dimen- sional character embeddings and a CNN with 30 \ufb01lters of width 3 characters followed by a tanh non-linearity for the token character encoder.",
            "The baseline sequence tagger uses 30 dimen- sional character embeddings and a CNN with 30 \ufb01lters of width 3 characters followed by a tanh non-linearity for the token character encoder. The sequence layer uses two bidirectional LSTMs with 200 hidden units. Following Ma and Hovy (2016) we added 50% dropout to the character embed- dings, the input to each LSTM layer (but not re- current connections) and to the output of the \ufb01nal LSTM layer. Pre-trained language models. The primary bidirectional LMs we used in this study were trained on the 1B Word Benchmark (Chelba et al., 2014), a publicly available benchmark for large- scale language modeling. The training split has approximately 800 million tokens, about a 4000X increase over the number training tokens in the CoNLL datasets. J\u00b4ozefowicz et al. (2016) ex- plored several model architectures and released their best single model and training recipes. Fol- lowing Sak et al.",
            "J\u00b4ozefowicz et al. (2016) ex- plored several model architectures and released their best single model and training recipes. Fol- lowing Sak et al. (2014), they used linear projec- tion layers at the output of each LSTM layer to reduce the computation time but still maintain a large LSTM state. Their single best model took three weeks to train on 32 GPUs and achieved 30.0 test perplexity. It uses a character CNN with 4096 \ufb01lters for input, followed by two stacked LSTMs, each with 8192 hidden units and a 1024 dimen- sional projection layer. We use CNN-BIG-LSTM to refer to this language model in our results. In addition to CNN-BIG-LSTM from J\u00b4ozefowicz et al. (2016),1 we used the same cor- pus to train two additional language models with fewer parameters: forward LSTM-2048-512 and backward LSTM-2048-512. Both language models use token embeddings as input to a single layer LSTM with 2048 units and a 512 dimension projection layer.",
            "Both language models use token embeddings as input to a single layer LSTM with 2048 units and a 512 dimension projection layer. We closely followed the proce- dure outlined in J\u00b4ozefowicz et al. (2016), except we used synchronous parameter updates across four GPUs instead of asynchronous updates across 32 GPUs and ended training after 10 epochs. The test set perplexities for our forward and backward LSTM-2048-512 language models are 47.7 and 47.3, respectively.2 1https:\/\/github.com\/tensorflow\/models\/ tree\/master\/lm_1b 2Due to different implementations, the perplexity of the forward LM with similar con\ufb01gurations in J\u00b4ozefowicz et al. (2016) is different (45.0 vs. 47.7).",
            "Model F1\u00b1 std Chiu and Nichols (2016) 90.91 \u00b1 0.20 Lample et al. (2016) 90.94 Ma and Hovy (2016) 91.37 Our baseline without LM 90.87 \u00b1 0.13 TagLM 91.93 \u00b1 0.19 Table 1: Test set F1 comparison on CoNLL 2003 NER task, using only CoNLL 2003 data and unla- beled text. Model F1\u00b1 std Yang et al. (2017) 94.66 Hashimoto et al. (2016) 95.02 S\u00f8gaard and Goldberg (2016) 95.28 Our baseline without LM 95.00 \u00b1 0.08 TagLM 96.37 \u00b1 0.05 Table 2: Test set F1 comparison on CoNLL 2000 Chunking task using only CoNLL 2000 data and unlabeled text. Training. All experiments use the Adam opti- mizer (Kingma and Ba, 2015) with gradient norms clipped at 5.0.",
            "Training. All experiments use the Adam opti- mizer (Kingma and Ba, 2015) with gradient norms clipped at 5.0. In all experiments, we \ufb01ne tune the pre-trained Senna word embeddings but \ufb01x all weights in the pre-trained language models. In ad- dition to explicit dropout regularization, we also use early stopping to prevent over-\ufb01tting and use the following process to determine when to stop training. We \ufb01rst train with a constant learning rate \u03b1 = 0.001 on the training data and monitor the development set performance at each epoch. Then, at the epoch with the highest development performance, we start a simple learning rate an- nealing schedule: decrease \u03b1 an order of magni- tude (i.e., divide by ten), train for \ufb01ve epochs, de- crease \u03b1 an order of magnitude again, train for \ufb01ve more epochs and stop. Following Chiu and Nichols (2016), we train each \ufb01nal model con\ufb01guration ten times with dif- ferent random seeds and report the mean and stan- dard deviation F1.",
            "Following Chiu and Nichols (2016), we train each \ufb01nal model con\ufb01guration ten times with dif- ferent random seeds and report the mean and stan- dard deviation F1. It is important to estimate the variance of model performance since the test data sets are relatively small. 3.1 Overall system results Tables 1 and 2 compare results from TagLM with previously published state of the art results without additional labeled data or task speci\ufb01c gazetteers. Tables 3 and 4 compare results of TagLM to other systems that include additional la- beled data or gazetteers. In both tasks, TagLM es- tablishes a new state of the art using bidirectional LMs (the forward CNN-BIG-LSTM and the back- ward LSTM-2048-512).",
            "In both tasks, TagLM es- tablishes a new state of the art using bidirectional LMs (the forward CNN-BIG-LSTM and the back- ward LSTM-2048-512). In the CoNLL 2003 NER task, our model scores 91.93 mean F1, which is a statistically signi\ufb01- cant increase over the previous best result of 91.62 \u00b10.33 from Chiu and Nichols (2016) that used gazetteers (at 95%, two-sided Welch t-test, p = 0.021). In the CoNLL 2000 Chunking task, TagLM achieves 96.37 mean F1, exceeding all previously published results without additional labeled data by more then 1% absolute F1. The improvement over the previous best result of 95.77 in Hashimoto et al. (2016) that jointly trains with Penn Treebank (PTB) POS tags is statistically signi\ufb01cant at 95% (p < 0.001 assuming standard deviation of 0.1).",
            "The improvement over the previous best result of 95.77 in Hashimoto et al. (2016) that jointly trains with Penn Treebank (PTB) POS tags is statistically signi\ufb01cant at 95% (p < 0.001 assuming standard deviation of 0.1). Importantly, the LM embeddings amounts to an average absolute improvement of 1.06 and 1.37 F1 in the NER and Chunking tasks, respectively. Adding external resources. Although we do not use external labeled data or gazetteers, we found that TagLM outperforms previous state of the art results in both tasks when external re- sources (labeled data or task speci\ufb01c gazetteers) are available. Furthermore, Tables 3 and 4 show that, in most cases, the improvements we obtain by adding LM embeddings are larger then the im- provements previously obtained by adding other forms of transfer or joint learning. For example, Yang et al.",
            "Furthermore, Tables 3 and 4 show that, in most cases, the improvements we obtain by adding LM embeddings are larger then the im- provements previously obtained by adding other forms of transfer or joint learning. For example, Yang et al. (2017) noted an improvement of only 0.06 F1 in the NER task when transfer learning from both CoNLL 2000 chunks and PTB POS tags and Chiu and Nichols (2016) reported an increase of 0.71 F1 when adding gazetteers to their base- line. In the Chunking task, previous work has re- ported from 0.28 to 0.75 improvement in F1 when including supervised labels from the PTB POS tags or CoNLL 2003 entities (Yang et al., 2017; S\u00f8gaard and Goldberg, 2016; Hashimoto et al., 2016). 3.2 Analysis To elucidate the characteristics of our LM aug- mented sequence tagger, we ran a number of addi- tional experiments on the CoNLL 2003 NER task. How to use LM embeddings?",
            "3.2 Analysis To elucidate the characteristics of our LM aug- mented sequence tagger, we ran a number of addi- tional experiments on the CoNLL 2003 NER task. How to use LM embeddings? In this experi- ment, we concatenate the LM embeddings at dif-",
            "F1 F1 Model External resources Without With \u2206 Yang et al. (2017) transfer from CoNLL 2000\/PTB-POS 91.2 91.26 +0.06 Chiu and Nichols (2016) with gazetteers 90.91 91.62 +0.71 Collobert et al. (2011) with gazetteers 88.67 89.59 +0.92 Luo et al. (2015) joint with entity linking 89.9 91.2 +1.3 Ours no LM vs TagLM unlabeled data only 90.87 91.93 +1.06 Table 3: Improvements in test set F1 in CoNLL 2003 NER when including additional labeled data or task speci\ufb01c gazetteers (except the case of TagLM where we do not use additional labeled resources). F1 F1 Model External resources Without With \u2206 Yang et al. (2017) transfer from CoNLL 2003\/PTB-POS 94.66 95.41 +0.75 Hashimoto et al.",
            "F1 F1 Model External resources Without With \u2206 Yang et al. (2017) transfer from CoNLL 2003\/PTB-POS 94.66 95.41 +0.75 Hashimoto et al. (2016) jointly trained with PTB-POS 95.02 95.77 +0.75 S\u00f8gaard and Goldberg (2016) jointly trained with PTB-POS 95.28 95.56 +0.28 Ours no LM vs TagLM unlabeled data only 95.00 96.37 +1.37 Table 4: Improvements in test set F1 in CoNLL 2000 Chunking when including additional labeled data (except the case of TagLM where we do not use additional labeled data).",
            "Use LM embeddings at F1\u00b1 std input to the \ufb01rst RNN layer 91.55 \u00b1 0.21 output of the \ufb01rst RNN layer 91.93 \u00b1 0.19 output of the second RNN layer 91.72 \u00b1 0.13 Table 5: Comparison of CoNLL-2003 test set F1 when the LM embeddings are included at different layers in the baseline tagger. ferent locations in the baseline sequence tagger. In particular, we used the LM embeddings hLM k to: \u2022 augment the input of the \ufb01rst RNN layer; i.e., xk = [ck; wk; hLM k ], \u2022 augment the output of the \ufb01rst RNN layer; i.e., hk,1 = [\u2212\u2192 h k,1; \u2190\u2212 h k,1; hLM k ],3 and \u2022 augment the output of the second RNN layer; i.e., hk,2 = [\u2212\u2192 h k,2; \u2190\u2212 h k,2; hLM k ]. Table 5 shows that the second alternative per- forms best.",
            "Table 5 shows that the second alternative per- forms best. We speculate that the second RNN layer in the sequence tagging model is able to cap- ture interactions between task speci\ufb01c context as expressed in the \ufb01rst RNN layer and general con- text as expressed in the LM embeddings in a way that improves overall system performance. These 3This con\ufb01guration the same as Eq. 3 in \u00a72.4. It was re- produced here for convenience. results are consistent with S\u00f8gaard and Goldberg (2016) who found that chunking performance was sensitive to the level at which additional POS su- pervision was added. Does it matter which language model to use? In this experiment, we compare six different con- \ufb01gurations of the forward and backward language models (including the baseline model which does not use any language models). The results are re- ported in Table 6. We \ufb01nd that adding backward LM embeddings consistently outperforms forward-only LM em- beddings, with F1 improvements between 0.22 and 0.27%, even with the relatively small back- ward LSTM-2048-512 LM.",
            "We \ufb01nd that adding backward LM embeddings consistently outperforms forward-only LM em- beddings, with F1 improvements between 0.22 and 0.27%, even with the relatively small back- ward LSTM-2048-512 LM. LM size is important, and replacing the forward LSTM-2048-512 with CNN-BIG-LSTM (test perplexities of 47.7 to 30.0 on 1B Word Bench- mark) improves F1 by 0.26 - 0.31%, about as much as adding backward LM. Accordingly, we hypothesize (but have not tested) that replacing the backward LSTM-2048-512 with a backward LM analogous to the CNN-BIG-LSTM would fur- ther improve performance. To highlight the importance of including lan- guage models trained on a large scale data, we also experimented with training a language model on just the CoNLL 2003 training and development data. Due to the much smaller size of this data",
            "Forward language model Backward language model LM perplexity F1\u00b1 std Fwd Bwd \u2014 \u2014 N\/A N\/A 90.87 \u00b1 0.13 LSTM-512-256\u2217 LSTM-512-256\u2217 106.9 104.2 90.79 \u00b1 0.15 LSTM-2048-512 \u2014 47.7 N\/A 91.40 \u00b1 0.18 LSTM-2048-512 LSTM-2048-512 47.7 47.3 91.62 \u00b1 0.23 CNN-BIG-LSTM \u2014 30.0 N\/A 91.66 \u00b1 0.13 CNN-BIG-LSTM LSTM-2048-512 30.0 47.3 91.93 \u00b1 0.19 Table 6: Comparison of CoNLL-2003 test set F1 for different language model combinations. All language models were trained and evaluated on the 1B Word Benchmark, except LSTM-512-256\u2217 which was trained and evaluated on the standard splits of the NER CoNLL 2003 dataset.",
            "All language models were trained and evaluated on the 1B Word Benchmark, except LSTM-512-256\u2217 which was trained and evaluated on the standard splits of the NER CoNLL 2003 dataset. set, we decreased the model size to 512 hidden units with a 256 dimension projection and normal- ized tokens in the same manner as input to the se- quence tagging model (lower-cased, with all dig- its replaced with 0). The test set perplexities for the forward and backward models (measured on the CoNLL 2003 test data) were 106.9 and 104.2, respectively. Including embeddings from these language models decreased performance slightly compared to the baseline system without any LM. This result supports the hypothesis that adding lan- guage models help because they learn composi- tion functions (i.e., the RNN parameters in the lan- guage model) from much larger data compared to the composition functions in the baseline tagger, which are only learned from labeled data. Importance of task speci\ufb01c RNN.",
            "Importance of task speci\ufb01c RNN. To under- stand the importance of including a task speci\ufb01c sequence RNN we ran an experiment that removed the task speci\ufb01c sequence RNN and used only the LM embeddings with a dense layer and CRF to predict output tags. In this setup, performance was very low, 88.17 F1, well below our baseline. This result con\ufb01rms that the RNNs in the baseline tag- ger encode essential information which is not en- coded in the LM embeddings. This is unsurprising since the RNNs in the baseline tagger are trained on labeled examples, unlike the RNN in the lan- guage model which is only trained on unlabeled examples. Note that the LM weights are \ufb01xed in this experiment. Dataset size. A priori, we expect the addition of LM embeddings to be most bene\ufb01cial in cases where the task speci\ufb01c annotated datasets are small. To test this hypothesis, we replicated the setup from Yang et al.",
            "Dataset size. A priori, we expect the addition of LM embeddings to be most bene\ufb01cial in cases where the task speci\ufb01c annotated datasets are small. To test this hypothesis, we replicated the setup from Yang et al. (2017) that samples 1% of the CoNLL 2003 training set and compared the performance of TagLM to our baseline with- out LM. In this scenario, test F1 increased 3.35% (from 67.66 to 71.01%) compared to an increase of 1.06% F1 for a similar comparison with the full training dataset. The analogous increases in Yang et al. (2017) are 3.97% for cross-lingual trans- fer from CoNLL 2002 Spanish NER and 6.28% F1 for transfer from PTB POS tags. However, they found only a 0.06% F1 increase when using the full training data and transferring from both CoNLL 2000 chunks and PTB POS tags.",
            "However, they found only a 0.06% F1 increase when using the full training data and transferring from both CoNLL 2000 chunks and PTB POS tags. Taken together, this suggests that for very small labeled training sets, transferring from other tasks yields a large improvement, but this improvement almost disappears when the training data is large. On the other hand, our approach is less dependent on the training set size and signi\ufb01cantly improves perfor- mance even with larger training sets. Number of parameters. Our TagLM formula- tion increases the number of parameters in the sec- ond RNN layer R2 due to the increase in the input dimension h1 if all other hyperparameters are held constant. To con\ufb01rm that this did not have a ma- terial impact on the results, we ran two additional experiments. In the \ufb01rst, we trained a system with- out a LM but increased the second RNN layer hid- den dimension so that number of parameters was the same as in TagLM. In this case, performance decreased slightly (by 0.15% F1) compared to the baseline model, indicating that solely increasing parameters does not improve performance.",
            "In this case, performance decreased slightly (by 0.15% F1) compared to the baseline model, indicating that solely increasing parameters does not improve performance. In the second experiment, we decreased the hidden di- mension of the second RNN layer in TagLM to give it the same number of parameters as the base- line no LM model. In this case, test F1 increased slightly to 92.00 \u00b1 0.11 indicating that the addi- tional parameters in TagLM are slightly hurting",
            "performance.4 Does the LM transfer across domains? One artifact of our evaluation framework is that both the labeled data in the chunking and NER tasks and the unlabeled text in the 1 Billion Word Benchmark used to train the bidirectional LMs are derived from news articles. To test the sensitiv- ity to the LM training domain, we also applied TagLM with a LM trained on news articles to the SemEval 2017 Shared Task 10, ScienceIE.5 Scien- ceIE requires end-to-end joint entity and relation- ship extraction from scienti\ufb01c publications across three diverse \ufb01elds (computer science, material sciences, and physics) and de\ufb01nes three broad en- tity types (Task, Material and Process). For this task, TagLM increased F1 on the development set by 4.12% (from 49.93 to to 54.05%) for entity ex- traction over our baseline without LM embeddings and it was a major component in our winning sub- mission to ScienceIE, Scenario 1 (Ammar et al., 2017).",
            "We conclude that LM embeddings can im- prove the performance of a sequence tagger even when the data comes from a different domain. 4 Related work Unlabeled data. TagLM was inspired by the widespread use of pre-trained word embeddings in supervised sequence tagging models. Besides pre-trained word embeddings, our method is most closely related to Li and McCallum (2005). In- stead of using a LM, Li and McCallum (2005) uses a probabilistic generative model to infer context- sensitive latent variables for each token, which are then used as extra features in a supervised CRF tagger (Lafferty et al., 2001). Other semi- supervised learning methods for structured pre- diction problems include co-training (Blum and Mitchell, 1998; Pierce and Cardie, 2001), expec- tation maximization (Nigam et al., 2000; Mohit and Hwa, 2005), structural learning (Ando and Zhang, 2005) and maximum discriminant func- tions (Suzuki et al., 2007; Suzuki and Isozaki, 2008).",
            "It is easy to combine TagLM with any of the above methods by including LM embed- dings as additional features in the discriminative components of the model (except for expectation maximization). A detailed discussion of semi- supervised learning methods in NLP can be found 4A similar experiment for the Chunking task did not im- prove F1 so this conclusion is task dependent. 5https:\/\/scienceie.github.io\/ in (S\u00f8gaard, 2013). Melamud et al. (2016) learned a context en- coder from unlabeled data with an objective func- tion similar to a bi-directional LM and applied it to several NLP tasks closely related to the unlabeled objective function: sentence completion, lexical substitution and word sense disambiguation. LM embeddings are related to a class of meth- ods (e.g., Le and Mikolov, 2014; Kiros et al., 2015; Hill et al., 2016) for learning sentence and document encoders from unlabeled data, which can be used for text classi\ufb01cation and textual en- tailment among other tasks.",
            "Dai and Le (2015) pre-trained LSTMs using language models and se- quence autoencoders then \ufb01ne tuned the weights for classi\ufb01cation tasks. In contrast to our method that uses unlabeled data to learn token-in-context embeddings, all of these methods use unlabeled data to learn an encoder for an entire text sequence (sentence or document). Neural language models. LMs have always been a critical component in statistical machine translation systems (Koehn, 2009). Recently, neu- ral LMs (Bengio et al., 2003; Mikolov et al., 2010) have also been integrated in neural machine trans- lation systems (e.g., Kalchbrenner and Blunsom, 2013; Devlin et al., 2014) to score candidate trans- lations. In contrast, TagLM uses neural LMs to encode words in the input sequence. Unlike forward LMs, bidirectional LMs have received little prior attention.",
            "In contrast, TagLM uses neural LMs to encode words in the input sequence. Unlike forward LMs, bidirectional LMs have received little prior attention. Most similar to our formulation, Peris and Casacuberta (2015) used a bidirectional neural LM in a statistical ma- chine translation system for instance selection. They tied the input token embeddings and soft- max weights in the forward and backward direc- tions, unlike our approach which uses two distinct models without any shared parameters. Frinken et al. (2012) also used a bidirectional n-gram LM for handwriting recognition. Interpreting RNN states. Recently, there has been some interest in interpreting the activations of RNNs. Linzen et al. (2016) showed that sin- gle LSTM units can learn to predict singular-plural distinctions. Karpathy et al. (2015) visualized character level LSTM states and showed that indi- vidual cells capture long-range dependencies such as line lengths, quotes and brackets. Our work complements these studies by showing that LM states are useful for downstream tasks as a way",
            "of interpreting what they learn. Other sequence tagging models. Current state of the art results in sequence tagging problems are based on bidirectional RNN models. However, many other sequence tagging models have been proposed in the literature for this class of problems (e.g., Lafferty et al., 2001; Collins, 2002). LM em- beddings could also be used as additional features in other models, although it is not clear whether the model complexity would be suf\ufb01cient to effec- tively make use of them. 5 Conclusion In this paper, we proposed a simple and general semi-supervised method using pre-trained neural language models to augment token representations in sequence tagging models. Our method signi\ufb01- cantly outperforms current state of the art models in two popular datasets for NER and Chunking. Our analysis shows that adding a backward LM in addition to traditional forward LMs consistently improves performance. The proposed method is robust even when the LM is trained on unlabeled data from a different domain, or when the base- line model is trained on a large number of labeled examples.",
            "Our analysis shows that adding a backward LM in addition to traditional forward LMs consistently improves performance. The proposed method is robust even when the LM is trained on unlabeled data from a different domain, or when the base- line model is trained on a large number of labeled examples. Acknowledgments We thank Chris Dyer, Julia Hockenmaier, Jayant Krishnamurthy, Matt Gardner and Oren Etzioni for comments on earlier drafts that led to substan- tial improvements in the \ufb01nal version. References Waleed Ammar, Matthew E. Peters, Chandra Bhaga- vatula, and Russell Power. 2017. The AI2 sys- tem at SemEval-2017 Task 10 (ScienceIE): semi- supervised end-to-end entity and relation extraction. In ACL workshop (SemEval). Rie Kubota Ando and Tong Zhang. 2005. A high- performance semi-supervised learning method for text chunking. In ACL. Yoshua Bengio, R\u00b4ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic lan- guage model. In JMLR.",
            "A high- performance semi-supervised learning method for text chunking. In ACL. Yoshua Bengio, R\u00b4ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic lan- guage model. In JMLR. Avrim Blum and Tom Mitchell. 1998. Combining la- beled and unlabeled data with co-training. In COLT. Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, and Phillipp Koehn. 2014. One bil- lion word benchmark for measuring progress in sta- tistical language modeling. CoRR abs\/1312.3005. Jason Chiu and Eric Nichols. 2016. Named entity recognition with bidirectional LSTM-CNNs. In TACL. Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah- danau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder ap- proaches. In SSST@EMNLP. Michael Collins.",
            "Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah- danau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder ap- proaches. In SSST@EMNLP. Michael Collins. 2002. Discriminative training meth- ods for hidden markov models: Theory and experi- ments with perceptron algorithms. In EMNLP. Ronan Collobert, Jason Weston, L\u00b4eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa. 2011. Natural language processing (almost) from scratch. In JMLR. Andrew M. Dai and Quoc V. Le. 2015. Semi- supervised sequence learning. In NIPS. Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard M Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In ACL.",
            "Semi- supervised sequence learning. In NIPS. Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard M Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In ACL. Volkmar Frinken, Alicia Forn\u00b4es, Josep Llad\u00b4os, and Jean-Marc Ogier. 2012. Bidirectional language model for handwriting recognition. In Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syn- tactic Pattern Recognition (SSPR). Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsu- ruoka, and Richard Socher. 2016. A joint many-task model: Growing a neural network for multiple nlp tasks. CoRR abs\/1611.01587. Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sentences from unlabelled data. In HLT-NAACL. Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997.",
            "Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sentences from unlabelled data. In HLT-NAACL. Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997. Long short-term memory. Neural Computation 9. Rafal J\u00b4ozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. 2016. Exploring the lim- its of language modeling. CoRR abs\/1602.02410. Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In EMNLP. Andrej Karpathy, Justin Johnson, and Li Fei-Fei. 2015. Visualizing and understanding recurrent networks. In ICLR workshop. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In ICLR.",
            "2015. Visualizing and understanding recurrent networks. In ICLR workshop. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In ICLR. Jamie Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Raquel Urtasun, Antonio Tor- ralba, and Sanja Fidler. 2015. Skip-thought vectors. In NIPS. Philipp Koehn. 2009. Statistical machine translation. Cambridge University Press.",
            "John D. Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random \ufb01elds: Prob- abilistic models for segmenting and labeling se- quence data. In ICML. Guillaume Lample, Miguel Ballesteros, Sandeep Sub- ramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In NAACL-HLT. Quoc V. Le and Tomas Mikolov. 2014. Distributed rep- resentations of sentences and documents. In ICML. Wei Li and Andrew McCallum. 2005. Semi-supervised sequence modeling with syntactic topic models. In AAAI. Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing the ability of LSTMs to learn syntax-sensitive dependencies. In TACL. Gang Luo, Xiaojiang Huang, Chin-Yew Lin, and Za- iqing Nie. 2015. Joint entity recognition and disam- biguation. In EMNLP.",
            "Assessing the ability of LSTMs to learn syntax-sensitive dependencies. In TACL. Gang Luo, Xiaojiang Huang, Chin-Yew Lin, and Za- iqing Nie. 2015. Joint entity recognition and disam- biguation. In EMNLP. Xuezhe Ma and Eduard H. Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNs- CRF. In ACL. Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning generic context em- bedding with bidirectional lstm. In CoNLL. Tomas Mikolov, Martin Kara\ufb01\u00b4at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recur- rent neural network based language model. In Inter- speech. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositional- ity. In NIPS.",
            "In Inter- speech. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositional- ity. In NIPS. Behrang Mohit and Rebecca Hwa. 2005. Syntax-based semi-supervised named entity tagging. In ACL. Kamal Nigam, Andrew Kachites McCallum, Sebastian Thrun, and Tom Mitchell. 2000. Text classi\ufb01cation from labeled and unlabeled documents using em. Machine learning . Jeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. Glove: Global vectors for word representation. In EMNLP. \u00b4Alvaro Peris and Francisco Casacuberta. 2015. A bidi- rectional recurrent neural language model for ma- chine translation. Procesamiento del Lenguaje Nat- ural . David Pierce and Claire Cardie. 2001. Limitations of co-training for natural language learning from large datasets.",
            "2015. A bidi- rectional recurrent neural language model for ma- chine translation. Procesamiento del Lenguaje Nat- ural . David Pierce and Claire Cardie. 2001. Limitations of co-training for natural language learning from large datasets. In EMNLP. Lev-Arie Ratinov and Dan Roth. 2009. Design chal- lenges and misconceptions in named entity recogni- tion. In CoNLL. Hasim Sak, Andrew W. Senior, and Franoise Beaufays. 2014. Long short-term memory recurrent neural network architectures for large scale acoustic mod- eling. In INTERSPEECH. Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the CoNLL-2000 shared task chunk- ing. In CoNLL\/LLL. Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In CoNLL. Anders S\u00f8gaard. 2013.",
            "Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In CoNLL. Anders S\u00f8gaard. 2013. Semi-supervised learning and domain adaptation in natural language processing. Synthesis Lectures on Human Language Technolo- gies . Anders S\u00f8gaard and Yoav Goldberg. 2016. Deep multi-task learning with low level tasks supervised at lower layers. In ACL. Jun Suzuki, Akinori Fujino, and Hideki Isozaki. 2007. Semi-supervised structured output learning based on a hybrid generative and discriminative approach. In EMNLP-CoNLL. Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised sequential labeling and segmentation using giga- word scale unlabeled data. In ACL. Zhilin Yang, Ruslan Salakhutdinov, and William W. Cohen. 2017. Transfer learning for sequence tag- ging with hierarchical recurrent networks. In ICLR."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1705.00108.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 11487.0,
    "avg_doclen_est": 179.484375
}
