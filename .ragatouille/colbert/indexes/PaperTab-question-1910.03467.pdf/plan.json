{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "arXiv:1910.03467v2  [cs.CL]  17 Oct 2019 Overcoming the Rare Word Problem for Low-Resource Language Pairs in Neural Machine Translation Thi-Vinh Ngo Thai Nguyen University ntvinh@ictu.edu.vn Phuong-Thai Nguyen Vietnam National University thainp@vnu.edu.vn Thanh-Le Ha Karlsruhe Institute of Technology thanh-le.ha@kit.edu Le-Minh Nguyen JAIST, Japan nguyenml@jaist.ac.jp Abstract Among the six challenges of neural machine translation (NMT) coined by (Koehn and Knowles, 2017), rare-word problem is considered the most severe one, especially in translation of low-resource languages. In this paper, we propose three solutions to address the rare words in neural machine translation systems. First, we enhance source context to predict the target words by connecting directly the source embeddings to the output of the attention component in NMT. Second, we propose an algorithm to learn morphology of unknown words for English in supervised way in order to minimize the adverse effect of rare-word problem.",
            "First, we enhance source context to predict the target words by connecting directly the source embeddings to the output of the attention component in NMT. Second, we propose an algorithm to learn morphology of unknown words for English in supervised way in order to minimize the adverse effect of rare-word problem. Finally, we exploit synonymous relation from the WordNet to overcome out- of-vocabulary (OOV) problem of NMT. We evaluate our approaches on two low-resource language pairs: English-Vietnamese and Japanese-Vietnamese. In our experiments, we have achieved signi\ufb01cant improvements of up to roughly +1.0 BLEU points in both language pairs. 1 Introduction NMT systems have achieved better perfor- mance compared to statistical machine transla- tion (SMT) systems in recent years not only on available data language pairs (Sennrich et al., 2016a; Cho et al., 2016), but also on low- resource language pairs (Nguyen and Chiang, 2017; Cettolo et al., 2016). Nevertheless, NMT still exists many challenges which have adverse effects on its effectiveness (Koehn and Knowles, 2017).",
            "Nevertheless, NMT still exists many challenges which have adverse effects on its effectiveness (Koehn and Knowles, 2017). One of these challenges is that NMT has biased tend in translating high-frequency words, thus words which have lower frequencies are of- ten translated incorrectly. This challenge has also been con\ufb01rmed again in (Nguyen and Chiang, 2017), and they have proposed two strategies to tackle this problem with modi\ufb01cations on the model\u2019s output distribution: one for normaliz- ing some matrices by \ufb01xing them to constants after several training epochs and another for adding a direct connection from source embed- dings through a simple feed forward neural net- work (FFNN). These approaches increase the size and the training time of their NMT systems. In this work, we follow their second approach but sim- plify the computations by replacing FFNN with two single operations.",
            "These approaches increase the size and the training time of their NMT systems. In this work, we follow their second approach but sim- plify the computations by replacing FFNN with two single operations. Despite above approaches can improve the pre- diction of rare words, however, NMT systems of- ten use limited vocabularies in their sizes, from 30K to 80K most frequent words of the training data, in order to reduce computational complex- ity and the sizes of the models (Bahdanau et al., 2015; Luong et al., 2015b), so the rare-word trans- lation are still problematic in NMT. Even when we use a larger vocabulary, this situation still ex- ists (Jean et al., 2015). A word which has not seen in the vocabulary of the input text (called unknown word) are presented by the unk symbol in NMT systems. Inspired by alignments and phrase ta- bles in phrase-based machine translation (SMT) as suggested by (Koehn et al., 2007), (Luong et al., 2015b) proposed to address OOV words using an annotated training corpus.",
            "Inspired by alignments and phrase ta- bles in phrase-based machine translation (SMT) as suggested by (Koehn et al., 2007), (Luong et al., 2015b) proposed to address OOV words using an annotated training corpus. They then used a dic- tionary generated from alignment model or maps between source and target words to determine the translations of unks if translations are not found. (Sennrich et al., 2016b) proposed to reduce un- known words using Gage\u2019s Byte Pair Encoding (BPE) algorithm (Gage, 1994), but NMT systems are less effective for low-resource language pairs due to the lack of data and also for other lan- guages that sub-word are not the optimal transla- tion unit. In this paper, we employ several tech- niques inspired by the works from NMT and the traditional SMT mentioned above. Instead of a",
            "loosely unsupervised approach, we suggest a su- pervised approach to solve this trouble using syn- onymous relation of word pairs from WordNet on Japanese\u2192Vietnamese and English\u2192Vietnamese systems. To leverage effectiveness of this relation in English, we transform variants of words in the source texts to their original forms by separating their af\ufb01xes collected by hand. Our contributes in this work are: \u2022 We release the state-of-the-art for Japanese- Vietnamese NMT systems. \u2022 We proposed the approach to deal with the rare word translation by integrating source embeddings to the attention component of NMT. \u2022 We present a supervised algorithm to re- duce the number of unknown words for the English\u2192Vietnamese translation system. \u2022 We demonstrate the effectiveness of leverag- ing linguistic information from WordNet to alleviate the rare-word problem in NMT. 2 Neural Machine Translation Our NMT system use a bidirectional recurrent neural network (biRNN) as an encoder and a single-directional RNN as a decoder with input feeding of (Luong et al., 2015a) and the attention mechanism of (Bahdanau et al., 2015).",
            "The En- coder\u2019s biRNN are constructed by two RNNs with the hidden units in the LSTM cell, one for for- ward and the other for backward of the source sen- tence x = (x1, ..., xn). Every word xi in sen- tence is \ufb01rst encoded into a continuous represen- tation Es(xi), called the source embedding.",
            "Every word xi in sen- tence is \ufb01rst encoded into a continuous represen- tation Es(xi), called the source embedding. Then x is transformed into a \ufb01xed-length hidden vec- tor hi representing the sentence at the time step i, which called the annotation vector, combined by the states of forward \u2212\u2192 h i and backward \u2190\u2212 h i: \u2212\u2192 h i = f(Es(xi), \u2212\u2192 h i\u22121) \u2190\u2212 h i = f(Es(xi), \u2190\u2212 h i+1) The decoder generates the target sentence y = (y1, ..., ym), and at the time step j, the predicted probability of the target word yj is estimated as follows: p(yj|y<j, x) \u221dsoftmax(Wzj + b) where zj is the output hidden states of the at- tention mechanism and computed by the previous output hidden states zj\u22121, the embedding of pre- vious target word Et(yj\u22121) and the context cj: zj = g(Et(yj\u22121), zj\u22121, cj) The source context cj is the weighted sum of the encoder\u2019s annotation vectors hi: cj = Pn i=1 \u03b1ijhi where \u03b1ij are the alignment weights, denoting the relevance between the current target word yj and all source annotation vectors hi.",
            "3 Rare Word translation In this section, we present the details about our approaches to overcome the rare word situation. While the \ufb01rst strategy augments the source con- text to translate low-frequency words, the remain- ing strategies reduce the number of OOV words in the vocabulary. 3.1 Low-frequency Word Translation The attention mechanism in RNN-based NMT maps the target word into source context corre- sponding through the annotation vectors hi. In the recurrent hidden unit, hi is computed from the previous state ht\u22121. Therefore, the infor- mation \ufb02ow of the words in the source sen- tence may be diminished over time. This leads to the accuracy reduction when translating low- frequency words, since there is no direct connec- tion between the target word and the source word.",
            "Therefore, the infor- mation \ufb02ow of the words in the source sen- tence may be diminished over time. This leads to the accuracy reduction when translating low- frequency words, since there is no direct connec- tion between the target word and the source word. To alleviate the adverse impact of this problem, (Nguyen and Chiang, 2017) combined the source embeddings with the predictive distribution over the output target word in several following steps: Firstly, the weighted average vector of the source embeddings is computed as follows: lt = tanh X e \u03b1j(e)fe where \u03b1j(e) are alignment weights in the attention component and fe = Es(x), are the embeddings of the source words. Then lj is transformed through one-hidden- layer FFNN with residual connection proposed by (He et al., 2015): tj = tanh(Wllj) + lj Finally, the output distribution over the target word is calculated by: p(yj|y<j, x) = softmax(Wzj + b + Wttj + bt) The matrices Wl, Wt and bt are trained together with other parameters of the NMT model.",
            "This approach improves the performance of the NMT systems but introduces more computations as the model size increase due to the additional parameters Wl, Wt and bt. We simplify this method by using the weighted average of source embeddings directly in the softmax output layer: p(yj|y<j, x) = softmax(W(zj + lj) + b) Our method does not learn any additional parame- ters. Instead, it requires the source embedding size to be compatible with the decoder\u2019s hidden states. With the additional information provided from the source embeddings, we achieve similar improve- ments compared to the more expensive method de- scribed in (Nguyen and Chiang, 2017). 3.2 Reducing Unknown Words In our previous experiments for English\u2192Vietnamese, BPE algo- rithm (Sennrich et al., 2016b) applied to the source side does not signi\ufb01cantly improves the systems despite it is able to reduce the number of unknown English words. We speculate that it might be due to the morphological differences be- tween the source and the target languages (English and Vietnamese in this case).",
            "We speculate that it might be due to the morphological differences be- tween the source and the target languages (English and Vietnamese in this case). The unsupervised way of BPE while learning sub-words in English thus might be not explicit enough to provide the morphological information to the Vietnamese side. In this work, we would like to attempt a more explicit, supervised way. We collect 52 popular af\ufb01xes (pre\ufb01xes and suf\ufb01xes) in English and then apply the separating af\ufb01xes algorithm (called SAA) to reduce the number of unknown words as well as to force our NMT systems to learn better morphological mappings between two languages. The main ideal of our SAA is to separate af\ufb01xes of unknown words while ensuring that the rest of them still exists in the vocabulary. Let the vocab- ulary V containing K most frequency words from the training set T1, a set of pre\ufb01xes P, a set of suf- \ufb01xes S, we call word w\u2032 is the rest of an unknown word or rare word w after delimiting its af\ufb01xes.",
            "We iteratively pick a w from N words (including unknown words and rare words) of the source text T2 to consider if w starts with a pre\ufb01x p in P or ends with a suf\ufb01x s in S, we then determine split- ting its af\ufb01xes if w\u2032 in V . A rare word in V also can be separated its af\ufb01xes if its frequency is less than the given threshold. We set this threshold by 2 in our experiments. Similarly to BPE approach, we also employ a pair of the special symbol @ for separating af\ufb01xes from the word. Listing 3.2 shows our SAA algorithm. Input : T1 , T2 , P , S ,",
            "We set this threshold by 2 in our experiments. Similarly to BPE approach, we also employ a pair of the special symbol @ for separating af\ufb01xes from the word. Listing 3.2 shows our SAA algorithm. Input : T1 , T2 , P , S , t h r e s h o l d =1 Output : t he out put t e x t T V = get_most_frequency_K_words ( T1 ) N = get _words_from_t he_source_t ext ( T2 ) T = T2 f o r each word w i n N: i f w not i n V or f r e q (w) <= t h r e s h o l d : f o r each p r e f i x p i n P : w1 = s e p a r a t e _ p r e f i x ( p ) i f w1 != w and w1 i n V: T = r e p l a c e (T , w, w1 ,",
            "w, w1 , p ) break f o r each s u f f i x s i n S : w2 = s e p a r a t e _ s u f f i x ( s ) i f w2 != w1 and w2 i n V: T = r e p l a c e (T , w2 , w1 , s ) break r e t u r n T Example : i n t e r c e p t e d \u2212> i n t e r c e p t @@ed i m p u l s i v e l y \u2212> i mpul si ve @@ly overl ooks \u2212> over@@ look @@s disowned \u2212> dis@@ own @@ed The proposed SAA for separating af\ufb01xes from words. 3.3 Dealing with OOV using WordNet WordNet is a lexical database grouping words into sets which share some semantic relations. Its version for English is proposed for the \ufb01rst time by (Fellbaum, 1998).",
            "3.3 Dealing with OOV using WordNet WordNet is a lexical database grouping words into sets which share some semantic relations. Its version for English is proposed for the \ufb01rst time by (Fellbaum, 1998). It becomes a useful re- source for many tasks of natural language process- ing (Kolte and Bhirud, 2008; M\u00e9ndez O., 2013; Gao et al., 2014). WordNet are available mainly for English and German, the version for other lan- guages are being developed including some Asian languages in such as Japanese, Chinese, Indone- sian and Vietnamese. Several works have em- ployed WordNet in SMT systems(Khodak et al., 2017; Arcan et al., 2019) but to our knowledge, none of the work exploits the bene\ufb01ts of WordNet in order to ease the rare word problem in NMT. In this work, we propose the learning synonymous algorithm (called LSW) from the WordNet of En- glish and Japanese to handle unknown words in our NMT systems.",
            "In this work, we propose the learning synonymous algorithm (called LSW) from the WordNet of En- glish and Japanese to handle unknown words in our NMT systems. In WordNet, synonymous words are organized in groups which are called synsets. Our aim is to replace an OOV word by its synonym which appears in the vocabulary of the translation sys- tem. From the training set of the source lan- guage T1, we extract the vocabulary V in size",
            "of K most frequent words. For each OOV word from T1, we learn its synonyms which exist in the V from the WordNet W. The synonyms are then arranged in the descending order of their fre- quencies to facilitate selection of the n best words which have the highest frequencies. The output \ufb01le C of the algorithm contains OOV words and its corresponding synonyms and then it is applied to the input text T2. We also utilize a frequency threshold for rare words in the same way as in SAA algorithm. In practice, we set this thresh- old as 0, meaning no words on V is replaced by its synonym. If a source sentence has m un- known words and each of them has n best syn- onyms, it would generate mn sentences. Transla- tion process allow us to select the best hypothe- sis based on their scores. Because of each word in the WordNet can belong to many synsets with different meanings, thus an inappropriate word can be placed in the current source context. We will solve this situation in the further works. Our systems only use 1-best synonym for each OOV word. Listing 3.3 presents the LSW algorithm.",
            "We will solve this situation in the further works. Our systems only use 1-best synonym for each OOV word. Listing 3.3 presents the LSW algorithm. Input : T1 , T2 , Ws , t h r e s h o l d =1 Output : \u2212C: The l i s t c o n t a i n s synonymous words f o r OOV words .",
            "Listing 3.3 presents the LSW algorithm. Input : T1 , T2 , Ws , t h r e s h o l d =1 Output : \u2212C: The l i s t c o n t a i n s synonymous words f o r OOV words . \u2212T : The i n p u t of t he t r a n s l a t i o n systems def learn_synonym ( ) V=get_most_frequency_K_words ( T1 ) N= get _words_from_t he_source_t ext ( T2 ) C={} f o r each word w i n N: i f w not i n V or f r e q (w) <= t h r e s h o l d : I =get_synonyms_from_WordNet (w, Ws ) f o r each i i n I : i f i not i n V: I=I \\ { i } #remove i from I sort_words_by_descend_of_frequency ( I ) C = C \u222a{w, I} r e t u r n C n_best =3 a p p l y _ t o _ i n p u t _ f i l e (C, T2 , n_best ) The LSW learns synonymous words from WordNet.",
            "4 Experiments We evaluate our approaches on the English- Vietnamese and the Japanese-Vietnamese trans- lation systems. Translation performance is mea- sured in BLEU (Kishore Papineni and Zhu, 2012) by the multi-BLEU scripts from Moses1. 1https:\/\/github.com\/moses-smt\/mosesdecoder\/tree\/master\/scripts 4.1 Datasets We consider two low-resource language pairs: Japanese-Vietnamese and English-Vietnamese. For Japanese-Vietnamese, we use the TED data provided by WIT3 (Cettolo et al., 2012) and compiled by (Ngo et al., 2018). The training set includes 106758 sentence pairs, the validation and test sets are dev2010 (568 pairs) and tst2010 (1220 pairs). For English\u2192Vietnamese, we use the dataset from IWSLT 2015 (Mauro Cettolo and Federico, 2015) with around 133K sentence pairs for the training set, 1553 pairs in tst2012 as the validation and 1268 pairs in tst2013 as the test sets.",
            "For LSW algorithm, we crawled pairs of syn- onymous words from Japanese-English Word- Net2 and achieved 315850 pairs for English and 1419948 pairs for Japanese. 4.2 Preprocessing For English and Vietnamese, we tokenized the texts and then true-cased the tokenized texts using Moses script. We do not use any word segmenta- tion tool for Vietnamese. For comparison purpose, Sennrich\u2019s BPE algorithm is applied for English texts. Following the same preprocessing steps for Japanese (JPBPE) in (Ngo et al., 2018), we use KyTea3 (Neubig et al., 2011) to tokenize texts and then apply BPE on those texts. The number of BPE merging operators are 50k for both Japanese and English. 4.3 Systems and Training We implement our NMT systems using OpenNMT-py framework4 (Klein et al., 2017) with the same settings as in (Ngo et al., 2018) for our baseline systems. Our system are built with two hidden layers in both encoder and decoder, each layer has 512 hidden units.",
            "Our system are built with two hidden layers in both encoder and decoder, each layer has 512 hidden units. In the encoder, a BiLSTM architecture is used for each layer and in the decoder, each layer are basically an LSTM layer. The size of embedding layers in both source and target sides is also 512. Adam optimizer is used with the initial learning rate of 0.001 and then we apply learning rate annealing. We train our systems for 16 epochs with the batch size of 32. Other parameters are the same as the default settings of OpenNMT-py. 2http:\/\/compling.hss.ntu.edu.sg\/wnja\/ 3http:\/\/www.phontron.com\/kytea\/ 4https:\/\/github.com\/OpenNMT\/OpenNMT-py",
            "No. Systems Japanese\u2192Vietnamese dev2010 tst2010 (1) Baseline 7.91 9.42 (2) + Source Embedding 7.77 9.96 (3) + LSW 8.37 10.34 (4) JPBPE+VNBPE at Ngo et al (2018) 7.77 9.04 (5) JPBPE+VNBPE + BT + Mixsource at Ngo et al (2018) 8.56 9.64 No. Systems Vietnamese\u2192Japanese dev2010 tst2010 (1) Baseline 9.53 (9.53) 10.95 (10.99) (2) + Source Embedding 10.51 (10.51) 11.37 (11.39) (3) JPBPE+VNBPE at Ngo et al (2018) 9.74 11.13 Table 1: Results of Japanese-Vietnamese NMT systems We then modify the baseline architecture with the alternative proposed in Section 3.1 in comparison to our baseline systems. All settings are the same as the baseline systems.",
            "All settings are the same as the baseline systems. 4.4 Results In this section, we show the effectiveness of our methods on two low-resource language pairs and compare them to the other works. The empirical results are shown in Table 1 for Japanese-Vietnamese and in Table 3 for English- Vietnamese. Note that, the Multi-BLEU is only measured in the Japanese\u2192Vietnamese direction and the standard BLEU points are written in brackets. 4.4.1 Japanese-Vietnamese Translation We conduct two out of the three proposed ap- proaches for Japanese-Vietnamese translation sys- tems and the results are given in the Table 1. Baseline Systems. We \ufb01nd that our transla- tion systems which use Sennrich\u2019s BPE method for Japanese texts and do not use word segmen- tation for Vietnamese texts are neither better or in- signi\ufb01cant differences compare to those systems used word segmentation in (Ngo et al., 2018).",
            "Par- ticularly, we obtained +0.38 BLEU points between (1) and (4) in the Japanese\u2192Vietnamese and - 0.18 BLEU points between (1) and (3) in the Vietnamese\u2192Japanese. Our Approaches. On the systems trained with the modi\ufb01ed architecture mentioned in the sec- tion 3.1, we obtained an improvements of +0.54 BLEU points in the Japanese\u2192Vietnamese and +0.42 BLEU points on the Vietnamese\u2192Japanese compared to the baseline systems. Due to the fact that Vietnamese WordNet is not available, we only exploit WordNet to tackle unknown words of Japanese texts in our Japanese\u2192Vietnamese translation system. After using Kytea, Japanese texts are applied LSW al- gorithm to replace OOV words by their synonyms. We choose 1-best synonym for each OOV word. Table 2 shows the number of OOV words replaced by their synonyms. The replaced texts are then BPEd and trained on the proposed architecture. The largest improvement is +0.92 between (1) and (3).",
            "We choose 1-best synonym for each OOV word. Table 2 shows the number of OOV words replaced by their synonyms. The replaced texts are then BPEd and trained on the proposed architecture. The largest improvement is +0.92 between (1) and (3). We observed an improvement of +0.7 BLEU points between (3) and (5) without using data aug- mentation described in (Ngo et al., 2018). Train dev2010 tst2010 Number of words 1015 36 25 Table 2: The number of Japanese OOV words replaced by their synonyms. 4.4.2 English-Vietnamese Translation We examine the effect of all approaches pre- sented in Section 3 for our English-Vietnamese translation systems. Table 3 summarizes those results and the scores from other sys- tems (Nguyen and Chiang, 2017; Huang et al., 2018). Baseline systems. After preprocessing data using Moses scripts, we train the systems of English\u2194Vietnamese on our baseline architec- ture.",
            "Baseline systems. After preprocessing data using Moses scripts, we train the systems of English\u2194Vietnamese on our baseline architec- ture. Our translation system obtained +0.82 BLEU points compared to (Nguyen and Chiang, 2017) in the English\u2192Vietnamese and this is lower than the system of (Huang et al., 2018) with neural phrase-based translation architecture. Our approaches. The datasets from the base- line systems are trained on our modi\ufb01ed NMT architecture. The improvements can be found as +0.55 BLEU points between (1) and (2) in the English\u2192Vietnamese and +0.45 BLEU",
            "points (in tst2012) between (1) and (2) in the Vietnamese\u2192English. For comparison purpose, English texts are split into sub-words using Sennrich\u2019s BPE methods. We observe that, the achieved BLEU points are lower Therefore, we then apply the SAA al- gorithm on the English texts from (2) in the English\u2192Vietnamese. The number of applied words are listed in Table 4. The improvement in BLEU are +0.74 between (4) and (1). Similarly to the Japanese\u2192Vietnamese system, we apply LSW algorithm on the English texts from (4) while selecting 1-best synonym for each OOV word. The number of replaced words on English texts are indicated in the Table 5. Again, we ob- tained a bigger gain of +0.99 (+1.02) BLEU points in English\u2192Vietnamese direction. Compared to the most recent work (Huang et al., 2018), our system reports an improvement of +0.47 standard BLEU points on the same dataset.",
            "Compared to the most recent work (Huang et al., 2018), our system reports an improvement of +0.47 standard BLEU points on the same dataset. We investigate some examples of translations generated by the English\u2192Vietnamese systems with our proposed methods in the Table 6. The bold texts in red color present correct or approx- imate translations while the italic texts in gray color denote incorrect translations. The \ufb01rst ex- ample, we consider two words: presentation and the unknown word applauded. The word presen- tation is predicted correctly as \"b\u00e0i thuy\u1ebft tr\u00ecnh\" in most cases when we combined source con- text through embeddings. The unknown word ap- plauded which has not seen in the vocabulary is ignored in the \ufb01rst two cases (baseline and source embedding) but it is roughly translated as \"hoan ngh\u00eanh\" in the SAA because it is separated into applaud and ed.",
            "The unknown word ap- plauded which has not seen in the vocabulary is ignored in the \ufb01rst two cases (baseline and source embedding) but it is roughly translated as \"hoan ngh\u00eanh\" in the SAA because it is separated into applaud and ed. In the second example, we ob- serve the translations of the unknown word tryout, they are mistaken in the \ufb01rst three cases but in the LSW, it is predicted with a closer meaning as \"b\u00e0i ki\u1ec3m tra\" due to the replacement by its synony- mous word as test. 5 Related Works Addressing unknown words was mentioned early in the Statistical Machine Translation (SMT) sys- tems. Some typical studies as: (Habash, 2008) proposed four techniques to overcome this situ- ation by extend the morphology and spelling of words or using a bilingual dictionary or translit- erating for names. These approaches are dif\ufb01- cult when manipulate to different domains. (Trieu, 2016) trained word embedding models to learn word similarity from monolingual data and an unknown word are then replaced by a its simi- lar word.",
            "These approaches are dif\ufb01- cult when manipulate to different domains. (Trieu, 2016) trained word embedding models to learn word similarity from monolingual data and an unknown word are then replaced by a its simi- lar word. (Madhyastha and Espa\u00f1a Bonet, 2017) used a linear model to learn maps between source and target spaces base on a small initial bilin- gual dictionary to \ufb01nd the translations of source words. However, in NMT, there are not so many works tackling this problem. (Jean et al., 2015) use a very large vocabulary to solve unknown words. (Luong et al., 2015b) generate a dictio- nary from alignment data based on annotated cor- pus to decide the hypotheses of unknown words. (Nguyen and Chiang, 2017) have introduced the solutions for dealing with the rare word problem, however, their models require more parameters, thus, decreasing the overall ef\ufb01ciency.",
            "(Nguyen and Chiang, 2017) have introduced the solutions for dealing with the rare word problem, however, their models require more parameters, thus, decreasing the overall ef\ufb01ciency. In another direction, (Sennrich et al., 2016b) exploited the BPE algorithm to reduce number of unknown words in NMT and achieved signif- icant ef\ufb01ciency on many language pairs. The sec- ond approach presented in this works follows this direction when instead of using an unsupervised method to split rare words and unknown words into sub-words that are able to translate, we use a supervised method. Our third approach using WordNet can be seen as a smoothing way, when we use the translations of the synonymous words to approximate the translation of an OOV word. Another work followed this direction is worth to mention is (Niehues et al., 2016), when they use the morphological and semantic information as the factors of the words to help translating rare words.",
            "Another work followed this direction is worth to mention is (Niehues et al., 2016), when they use the morphological and semantic information as the factors of the words to help translating rare words. 6 Conclusion In this study, we have proposed three difference strategies to handle rare words in NMT, in which the combination of methods brings signi\ufb01cant im- provements to the NMT systems on two low- resource language pairs. In future works, we will consider selecting some appropriate synonymous words for the source sentence from n-best synony- mous words to further improve the performance of the NMT systems and leverage more unsuper- vised methods based on monolingual data to ad- dress rare word problem. 7 Acknowledgments This work is supported by the project \"Building a machine translation system to support translation of documents between Vietnamese and Japanese",
            "No. Systems English\u2192Vietnamese tst2012 tst2013 (1) Baseline 26.91 (24.39) 29.86 (27.52) (2) + Source Embedding 27.41 (24.92) 30.41 (28.05) (3) + Sennrich\u2019s BPE 26.96 (24.46) 30.10 (27.84) (4) + SAA 27.16 (24.67) 30.60 (28.34) (5) + LSW 27.46 (24.99) 30.85 (28.54) (6) Nguyen and Chiang (2017) - 26.7 (7) Huang et al (2018) - 28.07 No.",
            "Systems Vietnamese\u2192English tst2012 tst2013 (1) Baseline 27.97 (28.52) 30.07 (29.89) (2) + Source Embedding 28.42 (29.04) 30.12 (29.93) Table 3: Results of English-Vietnamese NMT systems Train tst2012 tst2013 Number of words 5342 84 93 Table 4: The number of rare words in which their af- \ufb01xes are detached from the English texts in the SAA algorithm. Train tst2012 tst2013 Number of words 1889 37 41 Table 5: The number of English OOV words are re- placed by their synonyms. to help managers and businesses in Hanoi ap- proach to Japanese market\", No. TC.02-2016-03. References Mihael Arcan, John P. McCrae, and Paul Buitelaar. 2019. Polylingual wordnet. CoRR, abs\/1903.01411. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015.",
            "2019. Polylingual wordnet. CoRR, abs\/1903.01411. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate. Proceedings of Inter- national Conference on Learning Representations. M Cettolo, J Niehues, S St\u00fcker, L Bentivogli, R Cat- toni, and M Federico. 2016. The IWSLT 2016 Eval- uation Campaign. In Proceedings of the 13th Inter- national Workshop on Spoken Language Translation (IWSLT 2016), Seattle, WA, USA. Mauro Cettolo, Christian Girardi, and Marcello Fed- erico. 2012. Wit3: Web inventory of transcribed and translated talks. In Proceedings of the 16th Con- ference of the European Association for Machine Translation (EAMT), pages 261\u2013268, Trento, Italy.",
            "2012. Wit3: Web inventory of transcribed and translated talks. In Proceedings of the 16th Con- ference of the European Association for Machine Translation (EAMT), pages 261\u2013268, Trento, Italy. Eunah Cho, Jan Niehues, Thanh-Le Ha, Matthias Sper- ber, Mohammed Mediani, and Alex Waibel. 2016. Adaptation and combination of nmt systems: The kit translation systems for iwslt 2016. In Proceed- ings of the ninth International Workshop on Spoken Language Translation (IWSLT), Seattle, WA. Christiane Fellbaum. 1998. Wordnet: An electronic lexical database. In Bradford Books. Philip Gage. 1994. A new algorithm for data compres- sion. In C Users J., 12(2):23\u201338, February. Ningning Gao, Wanli Zuo, Yaokang Dai, and Wei Lv. 2014. Word sense disambiguation using wordnet se- mantic knowledge. Advances in Intelligent Systems and Computing, 278:147\u2013156.",
            "Ningning Gao, Wanli Zuo, Yaokang Dai, and Wei Lv. 2014. Word sense disambiguation using wordnet se- mantic knowledge. Advances in Intelligent Systems and Computing, 278:147\u2013156. Nizar Habash. 2008. Four techniques for online han- dling of out-of-vocabulary words in arabic english statistical machine translation. In Proceedings of the 46th Annual Meeting of the Association for Compu- tational Linguistics, pages 57\u201360. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep residual learning for image recog- nition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770\u2013778. Po-Sen Huang, Chong Wang, Sitao Huang, Dengyong Zhou, and Li Deng. 2018. Towards neural phrase- based machine translation. In International Confer- ence on Learning Representations. S\u2019ebastien Jean, Kyunghyun Cho, Roland Memise- vic, and Yoshua Bengio. 2015.",
            "2018. Towards neural phrase- based machine translation. In International Confer- ence on Learning Representations. S\u2019ebastien Jean, Kyunghyun Cho, Roland Memise- vic, and Yoshua Bengio. 2015. On using very large target vocabulary for neural machine transla- tion. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1\u201310. Association for Computational Linguistics. Mikhail Khodak, Andrej Risteski, Christiane Fell- baum, and Sanjeev Arora. 2017. Extending and improving wordnet via unsupervised word embed- dings. CoRR, abs\/1705.00217. Todd Ward Kishore Papineni, Salim Roukos and Wei- Jing Zhu. 2012. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Compu- tational Linguistics (ACL), pages 311\u2013318.",
            "2012. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Compu- tational Linguistics (ACL), pages 311\u2013318. Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M. Rush. 2017. Open- nmt: Open-source toolkit for neural machine trans- lation. In Proceedings of the 55th Annual Meeting",
            "Source which presentation have you applauded the most this morning ? Reference b\u00e0i thuy\u1ebft tr\u00ecnh n\u00e0o b\u1ea1n v\u1ed7tay nhi\u1ec1u nh\u1ea5t trong s\u00e1ng nay ? Baseline \u0111i\u1ec1u n\u00e0y c\u00f3 th\u1ec3di\u1ec5n ra trong bu\u1ed5i s\u00e1ng h\u00f4m nay ? +Source Embedding b\u00e0i thuy\u1ebft tr\u00ecnh n\u00e0o c\u00f3 th\u1ec3t\u1ea1o ra bu\u1ed5i s\u00e1ng h\u00f4m nay ? +SAA b\u00e0i thuy\u1ebft tr\u00ecnh n\u00e0y c\u00f3 hoan ngh\u00eanh bu\u1ed5i s\u00e1ng h\u00f4m nay kh\u00f4ng ? +LSW \u0111i\u1ec1u g\u00ec \u0111\u00e3 di\u1ec5n ra v\u1edbi b\u1ea1n bu\u1ed5i s\u00e1ng h\u00f4m nay ? Source I started this as a tryout in Esperance , in Western Australia . Reference t\u00f4i \u0111\u00e3 b\u1eaft \u0111\u1ea7u nh\u01b0 m\u1ed9t s\u1ef1th\u1eednghi\u1ec7m t\u1ea1i Esperance , t\u00e2y \u00dac . Baseline t\u00f4i b\u1eaft \u0111\u1ea7u nh\u01b0 th\u1ebfn\u00e0y nh\u01b0 l\u00e0 m\u1ed9t ng\u01b0\u1eddi \u0111\u00e0n \u00f4ng , \u1edfph\u01b0\u01a1ng T\u00e2y \u00dac .",
            "Baseline t\u00f4i b\u1eaft \u0111\u1ea7u nh\u01b0 th\u1ebfn\u00e0y nh\u01b0 l\u00e0 m\u1ed9t ng\u01b0\u1eddi \u0111\u00e0n \u00f4ng , \u1edfph\u01b0\u01a1ng T\u00e2y \u00dac . +Source Embedding t\u00f4i \u0111\u00e3 b\u1eaft \u0111\u1ea7u \u0111i\u1ec1u n\u00e0y nh\u01b0 l\u00e0 m\u1ed9t ng\u01b0\u1eddi \u0111\u00e0n \u00e1p \u1edfven bi\u1ec3n \u1edfT\u00e2y \u00dac . +SAA t\u00f4i \u0111\u00e3 b\u1eaft \u0111\u1ea7u nh\u01b0 th\u1ebfn\u00e0y v\u1edbi t\u01b0 c\u00e1ch l\u00e0 m\u1ed9t ng\u01b0\u1eddi \u0111\u00e0n \u00f4ng trong l\u0129nh v\u1ef1c n\u00e0y, \u1edfT\u00e2y \u00dac . +LSW t\u00f4i b\u1eaft \u0111\u1ea7u th\u00ed nghi\u1ec7m n\u00e0y nh\u01b0 m\u1ed9t b\u00e0i ki\u1ec3m tra \u1edfQu\u1ea3ng tr\u01b0\u1eddng , \u1edfT\u00e2y \u00dac . Table 6: Examples of outputs from the English\u2192Vietnamese translation systems with the proposed methods. of the Association for Computational Linguistics- System Demonstrations, pages 67\u201372, Vancouver, Canada, July 30 - August 4, 2017. Association for Computational Linguistics.",
            "Table 6: Examples of outputs from the English\u2192Vietnamese translation systems with the proposed methods. of the Association for Computational Linguistics- System Demonstrations, pages 67\u201372, Vancouver, Canada, July 30 - August 4, 2017. Association for Computational Linguistics. Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond\u02c7rej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the As- sociation for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Ses- sions, pages 177\u2013180, Prague, Czech Republic. As- sociation for Computational Linguistics. Philipp Koehn and Rebecca Knowles. 2017. Six chal- lenges for neural machine translation. Proceedings of the First Workshop on Neural Machine Transla- tion, abs\/1706.03872:28\u201339.",
            "As- sociation for Computational Linguistics. Philipp Koehn and Rebecca Knowles. 2017. Six chal- lenges for neural machine translation. Proceedings of the First Workshop on Neural Machine Transla- tion, abs\/1706.03872:28\u201339. S. G. Kolte and S. G. Bhirud. 2008. Word sense dis- ambiguation using wordnet domains. In 2008 First International Conference on Emerging Trends in En- gineering and Technology, pages 1187\u20131191. Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. 2015a. Effective approaches to attention- based neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Nat- ural Language Processing, pages 1412\u20131421. Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Wojciech Zaremba. 2015b. Address- ing the rare word problem in neural machine trans- lation.",
            "Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Wojciech Zaremba. 2015b. Address- ing the rare word problem in neural machine trans- lation. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, abs\/1410.8206:11\u201319. P.S. Madhyastha and C. Espa\u00f1a Bonet. 2017. Learn- ing bilingual projections of embeddings for vocabu- lary expansion in machine translation. In Proceed- ings of the 2nd Workshop on Representation Learn- ing for NLP. pp.139-145. Association for Computa- tional Linguistics, Vancouver, Canada (Aug 2017). Sebastian St\u00fcker Luisa Bentivogli Roldano Cattoni Mauro Cettolo, Jan Niehues and Marcello Federico. 2015. The iwslt 2015 evaluation campaign. In In- ternational Conference on Spoken Language. Moreno-Armend\u00e1riz M.A.",
            "2015. The iwslt 2015 evaluation campaign. In In- ternational Conference on Spoken Language. Moreno-Armend\u00e1riz M.A. M\u00e9ndez O., Calvo H. 2013. A reverse dictionary based on semantic analysis us- ing wordnet. In Advances in Arti\ufb01cial Intelligence and Its Applications, MICAI 2013. Lecture Notes in Computer Science, vol 8265. Springer, Berlin, Hei- delberg. Graham Neubig, Yosuke Nakata, and Shinsuke Mori. 2011. Pointwise prediction for robust, adaptable japanese morphological analysis. In Proceedings of the 49th Annual Meeting of the Association for Com- putational Linguistics: Human Language Technolo- gies: Short Papers - Volume 2, HLT \u201911, pages 529\u2013 533, Stroudsburg, PA, USA. Association for Com- putational Linguistics. Thi-Vinh Ngo, Thanh-Le Ha, Phuong-Thai Nguyen, and Le-Minh Nguyen. 2018. Combining advanced methods in japanese-vietnamese neural machine translation.",
            "Association for Com- putational Linguistics. Thi-Vinh Ngo, Thanh-Le Ha, Phuong-Thai Nguyen, and Le-Minh Nguyen. 2018. Combining advanced methods in japanese-vietnamese neural machine translation. 2018 10th International Conference on Knowledge and Systems Engineering (KSE), pages 318\u2013322. Toan Q. Nguyen and David Chiang. 2017. Improving lexical choice in neural machine translation. Pro- ceedings of NAACL-HLT 2018, pages 334\u2013343. Jan Niehues, Thanh-Le Ha, Eunah Cho, and Alex Waibel. 2016. Using factored word representation in neural network language models. In Proceedings of the First Conference on Machine Translation: Vol- ume 1, Research Papers, pages 74\u201382. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016a. Edinburgh Neural Machine Transla- tion Systems for WMT\u201916. arXiv preprint arXiv:1606.02891. Rico Sennrich, Barry Haddow, and Alexandra Birch.",
            "2016a. Edinburgh Neural Machine Transla- tion Systems for WMT\u201916. arXiv preprint arXiv:1606.02891. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016b. Neural Machine Translation of Rare Words with Subword Units. In Association for Computa- tional Linguistics (ACL 2016). Nguyen L. M. Nguyen P. T. Trieu, H. L. 2016. Dealing with out-of-vocabulary problem in sentence align- ment using word similarity. In Proceedings of the 30th Paci\ufb01c Asia Conference on Language, Informa- tion and Computation: Oral Papers (pp. 259-266)."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1910.03467.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 8457.999938964844,
    "avg_doclen_est": 172.61224365234375
}
