{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features Matteo Pagliardini* Iprova SA, Switzerland mpagliardini@iprova.com Prakhar Gupta* EPFL, Switzerland prakhar.gupta@epfl.ch Martin Jaggi EPFL, Switzerland martin.jaggi@epfl.ch Abstract The recent tremendous success of unsuper- vised word embeddings in a multitude of ap- plications raises the obvious question if simi- lar methods could be derived to improve em- beddings (i.e. semantic representations) of word sequences as well. We present a sim- ple but ef\ufb01cient unsupervised objective to train distributed representations of sentences. Our method outperforms the state-of-the-art unsu- pervised models on most benchmark tasks, highlighting the robustness of the produced general-purpose sentence embeddings. 1 Introduction Improving unsupervised learning is of key impor- tance for advancing machine learning methods, as to unlock access to almost unlimited amounts of data to be used as training resources. The ma- jority of recent success stories of deep learning does not fall into this category but instead relied on supervised training (in particular in the vision domain).",
      "The ma- jority of recent success stories of deep learning does not fall into this category but instead relied on supervised training (in particular in the vision domain). A very notable exception comes from the text and natural language processing domain, in the form of semantic word embeddings trained unsupervised (Mikolov et al., 2013b,a; Penning- ton et al., 2014). Within only a few years from their invention, such word representations \u2013 which are based on a simple matrix factorization model as we formalize below \u2013 are now routinely trained on very large amounts of raw text data, and have become ubiquitous building blocks of a majority of current state-of-the-art NLP applications. While very useful semantic representations are available for words, it remains challenging to pro- duce and learn such semantic embeddings for longer pieces of text, such as sentences, para- graphs or entire documents. Even more so, it re- * indicates equal contribution mains a key goal to learn such general-purpose representations in an unsupervised way.",
      "Even more so, it re- * indicates equal contribution mains a key goal to learn such general-purpose representations in an unsupervised way. Currently, two contrary research trends have emerged in text representation learning: On one hand, a strong trend in deep-learning for NLP leads towards increasingly powerful and com- plex models, such as recurrent neural networks (RNNs), LSTMs, attention models and even Neu- ral Turing Machine architectures. While ex- tremely strong in expressiveness, the increased model complexity makes such models much slower to train on larger datasets. On the other end of the spectrum, simpler \u201cshallow\u201d models such as matrix factorizations (or bilinear models) can bene\ufb01t from training on much larger sets of data, which can be a key advantage, especially in the unsupervised setting. Surprisingly, for constructing sentence embed- dings, naively using averaged word vectors was shown to outperform LSTMs (see Wieting et al. (2016b) for plain averaging, and Arora et al. (2017) for weighted averaging).",
      "Surprisingly, for constructing sentence embed- dings, naively using averaged word vectors was shown to outperform LSTMs (see Wieting et al. (2016b) for plain averaging, and Arora et al. (2017) for weighted averaging). This example shows potential in exploiting the trade-off be- tween model complexity and ability to process huge amounts of text using scalable algorithms, towards the simpler side. In view of this trade- off, our work here further advances unsupervised learning of sentence embeddings. Our proposed model can be seen as an extension of the C-BOW (Mikolov et al., 2013b,a) training objective to train sentence instead of word embeddings. We demon- strate that the empirical performance of our re- sulting general-purpose sentence embeddings very signi\ufb01cantly exceeds the state of the art, while keeping the model simplicity as well as training and inference complexity exactly as low as in aver- aging methods (Wieting et al., 2016b; Arora et al., 2017), thereby also putting the work by (Arora et al., 2017) in perspective.",
      "arXiv:1703.02507v3  [cs.CL]  28 Dec 2018",
      "Contributions. The main contributions in this work can be summarized as follows: \u2022 Model. We propose Sent2Vec1, a sim- ple unsupervised model allowing to com- pose sentence embeddings using word vec- tors along with n-gram embeddings, simulta- neously training composition and the embed- ding vectors themselves. \u2022 Ef\ufb01ciency & Scalability. The computational complexity of our embeddings is only O(1) vector operations per word processed, both during training and inference of the sentence embeddings. This strongly contrasts all neu- ral network based approaches, and allows our model to learn from extremely large datasets, in a streaming fashion, which is a crucial ad- vantage in the unsupervised setting. Fast in- ference is a key bene\ufb01t in downstream tasks and industry applications. \u2022 Performance. Our method shows signi\ufb01- cant performance improvements compared to the current state-of-the-art unsupervised and even semi-supervised models. The resulting general-purpose embeddings show strong ro- bustness when transferred to a wide range of prediction benchmarks.",
      "\u2022 Performance. Our method shows signi\ufb01- cant performance improvements compared to the current state-of-the-art unsupervised and even semi-supervised models. The resulting general-purpose embeddings show strong ro- bustness when transferred to a wide range of prediction benchmarks. 2 Model Our model is inspired by simple matrix factor models (bilinear models) such as recently very successfully used in unsupervised learning of word embeddings (Mikolov et al., 2013b,a; Pen- nington et al., 2014; Bojanowski et al., 2017) as well as supervised of sentence classi\ufb01cation (Joulin et al., 2017). More precisely, these models can all be formalized as an optimization problem of the form min U,V X S\u2208C fS(UV \u03b9S) (1) for two parameter matrices U \u2208Rk\u00d7h and V \u2208 Rh\u00d7|V|, where V denotes the vocabulary. Here, the columns of the matrix V represent the learnt source word vectors whereas those of U represent the target word vectors.",
      "Here, the columns of the matrix V represent the learnt source word vectors whereas those of U represent the target word vectors. For a given sentence S, 1 All our code and pre-trained models are publicly avail- able for download at http://github.com/epfml/ sent2vec which can be of arbitrary length, the indicator vec- tor \u03b9S \u2208{0, 1}|V| is a binary vector encoding S (bag of words encoding). Fixed-length context windows S running over the corpus are used in word embedding methods as in C-BOW (Mikolov et al., 2013b,a) and GloVe (Pennington et al., 2014). Here we have k = |V| and each cost function fS : Rk \u2192R only de- pends on a single row of its input, describing the observed target word for the given \ufb01xed-length context S. In contrast, for sentence embeddings which are the focus of our paper here, S will be entire sentences or documents (therefore vari- able length).",
      "This property is shared with the su- pervised FastText classi\ufb01er (Joulin et al., 2017), which however uses soft-max with k \u226a|V| being the number of class labels. 2.1 Proposed Unsupervised Model We propose a new unsupervised model, Sent2Vec, for learning universal sentence embeddings. Con- ceptually, the model can be interpreted as a natu- ral extension of the word-contexts from C-BOW (Mikolov et al., 2013b,a) to a larger sentence con- text, with the sentence words being speci\ufb01cally optimized towards additive combination over the sentence, by means of the unsupervised objective function. Formally, we learn a source (or context) embed- ding vw and target embedding uw for each word w in the vocabulary, with embedding dimension h and k = |V| as in (1). The sentence embedding is de\ufb01ned as the average of the source word em- beddings of its constituent words, as in (2).",
      "The sentence embedding is de\ufb01ned as the average of the source word em- beddings of its constituent words, as in (2). We augment this model furthermore by also learning source embeddings for not only unigrams but also n-grams present in each sentence, and averaging the n-gram embeddings along with the words, i.e., the sentence embedding vS for S is modeled as vS := 1 |R(S)|V \u03b9R(S) = 1 |R(S)| X w\u2208R(S) vw (2) where R(S) is the list of n-grams (including un- igrams) present in sentence S. In order to pre- dict a missing word from the context, our objective models the softmax output approximated by neg- ative sampling following (Mikolov et al., 2013b). For the large number of output classes |V| to be predicted, negative sampling is known to signi\ufb01- cantly improve training ef\ufb01ciency, see also (Gold- berg and Levy, 2014). Given the binary logistic",
      "loss function \u2113: x 7\u2192log (1 + e\u2212x) coupled with negative sampling, our unsupervised training ob- jective is formulated as follows: min U,V X S\u2208C X wt\u2208S \u0012 \u2113 \u0000u\u22a4 wtvS\\{wt} \u0001 + X w\u2032\u2208Nwt \u2113 \u0000\u2212u\u22a4 w\u2032vS\\{wt} \u0001\u0013 where S corresponds to the current sentence and Nwt is the set of words sampled negatively for the word wt \u2208S. The negatives are sampled2 following a multinomial distribution where each word w is associated with a probability qn(w) := \u221afw \u000e \u0000 P wi\u2208V p fwi \u0001 , where fw is the normal- ized frequency of w in the corpus. To select the possible target unigrams (posi- tives), we use subsampling as in (Joulin et al., 2017; Bojanowski et al., 2017), each word w be- ing discarded with probability 1 \u2212qp(w) where qp(w) := min \b 1, p t/fw + t/fw \t .",
      "Where t is the subsampling hyper-parameter. Subsampling prevents very frequent words of having too much in\ufb02uence in the learning as they would introduce strong biases in the prediction task. With positives subsampling and respecting the negative sampling distribution, the precise training objective function becomes min U,V X S\u2208C X wt\u2208S \u0012 qp(wt)\u2113 \u0000u\u22a4 wtvS\\{wt} \u0001 (3) + |Nwt| X w\u2032\u2208V qn(w\u2032)\u2113 \u0000\u2212u\u22a4 w\u2032vS\\{wt} \u0001\u0013 2.2 Computational Ef\ufb01ciency In contrast to more complex neural network based models, one of the core advantages of the pro- posed technique is the low computational cost for both inference and training. Given a sentence S and a trained model, computing the sentence rep- resentation vS only requires |S| \u00b7 h \ufb02oating point operations (or |R(S)| \u00b7 h to be precise for the n- gram case, see (2)), where h is the embedding di- mension.",
      "The same holds for the cost of training with SGD on the objective (3), per sentence seen in the training corpus. Due to the simplicity of the 2To ef\ufb01ciently sample negatives, a pre-processing table is constructed, containing the words corresponding to the square root of their corpora frequency. Then, the negatives Nwt are sampled uniformly at random from the negatives ta- ble except the target wt itself, following (Joulin et al., 2017; Bojanowski et al., 2017). model, parallel training is straight-forward using parallelized or distributed SGD. Also, in order to store higher-order n-grams ef\ufb01- ciently, we use the standard hashing trick, see e.g. (Weinberger et al., 2009), with the same hashing function as used in FastText (Joulin et al., 2017; Bojanowski et al., 2017).",
      "(Weinberger et al., 2009), with the same hashing function as used in FastText (Joulin et al., 2017; Bojanowski et al., 2017). 2.3 Comparison to C-BOW C-BOW (Mikolov et al., 2013b,a) aims to predict a chosen target word given its \ufb01xed-size context window, the context being de\ufb01ned by the average of the vectors associated with the words at a dis- tance less than the window size hyper-parameter ws. If our system, when restricted to unigram features, can be seen as an extension of C-BOW where the context window includes the entire sen- tence, in practice there are few important differ- ences as C-BOW uses important tricks to facilitate the learning of word embeddings. C-BOW \ufb01rst uses frequent word subsampling on the sentences, deciding to discard each token w with probability qp(w) or alike (small variations exist across imple- mentations). Subsampling prevents the generation of n-grams features, and deprives the sentence of an important part of its syntactical features.",
      "Subsampling prevents the generation of n-grams features, and deprives the sentence of an important part of its syntactical features. It also shortens the distance between subsampled words, implicitly increasing the span of the context win- dow. A second trick consists of using dynamic context windows: for each subsampled word w, the size of its associated context window is sam- pled uniformly between 1 and ws. Using dynamic context windows is equivalent to weighing by the distance from the focus word w divided by the window size (Levy et al., 2015). This makes the prediction task local, and go against our objective of creating sentence embeddings as we want to learn how to compose all n-gram features present in a sentence. In the results section, we report a signi\ufb01cant improvement of our method over C- BOW. 2.4 Model Training Three different datasets have been used to train our models: the Toronto book corpus3, Wikipedia sentences and tweets. The Wikipedia and Toronto books sentences have been tokenized using the Stanford NLP library (Manning et al., 2014), while for tweets we used the NLTK tweets tok- enizer (Bird et al., 2009).",
      "The Wikipedia and Toronto books sentences have been tokenized using the Stanford NLP library (Manning et al., 2014), while for tweets we used the NLTK tweets tok- enizer (Bird et al., 2009). For training, we select a 3http://www.cs.toronto.edu/\u02dcmbweb/",
      "sentence randomly from the dataset and then pro- ceed to select all the possible target unigrams us- ing subsampling. We update the weights using SGD with a linearly decaying learning rate. Also, to prevent over\ufb01tting, for each sentence we use dropout on its list of n-grams R(S) \\ {U(S)}, where U(S) is the set of all unigrams contained in sentence S. After empirically try- ing multiple dropout schemes, we \ufb01nd that drop- ping K n-grams (n > 1) for each sentence is giving superior results compared to dropping each token with some \ufb01xed probability. This dropout mechanism would negatively impact shorter sen- tences. The regularization can be pushed fur- ther by applying L1 regularization to the word vectors. Encouraging sparsity in the embedding vectors is particularly bene\ufb01cial for high dimen- sion h. The additional soft thresholding in every SGD step adds negligible computational cost. See also Appendix B. We train two models on each dataset, one with unigrams only and one with un- igrams and bigrams.",
      "See also Appendix B. We train two models on each dataset, one with unigrams only and one with un- igrams and bigrams. All training parameters for the models are provided in Table 5 in the supple- mentary material. Our C++ implementation builds upon the FastText library (Joulin et al., 2017; Bo- janowski et al., 2017). We will make our code and pre-trained models available open-source. 3 Related Work We discuss existing models which have been pro- posed to construct sentence embeddings. While there is a large body of works in this direction \u2013 several among these using e.g. labelled datasets of paraphrase pairs to obtain sentence embeddings in a supervised manner (Wieting et al., 2016a,b; Con- neau et al., 2017) to learn sentence embeddings \u2013 we here focus on unsupervised, task-independent models. While some methods require ordered raw text i.e., a coherent corpus where the next sentence is a logical continuation of the previous sentence, others rely only on raw text i.e., an unordered col- lection of sentences. Finally, we also discuss alter- native models built from structured data sources.",
      "While some methods require ordered raw text i.e., a coherent corpus where the next sentence is a logical continuation of the previous sentence, others rely only on raw text i.e., an unordered col- lection of sentences. Finally, we also discuss alter- native models built from structured data sources. 3.1 Unsupervised Models Independent of Sentence Ordering The ParagraphVector DBOW model (Le and Mikolov, 2014) is a log-linear model which is trained to learn sentence as well as word embed- dings and then use a softmax distribution to predict words contained in the sentence given the sentence vector representation. They also propose a dif- ferent model ParagraphVector DM where they use n-grams of consecutive words along with the sentence vector representation to predict the next word. (Lev et al., 2015) also presented an early ap- proach to obtain compositional embeddings from word vectors. They use different compositional techniques including static averaging or Fisher vectors of a multivariate Gaussian to obtain sen- tence embeddings from word2vec models. Hill et al. (2016a) propose a Sequential (De- noising) Autoencoder, S(D)AE.",
      "They use different compositional techniques including static averaging or Fisher vectors of a multivariate Gaussian to obtain sen- tence embeddings from word2vec models. Hill et al. (2016a) propose a Sequential (De- noising) Autoencoder, S(D)AE. This model \ufb01rst introduces noise in the input data: Firstly each word is deleted with probability p0, then for each non-overlapping bigram, words are swapped with probability px. The model then uses an LSTM- based architecture to retrieve the original sentence from the corrupted version. The model can then be used to encode new sentences into vector rep- resentations. In the case of p0 = px = 0, the model simply becomes a Sequential Autoencoder. Hill et al. (2016a) also propose a variant (S(D)AE + embs.) in which the words are represented by \ufb01xed pre-trained word vector embeddings. Arora et al. (2017) propose a model in which sentences are represented as a weighted average of \ufb01xed (pre-trained) word vectors, followed by post-processing step of subtracting the principal component.",
      "in which the words are represented by \ufb01xed pre-trained word vector embeddings. Arora et al. (2017) propose a model in which sentences are represented as a weighted average of \ufb01xed (pre-trained) word vectors, followed by post-processing step of subtracting the principal component. Using the generative model of (Arora et al., 2016), words are generated conditioned on a sentence \u201cdiscourse\u201d vector cs: Pr[w | cs] = \u03b1fw + (1 \u2212\u03b1)exp(\u02dcc\u22a4 s vw) Z\u02dccs , where Z\u02dccs := P w\u2208V exp(\u02dcc\u22a4 s vw) and \u02dccs := \u03b2c0 + (1 \u2212\u03b2)cs and \u03b1, \u03b2 are scalars. c0 is the common discourse vector, representing a shared component among all discourses, mainly related to syntax. It allows the model to better generate syntactical features. The \u03b1fw term is here to en- able the model to generate some frequent words even if their matching with the discourse vector \u02dccs is low.",
      "It allows the model to better generate syntactical features. The \u03b1fw term is here to en- able the model to generate some frequent words even if their matching with the discourse vector \u02dccs is low. Therefore, this model tries to generate sentences as a mixture of three type of words: words match- ing the sentence discourse vector cs, syntacti- cal words matching c0, and words with high fw. (Arora et al., 2017) demonstrated that for this model, the MLE of \u02dccs can be approximated by P w\u2208S a fw+avw, where a is a scalar. The sentence",
      "discourse vector can hence be obtained by sub- tracting c0 estimated by the \ufb01rst principal com- ponent of \u02dccs\u2019s on a set of sentences. In other words, the sentence embeddings are obtained by a weighted average of the word vectors strip- ping away the syntax by subtracting the com- mon discourse vector and down-weighting fre- quent tokens. They generate sentence embed- dings from diverse pre-trained word embeddings among which are unsupervised word embeddings such as GloVe (Pennington et al., 2014) as well as supervised word embeddings such as paragram- SL999 (PSL) (Wieting et al., 2015) trained on the Paraphrase Database (Ganitkevitch et al., 2013). In a very different line of work, C-PHRASE (Pham et al., 2015) relies on additional informa- tion from the syntactic parse tree of each sentence, which is incorporated into the C-BOW training objective. Huang and Anandkumar (2016) show that sin- gle layer CNNs can be modeled using a tensor decomposition approach.",
      "Huang and Anandkumar (2016) show that sin- gle layer CNNs can be modeled using a tensor decomposition approach. While building on an unsupervised objective, the employed dictionary learning step for obtaining phrase templates is task-speci\ufb01c (for each use-case), not resulting in general-purpose embeddings. 3.2 Unsupervised Models Depending on Sentence Ordering The SkipThought model (Kiros et al., 2015) com- bines sentence level models with recurrent neu- ral networks. Given a sentence Si from an or- dered corpus, the model is trained to predict Si\u22121 and Si+1. FastSent (Hill et al., 2016a) is a sentence- level log-linear bag-of-words model. Like SkipThought, it uses adjacent sentences as the pre- diction target and is trained in an unsupervised fashion. Using word sequences allows the model to improve over the earlier work of paragraph2vec (Le and Mikolov, 2014). (Hill et al., 2016a) aug- ment FastSent further by training it to predict the constituent words of the sentence as well.",
      "Using word sequences allows the model to improve over the earlier work of paragraph2vec (Le and Mikolov, 2014). (Hill et al., 2016a) aug- ment FastSent further by training it to predict the constituent words of the sentence as well. This model is named FastSent + AE in our compar- isons. Compared to our approach, Siamese C-BOW (Kenter et al., 2016) shares the idea of learning to average word embeddings over a sentence. How- ever, it relies on a Siamese neural network archi- tecture to predict surrounding sentences, contrast- ing our simpler unsupervised objective. Note that on the character sequence level in- stead of word sequences, FastText (Bojanowski et al., 2017) uses the same conceptual model to ob- tain better word embeddings. This is most similar to our proposed model, with two key differences: Firstly, we predict from source word sequences to target words, as opposed to character sequences to target words, and secondly, our model is averaging the source embeddings instead of summing them.",
      "This is most similar to our proposed model, with two key differences: Firstly, we predict from source word sequences to target words, as opposed to character sequences to target words, and secondly, our model is averaging the source embeddings instead of summing them. 3.3 Models requiring structured data DictRep (Hill et al., 2016b) is trained to map dic- tionary de\ufb01nitions of the words to the pre-trained word embeddings of these words. They use two different architectures, namely BOW and RNN (LSTM) with the choice of learning the input word embeddings or using them pre-trained. A similar architecture is used by the CaptionRep variant, but here the task is the mapping of given image captions to a pre-trained vector representation of these images. 4 Evaluation Tasks We use a standard set of supervised as well as un- supervised benchmark tasks from the literature to evaluate our trained models, following (Hill et al., 2016a). The breadth of tasks allows to fairly mea- sure generalization to a wide area of different do- mains, testing the general-purpose quality (univer- sality) of all competing sentence embeddings.",
      "The breadth of tasks allows to fairly mea- sure generalization to a wide area of different do- mains, testing the general-purpose quality (univer- sality) of all competing sentence embeddings. For downstream supervised evaluations, sentence em- beddings are combined with logistic regression to predict target labels. In the unsupervised evalua- tion for sentence similarity, correlation of the co- sine similarity between two embeddings is com- pared to human annotators. Downstream Supervised Evaluation. Sen- tence embeddings are evaluated for various su- pervised classi\ufb01cation tasks as follows. We evaluate paraphrase identi\ufb01cation (MSRP) (Dolan et al., 2004), classi\ufb01cation of movie review sen- timent (MR) (Pang and Lee, 2005), product re- views (CR) (Hu and Liu, 2004), subjectivity clas- si\ufb01cation (SUBJ) (Pang and Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005) and ques- tion type classi\ufb01cation (TREC) (Voorhees, 2002).",
      "To classify, we use the code provided by (Kiros et al., 2015) in the same manner as in (Hill et al., 2016a). For the MSRP dataset, containing pairs of sentences (S1, S2) with associated paraphrase la- bel, we generate feature vectors by concatenating",
      "their Sent2Vec representations |vS1 \u2212vS2| with the component-wise product vS1 \u2299vS2. The pre- de\ufb01ned training split is used to tune the L2 penalty parameter using cross-validation and the accuracy and F1 scores are computed on the test set. For the remaining 5 datasets, Sent2Vec embeddings are inferred from input sentences and directly fed to a logistic regression classi\ufb01er. Accuracy scores are obtained using 10-fold cross-validation for the MR, CR, SUBJ and MPQA datasets. For those datasets nested cross-validation is used to tune the L2 penalty. For the TREC dataset, as for the MRSP dataset, the L2 penalty is tuned on the pre- de\ufb01ned train split using 10-fold cross-validation, and the accuracy is computed on the test set. Unsupervised Similarity Evaluation. We per- form unsupervised evaluation of the learnt sen- tence embeddings using the sentence cosine sim- ilarity, on the STS 2014 (Agirre et al., 2014) and SICK 2014 (Marelli et al., 2014) datasets.",
      "We per- form unsupervised evaluation of the learnt sen- tence embeddings using the sentence cosine sim- ilarity, on the STS 2014 (Agirre et al., 2014) and SICK 2014 (Marelli et al., 2014) datasets. These similarity scores are compared to the gold- standard human judgements using Pearson\u2019s r (Pearson, 1895) and Spearman\u2019s \u03c1 (Spearman, 1904) correlation scores. The SICK dataset con- sists of about 10,000 sentence pairs along with relatedness scores of the pairs. The STS 2014 dataset contains 3,770 pairs, divided into six dif- ferent categories on the basis of the origin of sen- tences/phrases, namely Twitter, headlines, news, forum, WordNet and images. 5 Results and Discussion In Tables 1 and 2, we compare our results with those obtained by (Hill et al., 2016a) on different models.",
      "5 Results and Discussion In Tables 1 and 2, we compare our results with those obtained by (Hill et al., 2016a) on different models. Table 3 in the last column shows the dra- matic improvement in training time of our mod- els (and other C-BOW-inspired models) in con- trast to neural network based models. All our Sent2Vec models are trained on a machine with 2x Intel Xeon E5\u22122680v3, 12 cores @2.5GHz. Along with the models discussed in Section 3, this also includes the sentence embedding baselines obtained by simple averaging of word embeddings over the sentence, in both the C-BOW and skip- gram variants. TF-IDF BOW is a representation consisting of the counts of the 200,000 most com- mon feature-words, weighed by their TF-IDF fre- quencies. To ensure coherence, we only include unsupervised models in the main paper. Perfor- mance of supervised and semi-supervised models on these evaluations can be observed in Tables 6 and 7 in the supplementary material. Downstream Supervised Evaluation Results.",
      "To ensure coherence, we only include unsupervised models in the main paper. Perfor- mance of supervised and semi-supervised models on these evaluations can be observed in Tables 6 and 7 in the supplementary material. Downstream Supervised Evaluation Results. On running supervised evaluations and observing the results in Table 1, we \ufb01nd that on an aver- age our models are second only to SkipThought vectors. Also, both our models achieve state of the art results on the CR task. We also ob- serve that on half of the supervised tasks, our unigrams + bigram model is the best model af- ter SkipThought. Our models are weaker on the MSRP task (which consists of the identi\ufb01cation of labelled paraphrases) compared to state-of-the-art methods. However, we observe that the models which perform very strongly on this task end up faring very poorly on the other tasks, indicating a lack of generalizability. On rest of the tasks, our models perform ex- tremely well.",
      "However, we observe that the models which perform very strongly on this task end up faring very poorly on the other tasks, indicating a lack of generalizability. On rest of the tasks, our models perform ex- tremely well. The SkipThought model is able to outperform our models on most of the tasks as it is trained to predict the previous and next sentences and a lot of tasks are able to make use of this con- textual information missing in our Sent2Vec mod- els. For example, the TREC task is a poor measure of how one predicts the content of the sentence (the question) but a good measure of how the next sentence in the sequence (the answer) is predicted. Unsupervised Similarity Evaluation Results. In Table 2, we see that our Sent2Vec models are state-of-the-art on the majority of tasks when comparing to all the unsupervised models trained on the Toronto corpus, and clearly achieve the best averaged performance. Our Sent2Vec mod- els also on average outperform or are at par with the C-PHRASE model, despite signi\ufb01cantly lag- ging behind on the STS 2014 WordNet and News subtasks.",
      "Our Sent2Vec mod- els also on average outperform or are at par with the C-PHRASE model, despite signi\ufb01cantly lag- ging behind on the STS 2014 WordNet and News subtasks. This observation can be attributed to the fact that a big chunk of the data that the C- PHRASE model is trained on comes from English Wikipedia, helping it to perform well on datasets involving de\ufb01nition and news items. Also, C- PHRASE uses data three times the size of the Toronto book corpus. Interestingly, our model out- performs C-PHRASE when trained on Wikipedia, as shown in Table 3, despite the fact that we use no parse tree information. Of\ufb01cial STS 2017 benchmark. In the of\ufb01cial results of the most recent edition of the STS 2017 benchmark (Cer et al., 2017), our model also sig- ni\ufb01cantly outperforms C-PHRASE, and in fact de- livers the best unsupervised baseline method. 4For the Siamese C-BOW model trained on the Toronto",
      "Data Model MSRP (Acc / F1) MR CR SUBJ MPQA TREC Average Unordered Sentences: (Toronto Books; 70 million sentences, 0.9 Billion Words) SAE 74.3 / 81.7 62.6 68.0 86.1 76.8 80.2 74.7 SAE + embs. 70.6 / 77.9 73.2 75.3 89.8 86.2 80.4 79.3 SDAE 76.4 / 83.4 67.6 74.0 89.3 81.3 77.7 78.3 SDAE + embs.",
      "73.7 / 80.7 74.6 78.0 90.8 86.9 78.4 80.4 ParagraphVec DBOW 72.9 / 81.1 60.2 66.9 76.3 70.7 59.4 67.7 ParagraphVec DM 73.6 / 81.9 61.5 68.6 76.4 78.1 55.8 69.0 Skipgram 69.3 / 77.2 73.6 77.3 89.2 85.0 82.2 78.5 C-BOW 67.6 / 76.1 73.6 77.3 89.1 85.0 82.2 79.1 Unigram TFIDF 73.6 / 81.7 73.7 79.2 90.3 82.4 85.0 80.7 Sent2Vec uni.",
      "72.2 / 80.3 75.1 80.2 90.6 86.3 83.8 81.4 Sent2Vec uni. + bi. 72.5 / 80.8 75.8 80.3 91.2 85.9 86.4 82.0 Ordered Sentences: Toronto Books SkipThought 73.0 / 82.0 76.5 80.1 93.6 87.1 92.2 83.8 FastSent 72.2 / 80.3 70.8 78.4 88.7 80.6 76.8 77.9 FastSent+AE 71.2 / 79.1 71.8 76.7 88.8 81.5 80.4 78.4 2.8 Billion words C-PHRASE 72.2 / 79.6 75.7 78.8 91.1 86.2 78.8 80.5 Table 1: Comparison of the performance of different models on different supervised evaluation tasks.",
      "An underline indicates the best performance for the dataset. Top 3 performances in each data category are shown in bold. The average is calculated as the average of accuracy for each category (For MSRP, we take the accuracy). ) STS 2014 SICK 2014 Model News Forum WordNet Twitter Images Headlines Test + Train Average SAE .17/.16 .12/.12 .30/.23 .28/.22 .49/.46 .13/.11 .32/.31 .26/.23 SAE + embs. .52/.54 .22/.23 .60/.55 .60/.60 .64/.64 .41/.41 .47/.49 .50/.49 SDAE .07/.04 .11/.13 .33/.24 .44/.42 .44/.38 .36/.36 .46/.46 .31/.29 SDAE + embs.",
      ".51/.54 .29/.29 .56/.50 .57/.58 .59/.59 .43/.44 .46/.46 .49/.49 ParagraphVec DBOW .31/.34 .32/.32 .53/.50 .43/.46 .46/.44 .39/.41 .42/.46 .41/.42 ParagraphVec DM .42/.46 .33/.34 .51/.48 .54/.57 .32/.30 .46/.47 .44/.40 .43/.43 Skipgram .56/.59 .42/.42 .73/.70 .71/.74 .65/.67 .55/.58 .60/.69 .60/.63 C-BOW .57/.61 .43/.44 .72/.69 .71/.75 .71/.73 .55/.59 .60/.69 .60/.65 Unigram TF-IDF .48/.48 .40/.38 .60/.59 .63/.65 .72/.74 .49/.49 .52/.58 .55/.56 Sent2Vec uni. .62/.67 .49/.49 .75/.72 .70/.75 .78/.82 .61/.63 .61/.70 .65/.68 Sent2Vec uni.",
      ".62/.67 .49/.49 .75/.72 .70/.75 .78/.82 .61/.63 .61/.70 .65/.68 Sent2Vec uni. + bi.",
      ".62/.67 .49/.49 .75/.72 .70/.75 .78/.82 .61/.63 .61/.70 .65/.68 Sent2Vec uni. + bi. .62/.67 .51/.51 .71/.68 .70/.75 .75/.79 .59/.62 .62/.70 .65/.67 SkipThought .44/.45 .14/.15 .39/.34 .42/.43 .55/.60 .43/.44 .57/.60 .42/.43 FastSent .58/.59 .41/.36 .74/.70 .63/.66 .74/.78 .57/.59 .61/.72 .61/.63 FastSent+AE .56/.59 .41/.40 .69/.64 .70/.74 .63/.65 .58/.60 .60/.65 .60/.61 Siamese C-BOW4 .58/.59 .42/.41 .66/.61 .71/.73 .65/.65 .63/.64 \u2212 \u2212 C-PHRASE .69/.71 .43/.41 .76/.73 .60/.65 .75/.79 .60/.65 .60/.72 .63/.67 Table 2: Unsupervised Evaluation Tasks: Comparison of the performance of different models on Spearman/Pearson corre- lation measures.",
      "An underline indicates the best performance for the dataset. Top 3 performances in each data category are shown in bold. The average is calculated as the average of entries for each correlation measure. Macro Average. To summarize our contribu- tions on both supervised and unsupervised tasks, in Table 3 we present the results in terms of the macro average over the averages of both super- vised and unsupervised tasks along with the train- ing times of the models5. For unsupervised tasks, averages are taken over both Spearman and Pear- son scores. The comparison includes the best per- forming unsupervised and semi-supervised meth- ods described in Section 3. For models trained on the Toronto books dataset, we report a 3.8 % points improvement over the state of the art. Con- sidering all supervised, semi-supervised methods and all datasets compared in (Hill et al., 2016a), corpus, supervised evaluation as well as similarity evaluation results on the SICK 2014 dataset are unavailable. 5time taken to train C-PHRASE models is unavailable we report a 2.2 % points improvement.",
      "5time taken to train C-PHRASE models is unavailable we report a 2.2 % points improvement. We also see a noticeable improvement in ac- curacy as we use larger datasets like Twitter and Wikipedia. We furthermore see that the Sent2Vec models are faster to train when compared to meth- ods like SkipThought and DictRep, owing to the SGD optimizer allowing a high degree of paral- lelizability. We can clearly see Sent2Vec outperforming other unsupervised and even semi-supervised methods. This can be attributed to the superior generalizability of our model across supervised and unsupervised tasks. Comparison with Arora et al. (2017). We also compare our work with Arora et al. (2017) who also use additive compositionality to obtain sen- tence embeddings. However, in contrast to our",
      "Type Training corpus Method Supervised average Unsupervised average Macro average Training time (in hours) unsupervised twitter (19.7B words) Sent2Vec uni. + bi. 83.5 68.3 75.9 6.5* unsupervised twitter (19.7B words) Sent2Vec uni. 82.2 69.0 75.6 3* unsupervised Wikipedia (1.7B words) Sent2Vec uni. + bi. 83.3 66.2 74.8 2* unsupervised Wikipedia (1.7B words) Sent2Vec uni. 82.4 66.3 74.3 3.5* unsupervised Toronto books (0.9B words) Sent2Vec books uni. 81.4 66.7 74.0 1* unsupervised Toronto books (0.9B words) Sent2Vec books uni. + bi.",
      "81.4 66.7 74.0 1* unsupervised Toronto books (0.9B words) Sent2Vec books uni. + bi. 82.0 65.9 74.0 1.2* semi-supervised structured dictionary dataset DictRep BOW + emb 80.5 66.9 73.7 24** unsupervised 2.8B words + parse info. C-PHRASE 80.5 64.9 72.7 \u2212 unsupervised Toronto books (0.9B words) C-BOW 79.1 62.8 70.2 2 unsupervised Toronto books (0.9B words) FastSent 77.9 62.0 70.0 2 unsupervised Toronto books (0.9B words) SkipThought 83.8 42.5 63.1 336** Table 3: Best unsupervised and semi-supervised methods ranked by macro average along with their training times. ** indicates trained on GPU. * indicates trained on a single node using 30 threads.",
      "** indicates trained on GPU. * indicates trained on a single node using 30 threads. Training times for non-Sent2Vec models are due to Hill et al. (2016a). For CPU based competing methods, we were able to reproduce all published timings (+-10%) using our same hardware as for training Sent2Vec. Dataset Unsupervised GloVe (840B words) + WR Semi-supervised PSL + WR Sent2Vec Unigrams (19.7B words) Tweets Model Sent2Vec Unigrams + Bigrams (19.7B words) Tweets Model STS 2014 0.685 0.735 0.710 0.701 SICK 2014 0.722 0.729 0.710 0.715 Supervised average 0.815 0.807 0.822 0.835 Table 4: Comparison of the performance of the unsupervised and semi-supervised sentence embeddings by (Arora et al., 2017) with our models.",
      "Unsupervised comparisons are in terms of Pearson\u2019s correlation, while comparisons on supervised tasks are stating the average described in Table 1. model, they use \ufb01xed, pre-trained word embed- dings to build a weighted average of these em- beddings using unigram probabilities. While we couldn\u2019t \ufb01nd pre-trained state of the art word em- beddings trained on the Toronto books corpus, we evaluated their method using GloVe embeddings obtained from the larger Common Crawl Corpus6, which is 42 times larger than our twitter corpus, greatly favoring their method over ours. In Table 4, we report an experimental compar- ison to their model on unsupervised tasks. In the table, the suf\ufb01x W indicates that their down- weighting scheme has been used, while the suf- \ufb01x R indicates the removal of the \ufb01rst princi- pal component. They report values of a \u2208 [10\u22124, 10\u22123] as giving the best results and used a = 10\u22123 for all their experiments. We observe that our results are competitive with the embed- dings of Arora et al.",
      "They report values of a \u2208 [10\u22124, 10\u22123] as giving the best results and used a = 10\u22123 for all their experiments. We observe that our results are competitive with the embed- dings of Arora et al. (2017) for purely unsuper- vised methods. It is important to note that the scores obtained from supervised task-speci\ufb01c PSL embeddings trained for the purpose of semantic similarity outperform our method on both SICK and average STS 2014, which is expected as our model is trained purely unsupervised. In order to facilitate a more detailed compari- son, we also evaluated the unsupervised Glove + WR embeddings on downstream supervised tasks 6http://www.cs.toronto.edu/\u02dcmbweb/ and compared them to our twitter models. To use Arora et al. (2017)\u2019s method in a supervised setup, we precomputed and stored the common discourse vector c0 using 2 million random Wikipedia sen- tences.",
      "To use Arora et al. (2017)\u2019s method in a supervised setup, we precomputed and stored the common discourse vector c0 using 2 million random Wikipedia sen- tences. On an average, our models outperform their unsupervised models by a signi\ufb01cant margin, this despite the fact that they used GloVe embed- dings trained on larger corpora than ours (42 times larger). Our models also outperform their semi- supervised PSL + WR model. This indicates our model learns a more precise weighing scheme than the static one proposed by Arora et al. (2017). Figure 1: Left \ufb01gure: the pro\ufb01le of the word vector L2- norms as a function of log(fw) for each vocabulary word w, as learnt by our unigram model trained on Toronto books. Right \ufb01gure: down-weighting scheme proposed by Arora et al. (2017): weight(w) = a a+fw . The effect of datasets and n-grams. Despite being trained on three very different datasets, all of our models generalize well to sometimes very",
      "speci\ufb01c domains. Models trained on Toronto Cor- pus are the state-of-the-art on the STS 2014 im- ages dataset even beating the supervised Caption- Rep model trained on images. We also see that addition of bigrams to our models doesn\u2019t help much when it comes to unsupervised evaluations but gives a signi\ufb01cant boost-up in accuracy on supervised tasks. We attribute this phenomenon to the ability of bigrams models to capture some non-compositional features missed by unigrams models. Having a single representation for \u201cnot good\u201d or \u201cvery bad\u201d can boost the supervised model\u2019s ability to infer relevant features for the corresponding classi\ufb01er. For semantic similarity tasks however, the relative uniqueness of bigrams results in pushing sentence representations further apart, which can explain the average drop of scores for bigrams models on those tasks. On learning the importance and the direction of the word vectors. Our model \u2013 by learning how to generate and compose word vectors \u2013 has to learn both the direction of the word embeddings as well as their norm.",
      "On learning the importance and the direction of the word vectors. Our model \u2013 by learning how to generate and compose word vectors \u2013 has to learn both the direction of the word embeddings as well as their norm. Considering the norms of the used word vectors as by our averaging over the sentence, we observe an interesting distribution of the \u201cimportance\u201d of each word. In Figure 1 we show the pro\ufb01le of the L2-norm as a function of log(fw) for each w \u2208V, and compare it to the static down-weighting mechanism of Arora et al. (2017). We can observe that our model is learn- ing to down-weight frequent tokens by itself. It is also down-weighting rare tokens and the norm pro\ufb01le seems to roughly follow Luhn\u2019s hypothesis (Luhn, 1958), a well known information retrieval paradigm, stating that mid-rank terms are the most signi\ufb01cant to discriminate content. 6 Conclusion In this paper, we introduce a novel, computa- tionally ef\ufb01cient, unsupervised, C-BOW-inspired method to train and infer sentence embeddings.",
      "6 Conclusion In this paper, we introduce a novel, computa- tionally ef\ufb01cient, unsupervised, C-BOW-inspired method to train and infer sentence embeddings. On supervised evaluations, our method, on an av- erage, achieves better performance than all other unsupervised competitors with the exception of SkipThought. However, SkipThought vectors show a very poor performance on sentence simi- larity tasks while our model is state-of-the-art for these evaluations on average. Also, our model is generalizable, extremely fast to train, simple to un- derstand and easily interpretable, showing the rel- evance of simple and well-grounded representa- tion models in contrast to the models using deep architectures. Future work could focus on aug- menting the model to exploit data with ordered sentences. Furthermore, we would like to investi- gate the model\u2019s ability to use pre-trained embed- dings for downstream transfer learning tasks. Acknowledgments We are indebted to Piotr Bojanowski and Armand Joulin for helpful discussions. This project was supported by a Google Faculty Research Award.",
      "Furthermore, we would like to investi- gate the model\u2019s ability to use pre-trained embed- dings for downstream transfer learning tasks. Acknowledgments We are indebted to Piotr Bojanowski and Armand Joulin for helpful discussions. This project was supported by a Google Faculty Research Award. References Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2014. Semeval-2014 task 10: Multilin- gual semantic textual similarity. In Proceedings of the 8th international workshop on semantic evalua- tion (SemEval 2014). Association for Computational Linguistics Dublin, Ireland, pages 81\u201391. Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. 2016. A Latent Vari- able Model Approach to PMI-based Word Embed- dings. In Transactions of the Association for Com- putational Linguistics. pages 385\u2013399.",
      "2016. A Latent Vari- able Model Approach to PMI-based Word Embed- dings. In Transactions of the Association for Com- putational Linguistics. pages 385\u2013399. Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017. A simple but tough-to-beat baseline for sentence em- beddings. In International Conference on Learning Representations (ICLR). Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural language processing with Python: analyz- ing text with the natural language toolkit. \u201d O\u2019Reilly Media, Inc.\u201d. Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching Word Vectors with Subword Information. Transactions of the Associa- tion for Computational Linguistics 5:135\u2013146. Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez- Gazpio, and Lucia Specia. 2017.",
      "Enriching Word Vectors with Subword Information. Transactions of the Associa- tion for Computational Linguistics 5:135\u2013146. Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez- Gazpio, and Lucia Specia. 2017. SemEval-2017 Task 1: Semantic Textual Similarity Multilin- gual and Cross-lingual Focused Evaluation. In SemEval-2017 - Proceedings of the 11th Interna- tional Workshop on Semantic Evaluations. Asso- ciation for Computational Linguistics, Vancouver, Canada, pages 1\u201314. Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364 . Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase cor- pora: Exploiting massively parallel news sources. In Proceedings of the 20th international conference on",
      "Computational Linguistics. Association for Compu- tational Linguistics, page 350. Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. Ppdb: The paraphrase database. In HLT-NAACL. pages 758\u2013764. Yoav Goldberg and Omer Levy. 2014. word2vec Ex- plained: deriving Mikolov et al.\u2019s negative-sampling word-embedding method. arXiv . Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016a. Learning Distributed Representations of Sentences from Unlabelled Data. In Proceedings of NAACL-HLT. Felix Hill, KyungHyun Cho, Anna Korhonen, and Yoshua Bengio. 2016b. Learning to understand phrases by embedding the dictionary. TACL 4:17\u2013 30. Minqing Hu and Bing Liu. 2004. Mining and summa- rizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowl- edge discovery and data mining.",
      "TACL 4:17\u2013 30. Minqing Hu and Bing Liu. 2004. Mining and summa- rizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowl- edge discovery and data mining. ACM, pages 168\u2013 177. Furong Huang and Animashree Anandkumar. 2016. Unsupervised Learning of Word-Sequence Repre- sentations from Scratch via Convolutional Tensor Decomposition. arXiv . Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2017. Bag of Tricks for Ef\ufb01cient Text Classi\ufb01cation. In Proceedings of the 15th Con- ference of the European Chapter of the Association for Computational Linguistics, Short Papers. Valen- cia, Spain, pages 427\u2013431. Tom Kenter, Alexey Borisov, and Maarten de Rijke. 2016. Siamese CBOW: Optimizing Word Embed- dings for Sentence Representations.",
      "Valen- cia, Spain, pages 427\u2013431. Tom Kenter, Alexey Borisov, and Maarten de Rijke. 2016. Siamese CBOW: Optimizing Word Embed- dings for Sentence Representations. In ACL - Pro- ceedings of the 54th Annual Meeting of the Asso- ciation for Computational Linguistics. Berlin, Ger- many, pages 941\u2013951. Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-Thought Vectors. In NIPS 2015 - Advances in Neural Information Pro- cessing Systems 28. pages 3294\u20133302. Quoc V Le and Tomas Mikolov. 2014. Distributed Representations of Sentences and Documents. In ICML 2014 - Proceedings of the 31st International Conference on Machine Learning. volume 14, pages 1188\u20131196. Guy Lev, Benjamin Klein, and Lior Wolf. 2015.",
      "2014. Distributed Representations of Sentences and Documents. In ICML 2014 - Proceedings of the 31st International Conference on Machine Learning. volume 14, pages 1188\u20131196. Guy Lev, Benjamin Klein, and Lior Wolf. 2015. In de- fense of word embedding for generic text representa- tion. In International Conference on Applications of Natural Language to Information Systems. Springer, pages 35\u201350. Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im- proving distributional similarity with lessons learned from word embeddings. Transactions of the Associ- ation for Computational Linguistics 3:211\u2013225. Hans Peter Luhn. 1958. The automatic creation of lit- erature abstracts. IBM Journal of research and de- velopment 2(2):159\u2013165. Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David Mc- Closky. 2014. The stanford corenlp natural lan- guage processing toolkit.",
      "Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David Mc- Closky. 2014. The stanford corenlp natural lan- guage processing toolkit. In ACL (System Demon- strations). pages 55\u201360. Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zam- parelli. 2014. A sick cure for the evaluation of com- positional distributional semantic models. In LREC. pages 216\u2013223. Tomas Mikolov, Kai Chen, Greg Corrado, and Jef- frey Dean. 2013a. Ef\ufb01cient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 . Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013b. Distributed Representa- tions of Words and Phrases and their Compositional- ity.",
      "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013b. Distributed Representa- tions of Words and Phrases and their Compositional- ity. In NIPS - Advances in Neural Information Pro- cessing Systems 26. pages 3111\u20133119. Bo Pang and Lillian Lee. 2004. A sentimental educa- tion: Sentiment analysis using subjectivity summa- rization based on minimum cuts. In Proceedings of the 42nd annual meeting on Association for Compu- tational Linguistics. Association for Computational Linguistics, page 271. Bo Pang and Lillian Lee. 2005. Seeing stars: Ex- ploiting class relationships for sentiment categoriza- tion with respect to rating scales. In Proceedings of the 43rd annual meeting on association for compu- tational linguistics. Association for Computational Linguistics, pages 115\u2013124. Karl Pearson. 1895. Note on regression and inheritance in the case of two parents. Proceedings of the Royal Society of London 58:240\u2013242.",
      "Association for Computational Linguistics, pages 115\u2013124. Karl Pearson. 1895. Note on regression and inheritance in the case of two parents. Proceedings of the Royal Society of London 58:240\u2013242. Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In EMNLP. volume 14, pages 1532\u2013 1543. NT Pham, G Kruszewski, A Lazaridou, and M Baroni. 2015. Jointly optimizing word representations for lexical and sentential tasks with the c-phrase model. ACL/IJCNLP . R Tyrrell Rockafellar. 1976. Monotone operators and the proximal point algorithm. SIAM journal on con- trol and optimization 14(5):877\u2013898. Charles Spearman. 1904. The proof and measurement of association between two things. The American journal of psychology 15(1):72\u2013101.",
      "Ellen M Voorhees. 2002. Overview of the trec 2001 question answering track. In NIST special publica- tion. pages 42\u201351. Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh Attenberg. 2009. Feature hashing for large scale multitask learning. In Pro- ceedings of the 26th Annual International Confer- ence on Machine Learning. ACM, pages 1113\u2013 1120. Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emo- tions in language. Language resources and evalua- tion 39(2):165\u2013210. John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2016a. Charagram: Embedding Words and Sentences via Character n-grams. In EMNLP - Proceedings of the 2016 Conference on Empiri- cal Methods in Natural Language Processing.",
      "2016a. Charagram: Embedding Words and Sentences via Character n-grams. In EMNLP - Proceedings of the 2016 Conference on Empiri- cal Methods in Natural Language Processing. Asso- ciation for Computational Linguistics, Stroudsburg, PA, USA, pages 1504\u20131515. John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2016b. Towards universal paraphrastic sentence embeddings. In International Conference on Learning Representations (ICLR). John Wieting, Mohit Bansal, Kevin Gimpel, Karen Livescu, and Dan Roth. 2015. From paraphrase database to compositional paraphrase model and back. In TACL - Transactions of the Association for Computational Linguistics.",
      "Supplementary Material A Parameters for training models Model Embedding Dimensions Minimum word count Minimum Target word Count Initial Lear ning Rate Epochs Subsampling hyper-parameter Bigrams Dropped per sentence Number of negatives sampled Book corpus Sent2Vec unigrams 700 5 8 0.2 13 1 \u00d7 10\u22125 - 10 Book corpus Sent2Vec unigrams + bigrams 700 5 5 0.2 12 5 \u00d7 10\u22126 7 10 Wiki Sent2Vec unigrams 600 8 20 0.2 9 1 \u00d7 10\u22125 - 10 Wiki Sent2Vec unigrams + bigrams 700 8 20 0.2 9 5 \u00d7 10\u22126 4 10 Twitter Sent2Vec unigrams 700 20 20 0.2 3 1 \u00d7 10\u22126 - 10 Twitter Sent2Vec unigrams + bigrams 700 20 20 0.2 3 1 \u00d7 10\u22126 3 10 Table 5: Training parameters for the Sent2Vec models B L1 regularization of models Optionally,",
      "2 3 1 \u00d7 10\u22126 3 10 Table 5: Training parameters for the Sent2Vec models B L1 regularization of models Optionally, our model can be additionally improved by adding an L1 regularizer term in the objective function, leading to slightly better generalization performance. Additionally, encouraging sparsity in the embedding vectors is bene\ufb01cial for memory reasons, allowing higher embedding dimensions h. We propose to apply L1 regularization individually to each word (and n-gram) vector (both source and target vectors). Formally, the training objective function (3) then becomes min U,V X S\u2208C X wt\u2208S qp(wt) \u0012\u0010 \u2113 \u0000u\u22a4 wtvS\\{wt} \u0001 + \u03c4(\u2225uwt\u22251 + \u2225vS\\{wt}\u22251) \u0011 + (4) |Nwt| X w\u2032\u2208V qn(w\u2032) \u0010 \u2113 \u0000\u2212u\u22a4 w\u2032vS\\{wt} \u0001 + \u03c4(\u2225uw\u2032\u22251) \u0011\u0013 where \u03c4 is the regularization parameter.",
      "Now, in order to minimize a function of the form f(z) + g(z) where g(z) is not differentiable over the domain, we can use the basic proximal-gradient scheme. In this iterative method, after doing a gradient descent step on f(z) with learning rate \u03b1, we update z as zn+1 = prox\u03b1,g(zn+ 1 2 ) (5) where prox\u03b1,g(x) = arg miny{g(y)+ 1 2\u03b1\u2225y\u2212x\u22252 2} is called the proximal function (Rockafellar, 1976) of g with \u03b1 being the proximal parameter and zn+ 1 2 is the value of z after a gradient (or SGD) step on zn. In our case, g(z) = \u2225z\u22251 and the corresponding proximal operator is given by prox\u03b1,g(x) = sign(x) \u2299max(|xn| \u2212\u03b1, 0) (6) where \u2299corresponds to element-wise product. Similar to the proximal-gradient scheme, in our case we can optionally use the thresholding operator on the updated word and n-gram vectors after an SGD step.",
      "Similar to the proximal-gradient scheme, in our case we can optionally use the thresholding operator on the updated word and n-gram vectors after an SGD step. The soft thresholding parameter used for this update is \u03c4\u00b7lr\u2032 |R(S\\{wt})| and \u03c4 \u00b7 lr\u2032 for the source and target vectors respectively where lr\u2032 is the current learning rate, \u03c4 is the L1 regularization parameter and S is the sentence on which SGD is being run. We observe that L1 regularization using the proximal step gives our models a small boost in perfor- mance. Also, applying the thresholding operator takes only |R(S \\ {wt})| \u00b7 h \ufb02oating point operations for the updating the word vectors corresponding to the sentence and (|N| + 1) \u00b7 h for updating the target",
      "as well as the negative word vectors, where |N| is the number of negatives sampled and h is the em- bedding dimension. Thus, performing L1 regularization using soft-thresholding operator comes with a small computational overhead. We set \u03c4 to be 0.0005 for both the Wikipedia and the Toronto Book Corpus unigrams + bigrams models. C Performance comparison with Sent2Vec models trained on different corpora Data Model MSRP (Acc / F1) MR CR SUBJ MPQA TREC Average Unordered Sentences: (Toronto Books) Sent2Vec uni. 72.2 / 80.3 75.1 80.2 90.6 86.3 83.8 81.4 Sent2Vec uni. + bi. 72.5 / 80.8 75.8 80.3 91.2 85.9 86.4 82.0 Sent2Vec uni. + bi.",
      "+ bi. 72.5 / 80.8 75.8 80.3 91.2 85.9 86.4 82.0 Sent2Vec uni. + bi. L1-reg 71.6 / 80.1 76.1 80.9 91.1 86.1 86.8 82.1 Unordered sentences: Wikipedia (69 million sentences; 1.7 B words) Sent2Vec uni. 71.8 / 80.2 77.3 80.3 92.0 87.4 85.4 82.4 Sent2Vec uni. + bi. 72.4 / 80.8 77.9 80.9 92.6 86.9 89.2 83.3 Sent2Vec uni. + bi. L1-reg 73.6 / 81.5 78.1 81.5 92.8 87.2 87.4 83.4 Unordered sentences: Twitter (1.2 billion sentences; 19.7 B words) Sent2Vec uni.",
      "L1-reg 73.6 / 81.5 78.1 81.5 92.8 87.2 87.4 83.4 Unordered sentences: Twitter (1.2 billion sentences; 19.7 B words) Sent2Vec uni. 71.5 / 80.0 77.1 81.3 90.8 87.3 85.4 82.2 Sent2Vec uni. + bi.",
      "71.5 / 80.0 77.1 81.3 90.8 87.3 85.4 82.2 Sent2Vec uni. + bi. 72.4 / 80.6 78.0 82.1 91.8 86.7 89.8 83.5 Other structured Data Sources CaptionRep BOW 73.6 / 81.9 61.9 69.3 77.4 70.8 72.2 70.9 CaptionRep RNN 72.6 / 81.1 55.0 64.9 64.9 71.0 62.4 65.1 DictRep BOW 73.7 / 81.6 71.3 75.6 86.6 82.5 73.8 77.3 DictRep BOW+embs 68.4 / 76.8 76.7 78.7 90.7 87.2 81.0 80.5 DictRep RNN 73.2 / 81.6 67.8 72.7 81.4 82.5 75.8 75.6 DictRep RNN+embs.",
      "66.8 / 76.0 72.5 73.5 85.6 85.7 72.0 76.0 Table 6: Comparison of the performance of different Sent2Vec models with different semi- supervised/supervised models on different downstream supervised evaluation tasks. An underline indicates the best performance for the dataset and Sent2Vec model performances are bold if they per- form as well or better than all other non-Sent2Vec models, including those presented in Table 1. STS 2014 SICK 2014 Average Model News Forum WordNet Twitter Images Headlines Test + Train Sent2Vec book corpus uni. .62/.67 .49/.49 .75/.72. .70/.75 .78/.82 .61/.63 .61/.70 .65/.68 Sent2Vec book corpus uni. + bi. .62/.67 .51/.51 .71/.68 .70/.75 .75/.79 .59/.62 .62/.70 .65/.67 Sent2Vec book corpus uni. + bi.",
      "+ bi. .62/.67 .51/.51 .71/.68 .70/.75 .75/.79 .59/.62 .62/.70 .65/.67 Sent2Vec book corpus uni. + bi. L1-reg .62/.68 .51/.52 .72/.70 .69/.75 .76/.81 .60/.63 .62/.71 .66/.68 Sent2Vec wiki uni. .66/.71 .47/.47 .70/.68 .68/.72 .76/.79 .63/.67 .64/.71 .65/.68 Sent2Vec wiki uni. + bi. .68/.74 .50/.50 .66/.64 .67/.72 .75/.79 .62/.67 .63/.71 .65/.68 Sent2Vec wiki uni. + bi. L1-reg .69/.75 .52/.52 .72/.69 .67/.72 .76/.80 .61/.66 .63/.72 .66/.69 Sent2Vec twitter uni.",
      "+ bi. L1-reg .69/.75 .52/.52 .72/.69 .67/.72 .76/.80 .61/.66 .63/.72 .66/.69 Sent2Vec twitter uni. .67/.74 .52/.53 .75/.72 .72/.78 .77/.81 .64/.68 .62/.71 .67/.71 Sent2Vec twitter uni. + bi. .68/.74 .54/.54 .72/.69 .70/.77 .76/.79 .62/.67 .63/.72 .66/.70 CaptionRep BOW .26/.26 .29/.22 .50/.35 .37/.31 .78/.81 .39/.36 .45/.44 .54/.62 CaptionRep RNN .05/.05 .13/.09 .40/.33 .36/.30 .76/.82 .30/.28 .36/.35 .51/.59 DictRep BOW .62/.67 .42/.40 .81/.81 .62/.66 .66/.68 .53/.58 .61/.63 .58/.66 DictRep BOW + embs.",
      ".65/.72 .49/.47 .85/.86 .67/.72 .71/.74 .57/.61 .61/.70 .62/.70 DictRep RNN .40/.46 .26/.23 .78/.78 .42/.42 .56/.56 .38/.40 .47/.49 .49/.55 DictRep RNN + embs. .51/.60 .29/.27 .80/.81 .44/.47 .65/.70 .42/.46 .52/.56 .49/.59 Table 7: Unsupervised Evaluation: Comparison of the performance of different Sent2Vec models with semi-supervised/supervised models on Spearman/Pearson correlation measures. An underline indicates the best performance for the dataset and Sent2Vec model performances are bold if they perform as well or better than all other non-Sent2Vec models, including those presented in Table 2.",
      "An underline indicates the best performance for the dataset and Sent2Vec model performances are bold if they perform as well or better than all other non-Sent2Vec models, including those presented in Table 2. D Dataset Description STS 2014 SICK 2014 Wikipedia Dataset Twitter Dataset Book Corpus Dataset Sentence Length News Forum WordNet Twitter Images Headlines Test + Train Average 17.23 10.12 8.85 11.64 10.17 7.82 9.67 25.25 16.31 13.32 Standard Deviation 8.66 3.30 3.10 5.28 2.77 2.21 3.75 12.56 7.22 8.94 Table 8: Average sentence lengths for the datasets used in the comparison."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1703.02507.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":13366,
  "avg_doclen":173.5844155844,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1703.02507.pdf"
    }
  }
}