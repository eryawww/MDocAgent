{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Revisiting Low-Resource Neural Machine Translation: A Case Study Rico Sennrich1,2 Biao Zhang1 1School of Informatics, University of Edinburgh rico.sennrich@ed.ac.uk, B.Zhang@ed.ac.uk 2Institute of Computational Linguistics, University of Zurich Abstract It has been shown that the performance of neu- ral machine translation (NMT) drops starkly in low-resource conditions, underperforming phrase-based statistical machine translation (PBSMT) and requiring large amounts of aux- iliary data to achieve competitive results. In this paper, we re-assess the validity of these results, arguing that they are the result of lack of system adaptation to low-resource settings. We discuss some pitfalls to be aware of when training low-resource NMT systems, and re- cent techniques that have shown to be espe- cially helpful in low-resource settings, result- ing in a set of best practices for low-resource NMT.",
      "We discuss some pitfalls to be aware of when training low-resource NMT systems, and re- cent techniques that have shown to be espe- cially helpful in low-resource settings, result- ing in a set of best practices for low-resource NMT. In our experiments on German\u2013English with different amounts of IWSLT14 training data, we show that, without the use of any aux- iliary monolingual or multilingual data, an op- timized NMT system can outperform PBSMT with far less data than previously claimed. We also apply these techniques to a low-resource Korean\u2013English dataset, surpassing previously reported results by 4 BLEU.",
      "We also apply these techniques to a low-resource Korean\u2013English dataset, surpassing previously reported results by 4 BLEU. 1 Introduction While neural machine translation (NMT) has achieved impressive performance in high-resource data conditions, becoming dominant in the \ufb01eld (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017), recent research has ar- gued that these models are highly data-inef\ufb01cient, and underperform phrase-based statistical ma- chine translation (PBSMT) or unsupervised meth- ods in low-data conditions (Koehn and Knowles, 2017; Lample et al., 2018b). In this paper, we re-assess the validity of these results, arguing that they are the result of lack of system adaptation to low-resource settings.",
      "In this paper, we re-assess the validity of these results, arguing that they are the result of lack of system adaptation to low-resource settings. Our main contributions are as follows: \u2022 we explore best practices for low-resource 106 107 108 0 10 20 30 21.8 23.4 24.9 26.2 26.9 27.9 28.6 29.2 29.6 30.1 30.4 16.4 18.1 19.6 21.2 22.2 23.5 24.7 26.1 26.9 27.8 28.6 1.6 7.2 11.9 14.7 18.2 22.4 25.7 27.4 29.2 30.3 31.1 Corpus Size (English Words) BLEU Scores with Varying Amounts of Training Data Phrase-Based with Big LM Phrase-Based Neural Figure 1: quality of PBSMT and NMT in low-resource conditions according to (Koehn and Knowles, 2017).",
      "NMT, evaluating their importance with abla- tion studies. \u2022 we reproduce a comparison of NMT and PB- SMT in different data conditions, showing that when following our best practices, NMT outperforms PBSMT with as little as 100 000 words of parallel training data. 2 Related Work 2.1 Low-Resource Translation Quality Compared Across Systems Figure 1 reproduces a plot by Koehn and Knowles (2017) which shows that their NMT system only outperforms their PBSMT system when more than 100 million words (approx. 5 million sentences) of parallel training data are available. Results shown by Lample et al. (2018b) are similar, showing that unsupervised NMT outperforms supervised sys- tems if few parallel resources are available. In both papers, NMT systems are trained with hyper- parameters that are typical for high-resource set- arXiv:1905.11901v1  [cs.CL]  28 May 2019",
      "tings, and the authors did not tune hyperparame- ters, or change network architectures, to optimize NMT for low-resource conditions. 2.2 Improving Low-Resource Neural Machine Translation The bulk of research on low-resource NMT has focused on exploiting monolingual data, or par- allel data involving other language pairs.",
      "2.2 Improving Low-Resource Neural Machine Translation The bulk of research on low-resource NMT has focused on exploiting monolingual data, or par- allel data involving other language pairs. Meth- ods to improve NMT with monolingual data range from the integration of a separately trained lan- guage model (G\u00a8ulc\u00b8ehre et al., 2015) to the train- ing of parts of the NMT model with additional ob- jectives, including a language modelling objective (G\u00a8ulc\u00b8ehre et al., 2015; Sennrich et al., 2016b; Ra- machandran et al., 2017), an autoencoding objec- tive (Luong et al., 2016; Currey et al., 2017), or a round-trip objective, where the model is trained to predict monolingual (target-side) training data that has been back-translated into the source lan- guage (Sennrich et al., 2016b; He et al., 2016; Cheng et al., 2016).",
      "As an extreme case, mod- els that rely exclusively on monolingual data have been shown to work (Artetxe et al., 2018b; Lample et al., 2018a; Artetxe et al., 2018a; Lample et al., 2018b). Similarly, parallel data from other lan- guage pairs can be used to pre-train the network or jointly learn representations (Zoph et al., 2016; Chen et al., 2017; Nguyen and Chiang, 2017; Neu- big and Hu, 2018; Gu et al., 2018a,b; Kocmi and Bojar, 2018). While semi-supervised and unsupervised ap- proaches have been shown to be very effective for some language pairs, their effectiveness depends on the availability of large amounts of suitable auxiliary data, and other conditions being met.",
      "While semi-supervised and unsupervised ap- proaches have been shown to be very effective for some language pairs, their effectiveness depends on the availability of large amounts of suitable auxiliary data, and other conditions being met. For example, the effectiveness of unsupervised meth- ods is impaired when languages are morphologi- cally different, or when training domains do not match (S\u00f8gaard et al., 2018) More broadly, this line of research still accepts the premise that NMT models are data-inef\ufb01cient and require large amounts of auxiliary data to train. In this work, we want to re-visit this point, and will focus on techniques to make more ef- \ufb01cient use of small amounts of parallel training data. Low-resource NMT without auxiliary data has received less attention; work in this direction includes ( \u00a8Ostling and Tiedemann, 2017; Nguyen and Chiang, 2018). 3 Methods for Low-Resource Neural Machine Translation 3.1 Mainstream Improvements We consider the hyperparameters used by Koehn and Knowles (2017) to be our baseline.",
      "3 Methods for Low-Resource Neural Machine Translation 3.1 Mainstream Improvements We consider the hyperparameters used by Koehn and Knowles (2017) to be our baseline. This base- line does not make use of various advances in NMT architectures and training tricks. In contrast to the baseline, we use a BiDeep RNN architec- ture (Miceli Barone et al., 2017), label smoothing (Szegedy et al., 2016), dropout (Srivastava et al., 2014), word dropout (Sennrich et al., 2016a), layer normalization (Ba et al., 2016) and tied embed- dings (Press and Wolf, 2017). 3.2 Language Representation Subword representations such as BPE (Sennrich et al., 2016c) have become a popular choice to achieve open-vocabulary translation. BPE has one hyperparameter, the number of merge operations, which determines the size of the \ufb01nal vocabulary. For high-resource settings, the effect of vocabu- lary size on translation quality is relatively small; Haddow et al.",
      "BPE has one hyperparameter, the number of merge operations, which determines the size of the \ufb01nal vocabulary. For high-resource settings, the effect of vocabu- lary size on translation quality is relatively small; Haddow et al. (2018) report mixed results when comparing vocabularies of 30k and 90k subwords. In low-resource settings, large vocabularies re- sult in low-frequency (sub)words being repre- sented as atomic units at training time, and the ability to learn good high-dimensional representa- tions of these is doubtful. Sennrich et al. (2017a) propose a minimum frequency threshold for sub- word units, and splitting any less frequent subword into smaller units or characters. We expect that such a threshold reduces the need to carefully tune the vocabulary size to the dataset, leading to more aggressive segmentation on smaller datasets.1 3.3 Hyperparameter Tuning Due to long training times, hyperparameters are hard to optimize by grid search, and are of- ten re-used across experiments. However, best practices differ between high-resource and low- resource settings.",
      "However, best practices differ between high-resource and low- resource settings. While the trend in high-resource settings is towards using larger and deeper mod- els, Nguyen and Chiang (2018) use smaller and fewer layers for smaller datasets. Previous work has argued for larger batch sizes in NMT (Mor- ishita et al., 2017; Neishi et al., 2017), but we 1In related work, Cherry et al. (2018) have shown that, given deep encoders and decoders, character-level models can outperform other subword segmentations. In preliminary experiments, a character-level model performed poorly in our low-resource setting.",
      "\ufb01nd that using smaller batches is bene\ufb01cial in low- resource settings. More aggressive dropout, in- cluding dropping whole words at random (Gal and Ghahramani, 2016), is also likely to be more im- portant. We report results on a narrow hyperpa- rameter search guided by previous work and our own intuition. 3.4 Lexical Model Finally, we implement and test the lexical model by Nguyen and Chiang (2018), which has been shown to be bene\ufb01cial in low-data conditions. The core idea is to train a simple feed-forward net- work, the lexical model, jointly with the original attentional NMT model. The input of the lexical model at time step t is the weighted average of source embeddings f (the attention weights a are shared with the main model). After a feedforward layer (with skip connection), the lexical model\u2019s output hl t is combined with the original model\u2019s hidden state ho t before softmax computation.",
      "The input of the lexical model at time step t is the weighted average of source embeddings f (the attention weights a are shared with the main model). After a feedforward layer (with skip connection), the lexical model\u2019s output hl t is combined with the original model\u2019s hidden state ho t before softmax computation. fl t = tanh X s at(s)fs hl t = tanh(Wfl t) + fl t p(yt|y<t, x) = softmax(W oho t + bo + W lhl t + bl) Our implementation adds dropout and layer nor- malization to the lexical model.2 4 Experiments 4.1 Data and Preprocessing We use the TED data from the IWSLT 2014 German\u2192English shared translation task (Cettolo et al., 2014). We use the same data cleanup and train/dev split as Ranzato et al. (2016), resulting in 159 000 parallel sentences of training data, and 7584 for development.",
      "We use the same data cleanup and train/dev split as Ranzato et al. (2016), resulting in 159 000 parallel sentences of training data, and 7584 for development. As a second language pair, we evaluate our sys- tems on a Korean\u2013English dataset3 with around 90 000 parallel sentences of training data, 1000 for development, and 2000 for testing. For both PBSMT and NMT, we apply the same tokenization and truecasing using Moses scripts. For NMT, we also learn BPE subword segmen- tation with 30 000 merge operations, shared be- tween German and English, and independently for Korean\u2192English.",
      "For both PBSMT and NMT, we apply the same tokenization and truecasing using Moses scripts. For NMT, we also learn BPE subword segmen- tation with 30 000 merge operations, shared be- tween German and English, and independently for Korean\u2192English. 2Implementation released in Nematus: https://github.com/EdinburghNLP/nematus 3https://sites.google.com/site/ koreanparalleldata/ subword vocabulary sentences words (EN) DE/KO EN DE\u2192EN 159 000 3 220 000 18 870 13 830 80 000 1 610 000 9850 7740 40 000 810 000 7470 5950 20 000 400 000 5640 4530 10 000 200 000 3760 3110 5000 100 000 2380 1990 KO\u2192EN 94 000 2 300 000 32 082 16 006 Table 1: Training corpus size and subword vocabulary size for different subsets of IWSLT14 DE\u2192EN data, and for KO\u2192EN data.",
      "To simulate different amounts of training re- sources, we randomly subsample the IWSLT train- ing corpus 5 times, discarding half of the data at each step. Truecaser and BPE segmentation are learned on the full training corpus; as one of our experiments, we set the frequency threshold for subword units to 10 in each subcorpus (see 3.2). Table 1 shows statistics for each subcorpus, in- cluding the subword vocabulary. Translation outputs are detruecased, detok- enized, and compared against the reference with cased BLEU using sacreBLEU (Papineni et al., 2002; Post, 2018).4 Like Ranzato et al. (2016), we report BLEU on the concatenated dev sets for IWSLT 2014 (tst2010, tst2011, tst2012, dev2010, dev2012). 4.2 PBSMT Baseline We use Moses (Koehn et al., 2007) to train a PBSMT system.",
      "4.2 PBSMT Baseline We use Moses (Koehn et al., 2007) to train a PBSMT system. We use MGIZA (Gao and Vo- gel, 2008) to train word alignments, and lmplz (Hea\ufb01eld et al., 2013) for a 5-gram LM. Feature weights are optimized on the dev set to maxi- mize BLEU with batch MIRA (Cherry and Foster, 2012) \u2013 we perform multiple runs where indicated. Unlike Koehn and Knowles (2017), we do not use extra data for the LM. Both PBSMT and NMT can bene\ufb01t from monolingual data, so the availability of monolingual data is no longer an exclusive ad- vantage of PBSMT (see 2.2).",
      "BLEU ID system 100k 3.2M 1 phrase-based SMT 15.87 \u00b1 0.19 26.60 \u00b1 0.00 2 NMT baseline 0.00 \u00b1 0.00 25.70 \u00b1 0.33 3 2 + \u201dmainstream improvements\u201d (dropout, tied embeddings, 7.20 \u00b1 0.62 31.93 \u00b1 0.05 layer normalization, bideep RNN, label smoothing) 4 3 + reduce BPE vocabulary (14k \u21922k symbols) 12.10 \u00b1 0.16 - 5 4 + reduce batch size (4k \u21921k tokens) 12.40 \u00b1 0.08 31.97 \u00b1 0.26 6 5 + lexical model 13.03 \u00b1 0.49 31.80 \u00b1 0.22 7 5 + aggressive (word) dropout 15.87 \u00b1 0.09 33.60 \u00b1 0.14 8 7 + other hyperparameter tuning (learning rate, 16.57 \u00b1 0.26 32.80 \u00b1 0.08 model depth,",
      "22 7 5 + aggressive (word) dropout 15.87 \u00b1 0.09 33.60 \u00b1 0.14 8 7 + other hyperparameter tuning (learning rate, 16.57 \u00b1 0.26 32.80 \u00b1 0.08 model depth, label smoothing rate) 9 8 + lexical model 16.10 \u00b1 0.29 33.30 \u00b1 0.08 Table 2: German\u2192English IWSLT results for training corpus size of 100k words and 3.2M words (full corpus). Mean and standard deviation of three training runs reported.",
      "10 \u00b1 0.29 33.30 \u00b1 0.08 Table 2: German\u2192English IWSLT results for training corpus size of 100k words and 3.2M words (full corpus). Mean and standard deviation of three training runs reported. 105 106 0 10 20 30 32.8 30.8 28.7 24.4 20.6 16.6 26.6 24.9 23 20.5 18.3 16 25.7 18.5 11.6 1.8 1.3 0 corpus size (English words) BLEU neural MT optimized phrase-based SMT neural MT baseline Figure 2: German\u2192English learning curve, showing BLEU as a function of the amount of parallel training data, for PBSMT and NMT. 4.3 NMT Systems We train neural systems with Nematus (Sennrich et al., 2017b). Our baseline mostly follows the settings in (Koehn and Knowles, 2017); we use adam (Kingma and Ba, 2015) and perform early stopping based on dev set BLEU.",
      "Our baseline mostly follows the settings in (Koehn and Knowles, 2017); we use adam (Kingma and Ba, 2015) and perform early stopping based on dev set BLEU. We express our batch size in number of tokens, and set it to 4000 in the baseline (comparable to a batch size of 80 sentences used in previous work). We subsequently add the methods described in section 3, namely the bideep RNN, label smooth- ing, dropout, tied embeddings, layer normaliza- tion, changes to the BPE vocabulary size, batch 4Signature BLEU+c.mixed+#.1+s.exp+tok.13a+v.1.3.2. size, model depth, regularization parameters and learning rate. Detailed hyperparameters are re- ported in Appendix A. 5 Results Table 2 shows the effect of adding different meth- ods to the baseline NMT system, on the ultra-low data condition (100k words of training data) and the full IWSLT 14 training corpus (3.2M words). Our \u201dmainstream improvements\u201d add around 6\u20137 BLEU in both data conditions.",
      "Our \u201dmainstream improvements\u201d add around 6\u20137 BLEU in both data conditions. In the ultra-low data condition, reducing the BPE vocabulary size is very effective (+4.9 BLEU). Reducing the batch size to 1000 token re- sults in a BLEU gain of 0.3, and the lexical model yields an additional +0.6 BLEU. However, ag- gressive (word) dropout6 (+3.4 BLEU) and tuning other hyperparameters (+0.7 BLEU) has a stronger effect than the lexical model, and adding the lex- ical model (9) on top of the optimized con\ufb01g- uration (8) does not improve performance. To- gether, the adaptations to the ultra-low data setting yield 9.4 BLEU (7.2\u219216.6). The model trained on full IWSLT data is less sensitive to our changes (31.9\u219232.8 BLEU), and optimal hyperparameters differ depending on the data condition.",
      "The model trained on full IWSLT data is less sensitive to our changes (31.9\u219232.8 BLEU), and optimal hyperparameters differ depending on the data condition. Subse- quently, we still apply the hyperparameters that were optimized to the ultra-low data condition (8) 5beam search results reported by Wiseman and Rush (2016). 6p = 0.3 for dropping words; p = 0.5 for other dropout.",
      "system BLEU MIXER (Ranzato et al., 2016)5 21.8 BSO (Wiseman and Rush, 2016) 25.5 NPMT+LM (Huang et al., 2018) 30.1 MRT (Edunov et al., 2018) 32.84 \u00b1 0.08 Pervasive Attention (Elbayad et al., 2018) 33.8 Transformer Baseline (Wu et al., 2019) 34.4 Dynamic Convolution (Wu et al., 2019) 35.2 our PBSMT (1) 28.19 \u00b1 0.01 our NMT baseline (2) 27.16 \u00b1 0.38 our NMT best (7) 35.27 \u00b1 0.14 Table 3: Results on full IWSLT14 German\u2192English data on tokenized and lowercased test set with multi-bleu.perl.",
      "system BLEU (Gu et al., 2018b) 5.97 (supervised Transformer) phrase-based SMT 6.57 \u00b1 0.17 NMT baseline (2) 2.93 \u00b1 0.34 NMT optimized (8) 10.37 \u00b1 0.29 Table 4: Korean\u2192English results. Mean and standard deviation of three training runs reported. to other data conditions, and Korean\u2192English, for simplicity. For a comparison with PBSMT, and across different data settings, consider Figure 2, which shows the result of PBSMT, our NMT baseline, and our optimized NMT system. Our NMT base- line still performs worse than the PBSMT system for 3.2M words of training data, which is con- sistent with the results by Koehn and Knowles (2017). However, our optimized NMT system shows strong improvements, and outperforms the PBSMT system across all data settings. Some sample translations are shown in Appendix B. For comparison to previous work, we report lowercased and tokenized results on the full IWSLT 14 training set in Table 3.",
      "However, our optimized NMT system shows strong improvements, and outperforms the PBSMT system across all data settings. Some sample translations are shown in Appendix B. For comparison to previous work, we report lowercased and tokenized results on the full IWSLT 14 training set in Table 3. Our results far outperform the RNN-based results reported by Wiseman and Rush (2016), and are on par with the best reported results on this dataset. Table 4 shows results for Korean\u2192English, using the same con\ufb01gurations (1, 2 and 8) as for German\u2013English. Our results con\ufb01rm that the techniques we apply are successful across datasets, and result in stronger systems than pre- viously reported on this dataset, achieving 10.37 BLEU as compared to 5.97 BLEU reported by Gu et al. (2018b). 6 Conclusions Our results demonstrate that NMT is in fact a suit- able choice in low-data settings, and can outper- form PBSMT with far less parallel training data than previously claimed.",
      "(2018b). 6 Conclusions Our results demonstrate that NMT is in fact a suit- able choice in low-data settings, and can outper- form PBSMT with far less parallel training data than previously claimed. Recently, the main trend in low-resource MT research has been the bet- ter exploitation of monolingual and multilingual resources. Our results show that low-resource NMT is very sensitive to hyperparameters such as BPE vocabulary size, word dropout, and oth- ers, and by following a set of best practices, we can train competitive NMT systems without re- lying on auxiliary resources. This has practical relevance for languages where large amounts of monolingual data, or multilingual data involving related languages, are not available. Even though we focused on only using parallel data, our re- sults are also relevant for work on using auxiliary data to improve low-resource MT.",
      "This has practical relevance for languages where large amounts of monolingual data, or multilingual data involving related languages, are not available. Even though we focused on only using parallel data, our re- sults are also relevant for work on using auxiliary data to improve low-resource MT. Supervised sys- tems serve as an important baseline to judge the ef- fectiveness of semisupervised or unsupervised ap- proaches, and the quality of supervised systems trained on little data can directly impact semi- supervised work\ufb02ows, for instance for the back- translation of monolingual data. Acknowledgments Rico Sennrich has received funding from the Swiss National Science Foundation in the project CoNTra (grant number 105212 169888). Biao Zhang acknowledges the support of the Baidu Scholarship.",
      "References Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018a. Unsupervised Statistical Machine Transla- tion. In Proceedings of the 2018 Conference on Em- pirical Methods in Natural Language Processing, pages 3632\u20133642, Brussels, Belgium. Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. 2018b. Unsupervised Neural Ma- chine Translation. In International Conference on Learning Representations. Lei Jimmy Ba, Ryan Kiros, and Geoffrey E. Hin- ton. 2016. Layer Normalization. CoRR, abs/1607.06450. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In Proceedings of the International Conference on Learning Represen- tations (ICLR). Mauro Cettolo, Jan Niehues, Sebastian St\u00a8uker, Luisa Bentivogli, and Marcello Federico. 2014.",
      "In Proceedings of the International Conference on Learning Represen- tations (ICLR). Mauro Cettolo, Jan Niehues, Sebastian St\u00a8uker, Luisa Bentivogli, and Marcello Federico. 2014. Report on the 11th IWSLT Evaluation Campaign, IWSLT 2014. In Proceedings of the 11th Workshop on Spo- ken Language Translation, pages 2\u201316, Lake Tahoe, CA, USA. Yun Chen, Yang Liu, Yong Cheng, and Victor O.K. Li. 2017. A Teacher-Student Framework for Zero- Resource Neural Machine Translation. In Proceed- ings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers), pages 1925\u20131935, Vancouver, Canada. Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. 2016. Semi- Supervised Learning for Neural Machine Transla- tion.",
      "Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. 2016. Semi- Supervised Learning for Neural Machine Transla- tion. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 1965\u20131974, Berlin, Germany. Colin Cherry and George Foster. 2012. Batch Tuning Strategies for Statistical Machine Translation. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Compu- tational Linguistics: Human Language Technolo- gies, NAACL HLT \u201912, pages 427\u2013436, Montreal, Canada. Colin Cherry, George Foster, Ankur Bapna, Orhan Firat, and Wolfgang Macherey. 2018. Revisiting Character-Based Neural Machine Translation with Capacity and Compression. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4295\u20134305, Brussels, Belgium.",
      "2018. Revisiting Character-Based Neural Machine Translation with Capacity and Compression. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4295\u20134305, Brussels, Belgium. Anna Currey, Antonio Valerio Miceli Barone, and Ken- neth Hea\ufb01eld. 2017. Copied Monolingual Data Improves Low-Resource Neural Machine Transla- tion. In Proceedings of the Second Conference on Machine Translation, pages 148\u2013156, Copenhagen, Denmark. Sergey Edunov, Myle Ott, Michael Auli, David Grang- ier, and Marc\u2019Aurelio Ranzato. 2018. Classical structured prediction losses for sequence to se- quence learning. In Proceedings of the 2018 Con- ference of the North American Chapter of the Asso- ciation for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long Papers), pages 355\u2013364, New Orleans, Louisiana. Maha Elbayad, Laurent Besacier, and Jakob Verbeek. 2018.",
      "Maha Elbayad, Laurent Besacier, and Jakob Verbeek. 2018. Pervasive Attention: 2D Convolutional Neu- ral Networks for Sequence-to-Sequence Prediction. In Proceedings of the 22nd Conference on Compu- tational Natural Language Learning, pages 97\u2013107, Brussels, Belgium. Yarin Gal and Zoubin Ghahramani. 2016. A theoret- ically grounded application of dropout in recurrent neural networks. In Advances in Neural Information Processing Systems 29, pages 1019\u20131027. Qin Gao and Stephan Vogel. 2008. Parallel Implemen- tations of Word Alignment Tool. In Software En- gineering, Testing, and Quality Assurance for Natu- ral Language Processing, pages 49\u201357, Columbus, Ohio. Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor O.K. Li. 2018a. Universal Neural Machine Translation for Extremely Low Resource Languages.",
      "Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor O.K. Li. 2018a. Universal Neural Machine Translation for Extremely Low Resource Languages. In Pro- ceedings of the 2018 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol- ume 1 (Long Papers), pages 344\u2013354, New Orleans, Louisiana. Jiatao Gu, Yong Wang, Yun Chen, Victor O. K. Li, and Kyunghyun Cho. 2018b. Meta-Learning for Low- Resource Neural Machine Translation. In Proceed- ings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3622\u20133631, Brussels, Belgium. C\u00b8 aglar G\u00a8ulc\u00b8ehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho, Lo\u00a8\u0131c Barrault, Huei-Chi Lin, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2015. On Us- ing Monolingual Corpora in Neural Machine Trans- lation.",
      "2015. On Us- ing Monolingual Corpora in Neural Machine Trans- lation. CoRR, abs/1503.03535. Barry Haddow, Nikolay Bogoychev, Denis Emelin, Ulrich Germann, Roman Grundkiewicz, Kenneth Hea\ufb01eld, Antonio Valerio Miceli Barone, and Rico Sennrich. 2018. The University of Edinburgh\u2019s Sub- missions to the WMT18 News Translation Task. In Proceedings of the Third Conference on Machine Translation, pages 403\u2013413, Belgium, Brussels. Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tieyan Liu, and Wei-Ying Ma. 2016. Dual Learn- ing for Machine Translation. In Advances in Neural Information Processing Systems 29, pages 820\u2013828. Kenneth Hea\ufb01eld, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. 2013. Scalable Modi\ufb01ed",
      "Kneser-Ney Language Model Estimation. In Pro- ceedings of the 51st Annual Meeting of the Associa- tion for Computational Linguistics, pages 690\u2013696, So\ufb01a, Bulgaria. Po-Sen Huang, Chong Wang, Sitao Huang, Dengyong Zhou, and Li Deng. 2018. Towards Neural Phrase- based Machine Translation. In International Con- ference on Learning Representations. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In The Inter- national Conference on Learning Representations, San Diego, California, USA. Tom Kocmi and Ond\u02c7rej Bojar. 2018. Trivial Transfer Learning for Low-Resource Neural Machine Trans- lation. In Proceedings of the Third Conference on Machine Translation, pages 244\u2013252, Belgium, Brussels.",
      "Tom Kocmi and Ond\u02c7rej Bojar. 2018. Trivial Transfer Learning for Low-Resource Neural Machine Trans- lation. In Proceedings of the Third Conference on Machine Translation, pages 244\u2013252, Belgium, Brussels. Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond\u02c7rej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the ACL-2007 Demo and Poster Sessions, pages 177\u2013180, Prague, Czech Republic. Philipp Koehn and Rebecca Knowles. 2017. Six Chal- lenges for Neural Machine Translation. In Pro- ceedings of the First Workshop on Neural Machine Translation, pages 28\u201339, Vancouver. Guillaume Lample, Ludovic Denoyer, and Marc\u2019Aurelio Ranzato. 2018a. Unsupervised Machine Translation Using Monolingual Corpora Only.",
      "In Pro- ceedings of the First Workshop on Neural Machine Translation, pages 28\u201339, Vancouver. Guillaume Lample, Ludovic Denoyer, and Marc\u2019Aurelio Ranzato. 2018a. Unsupervised Machine Translation Using Monolingual Corpora Only. In International Conference on Learning Representations. Guillaume Lample, Myle Ott, Alexis Conneau, Lu- dovic Denoyer, and Marc\u2019Aurelio Ranzato. 2018b. Phrase-Based & Neural Unsupervised Machine Translation. In Proceedings of the 2018 Confer- ence on Empirical Methods in Natural Language Processing (EMNLP), pages 5039\u20135049, Brussels, Belgium. Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. 2016. Multi-task Se- quence to Sequence Learning. In The International Conference on Learning Representations. Antonio Valerio Miceli Barone, Jind\u02c7rich Helcl, Rico Sennrich, Barry Haddow, and Alexandra Birch. 2017.",
      "2016. Multi-task Se- quence to Sequence Learning. In The International Conference on Learning Representations. Antonio Valerio Miceli Barone, Jind\u02c7rich Helcl, Rico Sennrich, Barry Haddow, and Alexandra Birch. 2017. Deep Architectures for Neural Machine Translation. In Proceedings of the Second Confer- ence on Machine Translation, Volume 1: Research Papers, Copenhagen, Denmark. Makoto Morishita, Yusuke Oda, Graham Neubig, Koichiro Yoshino, Katsuhito Sudoh, and Satoshi Nakamura. 2017. An Empirical Study of Mini- Batch Creation Strategies for Neural Machine Trans- lation. In The First Workshop on Neural Ma- chine Translation (NMT), pages 61\u201368, Vancouver, Canada. Masato Neishi, Jin Sakuma, Satoshi Tohda, Shonosuke Ishiwatari, Naoki Yoshinaga, and Masashi Toyoda. 2017. A Bag of Useful Tricks for Practical Neural Machine Translation: Embedding Layer Initializa- tion and Large Batch Size.",
      "Masato Neishi, Jin Sakuma, Satoshi Tohda, Shonosuke Ishiwatari, Naoki Yoshinaga, and Masashi Toyoda. 2017. A Bag of Useful Tricks for Practical Neural Machine Translation: Embedding Layer Initializa- tion and Large Batch Size. In Proceedings of the 4th Workshop on Asian Translation (WAT2017), pages 99\u2013109, Taipei, Taiwan. Graham Neubig and Junjie Hu. 2018. Rapid Adap- tation of Neural Machine Translation to New Lan- guages. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Process- ing, pages 875\u2013880, Brussels, Belgium. Toan Nguyen and David Chiang. 2018. Improving Lexical Choice in Neural Machine Translation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 334\u2013343, New Or- leans, Louisiana. Toan Q. Nguyen and David Chiang. 2017.",
      "Toan Q. Nguyen and David Chiang. 2017. Transfer Learning across Low-Resource, Related Languages for Neural Machine Translation. In Proceedings of the Eighth International Joint Conference on Natu- ral Language Processing (Volume 2: Short Papers), pages 296\u2013301, Taipei, Taiwan. Robert \u00a8Ostling and J\u00a8org Tiedemann. 2017. Neural ma- chine translation for low-resource languages. CoRR, abs/1708.05729. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Com- putational Linguistics, pages 311\u2013318, Philadelphia, PA. Matt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186\u2013 191, Belgium, Brussels. O\ufb01r Press and Lior Wolf. 2017. Using the Output Em- bedding to Improve Language Models.",
      "A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186\u2013 191, Belgium, Brussels. O\ufb01r Press and Lior Wolf. 2017. Using the Output Em- bedding to Improve Language Models. In Proceed- ings of the 15th Conference of the European Chap- ter of the Association for Computational Linguistics (EACL), Valencia, Spain. Prajit Ramachandran, Peter Liu, and Quoc Le. 2017. Unsupervised pretraining for sequence to sequence learning. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Process- ing, pages 383\u2013391, Copenhagen, Denmark. Marc\u2019Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2016. Sequence Level Training with Recurrent Neural Networks. In The",
      "International Conference on Learning Representa- tions. Rico Sennrich, Alexandra Birch, Anna Currey, Ulrich Germann, Barry Haddow, Kenneth Hea\ufb01eld, An- tonio Valerio Miceli Barone, and Philip Williams. 2017a. The University of Edinburgh\u2019s Neural MT Systems for WMT17. In Proceedings of the Sec- ond Conference on Machine Translation, Volume 2: Shared Task Papers, Copenhagen, Denmark. Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexan- dra Birch, Barry Haddow, Julian Hitschler, Marcin Junczys-Dowmunt, Samuel L\u00a8aubli, Antonio Vale- rio Miceli Barone, Jozef Mokry, and Maria Nade- jde. 2017b. Nematus: a Toolkit for Neural Machine Translation. In Proceedings of the Software Demon- strations of the 15th Conference of the European Chapter of the Association for Computational Lin- guistics, pages 65\u201368, Valencia, Spain. Rico Sennrich, Barry Haddow, and Alexandra Birch.",
      "In Proceedings of the Software Demon- strations of the 15th Conference of the European Chapter of the Association for Computational Lin- guistics, pages 65\u201368, Valencia, Spain. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016a. Edinburgh Neural Machine Translation Sys- tems for WMT 16. In Proceedings of the First Con- ference on Machine Translation, Volume 2: Shared Task Papers, pages 368\u2013373, Berlin, Germany. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016b. Improving Neural Machine Translation Models with Monolingual Data. In Proceedings of the 54th Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers), pages 86\u201396, Berlin, Germany. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016c. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u2013 1725, Berlin, Germany.",
      "2016c. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u2013 1725, Berlin, Germany. Anders S\u00f8gaard, Sebastian Ruder, and Ivan Vulic. 2018. On the limitations of unsupervised bilingual dictionary induction. In Proceedings of the 56th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 778\u2013 788, Melbourne, Australia. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A Simple Way to Prevent Neural Networks from Over\ufb01tting. Journal of Machine Learning Re- search, 15:1929\u20131958. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to Sequence Learning with Neural Net- works.",
      "Journal of Machine Learning Re- search, 15:1929\u20131958. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to Sequence Learning with Neural Net- works. In Advances in Neural Information Process- ing Systems 27: Annual Conference on Neural Infor- mation Processing Systems 2014, pages 3104\u20133112, Montreal, Quebec, Canada. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Z. Wojna. 2016. Rethinking the Inception Architecture for Computer Vision. In 2016 IEEE Conference on Computer Vision and Pat- tern Recognition (CVPR), pages 2818\u20132826. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Pro- cessing Systems 30, pages 5998\u20136008.",
      "2017. Attention is All you Need. In Advances in Neural Information Pro- cessing Systems 30, pages 5998\u20136008. Sam Wiseman and Alexander M. Rush. 2016. Sequence-to-sequence learning as beam-search opti- mization. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Process- ing, pages 1296\u20131306, Austin, Texas. Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and Michael Auli. 2019. Pay less attention with lightweight and dynamic convolutions. In Interna- tional Conference on Learning Representations. Barret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight. 2016. Transfer Learning for Low-Resource Neural Machine Translation. In Proceedings of the 2016 Conference on Empirical Methods in Natu- ral Language Processing, pages 1568\u20131575, Austin, Texas.",
      "A Hyperparameters Table 5 lists hyperparameters used for the different experiments in the ablation study (Table 2). Hy- perparameters were kept constant across different data settings, except for the validation interval and subword vocabulary size (see Table 1). B Sample Translations Table 6 shows some sample translations that rep- resent typical errors of our PBSMT and NMT systems, trained with ultra-low (100k words) and low (3.2M words) amounts of data. For unknown words such as blutbe\ufb02eckten (\u2018bloodstained\u2019) or Spaniern (\u2018Spaniards\u2019, \u2018Spanish\u2019), PBSMT sys- tems default to copying, while NMT systems pro- duce translations on a subword-level, with vary- ing success (blue-\ufb02ect, bleed; spaniers, Spani- ans). NMT systems learn some syntactic dis- ambiguation even with very little data, for ex- ample the translation of das and die as relative pronouns (\u2019that\u2019, \u2019which\u2019, \u2019who\u2019), while PBSMT produces less grammatical translation.",
      "NMT systems learn some syntactic dis- ambiguation even with very little data, for ex- ample the translation of das and die as relative pronouns (\u2019that\u2019, \u2019which\u2019, \u2019who\u2019), while PBSMT produces less grammatical translation. On the \ufb02ip side, the ultra low-resource NMT system ig- nores some unknown words in favour of a more- or-less \ufb02uent, but semantically inadequate trans- lation: erobert (\u2019conquered\u2019) is translated into doing, and richtig aufgezeichnet (\u2019registered cor- rectly\u2019, \u2018recorded correctly\u2019) into really the \ufb01rst thing.",
      "system hyperparameter 2 3 5 6 7 8 9 hidden layer size 1024 embedding size 512 encoder depth 1 2 1 encoder recurrence transition depth 1 2 decoder depth 1 2 1 dec. recurrence transition depth (base) 2 4 2 dec. recurrence transition depth (high) - 2 - tie decoder embeddings - yes layer normalization - yes lexical model - yes - yes hidden dropout - 0.2 0.5 embedding dropout - 0.2 0.5 source word dropout - 0.1 0.3 target word dropout - 0.3 label smoothing - 0.1 0.2 maximum sentence length 200 minibatch size (# tokens) 4000 1000 learning rate 0.0001 0.0005 optimizer adam early stopping patience 10 validation interval: IWSLT 100k / 200k / 400k 50 100 400 IWSLT \u2265800k / KO-EN 2.3M 1000 2000 8000 beam size 5 Table 5: Con\ufb01gurations of NMT systems reported in Table 2.",
      "Empty \ufb01elds indicate that hyperparameter was unchanged compared to previous systems.",
      "source In einem blutbe\ufb02eckten Kontinent, waren diese Menschen die einzigen, die nie von den Spaniern erobert wurden. reference In a bloodstained continent, these people alone were never conquered by the Spanish. PBSMT 100k In a blutbe\ufb02eckten continent, were these people the only, the never of the Spaniern erobert were. PBSMT 3.2M In a blutbe\ufb02eckten continent, these people were the only ones that were never of the Spaniern conquered. NMT 3.2M (baseline) In a blinging tree continent, these people were the only ones that never had been conquered by the Spanians. NMT 100k (optimized) In a blue-\ufb02ect continent, these people were the only one that has never been doing by the spaniers. NMT 3.2M (optimized) In a bleed continent, these people were the only ones who had never been conquered by the Spanians. source Dies ist tatschlich ein Poster von Notre Dame, das richtig aufgezeichnet wurde.",
      "NMT 3.2M (optimized) In a bleed continent, these people were the only ones who had never been conquered by the Spanians. source Dies ist tatschlich ein Poster von Notre Dame, das richtig aufgezeichnet wurde. reference This is actually a poster of Notre Dame that registered correctly. PBSMT 100k This is actually poster of Notre lady, the right aufgezeichnet was. PBSMT 3.2M This is actually a poster of Notre Dame, the right recorded. NMT 3.2M (baseline) This is actually a poster of emergency lady who was just recorded properly. NMT 100k (optimized) This is actually a poster of Notre Dame, that was really the \ufb01rst thing. NMT 3.2M (optimized) This is actually a poster from Notre Dame, which has been recorded right. Table 6: German\u2192English translation examples with phrase-based SMT and NMT systems trained on 100k/3.2M words of parallel data."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1905.11901.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":8903,
  "avg_doclen":164.8703703704,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1905.11901.pdf"
    }
  }
}