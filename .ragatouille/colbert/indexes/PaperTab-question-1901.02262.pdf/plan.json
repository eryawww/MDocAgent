{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "arXiv:1901.02262v2  [cs.CL]  27 May 2019 Multi-Style Generative Reading Comprehension Kyosuke Nishida1, Itsumi Saito1, Kosuke Nishida1, Kazutoshi Shinoda2\u2217, Atsushi Otsuka1, Hisako Asano1, Junji Tomita1 1NTT Media Intelligence Laboratory, NTT Corporation 2The University of Tokyo kyosuke.nishida@acm.org Abstract This study tackles generative reading compre- hension (RC), which consists of answering questions based on textual evidence and nat- ural language generation (NLG). We propose a multi-style abstractive summarization model for question answering, called Masque. The proposed model has two key characteristics. First, unlike most studies on RC that have fo- cused on extracting an answer span from the provided passages, our model instead focuses on generating a summary from the question and multiple passages. This serves to cover various answer styles required for real-world applications.",
            "The proposed model has two key characteristics. First, unlike most studies on RC that have fo- cused on extracting an answer span from the provided passages, our model instead focuses on generating a summary from the question and multiple passages. This serves to cover various answer styles required for real-world applications. Second, whereas previous stud- ies built a speci\ufb01c model for each answer style because of the dif\ufb01culty of acquiring one gen- eral model, our approach learns multi-style an- swers within a model to improve the NLG ca- pability for all styles involved. This also en- ables our model to give an answer in the tar- get style. Experiments show that our model achieves state-of-the-art performance on the Q&A task and the Q&A + NLG task of MS MARCO 2.1 and the summary task of Nar- rativeQA. We observe that the transfer of the style-independent NLG capability to the target style is the key to its success. 1 Introduction Question answering has been a long-standing re- search problem.",
            "We observe that the transfer of the style-independent NLG capability to the target style is the key to its success. 1 Introduction Question answering has been a long-standing re- search problem. Recently, reading comprehension (RC), a challenge to answer a question given tex- tual evidence provided in a document set, has re- ceived much attention. Current mainstream stud- ies have treated RC as a process of extracting an answer span from one passage (Rajpurkar et al., 2016, 2018) or multiple passages (Joshi et al., 2017; Yang et al., 2018), which is usually done by predicting the start and end positions of the an- swer (Yu et al., 2018; Devlin et al., 2018). \u2217Work done during an internship at NTT. 0 1 0 1 10 weeks <\/s> it  takes 10 weeks to  get  new york state tax refund .    <\/s> Question: \u201chow long to get nys tax refund\u201d Generate from Voc.",
            "\u2217Work done during an internship at NTT. 0 1 0 1 10 weeks <\/s> it  takes 10 weeks to  get  new york state tax refund .    <\/s> Question: \u201chow long to get nys tax refund\u201d Generate from Voc. Copy from Question Copy from Passages Mixture weights [NLG] [Q&A] Figure 1: Visualization of how our model generates an answer on MS MARCO. Given an answer style (top: NLG, bottom: Q&A), the model controls the mixture of three distributions for generating words from a vocabulary and copying words from the ques- tion and multiple passages at each decoding step. The demand for answering questions in natural language is increasing rapidly, and this has led to the development of smart devices such as Alexa. In comparison with answer span extraction, how- ever, the natural language generation (NLG) capa- bility for RC has been less studied.",
            "The demand for answering questions in natural language is increasing rapidly, and this has led to the development of smart devices such as Alexa. In comparison with answer span extraction, how- ever, the natural language generation (NLG) capa- bility for RC has been less studied. While datasets such as MS MARCO (Bajaj et al., 2018) and Nar- rativeQA (Kocisk\u00b4y et al., 2018) have been pro- posed for providing abstractive answers, the state- of-the-art methods for these datasets are based on answer span extraction (Wu et al., 2018; Hu et al., 2018). Generative models suffer from a dearth of training data to cover open-domain questions. Moreover, to satisfy various information needs, intelligent agents should be capable of answer- ing one question in multiple styles, such as well- formed sentences, which make sense even without the context of the question and passages, and con- cise phrases. These capabilities complement each other, but previous studies cannot use and control different styles within a model. In this study, we propose Masque, a genera- tive model for multi-passage RC.",
            "These capabilities complement each other, but previous studies cannot use and control different styles within a model. In this study, we propose Masque, a genera- tive model for multi-passage RC. It achieves state- of-the-art performance on the Q&A task and the Q&A + NLG task of MS MARCO 2.1 and the summary task of NarrativeQA. The main contri-",
            "butions of this study are as follows. Multi-source abstractive summarization. We introduce the pointer-generator mechanism (See et al., 2017) for generating an abstractive answer from the question and multiple passages, which covers various answer styles. We extend the mech- anism to a Transformer (Vaswani et al., 2017) based one that allows words to be generated from a vocabulary and to be copied from the question and passages. Multi-style learning for style control and trans- fer. We introduce multi-style learning that en- ables our model to control answer styles and im- proves RC for all styles involved. We also ex- tend the pointer-generator to a conditional decoder by introducing an arti\ufb01cial token corresponding to each style, as in (Johnson et al., 2017). For each decoding step, it controls the mixture weights over three distributions with the given style (Figure 1). 2 Problem Formulation This paper considers the following task: PROBLEM 1. Given a question with J words xq = {xq 1, . . .",
            "For each decoding step, it controls the mixture weights over three distributions with the given style (Figure 1). 2 Problem Formulation This paper considers the following task: PROBLEM 1. Given a question with J words xq = {xq 1, . . . , xq J}, a set of K passages, where the k-th passage is composed of L words xpk = {xpk 1 , . . . , xpk L }, and an answer style label s, an RC model outputs an answer y = {y1, . . . , yT } condi- tioned on the style. In short, given a 3-tuple (xq, {xpk}, s), the sys- tem predicts P(y). The training data is a set of 6-tuples: (xq, {xpk}, s, y, a, {rpk}), where a and {rpk} are optional.",
            "The training data is a set of 6-tuples: (xq, {xpk}, s, y, a, {rpk}), where a and {rpk} are optional. Here, a is 1 if the question is answerable with the provided passages and 0 oth- erwise, and rpk is 1 if the k-th passage is required to formulate the answer and 0 otherwise. 3 Proposed Model We propose a Multi-style Abstractive Summa- rization model for QUEstion answering, called Masque. Masque directly models the conditional probability p(y|xq, {xpk}, s). As shown in Fig- ure 2, it consists of the following modules. 1. The question-passages reader (\u00a73.1) models interactions between the question and passages. 2. The passage ranker (\u00a73.2) \ufb01nds passages rele- vant to the question. 3. The answer possibility classi\ufb01er (\u00a73.3) identi- \ufb01es answerable questions.",
            "2. The passage ranker (\u00a73.2) \ufb01nds passages rele- vant to the question. 3. The answer possibility classi\ufb01er (\u00a73.3) identi- \ufb01es answerable questions. Masked  Multi-Head  Attention  Shared Encoder Block  Shared Encoder Block  Shared Encoder Block  Modeling Encoder Block  Modeling Encoder Block  Modeling  Encoder Block  Multi-Head  Attention  Multi-Head  Attention  Add & Norm Feed  Forward  Add & Norm Highway  Glove&ELMo  Add & Norm Dual Attention Concat Add & Norm Highway  Glove&ELMo  Highway  Glove&ELMo  Highway  Glove&ELMo  Additive  Attention  Additive  Attention  Combined  Attention  Multi-source Pointer-Generator Passage 1 Passage K Question Style + Answer (Shiftted)  8x 3x 5x 2x Answer Possibility  Answer Word Sequence  Passage Ranker Classi\ufb01er query  query  query  query  Decoder Reader Relevance  Figure 2: Masque model architecture. 4.",
            "4. The answer sentence decoder (\u00a73.4) outputs an answer sentence conditioned on the target style. Our model is based on multi-source abstractive summarization: the answer that it generates can be viewed as a summary from the question and pas- sages. The model also learns multi-style answers together. With these two characteristics, we aim to acquire the style-independent NLG ability and transfer it to the target style. In addition, to im- prove natural language understanding in the reader module, our model considers RC, passage rank- ing, and answer possibility classi\ufb01cation together as multi-task learning. 3.1 Question-Passages Reader The reader module is shared among multiple an- swer styles and the three task-speci\ufb01c modules. 3.1.1 Word Embedding Layer Let xq and xpk represent one-hot vectors (of size V ) for words in the question and the k-th pas- sage. First, this layer projects each of the vec- tors to a dword-dimensional vector with a pre- trained weight matrix W e \u2208Rdword\u00d7V such as GloVe (Pennington et al., 2014).",
            "First, this layer projects each of the vec- tors to a dword-dimensional vector with a pre- trained weight matrix W e \u2208Rdword\u00d7V such as GloVe (Pennington et al., 2014). Next, it uses con- textualized word representations via ELMo (Pe- ters et al., 2018), which allows our model to use morphological clues to form robust representa- tions for out-of-vocabulary words unseen in train- ing. Then, the concatenation of the word and con-",
            "textualized vectors is passed to a two-layer high- way network (Srivastava et al., 2015) to fuse the two types of embeddings, as in (Seo et al., 2017). The highway network is shared by the question and passages. 3.1.2 Shared Encoder Layer This layer uses a stack of Transformer blocks, which are shared by the question and passages, on top of the embeddings provided by the word embedding layer. The input of the \ufb01rst block is immediately mapped to a d-dimensional vector by a linear transformation. The outputs of this layer are Epk \u2208Rd\u00d7L for each k-th passage, and Eq \u2208Rd\u00d7J for the question. Transformer encoder block. The block con- sists of two sub-layers: a self-attention layer and a position-wise feed-forward network. For the self-attention layer, we adopt the multi-head atten- tion mechanism (Vaswani et al., 2017).",
            "Transformer encoder block. The block con- sists of two sub-layers: a self-attention layer and a position-wise feed-forward network. For the self-attention layer, we adopt the multi-head atten- tion mechanism (Vaswani et al., 2017). Following GPT (Radford et al., 2018), the feed-forward net- work consists of two linear transformations with a GELU (Hendrycks and Gimpel, 2016) activation function in between. Each sub-layer is placed in- side a residual block (He et al., 2016). For an in- put x and a given sub-layer function f, the output is LN(f(x) + x), where LN indicates the layer normalization (Ba et al., 2016). To facilitate these residual connections, all sub-layers produce a se- quence of d-dimensional vectors. Note that our model does not use any position embeddings in this block because ELMo gives the positional in- formation of the words in each sequence.",
            "To facilitate these residual connections, all sub-layers produce a se- quence of d-dimensional vectors. Note that our model does not use any position embeddings in this block because ELMo gives the positional in- formation of the words in each sequence. 3.1.3 Dual Attention Layer This layer uses a dual attention mechanism to fuse information from the question to the passages as well as from the passages to the question. It \ufb01rst computes a similarity matrix U pk \u2208 RL\u00d7J between the question and the k-th passage, as done in (Seo et al., 2017), where U pk lj = wa\u22a4[Epk l ; Eq j ; Epk l \u2299Eq j ] indicates the similarity between the l-th word of the k-th passage and the j-th question word. The wa \u2208R3d are learnable parameters. The \u2299 operator denotes the Hadamard product, and the [; ] operator denotes vector concatenation across the rows. Next, the layer obtains the row and column normalized similarity matrices Apk = softmaxj(U pk\u22a4) and Bpk = softmaxl(U pk).",
            "The \u2299 operator denotes the Hadamard product, and the [; ] operator denotes vector concatenation across the rows. Next, the layer obtains the row and column normalized similarity matrices Apk = softmaxj(U pk\u22a4) and Bpk = softmaxl(U pk). It then uses DCN (Xiong et al., 2017) to obtain dual attention representations, Gq\u2192pk \u2208R5d\u00d7L and Gp\u2192q \u2208R5d\u00d7J: Gq\u2192pk = [Epk; \u00afApk; \u00af\u00afApk; Epk \u2299\u00afApk; Epk \u2299\u00af\u00afApk] Gp\u2192q = [Eq; \u00afB; \u00af\u00afB; Eq \u2299\u00afB; Eq \u2299\u00af\u00afB]. Here, \u00afApk = EqApk, \u00afBpk = EpkBpk, \u00af\u00afApk = \u00afBpkApk, \u00af\u00afBpk = \u00afApkBpk, \u00afB = maxk( \u00afBpk), and \u00af\u00afB = maxk( \u00af\u00afBpk).",
            "3.1.4 Modeling Encoder Layer This layer uses a stack of the Transformer en- coder blocks for question representations and ob- tains Mq \u2208Rd\u00d7J from Gp\u2192q. It also uses an- other stack for passage representations and obtains Mpk \u2208Rd\u00d7L from Gq\u2192pk for each k-th pas- sage. The outputs of this layer, Mq and {Mpk}, are passed on to the answer sentence decoder; the {Mpk} are also passed on to the passage ranker and the answer possibility classi\ufb01er. 3.2 Passage Ranker The ranker maps the output of the modeling layer, {Mpk}, to the relevance score of each passage. It takes the output for the \ufb01rst word, Mpk 1 , which corresponds to the beginning-of-sentence token, to obtain the aggregate representation of each pas- sage sequence. Given wr \u2208Rd as learnable pa- rameters, it calculates the relevance of each k-th passage to the question as \u03b2pk = sigmoid(wr\u22a4Mpk 1 ).",
            "Given wr \u2208Rd as learnable pa- rameters, it calculates the relevance of each k-th passage to the question as \u03b2pk = sigmoid(wr\u22a4Mpk 1 ). 3.3 Answer Possibility Classi\ufb01er The classi\ufb01er maps the output of the modeling layer to a probability for the answer possibility. It also takes the output for the \ufb01rst word, Mpk 1 , for all passages and concatenates them. Given wc \u2208RKd as learnable parameters, it calculates the answer possibility for the question as P(a) = sigmoid(wc\u22a4[Mp1 1 ; . . . ; MpK 1 ]). 3.4 Answer Sentence Decoder Given the outputs provided by the reader mod- ule, the decoder generates a sequence of an- swer words one element at a time. It is auto- regressive (Graves, 2013), consuming the previ- ously generated words as additional input at each decoding step.",
            "3.4.1 Word Embedding Layer Let y represent one-hot vectors of the words in the answer. This layer has the same components as the word embedding layer of the reader module, except that it uses a unidirectional ELMo to ensure that the predictions for position t depend only on the known outputs at positions previous to t. Arti\ufb01cial tokens. To be able to use multiple an- swer styles within a single system, our model in- troduces an arti\ufb01cial token corresponding to the style at the beginning of the answer (y1), as done in (Johnson et al., 2017; Takeno et al., 2017). At test time, the user can specify the \ufb01rst token to control the style. This modi\ufb01cation does not re- quire any changes to the model architecture. Note that introducing the token at the decoder prevents the reader module from depending on the answer style. 3.4.2 Attentional Decoder Layer This layer uses a stack of Transformer decoder blocks on top of the embeddings provided by the word embedding layer.",
            "Note that introducing the token at the decoder prevents the reader module from depending on the answer style. 3.4.2 Attentional Decoder Layer This layer uses a stack of Transformer decoder blocks on top of the embeddings provided by the word embedding layer. The input is immedi- ately mapped to a d-dimensional vector by a lin- ear transformation, and the output is a sequence of d-dimensional vectors: {s1, . . . , sT }. Transformer decoder block. In addition to the encoder block, this block consists of the second and third sub-layers after the self-attention block and before the feed-forward network, as shown in Figure 2. As in (Vaswani et al., 2017), the self- attention sub-layer uses a sub-sequent mask to pre- vent positions from attending to subsequent posi- tions. The second and third sub-layers perform the multi-head attention over Mq and Mpall, respec- tively. The Mpall is the concatenated outputs of the encoder stack for the passages, Mpall = [Mp1, . . . , MpK] \u2208Rd\u00d7KL.",
            "The Mpall is the concatenated outputs of the encoder stack for the passages, Mpall = [Mp1, . . . , MpK] \u2208Rd\u00d7KL. Here, the [, ] operator denotes vector concatenation across the columns. This attention for the concate- nated passages produces attention weights that are comparable between passages. 3.4.3 Multi-source Pointer-Generator Our extended mechanism allows both words to be generated from a vocabulary and words to be copied from both the question and multiple pas- sages (Figure 3). We expect that the capability of copying words will be shared among answer styles. Additive  Attention  Additive  Attention  Final distribution  Mixing weights Feed-  Forward  Feed-  Forward  Context vec. Attention  weights  Voc. dist.  Passage  Representations  Question  Representations  Decoder  t-th state  Attention  dist.  query key,\u00cavalue  Figure 3: Multi-source pointer-generator mechanism. For each decoding step t, mixture weights \u03bbv, \u03bbq, \u03bbp for the probability of generating words from the vo- cabulary and copying words from the question and the passages are calculated.",
            "query key,\u00cavalue  Figure 3: Multi-source pointer-generator mechanism. For each decoding step t, mixture weights \u03bbv, \u03bbq, \u03bbp for the probability of generating words from the vo- cabulary and copying words from the question and the passages are calculated. The three distributions are weighted and summed to obtain the \ufb01nal distribution. Extended vocabulary distribution. Let the ex- tended vocabulary, Vext, be the union of the com- mon words (a small subset of the full vocabulary, V , de\ufb01ned by the input-side word embedding ma- trix) and all words appearing in the input question and passages. P v then denotes the probability dis- tribution of the t-th answer word, yt, over the ex- tended vocabulary. It is de\ufb01ned as: P v(yt) = softmax(W 2\u22a4(W 1st + b1)), where the output embedding W 2 \u2208Rdword\u00d7Vext is tied with the corresponding part of the input em- bedding (Inan et al., 2017), and W 1 \u2208Rdword\u00d7d and b1 \u2208Rdword are learnable parameters.",
            "P v(yt) is zero if yt is an out-of-vocabulary word for V . Copy distributions. A recent Transformer- based pointer-generator randomly chooses one of the attention-heads to form a copy distribution; that approach gave no signi\ufb01cant improvements in text summarization (Gehrmann et al., 2018). In contrast, our model uses an additional atten- tion layer for each copy distribution on top of the decoder stack. For the passages, the layer takes st as the query and outputs \u03b1p t \u2208RKL as the atten- tion weights and cp t \u2208Rd as the context vectors: epk l = wp\u22a4tanh(W pmMpk l + W psst + bp), \u03b1p t = softmax([ep1; . . . ; epK]), (1) cp t = P l \u03b1p tlMpall l , where wp, bp \u2208Rd and W pm, W ps \u2208Rd\u00d7d are learnable parameters. For the question, our model",
            "uses another identical layer and obtains \u03b1q t \u2208RJ and cq t \u2208Rd. As a result, P q and P p are the copy distributions over the extended vocabulary: P q(yt) = P j:xq j=yt \u03b1q tj, P p(yt) = P l:x pk(l) l =yt \u03b1p tl, where k(l) means the passage index correspond- ing to the l-th word in the concatenated passages. Final distribution. The \ufb01nal distribution of yt is de\ufb01ned as a mixture of the three distributions: P(yt) = \u03bbvP v(yt) + \u03bbqP q(yt) + \u03bbpP p(yt), \u03bbv, \u03bbq, \u03bbp = softmax(W m[st; cq t; cp t ] + bm), where W m \u2208R3\u00d73d and bm \u2208R3 are learnable parameters. 3.4.4 Combined Attention In order not to attend words in irrelevant passages, our model introduces a combined attention. While the original technique combined word and sen- tence level attentions (Hsu et al., 2018), our model combines the word and passage level attentions.",
            "3.4.4 Combined Attention In order not to attend words in irrelevant passages, our model introduces a combined attention. While the original technique combined word and sen- tence level attentions (Hsu et al., 2018), our model combines the word and passage level attentions. The word attention, Eq. 1, is re-de\ufb01ned as \u03b1p tl = \u03b1p tl\u03b2pk(l) P l\u2032 \u03b1p tl\u2032\u03b2pk(l\u2032) . 3.5 Loss Function We de\ufb01ne the training loss as the sum of losses via L(\u03b8) = Ldec + \u03b3rankLrank + \u03b3clsLcls where \u03b8 is the set of all learnable parameters, and \u03b3rank and \u03b3cls are balancing parameters. The loss of the decoder, Ldec, is the negative log likelihood of the whole target answer sentence averaged over Nable answerable examples: Ldec = \u2212 1 Nable X (a,y)\u2208D a T X t log P(yt), where D is the training dataset.",
            "The loss of the decoder, Ldec, is the negative log likelihood of the whole target answer sentence averaged over Nable answerable examples: Ldec = \u2212 1 Nable X (a,y)\u2208D a T X t log P(yt), where D is the training dataset. The losses of the passage ranker, Lrank, and the answer possibility classi\ufb01er, Lcls, are the binary cross entropy be- tween the true and predicted values averaged over all N examples: Lrank = \u2212 1 NK X k X rpk\u2208D \u0012rpk log \u03b2pk+ (1 \u2212rpk) log(1 \u2212\u03b2pk) \u0013 , Lcls = \u22121 N X a\u2208D \u0012a log P(a)+ (1 \u2212a) log(1 \u2212P(a)) \u0013 . Dataset Subset Train Dev. Eval. ALL 808,731 101,093 101,092 MS MARCO ANS 503,370 55,636 \u2013 NLG 153,725 12,467 \u2013 NarrativeQA Summary 32,747 3,461 10,557 Table 1: Numbers of questions used in the experiments.",
            "4 Experiments on MS MARCO 2.1 We evaluated our model on MS MARCO 2.1 (Ba- jaj et al., 2018). It is the sole dataset providing ab- stractive answers with multiple styles and serves as a great test bed for building open-domain QA agents with the NLG capability that can be used in smart devices. The details of our setup and output examples are in the supplementary material. 4.1 Setup Datasets. MS MARCO 2.1 provides two tasks for generative open-domain QA: the Q&A task and the Q&A + Natural Language Generation (NLG) task. Both tasks consist of questions sub- mitted to Bing by real users, and each question refers to ten passages. The dataset also includes annotations on the relevant passages, which were selected by humans to form the \ufb01nal answers, and on whether there was no answer in the passages. Answer styles. We associated the two tasks with two answer styles. The NLG task requires a well- formed answer that is an abstractive summary of the question and passages, averaging 16.6 words.",
            "Answer styles. We associated the two tasks with two answer styles. The NLG task requires a well- formed answer that is an abstractive summary of the question and passages, averaging 16.6 words. The Q&A task also requires an abstractive answer but prefers it to be more concise than in the NLG task, averaging 13.1 words, and many of the an- swers do not contain the context of the question. For the question \u201ctablespoon in cup\u201d, a reference answer in the Q&A task is \u201c16,\u201d while that in the NLG task is \u201cThere are 16 tablespoons in a cup.\u201d Subsets. In addition to the ALL dataset, we pre- pared two subsets for ablation tests as listed in Ta- ble 1. The ANS set consisted of answerable ques- tions, and the NLG set consisted of the answerable questions and well-formed answers, so that NLG \u2282ANS \u2282ALL.",
            "The ANS set consisted of answerable ques- tions, and the NLG set consisted of the answerable questions and well-formed answers, so that NLG \u2282ANS \u2282ALL. We note that multi-style learning enables our model to learn from different answer styles of data (i.e., the ANS set), and multi-task learning with the answer possibility classi\ufb01er en- ables our model to learn from both answerable and unanswerable data (i.e., the ALL set). Training and Inference. We trained our model with mini-batches consisting of multi-style an-",
            "NLG Q&A Model R-L B-1 R-L B-1 BiDAFa 16.91 9.30 23.96 10.64 Deep Cascade QAb 35.14 37.35 52.01 54.64 S-Net+CES2Sc 45.04 40.62 44.96 46.36 BERT+Multi-PGNetd 47.37 45.09 48.14 52.03 Selector+CCGe 47.39 45.26 50.63 52.03 VNETf 48.37 46.75 51.63 54.37 Masque (NLG; single) 49.19 49.63 48.42 48.68 Masque (NLG; ensemble) 49.61 50.13 48.92 48.75 Masque (Q&A; single) 25.66 36.62 50.93 42.37 Masque (Q&A; ensemble) 28.53 39.87 52.20 43.77 Human Performance 63.21 53.03 53.87 48.",
            "single) 25.66 36.62 50.93 42.37 Masque (Q&A; ensemble) 28.53 39.87 52.20 43.77 Human Performance 63.21 53.03 53.87 48.50 Table 2: Performance of our and competing models on the MS MARCO V2 leaderboard (4 March 2019). aSeo et al. (2017); bYan et al. (2019); cShao (unpublished), a variant of Tan et al. (2018); dLi (unpublished), a model using Devlin et al. (2018) and See et al. (2017); eQian (unpublished); fWu et al. (2018). Whether the compet- ing models are ensemble models or not is unreported. swers that were randomly sampled. We used a greedy decoding algorithm and did not use any beam search or random sampling, because they did not provide any improvements. Evaluation metrics and baselines.",
            "(2018). Whether the compet- ing models are ensemble models or not is unreported. swers that were randomly sampled. We used a greedy decoding algorithm and did not use any beam search or random sampling, because they did not provide any improvements. Evaluation metrics and baselines. ROUGE-L and BLEU-1 were used to evaluate the models\u2019 RC performance, where ROUGE-L is the main metric on the of\ufb01cial leaderboard. We used the reported scores of extractive (Seo et al., 2017; Yan et al., 2019; Wu et al., 2018), generative (Tan et al., 2018), and unpublished RC models at the submis- sion time. In addition, to evaluate the individual contribu- tions of our modules, we used MAP and MRR for the ranker and F1 for the classi\ufb01er, where the pos- itive class was the answerable questions. 4.2 Results Does our model achieve state-of-the-art on the two tasks with different styles? Table 2 shows the performance of our model and competing models on the leaderboard.",
            "4.2 Results Does our model achieve state-of-the-art on the two tasks with different styles? Table 2 shows the performance of our model and competing models on the leaderboard. Our ensemble model of six training runs, where each model was trained with the two answer styles, achieved state-of-the- art performance on both tasks in terms of ROUGE- L. In particular, for the NLG task, our single model outperformed competing models in terms of both ROUGE-L and BLEU-1. Does multi-style learning improve the NLG performance?",
            "Does multi-style learning improve the NLG performance? Table 3 lists the results of an ab- lation test for our single model (controlled with Model Train R-L B-1 Masque (NLG style; single) ALL 69.77 65.56 w\/o multi-style learning (\u00a73.4.2) NLG 68.20 63.95 \u0592\u2192w\/o Transformer (\u00a73.1.2, \u00a73.4.2) NLG 67.13 62.96 w\/o passage ranker (\u00a73.2) NLG 68.05 63.82 w\/o possibility classi\ufb01er (\u00a73.3) ANS 69.64 65.41 Masque w\/ gold passage ranker ALL 78.70 78.14 Table 3: Ablation test results on the NLG dev. set. The models were trained with the subset listed in \u201cTrain\u201d.",
            "set. The models were trained with the subset listed in \u201cTrain\u201d. Model Train MAP MRR Bing (initial ranking) - 34.62 35.00 Masque (single) ALL 69.51 69.96 w\/o answer decoder (\u00a73.4) ALL 67.03 67.49 w\/o multi-style learning (\u00a73.4.2) NLG 65.51 65.59 w\/o possibility classi\ufb01er (\u00a73.3) ANS 69.08 69.54 Table 4: Passage ranking results on the ANS dev. set. the NLG style) on the NLG dev. set1. Our model trained with both styles outperformed the model trained with the single NLG style. Multi-style learning enabled our model to improve its NLG performance by also using non-sentence answers. Does the Transformer-based pointer-generator improve the NLG performance? Table 3 shows that our model also outperformed the model that used RNNs and self-attentions instead of Transformer blocks as in MCAN (McCann et al., 2018).",
            "Does the Transformer-based pointer-generator improve the NLG performance? Table 3 shows that our model also outperformed the model that used RNNs and self-attentions instead of Transformer blocks as in MCAN (McCann et al., 2018). Our deep decoder captured the multi-hop interaction among the question, the passages, and the answer better than a single-layer LSTM de- coder could. Does joint learning with the ranker and classi- \ufb01er improve NLG performance? Furthermore, Table 3 shows that our model (jointly trained with the passage ranker and answer possibility classi- \ufb01er) outperformed the model that did not use the ranker and classi\ufb01er. Joint learning thus had a reg- ularization effect on the question-passages reader. We also con\ufb01rmed that the gold passage ranker, which can perfectly predict the relevance of pas- sages, signi\ufb01cantly improved the RC performance. Passage ranking will be a key to developing a sys- tem that can outperform humans. Does joint learning improve the passage rank- ing performance?",
            "Passage ranking will be a key to developing a sys- tem that can outperform humans. Does joint learning improve the passage rank- ing performance? Table 4 lists the passage ranking performance on the ANS dev. set2. The 1We con\ufb01rmed with the organizer that the dev. results were much better than the test results, but there was no prob- lem. 2This evaluation requires our ranker to re-rank 10 pas- sages. It is not the same as the Passage Re-ranking task.",
            "1.0 0.9 0.8 0.7 0.6 0.5 0.4 Precision 0.0 0.2 0.4 0.6 0.8 1.0 Recall F1=0.9 0.7 0.5 0.3 0.1 Figure 4: Precision-recall curve for answer possibility classi\ufb01cation on the ALL dev. set. 0 5 10 15 20 25 30 yesno which when who where all how what other why Prediction (Q&A) Reference (Q&A) Prediction (NLG) Reference (NLG) Length Figure 5: Lengths of answers generated by Masque broken down by the answer style and query type on the NLG dev. set. The error bars indicate standard errors. ranker shares the question-passages reader with the answer decoder, and this sharing contributed to improvements over the ranker trained without the answer decoder. Also, our ranker outperformed the initial ranking provided by Bing by a signi\ufb01- cant margin. Does our model accurately identify answerable questions?",
            "Also, our ranker outperformed the initial ranking provided by Bing by a signi\ufb01- cant margin. Does our model accurately identify answerable questions? Figure 4 shows the precision-recall curve for answer possibility classi\ufb01cation on the ALL dev. set. Our model identi\ufb01ed the answer- able questions well. The maximum F1 score was 0.7893, where the threshold of answer possibility was 0.4411. This is the \ufb01rst report on answer pos- sibility classi\ufb01cation with MS MARCO 2.1. Does our model control answer lengths with different styles? Figure 5 shows the lengths of the answers generated by our model broken down by the answer style and query type. The generated answers were relatively shorter than the reference answers, especially for the Q&A task, but well controlled with the target style for every query type. The short answers degraded our model\u2019s BLEU scores in the Q&A task (Table 2) because of BLEU\u2019s brevity penalty (Papineni et al., 2002).",
            "The short answers degraded our model\u2019s BLEU scores in the Q&A task (Table 2) because of BLEU\u2019s brevity penalty (Papineni et al., 2002). 5 Experiments on NarrativeQA Next, we evaluated our model on Narra- tiveQA (Kocisk\u00b4y et al., 2018). It requires under- standing the underlying narrative rather than re- lying on shallow pattern matching. Our detailed setup and output examples are in the supplemen- tary material. 5.1 Setup We only describe the settings speci\ufb01c to this ex- periment. Datasets. Following previous studies, we used the summary setting for the comparisons with the reported baselines, where each question refers to one summary (averaging 659 words), and there is no unanswerable questions. Our model therefore did not use the passage ranker and answer possi- bility classi\ufb01er. Answer styles. The NarrativeQA dataset does not explicitly provide multiple answer styles. In order to evaluate the effectiveness of multi-style learning, we used the NLG subset of MS MARCO as additional training data.",
            "Answer styles. The NarrativeQA dataset does not explicitly provide multiple answer styles. In order to evaluate the effectiveness of multi-style learning, we used the NLG subset of MS MARCO as additional training data. We associated the NarrativeQA and NLG datasets with two answer styles. The answer style of NarrativeQA (NQA) is different from that of MS MARCO (NLG) in that the answers are short (averaging 4.73 words) and contained frequently pronouns. For instance, for the question \u201cWho is Mark Hunter?\u201d, a reference is \u201cHe is a high school student in Phoenix.\u201d Evaluation metrics and baselines. BLEU-1 and 4, METEOR, and ROUGE-L were used in accordance with the evaluation in the dataset pa- per (Kocisk\u00b4y et al., 2018). We used the reports of top-performing extractive (Seo et al., 2017; Tay et al., 2018; Hu et al., 2018) and generative (Bauer et al., 2018; Indurthi et al., 2018) models.",
            "We used the reports of top-performing extractive (Seo et al., 2017; Tay et al., 2018; Hu et al., 2018) and generative (Bauer et al., 2018; Indurthi et al., 2018) models. 5.2 Results Does our model achieve state-of-the-art perfor- mance? Table 5 shows that our single model, trained with two styles and controlled with the NQA style, pushed forward the state-of-the-art by a signi\ufb01cant margin. The evaluation scores of the model controlled with the NLG style were low be- cause the two styles are different. Also, our model without multi-style learning (trained with only the NQA style) outperformed the baselines in terms of ROUGE-L. This indicates that our model architec-",
            "Model B-1 B-4 M R-L BiDAFa 33.72 15.53 15.38 36.30 DECAPROPb 42.00 23.42 23.42 40.07 MHPGM+NOICc 43.63 21.07 19.03 44.16 ConZNetd 42.76 22.49 19.24 46.67 RMR+A2De 50.4 26.5 N\/A 53.3 Masque (NQA) 54.11 30.43 26.13 59.87 w\/o multi-style learning 48.70 20.98 21.95 54.74 Masque (NLG) 39.14 18.11 24.62 50.09 Masque (NQA; valid.)f 52.78 28.72 25.38 58.94 Table 5: Performance of our and competing models on the NarrativeQA test set. aSeo et al. (2017); bTay et al. (2018); cBauer et al.",
            ")f 52.78 28.72 25.38 58.94 Table 5: Performance of our and competing models on the NarrativeQA test set. aSeo et al. (2017); bTay et al. (2018); cBauer et al. (2018); dIndurthi et al. (2018); eHu et al. (2018). fResults on the NarrativeQA valida- tion set. ture itself is powerful for natural language under- standing in RC. 6 Related Work and Discussion Transfer and multi-task learning in RC. Re- cent breakthroughs in transfer learning demon- strate that pre-trained language models perform well on RC with minimal modi\ufb01cations (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2018, 2019). In addition, our model also uses ELMo (Peters et al., 2018) for contextualized em- beddings.",
            "In addition, our model also uses ELMo (Peters et al., 2018) for contextualized em- beddings. Multi-task learning is a transfer mechanism to improve generalization performance (Caruana, 1997), and it is generally applied by sharing the hidden layers between all tasks, while keep- ing task-speci\ufb01c layers. Wang et al. (2018) and Nishida et al. (2018) reported that the sharing of the hidden layers between the multi-passage RC and passage ranking tasks was effective. Our re- sults also showed the effectiveness of the sharing of the question-passages reader module among the RC, passage ranking, and answer possibility clas- si\ufb01cation tasks. In multi-task learning without task-speci\ufb01c lay- ers, Devlin et al. (2018) and Chen et al. (2017) improved RC performance by learning multiple datasets from the same extractive RC setting. Mc- Cann et al. (2018) and Yogatama et al.",
            "(2018) and Chen et al. (2017) improved RC performance by learning multiple datasets from the same extractive RC setting. Mc- Cann et al. (2018) and Yogatama et al. (2019) in- vestigated multi-task and curriculum learning on many different NLP tasks; their results were below task-speci\ufb01c RC models. Our multi-style learning does not use style-speci\ufb01c layers; instead uses a style-conditional decoder. Generative RC. S-Net (Tan et al., 2018) used an extraction-then-synthesis mechanism for multi- passage RC. The models proposed by McCann et al. (2018), Bauer et al. (2018), and Indurthi et al. (2018) used an RNN-based pointer-generator mechanism for single-passage RC. Although these mechanisms can alleviate the lack of training data, large amounts of data are still required. Our multi- style learning will be a key technique enabling learning from many RC datasets with different styles. In addition to MS MARCO and NarrativeQA, there are other datasets that provide abstractive answers.",
            "Although these mechanisms can alleviate the lack of training data, large amounts of data are still required. Our multi- style learning will be a key technique enabling learning from many RC datasets with different styles. In addition to MS MARCO and NarrativeQA, there are other datasets that provide abstractive answers. DuReader (He et al., 2018), a Chinese multi-document RC dataset, provides longer doc- uments and answers than those of MS MARCO. DuoRC (Saha et al., 2018) and CoQA (Reddy et al., 2018) contain abstractive answers; most of the answers are short phrases. Controllable text generation. Many studies have been carried out in the framework of style transfer, which is the task of rephrasing a text so that it contains speci\ufb01c styles such as sentiment.",
            "Controllable text generation. Many studies have been carried out in the framework of style transfer, which is the task of rephrasing a text so that it contains speci\ufb01c styles such as sentiment. Recent studies have used arti\ufb01cial tokens (Sen- nrich et al., 2016; Johnson et al., 2017), varia- tional auto-encoders (Hu et al., 2017), or adver- sarial training (Fu et al., 2018; Tsvetkov et al., 2018) to separate the content and style on the en- coder side. On the decoder side, conditional lan- guage modeling has been used to generate out- put sentences with the target style. In addition, output length control with conditional language modeling has been well studied (Kikuchi et al., 2016; Takeno et al., 2017; Fan et al., 2018). Our style-controllable RC relies on conditional lan- guage modeling in the decoder. Multi-passage RC.",
            "Our style-controllable RC relies on conditional lan- guage modeling in the decoder. Multi-passage RC. The simplest approach is to concatenate the passages and \ufb01nd the answer from the concatenation, as in (Wang et al., 2017). Ear- lier pipelined models found a small number of rel- evant passages with a TF-IDF based ranker and passed them to a neural reader (Chen et al., 2017; Clark and Gardner, 2018), while more recent mod- els have used a neural re-ranker to more accurately select the relevant passages (Wang et al., 2018; Nishida et al., 2018). Also, non-pipelined models (including ours) consider all the provided passages and \ufb01nd the answer by comparing scores between passages (Tan et al., 2018; Wu et al., 2018). The most recent models make a proper trade-off be- tween ef\ufb01ciency and accuracy (Yan et al., 2019; Min et al., 2018).",
            "RC with unanswerable question identi\ufb01cation. The previous work of (Levy et al., 2017; Clark and Gardner, 2018) outputted a no-answer score de- pending on the probability of all answer spans. Hu et al. (2019) proposed an answer veri\ufb01er to com- pare an answer with the question. Sun et al. (2018) jointly learned an RC model and an answer veri- \ufb01er. Our model introduces a classi\ufb01er on top of the question-passages reader, which is not dependent on the generated answer. Abstractive summarization. Current state-of- the-art models use the pointer-generator mecha- nism (See et al., 2017). In particular, content se- lection approaches, which decide what to sum- marize, have recently been used with abstractive models.",
            "Abstractive summarization. Current state-of- the-art models use the pointer-generator mecha- nism (See et al., 2017). In particular, content se- lection approaches, which decide what to sum- marize, have recently been used with abstractive models. Most methods select content at the sen- tence level (Hsu et al., 2018; Chen and Bansal, 2018) or the word level (Pasunuru and Bansal, 2018; Li et al., 2018; Gehrmann et al., 2018). Our model incorporates content selection at the pas- sage level in the combined attention. Query-based summarization has rarely been studied because of a lack of datasets. Nema et al. (2017) proposed an attentional encoder-decoder model; however, Saha et al. (2018) reported that it performed worse than BiDAF on DuoRC. Has- selqvist et al. (2017) proposed a pointer-generator based model; however, it does not consider copy- ing words from the question. 7 Conclusion This study sheds light on multi-style generative RC.",
            "Has- selqvist et al. (2017) proposed a pointer-generator based model; however, it does not consider copy- ing words from the question. 7 Conclusion This study sheds light on multi-style generative RC. Our proposed model, Masque, is based on multi-source abstractive summarization and learns multi-style answers together. It achieved state- of-the-art performance on the Q&A task and the Q&A + NLG task of MS MARCO 2.1 and the summary task of NarrativeQA. The key to its success is transferring the style-independent NLG capability to the target style by use of the question-passages reader and the conditional pointer-generator decoder. In particular, the capa- bility of copying words from the question and pas- sages can be shared among the styles, while the capability of controlling the mixture weights for the generative and copy distributions can be ac- quired for each style. Our future work will involve exploring the potential of our multi-style learning towards natural language understanding. References Lei Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer normalization.",
            "Our future work will involve exploring the potential of our multi-style learning towards natural language understanding. References Lei Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer normalization. Computing Research Repository (CoRR), arXiv:1607.06450. Version 1. Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2018. MS MARCO: A human generated machine reading comprehension dataset. Computing Research Repository (CoRR), arXiv:1611.09268. Version 3. Lisa Bauer, Yicheng Wang, and Mohit Bansal. 2018. Commonsense for generative multi-hop question an- swering tasks. In Empirical Methods in Natural Language Processing (EMNLP), pages 4220\u20134230. Richard Caruana. 1997. Multitask learning.",
            "2018. Commonsense for generative multi-hop question an- swering tasks. In Empirical Methods in Natural Language Processing (EMNLP), pages 4220\u20134230. Richard Caruana. 1997. Multitask learning. Machine Learning, 28(1):41\u201375. Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer open\u2013 domain questions. In Association for Computa- tional Linguistics (ACL), pages 1870\u20131879. Yen-Chun Chen and Mohit Bansal. 2018. Fast abstrac- tive summarization with reinforce-selected sentence rewriting. In Association for Computational Lin- guistics (ACL), pages 675\u2013686. Christopher Clark and Matt Gardner. 2018. Simple and effective multi-paragraph reading comprehen- sion. In Association for Computational Linguistics (ACL), pages 845\u2013855. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: pre-training of deep bidirectional transformers for language under- standing.",
            "In Association for Computational Linguistics (ACL), pages 845\u2013855. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: pre-training of deep bidirectional transformers for language under- standing. Computing Research Repository (CoRR), arXiv:1810.04805. Version 1. Angela Fan, David Grangier, and Michael Auli. 2018. Controllable abstractive summarization. In Work- shop on Neural Machine Translation and Genera- tion (NMT@ACL), pages 45\u201354. Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, and Rui Yan. 2018. Style transfer in text: Exploration and evaluation. In Association for the Advancement of Arti\ufb01cial Intelligence (AAAI), pages 663\u2013670. Sebastian Gehrmann, Yuntian Deng, and Alexander M. Rush. 2018. Bottom-up abstractive summarization. In Empirical Methods in Natural Language Process- ing (EMNLP), pages 4098\u20134109. Alex Graves.",
            "Sebastian Gehrmann, Yuntian Deng, and Alexander M. Rush. 2018. Bottom-up abstractive summarization. In Empirical Methods in Natural Language Process- ing (EMNLP), pages 4098\u20134109. Alex Graves. 2013. Generating sequences with recur- rent neural networks. Computing Research Reposi- tory (CoRR), arXiv:1308.0850. Version 5. Johan Hasselqvist, Niklas Helmertz, and Mikael K\u02daageb\u00a8ack. 2017. Query-based abstractive summa- rization using neural networks. arXiv, 1712.06100.",
            "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recog- nition. In Computer Vision and Pattern Recognition (CVPR), pages 770\u2013778. Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wang. 2018. DuReader: a chinese machine read- ing comprehension dataset from real-world applica- tions. In Workshop on Machine Reading for Ques- tion Answering (MRQA@ACL), pages 37\u201346. Dan Hendrycks and Kevin Gimpel. 2016. Bridging nonlinearities and stochastic regularizers with gaus- sian error linear units. Computing Research Reposi- tory (CoRR), arXiv:1606.08415. Version 2. Wan-Ting Hsu, Chieh-Kai Lin, Ming-Ying Lee, Kerui Min, Jing Tang, and Min Sun.",
            "Computing Research Reposi- tory (CoRR), arXiv:1606.08415. Version 2. Wan-Ting Hsu, Chieh-Kai Lin, Ming-Ying Lee, Kerui Min, Jing Tang, and Min Sun. 2018. A uni\ufb01ed model for extractive and abstractive summarization using inconsistency loss. In Association for Compu- tational Linguistics (ACL), pages 132\u2013141. Minghao Hu, Yuxing Peng, Furu Wei, Zhen Huang, Dongsheng Li, Nan Yang, and Ming Zhou. 2018. Attention-guided answer distillation for machine reading comprehension. In Empirical Methods in Natural Language Processing (EMNLP), pages 2077\u20132086. Minghao Hu, Furu Wei, Yuxing Peng, Zhen Huang, Nan Yang, and Ming Zhou. 2019. Read + Verify: Machine reading comprehension with unanswerable questions. In Association for the Advancement of Arti\ufb01cial Intelligence (AAAI).",
            "Minghao Hu, Furu Wei, Yuxing Peng, Zhen Huang, Nan Yang, and Ming Zhou. 2019. Read + Verify: Machine reading comprehension with unanswerable questions. In Association for the Advancement of Arti\ufb01cial Intelligence (AAAI). Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P. Xing. 2017. Toward con- trolled generation of text. In International Con- ference on Machine Learning (ICML), pages 1587\u2013 1596. Hakan Inan, Khashayar Khosravi, and Richard Socher. 2017. Tying word vectors and word classi\ufb01ers: A loss framework for language modeling. In Inter- national Conference on Learning Representations (ICLR). Sathish Reddy Indurthi, Seunghak Yu, Seohyun Back, and Heriberto Cuay\u00b4ahuitl. 2018. Cut to the chase: A context zoom-in network for reading comprehen- sion.",
            "Sathish Reddy Indurthi, Seunghak Yu, Seohyun Back, and Heriberto Cuay\u00b4ahuitl. 2018. Cut to the chase: A context zoom-in network for reading comprehen- sion. In Empirical Methods in Natural Language Processing (EMNLP), pages 570\u2013575. Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Tho- rat, Fernanda B. Vi\u00b4egas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2017. Google\u2019s multilingual neural machine translation system: Enabling zero-shot translation. Transac- tions of the Association for Computational Linguis- tic (TACL), 5:339\u2013351. Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale dis- tantly supervised challenge dataset for reading com- prehension. In Association for Computational Lin- guistics (ACL), pages 1601\u20131611.",
            "2017. TriviaQA: A large scale dis- tantly supervised challenge dataset for reading com- prehension. In Association for Computational Lin- guistics (ACL), pages 1601\u20131611. Yuta Kikuchi, Graham Neubig, Ryohei Sasano, Hiroya Takamura, and Manabu Okumura. 2016. Control- ling output length in neural encoder-decoders. In Empirical Methods in Natural Language Processing (EMNLP), pages 1328\u20131338. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR). Tom\u00b4as Kocisk\u00b4y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00b4abor Melis, and Edward Grefenstette. 2018. The NarrativeQA read- ing comprehension challenge. Transactions of the Association for Computational Linguistic (TACL), 6:317\u2013328. Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer.",
            "2018. The NarrativeQA read- ing comprehension challenge. Transactions of the Association for Computational Linguistic (TACL), 6:317\u2013328. Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. 2017. Zero-shot relation extraction via reading comprehension. In Computational Natural Language Learning (CoNLL), pages 333\u2013342. Chenliang Li, Weiran Xu, Si Li, and Sheng Gao. 2018. Guiding generation for abstractive text sum- marization based on key information guide network. In North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies (NAACL-HLT), pages 55\u201360. Ilya Loshchilov and Frank Hutter. 2017. Fixing weight decay regularization in adam. Computing Research Repository (CoRR), arXiv:1711.05101. Version 1. Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2018.",
            "Fixing weight decay regularization in adam. Computing Research Repository (CoRR), arXiv:1711.05101. Version 1. Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2018. The natural language decathlon: Multitask learning as question answer- ing. Computing Research Repository (CoRR), arXiv:1806.08730. Version 1. Sewon Min, Victor Zhong, Richard Socher, and Caim- ing Xiong. 2018. Ef\ufb01cient and robust question an- swering from minimal context over documents. In Association for Computational Linguistics (ACL), pages 1725\u20131735. Preksha Nema, Mitesh M. Khapra, Anirban Laha, and Balaraman Ravindran. 2017. Diversity driven atten- tion model for query-based abstractive summariza- tion. In Association for Computational Linguistics (ACL), pages 1063\u20131072.",
            "2017. Diversity driven atten- tion model for query-based abstractive summariza- tion. In Association for Computational Linguistics (ACL), pages 1063\u20131072. Kyosuke Nishida, Itsumi Saito, Atsushi Otsuka, Hisako Asano, and Junji Tomita. 2018. Retrieve-and- read: Multi-task learning of information retrieval and reading comprehension. In Conference on Information and Knowledge Management (CIKM), pages 647\u2013656.",
            "Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Association for Computational Linguistics (ACL), pages 311\u2013318. Ramakanth Pasunuru and Mohit Bansal. 2018. Multi- -reward reinforced summarization with saliency and entailment. In North American Chapter of the Asso- ciation for Computational Linguistics: Human Lan- guage Technologies (NAACL-HLT), pages 646\u2013653. Jeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Nat- ural Language Processing (EMNLP), pages 1532\u2013 1543. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations.",
            "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. In North American Chapter of the Asso- ciation for Computational Linguistics: Human Lan- guage Technologies (NAACL-HLT), pages 2227\u2013 2237. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing by generative pre-training. Technical re- port, OpenAI. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Techni- cal report, OpenAI. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don\u2019t know: Unanswerable ques- tions for SQuAD. In Association for Computational Linguistics (ACL), pages 784\u2013789.",
            "Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don\u2019t know: Unanswerable ques- tions for SQuAD. In Association for Computational Linguistics (ACL), pages 784\u2013789. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ ques- tions for machine comprehension of text. In Em- pirical Methods in Natural Language Processing (EMNLP), pages 2383\u20132392. Siva Reddy, Danqi Chen, and Christopher D. Manning. 2018. CoQA: A conversational question answering challenge. Computing Research Repository (CoRR), arXiv:1808.07042. Version 1. Amrita Saha, Rahul Aralikatte, Mitesh M. Khapra, and Karthik Sankaranarayanan. 2018. DuoRC: Towards complex language understanding with paraphrased reading comprehension.",
            "Version 1. Amrita Saha, Rahul Aralikatte, Mitesh M. Khapra, and Karthik Sankaranarayanan. 2018. DuoRC: Towards complex language understanding with paraphrased reading comprehension. In Association for Compu- tational Linguistics (ACL), pages 1683\u20131693. Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with point- er-generator networks. In Association for Computa- tional Linguistics (ACL), pages 1073\u20131083. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Controlling politeness in neural machine translation via side constraints. In North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies (NAACL- HLT), pages 35\u201340. Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention \ufb02ow for machine comprehension.",
            "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention \ufb02ow for machine comprehension. In International Conference on Learning Representations (ICLR). Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov. 2014. Dropout: a simple way to prevent neural networks from over\ufb01tting. Journal of Machine Learning Research, 15(1):1929\u20131958. Rupesh Kumar Srivastava, Klaus Greff, and J\u00a8urgen Schmidhuber. 2015. Highway net- works. Computing Research Repository (CoRR), arXiv:1505.00387. Version 2. Fu Sun, Linyang Li, Xipeng Qiu, and Yang Liu. 2018. U-Net: Machine reading comprehension with unan- swerable questions. Computing Research Reposi- tory (CoRR), arXiv:1810.06638.",
            "Fu Sun, Linyang Li, Xipeng Qiu, and Yang Liu. 2018. U-Net: Machine reading comprehension with unan- swerable questions. Computing Research Reposi- tory (CoRR), arXiv:1810.06638. Version 1. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. 2016. Re- thinking the inception architecture for computer vi- sion. In Computer Vision and Pattern Recognition (CVPR), pages 2818\u20132826. Shunsuke Takeno, Masaaki Nagata, and Kazuhide Ya- mamoto. 2017. Controlling target features in neural machine translation via pre\ufb01x constraints. In Work- shop on Asian Translation (WAT@IJCNLP), pages 55\u201363. Chuanqi Tan, Furu Wei, Nan Yang, Bowen Du, Weifeng Lv, and Ming Zhou. 2018. S-Net: From answer extraction to answer synthesis for machine reading comprehension.",
            "Chuanqi Tan, Furu Wei, Nan Yang, Bowen Du, Weifeng Lv, and Ming Zhou. 2018. S-Net: From answer extraction to answer synthesis for machine reading comprehension. In Association for the Ad- vancement of Arti\ufb01cial Intelligence (AAAI), pages 5940\u20135947. Yi Tay, Anh Tuan Luu, Siu Cheung Hui, and Jian Su. 2018. Densely connected attention propagation for reading comprehension. In Advances in Neural Information Processing Systems (NeurIPS), pages 4911\u20134922. Yulia Tsvetkov, Alan W. Black, Ruslan Salakhutdi- nov, and Shrimai Prabhumoye. 2018. Style transfer through back-translation. In Association for Com- putational Linguistics (ACL), pages 866\u2013876. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need.",
            "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems (NIPS), pages 6000\u20136010. Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerald Tesauro, Bowen Zhou, and Jing Jiang. 2018. R3: Reinforced reader-ranker for open-domain question answering. In Association for the Advancement of Arti\ufb01cial Intelligence (AAAI), pages 5981\u20135988.",
            "Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and Ming Zhou. 2017. Gated self-matching net- works for reading comprehension and question an- swering. In Association for Computational Linguis- tics (ACL), pages 189\u2013198. Hua Wu, Haifeng Wang, Sujian Li, Wei He, Yizhong Wang, Jing Liu, Kai Liu, and Yajuan Lyu. 2018. Multi-passage machine reading comprehension with cross-passage answer veri\ufb01cation. In Association for Computational Linguistics (ACL), pages 1918\u2013 1927. Caiming Xiong, Victor Zhong, and Richard Socher. 2017. Dynamic coattention networks for question answering. In International Conference on Learn- ing Representations (ICLR). Ming Yan, Jiangnan Xia, Chen Wu, Bin Bi, Zhongzhou Zhao, Ji Zhang, Luo Si, Rui Wang, Wei Wang, and Haiqing Chen. 2019. A deep cascade model for multi-document reading comprehension.",
            "Ming Yan, Jiangnan Xia, Chen Wu, Bin Bi, Zhongzhou Zhao, Ji Zhang, Luo Si, Rui Wang, Wei Wang, and Haiqing Chen. 2019. A deep cascade model for multi-document reading comprehension. In Associ- ation for the Advancement of Arti\ufb01cial Intelligence (AAAI). Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben- gio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Empirical Methods in Natural Lan- guage Processing (EMNLP), pages 2369\u20132380. Dani Yogatama, Cyprien de Masson d\u2019Autume, Jerome Connor, Tom\u00b4as Kocisk\u00b4y, Mike Chrzanowski, Ling- peng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, and Phil Blunsom. 2019. Learning and evaluating general linguistic intelli- gence. Computing Research Repository (CoRR), arXiv:1901.11373.",
            "2019. Learning and evaluating general linguistic intelli- gence. Computing Research Repository (CoRR), arXiv:1901.11373. Version 1. Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V. Le. 2018. QANet: Combining local convolution with global self-attention for reading comprehen- sion. In International Conference on Learning Rep- resentations (ICLR). A Supplementary Material A.1 Experimental Setup for MS MARCO Model con\ufb01gurations. We trained our model on a machine with eight NVIDIA P100 GPUs. Our best model was jointly trained with the two answer styles in the ALL set for a total of eight epochs with a batch size of 80, where each batch con- sisted of multi-style answers that were randomly sampled. The training took roughly six days. The hidden size d was 304, and the number of atten- tion heads was 8. The inner state size of the feed- forward networks was 256.",
            "The training took roughly six days. The hidden size d was 304, and the number of atten- tion heads was 8. The inner state size of the feed- forward networks was 256. The numbers of shared encoding blocks, modeling blocks for a question, modeling blocks for passages, and decoder blocks were 3, 2, 5, and 8, respectively. We used the pre- trained uncased 300-dimensional GloVe (Penning- ton et al., 2014)3 and the original 512-dimensional ELMo (Peters et al., 2018)4. We used the spaCy tokenizer, and all input words were lowercased ex- cept the input for ELMo. The output words were also lowercase. The number of common words in Vext in the extended vocabulary was 5,000. Each passage and each answer were truncated to 100 words for training. Optimizer. We used Adam (Kingma and Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.999, and \u01eb = 10\u22128.",
            "Each passage and each answer were truncated to 100 words for training. Optimizer. We used Adam (Kingma and Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.999, and \u01eb = 10\u22128. The weights were initialized using N(0, 0.02), ex- cept that the biases of all the linear transformations were initialized with zero vectors. The learning rate was increased linearly from zero to 2.5\u00d710\u22124 in the \ufb01rst 2,000 steps and then annealed to 0 by using a cosine schedule. All parameter gradients were clipped to a maximum norm of 1. An expo- nential moving average was applied to all trainable variables with a decay rate of 0.9995. The balanc- ing factors for joint learning, \u03bbrank and \u03bbcls, were set to 0.5 and 0.1, respectively. Regularization. We used a modi\ufb01ed version of the L2 regularization proposed in (Loshchilov and Hutter, 2017), with w = 0.01 on all non-bias.",
            "Regularization. We used a modi\ufb01ed version of the L2 regularization proposed in (Loshchilov and Hutter, 2017), with w = 0.01 on all non-bias. We additionally used a dropout (Srivastava et al., 2014) rate of 0.3 for all highway networks and residual and scaled dot-product attention opera- tions in the multi-head attention mechanism. We also used one-sided label smoothing (Szegedy et al., 2016) for the passage relevance and answer possibility labels. We smoothed only the positive labels to 0.9. Ensemble model. The ensemble model con- sisted of six training runs with identical archi- tectures and hyperparameters but with different weight initializations. The \ufb01nal answer was de- cided with a weighted majority, where we used the ROUGE-L score for the dev. set as the weight of each model. Evaluation settings. We used the of\ufb01cial eval- uation script. The answers were normalized by making words lowercase.",
            "set as the weight of each model. Evaluation settings. We used the of\ufb01cial eval- uation script. The answers were normalized by making words lowercase. 3https:\/\/nlp.stanford.edu\/projects\/ glove\/ 4https:\/\/allennlp.org\/elmo",
            "A.2 Experimental Setup for NarrativeQA Model con\ufb01gurations. Our best model was jointly trained with the NarrativeQA and MS MARCO NLG datasets for a total of seven epochs with a batch size of 64, where each batch consisted of multi-style answers that were randomly sam- pled. For ef\ufb01cient multi-style learning, each sum- mary in the NarrativeQA dataset was divided into ten passages (size of 130 words) with sentence- level overlaps such that each sentence in the sum- mary was entirely contained in a passage. Each passage from MS MARCO was also truncated to 130 words. The rest of the con\ufb01guration was the same as in the MS MARCO experiments. Evaluation settings. An of\ufb01cial evaluation script is not provided, so we used the evaluation script created by Bauer et al. (2018)5. The answers were normalized by making words lowercase and removing punctuation marks. A.3 Output Examples Generated by Masque Tables 6 and 7 list the generated examples for questions from MS MARCO 2.1 and Narra- tiveQA, respectively.",
            "(2018)5. The answers were normalized by making words lowercase and removing punctuation marks. A.3 Output Examples Generated by Masque Tables 6 and 7 list the generated examples for questions from MS MARCO 2.1 and Narra- tiveQA, respectively. We can see from the exam- ples that our model could control answer styles appropriately for various question and reasoning types. We did \ufb01nd some important errors: style er- rors, yes\/no classi\ufb01cation errors, copy errors with respect to numerical values, grammatical errors, and multi-hop reasoning errors. 5https:\/\/github.com\/yicheng-w\/ CommonSenseMultiHopQA\/",
            "(a) Question: why your body would feel like it is shaking Relevant Passage: Shaking is a symptom in which a person has tremors (shakiness or small back and forth movements) in part or all of his body. Shaking can be due to cold body temperatures, rising fever (such as with infections), neurological problems, medicine effects, drug abuse, etc. ...Read more. Reference Answer (Q&A): Shaking can be due to cold body temperatures, rising fever (such as with infections), neurological problems, medicine effects, drug abuse, etc. Prediction (Q&A): because of cold body temperatures , rising fever , neurological problems , medicine effects , drug abuse . \u2713 Reference Answers (NLG): Shaking can be due to cold body temperatures, rising fever, neurological problems, medicine effects and drug abuse. \/ Body would feel like it is shaking due to cold body temperatures, rising fever, neurological problems, medicine effects, drug abuse. Prediction (NLG): your body would feel like it is shaking because of cold body temperatures , rising fever , neurological problems , medicine effects , drug abuse .",
            "\/ Body would feel like it is shaking due to cold body temperatures, rising fever, neurological problems, medicine effects, drug abuse. Prediction (NLG): your body would feel like it is shaking because of cold body temperatures , rising fever , neurological problems , medicine effects , drug abuse . \u2713 (b) Question: is the name used to refer to the era of legalized segregation in the united states Relevant Passage: Jim Crow law, in U.S. history, any of the laws that enforced racial segregation in the South between the end of Reconstruction in 1877 and the beginning of the civil rights movement in the 1950s. Reference Answer (Q&A): Jim Crow Prediction (Q&A): jim crow \u2713 Reference Answer (NLG): Jim Crow is the name used to refer to the era of legalized segregation in the United States. Prediction (NLG): jim crow is the name used to refer to the era of legalized segregation in the united states . \u2713 (c) Question: average height nba player Relevant Passage: The average height of an NBA player is around 6 feet 7 inches. The tallest NBA player ever was Gheorghe Muresan, who was 7 feet 7 inches tall.",
            "\u2713 (c) Question: average height nba player Relevant Passage: The average height of an NBA player is around 6 feet 7 inches. The tallest NBA player ever was Gheorghe Muresan, who was 7 feet 7 inches tall. In contrast, the shortest NBA player ever was Tyrone Muggsy Bogues, who was 5 feet 3 inches tall. Reference Answer (Q&A): Around 6 feet 7 inches Prediction (Q&A): 6 feet 7 inches \u2713 Reference Answers (NLG): The average height of NBA players is around 6 feet, 7 inches. \/ The height of NBA player is around 6 feet 7 inches. Prediction (NLG): the average height of an national basketball association player is 6 feet 7 inches . \u2713 (d) Question: population of henryetta ok Relevant Passage: With it\u2019s population of 5,927 people, Henryetta represented 14.79% of the county\u2019s total.",
            "\u2713 (d) Question: population of henryetta ok Relevant Passage: With it\u2019s population of 5,927 people, Henryetta represented 14.79% of the county\u2019s total. In addition, where the city of Henryetta has a population density of 891 people per square mile [344.2 people\/km2], Okmulgee County averages 57.5 people per square mile [22.19 people\/km2]. Reference Answer (Q&A): 5927 Prediction (Q&A): the population of henryetta , oklahoma is 5,927 . \u2713(content is OK, but style is NG \u2717) Reference Answer (NLG): The population of Henryetta, Oklahoma is 5,927. Prediction (NLG): the population of henryetta , oklahoma is 5,927 . \u2713 (e) Question: does gameplay programmer need math skill Relevant Passage: A good computer programmer is more of a problem solver and logical thinker than a math buff. And besides, the industry is peppered with many successful computer programmers who do not really know much about mathe- matics.",
            "\u2713 (e) Question: does gameplay programmer need math skill Relevant Passage: A good computer programmer is more of a problem solver and logical thinker than a math buff. And besides, the industry is peppered with many successful computer programmers who do not really know much about mathe- matics. Reference Answer (Q&A): No Prediction (Q&A): yes \u2717 Reference Answers (NLG): No, a gameplay programmer doesn\u2019t need math skill. \/ No, gameplay programmer do not need an math skill. Prediction (NLG): no , gameplay programmer does not need math skill . \u2713 (f) Question: how long does a freezer take to cool down Relevant Passage: Quick Answer. It takes anywhere from three to 24 hours for a refrigerator to reach safe temperatures for storing food, depending on the size and type of unit. When the refrigerator compartment reaches 40 degrees Fahrenheit and the freezer reaches 5 degrees Fahrenheit, it is safe to transfer food items. Keep Learning. Reference Answer (Q&A): 24 hours Prediction (Q&A): 4 to 5 hours \u2717 Reference Answers (NLG): A freezer takes 24 hours to cool down.",
            "Keep Learning. Reference Answer (Q&A): 24 hours Prediction (Q&A): 4 to 5 hours \u2717 Reference Answers (NLG): A freezer takes 24 hours to cool down. \/ A freezer take to cool down is 24 hours. Prediction (NLG): a freezer takes 4 to 12 hours to cool down . \u2717 Table 6: Output examples generated by Masque from MS MARCO. The model was trained with the Q&A and NLG styles. The relevant passage is one that an annotator selected to compose the reference answer. The model could control answer styles appropriately for (a) natural language, (b) cloze-style, and (c) keywords questions. (d) The answer style was incorrect. (e) The answers were not consistent between the styles. (f) Copying from numerical words worked poorly. There were some grammatical errors in the generative answers, which are underlined.",
            "(a) Question: Where does Mark broadcast his radio station? Summary: Mark Hunter (Slater), a high school student in a sleepy suburb of Phoenix, Arizona, starts an FM pirate radio station that broadcasts from the basement of his parents\u2019 house. Mark is a loner, an outsider, whose only outlet for his teenage angst and aggression is his unauthorized radio station. His pirate station\u2019s theme song is \u201dEverybody Knows\u201d by Leonard Cohen and there are glimpses of cassettes by such alternative musicians as The Jesus and Mary Chain, Camper Van Beethoven, Primal Scream, Soundgarden, Ice-T, Bad Brains, Concrete Blonde, Henry Rollins, and The Pixies. By day, Mark is seen as a loner, hardly talking to anyone around him; by night, he expresses his outsider views about what is wrong with American society. When he speaks his mind about what is going on at his school and in the community, more and more of his fellow students tune in to hear his show. (...) Reference Answers: In his parent\u2019s basement. \/ His parents\u2019 basement.",
            "When he speaks his mind about what is going on at his school and in the community, more and more of his fellow students tune in to hear his show. (...) Reference Answers: In his parent\u2019s basement. \/ His parents\u2019 basement. Prediction (NQA): the basement of his parents \u2019 house \u2713 Prediction (NLG): mark broadcast his radio station in the basement of his parents \u2019 house . \u2713 (b) Question: Fletch is a reporter for what newspaper? Summary: Los Angeles Times reporter Irwin \u201dFletch\u201d Fletcher (Chase) is writing an article exposing drug traf\ufb01cking on the beaches of Los Angeles. Posing as an addict during his investigation, he is approached by Boyd Aviation executive vice president Alan Stanwyk (Matheson) who mistakenly assumes Fletch is a junkie. Stanwyk claims to have bone cancer, with only months left to live, and wishes to avoid the pain and suffering. Stanwyk offers $50,000 for Fletch to come to his mansion in a few days time, kill him, and then escape to Rio de Janeiro, staging the murder to look like a burglary.",
            "Stanwyk offers $50,000 for Fletch to come to his mansion in a few days time, kill him, and then escape to Rio de Janeiro, staging the murder to look like a burglary. Fletch, while not completely convinced on the truth of Stanwyk\u2019s story, reluctantly agrees to the plan. Along with his colleague Larry (Davis), he begins investigating Stanwyk instead of completing his drug traf\ufb01cking expos, much to the disapproval of his overbearing editor Frank Walker (Libertini). Disguised as a doctor, Fletch accesses Stanwyk\u2019s \ufb01le at the hospital and learns Stanwyk lied about having cancer. (...) Reference Answers: Los Angeles Times \/ Los Angeles Prediction (NQA): los angeles times \u2713 Prediction (NLG): \ufb02etch is a reporter for los angeles times . \u2713 (c) Question: How long approximately was the voyage from London to Thailand supposed to take? Summary: (...) The story is set twenty-two years earlier, when Marlow was 20.",
            "\u2713 (c) Question: How long approximately was the voyage from London to Thailand supposed to take? Summary: (...) The story is set twenty-two years earlier, when Marlow was 20. With two years of experience, most recently as third mate aboard a crack clipper, Marlow receives a billet as second mate on the barque Judea. The skipper is Captain John Beard, a man of about 60. This is Beard\u2019s \ufb01rst command. The Judea is an old boat, belonging to a man \u201dWilmer, Wilcox or something similar\u201d, suffering from age and disuse in Shadewell basin. The 400-ton ship is commissioned to take 600 tons of coal from England to Thailand. The trip should take approximately 150 days. The ship leaves London loaded with sand ballast and heads north to the Senn river to pick up the cargo of coal. On her way, the Judea suffers from her ballast shifting aside and the crew go below to put things right again. The trip takes 16 days because of inclement weather, and the battered ship must use a tug boat to get into port.",
            "On her way, the Judea suffers from her ballast shifting aside and the crew go below to put things right again. The trip takes 16 days because of inclement weather, and the battered ship must use a tug boat to get into port. The Judea waits a month on the Tyne to be loaded with coal. The night before she ships out she is hit by a steamer, the Miranda or the Melissa. The damage takes another three weeks to repair. Three months after leaving London, the Judea ships off for Bangkok. The Judea travels through the North Sea and Britain. 300 miles west of the Lizard a winter storm, \u2019the famous winter gale of twenty-two years ago\u2019, hits. (...) Reference Answers: Approximately 150 days \/ 150 days Prediction (NQA): 150 days \u2713 Prediction (NLG): the voyage from london to thailand was supposed to take 150 days . \u2713 (d) Question: Why does Jamie start avoiding Landon? Summmary: (...) During these functions, Landon notices Jamie Sullivan, a girl he has known since kindergarten and who has attended many of the same classes as him, and is also the local minister\u2019s daughter.",
            "\u2713 (d) Question: Why does Jamie start avoiding Landon? Summmary: (...) During these functions, Landon notices Jamie Sullivan, a girl he has known since kindergarten and who has attended many of the same classes as him, and is also the local minister\u2019s daughter. Since he\u2019s one of the in-crowd, he has seldom paid any attention to Jamie, who wears modest dresses and owns only one sweater. Jamie is labeled an outsider and a geek. She makes no attempt to wear make-up or otherwise improve her looks or attract attention to herself. Landon has trouble learning his lines for the play. Jamie, who is also in the play, agrees to help him on one condition: Jamie warns Landon not to fall in love with her; he laughs it off and dismisses it as a foolish idea. Landon and Jamie begin practicing together at her house after school. They get to know each other and a spark of affection arises between them. On the opening night of the play, Jamie astounds Landon and the entire audience with her beauty and her voice. Onstage at the peak of the ending to the play, Jamie sings.",
            "They get to know each other and a spark of affection arises between them. On the opening night of the play, Jamie astounds Landon and the entire audience with her beauty and her voice. Onstage at the peak of the ending to the play, Jamie sings. When Jamie \ufb01nishes, Landon improvises and kisses her which is not a part of the play. Afterwards, Jamie avoids Landon, and it is not until Landon\u2019s friends play a cruel prank on Jamie and he protects her in opposition to his friends that she warms up to him again. Landon asks Jamie on a date soon after, but Jamie says her father doesn\u2019t allow her to date. (...) Reference Answers: Because he kissed her in the play. \/ He kisses her Prediction (NQA): he is not a part of the play \u2717 Prediction (NLG): he is not a part of the play \u2717 Table 7: Output examples generated by Masque from NarrativeQA. The model was trained with the NarrativeQA (NQA) and MS MARCO (NLG) styles.",
            "The model was trained with the NarrativeQA (NQA) and MS MARCO (NLG) styles. It could control answer styles appropriately for questions that required (a,b) single-sentence reasoning and (c) multi-sentence reasoning. (d) Example of an error in multi-sentence reasoning. There were some grammatical errors in the generative answers, which are underlined."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1901.02262.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 16259.000091552734,
    "avg_doclen_est": 171.14736938476562
}
