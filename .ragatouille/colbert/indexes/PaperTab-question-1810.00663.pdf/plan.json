{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation Xiaoxue Zang1\u2217, Ashwini Pokle1\u2217, Marynel V\u00b4azquez2, Kevin Chen1, Juan Carlos Niebles1, Alvaro Soto3, Silvio Savarese1 1 Stanford University, 2 Yale University, 3 P. Universidad Cat\u00b4olica de Chile 1{xzang, ashwinipokle, kchen92, jniebles, ssilvio}@stanford.edu, 2marynel.vazquez@yale.edu, 3asoto@ing.puc.cl Abstract We propose an end-to-end deep learning model for translating free-form natural lan- guage instructions to a high-level plan for behavioral robot navigation. The proposed model uses attention mechanisms to connect information from user instructions with a topo- logical representation of the environment. To evaluate this model, we collected a new dataset for the translation problem containing 11,051 pairs of user instructions and navigation plans. Our results show that the proposed model outperforms baseline approaches on the new dataset.",
            "To evaluate this model, we collected a new dataset for the translation problem containing 11,051 pairs of user instructions and navigation plans. Our results show that the proposed model outperforms baseline approaches on the new dataset. Overall, our work suggests that a topological map of the environment can serve as a relevant knowledge base for translating natural language instructions into a sequence of navigation behaviors. 1 Introduction Enabling robots to follow navigation instructions in natural language can facilitate human-robot in- teraction across a variety of applications. For in- stance, within the service robotics domain, robots can follow navigation instructions to help with mobile manipulation (Tellex et al., 2011) and de- livery tasks (Veloso et al., 2015). Interpreting navigation instructions in natural language is dif\ufb01cult due to the high variabil- ity in the way people describe routes (Chen and Mooney, 2011). For example, there are a variety of ways to describe the route in Fig.",
            "Interpreting navigation instructions in natural language is dif\ufb01cult due to the high variabil- ity in the way people describe routes (Chen and Mooney, 2011). For example, there are a variety of ways to describe the route in Fig. 1(a): \u2013 \u201cExit the room, turn right, follow the corri- dor until you pass a vase on your left, and enter the next room on your left\u201d; or \u2013 \u201cTurn right after you exit the room, and enter the room on the left right before the end of the corridor\u201d; or \u2013 \u201cAdvance forward to the right after going out of the door. Enter the room which is in the middle of two vases on your left.\u201d \u2217Both authors contributed equally to this work. Each fragment of a sentence within these instruc- tions can be mapped to one or more than one navi- gation behaviors. For instance, assume that a robot counts with a number of primitive, navigation be- haviors, such as \u201center the room on the left (or on right)\u201d , \u201cfollow the corridor\u201d, \u201ccross the inter- section\u201d, etc.",
            "For instance, assume that a robot counts with a number of primitive, navigation be- haviors, such as \u201center the room on the left (or on right)\u201d , \u201cfollow the corridor\u201d, \u201ccross the inter- section\u201d, etc. Then, the fragment \u201cadvance for- ward\u201d in a navigation instruction could be inter- preted as a \u201cfollow the corridor\u201d behavior, or as a sequence of \u201cfollow the corridor\u201d interspersed with \u201ccross the intersection\u201d behaviors depend- ing on the topology of the environment. Resolving such ambiguities often requires reasoning about \u201ccommon-sense\u201d concepts, as well as interpreting spatial information and landmarks, e.g., in sen- tences such as \u201cthe room on the left right before the end of the corridor\u201d and \u201cthe room which is in the middle of two vases\u201d. In this work, we pose the problem of inter- preting navigation instructions as \ufb01nding a map- ping (or grounding) of the commands into an ex- ecutable navigation plan.",
            "In this work, we pose the problem of inter- preting navigation instructions as \ufb01nding a map- ping (or grounding) of the commands into an ex- ecutable navigation plan. While the plan is typ- ically modeled as a formal speci\ufb01cation of low- level motions (Chen and Mooney, 2011) or a grammar (Artzi and Zettlemoyer, 2013; Matuszek et al., 2010), we focus speci\ufb01cally on translating instructions to a high-level navigation plan based on a topological representation of the environ- ment. This representation is a behavioral navi- gation graph, as recently proposed by (Sep\u00b4ulveda et al., 2018), designed to take advantage of the se- mantic structure typical of human environments. The nodes of the graph correspond to semanti- cally meaningful locations for the navigation task, such as kitchens or entrances to rooms in corri- dors. The edges are parameterized, visuo-motor behaviors that allow a robot to navigate between neighboring nodes, as illustrated in Fig. 1(b).",
            "The nodes of the graph correspond to semanti- cally meaningful locations for the navigation task, such as kitchens or entrances to rooms in corri- dors. The edges are parameterized, visuo-motor behaviors that allow a robot to navigate between neighboring nodes, as illustrated in Fig. 1(b). Un- der this framework, complex navigation routes can be achieved by sequencing behaviors without an explicit metric representation of the world. arXiv:1810.00663v1  [cs.CL]  24 Sep 2018",
            "Figure 1: Map of an environment (a), its (partial) behavioral navigation graph (b), and the problem setting of interest (c). The red part of (b) corresponds to the representation of the route highlighted in blue in (a). The codes \u201coo-left\u201d, \u201coo-right\u201d, \u201ccf\u201d, \u201cleft-io\u201d, and \u201cright-io\u201d correspond to the behaviors \u201cgo out and turn left\u201d, \u201cgo out and turn right\u201d, \u201cfollow the corridor\u201d, \u201center the room on left\u201d, and \u201center of\ufb01ce on right\u201d, respectively. We formulate the problem of following instruc- tions under the framework of (Sep\u00b4ulveda et al., 2018) as \ufb01nding a path in the behavioral naviga- tion graph that follows the desired route, given a known starting location. The edges (behaviors) along this path serve to reach the \u2013 sometimes im- plicit \u2013 destination requested by the user. As in (Zang et al., 2018), our focus is on the problem of interpreting navigation directions. We assume that a robot can realize valid navigation plans accord- ing to the graph.",
            "As in (Zang et al., 2018), our focus is on the problem of interpreting navigation directions. We assume that a robot can realize valid navigation plans accord- ing to the graph. We contribute a new end-to-end model for fol- lowing directions in natural language under the be- havioral navigation framework. Inspired by the information retrieval and question answering lit- erature (Lewis and Jones, 1996; Seo et al., 2017; Xiong et al., 2016; Palangi et al., 2016), we pro- pose to leverage the behavioral graph as a knowl- edge base to facilitate the interpretation of naviga- tion commands. More speci\ufb01cally, the proposed model takes as input user directions in text form, the behavioral graph of the environment encoded as \u27e8node; edge; node\u27e9triplets, and the initial location of the robot in the graph. The model then predicts a set of behaviors to reach the desired des- tination according to the instructions and the map (Fig. 1(c)).",
            "The model then predicts a set of behaviors to reach the desired des- tination according to the instructions and the map (Fig. 1(c)). Our main insight is that using atten- tion mechanisms to correlate navigation instruc- tions with the topological map of the environment can facilitate predicting correct navigation plans. This work also contributes a new dataset of 11, 050 pairs of free-form natural language in- structions and high-level navigation plans. This dataset was collected through Mechanical Turk using 100 simulated environments with a corre- sponding topological map and, to the best of our knowledge, it is the \ufb01rst of its kind for behavioral navigation. The dataset opens up opportunities to explore data-driven methods for grounding navi- gation commands into high-level motion plans. We conduct extensive experiments to study the generalization capabilities of the proposed model for following natural language instructions. We in- vestigate both generalization to new instructions in known and in new environments. We conclude this paper by discussing the bene\ufb01ts of the pro- posed approach as well as opportunities for future research based on our \ufb01ndings.",
            "We in- vestigate both generalization to new instructions in known and in new environments. We conclude this paper by discussing the bene\ufb01ts of the pro- posed approach as well as opportunities for future research based on our \ufb01ndings. 2 Related work This section reviews relevant prior work on fol- lowing navigation instructions. Readers interested in an in-depth review of methods to interpret spa- tial natural language for robotics are encouraged to refer to (Landsiedel et al., 2017). Typical approaches to follow navigation com- mands deal with the complexity of natural lan- guage by manually parsing commands, constrain- ing language descriptions, or using statistical ma- chine translation methods. While manually pars- ing commands is often impractical, the \ufb01rst type of approaches are foundational: they showed that it is possible to leverage the compositionality of semantic units to interpret spatial language (Bug- mann et al., 2004; Levit and Roy, 2007). Constraining language descriptions can reduce the size of the input space to facilitate the inter- pretation of user commands.",
            "Constraining language descriptions can reduce the size of the input space to facilitate the inter- pretation of user commands. For example, (Tal- bot et al., 2016) explored using structured, sym- bolic language phrases for navigation. As in this earlier work, we are also interested in navigation with a topological map of the environment. How- ever, we do not process symbolic phrases. Our aim is to translate free-form natural language instruc-",
            "tions to a navigation plan using information from a high-level representation of the environment. This translation problem requires dealing with missing actions in navigation instructions and actions with preconditions, such as \u201cat the end of the corridor, turn right\u201d (MacMahon et al., 2006). Statistical machine translation (Koehn, 2009) is at the core of recent approaches to enable robots to follow navigation instructions. These meth- ods aim to automatically discover translation rules from a corpus of data, and often leverage the fact that navigation directions are composed of sequen- tial commands. For instance, (Wong and Mooney, 2006; Matuszek et al., 2010; Chen and Mooney, 2011) used statistical machine translation to map instructions to a formal language de\ufb01ned by a grammar. Likewise, (Kollar et al., 2010; Tellex et al., 2011) mapped commands to spatial descrip- tion clauses based on the hierarchical structure of language in the navigation problem. Our ap- proach to machine translation builds on insights from these prior efforts.",
            "Likewise, (Kollar et al., 2010; Tellex et al., 2011) mapped commands to spatial descrip- tion clauses based on the hierarchical structure of language in the navigation problem. Our ap- proach to machine translation builds on insights from these prior efforts. In particular, we focus on end-to-end learning for statistical machine trans- lation due to the recent success of Neural Net- works in Natural Language Processing (Goodfel- low et al., 2016). Our work is inspired by methods that reduce the task of interpreting user commands to a sequential prediction problem (Shimizu and Haas, 2009; Mei et al., 2016; Anderson et al., 2018). Similar to Mei et al. and Anderson et al., we use a sequence-to- sequence model to enable a mobile agent to follow routes. But instead leveraging visual information to output low-level navigation commands, we fo- cus on using a topological map of the environment to output a high-level navigation plan.",
            "Similar to Mei et al. and Anderson et al., we use a sequence-to- sequence model to enable a mobile agent to follow routes. But instead leveraging visual information to output low-level navigation commands, we fo- cus on using a topological map of the environment to output a high-level navigation plan. This plan is a sequence of behaviors that can be executed by a robot to reach a desired destination (Sep\u00b4ulveda et al., 2018; Zang et al., 2018). We explore machine translation from the per- spective of automatic question answering. Follow- ing (Seo et al., 2017; Xiong et al., 2016), our ap- proach uses attention mechanisms to learn align- ments between different input modalities. In our case, the inputs to our model are navigation in- structions, a topological environment map, and the start location of the robot (Fig. 1(c)). Our results show that the map can serve as an effective source of contextual information for the translation task. Additionally, it is possible to leverage this kind of information in an end-to-end fashion.",
            "1(c)). Our results show that the map can serve as an effective source of contextual information for the translation task. Additionally, it is possible to leverage this kind of information in an end-to-end fashion. 3 Problem Formulation Our goal is to translate navigation instructions in text form into a sequence of behaviors that a robot can execute to reach a desired destination from a known start location. We frame this problem un- der a behavioral approach to indoor autonomous navigation (Sep\u00b4ulveda et al., 2018) and assume that prior knowledge about the environment is available for the translation task. This prior knowl- edge is a topological map, in the form of a behav- ioral navigation graph (Fig. 1(b)). The nodes of the graph correspond to semantically-meaningful locations for the navigation task, and its directed edges are visuo-motor behaviors that a robot can use to move between nodes. This formulation takes advantage of the rich semantic structure be- hind man-made environments, resulting in a com- pact route representation for robot navigation. Fig. 1(c) provides a schematic view of the prob- lem setting.",
            "This formulation takes advantage of the rich semantic structure be- hind man-made environments, resulting in a com- pact route representation for robot navigation. Fig. 1(c) provides a schematic view of the prob- lem setting. The inputs are: (1) a navigation graph m, (2) the starting node s of the robot in m, and (3) a set of free-form navigation instructions I in natural language. The instructions describe a path in the graph to reach from s to a \u2013 potentially im- plicit \u2013 destination node g. Using this informa- tion, the objective is to predict a suitable sequence of robot behaviors b1, . . . , bT to navigate from s to g according to I. From a supervised learning perspective, the goal is then to estimate: argmax b1,...,bT P(b1, . . . , bT |m, s, I) (1) based on a dataset of input-target pairs {(xi, yi) | 0 \u2264i \u2264N}, where xi = (m, s, I)i and yi = (b1, . . . , bT )i, respectively.",
            ". . , bT )i, respectively. The sequen- tial execution of the behaviors b1, . . . , bT should replicate the route intended by the instructions I. We assume no prior linguistic knowledge. Thus, translation approaches have to cope with the semantics and syntax of the language by discover- ing corresponding patterns in the data. 3.1 The Behavioral Graph: A Knowledge Base For Navigation We view the behavioral graph m as a knowledge base that encodes a set of navigational rules as triplets \u27e8pi; bl[attr]; pj\u27e9, where pi and pj are ad- jacent nodes in the graph, and the edge bl is an executable behavior to navigate from pi to pj. In general, each behaviors includes a list of relevant navigational attributes attr that the robot might encounter when moving between nodes.",
            "Behavior Description oo<d> Go out of the current place and turn <d> io<d> Turn <d> and enter the place straight ahead oio Exit current place and enter straight ahead <d>t Turn <d> at the intersection cf Follow (or go straight down) the corridor sp Go straight at a T intersection ch<d> Cross the hall and turn <d> Table 1: Behaviors (edges) of the navigation graphs consid- ered in this work. The direction <d> can be left or right. We consider 7 types of semantic locations, 11 types of behaviors, and 20 different types of land- marks. A location in the navigation graph can be a room, a lab, an of\ufb01ce, a kitchen, a hall, a corri- dor, or a bathroom. These places are labeled with unique tags, such as \u201droom-1\u201d or \u201dlab-2\u201d, except for bathrooms and kitchens which people do not typically refer to by unique names when describ- ing navigation routes. Table 1 lists the navigation behaviors that we consider in this work.",
            "These places are labeled with unique tags, such as \u201droom-1\u201d or \u201dlab-2\u201d, except for bathrooms and kitchens which people do not typically refer to by unique names when describ- ing navigation routes. Table 1 lists the navigation behaviors that we consider in this work. These behaviors can be de- scribed in reference to visual landmarks or objects, such as paintings, book shelfs, tables, etc. As in Fig. 1, maps might contain multiple landmarks of the same type. Please see the supplementary ma- terial (Appendix A) for more details. 4 Approach We leverage recent advances in deep learning to translate natural language instructions to a sequence of navigation behaviors in an end-to- end fashion. Our proposed model builds on the sequence-to-sequence translation model of (Bah- danau et al., 2015), which computes a soft- alignment between a source sequence (natural lan- guage instructions in our case) and the correspond- ing target sequence (navigation behaviors). As one of our main contributions, we augment the neural machine translation approach of Bah- danau et al.",
            "As one of our main contributions, we augment the neural machine translation approach of Bah- danau et al. to take as input not only natural lan- guage instructions, but also the corresponding be- havioral navigation graph m of the environment where navigation should take place. Speci\ufb01cally, at each step, the graph m operates as a knowl- edge base that the model can access to obtain in- formation about path connectivity, facilitating the grounding of navigation commands. Figure 2 shows the structure of the proposed model for interpreting navigation instructions. The model consists of six layers: Embed layer: The model \ufb01rst encodes each word and symbol in the input sequences I and m into \ufb01xed-length representations. The instruc- tions I are embedded into a 100-dimensional pre- trained GloVe vector (Pennington et al., 2014). Each of the triplet components, pi, bl[attr], and pj of the graph m, are one-hot encoded into vectors of dimensionality 2N +E, where N and E are the number of nodes and edges in m, respectively.",
            "Each of the triplet components, pi, bl[attr], and pj of the graph m, are one-hot encoded into vectors of dimensionality 2N +E, where N and E are the number of nodes and edges in m, respectively. Encoder layer: The model then uses two bidi- rectional Gated Recurrent Units (GRUs) (Cho et al., 2014) to independently process the infor- mation from I and m, and incorporate contextual cues from the surrounding embeddings in each se- quence. The outputs of the encoder layer are the matrix \u00afI \u2208RT\u00d72H for the navigational commands and the matrix \u00afG \u2208RL\u00d72H for the behavioral graph, where H is the hidden size of each GRU, T is the number of words in the instruction I, and L is the number of triplets in the graph m. Attention layer: Matrices \u00afI and \u00afG generated by the encoder layer are combined using an at- tention mechanism. We use one-way attention because the graph contains information about the whole environment, while the instruction has (po- tentially incomplete) local information about the route of interest.",
            "We use one-way attention because the graph contains information about the whole environment, while the instruction has (po- tentially incomplete) local information about the route of interest. The use of attention provides our model with a two-step strategy to interpret commands. This resembles the way people \ufb01nd paths on a map: \ufb01rst, relevant parts on the map are selected according to their af\ufb01nity to each of the words in the input instruction (attention layer); second, the selected parts are connected to assem- ble a valid path (decoder layer). More formally, let \u00afGi (i \u2208[1, L]) be the i-th row of \u00afG, and \u00afIj (j \u2208[1, T]) the j-th row of \u00afI. We use each en- coded triplet \u00afGi in \u00afG to calculate its associated attention distribution ai \u2208RT over all the atomic instructions \u00afIj: ei = [ \u00afGiW \u00afI\u22ba 1, . . .",
            "We use each en- coded triplet \u00afGi in \u00afG to calculate its associated attention distribution ai \u2208RT over all the atomic instructions \u00afIj: ei = [ \u00afGiW \u00afI\u22ba 1, . . . , \u00afGiW \u00afI\u22ba T ] (2) ai = softmax(ei) (3) where the matrix W \u2208R2H\u00d72H serves to com- bine the different sources of information \u00afG and \u00afI. Each component aij of the attention distributions ai quanti\ufb01es the af\ufb01nity between the i-th triplet in \u00afG and the j-th word in the corresponding input I. The model then uses each attention distribution ai to obtain a weighted sum of the encodings of the words in \u00afI, according to their relevance to the corresponding triplet \u00afGi. This results in L atten- tion vectors Ri \u2208R2H, Ri = PT j=1 aijIj. The \ufb01nal step in the attention layer concate- nates each Ri with \u00afGi to generate the outputs",
            "Figure 2: Model overview. The model contains six layers, takes the input of behavioral graph representation, free-form instruction, and the start location (yellow block marked as START in the decoder layer) and outputs a sequence of behaviors. Fi = [Ri; \u00afGi], i \u2208[1, L]. Following (Seo et al., 2017), we include the encoded triplet \u00afGi in the output tensor Fi of this layer to prevent early sum- maries of relevant map information. FC layer: The model reduces the dimension- ality of each individual vector Fi from 4H to H with a fully-connected (FC) layer. The resulting L vectors are output to the next layer as columns of a context matrix C \u2208RH\u00d7L. Decoder layer: After the FC layer, the model predicts likelihoods over the sequence of behav- iors that correspond to the input instructions with a GRU network. Without loss of generality, con- sider the t-th recurrent cell in the GRU network. This cell takes two inputs: a hidden state vector ht\u22121 from the prior cell, and a one-hot embedding of the previous behavior bt\u22121 that was predicted by the model.",
            "Without loss of generality, con- sider the t-th recurrent cell in the GRU network. This cell takes two inputs: a hidden state vector ht\u22121 from the prior cell, and a one-hot embedding of the previous behavior bt\u22121 that was predicted by the model. Based on these inputs, the GRU cell outputs a new hidden state ht to compute likeli- hoods for the next behavior. These likelihoods are estimated by combining the output state ht with relevant information from the context C: \u02c6dts = v\u22ba a tanh(W1ht + W2Cs) (4) dt = softmax( \u02c6dt1, . . . , \u02c6dtL) (5) where W1, W2, and va are trainable parameters. The attention vector dt \u2208RL in Eq. (5) quanti- \ufb01es the af\ufb01nity of ht with respect to each of the columns Cs of C, where s \u2208[1, L].",
            "The attention vector dt \u2208RL in Eq. (5) quanti- \ufb01es the af\ufb01nity of ht with respect to each of the columns Cs of C, where s \u2208[1, L]. The attention vector also helps to estimate a dynamic contextual vector St = PL s=1 dtsCs that the t-th GRU cell uses to compute logits for the next behavior: ot = W3[St; ht] (6) with W3 trainable parameters. Note that ot in- cludes a value for each of the pre-de\ufb01ned behav- iors in the graph m, as well as for a special \u201cstop\u201d symbol to identify the end of the output sequence. Output layer: The \ufb01nal layer of the model searches for a valid sequence of robot behaviors based on the robot\u2019s initial node, the connectivity of the graph m, and the output logits from the pre- vious decoder layer. Again, without loss of gen- erality, consider the t-th behavior bt that is \ufb01nally predicted by the model.",
            "Again, without loss of gen- erality, consider the t-th behavior bt that is \ufb01nally predicted by the model. The search for this behav- ior is implemented as: bt = argmax(softmax(ot + mask(m, nt))) (7) with mask(m, nt) a masking function that takes as input the graph m and the node nt that the robot reaches after following the sequence of behaviors b1, . . . , bt\u22121 previously predicted by the model. The mask function returns a vector of the same dimensionality as the logits ot, but with zeros for the valid behaviors after the last location nt and for the special stop symbol, and \u2212inf for any in- valid predictions according to the connectivity of the behavioral navigation graph. 5 Dataset We created a new dataset for the problem of fol- lowing navigation instructions under the behav- ioral navigation framework of (Sep\u00b4ulveda et al., 2018).1 This dataset was created using Amazon Mechanical Turk and 100 maps of simulated in- door environments, each with 6 to 65 rooms.",
            "To the best of our knowledge, this is the \ufb01rst bench- 1The dataset is publicly available through the website: follow-nav-directions.stanford.edu.",
            "Dataset # Single # Double Total Training 4062 2002 8066 Test-Repeated 944 34 1012 Test-New 962 0 962 Table 2: Dataset statistics. \u201c# Single\u201d indicates the number of navigation plans with a single natural language instruction. \u201c# Double\u201d is the number of plans with two different instruc- tions. The total number of plans is (# Single) \u00d7 2(# Double). mark for comparing translation models in the con- text of behavioral robot navigation. As shown in Table 2, the dataset consists of 8066 pairs of free-form natural language instruc- tions and navigation plans for training. This train- ing data was collected from 88 unique simulated environments, totaling 6064 distinct navigation plans (2002 plans have two different navigation instructions each; the rest has one). The dataset contains two test set variants: 1) Test-Repeated: Contains 1012 pairs of instruc- tions and navigation plans. These routes are not part of the training set; however, they are collected using environments that are part of the training set. 2) Test-New: Contains 962 pairs of instructions and navigation plans.",
            "These routes are not part of the training set; however, they are collected using environments that are part of the training set. 2) Test-New: Contains 962 pairs of instructions and navigation plans. This test set is more chal- lenging than the Test-Repeated dataset because it contains new routes on 12 new indoor environ- ments not included in the training set. While the dataset was collected with simulated en- vironments, no structure was imposed on the nav- igation instructions while crowd-sourcing data. Thus, many instructions in our dataset are am- biguous. Moreover, the order of the behaviors in the instructions is not always the same. For in- stance, a person said \u201cturn right and advance\u201d to describe part of a route, while another person said \u201cgo straight after turning right\u201d in a similar sit- uation. The high variability present in the natu- ral language descriptions of our dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collec- tion effort.",
            "The high variability present in the natu- ral language descriptions of our dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collec- tion effort. 6 Experiments This section describes our evaluation of the pro- posed approach for interpreting navigation com- mands in natural language. We provide both quan- titative and qualitative results. 6.1 Evaluation Metrics While computing evaluation metrics, we only con- sider the behaviors present in the route because they are suf\ufb01cient to recover the high-level navi- gation plan from the graph. Our metrics treat each behavior as a single token. For example, the sam- ple plan \u201cR-1 oor C-1 cf C-1 lt C-0 cf C-0 iol O-3\u201d is considered to have 5 tokens, each correspond- ing to one of its behaviors (\u201coor\u201d, \u201ccf\u201d, \u201clt\u201d, \u201ccf\u201d, \u201ciol\u201d). In this plan, \u201cR-1\u201d,\u201cC-1\u201d, \u201cC-0\u201d, and \u201cO- 3\u201d are symbols for locations (nodes) in the graph.",
            "In this plan, \u201cR-1\u201d,\u201cC-1\u201d, \u201cC-0\u201d, and \u201cO- 3\u201d are symbols for locations (nodes) in the graph. We compare the performance of translation ap- proaches based on four metrics: - Exact Match (EM). As in (Shimizu and Haas, 2009), EM is 1 if a predicted plan matches exactly the ground truth; otherwise it is 0. - F1 score (F1). The harmonic average of the pre- cision and recall over all the test set (Chinchor and Sundheim, 1993). - Edit Distance (ED). The minimum number of insertions, deletions or swap operations required to transform a predicted sequence of behaviors into the ground truth sequence (Navarro, 2001). - Goal Match (GM). GM is 1 if a predicted plan reaches the ground truth destination (even if the full sequence of behaviors does not match exactly the ground truth). Otherwise, GM is 0. 6.2 Models Used in the Evaluation We compare the proposed approach for translat- ing natural language instructions into a navigation plan against alternative deep-learning models: Baseline model.",
            "Otherwise, GM is 0. 6.2 Models Used in the Evaluation We compare the proposed approach for translat- ing natural language instructions into a navigation plan against alternative deep-learning models: Baseline model. The baseline approach is based on (Shimizu and Haas, 2009). It divides the task of interpreting commands for behavioral naviga- tion into two steps: path generation, and path ver- i\ufb01cation. For path generation, this baseline uses a standard sequence-to-sequence model augmented with an attention mechanism, similar to (Bah- danau et al., 2015; Zang et al., 2018). For path veri\ufb01cation, the baseline uses depth-\ufb01rst search to \ufb01nd a route in the graph that matches the sequence of predicted behaviors. If no route matches per- fectly, the baseline changes up to three behaviors in the predicted sequence to try to turn it into a valid path. Ablation model. To test the impact of using the behavioral graphs as an extra input to our trans- lation model, we implemented a version of our",
            "approach that only takes natural language instruc- tions as input. In this ablation model, the output of the bidirectional GRU that encodes the input in- struction I is directly fed to the decoder layer. This model does not have the attention and FC layers described in Sec. 4, nor uses the masking function in the output layer. Ablation with mask model. This model is the same as the previous Ablation model, but with the masking function in the output layer. 6.3 Implementation Details We pre-processed the inputs to the various models that are considered in our experiment. In partic- ular, we lowercased, tokenized, spell-checked and lemmatized the input instructions in text-form us- ing WordNet (Miller, 1995). We also truncated the graphs to a maximum of 300 triplets, and the navi- gational instructions to a maximum of 150 words.",
            "We also truncated the graphs to a maximum of 300 triplets, and the navi- gational instructions to a maximum of 150 words. Only 6.4% (5.4%) of the unique graphs in the training (validation) set had more than 300 triplets, and less than 0.15% of the natural language in- structions in these sets had more than 150 tokens. The dimensionality of the hidden state of the GRU networks was set to 128 in all the experi- ments. In general, we used 12.5% of the train- ing set as validation for choosing models\u2019 hyper- parameters. In particular, we used dropout after the encoder and the fully-connected layers of the proposed model to reduce over\ufb01tting. Best perfor- mance was achieved with a dropout rate of 0.5 and batch size equal to 256. We also used scheduled sampling (Bengio et al., 2015) at training time for all models except the baseline.",
            "Best perfor- mance was achieved with a dropout rate of 0.5 and batch size equal to 256. We also used scheduled sampling (Bengio et al., 2015) at training time for all models except the baseline. We input the triplets from the graph to our pro- posed model in alphabetical order, and consider a modi\ufb01cation where the triplets that surround the start location of the robot are provided \ufb01rst in the input graph sequence. We hypothesized that such rearrangement would help identify the starting lo- cation (node) of the robot in the graph. In turn, this could facilitate the prediction of correct output se- quences. In the remaining of the paper, we refer to models that were provided a rearranged graph, beginning with the starting location of the robot, as models with \u201cOrdered Triplets\u201d. 6.4 Quantitative Evaluation Table 3 shows the performance of the models con- sidered in our evaluation on both test sets. The next two sections discuss the results in detail.",
            "6.4 Quantitative Evaluation Table 3 shows the performance of the models con- sidered in our evaluation on both test sets. The next two sections discuss the results in detail. 6.4.1 Performance in the Test-Repeated Set First, we can observe that the \ufb01nal model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model in- creasing accuracy by 35% and 25% in comparison to the Baseline and Ablation models, respectively. These results suggest that providing the behavioral navigation graph to the model and allowing it to process this information as a knowledge base in an end-to-end fashion is bene\ufb01cial. We can also observe from Table 3 that the mask- ing function of Eq. (7) tends to increase perfor- mance in the Test-Repeated Set by constraining the output sequence to a valid set of navigation be- haviors. For the Ablation model, using the mask- ing function leads to about 10% increase in EM and GM accuracy.",
            "(7) tends to increase perfor- mance in the Test-Repeated Set by constraining the output sequence to a valid set of navigation be- haviors. For the Ablation model, using the mask- ing function leads to about 10% increase in EM and GM accuracy. For the proposed model (with or without reordering the graph triplets), the in- crease in accuracy is around 4%. Note that the impact of the masking function is less evident in terms of the F1 score because this metric considers if a predicted behavior exists in the ground truth navigation plan, irrespective of its speci\ufb01c posi- tion in the output sequence. The results in the last four rows of Table 3 sug- gest that ordering the graph triplets can facilitate predicting correct navigation plans in previously seen environments. Providing the triplets that sur- round the starting location of the robot \ufb01rst to the model leads to a boost of 4% in EM and GM per- formance. The rearrangement of the graph triplets also helps to reduce ED and increase F1.",
            "Providing the triplets that sur- round the starting location of the robot \ufb01rst to the model leads to a boost of 4% in EM and GM per- formance. The rearrangement of the graph triplets also helps to reduce ED and increase F1. Lastly, it is worth noting that our proposed model (last row of Table 3) outperforms all other models in previously seen environments. In partic- ular, we obtain over 4% increase in EM and GM between our model and the next best two models. 6.4.2 Performance in the Test-New Set The previous section evaluated model perfor- mance on new instructions (and corresponding navigation plans) for environments that were pre- viously seen at training time. Here, we examine whether the trained models succeed on environ- ments that are completely new. The evaluation on the Test-New Set helps un- derstand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous sec-",
            "Model Test-Repeated Set Test-New Set EM \u2191 F1 \u2191 ED \u2193 GM \u2191 EM \u2191 F1\u2191 ED \u2193 GM \u2191 Baseline 25.30 79.83 2.53 26.28 25.44 81.38 2.39 25.44 Ablation 36.36 90.28 1.36 36.36 24.82 88.65 1.71 24.92 Ablation with Mask 45.95 90.08 1.20 46.05 36.45 88.31 1.45 36.56 Ours without Mask 52.47 91.74 0.95 53.95 21.94 87.50 1.78 22.65 Ours with Mask 57.31 91.91 0.91 57.31 38.52 88.98 1.32 38.52 Ours without Mask and with Ordered Triplets 57.21 93.37 0.79 57.71 33.36 91.02 1.37 33.78 Ours with Mask and Ordered Triplets 61.",
            "98 1.32 38.52 Ours without Mask and with Ordered Triplets 57.21 93.37 0.79 57.71 33.36 91.02 1.37 33.78 Ours with Mask and Ordered Triplets 61.17 93.54 0.75 61.36 41.71 90.22 1.22 41.81 Table 3: Performance of different models on the test datasets. EM and GM report percentages, and ED corresponds to average edit distance. The symbol \u2191indicates that higher results are better in the corresponding column; \u2193indicates that lower is better. tion, as can be seen in performance drops in Ta- ble 3 for the new environments. Nonetheless, the insights from the previous section still hold: mask- ing in the output layer and reordering the graph triplets tend to increase performance. Even though the results in Table 3 suggest that there is room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environ- ments.",
            "Even though the results in Table 3 suggest that there is room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environ- ments. For instance, the difference between our model and the second best model in the Test-New set is about 3% EM and GM. Note that the average number of actions in the ground truth output se- quences is 7.07 for the Test-New set. Our model\u2019s predictions are just 1.22 edits off on average from the correct navigation plans. 6.5 Qualitative Evaluation This section discusses qualitative results to better understand how the proposed model uses the nav- igation graph. 6.5.1 Attention Visualization We analyze the evolution of the attention weights dt in Eq. (5) to assess if the decoder layer of the proposed model is attending to the correct parts of the behavioral graph when making predictions. Fig 3(b) shows an example of the resulting atten- tion map for the case of a correct prediction. In the Figure, the attention map is depicted as a scaled and normalized 2D array of color codes.",
            "Fig 3(b) shows an example of the resulting atten- tion map for the case of a correct prediction. In the Figure, the attention map is depicted as a scaled and normalized 2D array of color codes. Each col- umn in the array shows the attention distribution dt used to generate the predicted output at step t. Consequently, each row in the array represents a triplet in the corresponding behavioral graph. This graph consists of 72 triplets for Fig 3(b). We observe a locality effect associated to the attention coef\ufb01cients corresponding to high val- ues (bright areas) in each column of Fig 3(b). This suggests that the decoder is paying atten- tion to graph triplets associated to particular neigh- borhoods of the environment in each prediction Figure 3: Visualization of the attention weights of the de- coder layer. The color-coded and numbered regions on the map (left) correspond to the triplets that are highlighted with the corresponding color in the attention map (right). step. We include additional attention visualiza- tions in the supplementary Appendix, including cases where the dynamics of the attention distri- bution are harder to interpret.",
            "step. We include additional attention visualiza- tions in the supplementary Appendix, including cases where the dynamics of the attention distri- bution are harder to interpret. 6.5.2 Experiments with Sub-Optimal Paths All the routes in our dataset are the shortest paths from a start location to a given destination. Thus, we collected a few additional natural lan- guage instructions to check if our model was able to follow navigation instructions describing sub- optimal paths. One such example is shown in Fig. 4, where the blue route (shortest path) and the red route (alternative path) are described by: \u2013 Blue route: \u201cGo out the of\ufb01ce and make a left. Turn right at the corner and go down the hall. Make a right at the next corner and enter the kitchen in front of table.\u201d \u2013 Red route: \u201cExit the room 0 and turn right, go to the end of the corridor and turn left, go straight to the end of the corridor and turn left again.",
            "Make a right at the next corner and enter the kitchen in front of table.\u201d \u2013 Red route: \u201cExit the room 0 and turn right, go to the end of the corridor and turn left, go straight to the end of the corridor and turn left again. After passing bookshelf on your left and table on your right, Enter the kitchen on your right.\u201d For both routes, the proposed model was able to predict the correct sequence of navigation be- haviors. This result suggests that the model is in- deed using the input instructions and is not just ap- proximating shortest paths in the behavioral graph.",
            "Figure 4: An example of two different navigation paths be- tween the same pair of start and goal locations. Other examples on the prediction of sub-obtimal paths are described in the Appendix. 7 Conclusion This work introduced behavioral navigation through free-form natural language instructions as a challenging and a novel task that falls at the intersection of natural language processing and robotics. This problem has a range of interesting cross-domain applications, including information retrieval. We proposed an end-to-end system to trans- late user instructions to a high-level navigation plan. Our model utilized an attention mechanism to merge relevant information from the navigation instructions with a behavioral graph of the envi- ronment. The model then used a decoder to predict a sequence of navigation behaviors that matched the input commands. As part of this effort, we contributed a new dataset of 11,051 pairs of user instructions and navigation plans from 100 different environments. Our model achieved the best performance in this dataset in comparison to a two-step baseline ap- proach for interpreting navigation instructions, and a sequence-to-sequence model that does not consider the behavioral graph.",
            "Our model achieved the best performance in this dataset in comparison to a two-step baseline ap- proach for interpreting navigation instructions, and a sequence-to-sequence model that does not consider the behavioral graph. Our quantitative and qualitative results suggest that attention mech- anisms can help leverage the behavioral graph as a relevant knowledge base to facilitate the trans- lation of free-form navigation instructions. Over- all, our approach demonstrated practical form of learning for a complex and useful task. In future work, we are interested in investigat- ing mechanisms to improve generalization to new environments. For example, pointer and graph networks (Vinyals et al., 2015; Defferrard et al., 2016) are a promising direction to help supervise translation models and predict motion behaviors. Acknowledgments The Toyota Research Institute (TRI) provided funds to assist with this research, but this paper solely re\ufb02ects the opinions and conclusions of its authors and not TRI or any other Toyota entity. This work is also partially funded by Fondecyt grant 1181739, Conicyt, Chile.",
            "This work is also partially funded by Fondecyt grant 1181739, Conicyt, Chile. The authors would also like to thank Gabriel Sep\u00b4ulveda for his assis- tance with parts of this project. References Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S\u00a8underhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. 2018. Vision- and-language navigation: Interpreting visually- grounded navigation instructions in real environ- ments. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su- pervised learning of semantic parsers for mapping instructions to actions. Transactions of the Associa- tion of Computational Linguistics (TACL), 1:49\u201362. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate. In International Con- ference on Learning Representations (ICLR).",
            "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate. In International Con- ference on Learning Representations (ICLR). Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled sampling for se- quence prediction with recurrent neural networks. In Advances in Neural Information Processing Sys- tems, pages 1171\u20131179. Guido Bugmann, Ewan Klein, Stanislao Lauria, and Theocharis Kyriacou. 2004. Corpus-based robotics: A route instruction example. In Proceedings of In- telligent Autonomous Systems (IAS), pages 96\u2013103. Citeseer. David L. Chen and Raymond J. Mooney. 2011. Learn- ing to interpret natural language navigation instruc- tions from observations. In AAAI Conference on Ar- ti\ufb01cial Intelligence, pages 859\u2013865. Nancy Chinchor and Beth Sundheim. 1993.",
            "2011. Learn- ing to interpret natural language navigation instruc- tions from observations. In AAAI Conference on Ar- ti\ufb01cial Intelligence, pages 859\u2013865. Nancy Chinchor and Beth Sundheim. 1993. Muc-5 evaluation metrics. In Proceedings of the 5th confer- ence on Message understanding, pages 69\u201378. As- sociation for Computational Linguistics. Kyunghyun Cho, Bart van Merrienboer, aglar G\u00a8ulehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder",
            "for statistical machine translation. In Empiri- cal Methods in Natural Language Processing (EMNLP). Micha\u00a8el Defferrard, Xavier Bresson, and Pierre Van- dergheynst. 2016. Convolutional neural networks on graphs with fast localized spectral \ufb01ltering. CoRR, abs\/1606.09375. Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press. http:\/\/www. deeplearningbook.org. P. Koehn. 2009. Statistical Machine Translation. Cam- bridge University Press. T. Kollar, S. Tellex, D. Roy, and N. Roy. 2010. To- ward understanding natural language directions. In ACM\/IEEE International Conference on Human- Robot Interaction (HRI), pages 259\u2013266. Christian Landsiedel, Verena Rieser, Matthew Walter, and Dirk Wollherr. 2017. A review of spatial rea- soning and interaction for real-world robotics.",
            "Christian Landsiedel, Verena Rieser, Matthew Walter, and Dirk Wollherr. 2017. A review of spatial rea- soning and interaction for real-world robotics. Ad- vanced Robotics, 31(5):222\u2013242. Michael Levit and Deb Roy. 2007. Interpretation of spatial language in a map navigation task. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 37(3):667\u2013679. David D Lewis and Karen Sp\u00a8arck Jones. 1996. Natural language processing for information retrieval. Com- munications of the ACM, 39(1):92\u2013101. Matt MacMahon, Brian Stankiewicz, and Benjamin Kuipers. 2006. Walk the talk: Connecting language, knowledge, and action in route instructions. In Na- tional Conference on Arti\ufb01cial Intelligence (AAAI). C. Matuszek, D. Fox, and K. Koscher. 2010. Follow- ing directions using statistical machine translation.",
            "In Na- tional Conference on Arti\ufb01cial Intelligence (AAAI). C. Matuszek, D. Fox, and K. Koscher. 2010. Follow- ing directions using statistical machine translation. In ACM\/IEEE International Conference on Human- Robot Interaction (HRI), pages 251\u2013258. Hongyuan Mei, Mohit Bansal, and Matthew R. Walter. 2016. Listen, attend, and walk: Neural mapping of navigational instructions to action sequences. In Na- tional Conference on Arti\ufb01cial Intelligence (AAAI), pages 2772\u20132778. George A. Miller. 1995. Wordnet: A lexical database for english. Communications of the ACM. Gonzalo Navarro. 2001. A guided tour to approximate string matching. ACM computing surveys (CSUR), 33(1):31\u201388. Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen, Xinying Song, and Rabab Ward. 2016.",
            "ACM computing surveys (CSUR), 33(1):31\u201388. Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen, Xinying Song, and Rabab Ward. 2016. Deep sentence embedding using long short-term memory networks: Analysis and ap- plication to information retrieval. IEEE\/ACM Trans- actions on Audio, Speech, and Language Process- ing. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the conference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543. Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention \ufb02ow for machine comprehension. In International Conference on Learning Representations ICLR. G. Sep\u00b4ulveda, JC. Niebles, and A. Soto. 2018. A deep learning based behavioral approach to indoor au- tonomous navigation.",
            "Bidirectional attention \ufb02ow for machine comprehension. In International Conference on Learning Representations ICLR. G. Sep\u00b4ulveda, JC. Niebles, and A. Soto. 2018. A deep learning based behavioral approach to indoor au- tonomous navigation. In International Conference on Learning Representations (ICRA). Nobuyuki Shimizu and Andrew R. Haas. 2009. Learn- ing to follow navigational route instructions. In In- ternational Joint Conferences on Arti\ufb01cial Intelli- gence (IJCAI). Ben Talbot, Obadiah Lam, Ruth Schulz, Feras Dayoub, Ben Upcroft, and Gordon Wyeth. 2016. Find my of- \ufb01ce: Navigating real space from semantic descrip- tions. IEEE International Conference on Robotics and Automation (ICRA), pages 5782\u20135787. Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew R Walter, Ashis Gopal Banerjee, Seth J Teller, and Nicholas Roy. 2011.",
            "IEEE International Conference on Robotics and Automation (ICRA), pages 5782\u20135787. Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew R Walter, Ashis Gopal Banerjee, Seth J Teller, and Nicholas Roy. 2011. Understanding nat- ural language commands for robotic navigation and mobile manipulation. In National Conference on Arti\ufb01cial Intelligence (AAAI), volume 1, page 2. Manuela M Veloso, Joydeep Biswas, Brian Coltin, and Stephanie Rosenthal. 2015. Cobots: Robust sym- biotic autonomous mobile service robots. In Inter- national Joint Conferences on Arti\ufb01cial Intelligence (IJCAI), page 4423. Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2692\u20132700. Yuk Wah Wong and Raymond J Mooney.",
            "In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2692\u20132700. Yuk Wah Wong and Raymond J Mooney. 2006. Learn- ing for semantic parsing with statistical machine translation. In Proc. of the main conference on Hu- man Language Technology Conference of the North American Chapter of the Association of Computa- tional Linguistics, pages 439\u2013446. Caiming Xiong, Stephen Merity, and Richard Socher. 2016. Dynamic memory networks for visual and textual question answering. In International Con- ference on Machine Learning (ICML), pages 2397\u2013 2406. Xiaoxue Zang, Marynel V\u00b4azquez, Juan Carlos Niebles, Alvaro Soto, and Silvio Savarese. 2018. Behavioral indoor navigation with natural language directions. In ACM\/IEEE International Conference on Human- Robot Interaction (HRI), pages 283\u2013284."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1810.00663.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 10335.000366210938,
    "avg_doclen_est": 181.3157958984375
}
