[
  "Named Entity Recognition for Nepali Language Oyesh Mann Singh, Ankur Padia and Anupam Joshi University of Maryland, Baltimore County Baltimore, MD, USA {osingh1, pankur1, joshi}@umbc.edu Abstract Named Entity Recognition have been stud- ied for different languages like English, Ger- man, Spanish and many others but no study have focused on Nepali language. In this paper we propose a neural based Nepali NER using latest state-of-the-art architecture based on grapheme-level which doesn\u2019t re- quire any hand-crafted features and no data pre-processing. Our novel neural based model gained relative improvement of 33% to 50% compared to feature based SVM model and up to 10% improvement over state-of-the-art neu- ral based models developed for languages be- side Nepali. 1 Introduction Named Entity Recognition (NER) is a fore- most NLP task to label each atomic elements of a sentence into speci\ufb01c categories like \u201dPER- SON\u201d, \u201dLOCATION\u201d, \u201dORGANIZATION\u201d and others(Collobert et al., 2011).",
  "1 Introduction Named Entity Recognition (NER) is a fore- most NLP task to label each atomic elements of a sentence into speci\ufb01c categories like \u201dPER- SON\u201d, \u201dLOCATION\u201d, \u201dORGANIZATION\u201d and others(Collobert et al., 2011). There has been an extensive NER research on English, German, Dutch and Spanish language (Lample et al., 2016), (Ma and Hovy, 2016), (Devlin et al., 2018), (Pe- ters et al., 2018), (Akbik et al., 2018), and no- table research on low resource South Asian lan- guages like Hindi(Athavale et al., 2016), Indone- sian(Gunawan et al., 2018) and other Indian lan- guages (Kannada, Malayalam, Tamil and Tel- ugu)(Gupta et al., 2018). However, there has been no study on developing neural NER for Nepali lan- guage.",
  "However, there has been no study on developing neural NER for Nepali lan- guage. In this paper, we propose a neural based Nepali NER using latest state-of-the-art architec- ture based on grapheme-level which doesn\u2019t re- quire any hand-crafted features and no data pre- processing. Recent neural architecture like (Lample et al., 2016) is used to relax the need to hand-craft the features and need to use part-of-speech tag to determine the category of the entity. How- ever, this architecture have been studied for lan- guages like English, and German and not been ap- plied to languages like Nepali which is a low re- source language i.e limited data set to train the model. Traditional methods like Hidden Markov Model (HMM) with rule based approaches(Dey and Prukayastha, 2013),(Dey et al., 2014), and Support Vector Machine (SVM) with manual feature-engineering(Bam and Shahi, 2014) have been applied but they perform poor compared to neural. However, there has been no research in Nepali NER using neural network.",
  "However, there has been no research in Nepali NER using neural network. Therefore, we created the named entity annotated dataset partly with the help of Dataturk1 to train a neural model. The texts used for this dataset are collected from various daily news sources from Nepal2 around the year 2015-2016. Following are our contributions: 1. We present a novel Named Entity Recognizer (NER) for Nepali language. To best of our knowledge we are the \ufb01rst to propose neural based Nepali NER. 2. As there are not good quality dataset to train NER we release a dataset to support future research 3. We perform empirical evaluation of our model with state-of-the-art models with rel- ative improvement of upto 10% In this paper, we present works similar to ours in Section 2. We describe our approach and dataset statistics in Section 3 and 4, followed by our ex- periments, evaluation and discussion in Section 5, 6, and 7. We conclude with our observations in Section 8.",
  "We describe our approach and dataset statistics in Section 3 and 4, followed by our ex- periments, evaluation and discussion in Section 5, 6, and 7. We conclude with our observations in Section 8. 1https://dataturks.com/ 2https://github.com/sndsabin/Nepali-News-Classi\ufb01er arXiv:1908.05828v1  [cs.CL]  16 Aug 2019",
  "To facilitate further research our code and dataset will be made available at github.com/link- yet-to-be-updated 2 Related Work There has been a handful of research on Nepali NER task based on approaches like Support Vector Machine and gazetteer list(Bam and Shahi, 2014) and Hidden Markov Model and gazetteer list(Dey and Prukayastha, 2013),(Dey et al., 2014). (Bam and Shahi, 2014) uses SVM along with features like \ufb01rst word, word length, digit features and gazetteer (person, organization, location, mid- dle name, verb, designation and others). It uses one vs rest classi\ufb01cation model to classify each word into different entity classes. However, it does not the take context word into account while train- ing the model. Similarly, (Dey and Prukayastha, 2013) and (Dey et al., 2014) uses Hidden Markov Model with n-gram technique for extracting POS- tags.",
  "However, it does not the take context word into account while train- ing the model. Similarly, (Dey and Prukayastha, 2013) and (Dey et al., 2014) uses Hidden Markov Model with n-gram technique for extracting POS- tags. POS-tags with common noun, proper noun or combination of both are combined together, then uses gazetteer list as look-up table to identify the named entities. Researchers have shown that the neu- ral networks like CNN(LeCun et al., 1989), RNN(Rumelhart et al., 1988), LSTM(Hochreiter and Schmidhuber, 1997), GRU(Chung et al., 2014) can capture the semantic knowledge of language better with the help of pre-trained embbeddings like word2vec(Mikolov et al., 2013), glove(Pennington et al., 2014) or fast- text(Bojanowski et al., 2016).",
  "Similar approaches has been applied to many South Asian languages like Hindi(Athavale et al., 2016), Indonesian(Gunawan et al., 2018), Ben- gali(Banik and Rahman, 2018) and In this pa- per, we present the neural network architecture for NER task in Nepali language, which doesn\u2019t require any manual feature engineering nor any data pre-processing during training. First we are comparing BiLSTM(Hochreiter and Schmid- huber, 1997), BiLSTM+CNN(Chiu and Nichols, 2015), BiLSTM+CRF(Lample et al., 2016), BiL- STM+CNN+CRF(Ma and Hovy, 2016) models with CNN model(Collobert et al., 2011) and Stan- ford CRF model(Finkel et al., 2005). Secondly, we show the comparison between models trained on general word embeddings, word embedding + character-level embedding, word embedding + part-of-speech(POS) one-hot encoding and word Figure 1: The grapheme level convolution neural net- work to extract grapheme representation.",
  "Secondly, we show the comparison between models trained on general word embeddings, word embedding + character-level embedding, word embedding + part-of-speech(POS) one-hot encoding and word Figure 1: The grapheme level convolution neural net- work to extract grapheme representation. The dropout layer is applied after maxpooling layer. embedding + grapheme clustered or sub-word em- bedding(Park and Shin, 2018). The experiments were performed on the dataset that we created and on the dataset received from ILPRL lab3. Our ex- tensive study shows that augmenting word embed- ding with character or grapheme-level representa- tion and POS one-hot encoding vector yields bet- ter results compared to using general word embed- ding alone. 3 Approach In this section, we describe our approach in build- ing our model. This model is partly inspired from multiple models (Chiu and Nichols, 2015),(Lam- ple et al., 2016), and(Ma and Hovy, 2016) 3.1 Bidirectional LSTM We used Bi-directional LSTM to capture the word representation in forward as well as reverse direc- tion of a sentence.",
  "Generally, LSTMs take in- puts from left (past) of the sentence and computes the hidden state. However, it is proven bene\ufb01- cial(Dyer et al., 2015) to use bi-directional LSTM, where, hidden states are computed based from right (future) of sentence and both of these hidden states are concatenated to produce the \ufb01nal output as ht=[\u2212\u2192 ht;\u2190\u2212 ht], where \u2212\u2192 ht, \u2190\u2212 ht = hidden state com- puted in forward and backward direction respec- tively. 3http://ilprl.ku.edu.np/",
  "Figure 2: End-to-end model architecture of our neural network. W-EMB, GRAPH, POS represents pre-trained word embeddings, grapheme representations and POS one-hot encoding vectors. GRAPH is obtained from CNN as shown in \ufb01gure 1. The dashed line implies the application of dropout. 3.2 Features 3.2.1 Word embeddings We have used Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and FastText (Bo- janowski et al., 2016) word vectors of 300 dimen- sions. These vectors were trained on the corpus obtained from Nepali National Corpus4. This pre- lemmatized corpus consists of 14 million words from books, web-texts and news papers. This cor- pus was mixed with the texts from the dataset before training CBOW and skip-gram version of word2vec using gensim library( \u02c7Reh\u02dau\u02c7rek and So- jka, 2010). This trained model consists of vectors for 72782 unique words.",
  "This trained model consists of vectors for 72782 unique words. Light pre-processing was performed on the cor- pus before training it. For example, invalid char- acters or characters other than Devanagari were re- moved but punctuation and numbers were not re- moved. We set the window context at 10 and the rare words whose count is below 5 are dropped. These word embeddings were not frozen during the training session because \ufb01ne-tuning word em- bedding help achieve better performance com- pared to frozen one(Chiu and Nichols, 2015). We have used fasttext embeddings in particu- 4https://www.sketchengine.eu/nepali-national-corpus/ lar because of its sub-word representation abil- ity, which is very useful in highly in\ufb02ectional lan- guage as shown in Table 3. We have trained the word embedding in such a way that the sub-word size remains between 1 and 4.",
  "We have trained the word embedding in such a way that the sub-word size remains between 1 and 4. We particularly chose this size because in Nepali language a sin- gle letter can also be a word, for example e, t, C, r, l, n, u and a single character (grapheme) or sub-word can be formed after mixture of depen- dent vowel signs with consonant letters for exam- ple, C + O +\\ = CO\\, here three different consonant letters form a single sub-word. The two-dimensional visualization of an exam- ple word n\u0003pAl is shown in 4. Principal Compo- nent Analysis (PCA) technique was used to gen- erate this visualization which helps use to ana- lyze the nearest neighbor words of a given sam- ple word. 84 and 104 nearest neighbors were ob- served using word2vec and fasttext embedding re- spectively on the same corpus.",
  "84 and 104 nearest neighbors were ob- served using word2vec and fasttext embedding re- spectively on the same corpus. 3.2.2 Character-level embeddings (Chiu and Nichols, 2015) and (Ma and Hovy, 2016) successfully presented that the character- level embeddings, extracted using CNN, when combined with word embeddings enhances the NER model performance signi\ufb01cantly, as it is able",
  "Figure 3: Format of a sample sentence in our dataset. to capture morphological features of a word. Fig- ure 1 shows the grapheme-level CNN used in our model, where inputs to CNN are graphemes. Character-level CNN is also built in similar fash- ion, except the inputs are characters. Grapheme or Character -level embeddings are randomly ini- tialized from [0,1] with real values with uniform distribution of dimension 30. 3.2.3 Grapheme-level embeddings Grapheme is atomic meaningful unit in writ- ing system of any languages. Since, Nepali language is highly morphologically in\ufb02ectional, we compared grapheme-level representation with character-level representation to evaluate its ef- fect. For example, in character-level embedding, each character of a word n\u0003pAl results into n +\u0003 + p + A + l has its own embedding. However, in grapheme level, a word n\u0003pAl is clustered into graphemes, resulting into n\u0003 + pA + l. Here, each grapheme has its own embedding.",
  "However, in grapheme level, a word n\u0003pAl is clustered into graphemes, resulting into n\u0003 + pA + l. Here, each grapheme has its own embedding. This grapheme- level embedding results good scores on par with character-level embedding in highly in\ufb02ectional languages like Nepali, because graphemes also capture syntactic information similar to charac- ters. We created grapheme clusters using uniseg5 package which is helpful in unicode text segmen- tations. 3.2.4 Part-of-speech (POS) one hot encoding We created one-hot encoded vector of POS tags and then concatenated with pre-trained word em- beddings before passing it to BiLSTM network. A sample of data is shown in \ufb01gure 3. 5https://uniseg-python.readthedocs.io/en/latest /index.html 4 Dataset Statistics 4.1 OurNepali dataset Since, we there was no publicly available stan- dard Nepali NER dataset and did not receive any dataset from the previous researchers, we had to create our own dataset. This dataset contains the sentences collected from daily newspaper of the year 2015-2016.",
  "This dataset contains the sentences collected from daily newspaper of the year 2015-2016. This dataset has three major classes Person (PER), Location (LOC) and Orga- nization (ORG). Pre-processing was performed on the text before creation of the dataset, for example all punctuations and numbers besides \u2019,\u2019, \u2019-\u2019, \u2019\u2014\u2019 and \u2019.\u2019 were removed. Currently, the dataset is in standard CoNLL-2003 IO format(Tjong Kim Sang and De Meulder, 2003). Since, this dataset is not lemmatized originally, we lemmatized only the post-positions like Ek, kO, l\u0003, mA, m{, my, jF, s g, aEG which are just the few examples among 299 post positions in Nepali language. We obtained these post-positions from sanjaalcorps6 and added few more to match our dataset. We will be releasing this list in our github repository. We found out that lemmatizing the post-positions boosted the F1 score by almost 10%.",
  "We obtained these post-positions from sanjaalcorps6 and added few more to match our dataset. We will be releasing this list in our github repository. We found out that lemmatizing the post-positions boosted the F1 score by almost 10%. In order to label our dataset with POS-tags, we \ufb01rst created POS annotated dataset of 6946 sen- tences and 16225 unique words extracted from POS-tagged Nepali National Corpus and trained a BiLSTM model with 95.14% accuracy which was used to create POS-tags for our dataset. The dataset released in our github repository contains each word in newline with space sepa- rated POS-tags and Entity-tags. The sentences are separated by empty newline. A sample sentence from the dataset is presented in table 3. 4.2 ILPRL dataset After much time, we received the dataset from Bal Krishna Bal, ILPRL, KU. This dataset fol- lows standard CoNLL-2003 IOB format(Tjong Kim Sang and De Meulder, 2003) with POS tags. This dataset is prepared by ILPRL Lab7, KU and KEIV Technologies.",
  "This dataset fol- lows standard CoNLL-2003 IOB format(Tjong Kim Sang and De Meulder, 2003) with POS tags. This dataset is prepared by ILPRL Lab7, KU and KEIV Technologies. Few corrections like correct- ing the NER tags had to be made on the dataset. The statistics of both the dataset is presented in ta- ble 1. Table 2 presents the total entities (PER, LOC, ORG and MISC) from both of the dataset used in 6https://github.com/sanjaalcorps/NepaliStemmer 7http://ilprl.ku.edu.np/",
  "(a) Word2vec embedding (b) Fasttext embedding Figure 4: 2D Visualization of nearest neighbor word using PCA for a sample word n\u0003pAl Dataset ILPRL OurNepali PER 262 5059 ORG 180 3811 LOC 273 2313 MISC 461 0 Total entities w/o O 1176 11183 Others - O 12683 67904 Total entities w/ O 13859 79087 Total sentences 548 3606 Table 1: Dataset statistics Dataset ILPRL OurNepali Train 754 7165 Test 188 2033 Dev 234 1985 Table 2: Dataset division statistics. The number pre- sented are total count of entities token in each set. our experiments. The dataset is divided into three parts with 64%, 16% and 20% of the total dataset into training set, development set and test set re- spectively. 5 Experiments In this section, we present the details about train- ing our neural network. The neural network ar- chitecture are implemented using PyTorch frame- work (Paszke et al., 2017).",
  "5 Experiments In this section, we present the details about train- ing our neural network. The neural network ar- chitecture are implemented using PyTorch frame- work (Paszke et al., 2017). The training is per- formed on a single Nvidia Tesla P100 SXM2. We \ufb01rst run our experiment on BiLSTM, BiLSTM- CNN, BiLSTM-CRF BiLSTM-CNN-CRF using the hyper-parameters mentioned in Table 4. The training and evaluation was done on sentence- level. The RNN variants are initialized randomly from (\u2212 \u221a k, \u221a k) where k = 1 hidden size. First we loaded our dataset and built vocabulary using torchtext library8. This eased our process of data loading using its SequenceTaggingDataset class. We trained our model with shuf\ufb02ed training set using Adam optimizer with hyper-parameters mentioned in table 4. All our models were trained on single layer of LSTM network. We found out Adam was giving better performance and faster convergence compared to Stochastic Gradient De- scent (SGD).",
  "We trained our model with shuf\ufb02ed training set using Adam optimizer with hyper-parameters mentioned in table 4. All our models were trained on single layer of LSTM network. We found out Adam was giving better performance and faster convergence compared to Stochastic Gradient De- scent (SGD). We chose those hyper-parameters af- ter many ablation studies. The dropout of 0.5 is applied after LSTM layer. For CNN, we used 30 different \ufb01lters of sizes 3, 4 and 5. The embeddings of each charac- ter or grapheme involved in a given word, were passed through the pipeline of Convolution, Rec- ti\ufb01ed Linear Unit and Max-Pooling. The resulting vectors were concatenated and applied dropout of 0.5 before passing into linear layer to obtain the embedding size of 30 for the given word. This re- sulting embedding is concatenated with word em- beddings, which is again concatenated with one- hot POS vector.",
  "The resulting vectors were concatenated and applied dropout of 0.5 before passing into linear layer to obtain the embedding size of 30 for the given word. This re- sulting embedding is concatenated with word em- beddings, which is again concatenated with one- hot POS vector. 5.1 Tagging Scheme Currently, for our experiments we trained our model on IO (Inside, Outside) format for both the dataset, hence the dataset does not contain any B- type annotation unlike in BIO (Beginning, Inside, Outside) scheme. 8https://torchtext.readthedocs.io/en/latest/",
  "Embeddings OurNepali Raw Lemmatized Train Test Val Train Test Val Random 78.72 63.66 64.89 88.44 75.11 77.2 Word2Vec CBOW 82.33 74.59 75.15 88.05 81.96 83.82 Word2Vec Skip Gram 81.58 75.93 75.75 89.84 83.47 85.79 GloVe 87.54 76.86 76.7 92.48 82.24 84.16 fastText Pretrained 81.57 75.06 71.96 85.76 77.6 79.78 fastText CBOW 86.01 81.4 80.52 89.23 81.58 83.51 fastText Skip Gram 88.31 80.6 78.85 94.01 84.74 85.11 Table 3: Effect of different embedding with Bi-LSTM.",
  "5.2 Early Stopping We used simple early stopping technique where if the validation loss does not decrease after 10 epochs, the training was stopped, else the train- ing will run upto 100 epochs. In our experience, training usually stops around 30-50 epochs. 5.3 Hyper-parameters Tuning We ran our experiment looking for the best hyper- parameters by changing learning rate from (0,1, 0.01, 0.001, 0.0001), weight decay from [10\u22121, 10\u22122, 10\u22123, 10\u22124, 10\u22125, 10\u22126, 10\u22127], batch size from [1, 2, 4, 8, 16, 32, 64, 128], hidden size from [8, 16, 32, 64, 128, 256, 512 1024]. Table 4 shows all other hyper-parameter used in our experiment for both of the dataset.",
  "Table 4 shows all other hyper-parameter used in our experiment for both of the dataset. Hyper-parameters Value LSTM - hidden size 100 CNN - Filter size [3,4,5] CNN - Filter number 30 Learning rate 0.001 Weight decay 1.00E-006 Batch size 1 Dropout 0.5 Table 4: Hyper-parameters of our experiments 5.4 Effect of Dropout Figure 5 shows how we end up choosing 0.5 as dropout rate. When the dropout layer was not used, the F1 score are at the lowest. As, we slowly increase the dropout rate, the F1 score also grad- ually increases, however after dropout rate = 0.5, the F1 score starts falling down. Therefore, we have chosen 0.5 as dropout rate for all other ex- periments performed. Figure 5: F1 score based on different dropout val- ues using fastText embeddings (Skip Gram). All other hyper-parameter used for this evaluation are presented in table 4. 6 Evaluation In this section, we present the details regarding evaluation and comparison of our models with other baselines.",
  "Figure 5: F1 score based on different dropout val- ues using fastText embeddings (Skip Gram). All other hyper-parameter used for this evaluation are presented in table 4. 6 Evaluation In this section, we present the details regarding evaluation and comparison of our models with other baselines. Table 3 shows the study of various embeddings and comparison among each other in OurNepali dataset. Here, raw dataset represents such dataset where post-positions are not lemmatized. We can observe that pre-trained embeddings signi\ufb01- cantly improves the score compared to randomly initialized embedding. We can deduce that Skip Gram models perform better compared CBOW models for word2vec and fasttext. Here, fast- Text Pretrained represents the embedding readily available in fastText website9, while other embed- dings are trained on the Nepali National Corpus as mentioned in sub-section 3.2.1. From this table 3, we can clearly observe that model using fast- 9https://fasttext.cc/docs/en/crawl-vectors.html",
  "Figure 6: Sample output of the best model from ILPRL test dataset. First, second and third column indicates word to be predicted, ground truth and predicted truth respectively. We can see that not all the tags are classi- \ufb01ed correctly. Text Skip Gram embeddings outperforms all other models. Table 5 shows the model architecture com- parison between all the models experimented. The features used for Stanford CRF classi\ufb01er are words, letter n-grams of upto length 6, previous word and next word. This model is trained till the current function value is less than 1e\u22122. The hyper-parameters of neural network experiments are set as shown in table 4. Since, word embed- ding of character-level and grapheme-level is ran- dom, their scores are near. All models are evaluated using CoNLL-2003 evaluation script(Tjong Kim Sang and De Meul- der, 2003) to calculate entity-wise precision, recall and f1 score.",
  "All models are evaluated using CoNLL-2003 evaluation script(Tjong Kim Sang and De Meul- der, 2003) to calculate entity-wise precision, recall and f1 score. 7 Discussion In this paper we present that we can exploit the power of neural network to train the model to per- form downstream NLP tasks like Named Entity Recognition even in Nepali language. We showed that the word vectors learned through fasttext skip gram model performs better than other word em- bedding because of its capability to represent sub- word and this is particularly important to capture morphological structure of words and sentences in highly in\ufb02ectional like Nepali. This concept can come handy in other Devanagari languages as well because the written scripts have similar syntactical structure. We also found out that stemming post-positions can help a lot in improving model performance be- cause of in\ufb02ectional characteristics of Nepali lan- guage. So when we separate out its in\ufb02ections or morphemes, we can minimize the variations of same word which gives its root word a stronger word vector representations compared to its in- \ufb02ected versions.",
  "So when we separate out its in\ufb02ections or morphemes, we can minimize the variations of same word which gives its root word a stronger word vector representations compared to its in- \ufb02ected versions. We can clearly imply from tables 1, 2, and 5 that we need more data to get better results because OurNepali dataset volume is almost ten times big- ger compared to ILPRL dataset in terms of enti- ties. 8 Conclusion and Future work In this paper, we proposed a novel NER for Nepali language and achieved relative improvement of upto 10% and studies different factors effecting the performance of the NER for Nepali language. We also present a neural architecture BiLSTM+CNN(grapheme-level) which turns out to be performing on par with BiLSTM+CNN(character-level) under the same con\ufb01guration. We believe this will not only help Nepali language but also other languages falling under the umbrellas of Devanagari languages. Our model BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS outperforms all other model experimented in OurNepali and ILPRL dataset respectively.",
  "Our model BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS outperforms all other model experimented in OurNepali and ILPRL dataset respectively. Since this is the \ufb01rst named entity recognition research in Nepal language using neural network, there are many rooms for improvement. We be- lieve initializing the grapheme-level embedding with fasttext embeddings might help boosting the performance, rather than randomly initializing it. In future, we plan to apply other latest techniques like BERT, ELMo and FLAIR to study its ef- fect on low-resource language like Nepali. We also plan to improve the model using cross-lingual or multi-lingual parameter sharing techniques by jointly training with other Devanagari languages like Hindi and Bengali. Finally, we would like to contribute our dataset to Nepali NLP community to move forward the research going on in language understanding do- main. We believe there should be special com- mittee to create and maintain such dataset for Nepali NLP and organize various competitions",
  "Dataset OurNepali ILPRL Models Test Val Test Val Stanford CRF 75.16 79.61 56.25 72.07 BiLSTM 84.74 85.11 80.40 83.17 BiLSTM + POS 83.65 84.09 81.25 85.39 BiLSTM + CNN (C) 86.45 87.45 80.51 85.45 BiLSTM + CNN (G) 86.71 86.00 78.24 83.49 BiLSTM + CNN (C) + POS 85.40 86.50 81.46 86.64 BiLSTM + CNN (G) + POS 85.46 86.43 83.08 82.99 Table 5: Comparison of different variation of our models OurNepali ILPRL Model Test Test Bam et al. SVM 66.26 46.26 Ma and Hovy w/ glove 83.63 72.1 Lample et al. w/ fastText 85.78 82.29 Lample et al.",
  "SVM 66.26 46.26 Ma and Hovy w/ glove 83.63 72.1 Lample et al. w/ fastText 85.78 82.29 Lample et al. w/ word2vec 86.49 78.63 BiLSTM + CNN (G) 86.71 78.24 BiLSTM + CNN (G) + POS 85.46 83.08 Table 6: Comparison with previous models based on Test F1 score Dataset OurNepali ILPRL Entities Precision Recall F1 Precision Recall F1 PER 93.82 88.66 91.17 74.36 72.50 73.42 ORG 87.28 79.59 83.26 92.31 75.00 82.76 LOC 84.29 82.11 83.19 91.07 69.86 79.07 MISC NA NA NA 94.94 87.21 90.91 Overall 89.45 84.14 86.71 89.30 77.67 83.08 Table 7: Entity-wise comparison using best model for respective dataset.",
  "MISC-entity is not available for OurNepali dataset. which would elevate the NLP research in Nepal. Some of the future works are listed below: 1. Proper initialization of grapheme level em- bedding from fasttext embeddings. 2. Apply robust POS-tagger for Nepali dataset 3. Lemmatize the OurNepali dataset with robust and ef\ufb01cient lemmatizer 4. Improve Nepali language score with cross- lingual learning techniques 5. Create more dataset using Wikipedia/Wikidata framework 9 Acknowledgments The authors of this paper would like to express sin- cere thanks to Bal Krishna Bal, Kathmandu Uni- versity Professor for providing us the POS-tagged Nepali NER data. References Alan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual string embeddings for sequence labeling. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1638\u20131649, Santa Fe, New Mexico, USA. Associ- ation for Computational Linguistics.",
  "2018. Contextual string embeddings for sequence labeling. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1638\u20131649, Santa Fe, New Mexico, USA. Associ- ation for Computational Linguistics. Vinayak Athavale, Shreenivas Bharadwaj, Monik Pamecha, Ameya Prabhu, and Manish Shrivastava. 2016. Towards deep learning in hindi NER: an ap- proach to tackle the labelled data sparsity. CoRR, abs/1610.09756. Surya Bahadur Bam and Tej Bahadur Shahi. 2014. Named entity recognition for nepali text using sup-",
  "port vector machines. Intelligent Information Man- agement, 6(02):21. Nayan Banik and Md Hasan Ha\ufb01zur Rahman. 2018. Gru based named entity recognition system for bangla online newspapers. In 2018 International Conference on Innovation in Engineering and Tech- nology (ICIET), pages 1\u20136. IEEE. Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2016. Enriching word vec- tors with subword information. arXiv preprint arXiv:1607.04606. Jason P. C. Chiu and Eric Nichols. 2015. Named en- tity recognition with bidirectional lstm-cnns. CoRR, abs/1511.08308. Junyoung Chung, C\u00b8 aglar G\u00a8ulc\u00b8ehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence model- ing. CoRR, abs/1412.3555.",
  "Junyoung Chung, C\u00b8 aglar G\u00a8ulc\u00b8ehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence model- ing. CoRR, abs/1412.3555. Ronan Collobert, Jason Weston, L\u00b4eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa. 2011. Natural language processing (almost) from scratch. CoRR, abs/1103.0398. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: pre-training of deep bidirectional transformers for language under- standing. CoRR, abs/1810.04805. Arindam Dey, Abhijit Paul, and Bipul Syam Purkayastha. 2014. Named entity recognition for nepali language: A semi hybrid approach. Interna- tional Journal of Engineering and Innovative Tech- nology (IJEIT) Volume, 3:21\u201325.",
  "2014. Named entity recognition for nepali language: A semi hybrid approach. Interna- tional Journal of Engineering and Innovative Tech- nology (IJEIT) Volume, 3:21\u201325. Arindam Dey and Bipul Syam Prukayastha. 2013. Named entity recognition using gazetteer method and n-gram technique for an in\ufb02ectional language: A hybrid approach. International Journal of Com- puter Applications, 84(9). Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and Noah A. Smith. 2015. Transition- based dependency parsing with stack long short- term memory. CoRR, abs/1505.08075. Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local informa- tion into information extraction systems by gibbs sampling. In Proceedings of the 43rd annual meet- ing on association for computational linguistics, pages 363\u2013370. Association for Computational Lin- guistics.",
  "2005. Incorporating non-local informa- tion into information extraction systems by gibbs sampling. In Proceedings of the 43rd annual meet- ing on association for computational linguistics, pages 363\u2013370. Association for Computational Lin- guistics. William Gunawan, Derwin Suhartono, Fredy Purnomo, and Andrew Ongko. 2018. Named-entity recog- nition for indonesian language using bidirectional lstm-cnns. Procedia Computer Science, 135:425\u2013 432. Ayush Gupta, Meghna Ayyar, Ashutosh Kumar Singh, and Rajiv Ratn Shah. 2018. raiden11@ iecsil- \ufb01re-2018: Named entity recognition for indian lan- guages. FIRE Working Notes. Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735\u20131780. Guillaume Lample, Miguel Ballesteros, Sandeep Sub- ramanian, Kazuya Kawakami, and Chris Dyer.",
  "1997. Long short-term memory. Neural computation, 9(8):1735\u20131780. Guillaume Lample, Miguel Ballesteros, Sandeep Sub- ramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. CoRR, abs/1603.01360. Yann LeCun, Bernhard Boser, John S Denker, Don- nie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. 1989. Backpropagation ap- plied to handwritten zip code recognition. Neural computation, 1(4):541\u2013551. Xuezhe Ma and Eduard H. Hovy. 2016. End-to-end sequence labeling via bi-directional lstm-cnns-crf. CoRR, abs/1603.01354. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor- rado, and Jeffrey Dean. 2013. Distributed represen- tations of words and phrases and their composition- ality.",
  "CoRR, abs/1603.01354. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor- rado, and Jeffrey Dean. 2013. Distributed represen- tations of words and phrases and their composition- ality. CoRR, abs/1310.4546. Suzi Park and Hyopil Shin. 2018. Grapheme-level awareness in word embeddings for morphologi- cally rich languages. In Proceedings of the 11th Language Resources and Evaluation Conference, Miyazaki, Japan. European Language Resource As- sociation. Adam Paszke, Sam Gross, Soumith Chintala, Gre- gory Chanan, Edward Yang, Zachary DeVito, Zem- ing Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in pytorch. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation.",
  "2017. Automatic differentiation in pytorch. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, Doha, Qatar. Association for Computational Linguistics. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. CoRR, abs/1802.05365. Radim \u02c7Reh\u02dau\u02c7rek and Petr Sojka. 2010. Software Frame- work for Topic Modelling with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u201350, Val- letta, Malta. ELRA. http://is.muni.cz/ publication/884893/en. David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams.",
  "ELRA. http://is.muni.cz/ publication/884893/en. David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. 1988. Neurocomputing: Foundations of research. chapter Learning Representations by Back-propagating Errors, pages 696\u2013699. MIT Press, Cambridge, MA, USA.",
  "Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003 - Volume 4, CONLL \u201903, pages 142\u2013147, Stroudsburg, PA, USA. Association for Computational Linguistics."
]