{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "To lemmatize or not to lemmatize: how word normalisation affects ELMo performance in word sense disambiguation Andrey Kutuzov\u2217 University of Oslo Oslo, Norway andreku@ifi.uio.no Elizaveta Kuzmenko University of Trento Trento, Italy lizaku77@gmail.com Abstract In this paper, we critically evaluate the widespread assumption that deep learning NLP models do not require lemmatized input. To test this, we trained versions of contextualised word embedding ELMo models on raw tokenized corpora and on the corpora with word tokens replaced by their lemmas. Then, these models were evaluated on the word sense disambigua- tion task. This was done for the English and Russian languages. The experiments showed that while lemmatization is indeed not necessary for English, the situation is different for Rus- sian. It seems that for rich-morphology languages, using lemmatized training and testing data yields small but con- sistent improvements: at least for word sense disambiguation. This means that the decisions about text pre-processing before training ELMo should consider the linguistic nature of the language in question.",
            "This means that the decisions about text pre-processing before training ELMo should consider the linguistic nature of the language in question. 1 Introduction Deep contextualised representations of linguistic entities (words and\/or sentences) are used in many current state-of-the-art NLP systems. The most well-known examples of such models are arguably ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). A long-standing tradition if the \ufb01eld of apply- ing deep learning to NLP tasks can be summarised as follows: as minimal pre-processing as possible. It is widely believed that lemmatization or other text input normalisation is not necessary. Ad- vanced neural architectures based on character in- put (CNNs, BPE, etc) are supposed to be able to \u2217Both authors contributed equally to the paper. learn how to handle spelling and morphology vari- ations themselves, even for languages with rich morphology: \u2018just add more layers!\u2019. Contextu- alised embedding models follow this tradition: as a rule, they are trained on raw text collections, with minimal linguistic pre-processing. Below, we show that this is not entirely true.",
            "Contextu- alised embedding models follow this tradition: as a rule, they are trained on raw text collections, with minimal linguistic pre-processing. Below, we show that this is not entirely true. It is known that for the previous generation of word embedding models (\u2018static\u2019 ones like word2vec (Mikolov et al., 2013), where a word always has the same representation regardless of the context in which it occurs), lemmatization of the training and testing data improves their perfor- mance. Fares et al. (2017) showed that this is true at least for semantic similarity and analogy tasks. In this paper, we describe our experiments in \ufb01nding out whether lemmatization helps modern contextualised embeddings (on the example of ELMo). We compare the performance of ELMo models trained on the same corpus before and af- ter lemmatization. It is impossible to evaluate con- textualised models on \u2018static\u2019 tasks like lexical se- mantic similarity or word analogies. Because of this, we turned to word sense disambiguation in context (WSD) as an evaluation task.",
            "It is impossible to evaluate con- textualised models on \u2018static\u2019 tasks like lexical se- mantic similarity or word analogies. Because of this, we turned to word sense disambiguation in context (WSD) as an evaluation task. In brief, we use contextualised representations of ambiguous words from the top layer of an ELMo model to train word sense classi\ufb01ers and \ufb01nd out whether using lemmas instead of tokens helps in this task (see Section 5). We experiment with the English and Russian languages and show that they differ signi\ufb01cantly in the in\ufb02uence of lemmatization on the WSD performance of ELMo models. Our \ufb01ndings and the contributions of this paper are: 1. Linguistic text pre-processing still matters in some tasks, even for contemporary deep rep- resentation learning algorithms. 2. For the Russian language, with its rich mor- arXiv:1909.03135v1  [cs.CL]  6 Sep 2019",
            "English Russian Source Wikipedia Wikipedia + RNC Size, tokens 2 174 mln 989 mln Size, lemmas 1 977 mln 988 mln Table 1: Training corpora phology, lemmatizing the training and testing data for ELMo representations yields small but consistent improvements in the WSD task. This is unlike English, where the dif- ferences are negligible. 2 Related work ELMo contextual word representations are learned in an unsupervised way through language mod- elling (Peters et al., 2018). The general archi- tecture consists of a two-layer BiLSTM on top of a convolutional layer which takes character se- quences as its input. Since the model uses fully character-based token representations, it avoids the problem of out-of-vocabulary words. Because of this, the authors explicitly recommend not to use any normalisation except tokenization for the input text. However, as we show below, while this is true for English, for other languages feed- ing ELMo with lemmas instead of raw tokens can improve WSD performance.",
            "Because of this, the authors explicitly recommend not to use any normalisation except tokenization for the input text. However, as we show below, while this is true for English, for other languages feed- ing ELMo with lemmas instead of raw tokens can improve WSD performance. Word sense disambiguation or WSD (Navigli, 2009) is the NLP task consisting of choosing a word sense from a pre-de\ufb01ned sense inventory, given the context in which the word is used. WSD \ufb01ts well into our aim to intrinsically eval- uate ELMo models, since solving the problem of polysemy and homonymy was one of the original promises of contextualised embeddings: their pri- mary difference from the previous generation of word embedding models is that contextualised ap- proaches generate different representations for ho- mographs depending on the context. We use two lexical sample WSD test sets, further described in Section 4. 3 Training ELMo For the experiments described below, we trained our own ELMo models from scratch. For En- glish, the training corpus consisted of the En- glish Wikipedia dump1 from February 2017.",
            "3 Training ELMo For the experiments described below, we trained our own ELMo models from scratch. For En- glish, the training corpus consisted of the En- glish Wikipedia dump1 from February 2017. For 1https:\/\/dumps.wikimedia.org\/ Russian, it was a concatenation of the Russian Wikipedia dump from December 2018 and the full Russian National Corpus2 (RNC). The RNC texts were added to the Russian Wikipedia dump so as to make the Russian training corpus more compa- rable in size to the English one (Wikipedia texts would comprise only half of the size). As Table 1 shows, the English Wikipedia is still two times larger, but at least the order is the same. The texts were tokenized and lemmatized with the UDPipe models for the respective languages trained on the Universal Dependencies 2.3 tree- banks (Straka and Strakov\u00e1, 2017).",
            "The texts were tokenized and lemmatized with the UDPipe models for the respective languages trained on the Universal Dependencies 2.3 tree- banks (Straka and Strakov\u00e1, 2017). UDPipe yields lemmatization accuracy about 96% for English and 97% for Russian3; thus for the task at hand, we considered it to be gold and did not try to fur- ther improve the quality of normalisation itself (al- though it is not entirely error-free, see Section 4). ELMo models were trained on these corpora us- ing the original TensorFlow implementation4, for 3 epochs with batch size 192, on two GPUs. To train faster, we decreased the dimensionality of the LSTM layers from the default 4096 to 2048 for all the models.",
            "ELMo models were trained on these corpora us- ing the original TensorFlow implementation4, for 3 epochs with batch size 192, on two GPUs. To train faster, we decreased the dimensionality of the LSTM layers from the default 4096 to 2048 for all the models. 4 Word sense disambiguation test sets We used two WSD datasets for evaluation: \u2022 Senseval-3 for English (Mihalcea et al., 2004) \u2022 RUSSE\u201918 for Russian (Panchenko et al., 2018) The Senseval-3 dataset consists of lexical sam- ples for nouns, verbs and adjectives; we used only noun target words: 1. argument 2. arm 3. atmosphere 4. audience 5. bank 6. degree 7. difference 8. dif\ufb01culty 2http:\/\/ruscorpora.ru\/en\/ 3http:\/\/ufal.mff.cuni.cz\/udpipe\/ models#universal_dependencies_23_models 4https:\/\/github.com\/allenai\/bilm-tf",
            "9. disc 10. image 11. interest 12. judgement 13. organization 14. paper 15. party 16. performance 17. plan 18. shelter 19. sort 20. source An example for the ambiguous word argument is given below: In some situations Postscript can be faster than the escape sequence type of printer control \ufb01le. It uses post \ufb01x notation, where arguments come \ufb01rst and operators follow. This is basically the same as Reverse Polish Notation as used on certain cal- culators, and follows directly from the stack based approach. It this sentence, the word \u2018argument\u2019 is used in the sense of a mathematical operator. The RUSSE\u201918 dataset was created in 2018 for the shared task in Russian word sense induction. This dataset contains only nouns; the list of words with their English translations is given in Table 2.",
            "The RUSSE\u201918 dataset was created in 2018 for the shared task in Russian word sense induction. This dataset contains only nouns; the list of words with their English translations is given in Table 2. Originally, it includes also the words \u0431\u0430\u0439\u043a\u0430 \u2018tale\/\ufb02eece\u2019 and \u0433\u0432\u043e\u0437\u0434\u0438\u043a\u0430 \u2019clove\/small nail\u2019, but their senses are ambiguous only in some in\ufb02ec- tional forms (not in lemmas), therefore we decided to exclude these words from evaluation. The Russian dataset is more homogeneous com- pared to the English one, as for all the target words there is approximately the same number of context words in the examples. This is achieved by apply- ing the lexical window (25 words before and after the target word) and cropping everything that falls outside of that window. In the English dataset, on the contrary, the whole paragraph with the target word is taken into account. We have tried crop- ping the examples for English as well, but it did not result in any change in the quality of classi- \ufb01cation.",
            "In the English dataset, on the contrary, the whole paragraph with the target word is taken into account. We have tried crop- ping the examples for English as well, but it did not result in any change in the quality of classi- \ufb01cation. In the end, we decided not to apply the Target word Translation \u0430\u043a\u0446\u0438\u044f \u2018stock\/marketing event\u2019 \u0433\u0438\u043f\u0435\u0440\u0431\u043e\u043b\u0430 \u2018hyperbola\/exaggeration\u2019 \u0433\u0440\u0430\u0434 \u2018hail\/city\u2019 \u0433\u0443\u0441\u0435\u043d\u0438\u0446\u0430 \u2018caterpillar\/track\u2019 \u0434\u043e\u043c\u0438\u043d\u043e \u2018dominoes\/costume\u2019 \u043a\u0430\u0431\u0430\u0447\u043e\u043a \u2018squash\/restaurant\u2019 \u043a\u0430\u043f\u043e\u0442 \u2018hood (part of a car\/clothing)\u2019 \u043a\u0430\u0440\u044c\u0435\u0440 \u2018mine\/fast pace of a horse\u2019 \u043a\u043e\u043a \u2018cook\/hairstyle\u2019 \u043a\u0440\u043e\u043d\u0430 \u2018crown (tree\/coin)\u2019 \u043a\u0440\u0443\u043f \u2018crupper (part of a horse\/illness)\u2019 \u043c\u0430\u043d\u0434\u0430\u0440\u0438\u043d \u2018fruit\/a Chinese of\ufb01cial\u2019 \u0440\u043e\u043a \u2018rock (music\/destiny)\u2019 \u0441\u043b\u043e\u0433 \u2018syllable\/text style\u2019 \u0441\u0442\u043e\u043f\u043a\u0430 \u2018stack\/glass\u2019 \u0442\u0430\u0437 \u2018basin\/human body part\u2019 \u0442\u0430\u043a\u0441\u0430 \u2018tariff\/dog breed\u2019 \u0448\u0430\u0445 \u2018check\/prince\u2019 Table 2: Target ambiguous words for Russian (RUSSE\u201918) lexical window to the English dataset so as not to alter it and rather use it in the original form.",
            "Here is an example from the RUSSE\u201918 for the ambiguous word \u043c\u0430\u043d\u0434\u0430\u0440\u0438\u043d \u2018mandarin\u2019 in the sense \u2018Chinese of\ufb01cial title\u2019: \u201c...\u0434\u0438\u043f\u043b\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u043a\u043e\u0440\u043f\u0443\u0441\u0430 \u043e\u0441\u0442\u0430\u043d\u043a\u0430\u043c \u0431\u043e\u0433\u0434\u044b\u0445\u0430\u043d\u0430 \u0438 \u0438\u043c\u043f\u0435\u0440\u0430\u0442\u0440\u0438\u0446\u044b \u043e\u0431\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043e \u0431\u044b\u043b\u043e \u0441 \u043d\u0435\u043e\u0431\u044b\u0447\u0430\u0439\u043d\u043e\u0439 \u0442\u043e\u0440\u0436\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0441\u0442\u044c\u044e. \u0422\u044b\u0441\u044f\u0447\u0438 \u043c\u0430\u043d\u0434\u0430\u0440\u0438\u043d\u043e\u0432 \u0438 \u0434\u0440\u0443\u0433\u0438\u0445 \u0432\u044b\u0441\u043e\u043a\u043e\u043f\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0445 \u043b\u0438\u0446 \u0440\u0430\u0437\u043c\u0435\u0441\u0442\u0438\u043b\u0438\u0441\u044c \u0448\u043f\u0430\u043b\u0435\u0440\u0430\u043c\u0438 \u043d\u0430 \u0442\u0440\u0435\u0445 \u043c\u0440\u0430- \u043c\u043e\u0440\u043d\u044b\u0445 \u0442\u0435\u0440\u0440\u0430\u0441\u0430\u0445 \u0432\u0435\u0434\u0443\u0449\u0438\u0445 \u043a...\u201d \u2018...the diplomatic bodies of the Bogdikhan and the Empress was furnished with extraordinary solemnity. Thousands of mandarins and other dig- nitaries were placed on three marble terraces lead- ing to...\u2019. Table 3 compares both datasets. Before usage, they were pre-processed in the same way as the training corpora for ELMo (see Section 3), thus producing a lemmatized and a non-lemmatized versions of each.",
            "Table 3 compares both datasets. Before usage, they were pre-processed in the same way as the training corpora for ELMo (see Section 3), thus producing a lemmatized and a non-lemmatized versions of each. As we can see from Table 3, for 20 target words in English there are 24 lemmas, and for 18 tar- get words in Russian there are 36 different lem- mas. These numbers are explained by occasional errors in the UDPipe lemmatization. Another in- teresting thing to observe is the number of distinct",
            "Property Senseval-3 RUSSE\u201918 Target words 20 18 Distinct target forms 39 132 Distinct target lemmas 24 36 Examples per target 171 126 Tokens per example 126 25 Senses per target 6 2 Table 3: Characteristics of the WSD datasets. The numbers in the lower part are average values. word forms for every language. For English, there are 39 distinct forms for 20 target nouns: singu- lar and plural for every noun, except \u2018atmosphere\u2019 which is used only in the singular form. Thus, in\ufb02ectional variability of English nouns is cov- ered by the dataset almost completely. For Rus- sian, we observe 132 distinct forms for 18 target nouns, giving more than 7 in\ufb02ectional forms per each word. Note that this still covers only half of all the in\ufb02ectional variability of Russian: this lan- guage features 12 distinct forms for each noun (6 cases and 2 numbers).",
            "Note that this still covers only half of all the in\ufb02ectional variability of Russian: this lan- guage features 12 distinct forms for each noun (6 cases and 2 numbers). To sum up, the RUSSE\u201918 dataset is morpho- logically far more complex than the Senseval3, re- \ufb02ecting the properties of the respective languages. In the next section we will see that this leads to substantial differences regarding comparisons be- tween token-based and lemma-based ELMo mod- els. 5 Experiments Following Gorman and Bedrick (2019), we de- cided to avoid using any standard train-test splits for our WSD datasets. Instead, we rely on per- word random splits and 5-fold cross-validation. This means that for each target word we randomly generate 5 different divisions of its context sen- tences list into train and test sets, and then train and test 5 different classi\ufb01er models on this data. The resulting performance score for each target word is the average of 5 macro-F1 scores produced by these classi\ufb01ers.",
            "The resulting performance score for each target word is the average of 5 macro-F1 scores produced by these classi\ufb01ers. ELMo models can be employed for the WSD task in two different ways: either by \ufb01ne-tuning the model or by extracting word representations from it and then using them as features in a down- stream classi\ufb01er. We decided to stick to the sec- ond (feature extraction) approach, since it is con- ceptually and computationally simpler. Addition- Model English Russian Baselines Random \u22480.138 \u22480.444 MFS 0.119 0.391 Tokens SGNS (averaged) 0.299 0.851 ELMo (averaged) 0.362 0.885 ELMo (target) 0.463 0.875 Lemmas SGNS (averaged) 0.300 0.854 ELMo (averaged) 0.365 0.888 ELMo (target) 0.452 0.907 Table 4: Averaged macro-F1 scores for WSD ally, Peters et al.",
            "(2019) showed that for most NLP tasks (except those focused on sentence pairs) the performance of feature extraction and \ufb01ne-tuning is nearly the same. Thus we extracted the single vector of the target word from the ELMo top layer (\u2018target\u2019 rows in Table 4) or the averaged ELMo top layer vectors of all words in the context sen- tence (\u2018averaged\u2019 rows in Table 4). For comparison, we also report the scores of the \u2018averaged vectors\u2019 representations with Con- tinuous Skipgram (Mikolov et al., 2013) embed- ding models trained on the English or Russian Wikipedia dumps (\u2018SGNS\u2019 rows): before the ad- vent of contextualised models, this was one of the most widely used ways to \u2018squeeze\u2019 the mean- ing of a sentence into a \ufb01xed-size vector. Of course it does not mean that the meaning of a sen- tence always determines the senses all its words are used in. However, averaging representations of words in contexts as a proxy to the sense of one particular word is a long established tradition in WSD, starting at least from Sch\u00fctze (1998).",
            "However, averaging representations of words in contexts as a proxy to the sense of one particular word is a long established tradition in WSD, starting at least from Sch\u00fctze (1998). Also, since SGNS is a \u2018static\u2019 embedding model, it is of course not possible to use only target word vectors as features: they would be identical whatever the context is. Simple logistic regression was used as a classi\ufb01- cation algorithm. We also tested a multi-layer per- ceptron (MLP) classi\ufb01er with 200-neurons hidden layer, which yielded essentially the same results. This leads us to believe that our \ufb01ndings are not classi\ufb01er-dependent. Table 4 shows the results, together with the ran-",
            "Figure 1: Word sense disambiguation perfor- mance on the English data across words (ELMo target models). dom and most frequent sense (MFS) baselines for each dataset. First, ELMo outperforms SGNS for both lan- guages, which comes as no surprise. Second, the approach with averaging representations from all words in the sentence is not bene\ufb01cial for WSD with ELMo: for English data, it clearly loses to a single target word representation, and for Rus- sian there are no signi\ufb01cant differences (and using a single target word is preferable from the com- putational point of view, since it does not require the averaging operation). Thus, below we discuss only the single target word usage mode of ELMo. But the most important part is the comparison between using tokens or lemmas in the train and test data. For the \u2018static\u2019 SGNS embeddings, it does not signi\ufb01cantly change the WSD scores for both languages. The same is true for English ELMo models, where differences are negligible and seem to be simple \ufb02uctuations.",
            "For the \u2018static\u2019 SGNS embeddings, it does not signi\ufb01cantly change the WSD scores for both languages. The same is true for English ELMo models, where differences are negligible and seem to be simple \ufb02uctuations. However, for Russian, ELMo (target) on lemmas outperforms ELMo on tokens, with small but signi\ufb01cant5 im- provement. The most plausible explanation for this is that (despite of purely character-based in- put of ELMo) the model does not have to learn id- iosyncrasies of a particular language morphology. Instead, it can use its (limited) capacity to better learn lexical semantic structures, leading to better WSD performance. The box plots 1 and 2 illus- trate the scores dispersion across words in the test sets for English and Russian correspondingly (or- ange lines are medians). In the next section 6 we 5At p value of 0.1, according to the Welch\u2019s t-test. Figure 2: Word sense disambiguation perfor- mance on the Russian data across words (ELMo target models).",
            "In the next section 6 we 5At p value of 0.1, according to the Welch\u2019s t-test. Figure 2: Word sense disambiguation perfor- mance on the Russian data across words (ELMo target models). Word Tokens Lemmas STD \u0430\u043a\u0446\u0438\u044f 0.876 0.978 0.050 \u043a\u0440\u043e\u043d\u0430 0.978 1.000 0.018 \u043a\u0440\u0443\u043f 0.927 1.000 0.070 \u0434\u043e\u043c\u0438\u043d\u043e 0.910 0.874 0.057 Table 5: F1 scores for target words from RUSSE\u201918 with signi\ufb01cant differences between lemma-based and token-based models analyse the results qualitatively. 6 Qualitative analysis In this section we focus on the comparison of scores for the Russian dataset. The classi\ufb01er for Russian had to choose between fewer classes (two or three), which made the scores higher and more consistent than for the English dataset. Overall, we see improvements in the scores for the major- ity of words, which proves that lemmatization for morphologically rich languages is bene\ufb01cial.",
            "Overall, we see improvements in the scores for the major- ity of words, which proves that lemmatization for morphologically rich languages is bene\ufb01cial. We decided to analyse more closely those words for which the difference in the scores between lemma-based and token-based models was statisti- cally signi\ufb01cant. By \u2018signi\ufb01cant\u2019 we mean that the scores differ by more that one standard deviation (the largest standard deviation value in the two sets was taken). The resulting list of targets words with signi\ufb01cant difference in scores is given in Table 5. We can see that among 18 words in the dataset only 3 exhibit signi\ufb01cant improvement in their scores when moving from tokens to lemmas in the input data. It shows that even though the over-",
            "all F1 scores for the Russian data have shown the plausibility of lemmatization, this improvement is mostly driven by a few words. It should be noted that these words\u2019 scores feature very low standard deviation values (for other words, standard devia- tion values were above 0.1, making F1 differences insigni\ufb01cant). Such a behaviour can be caused by more consistent differentiation of context for var- ious senses of these 3 words. For example, with the word \u043a\u0430\u0431\u0430\u0447\u043e\u043a \u2018squash \/ small restaurant\u2019, the contexts for both senses can be similar, since they are all related to food. This makes the WSD scores unstable. On the other hand, for \u0430\u043a\u0446\u0438\u044f \u2018stock, share \/ event\u2019, \u043a\u0440\u043e\u043d\u0430 \u2018crown (tree \/ coin)\u2019 or \u043a\u0440\u0443\u043f \u2018croup (horse body part \/ illness)\u2019, their senses are not related, which resulted in more stable results and signi\ufb01cant difference in the scores (see Table 5). There is only one word in the RUSSE\u201918 dataset for which the score has strongly decreased when moving to lemma-based models: \u0434\u043e\u043c\u0438\u043d\u043e \u2018domino (game \/ costume)\u2019.",
            "There is only one word in the RUSSE\u201918 dataset for which the score has strongly decreased when moving to lemma-based models: \u0434\u043e\u043c\u0438\u043d\u043e \u2018domino (game \/ costume)\u2019. In fact, the score dif- ference here lies on the border of one standard de- viation, so strictly speaking it is not really signi\ufb01- cant. However, the word still presents an interest- ing phenomenon. \u0414\u043e\u043c\u0438\u043d\u043e is the only target noun in the RUSSE\u201918 that has no in\ufb02ected forms, since it is a borrowed word. This leaves no room for improve- ment when using lemma-based ELMo models: all tokens of this word are already identical. At the same time, some information about in\ufb02ected word forms in the context can be useful, but it is lost during lemmatization, and this leads to the de- creased score. Arguably, this means that lemmati- zation brings along both advantages and disadvan- tages for WSD with ELMo.",
            "Arguably, this means that lemmati- zation brings along both advantages and disadvan- tages for WSD with ELMo. For in\ufb02ected words (which constitute the majority of Russian vocab- ulary) pro\ufb01ts outweigh the losses, but for atypical non-changeable words it can be the opposite. The scores for the excluded target words \u0431\u0430\u0439- \u043a\u0430 \u2018tale \/ \ufb02eece\u2019 and \u0433\u0432\u043e\u0437\u0434\u0438\u043a\u0430 \u2019clove \/ small nail\u2019 are given in Table 6 (recall that they were excluded because of being ambiguous only in some in\ufb02ec- tional forms). For these words we can see a great improvement with lemma-based models. This, of course stems from the fact that these words in dif- ferent senses have different lemmas. Therefore, the results are heavily dependent on the quality of lemmatization. Word Tokens Lemmas STD \u0431\u0430\u0439\u043a\u0430 0.421 0.627 0.099 \u0433\u0432\u043e\u0437\u0434\u0438\u043a\u0430 0.553 0.619 0.038 Table 6: F1 scores for the excluded target words from RUSSE\u201918.",
            "Word Tokens Lemmas STD \u0431\u0430\u0439\u043a\u0430 0.421 0.627 0.099 \u0433\u0432\u043e\u0437\u0434\u0438\u043a\u0430 0.553 0.619 0.038 Table 6: F1 scores for the excluded target words from RUSSE\u201918. 7 Conclusion We evaluated how the ability of ELMo contextu- alised word embedding models to disambiguate word senses depends on the nature of the train- ing data. In particular, we compared the models trained on raw tokenized corpora and those trained on the corpora with word tokens replaced by their normal forms (lemmas). The models we trained are publicly available via the NLPL word embed- dings repository6 (Fares et al., 2017). In the majority of research papers on deep learn- ing approaches to NLP, it is assumed that lemma- tization is not necessary, especially when using powerful contextualised embeddings. Our experi- ments show that this is indeed true for languages with simple morphology (like English). However, for rich-morphology languages (like Russian), us- ing lemmatized training data yields small but con- sistent improvements in the word sense disam- biguation task.",
            "Our experi- ments show that this is indeed true for languages with simple morphology (like English). However, for rich-morphology languages (like Russian), us- ing lemmatized training data yields small but con- sistent improvements in the word sense disam- biguation task. These improvements are not ob- served for rare words which lack in\ufb02ected forms; this further supports our hypothesis that better WSD scores of lemma-based models are related to them better handling multiple word forms in morphology-rich languages. Of course, lemmatization is by all means not a silver bullet. In other tasks, where in\ufb02ectional properties of words are important, it can even hurt the performance. But this is true for any NLP sys- tems, not only deep learning based ones. The take-home message here is twofold: \ufb01rst, text pre-processing still matters for contemporary deep learning algorithms. Their impressive learn- ing abilities do not always allow them to infer nor- malisation rules themselves, from simply optimis- ing the language modelling task.",
            "The take-home message here is twofold: \ufb01rst, text pre-processing still matters for contemporary deep learning algorithms. Their impressive learn- ing abilities do not always allow them to infer nor- malisation rules themselves, from simply optimis- ing the language modelling task. Second, the na- ture of language at hand matters as well, and dif- ferences in this nature can result in different deci- sions being optimal or sub-optimal at the stage of deep learning models training. The simple truth \u2018English is not representative of all languages on Earth\u2019 still holds here. 6http:\/\/vectors.nlpl.eu\/repository\/",
            "In the future, we plan to extend our work by including more languages into the analysis. Us- ing Russian and English allowed us to hypothe- sise about the importance of morphological char- acter of a language. But we only scratched the surface of the linguistic diversity. To verify this claim, it is necessary to analyse more strongly in\ufb02ected languages like Russian as well as more weakly in\ufb02ected (analytical) languages similar to English. This will help to \ufb01nd out if the in\ufb02ection differences are important for training deep learn- ing models across human languages in general. References Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics. Murhaf Fares, Andrey Kutuzov, Stephan Oepen, and Erik Velldal.",
            "Associ- ation for Computational Linguistics. Murhaf Fares, Andrey Kutuzov, Stephan Oepen, and Erik Velldal. 2017. Word vectors, reuse, and replica- bility: Towards a community repository of large-text resources. In Proceedings of the 21st Nordic Con- ference on Computational Linguistics, pages 271\u2013 276, Gothenburg, Sweden. Association for Compu- tational Linguistics. Kyle Gorman and Steven Bedrick. 2019. We need to talk about standard splits. In Proceedings of the 57th Annual Meeting of the Association for Com- putational Linguistics, pages 2786\u20132791, Florence, Italy. Association for Computational Linguistics. Rada Mihalcea, Timothy Chklovski, and Adam Kil- garriff. 2004. The Senseval-3 English lexical sam- ple task. In Proceedings of SENSEVAL-3, the Third International Workshop on the Evaluation of Sys- tems for the Semantic Analysis of Text, pages 25\u201328, Barcelona, Spain. Association for Computational Linguistics.",
            "2004. The Senseval-3 English lexical sam- ple task. In Proceedings of SENSEVAL-3, the Third International Workshop on the Evaluation of Sys- tems for the Semantic Analysis of Text, pages 25\u201328, Barcelona, Spain. Association for Computational Linguistics. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their composition- ality. Advances in Neural Information Processing Systems 26, pages 3111\u20133119. Roberto Navigli. 2009. Word sense disambiguation: A survey. ACM computing surveys (CSUR), 41(2):10. Alexander Panchenko, Anastasia Lopukhina, Dmitry Ustalov, Konstantin Lopukhin, Nikolay Arefyev, Alexey Leontyev, and Natalia Loukachevitch. 2018. RUSSE\u20192018: A Shared Task on Word Sense In- duction for the Russian Language.",
            "2018. RUSSE\u20192018: A Shared Task on Word Sense In- duction for the Russian Language. In Computa- tional Linguistics and Intellectual Technologies: Pa- pers from the Annual International Conference \u201cDi- alogue\u201d, pages 547\u2013564, Moscow, Russia. RSUH. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. In Proceedings of the 2018 Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long Papers), pages 2227\u20132237, New Orleans, Louisiana. Association for Computational Linguistics. Matthew E. Peters, Sebastian Ruder, and Noah A. Smith. 2019. To tune or not to tune? Adapting pre- trained representations to diverse tasks.",
            "Association for Computational Linguistics. Matthew E. Peters, Sebastian Ruder, and Noah A. Smith. 2019. To tune or not to tune? Adapting pre- trained representations to diverse tasks. In Proceed- ings of the 4th Workshop on Representation Learn- ing for NLP (RepL4NLP-2019), pages 7\u201314, Flo- rence, Italy. Association for Computational Linguis- tics. Hinrich Sch\u00fctze. 1998. Automatic word sense discrim- ination. Computational Linguistics, 24(1):97\u2013123. Milan Straka and Jana Strakov\u00e1. 2017. Tokenizing, POS tagging, lemmatizing and parsing UD 2.0 with UDPipe. In Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Univer- sal Dependencies, pages 88\u201399, Vancouver, Canada. Association for Computational Linguistics."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1909.03135.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 6490.999786376953,
    "avg_doclen_est": 190.91175842285156
}
