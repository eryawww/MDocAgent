[
  "MITIGATING THE IMPACT OF SPEECH RECOGNITION ERRORS ON SPOKEN QUESTION ANSWERING BY ADVERSARIAL DOMAIN ADAPTATION Chia-Hsuan Lee, Yun-Nung Chen, Hung-Yi Lee College of Electrical Engineering and Computer Science National Taiwan University, Taiwan chiahsuan.li@gmail.com , y.v.chen@ieee.org , tlkagkb93901106@gmail.com ABSTRACT Spoken question answering (SQA) is challenging due to com- plex reasoning on top of the spoken documents. The recent studies have also shown the catastrophic impact of automatic speech recognition (ASR) errors on SQA. Therefore, this work proposes to mitigate the ASR errors by aligning the mismatch between ASR hypotheses and their corresponding reference transcriptions. An adversarial model is applied to this domain adaptation task, which forces the model to learn domain-invariant features the QA model can effectively uti- lize in order to improve the SQA results. The experiments successfully demonstrate the effectiveness of our proposed model, and the results are better than the previous best model by 2% EM score. Index Terms\u2014 adversarial learning, spoken question an- swering, SQA, domain adaptation 1.",
  "The experiments successfully demonstrate the effectiveness of our proposed model, and the results are better than the previous best model by 2% EM score. Index Terms\u2014 adversarial learning, spoken question an- swering, SQA, domain adaptation 1. INTRODUCTION Question answering (QA) has drawn a lot of attention in the past few years. QA tasks on images [1] have been widely studied, but most focused on understanding text docu- ments [2]. A representative dataset in text QA is SQuAD [2], in which several end-to-end neural models have accomplished promising performance [3]. Although there is a signi\ufb01cant progress in machine comprehension (MC) on text documents, MC on spoken content is a much less investigated \ufb01eld. In spoken question answering (SQA), after transcribing spoken content into text by automatic speech recognition (ASR), typ- ical approaches use information retrieval (IR) techniques [4] to \ufb01nd the proper answer from the ASR hypotheses. One attempt towards QA of spoken content is TOEFL listening comprehension by machine [5].",
  "One attempt towards QA of spoken content is TOEFL listening comprehension by machine [5]. TOEFL is an English exami- nation that tests the knowledge and skills of academic English for English learners whose native languages are not English. Another SQA corpus is Spoken-SQuAD[6], which is auto- matically generated from SQuAD dataset through Google Text-to-Speech (TTS) system. Recently ODSQA, a SQA corpus recorded by real speakers, is released [7]. To mitigate the impact of speech recognition errors, us- ing sub-word units is a popular approach for speech-related downstream tasks. It has been applied to spoken document retrieval [8] and spoken term detection [9] The prior work showed that, using phonectic sub-word units brought im- provements for both Spoken-SQuAD and ODSQA [6]. Instead of considering sub-word features, this paper pro- poses a novel approach to mitigate the impact of ASR errors. We consider reference transcriptions and ASR hypotheses as two domains, and adapt the source domain data (reference transcriptions) to the target domain data (ASR hypotheses) by projecting these two domains in the shared common space.",
  "We consider reference transcriptions and ASR hypotheses as two domains, and adapt the source domain data (reference transcriptions) to the target domain data (ASR hypotheses) by projecting these two domains in the shared common space. Therefore, it can effectively bene\ufb01t the SQA model by im- proving the robustness to ASR errors in the SQA model. Domain adaptation has been successfully applied on com- puter vision [10] and speech recognition [11]. It is also widely studied on NLP tasks such as sequence tagging and pars- ing [12, 13, 14]. Recently, adversarial domain adaptation has already been explored on spoken language understanding (SLU). Liu and Lane learned domain-general features to ben- e\ufb01t from multiple dialogue datasets [15]; Zhu et al. learned to transfer the model from the transcripts side to the ASR hy- potheses side [16]; Lan et al. constructed a shared space for slot tagging and language model [17]. This paper extends the capability of adversarial domain adaptation for SQA, which has not been explored yet. 2.",
  "constructed a shared space for slot tagging and language model [17]. This paper extends the capability of adversarial domain adaptation for SQA, which has not been explored yet. 2. SPOKEN QUESTION ANSWERING In SQA, each sample is a triple, (q, d, a), where q is a question in either spoken or text form, d is a multi-sentence spoken- form document, and a is the answer in text from. The task of this work is extractive SQA; that means a is a word span from the reference transcription of d. An overview framework of SQA is shown in Figure 1. In this paper, we frame the source domain as reference transcriptions and the target domain as ASR hypotheses. Hence, we can collect source domain data more easily, and adapt the model to the target domain. In this task, when the machine is given a spoken docu- ment, it needs to \ufb01nd the answer of a question from the spo- arXiv:1904.07904v1  [cs.CL]  16 Apr 2019",
  "Predicted  Answer Spoken Question Spoken Document Question  Transcription Document  Transcription ASR QA Model Text Question or Fig. 1. Flow diagram of the SQA system. ken document. SQA can be solved by the concatenation of an ASR module and a question answering module. Given the ASR hypotheses of a spoken document and a question, the question answering module can output a text answer. The most intuitive way to evaluate the text answer is to di- rectly compute the Exact Match (EM) and Macro-averaged F1 scores (F1) between the predicted text answer and the ground-truth text answer. We used the standard evaluation script from SQuAD [2] to evaluate the performance. 3. QUESTION ANSWERING MODEL The used architecture of the QA model is brie\ufb02y summarized below.",
  "We used the standard evaluation script from SQuAD [2] to evaluate the performance. 3. QUESTION ANSWERING MODEL The used architecture of the QA model is brie\ufb02y summarized below. Here we choose QANet [3] as the base model due to the following reasons: 1) it achieves the second best per- formance on SQuAD, and 2) since there are completely no recurrent networks in QANet, its training speed is 5x faster than BiDAF [18] when reaching the same performance on SQuAD. The network architecture is illustrated in Figure 2. The left blocks and the right blocks form two QANets, each of which takes a document and a question as the input and out- puts an answer. In QANet, \ufb01rstly, an embedding encoder ob- tains word and character embeddings for each word in q or d and then models the temporal interactions between words and re\ufb01nes word vectors to contextualized word representations. All encoder blocks used in QANet are composed exclusively of depth-wise separable convolutions and self-attention.",
  "All encoder blocks used in QANet are composed exclusively of depth-wise separable convolutions and self-attention. The intuition here is that convolution components can model lo- cal interactions and self-attention components focus on mod- eling global interactions. The context-query attention layer generates the question-document similarity matrix and com- putes the question-aware vector representations of the context words. After that, a model encoder layer containing seven encoder blocks captures the interactions among the context words conditioned on the question. Finally, the output layer predicts a start position and an end position in the document to extract the answer span from the document. Source  Embedding  Encoder Domain  Discriminator From Source? From Target?",
  "Finally, the output layer predicts a start position and an end position in the document to extract the answer span from the document. Source  Embedding  Encoder Domain  Discriminator From Source? From Target? Source Context  Query Attention  Layer Source Model  Encoder Layer Source Output Layer \ud835\udf13\ud835\udc60\ud835\udc5f\ud835\udc50(\ud835\udc5e) \ud835\udf13\ud835\udc60\ud835\udc5f\ud835\udc50(\ud835\udc51) \ud835\udc44\ud835\udc62\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc5e\ud835\udc37\ud835\udc5c\ud835\udc50\ud835\udc51 Tied / Untied \ud835\udc34\ud835\udc5b\ud835\udc60\ud835\udc64\ud835\udc52\ud835\udc5f\ud835\udc4e Target   Embedding  Encoder Target Context  Query Attention  Layer Target Model  Encoder Layer Target Output Layer \ud835\udf13\ud835\udc61\ud835\udc4e\ud835\udc5f(\ud835\udc5e) \ud835\udf13\ud835\udc61\ud835\udc4e\ud835\udc5f(\ud835\udc51) \ud835\udc44\ud835\udc62\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc5e\ud835\udc37\ud835\udc5c\ud835\udc50\ud835\udc51 \ud835\udc34\ud835\udc5b\ud835\udc60\ud835\udc64\ud835\udc52\ud835\udc5f\ud835\udc4e Fig.",
  "2. The overall architecture of the proposed QA model with a domain discriminator. Each layer can be tied or untied between the source and target models. 4. DOMAIN ADAPTATION APPROACH The main focus of this paper is to apply domain adaptation for SQA. In this approach, we have two SQA models (QANets), one trained from target domain data (ASR hypotheses) and another trained from source domain data (reference transcrip- tions). Because the two domains share common information, some layers in these two models can be tied in order to model the shared features. Hence, we can choose whether each layer in the QA model should be shared. Tying the weights between the source layer and the target layer in order to learn a sym- metric mapping is to project both source and target domain data to a shared common space. Different combinations will be investigated in our experiments.",
  "More speci\ufb01cally, we incorporate a domain discriminator into the SQA model shown in Figure 2, which can enforce the embedding encoder to project the sentences from both source and target domains into a shared common space and conse- quentially to be ASR-error robust. Although the embedding encoder for both domains may implicitly learn some common latent representations, adversarial learning can provide a more direct training signal for aligning the output distribution of the embedding encoder from both domains. The embedding encoder takes in a sequence of word vectors and generates a sequence of hidden vectors with the same length. We use \u03a8tar(q) and \u03a8tar(d) (\u03a8src(q) and \u03a8src(d)) to represent the hid- den vector sequence given the question q and the document d in the target (source) domain respectively. The domain discriminator D focuses on identifying the domain of the vector sequence is from given \u03a8tar or \u03a8src, where the objective is to minimize Ldis.",
  "The domain discriminator D focuses on identifying the domain of the vector sequence is from given \u03a8tar or \u03a8src, where the objective is to minimize Ldis. Ldis = E(q,d,a)\u223ctar [log D(\u03a8tar(q)) + log D(\u03a8tar(d))] (1) +E(q,d,a)\u223csrc [log(1 \u2212D(\u03a8src(q)) + log(1 \u2212D(\u03a8src(d))]. Given a training example from the target domain ((q, d, a) \u223c tar), D learns to assign a lower score to q and d in that exam- ple, that is, to minimize D(\u03a8tar(q)) and D(\u03a8tar(d)). On the other hand, given a training example from the source domain ((q, d, a) \u223csrc), D learns to assign a larger value to q and d. Furthermore, we update the parameters of the embedding encoders to maximize the domain classi\ufb01cation loss Ldis, which works adversarially towards the domain discriminator.",
  "We thus expect the model to learn features and structures that can generalize across domains when the outputs of \u03a8src are indistinguishable from the outputs of \u03a8tar. The loss function for embedding encoder, Lenc, is formulated as Lenc = Lqa \u2212\u03bbGLdis, (2) where \u03bbG is a hyperparameter. The two embedding encoders in the QA model are learned to maximize Ldis while mini- mizing the loss for QA, Lqa. Because the parameters of other layers in QA model are independent to the loss of the do- main discriminator, the loss function of other layers, Lother, is equivalent to Lqa, that is, Lother = Lqa. Although the discriminator is applied to the output of em- bedding encoder in Figure 2, it can be also applied to other layers.1 Considering that almost all QA model contains such embedding encoders, the proposed approach is expected to generalize to other QA models in addition to QANet. 5. EXPERIMENTS 5.1. Corpus Spoken-SQuAD is chosen as the target domain data for train- ing and testing.",
  "5. EXPERIMENTS 5.1. Corpus Spoken-SQuAD is chosen as the target domain data for train- ing and testing. Spoken-SQuAD [6] is an automatically gen- 1In the experiments, we found that applying the domain discriminator to embedding encoders yielded the best performance. Table 1. Illustration of domain mismatch, where the models are trained on the source domain (Text-SQuAD; T-SQuAD) or the target domain (Spoken-SQuAD; S-SQuAD) and then evaluated on both source and target domains. Model T-SQuAD S-SQuAD Training EM F1 EM F1 T-SQuAD (a) 61.31 72.66 42.27 55.61 S-SQuAD (b) 45.52 57.39 48.93 61.20 Finetune (c) 54.83 66.45 49.60 61.85 erated corpus in which the document is in spoken form and the question is in text form. The reference transcriptions are from SQuAD [2].",
  "The reference transcriptions are from SQuAD [2]. There are 37,111 and 5,351 question an- swer pairs in the training and testing sets respectively, and the word error rate (WER) of both sets is around 22.7%. The original SQuAD, Text-SQuAD, is chosen as the source domain data, where only question answering pairs appearing in Spoken-SQuAD are utilized. In our task setting, during training we train the proposed QA model on both Text-SQuAD and Spoken-SQuAD training sets. While in the testing stage, we evaluate the performance on Spoken- SQuAD testing set. 5.2. Experiment Setup We utilize fasttext [19] to generate the embeddings of all words from both Text-SQuAD and Spoken-SQuAD. We adopt the phoneme sequence embeddings to replace the orig- inal character sequence embeddings using the method pro- posed by Li et al. [6]. The source domain model and the target domain model share the same set of word embedding matrix to improve the alignment between these two domains.",
  "We adopt the phoneme sequence embeddings to replace the orig- inal character sequence embeddings using the method pro- posed by Li et al. [6]. The source domain model and the target domain model share the same set of word embedding matrix to improve the alignment between these two domains. W-GAN is adopted for our domain discriminator [20], which stacks 5 residual blocks of 1D convolutional layers with 96 \ufb01lters and \ufb01lter size 5 followed by one linear layer to convert each input vector sequence into one scalar value. All models used in the experiments are trained with batch size 20, using adam with learning rate 1e \u22123 and the early stop strategy. The dimension of the hidden state is set to 96 for all layers, and the number of self-attention heads is set to 2. The setup is slightly different but better than the setting suggested by the original QAnet. 5.3. Results 5.3.1. Domain Mismatch First, we highlight the domain mismatch phenomenon in our experiments shown in Table 1.",
  "The setup is slightly different but better than the setting suggested by the original QAnet. 5.3. Results 5.3.1. Domain Mismatch First, we highlight the domain mismatch phenomenon in our experiments shown in Table 1. Row (a) is when QANet is trained on Text-SQuAD, row (b) is when QANet is trained on Spoken-SQuAD, and row (c) is when QANet is trained on Text-SQuAD and then \ufb01netuned on Spoken-SQuAD. The",
  "Table 2. The EM/F1 scores of proposed adversarial domain adaptation approaches over Spoken-SQuAD. Model EM F1 Baseline S-SQuAD (a) 48.93 61.20 Finetune (b) 49.60 61.85 Li et al. [6] (c) 49.07 61.16 Adverarial Lan et al. [17] (d) 49.13 61.80 Completely Shared (e) 49.57 61.48 (e) + GAN on Embedding (f) 51.10 63.11 (e) + GAN on Attention (g) 48.30 61.11 columns show the evaluation on the testing sets of Text- SQuAD and Spoken-SQuAD. It is clear that the performance drops a lot when the train- ing and testing data mismatch, indicating that model train- ing on ASR hypotheses can not generalize well on reference transcriptions. The performance gap is nearly 20% F1 score (72% to 55%).",
  "It is clear that the performance drops a lot when the train- ing and testing data mismatch, indicating that model train- ing on ASR hypotheses can not generalize well on reference transcriptions. The performance gap is nearly 20% F1 score (72% to 55%). The row (c) shows the improved performance when testing on S-SQuAD due to the transfer learning via \ufb01ne-tuning. 5.3.2. Effectiveness of Adversarial Domain Adaptation To better demonstrate the effectiveness of the proposed model, we compare with baselines and show the results in Table 2. The baselines are: (a) trained on S-SQuAD, (b) trained on T-SQuAD and then \ufb01ne-tuned on S-SQuAD, and (c) previous best model trained on S-SQuAD [6] by using Dr.QA [21]. We also compare to the approach proposed by Lan et al. [17] in the row (d). This approach is originally proposed for spoken language understanding, and we adopt the same approach on the setting here.",
  "We also compare to the approach proposed by Lan et al. [17] in the row (d). This approach is originally proposed for spoken language understanding, and we adopt the same approach on the setting here. The approach models domain-speci\ufb01c features from the source and target domains separately by two different embedding encoders with a shared embedding encoder for modeling domain-general features. The domain-general parameters are adversarially trained by domain discriminator. Row (e) is the model that the weights of all layers are tied between the source domain and the target domain. Row (f) uses the same architecture as row (e) with an additional do- main discriminator applied to the embedding encoder. It can be found that row (f) outperforms row (e), indicating that the proposed domain adversarial learning is helpful. Therefore, our following experiments contain domain adversarial learn- ing. The proposed approach (row (f)) outperforms previous best model (row (c)) by 2% EM score and over 1.5% F1 score. We also show the results of applying the domain discriminator to the top of context query attention layer in row (g), which obtains poor performance.",
  "We also show the results of applying the domain discriminator to the top of context query attention layer in row (g), which obtains poor performance. To sum it up, incorporating adver- sarial learning by applying the domain discriminator on top Table 3. Investigation of different layer tying mechanisms, where \u2713means that weights of the layer are tied between the source model and the target model. (L1: embedding encoder, L2: context query attention layer, L3: model encoder layer, L4: output layer.) Combination L1 L2 L3 L4 EM F1 (a) \u2713 \u2713 \u2713 \u2713 51.10 63.11 (b) - \u2713 \u2713 \u2713 50.25 62.41 (c) - - \u2713 \u2713 49.72 61.97 (d) - \u2713 - \u2713 48.83 61.80 (e) - \u2713 \u2713 - 51.09 62.97 (f) \u2713 - - \u2713 49.01 61.40 (g) \u2713 - \u2713 - 49.28 61.71 (h) \u2713 \u2713 - - 49.61 61.72 of the embedding encoder layer is effective.",
  "5.3.3. Which Layer to Share? Layer weight tying or untying within the model indicates dif- ferent levels of symmetric mapping between the source and target domains. Different combinations are investigated and shown in Table 3. The row (a) in which all layers are tied is the row (e) of Table 2. The results show that untying context- query attention layer L2 (rows (c, f, g)) or model encoder layer L3 (rows (d, f, h)) lead to degenerated solutions in com- parison to row (a) where all layers are tied. Untying both of them simultaneously leads to the worst performance which is even worse than the \ufb01netuning (row (g) v.s. (c) from Table 2). These results imply that sharing the context-query attention layer and the model encoder layer are important for domain adaptation on SQA. We conjecture that these two layers ben- e\ufb01t from training on source domain data where there are no ASR errors, so the QA model learns to conduct attention or further reason well on target domain data with ASR errors.",
  "We conjecture that these two layers ben- e\ufb01t from training on source domain data where there are no ASR errors, so the QA model learns to conduct attention or further reason well on target domain data with ASR errors. Overall, it is not bene\ufb01cial to untie any layer, because no information can be shared across different domains. Untying the embedding encoder L1 and the output layer L4 leads to the least degradation in comparison to row (a). 6. CONCLUSION In this work, we incorporate a domain discriminator to align the mismatched domains between ASR hypotheses and refer- ence transcriptions. The adversarial learning allows the end- to-end QA model to learn domain-invariant features and im- prove the robustness to ASR errors. The experiments demon- strate that the proposed model successfully achieves superior performance and outperforms the previous best model by 2% EM score and over 1.5% F1 score.",
  "7. REFERENCES [1] C Lawrence Zitnick, Ramakrishna Vedantam, and Devi Parikh, \u201cAdopting abstract images for semantic scene understanding,\u201d IEEE transactions on pattern analysis and machine intelligence, vol. 38, no. 4, pp. 627\u2013638, 2016. [2] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang, \u201cSquad: 100,000+ questions for machine comprehension of text,\u201d arXiv preprint arXiv:1606.05250, 2016. [3] Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le, \u201cQanet: Combining local convolution with global self- attention for reading comprehension,\u201d arXiv preprint arXiv:1804.09541, 2018.",
  "[4] Sz-Rung Shiang, Hung-yi Lee, and Lin-shan Lee, \u201cSpo- ken question answering using tree-structured condi- tional random \ufb01elds and two-layer random walk,\u201d in Fifteenth Annual Conference of the International Speech Communication Association, 2014. [5] Bo-Hsiang Tseng, Sheng-Syun Shen, Hung-Yi Lee, and Lin-Shan Lee, \u201cTowards machine comprehension of spoken content: Initial toe\ufb02listening comprehension test by machine,\u201d arXiv preprint arXiv:1608.06378, 2016. [6] Chia-Hsuan Li, Szu-Lin Wu, Chi-Liang Liu, and Hung- yi Lee, \u201cSpoken squad: A study of mitigating the im- pact of speech recognition errors on listening compre- hension,\u201d arXiv preprint arXiv:1804.00320, 2018.",
  "[7] Chia-Hsuan Lee, Shang-Ming Wang, Huan-Cheng Chang, and Hung-Yi Lee, \u201cOdsqa: Open-domain spoken question answering dataset,\u201d arXiv preprint arXiv:1808.02280, 2018. [8] Kenney Ng and Victor W Zue, \u201cSubword unit represen- tations for spoken document retrieval,\u201d in Fifth Euro- pean Conference on Speech Communication and Tech- nology, 1997. [9] Charl van Heerden, Damianos Karakos, Karthik Narasimhan, Marelie Davel, and Richard Schwartz, \u201cConstructing sub-word units for spoken term detec- tion,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on. IEEE, 2017, pp. 5780\u20135784.",
  "IEEE, 2017, pp. 5780\u20135784. [10] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas- cal Germain, Hugo Larochelle, Franc\u00b8ois Laviolette, Mario Marchand, and Victor Lempitsky, \u201cDomain- adversarial training of neural networks,\u201d The Journal of Machine Learning Research, vol. 17, no. 1, pp. 2096\u2013 2030, 2016. [11] Yusuke Shinohara, \u201cAdversarial multi-task learning of deep neural networks for robust speech recognition.,\u201d in INTERSPEECH, 2016, pp. 2369\u20132372. [12] Zhilin Yang, Ruslan Salakhutdinov, and William W Cohen, \u201cTransfer learning for sequence tagging with hierarchical recurrent networks,\u201d arXiv preprint arXiv:1703.06345, 2017.",
  "2369\u20132372. [12] Zhilin Yang, Ruslan Salakhutdinov, and William W Cohen, \u201cTransfer learning for sequence tagging with hierarchical recurrent networks,\u201d arXiv preprint arXiv:1703.06345, 2017. [13] David McClosky, Eugene Charniak, and Mark Johnson, \u201cAutomatic domain adaptation for parsing,\u201d in Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computa- tional Linguistics, 2010, pp. 28\u201336. [14] Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao Li, Frederick Reiss, and Shivakumar Vaithyanathan, \u201cDo- main adaptation of rule-based annotators for named- entity recognition tasks,\u201d in Proceedings of the 2010 conference on empirical methods in natural language processing. Association for Computational Linguistics, 2010, pp. 1002\u20131012.",
  "Association for Computational Linguistics, 2010, pp. 1002\u20131012. [15] Bing Liu and Ian Lane, \u201cMulti-domain adversarial learning for slot \ufb01lling in spoken language understand- ing,\u201d arXiv preprint arXiv:1711.11310, 2017. [16] Su Zhu, Ouyu Lan, and Kai Yu, \u201cRobust spoken lan- guage understanding with unsupervised asr-error adap- tation,\u201d in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 6179\u20136183. [17] Ouyu Lan, Su Zhu, and Kai Yu, \u201cSemi-supervised training using adversarial multi-task learning for spoken language understanding,\u201d in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 6049\u20136053.",
  "IEEE, 2018, pp. 6049\u20136053. [18] Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi, \u201cBidirectional attention \ufb02ow for machine comprehension,\u201d arXiv preprint arXiv:1611.01603, 2016. [19] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov, \u201cEnriching word vectors with subword information,\u201d arXiv preprint arXiv:1607.04606, 2016. [20] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vin- cent Dumoulin, and Aaron C Courville, \u201cImproved training of wasserstein gans,\u201d in Advances in Neural Information Processing Systems, 2017, pp. 5767\u20135777.",
  "[21] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes, \u201cReading wikipedia to answer open-domain questions,\u201d arXiv preprint arXiv:1704.00051, 2017."
]