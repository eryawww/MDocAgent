[
  "Compound Probabilistic Context-Free Grammars for Grammar Induction Yoon Kim Harvard University Cambridge, MA, USA yoonkim@seas.harvard.edu Chris Dyer DeepMind London, UK cdyer@google.com Alexander M. Rush Harvard University Cambridge, MA, USA srush@seas.harvard.edu Abstract We study a formalization of the grammar induction problem that models sentences as being generated by a compound probabilis- tic context-free grammar. In contrast to traditional formulations which learn a sin- gle stochastic grammar, our grammar\u2019s rule probabilities are modulated by a per-sentence continuous latent variable, which induces marginal dependencies beyond the traditional context-free assumptions. Inference in this grammar is performed by collapsed variational inference, in which an amortized variational posterior is placed on the continuous variable, and the latent trees are marginalized out with dynamic programming. Experiments on En- glish and Chinese show the effectiveness of our approach compared to recent state-of-the- art methods when evaluated on unsupervised parsing. 1 Introduction Grammar induction is the task of inducing hier- archical syntactic structure from data. Statistical approaches to grammar induction require specify- ing a probabilistic grammar (e.g.",
  "1 Introduction Grammar induction is the task of inducing hier- archical syntactic structure from data. Statistical approaches to grammar induction require specify- ing a probabilistic grammar (e.g. formalism, num- ber and shape of rules), and \ufb01tting its parameters through optimization. Early work found that it was dif\ufb01cult to induce probabilistic context-free gram- mars (PCFG) from natural language data through direct methods, such as optimizing the log like- lihood with the EM algorithm (Lari and Young, 1990; Carroll and Charniak, 1992). While the rea- sons for the failure are manifold and not com- pletely understood, two major potential causes are the ill-behaved optimization landscape and the overly strict independence assumptions of PCFGs.",
  "While the rea- sons for the failure are manifold and not com- pletely understood, two major potential causes are the ill-behaved optimization landscape and the overly strict independence assumptions of PCFGs. More successful approaches to grammar induction have thus resorted to carefully-crafted auxiliary objectives (Klein and Manning, 2002), priors or Code: https://github.com/harvardnlp/compound-pcfg non-parametric models (Kurihara and Sato, 2006; Johnson et al., 2007; Liang et al., 2007; Wang and Blunsom, 2013), and manually-engineered fea- tures (Huang et al., 2012; Golland et al., 2012) to encourage the desired structures to emerge. We revisit these aforementioned issues in light of advances in model parameterization and infer- ence. First, contrary to common wisdom, we \ufb01nd that parameterizing a PCFG\u2019s rule probabil- ities with neural networks over distributed rep- resentations makes it possible to induce linguis- tically meaningful grammars by simply optimiz- ing log likelihood.",
  "First, contrary to common wisdom, we \ufb01nd that parameterizing a PCFG\u2019s rule probabil- ities with neural networks over distributed rep- resentations makes it possible to induce linguis- tically meaningful grammars by simply optimiz- ing log likelihood. While the optimization prob- lem remains non-convex, recent work suggests that there are optimization bene\ufb01ts afforded by over-parameterized models (Arora et al., 2018; Xu et al., 2018; Du et al., 2019), and we in- deed \ufb01nd that this neural PCFG is signi\ufb01cantly easier to optimize than the traditional PCFG. Second, this factored parameterization makes it straightforward to incorporate side information into rule probabilities through a sentence-level continuous latent vector, which effectively allows different contexts in a derivation to coordinate. In this compound PCFG\u2014continuous mixture of PCFGs\u2014the context-free assumptions hold con- ditioned on the latent vector but not uncondition- ally, thereby obtaining longer-range dependencies within a tree-based generative process. To utilize this approach, we need to ef\ufb01ciently optimize the log marginal likelihood of observed sentences.",
  "To utilize this approach, we need to ef\ufb01ciently optimize the log marginal likelihood of observed sentences. While compound PCFGs break ef\ufb01- cient inference, if the latent vector is known the distribution over trees reduces to a standard PCFG. This property allows us to perform grammar in- duction using a collapsed approach where the la- tent trees are marginalized out exactly with dy- namic programming. To handle the latent vec- tor, we employ standard amortized inference us- ing reparameterized samples from a variational arXiv:1906.10225v9  [cs.CL]  29 Mar 2020",
  "posterior approximated from an inference network (Kingma and Welling, 2014; Rezende et al., 2014). On standard benchmarks for English and Chi- nese, the proposed approach is found to perform favorably against recent neural approaches to un- supervised parsing (Shen et al., 2018, 2019; Droz- dov et al., 2019; Kim et al., 2019). 2 Probabilistic Context-Free Grammars We consider context-free grammars (CFG) con- sisting of a 5-tuple G = (S, N, P, \u03a3, R) where S is the distinguished start symbol, N is a \ufb01nite set of nonterminals, P is a \ufb01nite set of pretermi- nals,1 \u03a3 is a \ufb01nite set of terminal symbols, and R is a \ufb01nite set of rules of the form, S \u2192A, A \u2208N A \u2192B C, A \u2208N, B, C \u2208N \u222aP T \u2192w, T \u2208P, w \u2208\u03a3.",
  "A probabilistic context-free grammar (PCFG) consists of a grammar G and rule probabilities \u03c0 = {\u03c0r}r\u2208R such that \u03c0r is the probability of the rule r. Letting TG be the set of all parse trees of G, a PCFG de\ufb01nes a probability distribution over t \u2208TG via p\u03c0(t) = Q r\u2208tR \u03c0r where tR is the set of rules used in the derivation of t. It also de\ufb01nes a distribution over string of terminals x \u2208\u03a3\u2217via p\u03c0(x) = X t\u2208TG(x) p\u03c0(t), where TG(x) = {t | yield(t) = x}, i.e.",
  "the set of trees t such that t\u2019s leaves are x. We will slightly abuse notation and use p\u03c0(t | x) \u225c1[yield(t) = x]p\u03c0(t) p\u03c0(x) to denote the posterior distribution over the unob- served latent trees given the observed sentence x, where 1[\u00b7] is the indicator function.2 2.1 Parameterization The standard way to parameterize a PCFG is to simply associate a scalar to each rule \u03c0r with the 1Since we will be inducing a grammar directly from words, P is roughly the set of part-of-speech tags and N is the set of constituent labels. However, to avoid issues of label alignment, evaluation is only on the tree topology. 2Therefore when used in the context of a posterior distri- bution conditioned on a sentence x, the variable t does not include the leaves x and only refers to the unobserved non- terminal/preterminal symbols. constraint that they form valid probability distri- butions, i.e. each nonterminal is associated with a fully-parameterized categorical distribution over its rules.",
  "constraint that they form valid probability distri- butions, i.e. each nonterminal is associated with a fully-parameterized categorical distribution over its rules. This direct parameterization is algorith- mically convenient since the M-step in the EM al- gorithm (Dempster et al., 1977) has a closed form. However, there is a long history of work showing that it is dif\ufb01cult to learn meaningful grammars from natural language data with this parameteri- zation (Carroll and Charniak, 1992).3 Successful approaches to unsupervised parsing have therefore modi\ufb01ed the model/learning objective by guiding potentially unrelated rules to behave similarly. Recognizing that sharing among rule types is bene\ufb01cial, we propose a neural parameterization where rule probabilities are based on distributed representations. We associate embeddings with each symbol, introducing input embeddings wN for each symbol N on the left side of a rule (i.e. N \u2208{S} \u222aN \u222aP).",
  "We associate embeddings with each symbol, introducing input embeddings wN for each symbol N on the left side of a rule (i.e. N \u2208{S} \u222aN \u222aP). For each rule type r, \u03c0r is parameterized as follows, \u03c0S\u2192A = exp(u\u22a4 A f1(wS) + bA) P A\u2032\u2208N exp(u\u22a4 A\u2032 f1(wS) + bA\u2032), \u03c0A\u2192BC = exp(u\u22a4 BC wA + bBC) P B\u2032C\u2032\u2208M exp(u\u22a4 B\u2032C\u2032 wA + bB\u2032C\u2032), \u03c0T\u2192w = exp(u\u22a4 w f2(wT ) + bw) P w\u2032\u2208\u03a3 exp(u\u22a4 w\u2032 f2(wT ) + bw\u2032), where M is the product space (N \u222aP)\u00d7(N \u222aP), and f1, f2 are MLPs with two residual layers. Note that we do not use an MLP for rules of the type \u03c0A\u2192BC, as it did not empirically improve re- sults.",
  "Note that we do not use an MLP for rules of the type \u03c0A\u2192BC, as it did not empirically improve re- sults. See appendix A.1 for the full parameteriza- tion. Going forward, we will use EG = {wU | U \u2208 {S} \u222aN \u222aP} \u222a{uV | V \u2208N \u222aM \u222a\u03a3} to denote the set of input/output symbol embeddings for grammar G, and \u03bb to refer to the parameters of the neural network f1, f2 used to obtain the rule probabilities. A graphical model-like illustration of the neural PCFG is shown in Figure 1 (left). It is clear that the neural parameterization does not change the underlying probabilistic assump- tions. The difference between the two is anal- ogous to the difference between count-based vs. feed-forward neural language models, where feed- forward neural language models make the same Markov assumptions as the count-based models but are able to take advantage of shared, dis- tributed representations. 3In preliminary experiments we were indeed unable to learn linguistically meaningful grammars with this PCFG.",
  "A1 A2 T3 T1 T2 w1 w2 w3 \u03c0S \u03c0N \u03c0P EG N A1 A2 T3 T1 T2 w1 w2 w3 z \u03b3 c \u03c0z,S \u03c0z,N \u03c0z,P EG N Figure 1: A graphical model-like diagram for the neural PCFG (left) and the compound PCFG (right) for an example tree structure. In the above, A1, A2 \u2208N are nonterminals, T1, T2, T3 \u2208P are preterminals, w1, w2, w3 \u2208\u03a3 are terminals. In the neural PCFG, the global rule probabilities \u03c0 = \u03c0S \u222a\u03c0N \u222a\u03c0P are the output from a neural net run over the symbol embeddings EG, where \u03c0N are the set of rules with a nonterminal on the left hand side (\u03c0S and \u03c0P are similarly de\ufb01ned). In the compound PCFG, we have per-sentence rule probabilities \u03c0z = \u03c0z,S \u222a\u03c0z,N \u222a\u03c0z,P obtained from running a neural net over a random vector z (which varies across sentences) and global symbol embeddings EG.",
  "In the compound PCFG, we have per-sentence rule probabilities \u03c0z = \u03c0z,S \u222a\u03c0z,N \u222a\u03c0z,P obtained from running a neural net over a random vector z (which varies across sentences) and global symbol embeddings EG. In this case, the context-free assumptions hold conditioned on z, but they do not hold unconditionally: e.g. when conditioned on z and A2, the variables A1 and T1 are independent; however when conditioned on just A2, they are not independent due to the dependence path through z. Note that the rule probabilities are random variables in the compound PCFG but deterministic variables in the neural PCFG. 3 Compound PCFGs A compound probability distribution (Robbins, 1951) is a distribution whose parameters are them- selves random variables. These distributions gen- eralize mixture models to the continuous case, for example in factor analysis which assumes the fol- lowing generative process, z \u223cN(0, I), x \u223cN(Wz, \u03a3). Compound distributions provide the ability to model rich generative processes, but marginaliz- ing over the latent parameter can be computation- ally intractable unless conjugacy can be exploited.",
  "Compound distributions provide the ability to model rich generative processes, but marginaliz- ing over the latent parameter can be computation- ally intractable unless conjugacy can be exploited. In this work, we study compound probabilis- tic context-free grammars whose distribution over trees arises from the following generative process: we \ufb01rst obtain rule probabilities via z \u223cp\u03b3(z), \u03c0z = f\u03bb(z, EG), where p\u03b3(z) is a prior with parameters \u03b3 (spheri- cal Gaussian in this paper), and f\u03bb is a neural net- work that concatenates the input symbol embed- dings with z and outputs the sentence-level rule probabilities \u03c0z, \u03c0z,S\u2192A \u221dexp(u\u22a4 A f1([wS; z]) + bA), \u03c0z,A\u2192BC \u221dexp(u\u22a4 BC [wA; z] + bBC), \u03c0z,T\u2192w \u221dexp(u\u22a4 w f2([wT ; z]) + bw), where [w; z] denotes vector concatenation.",
  "Then a tree/sentence is sampled from a PCFG with rule probabilities given by \u03c0z, t \u223cPCFG(\u03c0z), x = yield(t). This can be viewed as a continuous mixture of PCFGs, or alternatively, a Bayesian PCFG with a prior on sentence-level rule probabilities parame- terized by z, \u03bb, EG.4 Importantly, under this gen- erative model the context-free assumptions hold conditioned on z, but they do not hold uncondi- tionally. This is shown in Figure 1 (right) where there is a dependence path through z if it is not conditioned upon. Compound PCFGs give rise to a marginal distribution over parse trees t via p\u03b8(t) = Z p(t | z)p\u03b3(z) dz, where p\u03b8(t | z) = Q r\u2208tR \u03c0z,r. The subscript in \u03c0z,r denotes the fact that the rule probabilities de- pend on z. Compound PCFGs are clearly more ex- pressive than PCFGs as each sentence has its own set of rule probabilities. However, it still assumes a tree-based generative process, making it possible to learn latent tree structures.",
  "However, it still assumes a tree-based generative process, making it possible to learn latent tree structures. One motivation for the compound PCFG is that simple, unlexicalized grammars (such as the PCFG we have been working with) are unlikely to represent an adequate model of natural lan- guage, although they do facilitate ef\ufb01cient learn- 4Under the Bayesian PCFG view, p\u03b3(z) is a distribution over z (a subset of the prior), and is thus a hyperprior.",
  "ing and inference.5 We can in principle model richer dependencies through vertical/horizontal Markovization (Johnson, 1998; Klein and Man- ning, 2003) and lexicalization (Collins, 1997). However such dependencies complicate training due to the rapid increase in the number of rules. Under this view, we can interpret the compound PCFG as a restricted version of some lexicalized, higher-order PCFG where a child can depend on structural and lexical context through a shared la- tent vector.6 We hypothesize that this dependence among siblings is especially useful in grammar in- duction from words, where (for example) if we know that watched is used as a verb then the noun phrase is likely to be a movie. In contrast to the usual Bayesian treatment of PCFGs which places priors on global rule proba- bilities (Kurihara and Sato, 2006; Johnson et al., 2007; Wang and Blunsom, 2013), the compound PCFG assumes a prior on local, sentence-level rule probabilities. It is therefore closely related to the Bayesian grammars studied by Cohen et al.",
  "It is therefore closely related to the Bayesian grammars studied by Cohen et al. (2009) and Cohen and Smith (2009), who also sample local rule probabilities from a logistic nor- mal prior for training dependency models with va- lence (DMV) (Klein and Manning, 2004). 3.1 Inference in Compound PCFGs The expressivity of compound PCFGs comes at a signi\ufb01cant challenge in learning and inference. Letting \u03b8 = {EG, \u03bb} be the parameters of the generative model, we would like to maximize the log marginal likelihood of the observed sentence log p\u03b8(x). In the neural PCFG the log marginal likelihood log p\u03b8(x) = log X t\u2208TG(x) p\u03b8(t), 5A piece of evidence for the misspeci\ufb01cation of unlexi- calized \ufb01rst-order PCFGs as a statistical model of natural lan- guage is that if one pretrains such a PCFG on supervised data and continues training with the unsupervised objective (i.e.",
  "log marginal likelihood), the resulting grammar deviates sig- ni\ufb01cantly from the supervised initial grammar while the log marginal likelihood improves (Johnson et al., 2007). Simi- lar observations have been made for part-of-speech induction with Hidden Markov Models (Merialdo, 1994). 6Note that the compound \u201cPCFG\u201d is a slight misnomer because the model is no longer context-free in the usual sense. Another interpretation of the model is to view it as a vectorized version of indexed grammars (Aho, 1968), which extend CFGs by augmenting nonterminals with additional in- dex strings that may be inherited or modi\ufb01ed during deriva- tion. Compound PCFGs instead equip nonterminals with a continuous vector that is always inherited.",
  "Compound PCFGs instead equip nonterminals with a continuous vector that is always inherited. can be obtained by summing out the latent tree structure using the inside algorithm (Baker, 1979), which is differentiable and thus amenable to gradient-based optimization.7 In the compound PCFG, the log marginal likelihood is given by, log p\u03b8(x) = log \u0010 Z p\u03b8(x | z)p\u03b3(z) dz \u0011 = log \u0010 Z X t\u2208TG(x) p\u03b8(t | z)p\u03b3(z) dz \u0011 . Notice that while the integral over z makes this quantity intractable, when we condition on z, we can tractably perform the inner summation to ob- tain p\u03b8(x | z) using the inside algorithm. We there- fore resort to collapsed amortized variational in- ference. We \ufb01rst obtain a sample z from a vari- ational posterior distribution (given by an amor- tized inference network), then perform the inner marginalization conditioned on this sample.",
  "We there- fore resort to collapsed amortized variational in- ference. We \ufb01rst obtain a sample z from a vari- ational posterior distribution (given by an amor- tized inference network), then perform the inner marginalization conditioned on this sample. The evidence lower bound ELBO(\u03b8, \u03c6; x) is then, Eq\u03c6(z | x)[log p\u03b8(x | z)] \u2212KL[q\u03c6(z | x) \u2225p\u03b3(z)], and we can calculate p\u03b8(x | z) given a sample z from a variational posterior q\u03c6(z | x). For the vari- ational family we use a diagonal Gaussian where the mean/log-variance vectors are given by an af\ufb01ne layer over max-pooled hidden states from an LSTM over x. We can obtain low-variance esti- mators for \u2207\u03b8,\u03c6 ELBO(\u03b8, \u03c6; x) by using the repa- rameterization trick for the expected reconstruc- tion likelihood and the analytical expression for the KL term (Kingma and Welling, 2014). We remark that under the Bayesian PCFG view, since the parameters of the prior (i.e.",
  "We remark that under the Bayesian PCFG view, since the parameters of the prior (i.e. \u03b8) are esti- mated from the data, our approach can be seen as an instance of empirical Bayes (Robbins, 1956).8 3.2 MAP Inference After training, we are interested in comparing the learned trees against an annotated treebank. This requires inferring the most likely tree given a sen- tence, i.e. argmaxt p\u03b8(t | x). For the neural PCFG we can obtain the most likely tree by using 7In the context of the EM algorithm, directly per- forming gradient ascent on the log marginal likelihood is equivalent to performing an exact E-step (with the inside- outside algorithm) followed by a gradient-based M-step, i.e. \u2207\u03b8 log p\u03b8(x) = Ep\u03b8(t | x)[\u2207\u03b8 log p\u03b8(t)] (Salakhutdinov et al., 2003; Berg-Kirkpatrick et al., 2010; Eisner, 2016).",
  "\u2207\u03b8 log p\u03b8(x) = Ep\u03b8(t | x)[\u2207\u03b8 log p\u03b8(t)] (Salakhutdinov et al., 2003; Berg-Kirkpatrick et al., 2010; Eisner, 2016). 8See Berger (1985) (chapter 4), Zhang (2003), and Co- hen (2016) (chapter 3) for further discussion on compound models and empirical Bayes.",
  "the Viterbi version of the inside algorithm (CKY algorithm). For the compound PCFG, the argmax is intractable to obtain exactly, and hence we esti- mate it with the following approximation, argmax t Z p\u03b8(t | x, z)p\u03b8(z | x) dz = argmax t p\u03b8 \u0000t | x, \u00b5\u03c6(x) \u0001 , where \u00b5\u03c6(x) is the mean vector from the infer- ence network. The above approximates the true posterior p\u03b8(z | x) with \u03b4(z \u2212\u00b5\u03c6(x)), the Dirac delta function at the mode of the variational pos- terior.9 This quantity is tractable as in the PCFG case. Other approximations are possible: for ex- ample we could use q\u03c6(z | x) as an importance sampling distribution to estimate the \ufb01rst integral. However we found the above approximation to be ef\ufb01cient and effective in practice.",
  "Other approximations are possible: for ex- ample we could use q\u03c6(z | x) as an importance sampling distribution to estimate the \ufb01rst integral. However we found the above approximation to be ef\ufb01cient and effective in practice. 4 Experimental Setup 4.1 Data We test our approach on the Penn Treebank (PTB) (Marcus et al., 1993) with the standard splits (2-21 for training, 22 for validation, 23 for test) and the same preprocessing as in recent works (Shen et al., 2018, 2019), where we discard punctuation, low- ercase all tokens, and take the top 10K most fre- quent words as the vocabulary. This setup is more challenging than traditional setups, which usually experiment on shorter sentences and use gold part- of-speech tags. We further experiment on Chinese with version 5.1 of the Chinese Penn Treebank (CTB) (Xue et al., 2005), with the same splits as in Chen and Manning (2014). On CTB we also remove punc- tuation and keep the top 10K word types.",
  "On CTB we also remove punc- tuation and keep the top 10K word types. 4.2 Hyperparameters Our PCFG uses 30 nonterminals and 60 pretermi- nals, with 256-dimensional symbol embeddings. The compound PCFG uses 64-dimensional latent vectors. The bidirectional LSTM inference net- work has a single layer with 512 dimensions, and the mean and the log variance vector for q\u03c6(z | x) are given by max-pooling the hidden states of the LSTM and passing it through an af\ufb01ne layer. Model parameters are initialized with Xavier uni- form initialization. For training we use Adam 9Since p\u03b8(t | x, z) is continuous with respect to z, we have R p\u03b8(t | x, z)\u03b4(z \u2212\u00b5\u03c6(x)) dz = p\u03b8 \u0000t | x, \u00b5\u03c6(x) \u0001 . (Kingma and Ba, 2015) with \u03b21 = 0.75, \u03b22 = 0.999 and learning rate of 0.001, with a maxi- mum gradient norm limit of 3.",
  "(Kingma and Ba, 2015) with \u03b21 = 0.75, \u03b22 = 0.999 and learning rate of 0.001, with a maxi- mum gradient norm limit of 3. We train for 10 epochs with batch size equal to 4. We employ a curriculum learning strategy (Bengio et al., 2009) where we train only on sentences of length up to 30 in the \ufb01rst epoch, and increase this length limit by 1 each epoch. Similar curriculum-based strate- gies have used in the past for grammar induction (Spitkovsky et al., 2012). During training we per- form early stopping based on validation perplex- ity.10 Finally, to mitigate against over\ufb01tting to PTB, experiments on CTB utilize the same hyper- parameters from PTB. 4.3 Baselines and Evaluation While we induce a full stochastic grammar (i.e. a distribution over symbolic rewrite rules) in this work, directly assessing the learned grammar is it- self nontrivial.",
  "4.3 Baselines and Evaluation While we induce a full stochastic grammar (i.e. a distribution over symbolic rewrite rules) in this work, directly assessing the learned grammar is it- self nontrivial. As a proxy, we adopt the usual ap- proach and instead evaluate the induced grammar as an unsupervised parsing system. However, even in this setting we observe that there is enough vari- ation across prior work on to render a meaningful comparison dif\ufb01cult. In particular, some important dimensions along which prior works vary include, (1) input data: earlier work on generally assumed gold (or in- duced) part-of-speech tags (Klein and Manning, 2004; Smith and Eisner, 2004; Bod, 2006; Sny- der et al., 2009), while more recent works induce grammar directly from words (Spitkovsky et al., 2013; Shen et al., 2018); (2) use of punctuation: even within papers that induce parse trees directly from words, some papers employ heuristics based on punctuation as punctuation is usually a strong signal for start/end of constituents (Seginer, 2007;",
  ", 2013; Shen et al., 2018); (2) use of punctuation: even within papers that induce parse trees directly from words, some papers employ heuristics based on punctuation as punctuation is usually a strong signal for start/end of constituents (Seginer, 2007; Ponvert et al., 2011; Spitkovsky et al., 2013), some train with punctuation (Jin et al., 2018; Drozdov et al., 2019; Kim et al., 2019), while others discard punctuation altogether for training (Shen et al., 2018, 2019); (3) train/test data: some works do not explicitly separate out train/test sets (Reichart and Rappoport, 2010; Golland et al., 2012) while some do (Huang et al., 2012; Parikh et al., 2014; Htut et al., 2018). Maintaining train/test splits is 10However, we used F1 against validation trees on PTB to select some hyperparameters (e.g.",
  ", 2012; Parikh et al., 2014; Htut et al., 2018). Maintaining train/test splits is 10However, we used F1 against validation trees on PTB to select some hyperparameters (e.g. grammar size), as is some- times done in grammar induction. Hence our PTB results are arguably not fully unsupervised in the strictest sense of the term. The hyperparameters of the PRPN/ON baselines are also tuned using validation F1 for fair comparison.",
  "PTB CTB Model Mean Max Mean Max PRPN (Shen et al., 2018) 37.4 38.1 \u2212 \u2212 ON (Shen et al., 2019) 47.7 49.4 \u2212 \u2212 URNNG\u2020 (Kim et al., 2019) \u2212 45.4 \u2212 \u2212 DIORA\u2020 (Drozdov et al., 2019) \u2212 58.9 \u2212 \u2212 Left Branching 8.7 9.7 Right Branching 39.5 20.0 Random Trees 19.2 19.5 15.7 16.0 PRPN (tuned) 47.3 47.9 30.4 31.5 ON (tuned) 48.1 50.0 25.4 25.7 Scalar PCFG < 35.0 < 15.0 Neural PCFG 50.8 52.6 25.7 29.5 Compound PCFG 55.2 60.1 36.0 39.8 Oracle Trees 84.3 81.",
  "7 Scalar PCFG < 35.0 < 15.0 Neural PCFG 50.8 52.6 25.7 29.5 Compound PCFG 55.2 60.1 36.0 39.8 Oracle Trees 84.3 81.1 Table 1: Unlabeled sentence-level F1 scores on PTB and CTB test sets. Top shows results from previous work while the rest of the results are from this paper. Mean/Max scores are obtained from 4 runs of each model with different random seeds. Oracle is the maximum score obtainable with bina- rized trees, since we compare against the non-binarized gold trees per convention. Results with \u2020 are trained on a version of PTB with punctuation, and hence not strictly comparable to the present work. For URNNG/DIORA, we take the parsed test set provided by the authors from their best runs and eval- uate F1 with our evaluation setup, which ignores punctuation. less of an issue for unsupervised structure learn- ing, however in this work we follow the latter and separate train/test data.",
  "less of an issue for unsupervised structure learn- ing, however in this work we follow the latter and separate train/test data. (4) evaluation: for unlabeled F1, almost all works ignore punctua- tion (even approaches that use punctuation dur- ing training typically ignore them during evalu- ation), but there is some variance in discarding trivial spans (width-one and sentence-level spans) and using corpus-level versus sentence-level F1.11 In this paper we discard trivial spans and evalu- ate on sentence-level F1 per recent work (Shen et al., 2018, 2019). Given the above, we mainly compare our approach against two recent, strong baselines with open source code: Parsing Predict Reading Network (PRPN)12 (Shen et al., 2018) and Ordered Neurons (ON)13 (Shen et al., 2019). These approaches train a neural language model with gated attention-like mechanisms to induce bi- nary trees, and achieve strong unsupervised pars- ing performance even when trained on corpora where punctuation is removed.",
  "These approaches train a neural language model with gated attention-like mechanisms to induce bi- nary trees, and achieve strong unsupervised pars- ing performance even when trained on corpora where punctuation is removed. Since the origi- nal results were on both language modeling and unsupervised parsing, their hyperparameters were presumably tuned to do well on both and thus may not be optimal for just unsupervised parsing. We 11Corpus-level F1 calculates precision/recall at the corpus level to obtain F1, while sentence-level F1 calculates F1 for each sentence and averages across the corpus. 12https://github.com/yikangshen/PRPN 13https://github.com/yikangshen/Ordered-Neurons PRPN ON Neural Compound PCFG PCFG Gold 47.3 48.1 50.8 55.2 Left 1.5 14.1 11.8 13.0 Right 39.9 31.0 27.7 28.4 Self 82.3 71.3 65.2 66.8 SBAR 50.0% 51.2% 52.5% 56.",
  "1 11.8 13.0 Right 39.9 31.0 27.7 28.4 Self 82.3 71.3 65.2 66.8 SBAR 50.0% 51.2% 52.5% 56.1% NP 59.2% 64.5% 71.2% 74.7% VP 46.7% 41.0% 33.8% 41.7% PP 57.2% 54.4% 58.8% 68.8% ADJP 44.3% 38.1% 32.5% 40.4% ADVP 32.8% 31.6% 45.5% 52.5% Table 2: (Top) Mean F1 similarity against Gold, Left, Right, and Self trees. Self F1 score is calculated by averaging over all 6 pairs obtained from 4 different runs. (Bottom) Frac- tion of ground truth constituents that were predicted as a con- stituent by the models broken down by label (i.e. label recall).",
  "and Self trees. Self F1 score is calculated by averaging over all 6 pairs obtained from 4 different runs. (Bottom) Frac- tion of ground truth constituents that were predicted as a con- stituent by the models broken down by label (i.e. label recall). therefore tune the hyperparameters of these base- lines for unsupervised parsing only (i.e. on valida- tion F1). 5 Results and Discussion Table 1 shows the unlabeled F1 scores for our models and various baselines. All models soundly outperform right branching baselines, and we \ufb01nd that the neural PCFG/compound PCFG are strong models for grammar induction. In particular the compound PCFG outperforms other models by an appreciable margin on both English and Chi- nese. We again note that we were unable to induce meaningful grammars through a traditional PCFG with the scalar parameterization despite a thor- ough hyperparameter search.14 See appendix A.2 for the full results broken down by sentence length for sentence- and corpus-level F1. Table 2 analyzes the learned tree structures.",
  "Table 2 analyzes the learned tree structures. We compare similarity as measured by F1 against gold, left, right, and \u201cself\u201d trees (top), where self F1 score is calculated by averaging over all 6 pairs obtained from 4 different runs. We \ufb01nd that PRPN is particularly consistent across multiple runs. We also observe that different models are better at identifying different constituent labels, as mea- sured by label recall (Table 2, bottom). While left as future work, this naturally suggests an ensemble approach wherein the empirical probabilities of constituents (obtained by averaging the predicted binary constituent labels from the different mod- els) are used either to supervise another model or directly as potentials in a CRF constituency parser. 14Training perplexity was much higher than in the neural case, indicating signi\ufb01cant optimization issues. However we did not experiment with online EM (Liang and Klein, 2009), and it is possible that such methods would yield better results.",
  "PPL Syntactic Eval. F1 LSTM LM 86.2 60.9% \u2212 PRPN 87.1 62.2% 47.9 Induced RNNG 95.3 60.1% 47.8 Induced URNNG 90.1 61.8% 51.6 ON 87.2 61.6% 50.0 Induced RNNG 95.2 61.7% 50.6 Induced URNNG 89.9 61.9% 55.1 Neural PCFG 252.6 49.2% 52.6 Induced RNNG 95.8 68.1% 51.4 Induced URNNG 86.0 69.1% 58.7 Compound PCFG 196.3 50.7% 60.1 Induced RNNG 89.8 70.0% 58.1 Induced URNNG 83.7 76.1% 66.9 RNNG on Oracle Trees 80.6 70.4% 71.9 + URNNG Fine-tuning 78.3 76.",
  "8 70.0% 58.1 Induced URNNG 83.7 76.1% 66.9 RNNG on Oracle Trees 80.6 70.4% 71.9 + URNNG Fine-tuning 78.3 76.1% 72.8 Table 3: Results from training RNNGs on induced trees from various models (Induced RNNG) on the PTB. Induced URNNG indicates \ufb01ne-tuning with the URNNG objective. We show perplexity (PPL), grammaticality judgment perfor- mance (Syntactic Eval.), and unlabeled F1. PPL/F1 are cal- culated on the PTB test set and Syntactic Eval. is from Marvin and Linzen (2018)\u2019s dataset. Results on top do not make any use of annotated trees, while the bottom two re- sults are trained on binarized gold trees. The perplexity numbers here are not comparable to standard results on the PTB since our models are generative model of sentences and hence we do not carry information across sentence bound- aries.",
  "The perplexity numbers here are not comparable to standard results on the PTB since our models are generative model of sentences and hence we do not carry information across sentence bound- aries. Also note that all the RNN-based models above (i.e. LSTM/PRPN/ON/RNNG/URNNG) have roughly the same model capacity (see appendix A.3). Finally, all models seemed to have some dif\ufb01culty in identifying SBAR/VP constituents which typi- cally span more words than NP constituents, in- dicating further opportunities for improvement on unsupervised parsing. 5.1 Induced Trees for Downstream Tasks While the compound PCFG has fewer indepen- dence assumptions than the neural PCFG, it is still a more constrained model of language than standard neural language models (NLM) and thus not competitive in terms of perplexity: the com- pound PCFG obtains a perplexity of 196.3 while an LSTM language model (LM) obtains 86.2 (Ta- ble 3).15 In contrast, both PRPN and ON perform as well as an LSTM LM while maintaining good unsupervised parsing performance.",
  "We thus experiment to see if it is possible to use the induced trees to supervise a more \ufb02exible generative model that can make use of 15We did manage to almost match the perplexity of an NLM by additionally conditioning the terminal probabilities on previous history, i.e. \u03c0z,T \u2192wt \u221dexp(u\u22a4 w f2([wT ; z; ht]) + bw), where ht is the hidden state from an LSTM over x<t. How- ever the unsupervised parsing performance was far worse (\u2248 25 F1 on the PTB). Figure 2: Alignment of induced nonterminals ordered from top based on predicted frequency (therefore NT-04 is the most frequently-predicted nonterminal). For each nonterminal we visualize the proportion of correctly-predicted constituents that correspond to particular gold labels. For reference we also show the precision (i.e. probability of correctly predict- ing unlabeled constituents) in the rightmost column. tree structures\u2014namely, recurrent neural network grammars (RNNG) (Dyer et al., 2016).",
  "For reference we also show the precision (i.e. probability of correctly predict- ing unlabeled constituents) in the rightmost column. tree structures\u2014namely, recurrent neural network grammars (RNNG) (Dyer et al., 2016). RNNGs are generative models of language that jointly model syntax and surface structure by incremen- tally generating a syntax tree and sentence. As with NLMs, RNNGs make no independence as- sumptions, and have been shown to outperform NLMs in terms of perplexity and grammatical- ity judgment when trained on gold trees (Kuncoro et al., 2018; Wilcox et al., 2019). We take the best run from each model and parse the training set,16 and use the induced trees to su- pervise an RNNG for each model using the param- eterization from Kim et al.",
  "We take the best run from each model and parse the training set,16 and use the induced trees to su- pervise an RNNG for each model using the param- eterization from Kim et al. (2019).17 We are also interested in syntactic evaluation of our models, and for this we utilize the framework and dataset from Marvin and Linzen (2018), where a model is presented two minimally different sentences such as: the senators near the assistant are old *the senators near the assistant is old and must assign higher probability to grammatical sentence. 16The train/test F1 was similar for all models. 17https://github.com/harvardnlp/urnng",
  "he retired as senior vice president \ufb01nance and administration and chief \ufb01nancial of\ufb01cer of the company oct. N kenneth j. \u27e8unk\u27e9who was named president of this thrift holding company in august resigned citing personal reasons the former president and chief executive eric w. \u27e8unk\u27e9resigned in june \u27e8unk\u27e9\u2019s president and chief executive of\ufb01cer john \u27e8unk\u27e9said the loss stems from several factors mr. \u27e8unk\u27e9is executive vice president and chief \ufb01nancial of\ufb01cer of \u27e8unk\u27e9and will continue in those roles charles j. lawson jr. N who had been acting chief executive since june N will continue as chairman \u27e8unk\u27e9corp. received an N million army contract for helicopter engines boeing co. received a N million air force contract for developing cable systems for the \u27e8unk\u27e9missile general dynamics corp. received a N million air force contract for \u27e8unk\u27e9training sets grumman corp.",
  "received an N million army contract for helicopter engines boeing co. received a N million air force contract for developing cable systems for the \u27e8unk\u27e9missile general dynamics corp. received a N million air force contract for \u27e8unk\u27e9training sets grumman corp. received an N million navy contract to upgrade aircraft electronics thomson missile products with about half british aerospace \u2019s annual revenue include the \u27e8unk\u27e9\u27e8unk\u27e9missile family already british aerospace and french \u27e8unk\u27e9\u27e8unk\u27e9\u27e8unk\u27e9on a british missile contract and on an air-traf\ufb01c control radar system meanwhile during the the s&p trading halt s&p futures sell orders began \u27e8unk\u27e9up while stocks in new york kept falling sharply but the \u27e8unk\u27e9of s&p futures sell orders weighed on the market and the link with stocks began to fray again on friday some market makers were selling again traders said futures traders say the s&p was \u27e8unk\u27e9that the dow could fall as much as N points meanwhile two initial public offerings \u27e8unk\u27e9the \u27e8unk\u27e9market in their \u27e8unk\u27e9day of national over-the-counter trading friday traders said most of their major institutional investors on the other hand sat tight Table 4: For each query sentence (bold), we show the 5 nearest neighbors based on cosine similarity, where we take the representation for each sentence to be the mean of the variational posterior.",
  "Additionally, Kim et al. (2019) report per- plexity improvements by \ufb01ne-tuning an RNNG trained on gold trees with the unsupervised RNNG (URNNG)\u2014whereas the RNNG is is trained to maximize the joint log likelihood log p(t), the URNNG maximizes a lower bound on the log marginal likelihood log P t\u2208TG(x) p(t) with a structured inference network that approximates the true posterior. We experiment with a similar approach where we \ufb01ne-tune RNNGs trained on induced trees with URNNGs. We perform early stopping for both RNNG and URNNG based on validation perplexity. See appendix A.3 for the full experimental setup. The results are shown in Table 3. For perplexity, RNNGs trained on induced trees (Induced RNNG in Table 3) are unable to improve upon an LSTM LM, in contrast to the supervised RNNG which does outperform the LSTM language model (Ta- ble 3, bottom).",
  "For perplexity, RNNGs trained on induced trees (Induced RNNG in Table 3) are unable to improve upon an LSTM LM, in contrast to the supervised RNNG which does outperform the LSTM language model (Ta- ble 3, bottom). For grammaticality judgment how- ever, the RNNG trained with compound PCFG trees outperforms the LSTM LM despite obtain- ing worse perplexity,18 and performs on par with the RNNG trained on binarized gold trees. Fine- tuning with the URNNG results in improvements in perplexity and grammaticality judgment across the board (Induced URNNG in Table 3). We also obtain large improvements on unsupervised pars- ing as measured by F1, with the \ufb01ne-tuned URN- NGs outperforming the respective original mod- els.19 This is potentially due to an ensembling ef- fect between the original model and the URNNG\u2019s structured inference network, which is parameter- 18Kuncoro et al. (2018, 2019) also observe that models that achieve lower perplexity do not necessarily perform better on syntactic evaluation tasks. 19Li et al.",
  "(2018, 2019) also observe that models that achieve lower perplexity do not necessarily perform better on syntactic evaluation tasks. 19Li et al. (2019) similarly obtain improvements by re\ufb01n- ing a model trained on induced trees on classi\ufb01cation tasks. ized as a neural CRF constituency parser (Durrett and Klein, 2015; Liu et al., 2018).20 5.2 Model Analysis We analyze our best compound PCFG model in more detail. Since we induce a full set of nonter- minals in our grammar, we can analyze the learned nonterminals to see if they can be aligned with lin- guistic constituent labels. Figure 2 visualizes the alignment between induced and gold labels, where for each nonterminal we show the empirical prob- ability that a predicted constituent of this type will correspond to a particular linguistic constituent in the test set, conditioned on its being a correct con- stituent (for reference we also show the precision). We observe that some of the induced nonterminals clearly align to linguistic nonterminals.",
  "We observe that some of the induced nonterminals clearly align to linguistic nonterminals. Further re- sults, including preterminal alignments to part-of- speech tags,21 are shown in appendix A.4. We next analyze the continuous latent space. Table 4 shows nearest neighbors of some sen- tences using the mean of the variational poste- rior as the continuous representation of each sen- tence. We qualitatively observe that the latent space seems to capture topical information. 20While left as future work, it is possible to use the com- pound PCFG itself as an inference network. Also note that the F1 scores for the URNNGs in Table 3 are optimistic since we selected the best-performing runs of the original models based on validation F1 to parse the training set. Finally, as noted by Kim et al. (2019), a URNNG trained from scratch fails to outperform a right-branching baseline on this version of PTB where punctuation is removed. 21As a POS induction system, the many-to-one perfor- mance of the compound PCFG using the preterminals is 68.0.",
  "21As a POS induction system, the many-to-one perfor- mance of the compound PCFG using the preterminals is 68.0. A similarly-parameterized compound HMM with 60 hidden states (an HMM is a particularly type of PCFG) obtains 63.2. This is still quite a bit lower than the state-of-the-art (Tran et al., 2016; He et al., 2018; Stratos, 2019), though compar- ison is confounded by various factors such as preprocessing. A neural PCFG/HMM obtains 68.2 and 63.4 respectively.",
  "NT-04 NT-12 T-22 w6 NT-20 T-40 w5 NT-20 T-35 w4 NT-07 T-45 w3 T-05 w2 T-13 w1 PC - of the company \u2019s capital structure in the company \u2019s divestiture program by the company \u2019s new board in the company \u2019s core businesses on the company \u2019s strategic plan PC + above the treasury \u2019s N-year note above the treasury \u2019s seven-year note above the treasury \u2019s comparable note above the treasury \u2019s \ufb01ve-year note measured the earth \u2019s ozone layer NT-23 NT-04 NT-12 NT-04 NT-12 T-21 w7 T-60 w6 T-13 w5 NT-06 T-41 w4 T-05 w3 T-13 w2 T-58 w1 PC - purchased through the exercise of stock options circulated by a handful of major brokers higher as a percentage of total loans common with a lot of large companies surprised by the storm of sell orders PC + brought to the u.s. against her will laid for the arrest of opposition activists uncertain about the magnitude of structural damage held after the assassination of his mother hurt as a result of the violations",
  "against her will laid for the arrest of opposition activists uncertain about the magnitude of structural damage held after the assassination of his mother hurt as a result of the violations NT-10 NT-05 NT-19 NT-04 T-43 w6 T-13 w5 NT-06 T-41 w4 T-05 w3 T-02 w2 T-55 w1 PC - to terminate their contract with warner to support a coup in panama to suit the bureaucrats in brussels to thwart his bid for amr to prevent the pound from rising PC + to change our strategy of investing to offset the growth of minimills to be a lot of art to change our way of life to increase the impact of advertising NT-05 NT-19 NT-04 NT-12 T-21 w7 T-60 w6 T-13 w5 NT-06 T-22 w4 NT-20 T-40 w3 T-05 w2 T-02 w1 PC - raise the minimum grant for smaller states veto a defense bill with inadequate funding avoid an imminent public or private injury \ufb01eld a competitive slate of congressional candidates alter a longstanding ban on such involvement PC + generate an offsetting pro\ufb01t by selling waves change an export loss to domestic",
  "T-02 w1 PC - raise the minimum grant for smaller states veto a defense bill with inadequate funding avoid an imminent public or private injury \ufb01eld a competitive slate of congressional candidates alter a longstanding ban on such involvement PC + generate an offsetting pro\ufb01t by selling waves change an export loss to domestic plus expect any immediate problems with margin calls make a positive contribution to our earnings \ufb01nd a trading focus discouraging much participation Table 5: For each subtree, we perform PCA on the variational posterior mean vectors that are associated with that particular subtree and take the top principal component. We then list the top 5 constituents that had the lowest (PC -) and highest (PC +) principal component values. We are also interested in the variation in the leaves due to z when the variation due to the tree structure is held constant. To investigate this, we use the parsed dataset to obtain pairs of the form (\u00b5\u03c6(x(n)), t(n) j ), where t(n) j is the j-th subtree of the (approximate) MAP tree t(n) for the n-th sen- tence.",
  "To investigate this, we use the parsed dataset to obtain pairs of the form (\u00b5\u03c6(x(n)), t(n) j ), where t(n) j is the j-th subtree of the (approximate) MAP tree t(n) for the n-th sen- tence. Therefore each mean vector \u00b5\u03c6(x(n)) is associated with |x(n)| \u22121 subtrees, where |x(n)| is the sentence length. Our de\ufb01nition of subtree here ignores terminals, and thus each subtree is associated with many mean vectors. For a fre- quently occurring subtree, we perform PCA on the set of mean vectors that are associated with the subtree to obtain the top principal compo- nent. We then show the constituents that had the 5 most positive/negative values for this top prin- cipal component in Table 5.",
  "We then show the constituents that had the 5 most positive/negative values for this top prin- cipal component in Table 5. For example, a par- ticularly common subtree\u2014associated with 180 unique constituents\u2014is given by (NT-04 (T-13 w1) (NT-12 (NT-20 (NT-20 (NT-07 (T-05 w2) (T-45 w3)) (T-35 w4)) (T-40 w5)) (T-22 w6))). The top 5 constituents with the most nega- tive/positive values are shown in the top left part of Table 5. We \ufb01nd that the leaves [w1, . . . , w6], which form a 6-word constituent, vary in a regu- lar manner as z is varied. We also observe that root of this subtree (NT-04) aligns to prepositional phrases (PP) in Figure 2, and the leaves in Ta- ble 5 (top left) are indeed mostly PP.",
  "We also observe that root of this subtree (NT-04) aligns to prepositional phrases (PP) in Figure 2, and the leaves in Ta- ble 5 (top left) are indeed mostly PP. However, the model fails to identify ((T-40 w5) (T-22 w6)) as a con- stituent in this case (as well as well in the bottom right example). See appendix A.5 for more exam- ples. It is possible that the model is utilizing the subtrees to capture broad template-like structures and then using z to \ufb01ll them in, similar to recent works that also train models to separate \u201cwhat to say\u201d from \u201chow to say it\u201d (Wiseman et al., 2018; Peng et al., 2019; Chen et al., 2019a,b). 5.3 Limitations We report on some negative results as well as im- portant limitations of our work. While distributed representations promote parameter sharing, we were unable to obtain improvements through more factorized parameterizations that promote even greater parameter sharing.",
  "5.3 Limitations We report on some negative results as well as im- portant limitations of our work. While distributed representations promote parameter sharing, we were unable to obtain improvements through more factorized parameterizations that promote even greater parameter sharing. In particular, for rules of the type A \u2192BC, we tried having the out- put embeddings be a function of the input embed- dings (e.g. uBC = g([wB; wC]) where g is an MLP), but obtained worse results. For rules of the type T \u2192w, we tried using a character-level CNN (dos Santos and Zadrozny, 2014; Kim et al., 2016) to obtain the output word embeddings uw (Jozefowicz et al., 2016; Tran et al., 2016), but found the performance to be similar to the word- level case.22 We were also unable to obtain im- provements by making the variational family more \ufb02exible through normalizing \ufb02ows (Rezende and Mohamed, 2015; Kingma et al., 2016).",
  "How- ever, given that we did not exhaustively explore the full space of possible parameterizations, the above modi\ufb01cations could eventually lead to im- 22It is also possible to take advantage of pretrained word embeddings by using them to initialize output word embed- dings or directly working with continuous emission distribu- tions (Lin et al., 2015; He et al., 2018)",
  "provements with the right setup. Relatedly, the models were quite sensitive to pa- rameterization (e.g. it was important to use resid- ual layers for f1, f2), grammar size, and optimiza- tion method. We also noticed some variance in results across random seeds, as shown in Table 2. Finally, despite vectorized GPU implementations, training was signi\ufb01cantly more expensive (both in terms of time and memory) than NLM-based un- supervised parsing systems due to the O(|R||x|3) dynamic program, which makes our approach po- tentially dif\ufb01cult to scale. 6 Related Work Grammar induction and unsupervised parsing has a long and rich history in natural language pro- cessing. Early work on with pure unsuper- vised learning was mostly negative (Lari and Young, 1990; Carroll and Charniak, 1992; Char- niak, 1993), though Pereira and Schabes (1992) reported some success on partially bracketed data.",
  "Clark (2001) and Klein and Manning (2002) were some of the \ufb01rst successful statistical ap- proaches. In particular, the constituent-context model (CCM) of Klein and Manning (2002), which explicitly models both constituents and dis- tituents, was the basis for much subsequent work (Klein and Manning, 2004; Huang et al., 2012; Golland et al., 2012). Other works have explored imposing inductive biases through Bayesian pri- ors (Johnson et al., 2007; Liang et al., 2007; Wang and Blunsom, 2013), modi\ufb01ed objectives (Smith and Eisner, 2004), and additional constraints on recursion depth (Noji et al., 2016; Jin et al., 2018). While the framework of specifying the struc- ture of a grammar and learning the parameters is common, other methods exist. Bod (2006) con- sider a nonparametric-style approach to unsuper- vised parsing by using random subsets of training subtrees to parse new sentences.",
  "While the framework of specifying the struc- ture of a grammar and learning the parameters is common, other methods exist. Bod (2006) con- sider a nonparametric-style approach to unsuper- vised parsing by using random subsets of training subtrees to parse new sentences. Seginer (2007) utilize an incremental algorithm to unsupervised parsing which makes local decisions to create con- stituents based on a complex set of heuristics. Ponvert et al. (2011) induce parse trees through cascaded applications of \ufb01nite state models. More recently, neural network-based ap- proaches have shown promising results on inducing parse trees directly from words. Shen et al. (2018, 2019) learn tree structures through soft gating layers within neural language models, while Drozdov et al. (2019) combine recursive autoencoders with the inside-outside algorithm. Kim et al. (2019) train unsupervised recurrent neural network grammars with a structured inference network to induce latent trees, and Shi et al. (2019) utilize image captions to identify and ground constituents.",
  "Kim et al. (2019) train unsupervised recurrent neural network grammars with a structured inference network to induce latent trees, and Shi et al. (2019) utilize image captions to identify and ground constituents. Our work is also related to latent variable PCFGs (Matsuzaki et al., 2005; Petrov et al., 2006; Cohen et al., 2012), which extend PCFGs to the la- tent variable setting by splitting nonterminal sym- bols into latent subsymbols. In particular, latent vector grammars (Zhao et al., 2018) and composi- tional vector grammars (Socher et al., 2013) also employ continuous vectors within their grammars. However these approaches have been employed for learning supervised parsers on annotated tree- banks, in contrast to the unsupervised setting of the current work. 7 Conclusion This work studies a neural network-based ap- proach grammar induction with PCFGs.",
  "However these approaches have been employed for learning supervised parsers on annotated tree- banks, in contrast to the unsupervised setting of the current work. 7 Conclusion This work studies a neural network-based ap- proach grammar induction with PCFGs. We \ufb01rst propose to parameterize a PCFG\u2019s rule probabil- ities with neural networks over distributed rep- resentations of latent symbols, and \ufb01nd that this neural PCFG makes it possible to induce linguis- tically meaningful grammars with simple maxi- mum likelihood learning. We then extend the neural PCFG through a sentence-level continu- ous latent vector, which induces marginal depen- dencies beyond the traditional \ufb01rst-order context- free assumptions. We show that this compound PCFG learns richer grammars and leads to im- proved performance when evaluated as an unsu- pervised parser. The collapsed amortized varia- tional inference approach is general and can be used for generative models which admit tractable inference through partial conditioning. Learning deep generative models which exhibit such condi- tional Markov properties is an interesting direction for future work.",
  "The collapsed amortized varia- tional inference approach is general and can be used for generative models which admit tractable inference through partial conditioning. Learning deep generative models which exhibit such condi- tional Markov properties is an interesting direction for future work. Acknowledgments We thank Phil Blunsom for initial discussions which seeded many of the core ideas in the present work. We also thank Yonatan Belinkov and Shay Cohen for helpful feedback, and Andrew Drozdov for providing the parsed dataset from their DIORA model. YK is supported by a Google Fellowship. AMR acknowledges the support of NSF 1704834, 1845664, AWS, and Oracle.",
  "References Alfred Aho. 1968. Indexed Grammars\u2014An Extension of Context-Free Grammars. Journal of the ACM, 15(4):647\u2013 671. Sanjeev Arora, Nadav Cohen, and Elad Hazan. 2018. On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization. In Proceedings of ICML. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer Normalization. In Proceedings of NIPS. James K. Baker. 1979. Trainable Grammars for Speech Recognition. In Proceedings of the Spring Conference of the Acoustical Society of America. Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Ja- son Weston. 2009. Curriculum Learning. In Proceedings of ICML. Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cote, John DeNero, and Dan Klein. 2010. Painless Unsupervised Learning with Features. In Proceedings of NAACL. James O. Berger. 1985. Statistical Decision Theory and Bayesian Analysis.",
  "Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cote, John DeNero, and Dan Klein. 2010. Painless Unsupervised Learning with Features. In Proceedings of NAACL. James O. Berger. 1985. Statistical Decision Theory and Bayesian Analysis. Springer. Rens Bod. 2006. An All-Subtrees Approach to Unsupervised Parsing. In Proceedings of ACL. Glenn Carroll and Eugene Charniak. 1992. Two Experi- ments on Learning Probabilistic Dependency Grammars from Corpora. In AAAI Workshop on Statistically-Based NLP Techniques. Eugene Charniak. 1993. Statistical Language Learning. MIT Press. Danqi Chen and Christopher D. Manning. 2014. A Fast and Accurate Dependency Parser using Neural Networks. In Proceedings of EMNLP. Mingda Chen, Qingming Tang, Sam Wiseman, and Kevin Gimpel. 2019a. Controllable Paraphrase Generation with a Syntactic Exemplar. In Proceedings of ACL.",
  "In Proceedings of EMNLP. Mingda Chen, Qingming Tang, Sam Wiseman, and Kevin Gimpel. 2019a. Controllable Paraphrase Generation with a Syntactic Exemplar. In Proceedings of ACL. Mingda Chen, Qingming Tang, Sam Wiseman, and Kevin Gimpel. 2019b. A Multi-task Approach for Disentangling Syntax and Semantics in Sentence Sepresentations. In Proceedings of NAACL. Alexander Clark. 2001. Unsupervised Induction of Stochas- tic Context Free Grammars Using Distributional Cluster- ing. In Proceedings of CoNLL. Shay B. Cohen. 2016. Bayesian Analysis in Natural Lan- guage Processing. Morgan and Claypool. Shay B. Cohen, Kevin Gimpel, and Noah A Smith. 2009. Lo- gistic Normal Priors for Unsupervised Probabilistic Gram- mar Induction. In Proceedings of NIPS. Shay B. Cohen and Noah A Smith. 2009.",
  "Shay B. Cohen, Kevin Gimpel, and Noah A Smith. 2009. Lo- gistic Normal Priors for Unsupervised Probabilistic Gram- mar Induction. In Proceedings of NIPS. Shay B. Cohen and Noah A Smith. 2009. Shared Logistic Normal Distributions for Soft Parameter Tying in Unsu- pervised Grammar Induction. In Proceedings of NAACL. Shay B. Cohen, Karl Stratos, Michael Collins, Dean P. Fos- ter, and Lyle Ungar. 2012. Spectral Learning of Latent- Variable PCFGs. In Proceedings of ACL. Michael Collins. 1997. Three Generative, Lexicalised Mod- els for Statistical Parsing. In Proceedings of ACL. Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. 1977. Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1\u201338. Andrew Drozdov, Patrick Verga, Mohit Yadev, Mohit Iyyer, and Andrew McCallum.",
  "Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1\u201338. Andrew Drozdov, Patrick Verga, Mohit Yadev, Mohit Iyyer, and Andrew McCallum. 2019. Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Auto- Encoders. In Proceedings of NAACL. Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. 2019. Gradient Descent Provably Optimizes Over- parameterized Neural Networks. In Proceedings of ICLR. Greg Durrett and Dan Klein. 2015. Neural CRF Parsing. In Proceedings of ACL. Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. 2016. Recurrent Neural Network Gram- mars. In Proceedings of NAACL. Jason Eisner. 2016. Inside-Outside and Forward-Backward Algorithms Are Just Backprop (Tutorial Paper).",
  "2016. Recurrent Neural Network Gram- mars. In Proceedings of NAACL. Jason Eisner. 2016. Inside-Outside and Forward-Backward Algorithms Are Just Backprop (Tutorial Paper). In Pro- ceedings of the Workshop on Structured Prediction for NLP. Dave Golland, John DeNero, and Jakob Uszkoreit. 2012. A Feature-Rich Constituent Context Model for Grammar In- duction. In Proceedings of ACL. Junxian He, Graham Neubig, and Taylor Berg-Kirkpatrick. 2018. Unsupervised Learning of Syntactic Structure with Invertible Neural Projections. In Proceedings of EMNLP. Phu Mon Htut, Kyunghyun Cho, and Samuel R. Bowman. 2018. Grammar Induction with Neural Language Models: An Unusual Replication. In Proceedings of EMNLP. Yun Huang, Min Zhang, and Chew Lim Tan. 2012. Improved Constituent Context Model with Features. In Proceedings of PACLIC.",
  "2018. Grammar Induction with Neural Language Models: An Unusual Replication. In Proceedings of EMNLP. Yun Huang, Min Zhang, and Chew Lim Tan. 2012. Improved Constituent Context Model with Features. In Proceedings of PACLIC. Lifeng Jin, Finale Doshi-Velez, Timothy Miller, William Schuler, and Lane Schwartz. 2018. Unsupervised Gram- mar Induction with Depth-bounded PCFG. In Proceed- ings of TACL. Mark Johnson. 1998. PCFG Models of Linguistic Tree Rep- resentations. Computational Linguistics, 24:613\u2013632. Mark Johnson, Thomas L. Grif\ufb01ths, and Sharon Goldwater. 2007. Bayesian Inference for PCFGs via Markov chain Monte Carlo. In Proceedings of NAACL. Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. 2016. Exploring the Limits of Language Modeling. arXiv:1602.02410.",
  "In Proceedings of NAACL. Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. 2016. Exploring the Limits of Language Modeling. arXiv:1602.02410. Yoon Kim, Yacine Jernite, David Sontag, and Alexander M. Rush. 2016. Character-Aware Neural Language Models. In Proceedings of AAAI. Yoon Kim, Alexander M. Rush, Lei Yu, Adhiguna Kuncoro, Chris Dyer, and G\u00b4abor Melis. 2019. Unsupervised Re- current Neural Network Grammars. In Proceedings of NAACL. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In Proceedings of ICLR. Diederik P. Kingma, Tim Salimans, and Max Welling. 2016. Improving Variational Inference with Autoregres- sive Flow. arXiv:1606.04934.",
  "Diederik P. Kingma and Max Welling. 2014. Auto-Encoding Variational Bayes. In Proceedings of ICLR. Nikita Kitaev and Dan Klein. 2018. Constituency Parsing with a Self-Attentive Encoder. In Proceedings of ACL. Dan Klein and Christopher Manning. 2002. A Generative Constituent-Context Model for Improved Grammar Induc- tion. In Proceedings of ACL. Dan Klein and Christopher Manning. 2004. Corpus-based Induction of Syntactic Structure: Models of Dependency and Constituency. In Proceedings of ACL. Dan Klein and Christopher D. Manning. 2003. Accurate Un- lexicalized Parsing. In Proceedings of ACL. Adhiguna Kuncoro, Chris Dyer, John Hale, Dani Yogatama, Stephen Clark, and Phil Blunsom. 2018. LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Model- ing Structure Makes Them Better. In Proceedings of ACL. Adhiguna Kuncoro, Chris Dyer, Laura Rimell, Stephen Clark, and Phil Blunsom.",
  "2018. LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Model- ing Structure Makes Them Better. In Proceedings of ACL. Adhiguna Kuncoro, Chris Dyer, Laura Rimell, Stephen Clark, and Phil Blunsom. 2019. Scalable Syntax-Aware Language Models Using Knowledge Distillation. In Pro- ceedings of ACL. Kenichi Kurihara and Taisuke Sato. 2006. Variational Bayesian Grammar Induction for Natural Language. In Proceedings of International Colloquium on Grammatical Inference. Karim Lari and Steve Young. 1990. The Estimation of Stochastic Context-Free Grammars Using the Inside- Outside Algorithm. Computer Speech and Language, 4:35\u201356. Bowen Li, Lili Mou, and Frank Keller. 2019. An Imitation Learning Approach to Unsupervised Parsing. In Proceed- ings of ACL. Percy Liang and Dan Klein. 2009. Online EM for Unsuper- vised models. In Proceedings of NAACL.",
  "2019. An Imitation Learning Approach to Unsupervised Parsing. In Proceed- ings of ACL. Percy Liang and Dan Klein. 2009. Online EM for Unsuper- vised models. In Proceedings of NAACL. Percy Liang, Slav Petrov, Michael I. Jordan, and Dan Klein. 2007. The In\ufb01nite PCFG using Hierarchical Dirichlet Pro- cesses. In Proceedings of EMNLP. Chu-Cheng Lin, Waleed Ammar, Chris Dyer, , and Lori Levin. 2015. Unsupervised POS Induction with Word Embeddings. In Proceedings of NAACL. Yang Liu, Matt Gardner, and Mirella Lapata. 2018. Struc- tured Alignment Networks for Matching Sentences. In Proceedings of EMNLP. Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19:313\u2013330. Rebecca Marvin and Tal Linzen.",
  "Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19:313\u2013330. Rebecca Marvin and Tal Linzen. 2018. Targeted Syntac- tic Evaluation of Language Models. In Proceedings of EMNLP. Takuya Matsuzaki, Yusuke Miyao, and Junichi Tsujii. 2005. Probabilistic CFG with Latent Annotations. In Proceed- ings of ACL. Bernard Merialdo. 1994. Tagging English Text with a Prob- abilistic Model. Computational Linguistics, 20(2):155\u2013 171. Hiroshi Noji, Yusuke Miyao, and Mark Johnson. 2016. Us- ing Left-corner Parsing to Encode Universal Structural Constraints in Grammar Induction. In Proceedings of EMNLP. Ankur P. Parikh, Shay B. Cohen, and Eric P. Xing. 2014. Spectral Unsupervised Parsing with Additive Tree Met- rics.",
  "In Proceedings of EMNLP. Ankur P. Parikh, Shay B. Cohen, and Eric P. Xing. 2014. Spectral Unsupervised Parsing with Additive Tree Met- rics. In Proceedings of ACL. Hao Peng, Ankur P. Parikh, Manaal Faruqui, Bhuwan Dhin- gra, and Dipanjan Das. 2019. Text Generation with Exemplar-based Adaptive Decoding. In Proceedings of NAACL. Fernando Pereira and Yves Schabes. 1992. Inside-Outside Reestimation from Partially Bracketed Corpora. In Pro- ceedings of ACL. Slav Petrov, Leon Barret, Romain Thibaux, and Dan Klein. 2006. Learning Accurate, Compact, and Interpretable Tree Annotation. In Proceedings of ACL. Elis Ponvert, Jason Baldridge, and Katrin Erk. 2011. Sim- pled Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Methods. In Proceedings of ACL. O\ufb01r Press and Lior Wolf. 2016.",
  "Elis Ponvert, Jason Baldridge, and Katrin Erk. 2011. Sim- pled Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Methods. In Proceedings of ACL. O\ufb01r Press and Lior Wolf. 2016. Using the Output Embedding to Improve Language Models. In Proceedings of EACL. Roi Reichart and Ari Rappoport. 2010. Improved Fully Un- supervised Parsing with Zoomed Learning. In Proceed- ings of EMNLP. Danilo J. Rezende and Shakir Mohamed. 2015. Variational Inference with Normalizing Flows. In Proceedings of ICML. Danilo J. Rezende, Shakir Mohamed, and Daan Wierstra. 2014. Stochastic Backpropagation and Approximate In- ference in Deep Generative Models. In Proceedings of ICML. Herbert Robbins. 1951. Asymptotically Subminimax Solu- tions of Compound Statistical Decision Problems.",
  "2014. Stochastic Backpropagation and Approximate In- ference in Deep Generative Models. In Proceedings of ICML. Herbert Robbins. 1951. Asymptotically Subminimax Solu- tions of Compound Statistical Decision Problems. In Pro- ceedings of the Second Berkeley Symposium on Mathemat- ical Statistics and Probability, pages 131\u2013149. Berkeley: University of California Press. Herbert Robbins. 1956. An Empirical Bayes Approach to Statistics. In Proceedings of the Third Berkeley Sympo- sium on Mathematical Statistics and Probability, pages 157\u2013163. Berkeley: University of California Press. Ruslan Salakhutdinov, Sam Roweis, and Zoubin Ghahra- mani. 2003. Optimization with EM and Expectation- Conjugate-Gradient. In Proceedings of ICML. C\u00b4\u0131cero Nogueira dos Santos and Bianca Zadrozny. 2014. Learning Character-level Representations for Part-of- Speech Tagging. In Proceedings of ICML. Yoav Seginer. 2007.",
  "In Proceedings of ICML. C\u00b4\u0131cero Nogueira dos Santos and Bianca Zadrozny. 2014. Learning Character-level Representations for Part-of- Speech Tagging. In Proceedings of ICML. Yoav Seginer. 2007. Fast Unsupervised Incremental Parsing. In Proceedings of ACL. Yikang Shen, Zhouhan Lin, Chin-Wei Huang, and Aaron Courville. 2018. Neural Language Modeling by Jointly Learning Syntax and Lexicon. In Proceedings of ICLR. Yikang Shen, Shawn Tan, Alessandro Sordoni, and Aaron Courville. 2019. Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks. In Proceed- ings of ICLR.",
  "Haoyue Shi, Jiayuan Mao, Kevin Gimpel, and Karen Livescu. 2019. Visually Grounded Neural Syntax Acquisition. In Proceedings of ACL. Noah A. Smith and Jason Eisner. 2004. Annealing Tech- niques for Unsupervised Statistical Language Learning. In Proceedings of ACL. Benjamin Snyder, Tahira Naseem, and Regina Barzilay. 2009. Unsupervised Multilingual Grammar Induction. In Proceedings of ACL. Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. 2013. Parsing with Compositional Vector Grammars. In Proceedings of ACL. Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf- sky. 2012. Three Dependency-and-Boundary Models for Grammar Induction. In Proceedings of EMNLP-CoNLL. Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2013. Breaking Out of Local Optima with Count Trans- forms and Model Recombination: A Study in Grammar Induction.",
  "In Proceedings of EMNLP-CoNLL. Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2013. Breaking Out of Local Optima with Count Trans- forms and Model Recombination: A Study in Grammar Induction. In Proceedings of EMNLP. Mitchell Stern, Jacob Andreas, and Dan Klein. 2017. A Min- imal Span-Based Neural Constituency Parser. In Proceed- ings of ACL. Karl Stratos. 2019. Mutual Information Maximization for Simple and Accurate Part-of-Speech Induction. In Pro- ceedings of NAACL. Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved Semantic Representations From Tree- Structured Long Short-Term Memory Networks. In Pro- ceedings of ACL. Ke Tran, Yonatan Bisk, Ashish Vaswani, Daniel Marcu, and Kevin Knight. 2016. Unsupervised Neural Hidden Markov Models. In Proceedings of the Workshop on Structured Prediction for NLP.",
  "In Pro- ceedings of ACL. Ke Tran, Yonatan Bisk, Ashish Vaswani, Daniel Marcu, and Kevin Knight. 2016. Unsupervised Neural Hidden Markov Models. In Proceedings of the Workshop on Structured Prediction for NLP. Pengyu Wang and Phil Blunsom. 2013. Collapsed Varia- tional Bayesian Inference for PCFGs. In Proceedings of CoNLL. Wenhui Wang and Baobao Chang. 2016. Graph-based De- pendency Parsing with Bidirectional LSTM. In Proceed- ings of ACL. Ethan Wilcox, Peng Qian, Richard Futrell, Miguel Balles- teros, and Roger Levy. 2019. Structural Supervision Im- proves Learning of Non-Local Grammatical Dependen- cies. In Proceedings of NAACL. Sam Wiseman, Stuart M. Shieber, and Alexander M. Rush. 2018. Learning Neural Templates for Text Generation. In Proceedings of EMNLP. Ji Xu, Daniel Hsu, and Arian Maleki. 2018.",
  "In Proceedings of NAACL. Sam Wiseman, Stuart M. Shieber, and Alexander M. Rush. 2018. Learning Neural Templates for Text Generation. In Proceedings of EMNLP. Ji Xu, Daniel Hsu, and Arian Maleki. 2018. Bene\ufb01ts of Over- Parameterization with EM. In Proceedings of NeurIPS. Naiwen Xue, Fei Xia, Fu dong Chiou, and Marta Palmer. 2005. The Penn Chinese Treebank: Phrase Structure An- notation of a Large Corpus. Natural Language Engineer- ing, 11:207\u2013238. Cun-Hui Zhang. 2003. Compound Decision Theory and Em- pirical Bayes Methods. The Annals of Statistics, 31:379\u2013 390. Yanpeng Zhao, Liwen Zhang, and Kewei Tu. 2018. Gaussian Mixture Latent Vector Grammars. In Proceedings of ACL. Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo. 2015. Long Short-Term Memory Over Tree Structures.",
  "2018. Gaussian Mixture Latent Vector Grammars. In Proceedings of ACL. Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo. 2015. Long Short-Term Memory Over Tree Structures. In Pro- ceedings of ICML.",
  "A Appendix A.1 Model Parameterization We associate an input embedding wN for each symbol N on the left side of a rule (i.e. N \u2208 {S} \u222aN \u222aP) and run a neural network over wN to obtain the rule probabilities. Concretely, each rule type \u03c0r is parameterized as follows, \u03c0S\u2192A = exp(u\u22a4 A f1(wS) + bA) P A\u2032\u2208N exp(u\u22a4 A\u2032 f1(wS) + bA\u2032), \u03c0A\u2192BC = exp(u\u22a4 BC wA + bBC) P B\u2032C\u2032\u2208M exp(u\u22a4 B\u2032C\u2032 wA + bB\u2032C\u2032), \u03c0T\u2192w = exp(u\u22a4 w f2(wT ) + bw) P w\u2032\u2208\u03a3 exp(u\u22a4 w\u2032 f2(wT ) + bw\u2032), where M is the product space (N \u222aP)\u00d7(N \u222aP), and f1, f2 are MLPs with two residual layers, fi(x) =gi,1(gi,2(Wix + bi)), gi,",
  "where M is the product space (N \u222aP)\u00d7(N \u222aP), and f1, f2 are MLPs with two residual layers, fi(x) =gi,1(gi,2(Wix + bi)), gi,j(y) = ReLU(Vi,j ReLU(Ui,jy + pi,j)+ qi,j) + y. In the compound PCFG the rule probabilities \u03c0z given a latent vector z, \u03c0z,S\u2192A = exp(u\u22a4 A f1([wS; z]) + bA) P A\u2032\u2208N exp(u\u22a4 A\u2032 f1([wS; z]) + bA\u2032), \u03c0z,A\u2192BC = exp(u\u22a4 BC [wA; z] + bBC) P B\u2032C\u2032\u2208M exp(u\u22a4 B\u2032C\u2032 [wA; z] + bB\u2032C\u2032), \u03c0z,T\u2192w = exp(u\u22a4 w f2([wT ; z]) + bw) P w\u2032\u2208\u03a3 exp(u\u22a4 w\u2032 f2([wT ; z]) + bw\u2032).",
  "z] + bB\u2032C\u2032), \u03c0z,T\u2192w = exp(u\u22a4 w f2([wT ; z]) + bw) P w\u2032\u2208\u03a3 exp(u\u22a4 w\u2032 f2([wT ; z]) + bw\u2032). Again f1, f2 are as before where the \ufb01rst layer\u2019s input dimensions are appropriately changed to ac- count for concatenation with z. A.2 Corpus/Sentence F1 by Sentence Length For completeness we show the corpus-level and sentence-level F1 broken down by sentence length in Table 6, averaged across 4 different runs of each model. In Figure 1 we use the following to refer to rule probabilities of different rule types for the neural PCFG (left), \u03c0S = {\u03c0r | r \u2208L(S)}, \u03c0N = {\u03c0r | r \u2208L(A), A \u2208N}, \u03c0P = {\u03c0r | r \u2208L(T), T \u2208P}, \u03c0 = \u03c0S \u222a\u03c0N \u222a\u03c0P, where L(A) denotes the set of rules with A on the left hand side.",
  "The set of rule probabilities for the compound PCFG (right) is similarly de\ufb01ned, \u03c0z,S = {\u03c0z,r | r \u2208L(S)}, \u03c0z,N = {\u03c0z,r | r \u2208L(A), A \u2208N}, \u03c0z,P = {\u03c0z,r | r \u2208L(T), T \u2208P}, \u03c0z = \u03c0z,S \u222a\u03c0z,N \u222a\u03c0z,P. A.3 Experiments with RNNGs For experiments on supervising RNNGs with in- duced trees, we use the parameterization and hy- perparameters from Kim et al. (2019), which uses a 2-layer 650-dimensional stack LSTM (with dropout of 0.5) and a 650-dimensional tree LSTM (Tai et al., 2015; Zhu et al., 2015) as the composi- tion function. Concretely, the generative story is as follows: \ufb01rst, the stack representation is used to predict the next action (SHIFT or REDUCE) via an af\ufb01ne trans- formation followed by a sigmoid.",
  "Concretely, the generative story is as follows: \ufb01rst, the stack representation is used to predict the next action (SHIFT or REDUCE) via an af\ufb01ne trans- formation followed by a sigmoid. If SHIFT is cho- sen, we obtain a distribution over the vocabulary via another af\ufb01ne transformation over the stack representation followed by a softmax. Then we sample the next word from this distribution and shift the generated word onto the stack using the stack LSTM. If REDUCE is chosen, we pop the last two elements off the stack and use the tree LSTM to obtain a new representation. This new repre- sentation is shifted onto the stack via the stack LSTM. Note that this RNNG parameterization is slightly different than the original from Dyer et al. (2016), which does not ignore constituent labels and utilizes a bidirectional LSTM as the compo- sition function instead of a tree LSTM. As our RNNG parameterization only works with binary trees, we binarize the gold trees with right bina- rization for the RNNG trained on gold trees (trees from the unsupervised methods explored in this paper are already binary).",
  "As our RNNG parameterization only works with binary trees, we binarize the gold trees with right bina- rization for the RNNG trained on gold trees (trees from the unsupervised methods explored in this paper are already binary). The RNNG also trains a discriminative parser alongside the generative model for evaluation with importance sampling. We use a CRF parser whose span score parame- terization is similar similar to recent works (Wang and Chang, 2016; Stern et al., 2017; Kitaev and Klein, 2018): position embeddings are added to word embeddings, and a bidirectional LSTM with 256 hidden dimensions is run over the input rep- resentations to obtain the forward and backward hidden states. The score sij \u2208R for a constituent",
  "Sentence-level F1 WSJ-10 WSJ-20 WSJ-30 WSJ-40 WSJ-Full Left Branching 17.4 12.9 9.9 8.6 8.7 Right Branching 58.5 49.8 44.4 41.6 39.5 Random Trees 31.8 25.2 21.5 19.7 19.2 PRPN (tuned) 58.4 54.3 50.9 48.5 47.3 ON (tuned) 63.9 57.5 53.2 50.5 48.1 Neural PCFG 64.6 58.1 54.6 52.6 50.8 Compound PCFG 70.5 63.4 58.9 56.6 55.2 Oracle 82.1 84.1 84.2 84.3 84.3 Corpus-level F1 WSJ-10 WSJ-20 WSJ-30 WSJ-40 WSJ-Full Left Branching 16.5 11.7 8.5 7.2 6.",
  "1 84.1 84.2 84.3 84.3 Corpus-level F1 WSJ-10 WSJ-20 WSJ-30 WSJ-40 WSJ-Full Left Branching 16.5 11.7 8.5 7.2 6.0 Right Branching 58.9 48.3 42.5 39.4 36.1 Random Trees 31.9 23.9 20.0 18.1 16.4 PRPN (tuned) 59.3 53.6 49.7 46.9 44.5 ON (tuned) 64.7 56.3 51.5 48.3 45.6 Neural PCFG 63.5 56.8 53.1 51.0 48.7 Compound PCFG 70.6 62.0 57.1 54.6 52.4 Oracle 83.5 85.2 84.9 84.9 84.7 Table 6: Average unlabeled F1 for the various models broken down by sentence length on the PTB test set.",
  "6 62.0 57.1 54.6 52.4 Oracle 83.5 85.2 84.9 84.9 84.7 Table 6: Average unlabeled F1 for the various models broken down by sentence length on the PTB test set. For example WSJ-10 refers to F1 calculated on the subset of the test set where the maximum sentence length is at most 10. Scores are averaged across 4 runs of the model with different random seeds. Oracle is the performance of binarized gold trees (with right branching binarization). Top shows sentence-level F1 and bottom shows corpus-level F1. spanning the i-th and j-th word is given by, sij = MLP([\u2212\u2192 h j+1 \u2212\u2212\u2192 h i; \u2190\u2212 h i\u22121 \u2212\u2190\u2212 h j]), where the MLP has a single hidden layer with ReLU nonlinearity followed by layer normaliza- tion (Ba et al., 2016).",
  "For experiments on \ufb01ne-tuning the RNNG with the unsupervised RNNG, we take the discrimina- tive parser (which is also pretrained alongside the RNNG on induced trees) to be the structured in- ference network for optimizing the evidence lower bound. We refer the reader to Kim et al. (2019) and their open source implementation23 for addi- tional details. We also observe that as noted by Kim et al. (2019), a URNNG trained from scratch on this version of PTB without punctuation failed to outperform a right-branching baseline. The LSTM language model baseline is the same size as the stack LSTM (i.e. 2 layers, 650 hid- den units, dropout of 0.5), and is therefore equiv- alent to an RNNG with completely right branch- ing trees. The PRPN/ON baselines for perplex- ity/syntactic evaluation in Table 3 also have 2 layers with 650 hidden units and 0.5 dropout. Therefore all models considered in Table 3 have roughly the same capacity.",
  "The PRPN/ON baselines for perplex- ity/syntactic evaluation in Table 3 also have 2 layers with 650 hidden units and 0.5 dropout. Therefore all models considered in Table 3 have roughly the same capacity. For all models we share input/output word embeddings (Press and 23https://github.com/harvardnlp/urnng Wolf, 2016). Perplexity estimation for the RNNGs and the compound PCFG uses 1000 importance- weighted samples. For grammaticality judgment, we modify the publicly available dataset from Marvin and Linzen (2018)24 to only keep sentence pairs that did not have any unknown words with respect to our PTB vocabulary of 10K words. This results in 33K sen- tence pairs for evaluation. A.4 Nonterminal/Preterminal Alignments Figure 3 shows the part-of-speech alignments and Table 7 shows the nonterminal label alignments for the compound PCFG/neural PCFG. A.5 Subtree Analysis Table 8 lists more examples of constituents within each subtree as the top principical component is varied.",
  "A.5 Subtree Analysis Table 8 lists more examples of constituents within each subtree as the top principical component is varied. Due to data sparsity, the subtree analysis is performed on the full dataset. See section 5.2 for more details. 24https://github.com/BeckyMarvin/LM syneval",
  "Figure 3: Preterminal alignment to part-of-speech tags for the compound PCFG (top) and the neural PCFG (bottom).",
  "Label S SBAR NP VP PP ADJP ADVP Other Freq. Acc. NT-01 0.0% 0.0% 81.8% 1.1% 0.0% 5.9% 0.0% 11.2% 2.9% 13.8% NT-02 2.2% 0.9% 90.8% 1.7% 0.9% 0.0% 1.3% 2.2% 1.1% 44.0% NT-03 1.0% 0.0% 2.3% 96.8% 0.0% 0.0% 0.0% 0.0% 1.8% 37.1% NT-04 0.3% 2.2% 0.5% 2.0% 93.9% 0.2% 0.6% 0.3% 11.0% 64.9% NT-05 0.2% 0.0% 36.4% 56.",
  "2% 0.5% 2.0% 93.9% 0.2% 0.6% 0.3% 11.0% 64.9% NT-05 0.2% 0.0% 36.4% 56.9% 0.0% 0.0% 0.2% 6.2% 3.1% 57.1% NT-06 0.0% 0.0% 99.1% 0.0% 0.1% 0.0% 0.2% 0.6% 5.2% 89.0% NT-07 0.0% 0.0% 99.7% 0.0% 0.3% 0.0% 0.0% 0.0% 1.3% 59.3% NT-08 0.5% 2.2% 23.3% 35.6% 11.3% 23.6% 1.7% 1.7% 2.0% 44.",
  "0% 1.3% 59.3% NT-08 0.5% 2.2% 23.3% 35.6% 11.3% 23.6% 1.7% 1.7% 2.0% 44.3% NT-09 6.3% 5.6% 40.2% 4.3% 32.6% 1.2% 7.0% 2.8% 2.6% 52.1% NT-10 0.1% 0.1% 1.4% 58.8% 38.6% 0.0% 0.8% 0.1% 3.0% 50.5% NT-11 0.9% 0.0% 96.5% 0.9% 0.9% 0.0% 0.0% 0.9% 1.1% 42.9% NT-12 0.5% 0.2% 94.4% 2.4% 0.2% 0.",
  "9% 0.9% 0.0% 0.0% 0.9% 1.1% 42.9% NT-12 0.5% 0.2% 94.4% 2.4% 0.2% 0.1% 0.2% 2.0% 8.9% 74.9% NT-13 1.6% 0.1% 0.2% 97.7% 0.2% 0.1% 0.1% 0.1% 6.2% 46.0% NT-14 0.0% 0.0% 0.0% 98.6% 0.0% 0.0% 0.0% 1.4% 0.9% 54.1% NT-15 0.0% 0.0% 99.7% 0.0% 0.3% 0.0% 0.0% 0.0% 2.0% 76.9% NT-16 0.0% 0.",
  "0% 0.0% 99.7% 0.0% 0.3% 0.0% 0.0% 0.0% 2.0% 76.9% NT-16 0.0% 0.0% 0.0% 100.0% 0.0% 0.0% 0.0% 0.0% 0.3% 29.9% NT-17 96.4% 2.9% 0.0% 0.7% 0.0% 0.0% 0.0% 0.0% 1.2% 24.4% NT-18 0.3% 0.0% 88.7% 2.8% 0.3% 0.0% 0.0% 7.9% 3.0% 28.3% NT-19 3.9% 1.0% 86.6% 2.4% 2.6% 0.4% 1.3% 1.8% 4.",
  "0% 7.9% 3.0% 28.3% NT-19 3.9% 1.0% 86.6% 2.4% 2.6% 0.4% 1.3% 1.8% 4.5% 53.4% NT-20 0.0% 0.0% 99.0% 0.0% 0.0% 0.3% 0.2% 0.5% 7.4% 17.5% NT-21 94.4% 1.7% 2.0% 1.4% 0.3% 0.1% 0.0% 0.1% 6.2% 34.7% NT-22 0.1% 0.0% 98.4% 1.1% 0.1% 0.0% 0.2% 0.2% 3.5% 77.6% NT-23 0.4% 0.9% 14.0% 53.1% 8.",
  "4% 1.1% 0.1% 0.0% 0.2% 0.2% 3.5% 77.6% NT-23 0.4% 0.9% 14.0% 53.1% 8.2% 18.5% 4.3% 0.7% 2.4% 49.1% NT-24 0.0% 0.2% 1.5% 98.3% 0.0% 0.0% 0.0% 0.0% 2.3% 47.3% NT-25 0.3% 0.0% 1.4% 98.3% 0.0% 0.0% 0.0% 0.0% 2.2% 34.6% NT-26 0.4% 60.7% 18.4% 3.0% 15.4% 0.4% 0.4% 1.3% 2.1% 23.4% NT-27 0.",
  "6% NT-26 0.4% 60.7% 18.4% 3.0% 15.4% 0.4% 0.4% 1.3% 2.1% 23.4% NT-27 0.0% 0.0% 48.7% 0.5% 0.7% 13.1% 3.2% 33.8% 2.0% 59.7% NT-28 88.2% 0.3% 3.8% 0.9% 0.1% 0.0% 0.0% 6.9% 6.7% 76.5% NT-29 0.0% 1.7% 95.8% 1.0% 0.7% 0.0% 0.0% 0.7% 1.0% 62.8% NT-30 1.6% 94.5% 0.6% 1.2% 1.2% 0.0% 0.4% 0.",
  "0% 0.0% 0.7% 1.0% 62.8% NT-30 1.6% 94.5% 0.6% 1.2% 1.2% 0.0% 0.4% 0.4% 2.1% 49.4% NT-01 0.0% 0.0% 0.0% 99.2% 0.0% 0.0% 0.0% 0.8% 2.6% 41.1% NT-02 0.0% 0.3% 0.3% 99.2% 0.0% 0.0% 0.0% 0.3% 5.3% 15.4% NT-03 88.2% 0.3% 3.6% 1.0% 0.1% 0.0% 0.0% 6.9% 7.2% 71.4% NT-04 0.0% 0.0% 100.0% 0.",
  "3% 3.6% 1.0% 0.1% 0.0% 0.0% 6.9% 7.2% 71.4% NT-04 0.0% 0.0% 100.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.5% 2.4% NT-05 0.0% 0.0% 0.0% 96.6% 0.0% 0.0% 0.0% 3.4% 5.0% 1.2% NT-06 0.0% 0.4% 0.4% 98.8% 0.0% 0.0% 0.0% 0.4% 1.2% 43.7% NT-07 0.2% 0.0% 95.3% 0.9% 0.0% 1.6% 0.1% 1.9% 2.8% 60.",
  "4% 1.2% 43.7% NT-07 0.2% 0.0% 95.3% 0.9% 0.0% 1.6% 0.1% 1.9% 2.8% 60.6% NT-08 1.0% 0.4% 95.3% 2.3% 0.4% 0.2% 0.3% 0.2% 9.4% 63.0% NT-09 0.6% 0.0% 87.4% 1.9% 0.0% 0.0% 0.0% 10.1% 1.0% 33.8% NT-10 78.3% 17.9% 3.0% 0.5% 0.0% 0.0% 0.0% 0.3% 1.9% 42.0% NT-11 0.3% 0.0% 99.0% 0.3% 0.0% 0.",
  "5% 0.0% 0.0% 0.0% 0.3% 1.9% 42.0% NT-11 0.3% 0.0% 99.0% 0.3% 0.0% 0.3% 0.0% 0.0% 0.9% 70.3% NT-12 0.0% 8.8% 76.5% 2.9% 5.9% 0.0% 0.0% 5.9% 2.0% 3.6% NT-13 0.5% 2.0% 1.0% 96.6% 0.0% 0.0% 0.0% 0.0% 1.7% 50.7% NT-14 0.0% 0.0% 99.1% 0.0% 0.0% 0.6% 0.0% 0.4% 7.7% 14.8% NT-15 2.9% 0.",
  "0% 0.0% 99.1% 0.0% 0.0% 0.6% 0.0% 0.4% 7.7% 14.8% NT-15 2.9% 0.5% 0.4% 95.5% 0.4% 0.0% 0.0% 0.2% 4.4% 45.2% NT-16 0.4% 0.4% 17.9% 5.6% 64.1% 0.4% 6.8% 4.4% 1.4% 38.1% NT-17 0.1% 0.0% 98.2% 0.5% 0.1% 0.1% 0.1% 0.9% 9.6% 85.4% NT-18 0.1% 0.0% 95.7% 1.6% 0.0% 0.1% 0.2% 2.3% 4.",
  "1% 0.9% 9.6% 85.4% NT-18 0.1% 0.0% 95.7% 1.6% 0.0% 0.1% 0.2% 2.3% 4.7% 56.2% NT-19 0.0% 0.0% 98.9% 0.0% 0.4% 0.0% 0.0% 0.7% 1.3% 72.6% NT-20 2.0% 22.7% 3.0% 4.8% 63.9% 0.6% 2.3% 0.6% 6.8% 59.0% NT-21 0.0% 0.0% 14.3% 42.9% 0.0% 0.0% 42.9% 0.0% 2.2% 0.7% NT-22 1.4% 0.0% 11.0% 86.3% 0.",
  "3% 42.9% 0.0% 0.0% 42.9% 0.0% 2.2% 0.7% NT-22 1.4% 0.0% 11.0% 86.3% 0.0% 0.0% 0.0% 1.4% 1.0% 15.2% NT-23 0.1% 0.0% 58.3% 0.8% 0.4% 5.0% 1.7% 33.7% 2.8% 62.7% NT-24 0.0% 0.0% 100.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.6% 70.2% NT-25 2.2% 0.0% 76.1% 4.3% 0.0% 2.2% 0.0% 15.2% 0.4% 23.5% NT-26 0.",
  "2% NT-25 2.2% 0.0% 76.1% 4.3% 0.0% 2.2% 0.0% 15.2% 0.4% 23.5% NT-26 0.0% 0.0% 2.3% 94.2% 3.5% 0.0% 0.0% 0.0% 0.8% 24.0% NT-27 96.6% 0.2% 1.5% 1.1% 0.3% 0.2% 0.0% 0.2% 4.3% 32.2% NT-28 1.2% 3.7% 1.5% 5.8% 85.7% 0.9% 0.9% 0.3% 7.6% 64.9% NT-29 3.0% 82.0% 1.5% 13.5% 0.0% 0.0% 0.0% 0.",
  "9% 0.9% 0.3% 7.6% 64.9% NT-29 3.0% 82.0% 1.5% 13.5% 0.0% 0.0% 0.0% 0.0% 0.6% 45.4% NT-30 0.0% 0.0% 1.0% 60.2% 19.4% 1.9% 4.9% 12.6% 2.1% 10.4% Gold 15.0% 4.8% 38.5% 21.7% 14.6% 1.7% 0.8% 2.9% Table 7: Analysis of label alignment for nonterminals in the compound PCFG (top) and the neural PCFG (bottom). Label alignment is the proportion of correctly-predicted constistuents that correspond to a particular gold label. We also show the predicted constituent frequency and accuracy (i.e. precision) on the right. Bottom line shows the frequency in the gold trees.",
  "(NT-13 (T-12 w1) (NT-25 (T-39 w2) (T-58 w3))) would be irresponsible has been growing could be delayed \u2019ve been neglected can be held had been made can be proven had been canceled could be used have been wary (NT-04 (T-13 w1) (NT-12 (T-60 w2) (NT-18 (T-60 w3) (T-21 w4)))) of federally subsidized loans in fairly thin trading of criminal racketeering charges in quiet expiration trading for individual retirement accounts in big technology stocks without prior congressional approval from small price discrepancies between the two concerns by futures-related program buying (NT-04 (T-13 w1) (NT-12 (T-05 w2) (NT-01 (T-18 w3) (T-25 w4)))) by the supreme court in a stock-index arbitrage of the bankruptcy code as a hedging tool to the bankruptcy court of the bond market in a foreign court leaving the stock market for the supreme court after the new york (NT-12 (NT-20 (NT-20 (T-05 w1) (T-40 w2)) (T-40",
  "of the bankruptcy code as a hedging tool to the bankruptcy court of the bond market in a foreign court leaving the stock market for the supreme court after the new york (NT-12 (NT-20 (NT-20 (T-05 w1) (T-40 w2)) (T-40 w3)) (T-22 w4)) a syrian troop pullout the frankfurt stock exchange a conventional soviet attack the late sell programs the house-passed capital-gains provision a great buying opportunity the of\ufb01cial creditors committee the most active stocks a syrian troop withdrawal a major brokerage \ufb01rm (NT-21 (NT-22 (NT-20 (T-05 w1) (T-40 w2)) (T-22 w3)) (NT-13 (T-30 w4) (T-58 w5))) the frankfurt market was mixed the gramm-rudman targets are met the u.s. unit edged lower a private meeting is scheduled a news release was prepared the key assumption is valid the stock market closed wednesday the budget scorekeeping is completed the stock market remains fragile the tax bill is enacted (NT-03 (T-07 w1) (NT-19 (NT-20",
  "u.s. unit edged lower a private meeting is scheduled a news release was prepared the key assumption is valid the stock market closed wednesday the budget scorekeeping is completed the stock market remains fragile the tax bill is enacted (NT-03 (T-07 w1) (NT-19 (NT-20 (NT-20 (T-05 w2) (T-40 w3)) (T-40 w4)) (T-22 w5))) have a high default risk rejected a reagan administration plan have a lower default risk approved a short-term spending bill has a strong practical aspect has an emergency relief program have a good strong credit writes the hud spending bill have one big marketing edge adopted the underlying transportation measure (NT-13 (T-12 w1) (NT-25 (T-39 w2) (NT-23 (T-58 w3) (NT-04 (T-13 w4) (T-43 w5))))) has been operating in paris will be used for expansion has been taken in colombia might be room for \ufb02exibility has been vacant since july may be built in britain have been dismal for years will be supported by advertising has been improving since then could be used as",
  "(T-43 w5))))) has been operating in paris will be used for expansion has been taken in colombia might be room for \ufb02exibility has been vacant since july may be built in britain have been dismal for years will be supported by advertising has been improving since then could be used as weapons (NT-04 (T-13 w1) (NT-12 (NT-06 (NT-20 (T-05 w2) (T-40 w3)) (T-22 w4)) (NT-04 (T-13 w5) (NT-12 (T-18 w6) (T-53 w7))))) for a health center in south carolina with an opposite trade in stock-index futures by a federal jury in new york from the recent volatility in \ufb01nancial markets of the appeals court in new york of another steep plunge in stock prices of the further thaw in u.s.-soviet relations over the past decade as pension funds of the service corps of retired executives by a modest recovery in share prices (NT-10 (T-55 w1) (NT-05 (T-02 w2) (NT-19 (NT-06 (T-05",
  "in u.s.-soviet relations over the past decade as pension funds of the service corps of retired executives by a modest recovery in share prices (NT-10 (T-55 w1) (NT-05 (T-02 w2) (NT-19 (NT-06 (T-05 w3) (T-41 w4)) (NT-04 (T-13 w5) (NT-12 (T-60 w6) (T-21 w7)))))) to integrate the products into their operations to defend the company in such proceedings to offset the problems at radio shack to dismiss an indictment against her claiming to purchase one share of common stock to death some N of his troops to tighten their hold on their business to drop their inquiry into his activities to use the microprocessor in future products to block the maneuver on procedural grounds (NT-13 (T-12 w1) (NT-25 (T-39 w2) (NT-23 (T-58 w3) (NT-04 (T-13 w4) (NT-12 (NT-20 (T-05 w5) (T-40 w6)) (T-22 w7)))))) has been mentioned as a takeover candidate",
  "w2) (NT-23 (T-58 w3) (NT-04 (T-13 w4) (NT-12 (NT-20 (T-05 w5) (T-40 w6)) (T-22 w7)))))) has been mentioned as a takeover candidate would be run by the joint chiefs has been stuck in a trading range would be made into a separate bill had left announced to the trading mob would be included in the \ufb01nal bill only become active during the closing minutes would be costly given the \ufb01nancial arrangement will get settled in the short term would be restricted by a new bill (NT-10 (T-55 w) (NT-05 (T-02 w1) (NT-19 (NT-06 (T-05 w2) (T-41 w3)) (NT-04 (T-13 w4) (NT-12 (T-60 w5) (NT-18 (T-18 w6) (T-53 w7))))))) to supply that country with other defense systems to enjoy a loyalty among junk bond investors to transfer its skill at designing military equipment to transfer their business to other clearing \ufb01rms to improve the availability of quality legal service",
  "w5) (NT-18 (T-18 w6) (T-53 w7))))))) to supply that country with other defense systems to enjoy a loyalty among junk bond investors to transfer its skill at designing military equipment to transfer their business to other clearing \ufb01rms to improve the availability of quality legal service to soften the blow of declining stock prices to unveil a family of high-end personal computers to keep a lid on short-term interest rates to arrange an acceleration of planned tariff cuts to urge the fed toward lower interest rates (NT-21 (NT-22 (T-60 w1) (NT-18 (T-60 w2) (T-21 w3))) (NT-13 (T-07 w4) (NT-02 (NT-27 (T-47 w5) (T-50 w6)) (NT-10 (T-55 w7) (NT-05 (T-47 w8) (T-50 w9)))))) unconsolidated pretax pro\ufb01t increased N % to N billion amex short interest climbed N % to N shares its total revenue rose N % to N billion its pretax pro\ufb01t rose N % to N million total operating revenue grew N %",
  "(T-50 w9)))))) unconsolidated pretax pro\ufb01t increased N % to N billion amex short interest climbed N % to N shares its total revenue rose N % to N billion its pretax pro\ufb01t rose N % to N million total operating revenue grew N % to N billion its pretax pro\ufb01t rose N % to N billion its group sales rose N % to N billion \ufb01scal \ufb01rst-half sales slipped N % to N million total operating expenses increased N % to N billion total operating expenses increased N % to N billion Table 8: For each subtree (shown at the top of each set of examples), we perform PCA on the variational posterior mean vectors that are associated with that particular subtree and take the top principal component. We then list the top 5 constituents that had the lowest (left) and highest (right) principal component values."
]