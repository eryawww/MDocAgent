{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "list with 123 elements starting with...",
            [
                "Compound Probabilistic Context-Free Grammars for Grammar Induction Yoon Kim Harvard University Cambridge, MA, USA yoonkim@seas.harvard.edu Chris Dyer DeepMind London, UK cdyer@google.com Alexander M. Rush Harvard University Cambridge, MA, USA srush@seas.harvard.edu Abstract We study a formalization of the grammar induction problem that models sentences as being generated by a compound probabilis- tic context-free grammar. In contrast to traditional formulations which learn a sin- gle stochastic grammar, our grammar\u2019s rule probabilities are modulated by a per-sentence continuous latent variable, which induces marginal dependencies beyond the traditional context-free assumptions. Inference in this grammar is performed by collapsed variational inference, in which an amortized variational posterior is placed on the continuous variable, and the latent trees are marginalized out with dynamic programming. Experiments on En- glish and Chinese show the effectiveness of our approach compared to recent state-of-the- art methods when evaluated on unsupervised parsing. 1 Introduction Grammar induction is the task of inducing hier- archical syntactic structure from data. Statistical approaches to grammar induction require specify- ing a probabilistic grammar (e.g.",
                "1 Introduction Grammar induction is the task of inducing hier- archical syntactic structure from data. Statistical approaches to grammar induction require specify- ing a probabilistic grammar (e.g. formalism, num- ber and shape of rules), and \ufb01tting its parameters through optimization. Early work found that it was dif\ufb01cult to induce probabilistic context-free gram- mars (PCFG) from natural language data through direct methods, such as optimizing the log like- lihood with the EM algorithm (Lari and Young, 1990; Carroll and Charniak, 1992). While the rea- sons for the failure are manifold and not com- pletely understood, two major potential causes are the ill-behaved optimization landscape and the overly strict independence assumptions of PCFGs.",
                "While the rea- sons for the failure are manifold and not com- pletely understood, two major potential causes are the ill-behaved optimization landscape and the overly strict independence assumptions of PCFGs. More successful approaches to grammar induction have thus resorted to carefully-crafted auxiliary objectives (Klein and Manning, 2002), priors or Code: https:\/\/github.com\/harvardnlp\/compound-pcfg non-parametric models (Kurihara and Sato, 2006; Johnson et al., 2007; Liang et al., 2007; Wang and Blunsom, 2013), and manually-engineered fea- tures (Huang et al., 2012; Golland et al., 2012) to encourage the desired structures to emerge. We revisit these aforementioned issues in light of advances in model parameterization and infer- ence. First, contrary to common wisdom, we \ufb01nd that parameterizing a PCFG\u2019s rule probabil- ities with neural networks over distributed rep- resentations makes it possible to induce linguis- tically meaningful grammars by simply optimiz- ing log likelihood."
            ]
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-1906.10225.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 2048,
    "num_embeddings_est": 20739.000045776367,
    "avg_doclen_est": 168.60975646972656
}
