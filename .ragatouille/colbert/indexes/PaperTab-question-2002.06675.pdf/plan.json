{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Speech Corpus of Ainu Folklore and End-to-end Speech Recognition for Ainu Language Kohei Matsuura, Sei Ueno, Masato Mimura, Shinsuke Sakai, Tatsuya Kawahara Graduate School of Informatics, Kyoto University Sakyo-ku, Kyoto 606-8501, Japan {matsuura, ueno, mimura, sakai, kawahara}@sap.ist.i.kyoto-u.ac.jp Abstract Ainu is an unwritten language that has been spoken by Ainu people who are one of the ethnic groups in Japan. It is recognized as critically endangered by UNESCO and archiving and documentation of its language heritage is of paramount importance. Although a considerable amount of voice recordings of Ainu folklore has been produced and accumulated to save their culture, only a quite limited parts of them are transcribed so far. Thus, we started a project of automatic speech recognition (ASR) for the Ainu language in order to contribute to the development of annotated language archives. In this paper, we report speech corpus development and the structure and performance of end-to-end ASR for Ainu.",
            "Thus, we started a project of automatic speech recognition (ASR) for the Ainu language in order to contribute to the development of annotated language archives. In this paper, we report speech corpus development and the structure and performance of end-to-end ASR for Ainu. We investigated four modeling units (phone, syllable, word piece, and word) and found that the syllable-based model performed best in terms of both word and phone recognition accuracy, which were about 60% and over 85% respectively in speaker-open condition. Furthermore, word and phone accuracy of 80% and 90% has been achieved in a speaker-closed setting. We also found out that a multilingual ASR training with additional speech corpora of English and Japanese further improves the speaker-open test accuracy. 1. Introduction Automatic speech recognition (ASR) technology has been made a dramatic progress and is currently brought to a prat- ical levels of performance assisted by large speech corpora and the introduction of deep learning techniques. However, this is not the case for low-resource languages which do not have large corpora like English and Japanese have.",
            "However, this is not the case for low-resource languages which do not have large corpora like English and Japanese have. There are about 5,000 languages in the world over half of which are faced with the danger of extinction. Therefore, con- structing ASR systems for these endangered languages is an important issue. The Ainu are an indigenous people of northern Japan and Sakhakin in Russia, but their language has been fading away ever since the Meiji Restoration and Modernization. On the other hand, active efforts to preserve their culture have been initiated by the Government of Japan, and excep- tionally large oral recordings have been made. Neverthe- less, a majority of the recordings have not been transcribed and utilized effectively. Since transcribing them requires expertise in the Ainu language, not so many people are able to work on this task. Hence, there is a strong demand for an ASR system for the Ainu language. We started a project of Ainu ASR and this article is the \ufb01rst report of this project.",
            "Hence, there is a strong demand for an ASR system for the Ainu language. We started a project of Ainu ASR and this article is the \ufb01rst report of this project. We have built an Ainu speech corpus based on data pro- vided by the Ainu Museum1 and the Nibutani Ainu Culture Museum2. The oral recordings in this data consist of folk- lore and folk songs, and we chose the former to construct the ASR model. The end-to-end method of speech recog- nition has been proposed recently and has achieved perfor- mance comparable to that of the conventional DNN-HMM hybrid modeling (Chiu et al., 2017; Povey et al., 2018; Han et al., 2019). End-to-end systems do not have a complex hierarchical structure and do not require expertise in target languages such as their phonology and morphology.",
            "End-to-end systems do not have a complex hierarchical structure and do not require expertise in target languages such as their phonology and morphology. In this study we adopt the attention mechanism (Chorowski et al., 2014; Bahdanau et al., 2016) and combine it with Connec- 1http:\/\/ainugo.ainu-museum.or.jp\/ 2http:\/\/www.town.biratori.hokkaido.jp\/biratori\/nibutani\/ tionist Temporal Classi\ufb01cation (CTC) (Graves et al., 2006; Graves and Jaitly, 2014). In this work, we investigate the modeling unit and utilization of corpora of other languages. 2. Overview of the Ainu Language This section brie\ufb02y overviews the background of the data collection, the Ainu language, and its writing system. After that, we describe how Ainu recordings are classi\ufb01ed and review previous works dealing with the Ainu language. 2.1.",
            "After that, we describe how Ainu recordings are classi\ufb01ed and review previous works dealing with the Ainu language. 2.1. Background The Ainu people had total population of about 20,000 in the mid-19th century (Hardacre, 1997) and they used to live widely distributed in the area that includes Hokkaido, Sakhalin, and the Kuril Islands. The number of native speakers, however, rapidly decreased through the assimila- tion policy after late 19th century. At present, there are only less than 10 native speakers, and UNESCO listed their lan- guage as critically endangered in 2009 (Alexandre, 2010). In response to this situation, Ainu folklore and songs have been actively recorded since the late 20th century in efforts initiated by the Government of Japan. For example, the Ainu Museum started audio recording of Ainu folklore in 1976 with the cooperation of a few Ainu elders which re- sulted in the collection of speech data with the total dura- tion of roughly 700 hours.",
            "For example, the Ainu Museum started audio recording of Ainu folklore in 1976 with the cooperation of a few Ainu elders which re- sulted in the collection of speech data with the total dura- tion of roughly 700 hours. This kind of data should be a key to the understanding of Ainu culture, but most of it is not transcribed and fully studied yet. 2.2. The Ainu Language and its Writing System The Ainu language is an agglutinative language and has some similarities to Japanese. However, its genealogical relationship with other languages has not been clearly un- derstood yet. Among its features such as closed syllables and personal verbal af\ufb01xes, one important feature is that there are many compound words. For example, a word atuykorkamuy (means \u201ca sea turtle\u201d) can be disassembled into atuy (\u201cthe sea\u201d), kor (\u201cto have\u201d), and kamuy (\u201cgod\u201d). arXiv:2002.06675v3  [cs.CL]  16 May 2020",
            "Table 1: Speaker-wise details of the corpus Speaker ID KM UT KT HS NN KS HY KK total duration (h:m:s) 19:40:58 7:14:53 3:13:37 2:05:39 1:44:32 1:43:29 1:36:35 1:34:55 38:54:38 duration (%) 50.6 18.6 8.3 5.4 4.5 4.4 4.1 4.1 100.0 # episodes 29 26 20 8 8 11 8 7 114 # IPUs 9170 3610 2273 2089 2273 1302 1220 1109 22345 Although the Ainu people did not traditionally have a writ- ing system, the Ainu language is currently written follow- ing the examples in a reference book \u201cAkor itak\u201d (The Hokkaido Utari Association, 1994).",
            "With this writing sys- tem, it is transcribed with sixteen Roman letters {a, c, e, h, i, k, m, n, o, p, r, s, t, u, w, y}. Since each of these letters correspond to a unique pronunciation, we call them \u201cphones\u201d for convenience. In addition, the symbol {=} is used for connecting a verb and a personal af\ufb01x and { \u2019 } is used to represent the pharyngeal stop. For the purpose of transcribing recordings, consonant symbols {b, d, g, z} are additionally used to transcribe Japanese sounds the speak- ers utter. The symbols { , } are used to transcribe drops and liaisons of phones. An example is shown below. original mos=an hine inkar\u2019=an translation I wake up and look structure mos =an hine inkar =an wake up 1sg and look 1sg 2.3.",
            "An example is shown below. original mos=an hine inkar\u2019=an translation I wake up and look structure mos =an hine inkar =an wake up 1sg and look 1sg 2.3. Types of Ainu Recordings The Ainu oral traditions are classi\ufb01ed into three types: \u201cyukar\u201d (heroic epics), \u201ckamuy yukar\u201d (mythic epics), and \u201cuwepeker\u201d (prose tales). Yukar and kamuy yukar are re- cited in the rhythm while uwepeker is not. In this study we focus on the the prose tales as the \ufb01rst step. 2.4. Previous Work There have so far been a few studies dealing with the Ainu language. Senuma and Aizawa (2018) built a de- pendency tree bank in the scheme of Universal Dependen- cies3. Nowakowski et al. (2019) developed tools for part- of-speech (POS) tagging and word segmentation.",
            "Senuma and Aizawa (2018) built a de- pendency tree bank in the scheme of Universal Dependen- cies3. Nowakowski et al. (2019) developed tools for part- of-speech (POS) tagging and word segmentation. Ainu speech recognition was tried by Anastasopoulos and Chi- ang (2018) with 2.5 hours of Ainu folklore data even though the Ainu language was not their main target. Their phone error rare was about 40% which is not an accuracy level for practical use yet. It appears that there has not been a substantial Ainu speech recognition study yet that utilizes corpora of a reasonable size. Therefore, our \ufb01rst step was to build a speech cor- pus for ASR based on the data sets provided by the Ainu Museum and the Nibutani Ainu Culture Museum. 3https:\/\/universaldependencies.org\/ Table 2: Text excerpted from the prose tale \u2018The Boy Who Became Porosir God\u2019 spoken by KM.",
            "3https:\/\/universaldependencies.org\/ Table 2: Text excerpted from the prose tale \u2018The Boy Who Became Porosir God\u2019 spoken by KM. original English translation Samormosir mosir In neighboring country noski ta at the middle (of it), a=kor hapo i=resu hine being raised by my mother, oka=an pe ne hike I was leading my life. kunne hene tokap hene Night and day, all day long, yam patek i=pareoyki I was fed with chestnut yam patek a=e kusu and all I ate was chestnut, somo hetuku=an pe ne kunak so, that I would not grow up a=ramu a korka was my thought. 3. Ainu Speech Corpus In this section we explain the content of the data sets and how we modi\ufb01ed it for our ASR corpus. 3.1. Numbers of Speakers and Episodes The corpus we have prepared for ASR in this study is com- posed of text and speech. Table 1 shows the number of episodes and the total speech duration for each speaker.",
            "3.1. Numbers of Speakers and Episodes The corpus we have prepared for ASR in this study is com- posed of text and speech. Table 1 shows the number of episodes and the total speech duration for each speaker. Among the total of eight speakers, the data of the speakers KM and UT is from the Ainu Museum, and the rest is from Nibutani Ainu Culture Museum. All speakers are female. The length of the recording for a speaker varies depending on the circumstances at the recording times. A sample text and its English translation are shown in Table 2. 3.2. Data Annotation For ef\ufb01cient training of ASR model, we have made some modi\ufb01cations to the provided data. First, from the tran- scripts explained in Section 2.1, the symbols { , , \u2019} have been removed as seen in the example below.",
            "Data Annotation For ef\ufb01cient training of ASR model, we have made some modi\ufb01cations to the provided data. First, from the tran- scripts explained in Section 2.1, the symbols { , , \u2019} have been removed as seen in the example below. original uymam\u2019=an wa isam=an hi okake ta modi\ufb01ed uymam=an wa isam=an hi okake ta Though the equal symbol (\u2018=\u2019) does not represent a sound, we keep it because it is used in almost all of the Ainu doc- uments and provides grammatical information. To train an ASR system, the speech data needs to be seg- mented into a set of manageable chunks. For the ease of au- tomatic processing, we chose to segment speech into inter- pausal units (IPUs) (Koiso et al., 1998)which is a stretch of",
            "Acoustic Features Encoder (BiLSTM * 5) Decoder (LSTM * 1) Attention Fully Connected Cross Entropy CTC Label 1 ( phone \/ syllable \/ word piece \/ word ) Label 2 ( phone ) Loss Figure 1: The attention model with CTC auxiliary task. speech bounded by pauses. The number of IPUs for each speaker is shown in Table 1. 4. End-to-end Speech Recognition In this section, the two approaches to end-to-end speech recognition that we adopt in this work are summarized. Then, we introduce four modeling units we explained, i.e., phone, syllable, word piece, and word. We also discuss multilingual training that we adopt for tackling the low re- source problem. 4.1. End-to-end Modeling End-to-end models have an architecture much simpler than that of conventional DNN-HMM hybrid models. Since they predict character or word symbols directly from acoustic features, pronunciation dictionaries and language model- ing are not required explicitly. In this paper, we utilize two kinds of end-to-end models, namely, Connectionist Tempo- ral Classi\ufb01cation (CTC) and the attention-based encoder- decoder model.",
            "In this paper, we utilize two kinds of end-to-end models, namely, Connectionist Tempo- ral Classi\ufb01cation (CTC) and the attention-based encoder- decoder model. CTC augments the output symbol set with the \u201cblank\u201d sym- bol \u2018\u03c6\u2019. It outputs symbols by contracting frame-wise out- puts from recurrent neural networks (RNNs). This is done by \ufb01rst collapsed repeating symbols and then removing all blank symbols as in the following example: aab\u03c6bbccc \u2192abbc The probability of an output sequence L for an input acous- tic feature sequence X, where |L| < |X|, is de\ufb01ned as follows. p(L|X) = X \u03a0 \u2208B\u22121(L) |\u03a0 |=|X| p(\u03a0 |X) (1) B is a function to contract the outputs of RNNs, so B\u22121(L) means the set of symbol sequences which is reduced to L. The model is trained to maximize (1). The attention-based encoder-decoder model is another method for mapping between two sequences with different lengths.",
            "The attention-based encoder-decoder model is another method for mapping between two sequences with different lengths. It has two RNNs called the \u201cencoder\u201d and the \u201cde- coder\u201d. In naive encoder-decoder model, the encoder con- verts the input sequence into a single context vector which is the last hidden state of the encoder RNN from which the decoder infers output symbols. In an attention-based model, the context vector cl at l-th decoding step is the sum of the product of all encoder outputs h1, ..., hT and the l-th attention weight \u03b11,l, ..., \u03b1T,l as shown in (2). Here, T is the length of the encoder output. cl = T X t=1 \u03b1t,lht (2) The attention weights \u03b11,l, ..., \u03b1T,l indicates the relative importances of the encoder output frames for the l-th de- coding step and the model parameters to generate these weights are determined in an end-to-end training. In our model, the attention-based model and the CTC share the encoder and are optimized simultaneously as shown in Figure 1.",
            "In our model, the attention-based model and the CTC share the encoder and are optimized simultaneously as shown in Figure 1.(Kim et al., 2016) Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) is used for RNNs in the encoder and the decoder. 4.2. Modeling Units In the conventional DNN-HMM hybrid modeling, the acoustic model outputs probabilities triphone states from each acoustic feature which is converted into the most likely word sequence. An end-to-end model, on the other hand, has some degree of freedom in the modeling unit other than phones, and there are some studies that use char- acters or words as a unit (Chan et al., 2015; Li et al., 2018). A word unit based end-to-end model can take long context into consideration at the inference time, but it has the data sparsity problem due to its large vocabulary size. Though phone unit based model does not have such a problem, it cannot grasp so long context. It depends on the size of available corpora to decide which to adopt.",
            "Though phone unit based model does not have such a problem, it cannot grasp so long context. It depends on the size of available corpora to decide which to adopt. In addition to these both models, a word piece unit, which is de\ufb01ned by automatically dividing a word into frequent parts, has been proposed (Schuster and Nakajima, 2012; L\u00a8uscher et al., 2019), and its vocabulary size can be determined almost freely. In this paper, we investigate the modeling unit for the end- to-end Ainu speech recognition since the optimal unit for this size of corpus is not obvious. (Irie et al., 2019) It is pre- supposed that all units can be converted into word units au- tomatically. The candidates are phone, syllable, word piece (WP), and word. Examples of them are shown in Table 3 and the details of each unit are described below. 4.2.1. Phone As mentioned in Section 2.1, we regard the Roman letters as phones.",
            "The candidates are phone, syllable, word piece (WP), and word. Examples of them are shown in Table 3 and the details of each unit are described below. 4.2.1. Phone As mentioned in Section 2.1, we regard the Roman letters as phones. \u2018=\u2019 and the special symbol \u2018\u27e8wb\u27e9\u2019, which means a word boundary, are added to make it possible to convert the output into a sequence of words like the \u2018original\u2019 in Table 3.",
            "Table 3: Examples of four modeling units. original a=saha i=kokopan wa phone a = s a h a \u27e8wb\u27e9i = k o k o p a n \u27e8wb\u27e9w a syllable a = sa ha \u27e8wb\u27e9i = ko pan \u27e8wb\u27e9wa WP \u27e8wb\u27e9a = saha \u27e8wb\u27e9i = ko p an \u27e8wb\u27e9wa word a = saha i = \u27e8unk\u27e9wa translation my elder sister told me not to do so 4.2.2. Syllable A syllable of the Ainu language takes the form of either V, CV, VC, or CVC, where \u2018C\u2019 and \u2018V\u2019 mean consonant and vowel, respectively. The phones {a, e, i, o, u} are vowels and the rest of the Roman letters in Section 2.2 are conso- nants. In this work, every word is divided into syllables by the following procedure. 1. A word with a single letter is unchanged. 2. Two consecutive Cs and Vs are given a syllable bound- ary between them.",
            "In this work, every word is divided into syllables by the following procedure. 1. A word with a single letter is unchanged. 2. Two consecutive Cs and Vs are given a syllable bound- ary between them. R\u2217{CC, VV}R\u2217\u2192R\u2217{C-C, V-V}R\u2217 (R := {C, V}) 3. Put a syllable boundary after the segment-initial V if it is following by at least two phones. VCR+\u2192V-CR+ 4. Put a syllable boundary after CV repeatedly from left to right until only CV or CVC is left. (CV)\u2217{CV, CVC} \u2192(CV-)\u2217{CV, CVC} In addition, \u2018=\u2019 and \u2018\u27e8wb\u27e9\u2019 are added as explained in Sec- tion 4.2.1. through the model training process. This procedure does not always generate a morphologically relevant syllable segmentation.",
            "This procedure does not always generate a morphologically relevant syllable segmentation. For example, a word iser- makus (meaning \u201c(for a god) to protect from behind\u201d) is divided as i-ser-ma-kus, but the right syllabi\ufb01cation is i- ser-mak-us. 4.2.3. Word Piece The byte pair encoding (BPE) (Sennrich et al., 2015) and the unigram language modeling (Kudo, 2018) are alterna- tive methods for dividing a word into word pieces. The for- mer repeatedly replaces the most common character pair with a new single symbol until the vocabulary becomes the intended size. The latter decides the segmentation to maximize the likelihood of occurrence of the sequence. We adopt the latter and use the open-source software Sentence- Piece4 (Kudo and Richardson, 2018). With this tool, \u2018\u27e8wb\u27e9\u2019 and other units are often merged to constitute a single piece as seen in Table 3. 4.2.4. Word The original text can be segmented into words separated by spaces.",
            "With this tool, \u2018\u27e8wb\u27e9\u2019 and other units are often merged to constitute a single piece as seen in Table 3. 4.2.4. Word The original text can be segmented into words separated by spaces. To make the vocabulary smaller for the ease of 4https:\/\/github.com\/google\/sentencepiece Acoustic Features Encoder Attention Decoder CE Label 2\u2019 ( phone ) Decoder CE Label 1 Loss CTC FC Label 2 Loss CTC FC Ainu Japanese or English Label 2\u2019 ( phone ) Figure 2: The architecture of the multilingual learning with two corpora. \u2018FC\u2019 and \u2018CE\u2019 means \u2018fully connected\u2019 and \u2018cross-entropy\u2019 respectively. training, \u2018=\u2019 is treated as a word and infrequent words are replaced with a special label \u2018\u27e8unk\u27e9\u2019. As seen in Table 3, \u2018a=saha\u2019 is dealt with as three words (\u2018a\u2019, \u2018=\u2019, \u2018saha\u2019) and the word \u2018kokopan\u2019 is replaced with \u2018\u27e8unk\u27e9\u2019. 4.3.",
            "As seen in Table 3, \u2018a=saha\u2019 is dealt with as three words (\u2018a\u2019, \u2018=\u2019, \u2018saha\u2019) and the word \u2018kokopan\u2019 is replaced with \u2018\u27e8unk\u27e9\u2019. 4.3. Multilingual Training When an enough amount of data is not available for the tar- get languages, the ASR model training can be enhanced by taking advantage of data from other languages (Toshniwal et al., 2018; Cho et al., 2018). There are some similari- ties between Ainu and Japanese language (Tamura, 2013). For instance, both have almost the same set of vowels and do not have consonant clusters (like \u2018str\u2019 of \u2018strike\u2019 in En- glish). Hence, the multilingual training with a Japanese corpus is expected to be effective. In addition, an English corpus is used for the purpose of comparison. The corpora used are the JNAS corpus (Itou et al., 1999) (in Japanese) and the WSJ corpus (Paul and Baker, 1992) (in English).",
            "In addition, an English corpus is used for the purpose of comparison. The corpora used are the JNAS corpus (Itou et al., 1999) (in Japanese) and the WSJ corpus (Paul and Baker, 1992) (in English). JNAS comprises roughly 80 hours from 320 speakers, and WSJ has about 70 hours of speech from 280 speakers. In the multilingual training, the encoder and the attention module are shared among the Ainu ASR model and the models for other languages, and they are trained using data for all languages. Figure 2 shows the architecture for the multilingual learning with two corpora. When the input acoustic features are from the Ainu ASR corpus, they go through the shared encoder and attention module and are delivered into the decoder on the left side in Figure 2 as a context vector. In this case, the right-side decoder is not trained. 5. Experimental Evaluation In this section the setting and results of ASR experiments are described and the results are discussed. 5.1. Data Setup The ASR experiments were performed in speaker-open condition as well as speaker-closed condition.",
            "In this case, the right-side decoder is not trained. 5. Experimental Evaluation In this section the setting and results of ASR experiments are described and the results are discussed. 5.1. Data Setup The ASR experiments were performed in speaker-open condition as well as speaker-closed condition. In the speaker-closed condition, two episodes were set aside from each speaker as development and test sets. Thereafter,",
            "Table 4: ASR performance for each speaker and modeling unit. The lowest error rates for each unit are highlighted. units KM UT KT HS NN KS HY KK average speaker-closed WER (%) phone 22.2 28.5 24.2 28.6 27.2 30.6 40.4 36.1 27.9 syllable 13.2 18.4 19.6 29.4 26.7 26.7 38.9 29.0 21.7 WP 14.4 20.0 21.6 25.0 27.1 23.2 37.8 42.5 22.3 word 14.7 19.6 21.3 32.9 27.1 24.6 40.7 31.2 23.1 PER (%) phone 10.7 16.3 7.9 5.6 7.4 13.6 10.1 14.8 11.1 syllable 3.2 6.9 4.4 7.7 7.9 9.",
            "7 16.3 7.9 5.6 7.4 13.6 10.1 14.8 11.1 syllable 3.2 6.9 4.4 7.7 7.9 9.5 9.4 10.7 6.3 WP 4.7 8.0 5.2 6.7 8.4 6.8 10.4 12.6 7.1 word 11.2 12.9 12.6 24.0 17.1 15.4 27.0 20.1 15.9 speaker-open WER (%) phone - - 38.8 40.5 41.9 53.1 35.9 54.7 43.4 syllable - - 33.4 37.8 37.3 47.2 32.0 48.6 38.6 WP - - 58.4 37.2 38.6 47.9 32.6 48.8 45.7 word - - 34.0 49.",
            "8 37.3 47.2 32.0 48.6 38.6 WP - - 58.4 37.2 38.6 47.9 32.6 48.8 45.7 word - - 34.0 49.0 39.4 48.9 31.5 84.3 46.6 PER (%) phone - - 14.9 13.9 15.9 21.4 11.2 27.0 17.1 syllable - - 10.7 12.6 13.5 16.5 10.3 22.0 13.8 WP - - 41.5 14.1 15.9 19.3 11.5 23.6 23.6 word - - 24.6 39.9 29.6 33.1 20.4 67.0 34.8 the total sizes of the development and test sets turns out to be 1585 IPUs spanning 2 hours 23 minutes and 1841 IPUs spanning 2 hours and 48 minutes respectively.",
            "9 29.6 33.1 20.4 67.0 34.8 the total sizes of the development and test sets turns out to be 1585 IPUs spanning 2 hours 23 minutes and 1841 IPUs spanning 2 hours and 48 minutes respectively. The ASR model is trained with the rest data. In the speaker- open condition, all the data except for the test speaker\u2019s were used for training As it would be dif\ufb01cult to train the model if all of the data of speaker KM or UT were removed, experiments using their speaker-open conditions were not conducted. 5.2. Experimental Setting The input acoustic features were 120-dimensional vec- tors made by frame stacking (Tian et al., 2017) three 40- dimensional log-mel \ufb01lter banks features at contiguous time frames. The window length and the frame shift were set to be 25ms and 10ms. The encoder was composed of \ufb01ve BiLSTM layers and the attention-based decoder had a single layer of LSTM. Each LSTM had 320 cells and their weights were randomly initialized using a uniform distri- bution He et al.",
            "The encoder was composed of \ufb01ve BiLSTM layers and the attention-based decoder had a single layer of LSTM. Each LSTM had 320 cells and their weights were randomly initialized using a uniform distri- bution He et al. (2015) with biases of zero. The fully connected layers were initialized following U(\u22120.1, 0.1). The weight decay (Krogh and Hertz, 1992) whose rate was 10\u22125 and the dropout (Srivastava et al., 2014) following Be(0.2) were used to alleviate over\ufb01tting. The parameters were optimized with Adam (Kingma and Ba, 2014). The learning rate was 10\u22123 at \ufb01rst and was multiplied by 10\u22121 at the beginning of 31st and 36th epoch (You et al., 2019). The mini-batch size was 30 and the utterances (IPUs) were sorted in an ascending order of length. To stabilize the training, we removed utterances longer than 12 seconds.",
            "The mini-batch size was 30 and the utterances (IPUs) were sorted in an ascending order of length. To stabilize the training, we removed utterances longer than 12 seconds. The loss function of the model was a linear sum of the loss from CTC and the attention-based decoder, Lall = \u03bbLattn + (1 \u2212\u03bb)LCTC, (3) where \u03bb was set to be 0.5. Through all experiments, the phone labels are used to train the auxiliary CTC task be- cause it is reported that the hierarchical architecture, using few and general labels in the auxiliary task, improves the performance (Sanabria and Metze, 2018). Strictly speaking, the number of each modeling units de- pends on the training set, but there are roughly 25-phone, 500-syllable, and 5,000-word units including special sym- bols that represent the start and end of a sentence. The words occurring less than twice were replaced with \u2018\u27e8unk\u27e9\u2019. The vocabulary size for word piece modeling was set to be 500. These settings were based on the results of prelimi- nary experiments with the development set.",
            "The words occurring less than twice were replaced with \u2018\u27e8unk\u27e9\u2019. The vocabulary size for word piece modeling was set to be 500. These settings were based on the results of prelimi- nary experiments with the development set. For the multilingual training, we made three training scripts by concatenating the script of Ainu and other languages (JNAS, WSJ, JNAS and WSJ). The model was trained by these scripts until 30th epoch. From 31st and 40th epoch, the model was \ufb01ne-turned by the Ainu script. Phone units are used for JNAS and WSJ throughout the experiments. 5.3. Results Table 4 shows the phone error rates (PERs) and word error rates (WERs) for the speaker-closed and speaker-open set- tings. The \u2018average\u2019 is weighted by the numbers of tokens in the ground truth transcriptions for speaker-wise evalua- tion sets. The word recognition accuracy reached about 80% in the speaker-closed setting. In the speaker-open setting it was 60% on average and varied greatly from speaker to speaker (from 50% to 70%).",
            "The word recognition accuracy reached about 80% in the speaker-closed setting. In the speaker-open setting it was 60% on average and varied greatly from speaker to speaker (from 50% to 70%). The best phone accuracies in the speaker-closed and speaker-open settings were about 94% and 86%. Regardless of the settings, the syllable-based modeling yielded the best WER and PER. This suggests that syllables provide reasonable coverage and constraints for the Ainu language in a corpus of this size. The PERs of the word unit model were larger than those of other units. This is because the word model often out- puts the \u2018\u27e8unk\u27e9\u2019 symbols while other unit models are able to output symbols similar in sound as below.",
            "Table 5: Results of multilingual training. speaker- closed open WER (%) Ainu 21.7 38.6 + JNAS 21.1 34.8 + WSJ 21.3 35.8 + both 21.4 34.7 PER (%) Ainu 6.3 13.8 + JNAS 6.0 11.7 + WSJ 6.0 12.1 + both 6.0 11.2 ground-truth i okake un a unuhu a onaha syllable model piokake un a unuhu a onaha word model \u27e8unk\u27e9un a unuhu a onaha In this example, the PER of the syllable model is 5% and that of the word model is 30% even though the WERs are the same. (The output of the syllable model is rewritten into words using the \u2018\u27e8wb\u27e9\u2019 symbol.) WERs are generally much larger than PERs and it is fur- ther aggravated with the Ainu language.",
            "(The output of the syllable model is rewritten into words using the \u2018\u27e8wb\u27e9\u2019 symbol.) WERs are generally much larger than PERs and it is fur- ther aggravated with the Ainu language. This is because, as mentioned in Section 2.1, the Ainu language has a lot of compound words and the model may be confused about whether the output is multiple words or a single compound word. The actual outputs frequently contain errors as be- low. The WER of this example is 57% though the PER is zero. ground-truth nen poka apkas an mak an kusu output nenpoka apkas an makan kusu The results of multilingual training in which the modeling unit is syllables are presented in Table 5. All error rates are the weighted averages of all evaluated speakers. Here, \u2018+ both\u2019 represents the result of training with both JNAS and WSJ corpora. The multilingual training is effective in the speaker-open setting, providing a relative WER im- provement of 10%. The JNAS corpus was more helpful than the WSJ corpus because of the similarities between Ainu and Japanese language. 6.",
            "The multilingual training is effective in the speaker-open setting, providing a relative WER im- provement of 10%. The JNAS corpus was more helpful than the WSJ corpus because of the similarities between Ainu and Japanese language. 6. Summary In this study, we \ufb01rst developed a speech corpus for Ainu ASR and then, using the end-to-end model with CTC and the attention mechanism, compared four modeling units: phones, syllables, word pieces, and words. The best per- formance was obtained with the syllable unit, with which WERs in the speaker-closed and speaker-open settings were respectively about 20% and 40% while PERs were about 6% and 14%. Multilingual training using the JNAS improved the performance in the speaker-open setting. Fu- ture tasks include reducing the between-speaker perfor- mance differences by using speaker adaptation techniques. 7. Acknowledgement The data sets used in this study are provided by the Ainu Museum and Nibutani Ainu Culture Museum. The authors would like to thank Prof.",
            "Fu- ture tasks include reducing the between-speaker perfor- mance differences by using speaker adaptation techniques. 7. Acknowledgement The data sets used in this study are provided by the Ainu Museum and Nibutani Ainu Culture Museum. The authors would like to thank Prof. Osami Okuda of Sapporo Gakuin University for his useful advices on the Ainu language. 8. References Alexandre, M. C. N. (2010). Atlas of the World\u2019s Lan- guages in Danger, 3rd edn. Paris, UNESCO Publishing. Anastasopoulos, A. and Chiang, D. (2018). Tied multitask learning for neural speech translation. In Proc. NAACL HLT, volume 1, pages 82\u201391. Bahdanau, D., Chorowski, J., Serdyuk, D., Brakel, P., and Bengio, Y. (2016). End-to-end attention-based large vo- cabulary speech recognition.",
            "Bahdanau, D., Chorowski, J., Serdyuk, D., Brakel, P., and Bengio, Y. (2016). End-to-end attention-based large vo- cabulary speech recognition. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4945\u20134949, March. Chan, W., Jaitly, N., Le, Q. V., and Vinyals, O. (2015). Listen, attend and spell. CoRR, abs\/1508.01211. Chiu, C., Sainath, T. N., Wu, Y., Prabhavalkar, R., Nguyen, P., Chen, Z., Kannan, A., Weiss, R. J., Rao, K., Go- nina, K., Jaitly, N., Li, B., Chorowski, J., and Bacchi- ani, M. (2017). State-of-the-art speech recognition with sequence-to-sequence models. CoRR, abs\/1712.01769.",
            "(2017). State-of-the-art speech recognition with sequence-to-sequence models. CoRR, abs\/1712.01769. Cho, J., Baskar, M. K., Li, R., Wiesner, M., Mallidi, S. H. R., Yalta, N., Kara\ufb01\u00b4at, M., Watanabe, S., and Hori, T. (2018). Multilingual sequence-to-sequence speech recognition: architecture, transfer learning, and language modeling. CoRR, abs\/1810.03459. Chorowski, J., Bahdanau, D., Cho, K., and Bengio, Y. (2014). End-to-end continuous speech recognition using attention-based recurrent nn: First results. In NIPS 2014 Workshop on Deep Learning, December 2014. Graves, A. and Jaitly, N. (2014). Towards end-to-end speech recognition with recurrent neural networks. In ICML. Graves, A., Fern\u00b4andez, S., Gomez, F. J., and Schmidhu- ber, J. (2006).",
            "(2014). Towards end-to-end speech recognition with recurrent neural networks. In ICML. Graves, A., Fern\u00b4andez, S., Gomez, F. J., and Schmidhu- ber, J. (2006). Connectionist temporal classi\ufb01cation: la- belling unsegmented sequence data with recurrent neural networks. In ICML. Han, K. J., Prieto, R., Wu, K., and Ma, T. (2019). State- of-the-art speech recognition using multi-stream self- attention with dilated 1d convolutions. In arXiv preprint arXiv:1910.00716. Hardacre, H. (1997). New directions in the study of Meiji Japan. Brill, Leiden New York. He, K., Zhang, X., Ren, S., and Sun, J. (2015). Delving deep into recti\ufb01ers: Surpassing human- level performance on imagenet classi\ufb01cation. CoRR, abs\/1502.01852.",
            "He, K., Zhang, X., Ren, S., and Sun, J. (2015). Delving deep into recti\ufb01ers: Surpassing human- level performance on imagenet classi\ufb01cation. CoRR, abs\/1502.01852. Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural Comput., 9(8):1735\u20131780, November. Irie, K., Prabhavalkar, R., Kannan, A., Bruguier, A., Ry- bach, D., and Nguyen, P. (2019). Model unit exploration for sequence-to-sequence speech recognition. CoRR, abs\/1902.01955. Itou, K., Yamamoto, M., Takeda, K., Takezawa, T., Mat- suoka, T., Kobayashi, T., Shikano, K., and Itahashi, S. (1999). Jnas: Japanese speech corpus for large vocabu- lary continuous speech recognition research.",
            "(1999). Jnas: Japanese speech corpus for large vocabu- lary continuous speech recognition research. Journal of the Acoustical Society of Japan (E), 20(3):199\u2013206.",
            "Kim, S., Hori, T., and Watanabe, S. (2016). Joint ctc- attention based end-to-end speech recognition using multi-task learning. CoRR, abs\/1609.06773. Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. CoRR, abs\/1412.6980. Koiso, H., Horiuchi, Y., Tutiya, S., Ichikawa, A., and Den, Y. (1998). An analysis of turn-taking and backchannels based on prosodic and syntactic features in japanese map task dialogs. Language and Speech, 41(3-4):295\u2013321. PMID: 10746360. Krogh, A. and Hertz, J. A. (1992). A simple weight de- cay can improve generalization. In ADVANCES IN NEU- RAL INFORMATION PROCESSING SYSTEMS 4, pages 950\u2013957. Morgan Kaufmann. Kudo, T. and Richardson, J. (2018).",
            "(1992). A simple weight de- cay can improve generalization. In ADVANCES IN NEU- RAL INFORMATION PROCESSING SYSTEMS 4, pages 950\u2013957. Morgan Kaufmann. Kudo, T. and Richardson, J. (2018). Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. CoRR, abs\/1808.06226. Kudo, T. (2018). Subword regularization: Improving neu- ral network translation models with multiple subword candidates. CoRR, abs\/1804.10959. Li, J., Ye, G., Das, A., Zhao, R., and Gong, Y. (2018). Advancing acoustic-to-word CTC model. CoRR, abs\/1803.05566. L\u00a8uscher, C., Beck, E., Irie, K., Kitza, M., Michel, W., Zeyer, A., Schl\u00a8uter, R., and Ney, H. (2019). RWTH ASR systems for librispeech: Hybrid vs attention - w\/o data augmentation.",
            "(2019). RWTH ASR systems for librispeech: Hybrid vs attention - w\/o data augmentation. CoRR, abs\/1905.03072. Nowakowski, K., Ptaszynski, M., Masui, F., and Momouci, Y. (2019). Applying support vector machines to pos tag- ging of the ainu language. Proc. Computational Methods for Endangered Languages, 2(4). Paul, D. B. and Baker, J. M. (1992). The design for the wall street journal-based CSR corpus. In Speech and Natural Language: Proceedings of a Workshop Held at Harri- man, New York, February 23-26, 1992. Povey, D., Cheng, G., Wang, Y., Li, K., Xu, H., Yarmoham- madi, M., and Khudanpur, S. (2018). Semi-orthogonal low-rank matrix factorization for deep neural networks. In Proc. Interspeech 2018, pages 3743\u20133747.",
            "(2018). Semi-orthogonal low-rank matrix factorization for deep neural networks. In Proc. Interspeech 2018, pages 3743\u20133747. Sanabria, R. and Metze, F. (2018). Hierarchical multi task learning with CTC. CoRR, abs\/1807.07104. Schuster, M. and Nakajima, K. (2012). Japanese and ko- rean voice search. 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5149\u20135152. Sennrich, R., Haddow, B., and Birch, A. (2015). Neural machine translation of rare words with subword units. CoRR, abs\/1508.07909. Senuma, H. and Aizawa, A. (2018). Universal Depen- dencies for Ainu.",
            "(2015). Neural machine translation of rare words with subword units. CoRR, abs\/1508.07909. Senuma, H. and Aizawa, A. (2018). Universal Depen- dencies for Ainu. In Nicoletta Calzolari (Conference chair), et al., editors, Proceedings of the Eleventh Inter- national Conference on Language Resources and Eval- uation (LREC 2018), Miyazaki, Japan, May 7-12, 2018. European Language Resources Association (ELRA). Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: A simple way to prevent neural networks from over\ufb01tting. Journal of Machine Learning Research, 15:1929\u20131958. Tamura, S. (2013). Ainu Go No Sekai (The World of Ainu Language). Yoshikawa-Kobun Kan. The Hokkaido Utari Association. (1994).",
            "Tamura, S. (2013). Ainu Go No Sekai (The World of Ainu Language). Yoshikawa-Kobun Kan. The Hokkaido Utari Association. (1994). Akor itak. CREWS. Tian, X., Zhang, J., Ma, Z., He, Y., and Wei, J. (2017). Frame stacking and retaining for recurrent neural net- work acoustic model. CoRR, abs\/1705.05992. Toshniwal, S., Sainath, T. N., Weiss, R. J., Li, B., Moreno, P., Weinstein, E., and Rao, K. (2018). Multilingual Speech Recognition with A Single End-To-End Model. In Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). You, K., Long, M., Wang, J., and Jordan, M. I. (2019). How does learning rate decay help modern neural net- works? In arXiv preprint arXiv:1908.01878."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-2002.06675.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 8573.00033569336,
    "avg_doclen_est": 171.4600067138672
}
