{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "index_bsize": 32,
        "nbits": 4,
        "kmeans_niters": 20,
        "resume": false,
        "pool_factor": 1,
        "clustering_mode": "hierarchical",
        "protected_tokens": 0,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "Local Contextual Attention with Hierarchical Structure for Dialogue Act Recognition Zhigang Dai1, Jinhua Fu2, Qile Zhu3, Hengbin Cui2, Xiaolong Li2 and Yuan Qi2 1South China University of Technology 2 Ant Financial Service Group Abstract Dialogue act recognition is a fundamental task for an intelligent dialogue system. Previous work models the whole dialog to predict dia- log acts, which may bring the noise from un- related sentences. In this work, we design a hierarchical model based on self-attention to capture intra-sentence and inter-sentence in- formation. We revise the attention distribution to focus on the local and contextual semantic information by incorporating the relative posi- tion information between utterances. Based on the found that the length of dialog affects the performance, we introduce a new dialog seg- mentation mechanism to analyze the effect of dialog length and context padding length un- der online and of\ufb02ine settings. The experi- ment shows that our method achieves promis- ing performance on two datasets: Switchboard Dialogue Act and DailyDialog with the accu- racy of 80.34% and 85.81% respectively.",
            "The experi- ment shows that our method achieves promis- ing performance on two datasets: Switchboard Dialogue Act and DailyDialog with the accu- racy of 80.34% and 85.81% respectively. Vi- sualization of the attention weights shows that our method can learn the context dependency between utterances explicitly. 1 Introduction Dialogue act (DA) characterizes the type of a speaker\u2019s intention in the course of producing an utterance and is approximately equivalent to the illocutionary act of Austin (1962) or the speech act of Searle (1969). The recognition of DA is essential for modeling and automatically detect- ing discourse structure, especially in developing a human-machine dialogue system. It is natural to predict the Answer acts following an utterance of type Question, and then match the Question utter- ance to each QA-pair in the knowledge base. The predicted DA can also guide the response genera- tion process (Zhao et al., 2017). For instance, sys- tem generates a Greeting type response to former Greeting type utterance.",
            "The predicted DA can also guide the response genera- tion process (Zhao et al., 2017). For instance, sys- tem generates a Greeting type response to former Greeting type utterance. Moreover, DA is bene- \ufb01cial to other online dialogue strategies, such as DA Utterance conventional-opening B: Hi, conventional-opening B: this is Donna Donahue. conventional-opening A: Hi, Donna. conventional-opening B: Hi. yes-no-question A: Ready to get started? yes answers B: Uh, yeah, statement-non-opinion B: I think so. other A: Okay. statement-non-opinion A: Sort of an interesting topic since I just got back from lunch here. acknowledge B: Okay. Table 1: A snippet of a conversation with the DA labels from Switchboard dataset. con\ufb02ict avoidance (Nakanishi et al., 2018). In the of\ufb02ine system, DA also plays a signi\ufb01cant role in summarizing and analyzing the collected utter- ances.",
            "con\ufb02ict avoidance (Nakanishi et al., 2018). In the of\ufb02ine system, DA also plays a signi\ufb01cant role in summarizing and analyzing the collected utter- ances. For instance, recognizing DAs of a wholly online service record between customer and agent is bene\ufb01cial to mine QA-pairs, which are selected and clustered then to expand the knowledge base. DA recognition is challenging due to the same ut- terance may have a different meaning in a different context. Table 1 shows an example of some utter- ances together with their DAs from Switchboard dataset. In this example, utterance \u201cOkay.\u201d corre- sponds to two different DA labels within different semantic context. Many approaches have been proposed for DA recognition. Previous work relies heavily on hand- crafted features which are domain-speci\ufb01c and dif\ufb01cult to scale up (Stolcke et al., 2000; Kim et al., 2010; Tava\ufb01et al., 2013).",
            "Previous work relies heavily on hand- crafted features which are domain-speci\ufb01c and dif\ufb01cult to scale up (Stolcke et al., 2000; Kim et al., 2010; Tava\ufb01et al., 2013). Recently, with great ability to do feature extraction, deep learn- ing has yielded state-of-the-art results for many NLP tasks, and also makes impressive advances in DA recognition. Liu et al. (2017); Bothe et al. (2018) built hierarchical CNN\/RNN models to en- code sentence and incorporate context information for DA recognition. Kumar et al. (2018) achieved promising performance by adding the CRF to en- arXiv:2003.06044v1  [cs.CL]  12 Mar 2020",
            "hance the dependency between labels. Raheja and Tetreault (2019) applied the self-attention mecha- nism coupled with a hierarchical recurrent neural network. However, previous approaches cannot make full use of the relative position relationship between utterances. It is natural that utterances in the local context always have strong dependencies in our daily dialog. In this paper, we propose a hierarchi- cal model based on self-attention (Vaswani et al., 2017) and revise the attention distribution to focus on a local and contextual semantic information by a learnable Gaussian bias which represents the rel- ative position information between utterances, in- spired by Yang et al. (2018). Further, to analyze the effect of dialog length quantitatively, we in- troduce a new dialog segmentation mechanism for the DA task and evaluate the performance of dif- ferent dialogue length and context padding length under online and of\ufb02ine settings. Experiment and visualization show that our method can learn the local contextual dependency between utterances explicitly and achieve promising performance in two well-known datasets.",
            "Experiment and visualization show that our method can learn the local contextual dependency between utterances explicitly and achieve promising performance in two well-known datasets. The contributions of this paper are: \u2022 We design a hierarchical model based on self- attention and revise the attention distribution to focus on a local and contextual semantic information by the relative position informa- tion between utterances. \u2022 We introduce a new dialog segmentation mechaism for the DA task and analyze the effect of dialog length and context padding length. \u2022 In addition to traditional of\ufb02ine prediction, we also analyze the accuracy and time com- plexity under the online setting. 2 Background 2.1 Related Work DA recognition is aimed to assign a label to each utterance in a conversation. It can be formulated as a supervised classi\ufb01cation problem.",
            "2 Background 2.1 Related Work DA recognition is aimed to assign a label to each utterance in a conversation. It can be formulated as a supervised classi\ufb01cation problem. There are two trends to solve this problem: 1) as a sequence la- beling problem, it will predict the labels for all ut- terances in the whole dialogue history (Dielmann and Renals, 2008; Lee and Dernoncourt, 2016; Kumar et al., 2018); 2) as a sentence classi\ufb01ca- tion problem, it will treat utterance independently without any context history (Kim et al., 2010; Khanpour et al., 2016). Early studies rely heavily on handcrafted features such as lexical, syntactic, contextual, prosodic and speaker information and achieve good results (Dielmann and Renals, 2008; Stolcke et al., 2000; Chen and Di Eugenio, 2013). Recent studies have applied deep learning based model for DA recognition.",
            "Recent studies have applied deep learning based model for DA recognition. Lee and Dernon- court (2016) proposed a model based on RNNs and CNNs that incorporates preceding short texts to classify current DAs. Liu et al. (2017); Bothe et al. (2018) used hierarchical CNN and RNN to model the utterance sequence in the conversation, which can extract high-level sentence information to predict its label. They found that there is a small performance difference among different hierarchi- cal CNN and RNN approaches. Kumar et al. (2018) added a CRF layer on the top of the hier- archical network to model the label transition de- pendency. Raheja and Tetreault (2019) applied the context-aware self-attention mechanism cou- pled with a hierarchical recurrent neural network and got a signi\ufb01cant improvement over state-of- the-art results on SwDA datasets. On another as- pect, Ji et al. (2016) combined a recurrent neu- ral network language model with a latent variable model over DAs. Zhao et al.",
            "On another as- pect, Ji et al. (2016) combined a recurrent neu- ral network language model with a latent variable model over DAs. Zhao et al. (2018) proposed a Discrete Information Variational Autoencoders (DI-VAE) model to learn discrete latent actions to incorporate sentence-level distributional seman- tics for dialogue generation. 2.2 Self-Attention Self-attention (Vaswani et al., 2017) achieves great success for its ef\ufb01ciently parallel computation and long-range dependency modeling. Given the input sequence s = (s1, ..., sn) of n elements where si \u2208Rds. Each attention head holds three parameter matrices, W Q h , W K h , W V h \u2208 Rds\u00d7dz where h present the index of head. For the head h, linear projection is applied to the sequence s to obtain key (K), query (Q), and value (V) repre- sentations. the attention module gets the weight by computing dot-products between key\/query pair and then softmax normalizes the result.",
            "For the head h, linear projection is applied to the sequence s to obtain key (K), query (Q), and value (V) repre- sentations. the attention module gets the weight by computing dot-products between key\/query pair and then softmax normalizes the result. it is de- \ufb01ned as: ATTh(Q, K, V ) = softmax(QKT \u221adz ) \u00d7 V, where \u221adz is the scaling factor to counteract this effect that the dot products may grow large in mag-",
            "\u2026 u0 S0 w0 w1 wn \u2026 u1 S1 w0 w1 wn \u2026 u\u2026 S\u2026 w0 w1 wn \u2026 un Sn w0 w1 wn + + + + y0 y\u2026 yn Utterance Encoder Attention Layer Label Prediction y1 Word Representation Figure 1: The model structure for DA recognition, where the LSTM with max pooling is simpli\ufb01ed as utterance encoder in our experiment. The area in the red dashed line represents the structure for online prediction. nitude. For all the heads, output = Concat(ATT1, ..., ATTh) \u00d7 W O, where W O \u2208R(dz\u2217h)\u00d7ds is the output projection. One weakness of self-attention model it that they cannot encode the position information ef- \ufb01ciently. Some methods have been proposed to encode the relative or absolute position of to- kens in the sequence as the additional input to the model. Vaswani et al. (2017) used sine and cosine functions of different frequencies and added posi- tional encodings to the input embeddings together.",
            "Some methods have been proposed to encode the relative or absolute position of to- kens in the sequence as the additional input to the model. Vaswani et al. (2017) used sine and cosine functions of different frequencies and added posi- tional encodings to the input embeddings together. It used absolute position embedding to capture rel- ative positional relation by the characteristic of sine and cosine functions. Moreover, several stud- ies show that explicitly modeling relative posi- tion can further improve performance. For ex- ample, Shaw et al. (2018) proposed relative posi- tion encoding to explicitly model relative position by independent semantic parameter. It demon- strated signi\ufb01cant improvements even when en- tirely replacing conventional absolute position en- codings. Yang et al. (2018) proposed to model lo- calness for the self-attention network by a learn- able Gaussian bias which enhanced the ability to model local relationship and demonstrated the ef- fectiveness on the translation task. In our study, we design a local contextual atten- tion model, which incorporates relative position information by a learnable Gaussian bias into orig- inal attention distribution.",
            "In our study, we design a local contextual atten- tion model, which incorporates relative position information by a learnable Gaussian bias into orig- inal attention distribution. Different from Yang et al. (2018), in our method, the distribution cen- ter is regulated around the corresponding utterance with a window, which indicates the context depen- dency preference, for capturing more local contex- tual dependency. 3 Methodology Before we describe the proposed model in de- tail, we \ufb01rst de\ufb01ne the mathematical notation for the DA recognition task in this paper. Given the dataset, X = (D1, D2, ...DL) with correspond- ing DA labels (Y1, Y2, ...YL). Each dialogue is a sequence of Nl utterances Dl = (u1, u2, ...uNl) with Yl = (y1, y2, ...yNl). Each utterance is padded or truncated to the length of M words, uj = (w1, w2, ...wM). Figure 1 shows our overall model structure.",
            "Each utterance is padded or truncated to the length of M words, uj = (w1, w2, ...wM). Figure 1 shows our overall model structure. For the \ufb01rst layer, we encode each utterance uj into a vector representation. Each word wm of the utter- ance uj is converted into dense vector representa- tions em from one-hot token representation. And then, we apply LSTM (Hochreiter and Schmidhu- ber, 1997), a powerful and effective structure for sequence modeling, to encode the word sequence. Formally, for the utterance uj: em = embed(wm) \u2200m \u2208{1, ...M}, (1) hm = LSTM(hm\u22121, em) \u2200m \u2208{1, ...M}, (2) where embed represents the embedding layer which can be initialized by pre-trained embed- dings. To make a fair comparison with previ- ous work, we do not use the \ufb01ne-grained em- bedding presented in Chen et al. (2018). LSTM helps us get the context-aware sentence represen-",
            "tation for the input sequence. There are several ap- proaches to represent the sentence from the words. Following Alexis et al. (2017), we add a max- pooling layer after LSTM, which selects the max- imum value in each dimension from the hidden units. In our experiment, LSTM with max-pooling does perform a little better than LSTM with last- pooling, which is used in Kumar et al. (2018). Afterwards, we get the utterances vector repre- sentations u = (u1, ..., uNl) of Nl elements for the dialogue Dl where uj \u2208Rds, ds is the dimension of hidden units. As we discussed in section 2.2, given the sequence s \u2208RNl\u2217ds, self-attention mechanism calculates the attention weights be- tween each pair of utterances in the sequence and get the weighted sum as output. The attention module explicitly models the context dependency between utterances. We employ a residual connec- tion (He et al., 2016) around the attention module, which represents the dependency encoder between utterances, and the current utterance encoder s: output = output + s.",
            "The attention module explicitly models the context dependency between utterances. We employ a residual connec- tion (He et al., 2016) around the attention module, which represents the dependency encoder between utterances, and the current utterance encoder s: output = output + s. (3) Finally, we apply a two-layer fully connected network with a Recti\ufb01ed Linear Unit (ReLU) to get the \ufb01nal classi\ufb01cation output for each utter- ance. 3.1 Modeling Local Contextual Attention The attention explicitly models the interaction be- tween the utterances. However, for context mod- eling, original attention mechanism always con- siders all of the utterances in a dialogue which inhibits the relation among the local context and is prone to over\ufb01tting during training. It is nat- ural that utterances in the local context always have strong dependencies in our daily dialog. Therefore, we add a learnable Gaussian bias with the local constraint to the weight normalized by softmax to enhance the interaction between con- cerned utterances and its neighbors. The attention module formula is revised as: ATT(Q, K) = softmax(QKT \u221a d + POS).",
            "Therefore, we add a learnable Gaussian bias with the local constraint to the weight normalized by softmax to enhance the interaction between con- cerned utterances and its neighbors. The attention module formula is revised as: ATT(Q, K) = softmax(QKT \u221a d + POS). (4) The \ufb01rst term is the original dot product self- attention model. POS \u2208RN\u00d7N is the bias ma- trix, where N is the length of dialogue. The ele- ment POSi,j is de\ufb01ned following by gaussian dis- tribution: POSi,j = \u2212(j \u2212ci)2 2w2 i , (5) POSi,j measures the dependency between the ut- terance uj and the utterance ui in terms of the relative position prior. wi represents for the stan- dard deviation, which controls the weight decay- ing.",
            "wi represents for the stan- dard deviation, which controls the weight decay- ing. Because of local constraint, |ci \u2212i| <= C, for each utterance ui, the predicted center position ci and window size wi is de\ufb01ned as followed: ci = i + C \u00d7 tanh(W c i \u00d7 K), (6) wi = D \u00d7 sigmoid(W d i \u00d7 K), (7) where W c i , W d i \u2208R1\u2217N are both learnable param- eters. We initialized the parameter W c i to 0, which leads to center position ci = i by default. Further- more, ci and wi are both related to the semantic context of the utterances, so we assign the mean of key K in attention mechanism to represent the context information. Moreover, the central posi- tion also indicates the dependency preference of the preceding utterances or subsequent utterances. It is worth noting that there is a little difference with Yang et al. (2018), although we both revise the attention module by the Gaussian distribution.",
            "Moreover, the central posi- tion also indicates the dependency preference of the preceding utterances or subsequent utterances. It is worth noting that there is a little difference with Yang et al. (2018), although we both revise the attention module by the Gaussian distribution. In our method, for the given utterance ui, the dis- tribution center ci is regulated for capturing the not only local but also contextual dependency, which can be formally expressed as: ci \u2208(i \u2212C, i + C). However, in their work, the distribution center can be anywhere in the sequence, and it is designed for capturing the phrasal patterns, which are essential for Neural Machine Translation task. 3.2 Online and Of\ufb02ine Predictions Previous work mainly focuses on the of\ufb02ine set- ting where we can access the whole utterances in the dialogue and predict all the DA labels simulta- neously. However, the online setting is the nat- ural demand in our real-time applications.",
            "However, the online setting is the nat- ural demand in our real-time applications. For the online setting, we only care about the recog- nition result of the last utterance in the given con- text, as seen in the area with the red dashed line in Figure 1, our model is well compatible with on- line setting, we can calculate the attention between the last utterance and the other utterances directly where K \u2208R1\u00d7d, Q \u2208Rn\u00d7d, V \u2208Rn\u00d7d. For LSTM, we still have to model the entire sequence, which is slower than attention based models. Ta- ble 2 shows the time complexity comparison ex- cluding the time cost of \ufb01rst layer encoding, and the dialogue length n is smaller than the repre- sentation dimension d. Our model is easy to ex- pand into the online setting, however, to have a",
            "fair comparison with previous work, in our exper- iments, we applied the models under the of\ufb02ine setting by default. 3.3 Separate into Sub-dialogues The length of different dialogues in the dataset varies a lot. It is worth noting that the length of dialog affects the model prediction. On the one hand, under the of\ufb02ine setting, we can ac- cess the whole utterances in the dialogue and predict all the DA labels simultaneously, so the more utterances, the more ef\ufb01cient. However, on the other hand, if we put too many utter- ances in once prediction, it will model too much unrelated dependency in the long utterances se- quence for both LSTM and attention mechanism based model. The sub-dialogues with the same length also enable ef\ufb01ciently batch training. To study how the dialogue length and context padding length will affect the performance, so we de\ufb01ned a sliding window W which is the sub-dialogue length. Then, we separate each long dialogue into several small sub-dialogues.",
            "To study how the dialogue length and context padding length will affect the performance, so we de\ufb01ned a sliding window W which is the sub-dialogue length. Then, we separate each long dialogue into several small sub-dialogues. For example, the dialog D is a sequence of utterances with length n, and we will get \u2308x\/w\u2309sub-dialogues, for the k-th sub-dialogues, the utterances sequence is (u(k\u22121)\u2217W+1, u(k\u22121)\u2217W+2, ..., uk\u2217W ). In or- der to avoid losing some context information caused by being separated, which will affect the context modeling for the utterances in the begin and end of the sub-dialog, we add the corresponding context with P (stands for con- text padding) utterances at the begin and the end of each sliding window, so for the k-th sub-dialogues, the revised utterances sequence is (u(k\u22121)\u2217W\u2212P+1, u(k\u22121)\u2217W\u2212P+2, ..., uk\u2217W+P ).",
            "Moreover, we mask the loss for the context padding utterances, which can be formally ex- pressed as: loss = 1 W X i M(i)L( \u02c6yi, yi), (8) M(i) = 0 if utterance i is in the context padding otherwise 1, L is the cross entropy. The W and P are both hyperparameters; in the experiment 4.2, we will talk about the effect of the window size and the context padding length. model of\ufb02ine setting online setting LSTM n \u00d7 d2 n \u00d7 d2 Self-Attention n2 \u00d7 d n \u00d7 d Table 2: Time complexity between LSTM and self- attention for both online and of\ufb02ine predictions exclud- ing the time cost of \ufb01rst layer encoding. The parameter n represents for the dialogue length in the sliding win- dow and d represent for the dimension of representation unit.",
            "The parameter n represents for the dialogue length in the sliding win- dow and d represent for the dimension of representation unit. Dataset |C| |U| train validation test SwDA 42 176 1K(177K) 112(18K) 19(4K) Daily 4 8 11K(87K) 1K(8K) 1K(8K) Table 3: |C| indicates the number of classes. |U| indicates the average length of dialogues. The train\/validation\/test columns indicate the number of di- alogues (the number of sentences) in the respective splits. 4 Experiments 4.1 Datasets We evaluate the performance of our model on two high-quality datasets: Switchboard Dialogue Act Corpus (SwDA) (Stolcke et al., 2000) and Daily- Dialog (Li et al., 2017). SwDA has been widely used in previous work for the DA recognition task. It is annotated on 1155 human to human tele- phonic conversations about the given topic.",
            "SwDA has been widely used in previous work for the DA recognition task. It is annotated on 1155 human to human tele- phonic conversations about the given topic. Each utterance in the conversation is manually labeled as one of 42 dialogue acts according to SWBD- DAMSL taxonomy (Jurafsky et al., 1997). In Ra- heja and Tetreault (2019), they used 43 categories of dialogue acts, which is different from us and previous work. The difference in the number of la- bels is mainly due to the special label \u201c+\u201d, which represents that the utterance is interrupted by the other speaker (and thus split into two or more parts). We used the same processing with Mila- jevs and Purver (2014), which concatenated the parts of an interrupted utterance together, giving the result the tag of the \ufb01rst part and putting it in its place in the conversation sequence. It is critical for fair comparison because there are nearly 8% data has the label \u201c+\u201d. Lacking standard splits, we followed the training\/validation\/test splits by Lee and Dernoncourt (2016).",
            "It is critical for fair comparison because there are nearly 8% data has the label \u201c+\u201d. Lacking standard splits, we followed the training\/validation\/test splits by Lee and Dernoncourt (2016). DailyDialog dataset con- tains 13118 multi-turn dialogues, which mainly re\ufb02ect our daily communication style. It covers various topics about our daily life. Each utter- ance in the conversation is manually labeled as one out of 4 dialogue act classes. Table 3 presents",
            "models Acc(%) previous approaches BLSTM+Attention+BLSTM (2019) 82.9 Hierarchical BLSTM-CRF (2018) 79.2 CRF-ASN (2018) 78.71 Hierarchical CNN (window 4) (2017) 78.32 mLSTM-RNN (2018) 77.3 DRLM-Conditional (2016) 77.0 LSTM-Softmax (2016) 75.8 RCNN (2013) 73.9 CNN (2016) 73.1 CRF (2010) 72.2 reimplemented and proposed approaches CNN 75.27 LSTM 75.59 BERT(2018) 76.88 LSTM+BLSTM 80.00 LSTM+Attention 80.12 LSTM+Local Contextual Attention 80.34 Human annotator 84.0 Table 4: Comparison results with the previous ap- proaches and our approaches on SwDA dataset. the statistics for both datasets.",
            "the statistics for both datasets. In our preprocess- ing, the text was lowercased before tokenized, and then sentences were tokenized by WordPiece tok- enizer (Wu et al., 2016) with a 30,000 token vo- cabulary to alleviate the Out-of-Vocabulary prob- lem. 4.2 Results on SwDA In this section, we evaluate the proposed ap- proaches on SwDA dataset. Table 4 shows our experimental results and the previous ones on SwDA dataset. It is worth noting that Raheja and Tetreault (2019) combined GloVe(Pennington et al., 2014) and pre-trained ELMo representa- tions(Peters et al., 2018) as word embeddings. However, in our work, we only applied the pre- trained word embedding. To illustrate the im- portance of context information, we also evalu- ate several sentence classi\ufb01cation methods (CNN, LSTM, BERT) as baselines.",
            "However, in our work, we only applied the pre- trained word embedding. To illustrate the im- portance of context information, we also evalu- ate several sentence classi\ufb01cation methods (CNN, LSTM, BERT) as baselines. For baseline mod- els, both CNN and LSTM, got similar accuracy 1The author claimed that they achieved 78.7%(81.3%) ac- curacy with pre-trained word embedding (\ufb01ne-grained em- bedding). For a fair comparison, both previous and our work is simply based on pre-trained word embedding. 2The author randomly selected two test sets which are dif- ferent from previous and our work and achieved 77.15% and 79.74%, and we reimplemented in standard test sets.",
            "For a fair comparison, both previous and our work is simply based on pre-trained word embedding. 2The author randomly selected two test sets which are dif- ferent from previous and our work and achieved 77.15% and 79.74%, and we reimplemented in standard test sets. W P models Acc(%) 1 0 CNN 75.27 LSTM 75.59 BERT 76.88 1 1 LSTM+BLSTM 78.60 LSTM+Attention 78.74 1 3 LSTM+BLSTM 79.36 LSTM+Attention 79.98 1 5 LSTM+BLSTM 80.00 LSTM+Attention 80.12 5 5 LSTM+BLSTM 78.50 LSTM+Attention 79.43 LSTM+LC Attention 80.27 10 5 LSTM+BLSTM 78.31 LSTM+Attention 79.00 LSTM+LC Attention 80.34 20 5 LSTM+BLSTM 78.55 LSTM+Attention 78.57 LSTM+LC Attention 80.17 online prediction LSTM+LSTM 78.62 LSTM+Attention 78.86 LSTM+LC Attention 78.93 Table 5: Experiment results about the hyperparameter W and P on SwDA dataset and online prediction re- sult.",
            "W, P indicate the size of sliding window and con- text padding length during training and testing. (75.27% and 75.59% respectively). We also \ufb01ne- tuned BERT (Devlin et al., 2018) to do recog- nition based on single utterance. As seen, with the powerful unsupervised pre-trained language model, BERT (76.88% accuracy) outperformed LSTM and CNN models for single sentence clas- si\ufb01cation. However, it was still much lower than the models based on context information. It indi- cates that context information is crucial in the DA recognition task. BERT can boost performance in a large margin. However, it costs too much time and resources. In this reason, we chose LSTM as our utterance encoder in further experiment. By modeling context information, the perfor- mance of the hierarchical model is improved by at least 3%, even compared to BERT. In order to better analyze the semantic dependency learned by attention, in our experiments, we removed the CRF module. In terms of different hierarchical models, our LSTM+BLSTM achieved good re- sult.",
            "In order to better analyze the semantic dependency learned by attention, in our experiments, we removed the CRF module. In terms of different hierarchical models, our LSTM+BLSTM achieved good re- sult. The accuracy was 80.00% which is even a little better than Hierarchical BLSTM-CRF (Ku-",
            "mar et al., 2018). Relying on attention mecha- nism and local contextual modeling, our model, LSTM+Attention and LSTM+Local Contextual Attention, achieved 80.12% and 80.34% accuracy respectively. Compared with the previous best approach Hierarchical BLSTM-CRF, we can ob- tain a relative accuracy gain with 1.1% by our best model. It indicated that self-attention model can capture context dependency better than the BLSTM model. With adding the local constraint, we can get an even better result. To further illustrate the effect of the context length, we also performed experiments with dif- ferent sliding window W and context padding P. Table 5 shows the result. It is worth noting that it is actually the same as single sentence classi- \ufb01cation when P = 0 (without any context pro- vided). First, we set W to 1 to discuss how the length of context padding will affect.",
            "It is worth noting that it is actually the same as single sentence classi- \ufb01cation when P = 0 (without any context pro- vided). First, we set W to 1 to discuss how the length of context padding will affect. As seen in the result, the accuracy increased when more con- text padding was used for both LSTM+BLSTM and LSTM+Attention approaches, so we did not evaluate the performance of LSTM+LC Attention when context padding is small. There was no further accuracy improvement when the length of context padding was beyond 5. Therefore, we \ufb01xed the context padding length P to 5 and in- creased the size of the sliding window to see how it works. With sliding window size increas- ing, the more context was involved together with more unnecessary information. From the experi- ments, we can see that both LSTM+BLSTM and LSTM+Attention achieved the best performance when window size was 1 and context padding length was 5. When window size increased, the performances of these two models dropped.",
            "From the experi- ments, we can see that both LSTM+BLSTM and LSTM+Attention achieved the best performance when window size was 1 and context padding length was 5. When window size increased, the performances of these two models dropped. However, our model (LSTM+LC Attention) can leverage the context information more ef\ufb01ciently, which achieved the best performance when win- dow size was 10, and the model was more stable and robust to the different setting of window size. For online prediction, we only care about the recognition result of the last utterance in the given context. We added 5 preceding utterances as con- text padding for every predicted utterance because we cannot access subsequent utterances in the on- line setting. As seen in Table 5, without subse- quent utterances, the performances of these three models dropped. However, LSTM+LC Attention still outperformed the other two models.",
            "As seen in Table 5, without subse- quent utterances, the performances of these three models dropped. However, LSTM+LC Attention still outperformed the other two models. W P models Acc(%) 1 0 CNN 82.22 LSTM 82.58 BERT 83.22 1 1 LSTM+BLSTM 84.88 LSTM+Attention 85.10 1 2 LSTM+BLSTM 85.06 LSTM+Attention 85.36 1 3 LSTM+BLSTM 84.97 LSTM+Attention 85.05 5 2 LSTM+BLSTM 85.01 LSTM+Attention 85.26 LSTM+LC Attention 85.81 10 2 LSTM+BLSTM 84.97 LSTM+Attention 85.13 LSTM+LC Attention 85.72 online prediction LSTM+LSTM 84.55 LSTM+Attention 84.68 LSTM+LC Attention 84.83 Table 6: Experiment results on DailyDialog dataset. 4.3 Result on DailyDialog The classi\ufb01cation accuracy of DailyDialog dataset is summarized in Table 6.",
            "4.3 Result on DailyDialog The classi\ufb01cation accuracy of DailyDialog dataset is summarized in Table 6. As for sentence clas- si\ufb01cation without context information, the \ufb01ne- tuned BERT still outperformed LSTM and CNN based models. From table 3 we can see that, the average dialogue length |U| in DailyDialog is much shorter than the average length of SwDA. So, in our experiment, we set the maximum of the W to 10, which almost covers the whole ut- terances in the dialogue. Using the same way as SwDA dataset, we, \ufb01rst, set W to 1 and in- creased the length of context padding. As seen, modeling local context information, hierarchical models yielded signi\ufb01cant improvement than sen- tence classi\ufb01cation. There was no further accuracy improvement when the length of context padding was beyond 2, so we \ufb01xed the context padding length P to 2 and increased the size of sliding win- dow size W. From the experiments, we can see that LSTM+Attention always got a little better ac- curacy than LSTM+BLSTM.",
            "With window size increasing, the performances of these two mod- els dropped. Relying on modeling local contex- tual information, LSTM+LC Attention achieved the best accuracy (85.81%) when the window size was 5. For the longer sliding window, the perfor- mance of LSTM+LC Attention was still better and",
            "(a) original attention weight matrix (b) local contextual attention weight matrix Figure 2: Visualization of original attention and local contextual attention. Each colored grid represents the depen- dency score between two sentences. The deeper the color is, the higher the dependency score is. more robust than the other two models. For on- line prediction, we added 2 preceding utterances as context padding, and the experiment shows that LSTM+LC Attention outperformed the other two models under the online setting, although the per- formances of these three models dropped without subsequent utterances. 4.4 Visualization In this section, we visualize the attention weights for analyzing how local contextual attention works in detail. Figure 2 shows the visualization of orig- inal attention and local contextual attention for the example dialogue shown in Table 1. The atten- tion matrix M explicitly measures the dependency among utterances. Each row of grids is normalized by softmax, Mij represents for the dependency score between the utterance i and utterance j. As demonstrated in Figure 2a, there are some wrong and uninterpretable attention weights annotated with red color, which is learned by the original attention.",
            "Each row of grids is normalized by softmax, Mij represents for the dependency score between the utterance i and utterance j. As demonstrated in Figure 2a, there are some wrong and uninterpretable attention weights annotated with red color, which is learned by the original attention. The original attention model gives the utterance \u201cB: Hi\u201d (position 0) and \u201cA: Okay.\u201d (po- sition 7) a high dependency score. However, local contextual attention weakens its attention weights due to the long distance apart. Overall, the additional Gaussian bias trend to centralize the attention distribution to the diago- nal of the matrix, which is in line with our lin- guistic intuition that utterances that are far apart usually don\u2019t have too strong dependencies. As demonstrated in Figure 2b, bene\ufb01ting of the addi- tional Gaussian bias, the revised attention mecha- nism weakens the attention weights between utter- ances which cross the long relative distance. For the grids near diagonal, it strengthens their depen- dency score and doesn\u2019t bring other useless depen- dencies for its learnable magnitude.",
            "For the grids near diagonal, it strengthens their depen- dency score and doesn\u2019t bring other useless depen- dencies for its learnable magnitude. 5 Conclusions and Future Work In the paper, we propose our hierarchical model with local contextual attention to the Dialogue Act Recognition task. Our model can explicitly capture the semantic dependencies between utter- ances inside the dialogue. To enhance our model with local contextual information, we revise the at- tention distribution by a learnable Gaussian bias to make it focus on the local neighbors. Based on our dialog segmentation mechanism, we \ufb01nd that lo- cal contextual attention reduces the noises through relative position information, which is essential for dialogue act recognition. And this segmenta- tion mechanism can be applied under online and of\ufb02ine settings. Our model achieves promising performance in two well-known datasets, which shows that modeling local contextual information is crucial for dialogue act recognition. There is a close relation between dialogue act recognition and discourse parsing (Asher et al., 2016). The most discourse parsing process is com- posed of two stages: structure construction and dependency labeling (Wang et al., 2017; Shi and Huang, 2018).",
            "There is a close relation between dialogue act recognition and discourse parsing (Asher et al., 2016). The most discourse parsing process is com- posed of two stages: structure construction and dependency labeling (Wang et al., 2017; Shi and Huang, 2018). For future work, a promising di- rection is to apply our method to multi-task train- ing with two stages jointly. Incorporating super- vised information from dependency between utter- ances may enhance the self-attention and further improve the accuracy of dialogue act recognition.",
            "References Conneau Alexis, Kiela Douwe, Schwenk Holger, Bar- raul Lo c, and Bordes Antoine. 2017. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Nat- ural Language Processing, page 670680. Nicholas Asher, Julie Hunter, Mathieu Morey, Farah Benamara, and Stergos D Afantenos. 2016. Dis- course structure and dialogue acts in multiparty dia- logue: the stac corpus. In LREC. J. L. Austin. 1962. How to Do Things with Words. Clarendon Press. Chandrakant Bothe, Cornelius Weber, Sven Magg, and Stefan Wermter. 2018. A context-based approach for dialogue act recognition using simple recurrent neural networks. In Proceedings of the Eleventh In- ternational Conference on Language Resources and Evaluation (LREC-2018). Lin Chen and Barbara Di Eugenio. 2013.",
            "2018. A context-based approach for dialogue act recognition using simple recurrent neural networks. In Proceedings of the Eleventh In- ternational Conference on Language Resources and Evaluation (LREC-2018). Lin Chen and Barbara Di Eugenio. 2013. Multimodal- ity and dialogue act classi\ufb01cation in the robohelper project. In Proceedings of the SIGDIAL 2013 Con- ference, pages 183\u2013192. Zheqian Chen, Rongqin Yang, Zhou Zhao, Deng Cai, and Xiaofei He. 2018. Dialogue act recognition via crf-attentive structured network. In The 41st Inter- national ACM SIGIR Conference on Research & De- velopment in Information Retrieval, pages 225\u2013234. ACM. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805. Alfred Dielmann and Steve Renals. 2008.",
            "2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805. Alfred Dielmann and Steve Renals. 2008. Recogni- tion of dialogue acts in multiparty meetings using a switching dbn. IEEE transactions on audio, speech, and language processing, 16(7):1303\u20131314. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recog- nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013 778. Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735\u20131780. Yangfeng Ji, Gholamreza Haffari, and Jacob Eisen- stein. 2016. A latent variable recurrent neural net- work for discourse relation language models. In Proceedings of NAACL-HLT, pages 332\u2013342.",
            "Yangfeng Ji, Gholamreza Haffari, and Jacob Eisen- stein. 2016. A latent variable recurrent neural net- work for discourse relation language models. In Proceedings of NAACL-HLT, pages 332\u2013342. Dan Jurafsky, Elizabeth Shriberg, and Debra Biasca. 1997. Switchboard swbd-damsl labeling project coders manual. Draft 13. Technical Report 97-02. Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent convolutional neural networks for discourse compo- sitionality. In Proceedings of the Workshop on Con- tinuous Vector Space Models and their Composition- ality, pages 119\u2013126. Hamed Khanpour, Nishitha Guntakandla, and Rod- ney Nielsen. 2016. Dialogue act classi\ufb01cation in domain-independent conversations using a deep re- current neural network. In Proceedings of COLING 2016, the 26th International Conference on Compu- tational Linguistics: Technical Papers, pages 2012\u2013 2021.",
            "2016. Dialogue act classi\ufb01cation in domain-independent conversations using a deep re- current neural network. In Proceedings of COLING 2016, the 26th International Conference on Compu- tational Linguistics: Technical Papers, pages 2012\u2013 2021. Su Nam Kim, Lawrence Cavedon, and Timothy Bald- win. 2010. Classifying dialogue acts in one-on-one live chats. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Pro- cessing, pages 862\u2013871. Association for Computa- tional Linguistics. Harshit Kumar, Arvind Agarwal, Riddhiman Dasgupta, and Sachindra Joshi. 2018. Dialogue act sequence labeling using hierarchical encoder with crf. In Thirty-Second AAAI Conference on Arti\ufb01cial Intel- ligence. Ji Young Lee and Franck Dernoncourt. 2016. Sequen- tial short-text classi\ufb01cation with recurrent and con- volutional neural networks.",
            "In Thirty-Second AAAI Conference on Arti\ufb01cial Intel- ligence. Ji Young Lee and Franck Dernoncourt. 2016. Sequen- tial short-text classi\ufb01cation with recurrent and con- volutional neural networks. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, pages 515\u2013520. Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017. Dailydialog: A manually labelled multi-turn dialogue dataset. In Proceedings of the Eighth International Joint Conference on Nat- ural Language Processing (Volume 1: Long Papers), pages 986\u2013995. Asian Federation of Natural Lan- guage Processing. Yang Liu, Kun Han, Zhao Tan, and Yun Lei. 2017. Us- ing context information for dialog act classi\ufb01cation in dnn framework. In Proceedings of the 2017 Con- ference on Empirical Methods in Natural Language Processing, pages 2170\u20132178.",
            "2017. Us- ing context information for dialog act classi\ufb01cation in dnn framework. In Proceedings of the 2017 Con- ference on Empirical Methods in Natural Language Processing, pages 2170\u20132178. Dmitrijs Milajevs and Matthew Purver. 2014. Inves- tigating the contribution of distributional semantic information for dialogue act classi\ufb01cation. In Pro- ceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC), pages 40\u201347. Ryosuke Nakanishi, Koji Inoue, Shizuka Nakamura, Katsuya Takanashi, and Tatsuya Kawahara. 2018. Generating \ufb01llers based on dialog act pairs for smooth turn-taking by humanoid robot. In Proc. Intl Workshop Spoken Dialogue Systems (IWSDS). Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation.",
            "Generating \ufb01llers based on dialog act pairs for smooth turn-taking by humanoid robot. In Proc. Intl Workshop Spoken Dialogue Systems (IWSDS). Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 confer- ence on empirical methods in natural language pro- cessing (EMNLP), pages 1532\u20131543.",
            "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. In Proceedings of the 2018 Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long Papers), pages 2227\u20132237. Vipul Raheja and Joel Tetreault. 2019. Dialogue act classi\ufb01cation with context-aware self-attention. arXiv preprint arXiv:1904.02594. John Rogers Searle. 1969. Speech acts: An essay in the philosophy of language, volume 626. Cambridge university press. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position represen- tations.",
            "1969. Speech acts: An essay in the philosophy of language, volume 626. Cambridge university press. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position represen- tations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 2 (Short Papers), volume 2, pages 464\u2013468. Zhouxing Shi and Minlie Huang. 2018. A deep se- quential model for discourse parsing on multi-party dialogues. arXiv preprint arXiv:1812.00176. Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza- beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor, Rachel Martin, Carol Van Ess-Dykema, and Marie Meteer. 2000. Dialogue act modeling for au- tomatic tagging and recognition of conversational speech. Computational linguistics, 26(3):339\u2013373.",
            "2000. Dialogue act modeling for au- tomatic tagging and recognition of conversational speech. Computational linguistics, 26(3):339\u2013373. Maryam Tava\ufb01, Yashar Mehdad, Sha\ufb01q Joty, Giuseppe Carenini, and Raymond Ng. 2013. Dialogue act recognition in synchronous and asynchronous con- versations. In Proceedings of the SIGDIAL 2013 Conference, pages 117\u2013121. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, pages 5998\u20136008. Yizhong Wang, Sujian Li, and Houfeng Wang. 2017. A two-stage parsing method for text-level discourse analysis. In Proceedings of the 55th Annual Meet- ing of the Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pages 184\u2013188.",
            "2017. A two-stage parsing method for text-level discourse analysis. In Proceedings of the 55th Annual Meet- ing of the Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pages 184\u2013188. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google\u2019s neural ma- chine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144. Baosong Yang, Zhaopeng Tu, Derek F Wong, Fandong Meng, Lidia S Chao, and Tong Zhang. 2018. Mod- eling localness for self-attention networks. In Pro- ceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing, pages 4449\u2013 4458. Tiancheng Zhao, Kyusong Lee, and Maxine Eskenazi. 2018.",
            "In Pro- ceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing, pages 4449\u2013 4458. Tiancheng Zhao, Kyusong Lee, and Maxine Eskenazi. 2018. Unsupervised discrete sentence representa- tion learning for interpretable neural dialog gener- ation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 1098\u20131107. Associa- tion for Computational Linguistics. Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi. 2017. Learning discourse-level diversity for neural dialog models using conditional variational autoen- coders. arXiv preprint arXiv:1703.10960."
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "PaperTab-question-2003.06044.pdf",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2025-05\/17\/10.22.50",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 1,
        "avoid_fork_if_possible": false
    },
    "num_chunks": 1,
    "num_partitions": 1024,
    "num_embeddings_est": 9804.999633789062,
    "avg_doclen_est": 175.0892791748047
}
