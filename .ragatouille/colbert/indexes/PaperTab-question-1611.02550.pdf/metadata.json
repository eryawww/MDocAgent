{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "DISCRIMINATIVE ACOUSTIC WORD EMBEDDINGS: RECURRENT NEURAL NETWORK-BASED APPROACHES Shane Settle, Karen Livescu Toyota Technological Institute at Chicago {settle.shane, klivescu}@ttic.edu ABSTRACT Acoustic word embeddings \u2014 \ufb01xed-dimensional vector rep- resentations of variable-length spoken word segments \u2014 have begun to be considered for tasks such as speech recog- nition and query-by-example search. Such embeddings can be learned discriminatively so that they are similar for speech segments corresponding to the same word, while being dis- similar for segments corresponding to different words. Recent work has found that acoustic word embeddings can outper- form dynamic time warping on query-by-example search and related word discrimination tasks. However, the space of embedding models and training approaches is still rela- tively unexplored. In this paper we present new discrimina- tive embedding models based on recurrent neural networks (RNNs).",
      "However, the space of embedding models and training approaches is still rela- tively unexplored. In this paper we present new discrimina- tive embedding models based on recurrent neural networks (RNNs). We consider training losses that have been success- ful in prior work, in particular a cross entropy loss for word classi\ufb01cation and a contrastive loss that explicitly aims to separate same-word and different-word pairs in a \u201dSiamese network\u201d training setting. We \ufb01nd that both classi\ufb01er-based and Siamese RNN embeddings improve over previously re- ported results on a word discrimination task, with Siamese RNNs outperforming classi\ufb01cation models. In addition, we present analyses of the learned embeddings and the effects of variables such as dimensionality and network structure. Index Terms\u2014 acoustic word embeddings, recurrent neural networks, Siamese networks 1. INTRODUCTION Many speech processing tasks \u2013 such as automatic speech recognition or spoken term detection \u2013 hinge on associating segments of speech signals with word labels.",
      "Index Terms\u2014 acoustic word embeddings, recurrent neural networks, Siamese networks 1. INTRODUCTION Many speech processing tasks \u2013 such as automatic speech recognition or spoken term detection \u2013 hinge on associating segments of speech signals with word labels. In most systems developed for such tasks, words are broken down into sub- word units such as phones, and models are built for the in- dividual units. An alternative, which has been considered by some researchers, is to consider each entire word segment as a single unit, without assigning parts of it to sub-word units. One motivation for the use of whole-word approaches is that This research was supported by a Google faculty research award and NSF grant IIS-1321015. The opinions expressed in this work are those of the authors and do not necessarily re\ufb02ect the views of the funding agency. they avoid the need for sub-word models. This is helpful since, despite decades of work on sub-word modeling [1, 2], it still poses signi\ufb01cant challenges. For example, speech pro- cessing systems are still hampered by differences in conver- sational pronunciations [3].",
      "This is helpful since, despite decades of work on sub-word modeling [1, 2], it still poses signi\ufb01cant challenges. For example, speech pro- cessing systems are still hampered by differences in conver- sational pronunciations [3]. A second motivation is that con- sidering whole words at once allows us to consider a more \ufb02exible set of features and reason over longer time spans. Whole-word approaches typically involve, at some level, template matching. For example, in template-based speech recognition [4, 5], word scores are computed from dynamic time warping (DTW) distances between an observed segment and training segments of the hypothesized word. In query- by-example search, putative matches are typically found by measuring the DTW distance between the query and segments of the search database [6, 7, 8, 9]. In other words, whole- word approaches often boil down to making decisions about whether two segments are examples of the same word or not. An alternative to DTW that has begun to be explored is the use of acoustic word embeddings (AWEs), or vector represen- tations of spoken word segments.",
      "An alternative to DTW that has begun to be explored is the use of acoustic word embeddings (AWEs), or vector represen- tations of spoken word segments. AWEs are representations that can be learned from data, ideally such that the embed- dings of two segments corresponding to the same word are close, while embeddings of segments corresponding to differ- ent words are far apart. Once word segments are represented via \ufb01xed-dimensional embeddings, computing distances is as simple as measuring a cosine or Euclidean distance between two vectors. There has been some, thus far limited, work on acoustic word embeddings, focused on a number of embedding mod- els, training approaches, and tasks [10, 11, 12, 13, 14, 15, 16, 17]. In this paper we explore new embedding models based on recurrent neural networks (RNNs), applied to a word dis- crimination task related to query-by-example search. RNNs are a natural model class for acoustic word embeddings, since they can handle arbitrary-length sequences. We compare sev- eral types of RNN-based embeddings and analyze their prop- erties.",
      "RNNs are a natural model class for acoustic word embeddings, since they can handle arbitrary-length sequences. We compare sev- eral types of RNN-based embeddings and analyze their prop- erties. Compared to prior embeddings tested on the same task, our best models achieve sizable improvements in average pre- cision. arXiv:1611.02550v1  [cs.CL]  8 Nov 2016",
      "2. RELATED WORK We next brie\ufb02y describe the most closely related prior work. Maas et al. [10] and Bengio and Heigold [11] used acous- tic word embeddings, based on convolutional neural networks (CNNs), to generate scores for word segments in automatic speech recognition. Maas et al. trained CNNs to predict (continuous-valued) embeddings of the word labels, and used the resulting embeddings to de\ufb01ne feature functions in a seg- mental conditional random \ufb01eld [18] rescoring system. Ben- gio and Heigold also developed CNN-based embeddings for lattice rescoring, but with a contrastive loss to separate em- beddings of a given word from embeddings of other words. Levin et al. [12] developed unsupervised embeddings based on representing each word as a vector of DTW dis- tances to a collection of reference word segments. This representation was subsequently used in several applications: a segmental approach for query-by-example search [13], lexical clustering [19], and unsupervised speech recogni- tion [20]. Voinea et al.",
      "This representation was subsequently used in several applications: a segmental approach for query-by-example search [13], lexical clustering [19], and unsupervised speech recogni- tion [20]. Voinea et al. [16] developed a representation also based on templates, in their case phone templates, designed to be invariant to speci\ufb01c transformations, and showed their robustness on digit classi\ufb01cation. Kamper et al. [14] compared several types of acoustic word embeddings for a word discrimination task related to query-by-example search, \ufb01nding that embeddings based on convolutional neural networks (CNNs) trained with a con- trastive loss outperformed the reference vector approach of Levin et al. [12] as well as several other CNN and DNN em- beddings and DTW using several feature types. There have now been a number of approaches compared on this same task and data [12, 21, 22, 23]. For a direct comparison with this prior work, in this paper we use the same task and some of the same training losses as Kamper et al., but develop new embedding models based on RNNs.",
      "For a direct comparison with this prior work, in this paper we use the same task and some of the same training losses as Kamper et al., but develop new embedding models based on RNNs. The only prior work of which we are aware using RNNs for acoustic word embeddings is that of Chen et al. [17] and Chung et al. [15]. Chen et al. learned a long short-term mem- ory (LSTM) RNN for word classi\ufb01cation and used the result- ing hidden state vectors as a word embedding in a query- by-example task. The setting was quite speci\ufb01c, however, with a small number of queries and speaker-dependent train- ing. Chung et al. [15] worked in an unsupervised setting and trained single-layer RNN autoencoders to produce em- beddings for a word discrimination task. In this paper we fo- cus on the supervised setting, and compare a variety of RNN- based structures trained with different losses. 3.",
      "[15] worked in an unsupervised setting and trained single-layer RNN autoencoders to produce em- beddings for a word discrimination task. In this paper we fo- cus on the supervised setting, and compare a variety of RNN- based structures trained with different losses. 3. APPROACH An acoustic word embedding is a function that takes as input a speech segment corresponding to a word, X = {xt}T t=1, where each xt is a vector of frame-level acoustic features, and outputs a \ufb01xed-dimensional vector representing the seg- ment, g(X). The basic embedding model structure we use is fully         connected layers LSTM cell recurrent  connections input acoustic  features  LSTM cell LSTM cell LSTM cell output acoustic  word embedding LSTM cell LSTM cell LSTM cell LSTM cell \u210e\ud835\udc61 1, \ud835\udc50\ud835\udc61 1 \u210e\ud835\udc61 \ud835\udc46, \ud835\udc50\ud835\udc61 \ud835\udc46 stacked layers \ud835\udc65\ud835\udc61 \ud835\udc54\ud835\udc4b \ud835\udc4b \ud835\udc39 \ud835\udc46 Fig. 1: LSTM-based acoustic word embedding model.",
      "1: LSTM-based acoustic word embedding model. For GRU- based models, the structure is the same, but the LSTM cells are re- placed with GRU cells, and there is no cell activation vector; the recurrent connections only carry the hidden state vector hl t. shown in Fig. 1. The model consists of a deep RNN with some number S of stacked layers, whose \ufb01nal hidden state vector is passed as input to a set of F of fully connected layers; the out- put of the \ufb01nal fully connected layer is the embedding g(X). The RNN hidden state at each time frame can be viewed as a representation of the input seen thus far, and its value in the last time frame T could itself serve as the \ufb01nal word em- bedding. The fully connected layers are added to account for the fact that some additional transformation may improve the representation. For example, the hidden state may need to be larger than the desired word embedding dimension, in order to be able to \u201dremember\u201d all of the needed intermediate infor- mation. Some of that information may not be needed in the \ufb01- nal embedding.",
      "For example, the hidden state may need to be larger than the desired word embedding dimension, in order to be able to \u201dremember\u201d all of the needed intermediate infor- mation. Some of that information may not be needed in the \ufb01- nal embedding. In addition, the information maintained in the hidden state may not necessarily be discriminative; some ad- ditional linear or non-linear transformation may help to learn a discriminative embedding. Within this class of embedding models, we focus on Long Short-Term Memory (LSTM) networks [24] and Gated Re- current Unit (GRU) networks [25]. These are both types of RNNs that include a mechanism for selectively retaining or discarding information at each time frame when updating the hidden state, in order to better utilize long-term context. Both of these RNN variants have been used successfully in speech recognition [26, 27, 28, 29]. In an LSTM RNN, at each time frame both the hidden state ht and an associated \u201ccell memory\u201d vector ct, are up- dated and passed on to the next time frame.",
      "In an LSTM RNN, at each time frame both the hidden state ht and an associated \u201ccell memory\u201d vector ct, are up- dated and passed on to the next time frame. In other words, each forward edge in Figure 1 can be viewed as carrying both the cell memory and hidden state vectors. The updates are modulated by the values of several gating vectors, which con- trol the degree to which the cell memory and hidden state are",
      "updated in light of new information in the current frame. For a single-layer LSTM network, the updates are as follows: it = \u03c3(Wi[xt, ht\u22121] + bi) input gate ft = \u03c3(Wf[xt, ht\u22121] + bf) forget gate ect = tanh(Wc[xt, ht\u22121] + bc) candidate cell memory ct = it \u2299ect + ft \u2299ct\u22121 cell memory ot = \u03c3(Wo[xt, ht\u22121] + bo) output gate ht = ot \u2299tanh(ct) hidden state where ht, ct,ect, it, ft, and ot are all vectors of the same di- mensionality, Wi, Wo, Wf, and Wc are learned weight ma- trices of the appropriate sizes, bi, bo, bf and bc are learned bias vectors, \u03c3(\u00b7) is a componentwise logistic activation, and \u2299refers to the Hadamard (componentwise) product. Similarly, in a GRU network, at each time step a GRU cell determines what components of old information are retained, overwritten, or modi\ufb01ed in light of the next step in the input sequence.",
      "Similarly, in a GRU network, at each time step a GRU cell determines what components of old information are retained, overwritten, or modi\ufb01ed in light of the next step in the input sequence. The output from a GRU cell is only the hidden state vector. A GRU cell uses a reset gate rt and an update gate ut as described below for a single-layer network: rt = \u03c3(Wr[xt, ht\u22121] + br) reset gate ut = \u03c3(Wu[xt, ht\u22121] + bu) update gate eht = tanh(Wh[xt, rt \u2299ht\u22121] + bh) candidate hidden ht = ut \u2299ht\u22121 + (1 \u2212ut) \u2299eht hidden state where rt, ut, eht, and ht are all the same dimensionality, Wr, Wu, and Wh are learned weight matrices of the appro- priate size, and br, bu and bh are learned bias vectors. All of the above equations refer to single-layer networks.",
      "All of the above equations refer to single-layer networks. In a deep network, with multiple stacked layers, the same up- date equations are used in each layer, with the state, cell, and gate vectors replaced by layer-speci\ufb01c vectors hl t, cl t, and so on for layer l. For all but the \ufb01rst layer, the input xt is replaced by the hidden state vector from the previous layer hl\u22121 t . For the fully connected layers, we use recti\ufb01ed linear unit (ReLU) [30] activation, except for the \ufb01nal layer which de- pends on the form of supervision and loss used in training. 3.1. Training We train the RNN-based embedding models using a set of pre-segmented spoken words. We use two main training ap- proaches, inspired by prior work but with some differences in the details. As in [14, 11], our \ufb01rst approach is to use the word labels of the training segments and train the networks to classify the word. In this case, the \ufb01nal layer of g(X) is a log-softmax layer.",
      "As in [14, 11], our \ufb01rst approach is to use the word labels of the training segments and train the networks to classify the word. In this case, the \ufb01nal layer of g(X) is a log-softmax layer. Here we are limited to the subset of the training set that has a suf\ufb01cient number of segments per word to train a good classi\ufb01er, and the output dimensionality is equal to the number of words (but see [14] for a study of varying the dimensionality in such a classi\ufb01er-based embed- ding model by introducing a bottleneck layer). This model is trained end-to-end and is optimized with a cross entropy loss. Although labeled data is necessarily limited, the hope is that the learned models will be useful even when applied to spoken examples of words not previously seen in the training data. For words not seen in training, the embeddings should correspond to some measure of similarity of the word to the training words, measured via the posterior probabilities of the previously seen words. In the experiments below, we exam- ine this assumption by analyzing performance on words that appear in the training data compared to those that do not.",
      "In the experiments below, we exam- ine this assumption by analyzing performance on words that appear in the training data compared to those that do not. The second training approach, based on earlier work of Kamper et al. [14], is to train \u201dSiamese\u201d networks [31]. In this approach, full supervision is not needed; rather, we use weak supervision in the form of pairs of segments labeled as same or different. The base model remains the same as before\u2014an RNN followed by a set of fully connected layers\u2014but the \ufb01- nal layer is no longer a softmax but rather a linear activation layer of arbitrary size. In order to learn the parameters, we si- multaneously feed three word segments through three copies of our model (i.e. three networks with shared weights). One input segment is an \u201canchor\u201d, xa, the second is another seg- ment with the same word label, xs, and the third is a segment corresponding to a different word label, xd.",
      "three networks with shared weights). One input segment is an \u201canchor\u201d, xa, the second is another seg- ment with the same word label, xs, and the third is a segment corresponding to a different word label, xd. Then, the net- work is trained using a \u201ccos-hinge\u201d loss: lcos hinge = max{0, m + dcos(xa, xs) \u2212dcos(xa, xd)} where dcos(x1, x2) = 1 \u2212cos(x1, x2) is the cosine distance between x1, x2. Unlike cross entropy training, here we di- rectly aim to optimize relative (cosine) distance between same and different word pairs. For tasks such as query-by-example search, this training loss better respects our end objective, and can use more data since neither fully labeled data nor any min- imum number of examples of each word should be needed. 4. EXPERIMENTS Our end goal is to improve performance on downstream tasks requiring accurate word discrimination. In this paper we use an intermediate task that more directly tests whether same- and different-word pairs have the expected relationship.",
      "4. EXPERIMENTS Our end goal is to improve performance on downstream tasks requiring accurate word discrimination. In this paper we use an intermediate task that more directly tests whether same- and different-word pairs have the expected relationship. and that allows us to compare to a variety of prior work. Speci\ufb01- cally, we use the word discrimination task of Carlin et al. [21], which is similar to a query-by-example task where the word segmentations are known. The evaluation consists of deter- mining, for each pair of evaluation segments, whether they are examples of the same or different words, and measuring performance via the average precision (AP). We do this by measuring the cosine similarity between their acoustic word embeddings and declaring them to be the same if the distance is below a threshold. By sweeping the threshold, we obtain a precision-recall curve from which we compute the AP. The data used for this task is drawn from the Switchboard conversational English corpus [32]. The word segments range from 50 to 200 frames in length. The acoustic features in each",
      "frame (the input to the word embedding models xt) are 39- dimensional MFCCs+\u2206+\u2206\u2206. We use the same train, devel- opment, and test partitions as in prior work [14, 12], and the same acoustic features as in [14], for as direct a comparison as possible. The train set contains approximately 10k example segments, while dev and test each contain approximately 11k segments (corresponding to about 60M pairs for computing the dev/test AP). As in [14], when training the classi\ufb01cation- based embeddings, we use a subset of the training set contain- ing all word types with a minimum of 3 occurrences, reducing the training set size to approximately 9k segments.1 When training the Siamese networks, the training data consists of all of the same-word pairs in the full training set (approximately 100k pairs). For each such training pair, we randomly sample a third example belonging to a different word type, as required for the lcos hinge loss. 4.1.",
      "For each such training pair, we randomly sample a third example belonging to a different word type, as required for the lcos hinge loss. 4.1. Classi\ufb01cation network details Our classi\ufb01er-based embeddings use LSTM or GRU networks with 2\u20134 stacked layers and 1\u20133 fully connected layers. The \ufb01nal embedding dimensionality is equal to the number of unique word labels in the training set, which is 1061. The recurrent hidden state dimensionality is \ufb01xed at 512 and dropout [33] between stacked recurrent layers is used with probability p = 0.3. The fully connected hidden layer di- mensionality is \ufb01xed at 1024. Recti\ufb01ed linear unit (ReLU) non-linearities and dropout with p = 0.5 are used between fully-connected layers. However, between the \ufb01nal recur- rent hidden state output and the \ufb01rst fully-connected layer no non-linearity or dropout is applied. These settings were determined through experiments on the development set.",
      "However, between the \ufb01nal recur- rent hidden state output and the \ufb01rst fully-connected layer no non-linearity or dropout is applied. These settings were determined through experiments on the development set. The classi\ufb01er network is trained with a cross entropy loss and optimized using stochastic gradient descent (SGD) with Nesterov momentum [34]. The learning rate is initialized at 0.1 and is reduced by a factor of 10 according to the following heuristic: If 99% of the current epoch\u2019s average batch loss is greater than the running average of batch losses over the last 3 epochs, this is considered a plateau; if there are 3 consecutive plateau epochs, then the learning rate is reduced. Training stops when reducing the learning rate no longer improves dev set AP. Then, the model from the epoch corresponding to the the best dev set AP is chosen. Several other optimizers\u2014 Adagrad [35], Adadelta [36], and Adam [37]\u2014were explored in initial experiments on the dev set, but all reported results were obtained using SGD with Nesterov momentum. 4.2.",
      "Several other optimizers\u2014 Adagrad [35], Adadelta [36], and Adam [37]\u2014were explored in initial experiments on the dev set, but all reported results were obtained using SGD with Nesterov momentum. 4.2. Siamese network details For experiments with Siamese networks, we initialize (warm- start) the networks with the tuned classi\ufb01cation network, re- moving the \ufb01nal log-softmax layer and replacing it with a lin- ear layer of size equal to the desired embedding dimensional- ity. We explored embeddings with dimensionalities between 1We thank Herman Kamper for assistance with the data and evaluation. 8 and 2048. We use a margin of 0.4 in the cos-hinge loss. In training the Siamese networks, each training mini- batch consists of 2B triplets. B triplets are of the form (xa, xs, xd) where xa and xs are examples of the same class (a pair from the 100k same-word pair set) and xd is a ran- domly sampled example from a different class.",
      "B triplets are of the form (xa, xs, xd) where xa and xs are examples of the same class (a pair from the 100k same-word pair set) and xd is a ran- domly sampled example from a different class. Then, for each of these B triplets (xa, xs, xd), an additional triplet (xs, xa, xd) is added to the mini-batch to allow all segments to serve as anchors. This is a slight departure from earlier work [14], which we found to improve stability in training and performance on the development set. In preliminary experiments, we compared two methods for choosing the negative examples xd during training, a uni- form sampling approach and a non-uniform one. In the case of uniform sampling, we sample xd uniformly at random from the full set of training examples with labels different from xa. This sampling method requires only word-pair supervi- sion. In the case of non-uniform sampling, xd is sampled in two steps. First, we construct a distribution Py|label(xa) over word labels y and sample a different label from it. Second, we sample an example uniformly from within the subset with the chosen label.",
      "In the case of non-uniform sampling, xd is sampled in two steps. First, we construct a distribution Py|label(xa) over word labels y and sample a different label from it. Second, we sample an example uniformly from within the subset with the chosen label. The goal of this method is to speed up train- ing by targeting pairs that violate the margin constraint. To construct the multinomial PMF Py|label(xa), we maintain an n \u00d7 n matrix S, where n is the number of unique word labels in training. Each word label corresponds to an integer i \u2208[1, n] and therefore a row in S. The values in a row of S are considered similarity scores, and we can retrieve the desired PMF for each row by normalizing by its sum. At the start of each epoch, we initialize S with 0\u2019s along the diagonal and 1\u2019s elsewhere (which reduces to uniform sampling).",
      "At the start of each epoch, we initialize S with 0\u2019s along the diagonal and 1\u2019s elsewhere (which reduces to uniform sampling). For each training pair (dcos(xa, xs), dcos(xa, xd)), we update S for both (i, j) = (label(xa), label(xd)) and (i, j) = (label(xd), label(xa)): si,j += ( cos(xa, xd) dcos(xa, xd) \u2264dcos(xa, xs) + m\u2217 0 otherwise The PMFs Py|label(xa) are updated after the forward pass of an entire mini-batch. The constant m\u2217enforces a potentially stronger constraint than is used in the lcos hinge loss, in or- der to promote diverse sampling. In all experiments, we set m\u2217= 0.6. This is a heuristic approach, and it would be in- teresting to consider various alternatives. Preliminary exper- iments showed that the non-uniform sampling method out- performed uniform sampling, and in the following we report results with non-uniform sampling.",
      "This is a heuristic approach, and it would be in- teresting to consider various alternatives. Preliminary exper- iments showed that the non-uniform sampling method out- performed uniform sampling, and in the following we report results with non-uniform sampling. We optimize the Siamese network model using SGD with Nesterov momentum for 15 epochs. The learning rate is ini- tialized to 0.001 and dropped every 3 epochs until no im- provement is seen on the dev set. The \ufb01nal model is taken from the epoch with the highest dev set AP. All models were implemented in Torch [38] and used the rnn library of [39].",
      "Table 1: Final test set results in terms of average precision (AP). Dimensionalities marked with * refer to dimensionality per frame for DTW-based approaches. For CNN and LSTM models, results are given as means over several training runs (5 and 10, respectively) along with their standard deviations. Model Dim AP MFCCs + DTW [14] 39\u2217 0.214 Corr. autoencoder + DTW [22] 100\u2217 0.469 Classi\ufb01er CNN [14] 1061 0.532 \u00b1 0.014 Siamese CNN [14] 1024 0.549 \u00b1 0.011 Classi\ufb01er LSTM 1061 0.616 \u00b1 0.009 Siamese LSTM 1024 0.671 \u00b1 0.011 5. RESULTS Based on development set results, our \ufb01nal embedding mod- els are LSTM networks with 3 stacked layers and 3 fully con- nected layers, with output dimensionality of 1024 in the case of Siamese networks. Final test set results are given in Ta- ble 1.",
      "Final test set results are given in Ta- ble 1. We include a comparison with the best prior results on this task from [14], as well as the result of using stan- dard DTW on the input MFCCs (reproduced from [14]) and the best prior result using DTW, obtained with frame features learned with correlated autoencoders [22]. Both classi\ufb01er and Siamese LSTM embedding models outperform all prior re- sults on this task of which we are aware.2 We next analyze the effects of model design choices, as well as the learned embeddings themselves. 5.1. Effect of model structure Table 2 shows the effect on development set performance of the number of stacked layers S, the number of fully con- nected layers F, and LSTM vs. GRU cells, for classi\ufb01er- based embeddings. The best performance in this experiment is achieved by the LSTM network with S = F = 3. However, performance still seems to be improving with additional lay- ers, suggesting that we may be able to further improve perfor- mance by adding even more layers of either type.",
      "The best performance in this experiment is achieved by the LSTM network with S = F = 3. However, performance still seems to be improving with additional lay- ers, suggesting that we may be able to further improve perfor- mance by adding even more layers of either type. However, we \ufb01xed the model to S = F = 3 in order to allow for more experimentation and analysis within a reasonable time. Table 2: Average precision on the dev set, using classi\ufb01er-based embeddings. S = # stacked layers, F = # fully connected layers. S F GRU AP LSTM AP 2 1 0.213 0.240 3 1 0.252 0.244 4 1 0.303 0.267 3 2 0.412 0.418 3 3 0.445 0.519 2Yuan et al. [40] have recently been able to improve AP on this test set even further with CNN embeddings, by using a large set of additional (cross- lingual) training data. We do not consider these results to be comparable because of their reliance on additional data.",
      "[40] have recently been able to improve AP on this test set even further with CNN embeddings, by using a large set of additional (cross- lingual) training data. We do not consider these results to be comparable because of their reliance on additional data. 24 26 28 210 0.4 0.6 0.8 1 dim dev AP Siamese Classi\ufb01er 0 5 10 15 min. occurrence count Fig. 2: Effect of embedding dimensionality (left) and occurrences in training set (right). Table 2 reveals an interesting trend. When only one fully connected layer is used, the GRU networks outperform the LSTMs given a suf\ufb01cient number of stacked layers. On the other hand, once we add more fully connected layers, the LSTMs outperform the GRUs. In the \ufb01rst few lines of Ta- ble 2, we use 2, 3, and 4 layer stacks of LSTMs and GRUs while holding \ufb01xed the number of fully-connected layers at F = 1.",
      "In the \ufb01rst few lines of Ta- ble 2, we use 2, 3, and 4 layer stacks of LSTMs and GRUs while holding \ufb01xed the number of fully-connected layers at F = 1. There is clear utility in stacking additional layers; however, even with 4 stacked layers the RNNs still under- perform the CNN-based embeddings of [14] until we begin adding fully connected layers. After exploring a variety of stacked RNNs, we \ufb01xed the stack to 3 layers and varied the number of fully connected layers. The value of each additional fully connected layer is clearly greater than that of adding stacked layers. All net- works trained with 2 or 3 fully connected layers obtain more than 0.4 AP on the development set, while stacked RNNs with 1 fully connected layer are at around 0.3 AP or less. This may raise the question of whether some simple fully connected model may be all that is needed; however, previous work has shown that this approach is not competitive [14], and convo- lutional or recurrent layers are needed to summarize arbitrary- length segments into a \ufb01xed-dimensional representation.",
      "This may raise the question of whether some simple fully connected model may be all that is needed; however, previous work has shown that this approach is not competitive [14], and convo- lutional or recurrent layers are needed to summarize arbitrary- length segments into a \ufb01xed-dimensional representation. 5.2. Effect of embedding dimensionality For the Siamese networks, we varied the output embedding dimensionality, as shown in Fig. 2. This analysis shows that the embeddings learned by the Siamese RNN network are quite robust to reduced dimensionality, outperforming the classi\ufb01er model for all dimensionalities 32 or higher and outperforming previously reported dev set performance with CNN-based embeddings [14] for all dimensionalities \u226516. 5.3. Effect of training vocabulary We might expect the learned embeddings to be more accu- rate for words that are seen in training than for ones that are not. Fig. 2 measures this effect by showing performance as a function of the number of occurrences of the dev words in the",
      "Fig. 3: t-SNE visualization of word embeddings from the dev set produced by the classi\ufb01er (top) vs. Siamese (bottom) models. Word labels seen at training time are denoted by triangles and word labels unseen at training time are denoted by circles. training set. Indeed, both model types are much more suc- cessful for in-vocabulary words, and their performance im- proves the higher the training frequency of the words. How- ever, performance increases more quickly for the Siamese network than for the classi\ufb01er as training frequency increases. This may be due to the fact that, if a word type occurs at least k times in the classi\ufb01er training set, then it occurs at least 2 \u00d7 \u0000k 2 \u0001 times in the Siamese paired training data. 5.4. Visualization of embeddings In order to gain a better qualitative understanding of the dif- ferences between clasif\ufb01er and Siamese-based embeddings, and of the learned embedding space more generally, we plot a two-dimensional visualization of some of our learned embed- dings via t-SNE [41] in Fig.",
      "3. For both classi\ufb01er and Siamese embeddings, there is a marked difference in the quality of clusters formed by embeddings of words that were previ- ously seen vs. previously unseen in training. However, the Siamese network embeddings appear to have better relative distances between word clusters with similar and dissimilar pronunciations. For example, the word programs appears equidistant from problems and problem in the classi\ufb01er- based embedding space, but in the Siamese embedding space problems falls between problem and programs. Sim- ilarly, the cluster for democracy shifts with respect to actually and especially to better respect differences in pronunciation. More study of learned embeddings, us- ing more data and word types, is needed to con\ufb01rm such patterns in general. Improvements in unseen word embed- dings from the classi\ufb01er embedding space to the Siamese embedding space (such as for democracy, morning, and basketball) are a likely result of optimizing the model for relative distances between words. 6.",
      "Improvements in unseen word embed- dings from the classi\ufb01er embedding space to the Siamese embedding space (such as for democracy, morning, and basketball) are a likely result of optimizing the model for relative distances between words. 6. CONCLUSION Our main \ufb01nding is that RNN-based acoustic word embed- dings outperform prior approaches, as measured via a word discrimination task related to query-by-example search. Our best results are obtained with deep LSTM RNNs with a com- bination of several stacked layers and several fully connected layers, optimized with a contrastive Siamese loss. Siamese networks have the bene\ufb01t that, for any given training data set, they are effectively trained on a much larger set, in the sense that they measure a loss and gradient for every possible pair of data points. Our experiments suggest that the models could still be improved with additional layers. In addition, we have found that, for the purposes of acoustic word embeddings, fully connected layers are very important and have a more signi\ufb01cant effect per layer than stacked layers, particularly when trained with the cross entropy loss function.",
      "Our experiments suggest that the models could still be improved with additional layers. In addition, we have found that, for the purposes of acoustic word embeddings, fully connected layers are very important and have a more signi\ufb01cant effect per layer than stacked layers, particularly when trained with the cross entropy loss function. These experiments represent an initial exploration of se- quential neural models for acoustic word embeddings. There are a number of directions for further work. For example, while our analyses suggest that Siamese networks are better than classi\ufb01er-based models at embedding previously unseen words, our best embeddings are still much poorer for unseen words. Improvements in this direction may come from larger training sets, or may require new models that better model the shared structure between words. Other directions for future work include additional forms of supervision and training, as well as application to downstream tasks.",
      "7. REFERENCES [1] ISCA, Proceedings of the International Tutorial and Re- search Workshop on Pronunciation Modeling and Lexi- con Adaptation for Spoken Language Technology, Estes Park, Colorado, 2002. [2] M. Ostendorf, \u201cMoving Beyond the \u2019Beads-on-a-String\u2019 Model of Speech,\u201d in IEEE Automatic Speech Recogni- tion & Understanding (ASRU), 1999. [3] Karen Livescu, Eric Fosler-Lussier, and Florian Metze, \u201cSubword modeling for automatic speech recognition: Past, present, and emerging approaches,\u201d IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 44\u201357, 2012. [4] Mathias De Wachter, Mike Matton, Kris Demuynck, Patrick Wambacq, Ronald Cools, and Dirk Van Comper- nolle, \u201cTemplate-based continuous speech recognition,\u201d IEEE Trans. Audio, Speech, Language Process., vol. 15, no. 4, pp. 1377\u20131390, 2007.",
      "Audio, Speech, Language Process., vol. 15, no. 4, pp. 1377\u20131390, 2007. [5] Georg Heigold, Patrick Nguyen, Mitchel Weintraub, and Vincent Vanhoucke, \u201cInvestigations on exemplar- based features for speech recognition towards thousands of hours of unsupervised, noisy data,\u201d in IEEE Interna- tional Conference on Acoustics, Speech and Signal Pro- cessing (ICASSP), 2012, pp. 4437\u20134440. [6] Florian Metze, Xavier Anguera, Etienne Barnard, Mare- lie Davel, and Guillaume Gravier, \u201cThe spoken web search task at MediaEval 2012,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013. [7] Xavier Anguera, \u201cSpeaker independent discriminant feature extraction for acoustic pattern-matching,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2012.",
      "[7] Xavier Anguera, \u201cSpeaker independent discriminant feature extraction for acoustic pattern-matching,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2012. [8] Yaodong Zhang, Kiarash Adl, and James Glass, \u201cFast spoken query detection using lower-bound dynamic time warping on graphical processing units,\u201d in IEEE International Conference on Acoustics, Speech and Sig- nal Processing (ICASSP), 2012, pp. 5173\u20135176. [9] Igor Sz\u00a8oke, Miroslav Sk\u00b4acel, Luk\u00b4a\u02d8s Burget, and Jan \u201cHonza\u201d \u02d8Cernock\u00b4y, \u201cCoping with channel mis- match in query-by-example - BUT QUESST 2014,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.",
      "[10] Andrew L Maas, Stephen D Miller, Tyler M O\u2019neil, An- drew Y Ng, and Patrick Nguyen, \u201cWord-level acoustic modeling with convolutional vector regression,\u201d in In- ternational Conference on Machine Learning (ICML), Representation Learning Workshop, 2012. [11] Samy Bengio and Georg Heigold, \u201cWord embeddings for speech recognition,\u201d in Interspeech, 2014. [12] Keith Levin, Katharine Henry, Aren Jansen, and Karen Livescu, \u201cFixed-dimensional acoustic embeddings of variable-length segments in low-resource settings,\u201d in IEEE Automatic Speech Recognition & Understanding (ASRU), 2013. [13] Keith Levin, Aren Jansen, and Benjamin Van Durme, \u201cSegmental acoustic indexing for zero resource keyword search,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015. [14] Herman Kamper, Weiran Wang, and Karen Livescu, \u201cDeep convolutional acoustic word embeddings using word-pair side information,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 4950\u20134954.",
      "[14] Herman Kamper, Weiran Wang, and Karen Livescu, \u201cDeep convolutional acoustic word embeddings using word-pair side information,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 4950\u20134954. [15] Yu-An Chung, Chao-Chung Wu, Chia-Hao Shen, and Hung-Yi Lee, \u201cUnsupervised learning of audio seg- ment representations using sequence-to-sequence recur- rent neural networks,\u201d Interspeech, 2016. [16] Stephen Voinea, Chiyuan Zhang, Georgios Evan- gelopoulos, Lorenzo Rosasco, and Tomaso Poggio, \u201cWord-level invariant representations from acoustic waveforms.,\u201d in Interspeech, 2014, pp. 2385\u20132389. [17] Guoguo Chen, Carolina Parada, and Tara N Sainath, \u201cQuery-by-example keyword spotting using long short- term memory networks,\u201d in IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP), 2015.",
      "[17] Guoguo Chen, Carolina Parada, and Tara N Sainath, \u201cQuery-by-example keyword spotting using long short- term memory networks,\u201d in IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP), 2015. [18] Geoffrey Zweig and Patrick Nguyen, \u201cA segmental CRF approach to large vocabulary continuous speech recog- nition,\u201d in IEEE Automatic Speech Recognition & Un- derstanding (ASRU), 2009, pp. 152\u2013157. [19] Herman Kamper, Aren Jansen, Simon King, and Sharon Goldwater, \u201cUnsupervised lexical clustering of speech segments using \ufb01xed-dimensional acoustic em- beddings,\u201d in IEEE Spoken Language Technology Work- shop (SLT), 2014, pp. 100\u2013105. [20] Herman Kamper, Aren Jansen, and Sharon Goldwater, \u201cFully unsupervised small-vocabulary speech recogni- tion using a segmental bayesian model,\u201d in Interspeech, 2015.",
      "100\u2013105. [20] Herman Kamper, Aren Jansen, and Sharon Goldwater, \u201cFully unsupervised small-vocabulary speech recogni- tion using a segmental bayesian model,\u201d in Interspeech, 2015. [21] Michael A Carlin, Samuel Thomas, Aren Jansen, and Hynek Hermansky, \u201cRapid evaluation of speech repre- sentations for spoken term discovery,\u201d in Interspeech, 2011.",
      "[22] H. Kamper, M. Elsner, A. Jansen, and S. J. Goldwater, \u201cUnsupervised neural network based feature extraction using weak top-down constraints,\u201d in IEEE Interna- tional Conference on Acoustics, Speech and Signal Pro- cessing (ICASSP), 2015. [23] Aren Jansen, Samuel Thomas, and Hynek Hermansky, \u201cWeak top-down constraints for unsupervised acous- tic model training,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013. [24] Sepp Hochreiter and J\u00a8urgen Schmidhuber, \u201cLong short- term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997. [25] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio, \u201cEmpirical evaluation of gated re- current neural networks on sequence modeling,\u201d Neural Information Processing Systems (NIPS), 2014.",
      "[25] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio, \u201cEmpirical evaluation of gated re- current neural networks on sequence modeling,\u201d Neural Information Processing Systems (NIPS), 2014. [26] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in IEEE International Conference on Acous- tics, Speech and Signal Processing (ICASSP), 2013, pp. 6645\u20136649. [27] Has\u00b8im Sak, Andrew Senior, and Franc\u00b8oise Beaufays, \u201cLong short-term memory based recurrent neural net- work architectures for large vocabulary speech recogni- tion,\u201d arXiv preprint arXiv:1402.1128, 2014. [28] Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, \u201cAttention-based models for speech recognition,\u201d in Neural Information Processing Systems (NIPS), 2015, pp. 577\u2013585.",
      "577\u2013585. [29] Liang Lu, Xingxing Zhang, Kyunghyun Cho, and Steve Renals, \u201cA study of the recurrent neural network encoder-decoder for large vocabulary speech recogni- tion,\u201d in Interspeech, 2015. [30] Vinod Nair and Geoffrey E Hinton, \u201cRecti\ufb01ed linear units improve restricted boltzmann machines,\u201d in In- ternational Conference on Machine Learning (ICML), 2010, pp. 807\u2013814. [31] Jane Bromley, James W Bentz, L\u00b4eon Bottou, Isabelle Guyon, Yann LeCun, Cliff Moore, Eduard S\u00a8ackinger, and Roopak Shah, \u201cSignature veri\ufb01cation using a \u2018Siamese\u2019 time delay neural network,\u201d Int. J. Pattern Rec., vol. 7, no. 4, pp. 669\u2013688, 1993.",
      "J. Pattern Rec., vol. 7, no. 4, pp. 669\u2013688, 1993. [32] John J Godfrey, Edward C Holliman, and Jane Mc- Daniel, \u201cSwitchboard: Telephone speech corpus for re- search and development,\u201d in IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP), 1992, vol. 1, pp. 517\u2013520. [33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov, \u201cDropout: a simple way to prevent neural networks from over\ufb01t- ting.,\u201d Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014. [34] Yurii Nesterov, \u201cA method of solving a convex program- ming problem with convergence rate O(1/k2),\u201d . [35] John Duchi, Elad Hazan, and Yoram Singer, \u201cAdaptive subgradient methods for online learning and stochastic optimization,\u201d Tech. Rep.",
      "[35] John Duchi, Elad Hazan, and Yoram Singer, \u201cAdaptive subgradient methods for online learning and stochastic optimization,\u201d Tech. Rep. UCB/EECS-2010-24, EECS Department, University of California, Berkeley, Mar 2010. [36] Matthew D. Zeiler, \u201cADADELTA: an adaptive learning rate method,\u201d CoRR, vol. abs/1212.5701, 2012. [37] Diederik P. Kingma and Jimmy Ba, \u201cAdam: A method for stochastic optimization,\u201d CoRR, vol. abs/1412.6980, 2014. [38] Ronan Collobert, Koray Kavukcuoglu, and Cl\u00b4ement Farabet, \u201cTorch7: A matlab-like environment for ma- chine learning,\u201d in BigLearn, Neural Information Pro- cessing (NIPS) Workshop, 2011, number EPFL-CONF- 192376. [39] Nicholas L\u00b4eonard, Sagar Waghmare, Yang Wang, and Jin-Hwa Kim, \u201crnn : Recurrent library for torch,\u201d CoRR, vol.",
      "[39] Nicholas L\u00b4eonard, Sagar Waghmare, Yang Wang, and Jin-Hwa Kim, \u201crnn : Recurrent library for torch,\u201d CoRR, vol. abs/1511.07889, 2015. [40] Yougen Yuan, Cheung-Chi Leung, Lei Xie, Bin Ma, and Haizhou Li, \u201cLearning neural network representations using cross-lingual bottleneck features with word-pair information,\u201d in Interspeech, 2016. [41] Laurens van der Maaten and Geoffrey Hinton, \u201cVisu- alizing data using t-sne,\u201d Journal of Machine Learning Research, vol. 9, no. Nov, pp. 2579\u20132605, 2008."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1611.02550.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":9542,
  "avg_doclen":187.0980392157,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1611.02550.pdf"
    }
  }
}