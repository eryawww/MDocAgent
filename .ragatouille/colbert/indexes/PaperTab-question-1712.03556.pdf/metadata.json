{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Stochastic Answer Networks for Machine Reading Comprehension Xiaodong Liu\u2020, Yelong Shen\u2020, Kevin Duh\u2021 and Jianfeng Gao\u2020 \u2020 Microsoft Research, Redmond, WA, USA \u2021 Johns Hopkins University, Baltimore, MD, USA \u2020{xiaodl,yeshen,jfgao}@microsoft.com \u2021kevinduh@cs.jhu.edu Abstract We propose a simple yet robust stochastic answer network (SAN) that simulates multi-step reasoning in machine reading comprehension. Compared to previous work such as ReasoNet which used rein- forcement learning to determine the num- ber of steps, the unique feature is the use of a kind of stochastic prediction dropout on the answer module (\ufb01nal layer) of the neu- ral network during the training. We show that this simple trick improves robustness and achieves results competitive to the state-of-the-art on the Stanford Question Answering Dataset (SQuAD), the Adver- sarial SQuAD, and the Microsoft MA- chine Reading COmprehension Dataset (MS MARCO).",
      "We show that this simple trick improves robustness and achieves results competitive to the state-of-the-art on the Stanford Question Answering Dataset (SQuAD), the Adver- sarial SQuAD, and the Microsoft MA- chine Reading COmprehension Dataset (MS MARCO). 1 Introduction Machine reading comprehension (MRC) is a chal- lenging task: the goal is to have machines read a text passage and then answer any question about the passage. This task is an useful benchmark to demonstrate natural language understanding, and also has important applications in e.g. conversa- tional agents and customer service support. It has been hypothesized that dif\ufb01cult MRC problems re- quire some form of multi-step synthesis and rea- soning. For instance, the following example from the MRC dataset SQuAD (Rajpurkar et al., 2016) illustrates the need for synthesis of information across sentences and multiple steps of reasoning: Q: What collection does the V&A Theator & Performance galleries hold? P: The V&A Theator & Performance gal- leries opened in March 2009.",
      "P: The V&A Theator & Performance gal- leries opened in March 2009. ... They hold the UK\u2019s biggest national collection of material about live performance. To infer the answer (the underlined portion of the passage P), the model needs to \ufb01rst perform coref- erence resolution so that it knows \u201cThey\u201d refers \u201cV&A Theator\u201d, then extract the subspan in the direct object corresponding to the answer. This kind of iterative process can be viewed as a form of multi-step reasoning. Several recent MRC models have embraced this kind of multi- step strategy, where predictions are generated after making multiple passes through the same text and integrating intermediate information in the pro- cess. The \ufb01rst models employed a predetermined \ufb01xed number of steps (Hill et al., 2016; Dhingra et al., 2016; Sordoni et al., 2016; Kumar et al., 2015). Later, Shen et al. (2016) proposed using reinforcement learning to dynamically determine the number of steps based on the complexity of the question. Further, Shen et al.",
      "Later, Shen et al. (2016) proposed using reinforcement learning to dynamically determine the number of steps based on the complexity of the question. Further, Shen et al. (2017) empir- ically showed that dynamic multi-step reasoning outperforms \ufb01xed multi-step reasoning, which in turn outperforms single-step reasoning on two dis- tinct MRC datasets (SQuAD and MS MARCO). In this work, we derive an alternative multi-step reasoning neural network for MRC. During train- ing, we \ufb01x the number of reasoning steps, but per- form stochastic dropout on the answer module (\ufb01- nal layer predictions). During decoding, we gener- ate answers based on the average of predictions in all steps, rather than the \ufb01nal step. We call this a stochastic answer network (SAN) because the stochastic dropout is applied to the answer mod- ule; albeit simple, this technique signi\ufb01cantly im- proves the robustness and overall accuracy of the model.",
      "We call this a stochastic answer network (SAN) because the stochastic dropout is applied to the answer mod- ule; albeit simple, this technique signi\ufb01cantly im- proves the robustness and overall accuracy of the model. Intuitively this works because while the model successively re\ufb01nes its prediction over mul- tiple steps, each step is still trained to generate the same answer; we are performing a kind of stochas- tic ensemble over the model\u2019s successive predic- arXiv:1712.03556v2  [cs.CL]  15 May 2018",
      "st-1 st st+1 x Figure 1: Illustration of \u201cstochastic prediction dropout\u201d in the answer module during training. At each reasoning step t, the model combines mem- ory (bottom row) with hidden states st\u22121 to gener- ate a prediction (multinomial distribution). Here, there are three steps and three predictions, but one prediction is dropped and the \ufb01nal result is an av- erage of the remaining distributions. tion re\ufb01nements. Stochastic prediction dropout is illustrated in Figure 1. 2 Proposed model: SAN The machine reading comprehension (MRC) task as de\ufb01ned here involves a question Q = {q0, q1, ..., qm\u22121} and a passage P = {p0, p1, ..., pn\u22121} and aims to \ufb01nd an answer span A = {astart, aend} in P. We assume that the answer exists in the passage P as a contiguous text string. Here, m and n denote the number of tokens in Q and P, respectively. The learning algorithm for reading comprehension is to learn a function f(Q, P) \u2192A.",
      "Here, m and n denote the number of tokens in Q and P, respectively. The learning algorithm for reading comprehension is to learn a function f(Q, P) \u2192A. The training data is a set of the query, passage and answer tuples < Q, P, A >. We now describe our model from the ground up. The main contribution of this work is the answer module, but in order to understand what goes into this module, we will start by describing how Q and P are processed by the lower layers. Note the lower layers also have some novel variations that are not used in previous work. As shown in Fig- ure 2, our model contains four different layers to capture different concept of representations. The detailed description of our model is provided as follows. Lexicon Encoding Layer. The purpose of the \ufb01rst layer is to extract information from Q and P at the word level and normalize for lexical vari- ants. A typical technique to obtain lexicon embed- ding is concatenation of its word embedding with other linguistic embedding such as those derived from Part-Of-Speech (POS) tags.",
      "A typical technique to obtain lexicon embed- ding is concatenation of its word embedding with other linguistic embedding such as those derived from Part-Of-Speech (POS) tags. For word em- beddings, we use the pre-trained 300-dimensional GloVe vectors (Pennington et al., 2014) for the both Q and P. Following Chen et al. (2017), we use three additional types of linguistic features for each token pi in the passage P: \u2022 9-dimensional POS tagging embedding for total 56 different types of the POS tags. \u2022 8-dimensional named-entity recognizer (NER) embedding for total 18 different types of the NER tags. We utilized small embedding sizes for POS and NER to reduce model size. They mainly serve the role of coarse-grained word clusters. \u2022 A 3-dimensional binary exact match fea- ture de\ufb01ned as fexact match(pi) = I(pi \u2208 Q). This checks whether a passage token pi matches the original, lowercase or lemma form of any question token.",
      "They mainly serve the role of coarse-grained word clusters. \u2022 A 3-dimensional binary exact match fea- ture de\ufb01ned as fexact match(pi) = I(pi \u2208 Q). This checks whether a passage token pi matches the original, lowercase or lemma form of any question token. \u2022 Question enhanced passages word embed- dings: falign(pi) = P j \u03b3i,jg(GloV e(qj)), where g(\u00b7) is a 280-dimensional single layer neural network ReLU(W0x) and \u03b3i,j = exp(g(GloV e(pj))\u00b7g(GloV e(qi))) P j\u2032 exp(g(GloV e(pi))\u00b7g(GloV e(qj\u2032))) mea- sures the similarity in word embedding space between a token pi in the passage and a to- ken qj in the question. Compared to the ex- act matching features, these embeddings en- code soft alignments between similar but not- identical words. In summary, each token pi in the passage is repre- sented as a 600-dimensional vector and each token qj is represented as a 300-dimensional vector.",
      "Compared to the ex- act matching features, these embeddings en- code soft alignments between similar but not- identical words. In summary, each token pi in the passage is repre- sented as a 600-dimensional vector and each token qj is represented as a 300-dimensional vector. Due to different dimensions for the passages and questions, in the next layer two different bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) may be required to encode the contextual information. This, however, in- troduces a large number of parameters. To pre- vent this, we employ an idea inspired by (Vaswani et al., 2017): use two separate two-layer position- wise Feed-Forward Networks (FFN), FFN(x) = W2ReLU(W1x+b1)+b2, to map both the passage and question lexical encodings into the same num- ber of dimensions. Note that this FFN has fewer",
      "Lexicon Encoding Layer Beyonc\u00e9 is \u2026 what religion? 2 Layers Position-Wise FFN Beyonc\u00e9  was born ... in a Methodist household. 2 Layers Position-Wise FFN Beyonc\u00e9  was born ... in a Methodist household. 2 Layers Position-Wise FFN Contextual Encoding Layer Attention Self Attention 2 Layers BiLSTM  with Maxout  Memory  Self Attended Sum  GRU st-1 st st+1 Figure 2: Architecture of the SAN for Reading Comprehension: The \ufb01rst layer is a lexicon encoding layer that maps words to their embeddings independently for the question (left) and the passage (right): this is a concatenation of word embeddings, POS embeddings, etc. followed by a position-wise FFN. The next layer is a context encoding layer, where a BiLSTM is used on the top of the lexicon embedding layer to obtain the context representation for both question and passage. In order to reduce the parameters, a maxout layer is applied on the output of BiLSTM. The third layer is the working memory: First we compute an alignment matrix between the question and passage using an attention mechanism, and use this to derive a question-aware passage representation.",
      "In order to reduce the parameters, a maxout layer is applied on the output of BiLSTM. The third layer is the working memory: First we compute an alignment matrix between the question and passage using an attention mechanism, and use this to derive a question-aware passage representation. Then we concatenate this with the context representation of passage and the word embedding, and employ a self attention layer to re-arrange the information gathered. Finally, we use another LSTM to generate a working memory for the passage. At last, the fourth layer is the answer module, which is a GRU that outputs predictions at each state st. parameters compared to a BiLSTM. Thus, we ob- tain the \ufb01nal lexicon embeddings for the tokens in Q as a matrix Eq \u2208Rd\u00d7m and tokens in P as Ep \u2208Rd\u00d7n. Contextual Encoding Layer. Both passage and question use a shared two-layers BiLSTM as the contextual encoding layer, which projects the lexicon embeddings to contextual embeddings.",
      "Contextual Encoding Layer. Both passage and question use a shared two-layers BiLSTM as the contextual encoding layer, which projects the lexicon embeddings to contextual embeddings. We concatenate a pre-trained 600-dimensional CoVe vectors1 (McCann et al., 2017) trained on German-English machine translation dataset, with 1https://github.com/salesforce/cove the aforementioned lexicon embeddings as the \ufb01- nal input of the contextual encoding layer, and also with the output of the \ufb01rst contextual encoding layer as the input of its second encoding layer. To reduce the parameter size, we use a maxout layer (Goodfellow et al., 2013) at each BiLSTM layer to shrink its dimension. By a concatena- tion of the outputs of two BiLSTM layers, we obtain Hq \u2208R2d\u00d7m as representation of Q and Hp \u2208R2d\u00d7n as representation of P, where d is the hidden size of the BiLSTM. Memory Generation Layer. In the memory",
      "generation layer, We construct the working mem- ory, a summary of information from both Q and P. First, a dot-product attention is adopted like in (Vaswani et al., 2017) to measure the similarity between the tokens in Q and P. Instead of using a scalar to normalize the scores as in (Vaswani et al., 2017), we use one layer network to transform the contextual information of both Q and P: C = dropout(fattention( \u02c6Hq, \u02c6Hp)) \u2208Rm\u00d7n (1) C is an attention matrix. Note that \u02c6 Hq and \u02c6 Hp is transformed from Hq and Hp by one layer neu- ral network ReLU(W3x), respectively. Next, we gather all the information on passages by a sim- ple concatenation of its contextual information Hp and its question-aware representation Hq \u00b7 C: Up = concat(Hp, HqC) \u2208R4d\u00d7n (2) Typically, a passage may contain hundred of to- kens, making it hard to learn the long dependen- cies within it.",
      "Inspired by (Lin et al., 2017), we apply a self-attended layer to rearrange the infor- mation Up as: \u02c6Up = Updropdiag(fattention(Up, Up)). (3) In other words, we \ufb01rst obtain an n \u00d7 n attention matrix with Up onto itself, apply dropout, then multiply this matrix with Up to obtain an updated \u02c6Up. Instead of using a penalization term as in (Lin et al., 2017), we dropout the diagonal of the sim- ilarity matrix forcing each token in the passage to align to other tokens rather than itself. At last, the working memory is generated by us- ing another BiLSTM based on all the information gathered: M = BiLSTM([Up; \u02c6Up]) (4) where the semicolon mark ; indicates the vec- tor/matrix concatenation operator. Answer module. There is a Chinese proverb that says: \u201cwisdom of masses exceeds that of any individual.\u201d Unlike other multi-step reasoning models, which only uses a single output either at the last step or some dynamically determined \ufb01nal step, our answer module employs all the outputs of multiple step reasoning.",
      "Answer module. There is a Chinese proverb that says: \u201cwisdom of masses exceeds that of any individual.\u201d Unlike other multi-step reasoning models, which only uses a single output either at the last step or some dynamically determined \ufb01nal step, our answer module employs all the outputs of multiple step reasoning. Intuitively, by applying dropout, it avoids a \u201cstep bias problem\u201d (where models places too much emphasis one particular step\u2019s predictions) and forces the model to produce good predictions at every individual step. Further, during decoding, we reuse wisdom of masses in- stead of individual to achieve a better result. We call this method \u201cstochastic prediction dropout\u201d because dropout is being applied to the \ufb01nal pre- dictive distributions. Formally, our answer module will compute over T memory steps and output the answer span. This module is a memory network and has some sim- ilarities to other multi-step reasoning networks: namely, it maintains a state vector, one state per step.",
      "Formally, our answer module will compute over T memory steps and output the answer span. This module is a memory network and has some sim- ilarities to other multi-step reasoning networks: namely, it maintains a state vector, one state per step. At the beginning, the initial state s0 is the summary of the Q: s0 = P j \u03b1jHq j , where \u03b1j = exp(w4\u00b7Hq j ) P j\u2032 exp(w4\u00b7Hq j\u2032). At time step t in the range of {1, 2, ..., T \u22121}, the state is de\ufb01ned by st = GRU(st\u22121, xt). Here, xt is computed from the previous state st\u22121 and memory M: xt = P j \u03b2jMj and \u03b2j = softmax(st\u22121W5M). Fi- nally, a bilinear function is used to \ufb01nd the begin and end point of answer spans at each reasoning step t \u2208{0, 1, . . . , T \u22121}.",
      "Fi- nally, a bilinear function is used to \ufb01nd the begin and end point of answer spans at each reasoning step t \u2208{0, 1, . . . , T \u22121}. P begin t = softmax(stW6M) (5) P end t = softmax([st; X j P begin t,j Mj]W7M). (6) From a pair of begin and end points, the an- swer string can be extracted from the passage. However, rather than output the results (start/end points) from the \ufb01nal step (which is \ufb01xed at T \u22121 as in Memory Networks or dynamically deter- mined as in ReasoNet), we utilize all of the T out- puts by averaging the scores: P begin = avg([P begin 0 , P begin 1 , ..., P begin T\u22121 ]) (7) P end = avg([P end 0 , P end 1 , ..., P end T\u22121]) (8) Each P begin t or P end t is a multinomial distribu- tion over {1, . . . , n}, so the average distribution is straightforward to compute.",
      ". . , n}, so the average distribution is straightforward to compute. During training, we apply stochastic dropout to before the above averaging operation. For exam- ple, as illustrated in Figure 1, we randomly delete several steps\u2019 predictions in Equations 7 and 8 so that P begin might be avg([P begin 1 , P begin 3 ]) and P end might be avg([P end 0 , P end 3 , P end 4 ]). The use of averaged predictions and dropout during train- ing improves robustness. Our stochastic prediction dropout is similar in motivation to the dropout introduced by (Srivas- tava et al., 2014). The difference is that theirs",
      "is dropout at the intermediate node-level, whereas ours is dropout at the \ufb01nal layer-level. Dropout at the node-level prevents correlation between fea- tures. Dropout at the \ufb01nal layer level, where ran- domness is introduced to the averaging of predic- tions, prevents our model from relying exclusively on a particular step to generate correct output. We used a dropout rate of 0.4 in experiments. 3 Experiment Setup Dataset: We evaluate on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016). This contains about 23K passages and 100K questions. The passages come from approx- imately 500 Wikipedia articles and the questions and answers are obtained by crowdsourcing. The crowdsourced workers are asked to read a passage (a paragraph), come up with questions, then mark the answer span. All results are on the of\ufb01cial de- velopment set, unless otherwise noted.",
      "The crowdsourced workers are asked to read a passage (a paragraph), come up with questions, then mark the answer span. All results are on the of\ufb01cial de- velopment set, unless otherwise noted. Two evaluation metrics are used: Exact Match (EM), which measures the percentage of span pre- dictions that matched any one of the ground truth answer exactly, and Macro-averaged F1 score, which measures the average overlap between the prediction and the ground truth answer. Implementation details: The spaCy tool2 is used to tokenize the both passages and questions, and generate lemma, part-of-speech and named entity tags. We use 2-layer BiLSTM with d = 128 hidden units for both passage and question encod- ing. The mini-batch size is set to 32 and Adamax (Kingma and Ba, 2014) is used as our optimizer. The learning rate is set to 0.002 at \ufb01rst and de- creased by half after every 10 epochs. We set the dropout rate for all the hidden units of LSTM, and the answer module output layer to 0.4.",
      "The learning rate is set to 0.002 at \ufb01rst and de- creased by half after every 10 epochs. We set the dropout rate for all the hidden units of LSTM, and the answer module output layer to 0.4. To prevent degenerate output, we ensure that at least one step in the answer module is active during training. 4 Results The main experimental question we would like to answer is whether the stochastic dropout and av- eraging in the answer module is an effective tech- nique for multi-step reasoning. To do so, we \ufb01xed all lower layers and compared different architec- tures for the answer module: 1. Standard 1-step: generate prediction from s0, the \ufb01rst initial state. 2https://spacy.io 2. 5-step memory network: this is a memory network \ufb01xed at 5 steps. We try two variants: the standard variant outputs result from the \ufb01- nal step sT\u22121. The averaged variant outputs results by averaging across all 5 steps, and is like SAN without the stochastic dropout. 3.",
      "We try two variants: the standard variant outputs result from the \ufb01- nal step sT\u22121. The averaged variant outputs results by averaging across all 5 steps, and is like SAN without the stochastic dropout. 3. ReasoNet3: this answer module dynamically decides the number of steps and outputs re- sults conditioned on the \ufb01nal step. 4. SAN: proposed answer module that uses stochastic dropout and prediction averaging. The main results in terms of EM and F1 are shown in Table 1. We observe that SAN achieves 76.235 EM and 84.056 F1, outperforming all other models. Standard 1-step model only achieves 75.139 EM and dynamic steps (via ReasoNet) achieves only 75.355 EM. SAN also outperforms a 5-step memory net with averaging, which implies averaging predictions is not the only thing that led to SAN\u2019s superior results; indeed, stochastic pre- diction dropout is an effective technique. The K-best oracle results is shown in Figure 3. The K-best spans are computed by ordering the spans according the their probabilities P begin \u00d7 P end.",
      "The K-best oracle results is shown in Figure 3. The K-best spans are computed by ordering the spans according the their probabilities P begin \u00d7 P end. We limit K in the range 1 to 4 and then pick the span with the best EM or F1 as oracle. SAN also outperforms the other models in terms of K-best oracle scores. Impressively, these mod- els achieve human performance at K = 2 for EM and K = 3 for F1. Finally, we compare our results with other top models in Table 2. Note that all the results in Ta- ble 2 are taken from the published papers. We see that SAN is very competitive in both single and ensemble settings (ranked in second) despite its simplicity. Note that the best-performing model (Peters et al., 2018) used a large-scale language model as an extra contextual embedding, which gave a signi\ufb01cant improvement (+4.3% dev F1). We expect signi\ufb01cant improvements if we add this to SAN in future work.",
      "We expect signi\ufb01cant improvements if we add this to SAN in future work. 3The ReasoNet here is not an exact re-implementation of (Shen et al., 2017). The answer module is the same as (Shen et al., 2017) but the lower layers are set to be the same as SAN, 5-step memory network, and standard 1-step as de- scribed in Figure 2. We only vary the answer module in our experiments for a fair comparison.",
      "Answer Module EM F1 Standard 1-step 75.139 83.367 Fixed 5-step with Memory Network (prediction from \ufb01nal step) 75.033 83.327 Fixed 5-step with Memory Network (prediction averaged from all steps) 75.256 83.215 Dynamic steps (max 5) with ReasoNet 75.355 83.360 Stochastic Answer Network (SAN ), Fixed 5-step 76.235 84.056 Table 1: Main results\u2014Comparison of different answer module architectures. Note that SAN performs best in both Exact Match and F1 metrics. Ensemble model results: Dev Set (EM/F1) Test Set (EM/F1) BiDAF + Self Attention + ELMo (Peters et al., 2018) -/- 81.003/87.432 SAN (Ensemble model) 78.619/85.866 79.608/86.496 AIR-FusionNet (Huang et al., 2017) -/- 78.978/86.016 DCN+ (Xiong et al., 2017) -/- 78.852/85.",
      "619/85.866 79.608/86.496 AIR-FusionNet (Huang et al., 2017) -/- 78.978/86.016 DCN+ (Xiong et al., 2017) -/- 78.852/85.996 M-Reader (Hu et al., 2017) -/- 77.678/84.888 Conductor-net (Liu et al., 2017b) 74.8 / 83.3 76.996/84.630 r-net (Wang et al., 2017) 77.7/83.7 76.9/84.0 ReasoNet++ (Shen et al., 2017) 75.4/82.9 75.0/82.6 Individual model results: BiDAF + Self Attention + ELMo(Peters et al., 2018) -/- 78.580/85.833 SAN (single model) 76.235/84.056 76.828/84.396 AIR-FusionNet(Huang et al., 2017) 75.3/83.",
      ", 2018) -/- 78.580/85.833 SAN (single model) 76.235/84.056 76.828/84.396 AIR-FusionNet(Huang et al., 2017) 75.3/83.6 75.968/83.900 RaSoR + TR (Salant and Berant, 2017) -/- 75.789/83.261 DCN+(Xiong et al., 2017) 74.5/83.1 75.087/83.081 r-net(Wang et al., 2017) 72.3/80.6 72.3/80.7 ReasoNet++(Shen et al., 2017) 70.8/79.4 70.6/79.36 BiDAF (Seo et al., 2016) 67.7/77.3 68.0/77.3 Human Performance 80.3/90.5 82.3/91.2 Table 2: Test performance on SQuAD. Results are sorted by Test F1.",
      ", 2016) 67.7/77.3 68.0/77.3 Human Performance 80.3/90.5 82.3/91.2 Table 2: Test performance on SQuAD. Results are sorted by Test F1. 5 Analysis 5.1 How robust are the results? We are interested in whether the proposed model is sensitive to different random initial conditions. Table 3 shows the development set scores of SAN trained from initialization with different random seeds. We observe that the SAN results are con- sistently strong regardless of the 10 different ini- tializations. For example, the mean EM score is 76.131 and the lowest EM score is 75.922, both of which still outperform the 75.355 EM of the Dy- namic step ReasoNet in Table 1.4 We are also interested in how sensitive are the results to the number of reasoning steps, which 4Note the Dev EM/F1 scores of ReasoNet in Table 1 do not match those of ReasoNet++ in Table 2. While the answer module is the same architecture, the lower encoding layers are different.",
      "While the answer module is the same architecture, the lower encoding layers are different. is a \ufb01xed hyper-parameter. Since we are using dropout, a natural question is whether we can ex- tend the number of steps to an extremely large number. Table 4 shows the development set scores for T = 1 to T = 10. We observe that there is a gradual improvement as we increase T = 1 to T = 5, but after 5 steps the improvements have saturated. In fact, the EM/F1 scores drop slightly, but considering that the random initialization re- sults in Table 3 show a standard deviation of 0.142 and a spread of 0.426 (for EM), we believe that the T = 10 result does not statistically differ from the T = 5 result. In summary, we think it is useful to perform some approximate hyper-parameter tun- ing for the number of steps, but it is not necessary to \ufb01nd the exact optimal value. Finally, we test SAN on two Adversarial SQuAD datasets, AddSent and AddOneSent (Jia and Liang, 2017), where the passages contain",
      "(a) EM comparison on different systems. (b) F1 score comparison on different systems. Figure 3: K-Best Oracle results auto-generated adversarial distracting sentences to fool computer systems that are developed to an- swer questions about the passages. For example, AddSent is constructed by adding sentences that look similar to the question, but do not actually contradict the correct answer. AddOneSent is con- structed by appending a random human-approved sentence to the passage. We evaluate the single SAN model (i.e., the one presented in Table 2) on both AddSent and Ad- dOneSent. The results in Table 5 show that SAN achieves the new state-of-the-art performance and SAN\u2019s superior result is mainly attributed to the multi-step answer module, which leads to signif- icant improvement in F1 score over the Standard 1-step answer module, i.e., +1.2 on AddSent and +0.7 on AddOneSent. 5.2 Is it possible to use different numbers of steps in test vs. train? For practical deployment scenarios, prediction speed at test time is an important criterion.",
      "5.2 Is it possible to use different numbers of steps in test vs. train? For practical deployment scenarios, prediction speed at test time is an important criterion. There- fore, one question is whether SAN can train with, e.g. T = 5 steps but test with T = 1 steps. Table 6 shows the results of a SAN trained on T = 5 steps, but tested with different number of steps. As ex- Seed# EM F1 Seed# EM F1 Seed 1 76.24 84.06 Seed 6 76.23 83.99 Seed 2 76.30 84.13 Seed 7 76.35 84.09 Seed 3 75.92 83.90 Seed 8 76.07 83.71 Seed 4 76.00 83.95 Seed 9 75.93 83.85 Seed 5 76.12 83.99 Seed 10 76.15 84.11 Mean: 76.131, Std. deviation: 0.142 (EM) Mean: 83.977, Std.",
      "deviation: 0.142 (EM) Mean: 83.977, Std. deviation: 0.126 (F1) Table 3: Robustness of SAN (5-step) on dif- ferent random seeds for initialization: best and worst scores are boldfaced. Note that our of\ufb01cial submit is trained on seed 1. SAN EM F1 SAN EM F1 1 step 75.38 83.29 6 step 75.99 83.72 2 step 75.43 83.41 7 step 76.04 83.92 3 step 75.89 83.57 8 step 76.03 83.82 4 step 75.92 83.85 9 step 75.95 83.75 5 step 76.24 84.06 10 step 76.04 83.89 Table 4: Effect of number of steps: best and worst results are boldfaced.",
      "pected, the results are best when T matches during training and test; however, it is important to note that small numbers of steps T = 1 and T = 2 nevertheless achieve strong results. For example, prediction at T = 1 achieves 75.58, which out- performs a standard 1-step model (75.14 EM) as in Table 1 that has approximate equivalent predic- tion time. 5.3 How does the training time compare? The average training time per epoch is compara- ble: our implementation running on a GTX Titan X is 22 minutes for 5-step memory net, 30 minutes for ReasoNet, and 24 minutes for SAN. The learn- ing curve is shown in Figure 4. We observe that all systems improve at approximately the same rate up to 10 or 15 epochs. However, SAN continues to improve afterwards as other models start to sat- urate. This observation is consistent with previous works using dropout (Srivastava et al., 2014).",
      "We observe that all systems improve at approximately the same rate up to 10 or 15 epochs. However, SAN continues to improve afterwards as other models start to sat- urate. This observation is consistent with previous works using dropout (Srivastava et al., 2014). We believe that while training time per epoch is sim- ilar between SAN and other models, it is recom- mended to train SAN for more epochs in order to achieve gains in EM/F1.",
      "Single model: AddSent AddOneSent LR (Rajpurkar et al., 2016) 23.2 30.3 SEDT (Liu et al., 2017a) 33.9 44.8 BiDAF (Seo et al., 2016) 34.3 45.7 jNet (Zhang et al., 2017) 37.9 47.0 ReasoNet(Shen et al., 2017) 39.4 50.3 RaSoR(Lee et al., 2016) 39.5 49.5 Mnemonic(Hu et al., 2017) 46.6 56.0 QANet(Yu et al., 2018) 45.2 55.7 Standard 1-step in Table 1 45.4 55.8 SAN 46.6 56.5 Table 5: Test performance on the adversarial SQuAD dataset in F1 score.",
      "T = EM F1 T = EM F1 1 75.58 83.86 4 76.12 83.98 2 75.85 83.90 5 76.24 84.06 3 75.98 83.95 10 75.89 83.88 Table 6: Prediction on different steps T. Note that the SAN model is trained using 5 steps. (a) EM (b) F1 Figure 4: Learning curve measured on Dev set. Figure 5: Score breakdown by question type. 5.4 How does SAN perform by question type? To see whether SAN performs well on a particular type of question, we divided the development set by questions type based on their respective Wh- word, such as \u201cwho\u201d and \u201cwhere\u201d. The score breakdown by F1 is shown in Figure 5. We ob- serve that SAN seems to outperform other models uniformly across all types. The only exception is the Why questions, but there is too little data to derive strong conclusions.",
      "The score breakdown by F1 is shown in Figure 5. We ob- serve that SAN seems to outperform other models uniformly across all types. The only exception is the Why questions, but there is too little data to derive strong conclusions. 5.5 Experiments results on MS MARCO MS MARCO (Nguyen et al., 2016) is a large scale real-word RC dataset which contains 100,100 (100K) queries collected from anonymized user logs from the Bing search engine. The character- istic of MS MARCO is that all the questions are real user queries and passages are extracted from real web documents. For each query, approximate 10 passages are extracted from public web docu- ments. The answers are generated by humans. The data is partitioned into a 82,430 training, a 10,047 development and 9,650 test tuples. The evalua- tion metrics are BLEU(Papineni et al., 2002) and ROUGE-L (Lin, 2004) due to its free-form text answer style.",
      "The evalua- tion metrics are BLEU(Papineni et al., 2002) and ROUGE-L (Lin, 2004) due to its free-form text answer style. To apply the same RC model, we search for a span in MS MARCO\u2019s passages that maximizes the ROUGE-L score with the raw free- form answer. It has an upper bound of 93.45 BLEU and 93.82 ROUGE-L on the development set. The MS MARCO dataset contains multiple pas- sages per query. Our model as shown in Figure 2 is developed to generate answer from a single pas- sage. Thus, we need to extend it to handle multiple passages. Following (Shen et al., 2017), we take two steps to generate an answer to a query Q from J passages, P 1, ..., P J. First, we run SAN on ev-",
      "SingleModel ROUGE BLEU ReasoNet++(Shen et al., 2017) 38.01 38.62 V-Net(Wang et al., 2018) 45.65 - Standard 1-step in Table 1 42.30 42.39 SAN 46.14 43.85 Table 7: MS MARCO devset results. ery (P j, Q) pair, generating J candidate answer spans, one from each passage. Then, we multiply the SAN score of each candidate answer span with its relevance score r(P j, Q) assigned by a passage ranker, and output the span with the maximum score as the answer. In our experiments, we use the passage ranker described in (Liu et al., 2018)5. The ranker is trained on the same MS MARCO training data, and achieves 37.1 p@1 on the devel- opment set. The results in Table 7 show that SAN outper- forms V-Net (Wang et al., 2018) and becomes the new state of the art6.",
      "The results in Table 7 show that SAN outper- forms V-Net (Wang et al., 2018) and becomes the new state of the art6. 6 Related Work The recent big progress on MRC is largely due to the availability of the large-scale datasets (Ra- jpurkar et al., 2016; Nguyen et al., 2016; Richard- son et al., 2013; Hill et al., 2016), since it is possi- ble to train large end-to-end neural network mod- els. In spite of the variety of model structures and attenion types (Bahdanau et al., 2015; Chen et al., 2016; Xiong et al., 2016; Seo et al., 2016; Shen et al., 2017; Wang et al., 2017), a typical neural network MRC model \ufb01rst maps the symbolic rep- resentation of the documents and questions into a neural space, then search answers on top of it. We categorize these models into two groups based on the difference of the answer module: single- step and multi-step reasoning.",
      "We categorize these models into two groups based on the difference of the answer module: single- step and multi-step reasoning. The key difference between the two is what strategies are applied to search the \ufb01nal answers in the neural space. A single-step model matches the question and document only once and produce the \ufb01nal an- swers. It is simple yet ef\ufb01cient and can be trained using the classical back-propagation algorithm, thus it is adopted by most systems (Chen et al., 2016; Seo et al., 2016; Wang et al., 2017; Liu et al., 2017b; Chen et al., 2017; Weissenborn et al., 2017; 5It is the same model structure as (Liu et al., 2018) by using softmax over all candidate passages. A simple baseline, TF-IDF, obtains 20.1 p@1 on MS MARCO development. 6The of\ufb01cial evaluation on MS MARCO on test is closed, thus here we only report the results on the development set. Hu et al., 2017).",
      "A simple baseline, TF-IDF, obtains 20.1 p@1 on MS MARCO development. 6The of\ufb01cial evaluation on MS MARCO on test is closed, thus here we only report the results on the development set. Hu et al., 2017). However, since humans often solve question answering tasks by re-reading and re-digesting the document multiple times before reaching the \ufb01nal answers (this may be based on the complexity of the questions/documents), it is natural to devise an iterative way to \ufb01nd answers as multi-step reasoning. Pioneered by (Hill et al., 2016; Dhingra et al., 2016; Sordoni et al., 2016; Kumar et al., 2015), who used a predetermined \ufb01xed number of rea- soning steps, Shen et al (2016; 2017) showed that multi-step reasoning outperforms single-step ones and dynamic multi-step reasoning further outperforms the \ufb01xed multi-step ones on two dis- tinct MRC datasets (SQuAD and MS MARCO).",
      "But these models have to be trained using rein- forcement learning methods, e.g., policy gradient, which are tricky to implement due to the instabil- ity issue. Our model is different in that we \ufb01x the number of reasoning steps, but perform stochastic dropout to prevent step bias. Further, our model can also be trained by using the back-propagation algorithm, which is simple and yet ef\ufb01cient. 7 Conclusion We introduce Stochastic Answer Networks (SAN), a simple yet robust model for machine reading comprehension. The use of stochastic dropout in training and averaging in test at the answer module leads to robust improvements on SQuAD, outperforming both \ufb01xed step memory networks and dynamic step ReasoNet. We further empirically analyze the properties of SAN in detail. The model achieves results competitive with the state-of-the-art on the SQuAD leader- board, as well as on the Adversarial SQuAD and MS MARCO datasets. Due to the strong connection between the proposed model with memory networks and ReasoNet, we would like to delve into the theoretical link between these models and its training algorithms.",
      "Due to the strong connection between the proposed model with memory networks and ReasoNet, we would like to delve into the theoretical link between these models and its training algorithms. Further, we also would like to explore SAN on other tasks, such as text classi\ufb01cation and natural language inference for its generalization in the future. Acknowledgments We thank Pengcheng He, Yu Wang and Xinying Song for help to set up dockers. We also thank Pranav Samir Rajpurkar for help on SQuAD eval- uations, and the anonymous reviewers for valuable discussions and comments.",
      "References Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate. International Con- ference on Learning Representations (ICLR2015) . Danqi Chen, Jason Bolton, and Christopher D. Man- ning. 2016. A thorough examination of the cnn/daily mail reading comprehension task. In Pro- ceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Berlin, Germany, pages 2358\u20132367. http://www.aclweb.org/anthology/P16-1223. Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer open- domain questions. In Association for Computa- tional Linguistics (ACL). Bhuwan Dhingra, Hanxiao Liu, William W Cohen, and Ruslan Salakhutdinov. 2016. Gated-attention readers for text comprehension.",
      "Reading Wikipedia to answer open- domain questions. In Association for Computa- tional Linguistics (ACL). Bhuwan Dhingra, Hanxiao Liu, William W Cohen, and Ruslan Salakhutdinov. 2016. Gated-attention readers for text comprehension. arXiv preprint arXiv:1606.01549 . Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. 2013. Maxout networks. arXiv preprint arXiv:1302.4389 . Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston. 2016. The goldilocks principle: Reading children\u2019s books with explicit memory representa- tions. ICLR . Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997. Long short-term memory. Neural computation 9(8):1735\u20131780. Minghao Hu, Yuxing Peng, and Xipeng Qiu. 2017. Mnemonic reader for machine comprehension.",
      "1997. Long short-term memory. Neural computation 9(8):1735\u20131780. Minghao Hu, Yuxing Peng, and Xipeng Qiu. 2017. Mnemonic reader for machine comprehension. arXiv preprint arXiv:1705.02798 . Hsin-Yuan Huang, Chenguang Zhu, Yelong Shen, and Weizhu Chen. 2017. Fusionnet: Fusing via fully- aware attention with application to machine compre- hension. arXiv preprint arXiv:1711.07341 . Robin Jia and Percy Liang. 2017. Adversarial exam- ples for evaluating reading comprehension systems. In Proceedings of the 2017 Conference on Empiri- cal Methods in Natural Language Processing. pages 2021\u20132031. Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 .",
      "pages 2021\u20132031. Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 . Ankit Kumar, Ozan Irsoy, Jonathan Su, James Brad- bury, Robert English, Brian Pierce, Peter Ondruska, Ishaan Gulrajani, and Richard Socher. 2015. Ask me anything: Dynamic memory networks for nat- ural language processing. CoRR abs/1506.07285. http://arxiv.org/abs/1506.07285. Kenton Lee, Tom Kwiatkowski, Ankur Parikh, and Di- panjan Das. 2016. Learning recurrent span repre- sentations for extractive question answering. arXiv preprint arXiv:1611.01436 . Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San- tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio.",
      "Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San- tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. 2017. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130 . Rui Liu, Junjie Hu, Wei Wei, Zi Yang, and Eric Ny- berg. 2017a. Structural embedding of syntactic trees for machine comprehension. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. pages 815\u2013824. Rui Liu, Wei Wei, Weiguang Mao, and Maria Chik- ina. 2017b. Phase conductor on multi-layered at- tentions for machine comprehension. arXiv preprint arXiv:1710.10504 . Xiaodong Liu, Kevin Duh, and Jianfeng Gao. 2018. Stochastic answer networks for natural language in- ference.",
      "arXiv preprint arXiv:1710.10504 . Xiaodong Liu, Kevin Duh, and Jianfeng Gao. 2018. Stochastic answer networks for natural language in- ference. arXiv preprint arXiv:1804.07888 . Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in transla- tion: Contextualized word vectors. arXiv preprint arXiv:1708.00107 . Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. Ms marco: A human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268 . Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th annual meeting on association for compu- tational linguistics.",
      "Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th annual meeting on association for compu- tational linguistics. Association for Computational Linguistics, pages 311\u2013318. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 Con- ference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computa- tional Linguistics, Doha, Qatar, pages 1532\u20131543. http://www.aclweb.org/anthology/D14-1162. M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. 2018. Deep contextualized word representations. ArXiv e-prints . Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.",
      "2018. Deep contextualized word representations. ArXiv e-prints . Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text pages 2383\u20132392. https://aclweb.org/anthology/D16-1264.",
      "Matthew Richardson, Christopher J.C. Burges, and Erin Renshaw. 2013. MCTest: A challenge dataset for the open-domain machine comprehension of text. In Proceedings of the 2013 Conference on Em- pirical Methods in Natural Language Processing. Association for Computational Linguistics, Seattle, Washington, USA, pages 193\u2013203. S. Salant and J. Berant. 2017. Contextualized Word Representations for Reading Comprehension. ArXiv e-prints . Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Bidirectional attention \ufb02ow for machine comprehension. arXiv preprint arXiv:1611.01603 . Yelong Shen, Po-Sen Huang, Jianfeng Gao, and Weizhu Chen. 2016. Reasonet: Learning to stop reading in machine comprehension. arXiv preprint arXiv:1609.05284 . Yelong Shen, Xiaodong Liu, Kevin Duh, and Jianfeng Gao. 2017.",
      "2016. Reasonet: Learning to stop reading in machine comprehension. arXiv preprint arXiv:1609.05284 . Yelong Shen, Xiaodong Liu, Kevin Duh, and Jianfeng Gao. 2017. An empirical analysis of multiple-turn reasoning strategies in reading comprehension tasks. arXiv preprint arXiv:1711.03230 . Alessandro Sordoni, Philip Bachman, Adam Trischler, and Yoshua Bengio. 2016. Iterative alternating neu- ral attention for machine reading. arXiv preprint arXiv:1606.02245 . Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from over\ufb01tting. Journal of Machine Learning Re- search 15(1):1929\u20131958.",
      "2014. Dropout: a simple way to prevent neural networks from over\ufb01tting. Journal of Machine Learning Re- search 15(1):1929\u20131958. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. arXiv preprint arXiv:1706.03762 . Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and Ming Zhou. 2017. Gated self-matching net- works for reading comprehension and question an- swering. In Proceedings of the 55th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers). volume 1, pages 189\u2013198. Y. Wang, K. Liu, J. Liu, W. He, Y. Lyu, H. Wu, S. Li, and H. Wang. 2018.",
      "volume 1, pages 189\u2013198. Y. Wang, K. Liu, J. Liu, W. He, Y. Lyu, H. Wu, S. Li, and H. Wang. 2018. Multi-Passage Ma- chine Reading Comprehension with Cross-Passage Answer Veri\ufb01cation. ArXiv e-prints . Dirk Weissenborn, Georg Wiese, and Laura Seiffe. 2017. Fastqa: A simple and ef\ufb01cient neural ar- chitecture for question answering. arXiv preprint arXiv:1703.04816 . Caiming Xiong, Victor Zhong, and Richard Socher. 2016. Dynamic coattention networks for question answering. arXiv preprint arXiv:1611.01604 . Caiming Xiong, Victor Zhong, and Richard Socher. 2017. Dcn+: Mixed objective and deep residual coattention for question answering. arXiv preprint arXiv:1711.00106 .",
      "Caiming Xiong, Victor Zhong, and Richard Socher. 2017. Dcn+: Mixed objective and deep residual coattention for question answering. arXiv preprint arXiv:1711.00106 . Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V. Le. 2018. Qanet: Combining local convolution with global self-attention for reading comprehension. Junbei Zhang, Xiaodan Zhu, Qian Chen, Lirong Dai, and Hui Jiang. 2017. Exploring ques- tion understanding and adaptation in neural- network-based question answering. arXiv preprint arXiv:1703.04617 ."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1712.03556.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":9991,
  "avg_doclen":172.2586206897,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1712.03556.pdf"
    }
  }
}