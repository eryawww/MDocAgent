{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "Bringing Stories Alive: Generating Interactive Fiction Worlds Prithviraj Ammanabrolu\u2217, Wesley Cheung\u2217, Dan Tu , William Broniec and Mark O. Riedl School of Interactive Computing Georgia Institute of Technology {raj.ammanabrolu, wcheung8, dan.tu wbroniec3, riedl}@gatech.edu, Abstract World building forms the foundation of any task that requires narrative intelligence. In this work, we focus on procedurally generating interactive \ufb01c- tion worlds\u2014text-based worlds that players \u201csee\u201d and \u201ctalk to\u201d using natural language. Generat- ing these worlds requires referencing everyday and thematic commonsense priors in addition to being semantically consistent, interesting, and coherent throughout. Using existing story plots as inspira- tion, we present a method that \ufb01rst extracts a par- tial knowledge graph encoding basic information regarding world structure such as locations and ob- jects.",
      "Using existing story plots as inspira- tion, we present a method that \ufb01rst extracts a par- tial knowledge graph encoding basic information regarding world structure such as locations and ob- jects. This knowledge graph is then automatically completed utilizing thematic knowledge and used to guide a neural language generation model that \ufb02eshes out the rest of the world.We perform hu- man participant-based evaluations, testing our neu- ral model\u2019s ability to extract and \ufb01ll-in a knowl- edge graph and to generate language conditioned on it against rule-based and human-made base- lines. Our code is available at https://github.com/ rajammanabrolu/WorldGeneration. 1 Introduction Interactive \ufb01ctions\u2014also called text-adventure games or text- based games\u2014are games in which a player interacts with a virtual world purely through textual natural language\u2014 receiving descriptions of what they \u201csee\u201d and writing out how they want to act, an example can be seen in Figure 1. Interac- tive \ufb01ction games are often structured as puzzles, or quests, set within the con\ufb01nes of given game world.",
      "Interac- tive \ufb01ction games are often structured as puzzles, or quests, set within the con\ufb01nes of given game world. Interactive \ufb01ctions have been adopted as a test-bed for real-time game playing agents [Narasimhan et al., 2015; C\u02c6ot\u00b4e et al., 2018; Hausknecht et al., 2019b]. Unlike other, graphical games, interactive \ufb01ctions test agents\u2019 abilities to infer the state of the world through communication and to indirectly affect change in the world through language. Interactive \ufb01ctions are typically modeled after real or fantasy worlds; com- monsense knowledge is an important factor in successfully \u2217Denotes equal contribution. playing interactive \ufb01ctions [Ammanabrolu and Riedl, 2019a; Yin and May, 2019]. In this paper we explore a different challenge for arti\ufb01- cial intelligence: automatically generating text-based virtual worlds for interactive \ufb01ctions. A core component of many narrative-based tasks\u2014everything from storytelling to game generation\u2014is world building.",
      "In this paper we explore a different challenge for arti\ufb01- cial intelligence: automatically generating text-based virtual worlds for interactive \ufb01ctions. A core component of many narrative-based tasks\u2014everything from storytelling to game generation\u2014is world building. The world of a story or game de\ufb01nes the boundaries of where the narrative is allowed and what the player is allowed to do. There are four core chal- lenges to world generation: (1) commonsense knowledge: the world must reference priors that the player possesses so that players can make sense of the world and build expectations on how to interact with it. This is especially true in interac- tive \ufb01ctions where the world is presented textually because many details of the world necessarily be left out (e.g., the pot is on a stove; kitchens are found in houses) that might oth- erwise be literal in a graphical virtual world. (2) Thematic knowledge: interactive \ufb01ctions usually involve a theme or genre that comes with its own expectations. For example, light speed travel is plausible in sci-\ufb01worlds but not realistic in the real world.",
      "(2) Thematic knowledge: interactive \ufb01ctions usually involve a theme or genre that comes with its own expectations. For example, light speed travel is plausible in sci-\ufb01worlds but not realistic in the real world. (3) Coherence: the world must not appear to be an random assortment of locations. (3) Natural language: The descriptions of the rooms as well as the permissible ac- tions must text, implying that the system has natural language generation capability. Because worlds are conveyed entirely through natural lan- guage, the potential output space for possible generated worlds is combinatorially large. To constrain this space and to make it possible to evaluate generated world, we present an approach which makes use of existing stories, building on the worlds presented in them but leaving enough room for the worlds to be unique. Speci\ufb01cally, we take a story such as Sherlock Holmes or Rapunzel\u2014a linear reading experience\u2014and extract the description of the world the story is set in to make an interactive world the player can explore.",
      "Speci\ufb01cally, we take a story such as Sherlock Holmes or Rapunzel\u2014a linear reading experience\u2014and extract the description of the world the story is set in to make an interactive world the player can explore. Our method \ufb01rst extracts a partial, potentially disconnected knowledge graph from the story, encoding information re- garding locations, characters, and objects in the form of \u27e8entity, relation, entity\u27e9triples. Relations between these types of entities as well as their properties are captured in this knowledge graph. However, stories often do not explic- itly contain all the information required to fully \ufb01ll out such a graph. A story may mention that there is a sword stuck in a stone but not what you can do with the sword or where it is arXiv:2001.10161v1  [cs.AI]  28 Jan 2020",
      "Bank vault It is about three feet in height, and one and a half in width. Exits: Baker Street and Wilson\u2019s shop You see: Archie, Helper and John Clay Action: Examine John Clay John Clay Short, stocky, and one of the taller kind, John Clay is the kind of man who lives and dies by the watch he keeps. Action: Go to Baker Street Baker Street Like any other street in London, it is a-stage-set, with the best and the worst of society crammed into one place. Figure 1: Example player interaction in the deep neural generated mystery setting. in relation to everything else. Our method \ufb01lls in missing re- lation and affordance information using thematic knowledge gained from training on stories in a similar genre. This knowl- edge graph is then used to guide the text description genera- tion process for the various locations, characters, and objects. The game is then assembled on the basis of the knowledge graph and the corresponding generated descriptions. We have two major contributions. (1) A neural model and a rules-based baseline for each of the tasks described above.",
      "The game is then assembled on the basis of the knowledge graph and the corresponding generated descriptions. We have two major contributions. (1) A neural model and a rules-based baseline for each of the tasks described above. The phases are that of graph extraction and comple- tion followed by description generation and game formula- tion. Each of these phases are relatively distinct and utilize their own models. (2) A human subject study for compar- ing the neural model and variations on it to the rules-based and human-made approaches. We perform two separate hu- man subject studies\u2014one for the \ufb01rst phase of knowledge graph construction and another for the overall game creation process\u2014testing speci\ufb01cally for coherence, interestingness, and the ability to maintain a theme or genre. 2 Related Work There has been a slew of recent work in developing agents that can play text games [Narasimhan et al., 2015; Haroush et al., 2018; C\u02c6ot\u00b4e et al., 2018; Hausknecht et al., 2019a].",
      "Am- manabrolu and Riedl [2019a; 2019b; 2020] in particular use knowledge graphs as state representations for game-playing agents. Yuan et al. [2019] propose QAit, a set of ques- tion answering tasks framed as text-based or interactive \ufb01c- tion games. QAit focuses on helping agents learn procedural knowledge through interaction with a dynamic environment. These works all focus on agents that learn to play a given set of interactive \ufb01ction games as opposed to generating them. Scheherazade [Li et al., 2012] is a system that learns a plot graph based on stories written by crowd sourcing the task of writing short stories. The learned plot graph contains details relevant to ensure story coherence. It includes: plot events, temporal precedence, and mutual exclusion relations. Scheherazade-IF [Guzdial et al., 2015] extends the system to generate choose-your-own-adventure style interactive \ufb01ctions in which the player chooses from prescribed options.",
      "It includes: plot events, temporal precedence, and mutual exclusion relations. Scheherazade-IF [Guzdial et al., 2015] extends the system to generate choose-your-own-adventure style interactive \ufb01ctions in which the player chooses from prescribed options. Wom- ack and Freeman [2019] explore a method of creating interac- tive narratives revolving around locations, wherein sentences are mapped to a real-world GPS location from a corpus of sentences belonging to a certain genre. Narratives are made by chaining together sentences selected based on the player\u2019s current real-world location. In contrast to these models, our method generates a parser-based interactive \ufb01ction in which the player types in a textual command, allowing for greater expressiveness. Ammanabrolu et al. [2019] de\ufb01ne the problem of proce- dural content generation in interactive \ufb01ction games in terms of the twin considerations of world and quest generation and focus on the latter. They present a system in which quest content is \ufb01rst generated by learning from a corpus and then grounded into a given interactive \ufb01ction world.",
      "They present a system in which quest content is \ufb01rst generated by learning from a corpus and then grounded into a given interactive \ufb01ction world. The work is this paper focuses on the world generation problem glossed in the prior work. Thus these two systems can be seen as complimentary. Light [Urbanek et al., 2019] is a crowdsourced dataset of grounded text-adventure game dialogues. It contains infor- mation regarding locations, characters, and objects set in a fantasy world. The authors demonstrate that the supervised training of transformer-based models lets us contextually rel- evant dialog, actions, and emotes. Most in line with the spirit of this paper, Fan et al. [2019] leverage Light to generate worlds for text-based games. They train a neural network based model using Light to compositionally arrange loca- tions, characters, and objects into an interactive world. Their model is tested using a human subject study against other ma- chine learning based algorithms with respect to the cohesive- ness and diversity of generated worlds. Our work, in contrast, focuses on extracting the information necessary for building interactive worlds from existing story plots.",
      "Their model is tested using a human subject study against other ma- chine learning based algorithms with respect to the cohesive- ness and diversity of generated worlds. Our work, in contrast, focuses on extracting the information necessary for building interactive worlds from existing story plots. 3 World Generation World generation happens in two phases. In the \ufb01rst phase, a partial knowledge graph is extracted from a story plot and then \ufb01lled in using thematic commonsense knowledge. In the second phase, the graph is used as the skeleton to generate a full interactive \ufb01ction game\u2014generating textual descriptions or \u201c\ufb02avortext\u201d for rooms and embedded objects. We present a novel neural approach in addition to a rule guided baseline for each of these phases in this section. 3.1 Knowledge Graph Construction The \ufb01rst phase is to extract a knowledge graph from the story that depicts locations, characters, objects, and the relations between these entities. We present two techniques. The \ufb01rst uses neural question-answering technique to extract relations from a story text.",
      "3.1 Knowledge Graph Construction The \ufb01rst phase is to extract a knowledge graph from the story that depicts locations, characters, objects, and the relations between these entities. We present two techniques. The \ufb01rst uses neural question-answering technique to extract relations from a story text. The second, provided as a baseline, uses OpenIE51, a commonly used rule-based information extrac- 1https://github.com/dair-iitd/OpenIE-standalone",
      "John Clay arti\ufb01cial knee bank vault Baker Street Wilson's shop Archie helper trouser knees red hair taps on the  pavement Location \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  Character Object has has has has has has has next to next to Figure 2: Example knowledge graph constructed by AskBERT. Story Plot ALBERT-QA ALBERT-QA Entity Extraction Questions Relation Extraction Questions Text Game  Knowlege Graph Vertex Set 1 2 Figure 3: Overall AskBERT pipeline for graph construction. tion technique. For the sake of simplicity, we considered pri- marily the location-location and location-character/object re- lations, represented by the \u201cnext to\u201d and \u201chas\u201d edges respec- tively in Figure 2. Neural Graph Construction While many neural models already exist that perform simi- lar tasks such as named entity extraction and part of speech tagging, they often come at the cost of large amounts of specialized labeled data suited for that task. We in- stead propose a new method that leverages models trained for context-grounded question-answering tasks to do en- tity extraction with no task dependent data or \ufb01ne-tuning necessary.",
      "We in- stead propose a new method that leverages models trained for context-grounded question-answering tasks to do en- tity extraction with no task dependent data or \ufb01ne-tuning necessary. Our method, dubbed AskBERT, leverages the Question-Answering (QA) model ALBERT [Lan et al., 2019]. AskBERT consists of two main steps as shown in Fig- ure 3: vertex extraction and graph construction. The \ufb01rst step is to extract the set of entities\u2014graph vertices\u2014from the story. We are looking to extract informa- tion speci\ufb01cally regarding characters, locations, and objects. This is done by using asking the QA model questions such as \u201cWho is a character in the story?\u201d. Ribeiro et al. [2019] have shown that the phrasing of questions given to a QA model is important and this forms the basis of how we formulate our questions\u2014questions are asked so that they are more likely to return a single answer, e.g. asking \u201cWhere is a location in the story?\u201d as opposed to \u201cWhere are the locations in the story?\u201d.",
      "asking \u201cWhere is a location in the story?\u201d as opposed to \u201cWhere are the locations in the story?\u201d. In particular, we notice that pronoun choice can be crucial; \u201cWhere is a location in the story?\u201d yielded more con- sistent extraction than \u201cWhat is a location in the story?\u201d. AL- BERT QA is trained to also output a special <no-answer> token when it cannot \ufb01nd an answer to the question within the story. Our method makes use of this by iteratively asking QA model a question and masking out the most likely answer outputted on the previous step. This process continues until the <no-answer> token becomes the most likely answer. The next step is graph construction. Typical interactive \ufb01c- tion worlds are usually structured as trees, i.e. no cycles ex- cept between locations. Using this fact, we use an approach that builds a graph from the vertex set by one relation\u2014or edge\u2014at a time.",
      "The next step is graph construction. Typical interactive \ufb01c- tion worlds are usually structured as trees, i.e. no cycles ex- cept between locations. Using this fact, we use an approach that builds a graph from the vertex set by one relation\u2014or edge\u2014at a time. Once again using the entire story plot as context, we query the ALBERT-QA model picking a ran- dom starting location x from the set of vertices previously extracted.and asking the questions \u201cWhat location can I visit from x?\u201d and \u201cWho/What is in x?\u201d. The methodology for phrasing these questions follows that described for the vertex extraction. The answer given by the QA model is matched to the vertex set by picking the vertex u that contains the best word-token overlap with the answer. Relations between ver- tices are added by computing a relation probability on the basis of the output probabilities of the answer given by the QA model.",
      "The answer given by the QA model is matched to the vertex set by picking the vertex u that contains the best word-token overlap with the answer. Relations between ver- tices are added by computing a relation probability on the basis of the output probabilities of the answer given by the QA model. The probability that vertices x, u are related: P(x, u) = p(x, u) + p(u, x) 2 (1) where p(x, u) = X o\u2208QA outputs p(o)1{u = argmax v (v \u2229o)} (2) is the sum of the individual token probabilities of all the over- lapping tokens in the answer from the QA model and u. Rule-Based Graph Construction We compared our proposed AskBERT method with a non- neural, rule-based approach. This approach is based on the information extracted by OpenIE5, followed by some post-processing such as named-entity recognition and part- of-speech tagging. OpenIE5 combines several cutting-edge ideas from several existing papers [Saha and Mausam, 2018; Pal and Mausam, 2016; Christensen et al., 2011] to create a powerful information extraction tools.",
      "OpenIE5 combines several cutting-edge ideas from several existing papers [Saha and Mausam, 2018; Pal and Mausam, 2016; Christensen et al., 2011] to create a powerful information extraction tools. For a given sen- tence, OpenIE5 generates multiple triples in the format of \u27e8entity, relation, entity\u27e9as concise representations of the sentence, each with a con\ufb01dence score. These triples are also occasionally annotated with location information indicating that a triple happened in a location. As in the neural AskBERT model, we attempt to extract information regarding locations, characters, and objects. The entire story plot is passed into the OpenIE5 and we receive a set of triples. The location annotations on the triples are used to create a set of locations. We mark which sentences in the story contain these locations. POS tagging based on marking",
      "noun-phrases is then used in conjunction with NER to further \ufb01lter the set of triples\u2014identifying the set of characters and objects in the story. The graph is constructed by linking the set of triples on the basis of the location they belong to. While some sen- tences contain very explicit location information for OpenIE5 to mark it out in the triples, most of them do not. We therefore make the assumption that the location remains the same for all triples extracted in between sentences where locations are explicitly mentioned. For example, if there exists locationA in the 1st sentence and locationB in the 5th sentence of the story, all the events described in sentences 1-4 are considered to take place in locationA. The entities mentioned in these events are connected to locationA in the graph. 3.2 Description Generation The second phase involves using the constructed knowledge graph to generate textual descriptions of the entities we have extracted, also known as \ufb02avortext. This involves generating descriptions of what a player \u201csees\u201d when they enter a loca- tion and short blurbs for each object and character.",
      "This involves generating descriptions of what a player \u201csees\u201d when they enter a loca- tion and short blurbs for each object and character. These descriptions need to not only be faithful to the information present in the knowledge graph and the overall story plot but to also contain \ufb02avor and be interesting for the player. Neural Description Generation Here, we approach the problem of description generation by taking inspiration from conditional transformer-based gener- ation methods [Shirish Keskar et al., 2019]. Our approach is outlined in Figure 4 and an example description shown in Figure 1. For any given entity in the story, we \ufb01rst locate it in the story plot and then construct a prompt which consists of the entire story up to and including the sentence when the entity is \ufb01rst mentioned in the story followed by a question asking to describe that entity. With respect to prompts, we found that more direct methods such as question-answering were more consistent than open-ended sentence completion. For example, \u201cQ: Who is the prince? A:\u201d often produced descriptions that were more faithful to the information al- ready present about the prince in the story than \u201cYou see the prince.",
      "For example, \u201cQ: Who is the prince? A:\u201d often produced descriptions that were more faithful to the information al- ready present about the prince in the story than \u201cYou see the prince. He is/looks\u201d. For our transformer-based genera- tion, we use a pre-trained 355M GPT-2 model [Radford et al., 2019] \ufb01netuned on a corpus of plot summaries collected from Wikipedia. The plots used for \ufb01netuning are tailored speci\ufb01c to the genre of the story in order to provide more relevant gen- eration for the target genre. Additional details regarding the datasets used are provided in Section 4. This method strikes a balance between knowledge graph verbalization techniques which often lack \u201c\ufb02avor\u201d and open ended generation which struggles to maintain semantic coherence. Rules-Based Description Generation In the rule-based approach, we utilized the templates from the built-in text game generator of TextWorld [C\u02c6ot\u00b4e et al., 2018] to generate the description for our graphs.",
      "Rules-Based Description Generation In the rule-based approach, we utilized the templates from the built-in text game generator of TextWorld [C\u02c6ot\u00b4e et al., 2018] to generate the description for our graphs. TextWorld is an open-source library that provides a way to generate text- game learning environments for training reinforcement learn- ing agents using pre-built grammars. Two major templates involved here are the Room In- tro Templates and Container Description Templates from TextWorld, responsible for generating descriptions of loca- tions and blurbs for objects/characters respectively. The lo- cation and object/character information are taken from the knowledge graph constructed previously. \u2022 Example of Room Intro Templates: \u201cThis might come as a shock to you, but you\u2019ve just #entered# a <location-name>\u201d \u2022 Example of Container Description Templates: \u201cThe <location-name> #contains# <object/person- name>\u201d Each token surrounded by # sign can be expanded using a se- lect set of terminal tokens. For instance, #entered# could be \ufb01lled with any of the following phrases here: entered; walked into; fallen into; moved into; stumbled into; come into.",
      "For instance, #entered# could be \ufb01lled with any of the following phrases here: entered; walked into; fallen into; moved into; stumbled into; come into. Additional pre\ufb01xes, suf\ufb01xes and adjectives were added to increase the relative variety of descriptions. Unlike the neural methods, the rule-based approach is not able to gen- erate detailed and \ufb02avorful descriptions of the properties of the locations/objects/characters. By virtue of the templates, however, it is much better at maintaining consistency with the information contained in the knowledge graph. 4 Evaluation We conducted two sets of human participant evaluations by recruiting participants over Amazon Mechanical Turk. The \ufb01rst evaluation tests the knowledge graph construction phase, in which we measure perceived coherence and genre or theme resemblance of graphs extracted by different models. The second study compares full games\u2014including description generation and game assembly, which can\u2019t easily be iso- lated from graph construction\u2014generated by different meth- ods. This study looks at how interesting the games were to the players in addition to overall coherence and genre re- semblance. Both studies are performed across two genres: mystery and fairy-tales.",
      "This study looks at how interesting the games were to the players in addition to overall coherence and genre re- semblance. Both studies are performed across two genres: mystery and fairy-tales. This is done in part to test the rela- tive effectiveness of our approach across different genres with varying thematic commonsense knowledge. The dataset used was compiled via story summaries that were scraped from Wikipedia via a recursive crawling bot. The bot searched pages for both for plot sections as well as links to other poten- tial stories. From the process, 695 fairy-tales and 536 mys- tery stories were compiled from two categories: novels and short stories. We note that the mysteries did not often contain many fantasy elements, i.e. they consisted of mysteries set in our world such as Sherlock Holmes, while the fairy-tales were much more removed from reality. Details regarding how each of the studies were conducted and the corresponding setup are presented below. 4.1 Knowledge Graph Construction Evaluation We \ufb01rst select a subset of 10 stories randomly from each genre and then extract a knowledge graph using three dif- ferent models.",
      "Details regarding how each of the studies were conducted and the corresponding setup are presented below. 4.1 Knowledge Graph Construction Evaluation We \ufb01rst select a subset of 10 stories randomly from each genre and then extract a knowledge graph using three dif- ferent models. Each participant is presented with the three graphs extracted from a single story in each genre and then asked to rank them on the basis of how coherent they were and how well the graphs match the genre. The graphs resem- bles the one shown in in Figure 2 and are presented to the",
      "Target  Entity Story Plot Jabez Wilson, a London pawnbroker, comes to consult Sherlock Holmes and Doctor Watson.  ... They are John Clay, who has a long history of criminal activity already, and his helper Archie. Q: Who is John Clay? A: Short, stocky, and one of the taller kind, John Clay is the kind of man who lives and dies by the watch he keeps. Target Entity Description Conditioned GPT-2 Generation Generation Prompt Figure 4: Overview for neural description generation. Genre Category Neural Rules Mystery Locations 7.2 3.5 Characters 4.8 4.1 Objects 3.2 12.2 Fairy-tale Locations 4.0 1.8 Characters 3.3 1.2 Objects 4.1 8.7 Table 1: Vertex statistics: Average vertex count by type per genre. The random model has the same vertex statistics as the neural model. Genre Statistic Neural Rules Random Mystery Avg. Edges 10.7 22.3 10.7 Avg.",
      "The random model has the same vertex statistics as the neural model. Genre Statistic Neural Rules Random Mystery Avg. Edges 10.7 22.3 10.7 Avg. Degree 1.63 \u00b1 1.77 2.15 \u00b1 0.38 1.63 \u00b1 1.63 Fairy-tale Avg. Edges 16.7 12 16.7 Avg. Degree 1.73 \u00b1 2.04 1.98 \u00b1 0.29 1.73 \u00b1 1.64 Table 2: Edge and degree statistics: Average edge count , average degree count, and degree standard deviation of the graphs per genre. participant sequentially. The exact order of the graphs and genres was also randomized to mitigate any potential latent correlations. Overall, this study had a total of 130 partici- pants.This ensures that, on average, graphs from every story were seen by 13 participants. In addition to the neural AskBERT and rules-based meth- ods, we also test a variation of the neural model which we dub to be the \u201crandom\u201d approach.",
      "In addition to the neural AskBERT and rules-based meth- ods, we also test a variation of the neural model which we dub to be the \u201crandom\u201d approach. The method of vertex ex- traction remains identical to the neural method, but we in- stead connect the vertices randomly instead of selecting the most con\ufb01dent according to the QA model. We initialize the graph with a starting location entity. Then, we randomly sam- ple from the vertex set and connect it to a randomly sampled location in the graph until every vertex has been connected. This ablation in particular is designed to test the ability of our neural model to predict relations between entities. It lets us observe how accurately linking related vertices effects each of the metrics that we test for. For a fair comparison between the graphs produced by different approaches, we randomly removed some of the nodes and edges from the initial graphs so that the maximum number of locations per graph and the maximum number of objects/people per location in each story genre are the same. The results are shown in Table 3. We show the median rank of each of the models for both questions across the gen- res.",
      "The results are shown in Table 3. We show the median rank of each of the models for both questions across the gen- res. Ranked data is generally closely interrelated and so we perform Friedman\u2019s test between the three models to vali- Genre Questions Neural Rules Random p-value Mystery Resembles Genre 2 1 3 0.35 Coherence 1 2 3 0.049\u2217 Fairy-tale Resembles Genre 1 3 2 0.014\u2217 Coherence 1 3 2 0.013\u2217 Table 3: Results of the knowledge graph evaluation study. date that the results are statistically signi\ufb01cant. This is pre- sented as the p-value in table (asterisks indicate signi\ufb01cance at p < 0.05). In cases where we make comparisons between speci\ufb01c pairs of models, when necessary, we additionally per- form the Mann-Whitney U test to ensure that the rankings differed signi\ufb01cantly. In the mystery genre, the rules-based method was often ranked \ufb01rst in terms of genre resemblance, followed by the neural and random models.",
      "In the mystery genre, the rules-based method was often ranked \ufb01rst in terms of genre resemblance, followed by the neural and random models. This particular result was not statistically signi\ufb01cant however, likely indicating that all the models performed approximately equally in this category. The neural approach was deemed to be the most coherent followed by the rules and random. For the fairy-tales, the neural model ranked higher on both of the questions asked of the participants. In this genre, the random neural model also performed better than the rules based approach. Tables 1 and 2 show the statistics of the constructed knowl- edge graphs in terms of vertices and edges. We see that the rules-based graph construction has a lower number of loca- tions, characters, and relations between entities but far more objects in general. The greater number of objects is likely due to the rules-based approach being unable to correctly iden- tify locations and characters. The gap between the methods is less pronounced in the mystery genre as opposed to the fairy-tales, in fact the rules-based graphs have more relations than the neural ones.",
      "The greater number of objects is likely due to the rules-based approach being unable to correctly iden- tify locations and characters. The gap between the methods is less pronounced in the mystery genre as opposed to the fairy-tales, in fact the rules-based graphs have more relations than the neural ones. The random and neural models have the same number of entities in all categories by construction but random in general has lower variance on the number of relations found. In this case as well, the variance is lower for mystery as opposed to fairy-tales. When taken in the context of the results in Table 3, it appears to indicate that leveraging thematic commonsense in the form of AskBERT for graph construction directly results in graphs that are more coherent and maintain genre more easily. This is especially true in the case of the fairy-tales where the thematic and everyday com- monsense diverge more than than in the case of the mysteries.",
      "4.2 Full Game Evaluation This participant study was designed to test the overall game formulation process encompassing both phases described in Section 3. A single story from each genre was chosen by hand from the 10 stories used for the graph evaluation pro- cess. From the knowledge graphs for this story, we gen- erate descriptions using the neural, rules, and random ap- proaches described previously. Additionally, we introduce a human-authored game for each story here to provide an ad- ditional benchmark. This author selected was familiar with text-adventure games in general as well as the genres of de- tective mystery and fairy tale. To ensure a fair comparison, we ensure that the maximum number of locations and maxi- mum number of characters/objects per location matched the other methods. After setting general format expectations, the author read the selected stories and constructed knowledge graphs in a corresponding three step process of: identifying the n most important entities in the story, mapping positional relationships between entities, and then synthesizing \ufb02avor text for the entities based off of said location, the overall story plot, and background topic knowledge.",
      "Once the knowledge graph and associated descriptions are generated for a particular story, they are then automatically turned into a fully playable text-game using the text game engine Evennia2. Evennia was chosen for its \ufb02exibility and customization, as well as a convenient web client for end user testing. The data structures were translated into builder com- mands within Evennia that constructed the various layouts, \ufb02avor text, and rules of the game world. Users were placed in one \u201croom\u201d out of the different world locations within the game they were playing, and asked to explore the game world that was available to them. Users achieved this by moving be- tween rooms and investigating objects. Each time a new room was entered or object investigated, the player\u2019s total number of explored entities would be displayed as their score. Each participant was was asked to play the neural game and then another one from one of the three additional mod- els within a genre. The completion criteria for each game is collect half the total score possible in the game, i.e. explore half of all possible rooms and examine half of all possible entities.",
      "The completion criteria for each game is collect half the total score possible in the game, i.e. explore half of all possible rooms and examine half of all possible entities. This provided the participant with multiple possi- ble methods of \ufb01nishing a particular game. On completion, the participant was asked to rank the two games according to overall perceived coherence, interestingness, and adherence to the genre. We additionally provided a required initial tu- torial game which demonstrated all of these mechanics. The order in which participants played the games was also ran- domized as in the graph evaluation to remove potential corre- lations. We had 75 participants in total, 39 for mystery and 36 for fairy-tales. As each player played the neural model cre- ated game and one from each of the other approaches\u2014this gave us 13 on average for the other approaches in the mystery genre and 12 for fairy-tales. The summary of the results of the full game study is shown in Table 4.",
      "As each player played the neural model cre- ated game and one from each of the other approaches\u2014this gave us 13 on average for the other approaches in the mystery genre and 12 for fairy-tales. The summary of the results of the full game study is shown in Table 4. As the comparisons made in this study are all made pairwise between our neural model and one of the baselines\u2014they are presented in terms of what percentage of 2http://www.evennia.com/ Genre Questions Random Rules Human Mystery Interesting 45 72* 69* Coherence 36* 45* 69* Resembles Genre 45 38* 75* Fairy-tale Interesting 42 37* 64* Coherence 25* 25* 45 Resembles Genre 25* 37* 69* Table 4: Results of the full game evaluation participant study. *In- dicates statistical signi\ufb01cance (p < 0.05). participants prefer the baseline game over the neural game.",
      "*In- dicates statistical signi\ufb01cance (p < 0.05). participants prefer the baseline game over the neural game. Once again, as this is highly interrelated ranked data, we per- form the Mann-Whitney U test between each of the pairs to ensure that the rankings differed signi\ufb01cantly. This is also indicated on the table. In the mystery genre, the neural approach is generally pre- ferred by a greater percentage of participants than the rules or random. The human-made game outperforms them all. A signi\ufb01cant exception to is that participants thought that the rules-based game was more interesting than the neural game. The trends in the fairy-tale genre are in general similar with a few notable deviations. The \ufb01rst deviation is that the rules- based and random approaches perform signi\ufb01cantly worse than neural in this genre. We see also that the neural game is as coherent as the human-made game. As in the previous study, we hypothesize that this is likely due to the rules-based approach being more suited to the mys- tery genre, which is often more mundane and contains less fantastical elements.",
      "We see also that the neural game is as coherent as the human-made game. As in the previous study, we hypothesize that this is likely due to the rules-based approach being more suited to the mys- tery genre, which is often more mundane and contains less fantastical elements. By extension, we can say that thematic commonsense in fairy-tales has less overlap with everyday commonsense than for mundane mysteries. This has a few implications, one of which is that this theme speci\ufb01c infor- mation is unlikely to have been seen by OpenIE5 before. This is indicated in the relatively improved performance of the rules-based model in this genre across in terms of both in- terestingness and coherence.The genre difference can also be observed in terms of the performance of the random model. This model also lacking when compared to our neural model across all the questions asked especially in the fairy-tale set- ting. This appears to imply that \ufb01lling in gaps in the knowl- edge graph using thematically relevant information such as with AskBERT results in more interesting and coherent de- scriptions and games especially in settings where the thematic commonsense diverges from everyday commonsense.",
      "This appears to imply that \ufb01lling in gaps in the knowl- edge graph using thematically relevant information such as with AskBERT results in more interesting and coherent de- scriptions and games especially in settings where the thematic commonsense diverges from everyday commonsense. 5 Conclusion Procedural world generation systems are required to be se- mantically consistent, comply with thematic and everyday commonsense understanding, and maintain overall interest- ingness. We describe an approach that transform a linear reading experience in the form of a story plot into a inter- active narrative experience. Our method, AskBERT, extracts and \ufb01lls in a knowledge graph using thematic commonsense and then uses it as a skeleton to \ufb02esh out the rest of the world. A key insight from our human participant study reveals that the ability to construct a thematically consistent knowledge graph is critical to overall perceptions of coherence and inter- estingness particularly when the theme diverges from every- day commonsense understanding.",
      "References [Ammanabrolu and Hausknecht, 2020] Prithviraj Am- manabrolu and Matthew Hausknecht. Graph constrained reinforcement learning for natural language action spaces. In International Conference on Learning Representations, 2020. [Ammanabrolu and Riedl, 2019a] Prithviraj Ammanabrolu and Mark Riedl. Transfer in deep reinforcement learning using knowledge graphs. In TextGraphs-13 at EMNLP-19, pages 1\u201310, Hong Kong, November 2019. Association for Computational Linguistics. [Ammanabrolu and Riedl, 2019b] Prithviraj Ammanabrolu and Mark O. Riedl. Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of 2019 Annual Conference of the North American Chap- ter of the Association for Computational Linguistics: Hu- man Language Technologies, NAACL-HLT 2019, 2019.",
      "Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of 2019 Annual Conference of the North American Chap- ter of the Association for Computational Linguistics: Hu- man Language Technologies, NAACL-HLT 2019, 2019. [Ammanabrolu et al., 2019] Prithviraj Ammanabrolu, William Broniec, Alex Mueller, Jeremy Paul, and Mark O Riedl. Toward automated quest generation in text-adventure games. Proceedings of the 4th Work- shop on Computational Creativity in Natural Language Generation in INLG-19, 2019. [Christensen et al., 2011] Janara Christensen, Stephen Soderland, Oren Etzioni, and Mausam. An analysis of open information extraction based on semantic role label- ing. In Proceedings of the sixth international conference on Knowledge capture, pages 113\u2013120. ACM, 2011.",
      "An analysis of open information extraction based on semantic role label- ing. In Proceedings of the sixth international conference on Knowledge capture, pages 113\u2013120. ACM, 2011. [C\u02c6ot\u00b4e et al., 2018] Marc-Alexandre C\u02c6ot\u00b4e, \u00b4Akos K\u00b4ad\u00b4ar, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mah- moud Adada, et al. Textworld: A learning environment for text-based games. arXiv preprint arXiv:1806.11532, 2018. [Fan et al., 2019] Angela Fan, Jack Urbanek, Pratik Ring- shia, Emily Dinan, Emma Qian, Siddharth Karamcheti, Shrimai Prabhumoye, Douwe Kiela, Tim Rocktaschel, Arthur Szlam, et al. Generating interactive worlds with text. arXiv preprint arXiv:1911.09194, 2019.",
      "Generating interactive worlds with text. arXiv preprint arXiv:1911.09194, 2019. [Guzdial et al., 2015] Matthew Guzdial, Brent Harrison, Boyang Li, and Mark Riedl. Crowdsourcing open inter- active narrative. In FDG, 2015. [Haroush et al., 2018] Matan Haroush, Tom Zahavy, Daniel J Mankowitz, and Shie Mannor. Learning How Not to Act in Text-Based Games. In Workshop Track at ICLR 2018, pages 1\u20134, 2018. [Hausknecht et al., 2019a] Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre C\u02c6ot\u00b4e, and Xingdi Yuan. Interactive \ufb01ction games: A colossal adventure. CoRR, abs/1909.05398, 2019. [Hausknecht et al., 2019b] Matthew Hausknecht, Ricky Loynd, Greg Yang, Adith Swaminathan, and Jason D. Williams.",
      "CoRR, abs/1909.05398, 2019. [Hausknecht et al., 2019b] Matthew Hausknecht, Ricky Loynd, Greg Yang, Adith Swaminathan, and Jason D. Williams. Nail: A general interactive \ufb01ction agent. CoRR, abs/1902.04259, 2019. [Lan et al., 2019] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori- cut. ALBERT: A Lite BERT for Self-supervised Learn- ing of Language Representations. arXiv e-prints, page arXiv:1909.11942, Sep 2019. [Li et al., 2012] Boyang Li, Stephen Lee-Urban, Dar- ren Scott Appling, and Mark O. Riedl. Crowdsourcing Narrative Intelligence . In Advances in Cognitive Systems, volume 1, pages 1\u201318, 2012.",
      "[Li et al., 2012] Boyang Li, Stephen Lee-Urban, Dar- ren Scott Appling, and Mark O. Riedl. Crowdsourcing Narrative Intelligence . In Advances in Cognitive Systems, volume 1, pages 1\u201318, 2012. [Narasimhan et al., 2015] Karthik Narasimhan, Tejas Kulka- rni, and Regina Barzilay. Language Understanding for Text-based Games Using Deep Reinforcement Learning. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2015. [Pal and Mausam, 2016] Harinder Pal and Mausam. De- monyms and compound relational nouns in nominal open ie. In Proceedings of the 5th Workshop on Automated Knowledge Base Construction, pages 35\u201339, 2016. [Radford et al., 2019] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.",
      "[Radford et al., 2019] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. [Ribeiro et al., 2019] Marco Tulio Ribeiro, Carlos Guestrin, and Sameer Singh. Are red roses red? evaluating consis- tency of question-answering models. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6174\u20136184, 2019. [Saha and Mausam, 2018] Swarnadeep Saha and Mausam. Open information extraction from conjunctive sentences. In Proceedings of the 27th International Conference on Computational Linguistics, pages 2288\u20132299, 2018. [Shirish Keskar et al., 2019] Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher. CTRL: A Conditional Transformer Language Model for Controllable Generation.",
      "[Shirish Keskar et al., 2019] Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher. CTRL: A Conditional Transformer Language Model for Controllable Generation. arXiv e-prints, page arXiv:1909.05858, Sep 2019. [Urbanek et al., 2019] Jack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim Rockt\u00a8aschel, Douwe Kiela, Arthur Szlam, and Jason Weston. Learning to speak and act in a fantasy text ad- venture game. In Proceedings of EMNLP-IJCNLP, pages 673\u2013683, 2019. [Womack and Freeman, 2019] Jon Womack and William Freeman. Interactive narrative generation using location and genre speci\ufb01c context. In Interactive Storytelling, pages 343\u2013347, Cham, 2019. [Yin and May, 2019] Xusen Yin and Jonathan May.",
      "Interactive narrative generation using location and genre speci\ufb01c context. In Interactive Storytelling, pages 343\u2013347, Cham, 2019. [Yin and May, 2019] Xusen Yin and Jonathan May. Com- prehensible context-driven text game playing. CoRR, abs/1905.02265, 2019. [Yuan et al., 2019] Xingdi Yuan, Marc-Alexandre C\u02c6ot\u00b4e, Jie Fu, Zhouhan Lin, Christopher Pal, Yoshua Bengio, and Adam Trischler. Interactive language learning by ques- tion answering. In Conference on Empirical Methods in Natural Language Processing, 2019."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-2001.10161.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":8922,
  "avg_doclen":185.875,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-2001.10161.pdf"
    }
  }
}