{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":4,
    "kmeans_niters":20,
    "resume":false,
    "pool_factor":1,
    "clustering_mode":"hierarchical",
    "protected_tokens":0,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":256,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "arXiv:1902.09666v2  [cs.CL]  16 Apr 2019 Predicting the Type and Target of Offensive Posts in Social Media Marcos Zampieri1, Shervin Malmasi2, Preslav Nakov3, Sara Rosenthal4 Noura Farra5, Ritesh Kumar6 1University of Wolverhampton, UK, 2Harvard Medical School, USA 3Qatar Computing Research Institute, HBKU, Qatar, 4IBM Research, USA 5Columbia University, USA, 6Bhim Rao Ambedkar University, India m.zampieri@wlv.ac.uk Abstract As offensive content has become pervasive in social media, there has been much research in identifying potentially offensive messages. However, previous work on this topic did not consider the problem as a whole, but rather fo- cused on detecting very speci\ufb01c types of offen- sive content, e.g., hate speech, cyberbulling, or cyber-aggression. In contrast, here we tar- get several different kinds of offensive content. In particular, we model the task hierarchically, identifying the type and the target of offensive messages in social media.",
      "In contrast, here we tar- get several different kinds of offensive content. In particular, we model the task hierarchically, identifying the type and the target of offensive messages in social media. For this purpose, we complied the Offensive Language Identi- \ufb01cation Dataset (OLID), a new dataset with tweets annotated for offensive content using a \ufb01ne-grained three-layer annotation scheme, which we make publicly available. We discuss the main similarities and differences between OLID and pre-existing datasets for hate speech identi\ufb01cation, aggression detection, and simi- lar tasks. We further experiment with and we compare the performance of different machine learning models on OLID. 1 Introduction Offensive content has become pervasive in social media and thus a serious concern for government organizations, online communities, and social media platforms. One of the most common strategies to tackle the problem is to train systems capable of recognizing offensive content, which can then be deleted or set aside for human moderation. In the last few years, there have been several studies on the application of computational methods to deal with this problem.",
      "One of the most common strategies to tackle the problem is to train systems capable of recognizing offensive content, which can then be deleted or set aside for human moderation. In the last few years, there have been several studies on the application of computational methods to deal with this problem. Prior work has studied offensive language in Twitter (Xu et al., 2012; Burnap and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments,1 and Facebook posts (Kumar et al., 2018).",
      "Prior work has studied offensive language in Twitter (Xu et al., 2012; Burnap and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments,1 and Facebook posts (Kumar et al., 2018). 1http://bit.ly/2FhLMVz Previous studies have looked into different aspects of offensive language such as the use of abusive language (Nobata et al., 2016; Mubarak et al., 2017), (cyber-)aggression (Kumar et al., 2018), (cyber-)bullying (Xu et al., 2012; Dadvar et al., 2013), toxic comments1, hate speech (Kwok and Wang, 2013; Djuric et al., 2015; Burnap and Williams, 2015; Davidson et al., 2017; Malmasi and Zampieri, 2017, 2018), and offensive language (Wiegand et al., 2018). Recently, Waseem et al.",
      "Recently, Waseem et al. (2017) analyzed the similarities between different approaches pro- posed in previous work and argued that there was a need for a typology that differentiates between whether the (abusive) language is directed towards a speci\ufb01c individual or entity, or towards a gener- alized group, and whether the abusive content is explicit or implicit. Wiegand et al. (2018) further applied this idea to German tweets. They experi- mented with a task on detecting offensive vs. non- offensive tweets, and also with a second task on further sub-classifying the offensive tweets as pro- fanity, insult, or abuse. However, to the best of our knowledge, no prior work has explored the target of the offensive language, which might be impor- tant in many scenarios, e.g., when studying hate speech with respect to a speci\ufb01c target. Below, we aim at bridging this gap.",
      "However, to the best of our knowledge, no prior work has explored the target of the offensive language, which might be impor- tant in many scenarios, e.g., when studying hate speech with respect to a speci\ufb01c target. Below, we aim at bridging this gap. More generally, in this paper, we expand on the above ideas by proposing a novel three-level hier- archical annotation schema that encompasses the following three general categories: A: Offensive Language Detection B: Categorization of Offensive Language C: Offensive Language Target Identi\ufb01cation",
      "Tweet A B C @USER He is so generous with his offers. NOT \u2014 \u2014 IM FREEEEE!!!! WORST EXPERIENCE OF MY FUCKING LIFE OFF UNT \u2014 @USER Fuk this fat cock sucker OFF TIN IND @USER Figures! What is wrong with these idiots? Thank God for @USER OFF TIN GRP Table 1: Four tweets from the dataset, with their labels for each level of the annotation schema. We further use the above schema to annotate a large dataset of English tweets, which we make publicly available online.2 The key contributions of this paper can be sum- marized as follows: \u2022 We propose a new three-level hierarchical an- notation schema for abusive language detec- tion and characterization. \u2022 We apply the schema to create Offensive Lan- guage Identi\ufb01cation Dataset (OLID), a new large-scale dataset of English tweets with high-quality annotation of the target and type of offenses. \u2022 We perform experiments on OLID using different machine learning models for each level of the annotation, thus setting important baselines to compare to in future work.",
      "\u2022 We perform experiments on OLID using different machine learning models for each level of the annotation, thus setting important baselines to compare to in future work. While each of these sub-tasks tackles a particular type of abuse or offense, they share similar prop- erties and the hierarchical annotation model pro- posed in this paper aims to capture this. Consid- ering that, for example, an insult targeted at an in- dividual is commonly known as cyberbulling and that insults targeted at a group are known as hate speech, we believe that OLID\u2019s use of a hierarchi- cal annotation schema makes it a useful resource for various offensive language identi\ufb01cation and characterization tasks. 2 Hierarchically Modelling Offensive Content In the OLID dataset, we use a hierarchical anno- tation schema split into three levels to distinguish between whether the language is offensive or not (A), its type (B), and its target (C). Each level is described in more detail in the following subsec- tions and examples are shown in Table 1.",
      "Each level is described in more detail in the following subsec- tions and examples are shown in Table 1. 2The data can be downloaded from the following address: http://scholar.harvard.edu/malmasi/olid 2.1 Level A: Offensive language Detection Level A discriminates between the following types of tweets: \u2022 Not Offensive (NOT): Posts that do not con- tain offense or profanity; \u2022 Offensive (OFF): Posts containing any form of non-acceptable language (profanity) or a targeted offense, which can be veiled or di- rect. This includes insults, threats, and posts containing profane language or swear words. 2.2 Level B: Categorization of Offensive Language Level B categorizes the type of offense: \u2022 Targeted Insult (TIN): Posts containing in- sult/threat to an individual, a group, or others; \u2022 Untargeted (UNT): Posts containing non- targeted profanity and swearing. Posts with general profanity are not targeted, but they contain non-acceptable language.",
      "Posts with general profanity are not targeted, but they contain non-acceptable language. 2.3 Level C: Offensive Language Target Identi\ufb01cation Level C categorizes the targets of insults/threats: \u2022 Individual (IND): Posts targeting an individ- ual. This can be a famous person, a named individual or an unnamed participant in the conversation. Insults and threats targeted at individuals are often de\ufb01ned as cyberbulling. \u2022 Group (GRP): Posts targeting a group of people considered as a unity due to the same ethnicity, gender or sexual orientation, polit- ical af\ufb01liation, religious belief, or other com- mon characteristic. Many of the insults and threats targeted at a group correspond to what is commonly understood as hate speech. \u2022 Other (OTH) The target of these offensive posts does not belong to any of the previous two categories (e.g., an organization, a situa- tion, an event, or an issue).",
      "Keyword Offensive % medical marijuana 0.0 they are 5.9 to:NewYorker 8.3 you are 21.0 she is 26.6 to:BreitBartNews 31.6 he is 32.4 gun control 34.7 -\ufb01lter:safe 58.9 conservatives 23.2 antifa 26.7 MAGA 27.7 liberals 38.0 Table 2: The keywords from the full dataset (except for the \ufb01rst three rows) and the percentage of offensive tweets for each keyword. 3 Data Collection We retrieved the examples in OLID from Twitter using its API and searching for keywords and con- structions that are often included in offensive mes- sages, such as \u2018she is\u2019 or \u2018to:BreitBartNews\u20193. The full list of keywords we used is shown in Table 2. We \ufb01rst carried out a round of trial annotation of 300 instances with six experts using nine key- words.",
      "The full list of keywords we used is shown in Table 2. We \ufb01rst carried out a round of trial annotation of 300 instances with six experts using nine key- words. The goal of the trial annotation was (i) to evaluate the proposed tagset, (ii) to evaluate the data retrieval method, and (iii) to create a gold standard with instances that could be used as test questions to ensure the quality of the annotators for the rest of the data, which was carried out us- ing crowdsourcing. The keywords used in the trial annotation are shown in the \ufb01rst nine rows of Ta- ble 2. We included left (@NewYorker) and far- right (@BreitBartNews) news accounts because there tends to be political offense in the comments for such accounts. The keyword that resulted in the highest concentration of offensive content was the Twitter \u2018safe\u2019 \ufb01lter, corresponding to tweets that were \ufb02agged as unsafe by Twitter (the \u2018-\u2019 sym- biol indicates \u2018not safe\u2019). Since the vast majority of content on Twitter is not offensive, we tried different strategies to keep the distribution of offensive tweets at around 30% of the dataset.",
      "Since the vast majority of content on Twitter is not offensive, we tried different strategies to keep the distribution of offensive tweets at around 30% of the dataset. We excluded some keywords that were not high in offensive content during the trial annotation such as \u2018they are\u2019 and \u2018to:NewYorker\u2019. 3to is a special Twitter API word indicating that the tweet was directed at a speci\ufb01c account (e.g., BreitBartNews). Although \u2018he is\u2019 was poor in offensive content in the trial dataset (15%), we kept it as a keyword in order to avoid gender bias, and we found that in the full dataset it was more offensive (32.4%). The trial keywords that we ultimately decided to exclude due to low percentage of offensive tweets are shown in the top portion of Table 2. We computed Fleiss\u2019 kappa on the trial dataset for the \ufb01ve annotators on 21 of the tweets. The value was .83 for Layer A (OFF vs. NOT) indi- cating high agreement. As to normalization and anonymization, we did not store any user meta- data or Twitter IDs, and we substituted the URLs and the Twitter mentions by placeholders.",
      "The value was .83 for Layer A (OFF vs. NOT) indi- cating high agreement. As to normalization and anonymization, we did not store any user meta- data or Twitter IDs, and we substituted the URLs and the Twitter mentions by placeholders. During the full annotation task, we decided to search for more political keywords as they tend to be richer in offensive content. Thus, we sam- pled our full dataset, so that 50% of the tweets come from political keywords, and the other 50% come from non-political keywords. Within these two groups, tweets were evenly sampled for the keywords. In addition to \u2018gun control\u2019, and \u2018to:BreitbartNews\u2019 used during the trial annota- tion, four new political keywords were used to col- lect tweets for the full dataset: \u2018MAGA\u2019, \u2018antifa\u2019, \u2018conservatives\u2019, and \u2018liberals\u2019. The breakdown of keywords and their offensive content in the full dataset is shown in the bottom of Table 2. We follow prior work in related ar- eas (Burnap and Williams, 2015; Davidson et al., 2017) and we annotate our data using crowdsourc- ing.",
      "The breakdown of keywords and their offensive content in the full dataset is shown in the bottom of Table 2. We follow prior work in related ar- eas (Burnap and Williams, 2015; Davidson et al., 2017) and we annotate our data using crowdsourc- ing. We used Figure Eight4 and we ensured data quality by (i) only hiring annotators who were experienced in the platform, and (ii) using test questions to discard annotations by individuals who did not reach a certain threshold. Each instance in the dataset was annotated by multiple annotators and inter-annotator agreement was calculated at the end. We \ufb01rst acquired two annotations for each in- stance. In the case of disagreement, we requested a third annotation, and we then took a majority vote. The annotators were asked to label each tweet at all three levels of the annotation scheme, and we considered there to be agreement only when the annotators agreed on the labels for all levels. Approximately 60% of the time, the two annotators agreed, and thus no additional annota- tion was needed.",
      "Approximately 60% of the time, the two annotators agreed, and thus no additional annota- tion was needed. A third annotation was requested for the rest of the tweets; there was no instance 4http://www.figure-eight.com",
      "A B C Training Test Total OFF TIN IND 2,407 100 2,507 OFF TIN OTH 395 35 430 OFF TIN GRP 1,074 78 1,152 OFF UNT \u2014 524 27 551 NOT \u2014 \u2014 8,840 620 9,460 All 13,240 860 14,100 Table 3: Distribution of label combinations in OLID. when more than three annotations were needed. The breakdown of the data into training and test- ing for the labels from each level is shown in Ta- ble 3. It is worth noting that one of the key chal- lenges we observed when collecting for OLID was producing a dataset containing a suf\ufb01cient num- ber of instances for each class. This is particu- larly evident in the sizes for Subtasks B and C. Other studies also had this issue when collecting similar datasets.",
      "This is particu- larly evident in the sizes for Subtasks B and C. Other studies also had this issue when collecting similar datasets. For example, in (Davidson et al., 2017), only 5% of the tweets were considered hate speech by the majority of the annotators, and in (Burnap and Williams, 2015) only 11.6% of the examples were labeled as hate speech. 4 Experiments and Evaluation We experiment with various models: SVM Our simplest machine learning model is a linear SVM trained on word unigrams. SVMs have achieved state-of-the-art results for many text classi\ufb01cation tasks (Zampieri et al., 2018). BiLSTM We also experiment with a bidi- rectional Long Short-Term-Memory (BiLSTM) model, which we adapted from a pre-existing model for sentiment analysis (Rasooli et al., 2018). The model consists of (i) an input em- bedding layer, (ii) a bidirectional LSTM layer, and (iii) an average pooling layer of input fea- tures.",
      "The model consists of (i) an input em- bedding layer, (ii) a bidirectional LSTM layer, and (iii) an average pooling layer of input fea- tures. The concatenation of the LSTM layer and the average pooling layer is further passed through a dense layer, whose output is ultimately passed through a softmax to produce the \ufb01nal predic- tion. We set two input channels for the input em- bedding layers: pre-trained FastText embeddings (Bojanowski et al., 2017), as well as updatable embeddings learned by the model during training. CNN Finally, we experiment with a Convolu- tional Neural Network (CNN) model based on the architecture of (Kim, 2014), and using the same multi-channel inputs as the above BiLSTM. Our models are trained on the training dataset, and evaluated by predicting the labels for the held-out test set. As the label distribution is highly im- balanced (see Table 3), we evaluate and we com- pare the performance of the different models us- ing macro-averaged F1-score.",
      "Our models are trained on the training dataset, and evaluated by predicting the labels for the held-out test set. As the label distribution is highly im- balanced (see Table 3), we evaluate and we com- pare the performance of the different models us- ing macro-averaged F1-score. We further report per-class Precision (P), Recall (R), and F1-score (F1), and weighted average. Finally, we compare the performance of the models against simple ma- jority and minority class baselines. 4.1 Offensive Language Detection The performance on discriminating between of- fensive (OFF) and non-offensive (NOT) posts is reported in Table 4. We can see that all models perform signi\ufb01cantly better than chance, with the neural models performing substantially better than the SVM. The CNN outperforms the RNN model, achieving a macro-F1 score of 0.80. 4.2 Categorization of Offensive Language In this set of experiments, the models were trained to discriminate between targeted insults and threats (TIN) and untargeted (UNT) offenses, which generally refer to profanity. The results are shown in Table 5.",
      "4.2 Categorization of Offensive Language In this set of experiments, the models were trained to discriminate between targeted insults and threats (TIN) and untargeted (UNT) offenses, which generally refer to profanity. The results are shown in Table 5. We can see that the CNN per- forms better than the BiLSTM, with a macro-F1 score of 0.69. Note that all models perform better at identifying TIN compared to UNT. 4.3 Offensive Language Target Identi\ufb01cation The results for the offensive target identi\ufb01cation experiment are shown in Table 6. Here the models were trained to distinguish between three targets: a group (GRP), an individual (IND), or others (OTH). We can see that all three models achieved similar results, far surpassing the random base- lines, with a slight performance edge for the neural models. The performance of all models for the OTH class is 0, which can be explained by two factors. First, unlike the two other classes, OTH is a het- erogeneous collection of targets.",
      "The performance of all models for the OTH class is 0, which can be explained by two factors. First, unlike the two other classes, OTH is a het- erogeneous collection of targets. It includes of- fensive tweets targeted at organizations, situations, events, etc., thus making it more challenging for models to learn discriminative properties for this class. Second, there are fewer training instances for this class compared to the other two: there are only 395 instances for OTH vs. 1,075 for GRP and 2,407 for IND.",
      "NOT OFF Weighted Average Model P R F1 P R F1 P R F1 F1 Macro SVM 0.80 0.92 0.86 0.66 0.43 0.52 0.76 0.78 0.76 0.69 BiLSTM 0.83 0.95 0.89 0.81 0.48 0.60 0.82 0.82 0.81 0.75 CNN 0.87 0.93 0.90 0.78 0.63 0.70 0.82 0.82 0.81 0.80 All NOT - 0.00 0.00 0.72 1.00 0.84 0.52 0.72 0. 0.42 All OFF 0.28 1.00 0.44 - 0.00 0.00 0.08 0.28 0.12 0.22 Table 4: Results for offensive language detection (Level A).",
      "0.42 All OFF 0.28 1.00 0.44 - 0.00 0.00 0.08 0.28 0.12 0.22 Table 4: Results for offensive language detection (Level A). We report Precision (P), Recall (R), and F1 for each model/baseline on all classes (NOT, OFF), and weighted averages. Macro-F1 is also listed (best in bold).",
      "We report Precision (P), Recall (R), and F1 for each model/baseline on all classes (NOT, OFF), and weighted averages. Macro-F1 is also listed (best in bold). TIN UNT Weighted Average Model P R F1 P R F1 P R F1 F1 Macro SVM 0.91 0.99 0.95 0.67 0.22 0.33 0.88 0.90 0.88 0.64 BiLSTM 0.95 0.83 0.88 0.32 0.63 0.42 0.88 0.81 0.83 0.66 CNN 0.94 0.90 0.92 0.32 0.63 0.42 0.88 0.86 0.87 0.69 All TIN 0.89 1.00 0.94 - 0.00 0.00 0.79 0.89 0.83 0.47 All UNT - 0.00 0.00 0.11 1.00 0.20 0.01 0.11 0.02 0.10 Table 5: Results for offensive language categorization (level B).",
      "We report Precision (P), Recall (R), and F1 for each model/baseline on all classes (TIN, UNT), and weighted averages. Macro-F1 is also listed (best in bold). GRP IND OTH Weighted Average Model P R F1 P R F1 P R F1 P R F1 F1 Macro SVM 0.66 0.50 0.57 0.61 0.92 0.73 0.33 0.03 0.05 0.58 0.62 0.56 0.45 BiLSTM 0.62 0.69 0.65 0.68 0.86 0.76 0.00 0.00 0.00 0.55 0.66 0.60 0.47 CNN 0.75 0.60 0.67 0.63 0.94 0.75 0.00 0.00 0.00 0.57 0.66 0.60 0.47 All GRP 0.37 1.00 0.54 - 0.00 0.",
      "67 0.63 0.94 0.75 0.00 0.00 0.00 0.57 0.66 0.60 0.47 All GRP 0.37 1.00 0.54 - 0.00 0.00 - 0.00 0.00 0.13 0.37 0.20 0.18 All IND - 0.00 0.00 0.47 1.00 0.64 - 0.00 0.00 0.22 0.47 0.30 0.21 All OTH - 0.00 0.00 - 0.00 0.00 0.16 1.00 0.28 0.03 0.16 0.05 0.09 Table 6: Results for offense target identi\ufb01cation (level C). We report Precision (P), Recall (R), and F1 for each model/baseline on all classes (GRP, IND, OTH), and weighted averages. Macro-F1 is also listed (best in bold).",
      "We report Precision (P), Recall (R), and F1 for each model/baseline on all classes (GRP, IND, OTH), and weighted averages. Macro-F1 is also listed (best in bold). 5 Conclusion and Future Work We presented OLID, a new dataset with annota- tion of type and target of offensive language. It is the of\ufb01cial dataset of the shared task SemEval 2019 Task 6: Identifying and Categorizing Of- fensive Language in Social Media (OffensEval) (Zampieri et al., 2019).5 In OffensEval, each an- notation level in OLID is an independent sub-task. To the best of our knowledge, this is the \ufb01rst dataset to contain annotation of type and target of offenses in social media, and it opens interesting research directions. We further presented baseline experiments us- ing SVMs and neural networks, which have shown 5http://competitions.codalab.org/competitions/20011 that this is a challenging, yet doable task. In future work, we would like to make a cross- corpus comparison of OLID vs.",
      "In future work, we would like to make a cross- corpus comparison of OLID vs. datasets anno- tated for similar tasks such as aggression identi- \ufb01cation (Kumar et al., 2018) and hate speech de- tection (Davidson et al., 2017). We further plan to create similar datasets for other languages, follow- ing OLID\u2019s hierarchical annotation scheme. Acknowledgments We would like to thank the anonymous NAACL reviewers for their valuable suggestions and Nikola Ljube\u02c7si\u00b4c for the feedback provided. This research presented was partially sup- ported by an ERAS fellowship awarded to Marcos Zampieri by the University of Wolverhampton.",
      "References Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching Word Vectors with Subword Information. Transactions of the Associa- tion for Computational Linguistics, 5:135\u2013146. Pete Burnap and Matthew L Williams. 2015. Cyber hate speech on twitter: An application of machine classi\ufb01cation and statistical modeling for policy and decision making. Policy & Internet, 7(2):223\u2013242. Maral Dadvar, Dolf Trieschnigg, Roeland Ordelman, and Franciska de Jong. 2013. Improving Cyberbul- lying Detection with User Context. In Advances in Information Retrieval, pages 693\u2013696. Springer. Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated Hate Speech Detection and the Problem of Offensive Language. In Proceedings of ICWSM.",
      "In Advances in Information Retrieval, pages 693\u2013696. Springer. Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated Hate Speech Detection and the Problem of Offensive Language. In Proceedings of ICWSM. Nemanja Djuric, Jing Zhou, Robin Morris, Mihajlo Gr- bovic, Vladan Radosavljevic, and Narayan Bhamidi- pati. 2015. Hate Speech Detection with Comment Embeddings. In Proceedings of WWW. Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classi\ufb01cation. Proceedings of EMNLP. Ritesh Kumar, Atul Kr Ojha, Shervin Malmasi, and Marcos Zampieri. 2018. Benchmarking Aggression Identi\ufb01cation in Social Media. In Proceedings of TRAC. Irene Kwok and Yuzhou Wang. 2013. Locate the Hate: Detecting Tweets Against Blacks. In Proceedings of AAAI. Shervin Malmasi and Marcos Zampieri. 2017.",
      "In Proceedings of TRAC. Irene Kwok and Yuzhou Wang. 2013. Locate the Hate: Detecting Tweets Against Blacks. In Proceedings of AAAI. Shervin Malmasi and Marcos Zampieri. 2017. Detect- ing Hate Speech in Social Media. In Proceedings of RANLP. Shervin Malmasi and Marcos Zampieri. 2018. Chal- lenges in Discriminating Profanity from Hate Speech. Journal of Experimental & Theoretical Ar- ti\ufb01cial Intelligence, 30:1 \u2013 16. Hamdy Mubarak, Darwish Kareem, and Magdy Walid. 2017. Abusive Language Detection on Arabic So- cial Media. In Proceedings of ALW. Chikashi Nobata, Joel Tetreault, Achint Thomas, Yashar Mehdad, and Yi Chang. 2016. Abusive Lan- guage Detection in Online User Content. In Pro- ceedings of WWW.",
      "In Proceedings of ALW. Chikashi Nobata, Joel Tetreault, Achint Thomas, Yashar Mehdad, and Yi Chang. 2016. Abusive Lan- guage Detection in Online User Content. In Pro- ceedings of WWW. Mohammad Sadegh Rasooli, Noura Farra, Axinia Radeva, Tao Yu, and Kathleen McKeown. 2018. Cross-lingual Sentiment Transfer with Limited Re- sources. Machine Translation, 32(1-2):143\u2013165. Zeerak Waseem, Thomas Davidson, Dana Warmsley, and Ingmar Weber. 2017. Understanding Abuse: A Typology of Abusive Language Detection Subtasks. Proceedings of ALW. Michael Wiegand, Melanie Siegel, and Josef Rup- penhofer. 2018. Overview of the GermEval 2018 Shared Task on the Identi\ufb01cation of Offensive Lan- guage. In Proceedings of GermEval. Jun-Ming Xu, Kwang-Sung Jun, Xiaojin Zhu, and Amy Bellmore. 2012.",
      "2018. Overview of the GermEval 2018 Shared Task on the Identi\ufb01cation of Offensive Lan- guage. In Proceedings of GermEval. Jun-Ming Xu, Kwang-Sung Jun, Xiaojin Zhu, and Amy Bellmore. 2012. Learning from Bullying Traces in Social Media. In Proceedings of NAACL. Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Ahmed Ali, Suwon Shon, James Glass, Yves Scher- rer, Tanja Samard\u02c7zi\u00b4c, Nikola Ljube\u02c7si\u00b4c, J\u00a8org Tiede- mann, et al. 2018. Language Identi\ufb01cation and Mor- phosyntactic Tagging: The Second VarDial Evalua- tion Campaign. In Proceedings of VarDial. Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. 2019.",
      "In Proceedings of VarDial. Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. 2019. SemEval-2019 Task 6: Identifying and Cat- egorizing Offensive Language in Social Media (Of- fensEval). In Proceedings of SemEval."
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"PaperTab-question-1902.09666.pdf",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2025-05/17/10.22.50",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":1,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":5804,
  "avg_doclen":165.8285714286,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"PaperTab-question-1902.09666.pdf"
    }
  }
}