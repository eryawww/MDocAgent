{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('results/PaperTab/rag1/2025-05-19-08-41_results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== User Prompt: \n",
      "You are tasked with summarizing and evaluating the collective responses provided by multiple agents. You have access to the following information:\n",
      "Answers: The individual answers from all agents.\n",
      "Using this information, perform the following tasks:\n",
      "Analyze: Evaluate the quality, consistency, and relevance of each answer. Identify commonalities, discrepancies, or gaps in reasoning.\n",
      "Synthesize: Summarize the most accurate and reliable information based on the evidence provided by the agents and their discussions.\n",
      "Conclude: Provide a final, well-reasoned answer to the question or task. Your conclusion should reflect the consensus (if one exists) or the most credible and well-supported answer.\n",
      "Based on the provided answers from all agents, summarize the final decision clearly. You should only return the final answer in this dictionary format: {\"Answer\": <Your final answer here>}. Don't give other information.\n",
      "==================== General Agent: \n",
      "The datasets used in the experiments are ShapeWorldICE datasets. These datasets are developed to examine different aspects of reasoning abilities in image captioning tasks. The datasets are designed with a similar idea to the \"skill tasks\" in the bAbI framework.\n",
      "==================== Text Agent: \n",
      "To answer the question, I will extract key details related to the datasets used in the text.\n",
      "\n",
      "According to the text, the ShapeWorldICE datasets are used, which have a similar idea to the \"skill tasks\" in the bAbI framework. The text mentions three different types of captioning tasks, each focusing on a distinct aspect of reasoning abilities:\n",
      "\n",
      "1. Existential descriptions\n",
      "2. Spatial descriptions\n",
      "3. Quantiﬁcation descriptions\n",
      "\n",
      "Each type of dataset has two variants, enabling different levels of visual complexity or specific aspects of the same reasoning type. The datasets consist of:\n",
      "\n",
      "* Around 200k training instances\n",
      "* 4,096 validation instances\n",
      "* 4,096 test instances\n",
      "\n",
      "Each training instance consists of an image and a reference caption. At test time, only the test images are available to the evaluated models.\n",
      "==================== Image Agent: \n",
      "The datasets used are the ShapeWorldICE datasets, with a similar idea to the \"skill tasks\" in the bAbI framework. The specific datasets mentioned are Existential-MultiShapes.\n",
      "==================== Response Chat: \n",
      "{\"Answer\": \"The ShapeWorldICE datasets are used in the experiments, which are designed to examine different aspects of reasoning abilities in image captioning tasks. The datasets have a similar idea to the \"skill tasks\" in the bAbI framework and consist of three types of captioning tasks: Existential descriptions, Spatial descriptions, and Quantification descriptions. Each type of dataset has two variants, enabling different levels of visual complexity or specific aspects of the same reasoning type. The datasets consist of around 200k training instances, 4,096 validation instances, and 4,096 test instances. Each training instance consists of an image and a reference caption, and at test time, only the test images are available to the evaluated models.\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    user_chat: str\n",
    "    response_chat: str\n",
    "\n",
    "    user_prompt: str\n",
    "    general_agent: str\n",
    "    text_agent: str\n",
    "    image_agent: str\n",
    "\n",
    "    @staticmethod\n",
    "    def from_json(user_ai_interaction: dict):\n",
    "        user_chat = user_ai_interaction[0]['content'][0]['text']\n",
    "        response_chat = user_ai_interaction[1]['content'][0]['text']\n",
    "        \n",
    "        # Extract agent responses\n",
    "        user_prompt = \"\"\n",
    "        general_agent = \"\"\n",
    "        text_agent = \"\"\n",
    "        image_agent = \"\"\n",
    "        \n",
    "        if \"General Agent:\" in user_chat:\n",
    "            parts = user_chat.split(\"General Agent:\")\n",
    "            if len(parts) > 1:\n",
    "                user_prompt = parts[0].strip()\n",
    "                general_part = parts[1].split(\"Text Agent:\")[0].strip()\n",
    "                general_agent = general_part\n",
    "                \n",
    "        if \"Text Agent:\" in user_chat:\n",
    "            parts = user_chat.split(\"Text Agent:\")\n",
    "            if len(parts) > 1:\n",
    "                text_part = parts[1].split(\"Image Agent:\")[0].strip()\n",
    "                text_agent = text_part\n",
    "                \n",
    "        if \"Image Agent:\" in user_chat:\n",
    "            parts = user_chat.split(\"Image Agent:\")\n",
    "            if len(parts) > 1:\n",
    "                image_part = parts[1].strip()\n",
    "                image_agent = image_part\n",
    "        \n",
    "        return ExperimentResult(\n",
    "            user_chat=user_chat,\n",
    "            user_prompt=user_prompt,\n",
    "            general_agent=general_agent,\n",
    "            text_agent=text_agent,\n",
    "            image_agent=image_agent,\n",
    "            response_chat=response_chat\n",
    "        )\n",
    "\n",
    "    def __str__(self):\n",
    "        return \\\n",
    "            f\"{'='*20} User Prompt: \\n{self.user_prompt}\\n\" \\\n",
    "            f\"{'='*20} General Agent: \\n{self.general_agent}\\n\" \\\n",
    "            f\"{'='*20} Text Agent: \\n{self.text_agent}\\n\" \\\n",
    "            f\"{'='*20} Image Agent: \\n{self.image_agent}\\n\" \\\n",
    "            f\"{'='*20} Response Chat: \\n{self.response_chat}\"\n",
    "\n",
    "sample_result = ExperimentResult.from_json(df['ans_2_message'].iloc[18])\n",
    "print(sample_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== User Prompt: \n",
      "You are tasked with summarizing and evaluating the collective responses provided by multiple agents. You have access to the following information:\n",
      "Answers: The individual answers from all agents.\n",
      "Using this information, perform the following tasks:\n",
      "Analyze: Evaluate the quality, consistency, and relevance of each answer. Identify commonalities, discrepancies, or gaps in reasoning.\n",
      "Synthesize: Summarize the most accurate and reliable information based on the evidence provided by the agents and their discussions.\n",
      "Conclude: Provide a final, well-reasoned answer to the question or task. Your conclusion should reflect the consensus (if one exists) or the most credible and well-supported answer.\n",
      "Based on the provided answers from all agents, summarize the final decision clearly. You should only return the final answer in this dictionary format: {\"Answer\": <Your final answer here>}. Don't give other information.\n",
      "==================== General Agent: \n",
      "The authors compared the performance of deep LSTM models with other models, including:\n",
      "\n",
      "1. Amap model further trained with Amap dataset\n",
      "2. Shenma model trained with sMBR on Amap dataset\n",
      "3. Shenma model + Amap sMBR\n",
      "\n",
      "These models were compared to evaluate the effectiveness of transfer learning and the use of sMBR for improving recognition accuracy in the context of large vocabulary continuous speech recognition (LVCSR).\n",
      "==================== Text Agent: \n",
      "Based on the text, the authors compared the following LSTM models:\n",
      "\n",
      "1. 6-layers model\n",
      "2. 7-layers model\n",
      "3. 8-layers model\n",
      "4. 9-layers model\n",
      "\n",
      "They also compared the performance of these models with and without layer-wise training, Xavier initialization, and sequence discriminative learning (sMBR). Additionally, they compared the performance of a 2-layers model that was trained with and without distillation, and a 2-layers model that was trained with transfer learning using the Shenma model.\n",
      "==================== Image Agent: \n",
      "The authors compared their deep LSTM models with conventional LSTM models.\n",
      "==================== Response Chat: \n",
      "{\"Answer\": \"The authors compared the performance of deep LSTM models with other models, including a 6-layers model, a 7-layers model, an 8-layers model, and a 9-layers model. They also compared the performance of these models with and without layer-wise training, Xavier initialization, and sequence discriminative learning (sMBR). Additionally, they compared the performance of a 2-layers model that was trained with and without distillation, and a 2-layers model that was trained with transfer learning using the Shenma model.\"}\n"
     ]
    }
   ],
   "source": [
    "correct_df = df[df['binary_correctness'] == 1]\n",
    "sample_result = ExperimentResult.from_json(correct_df['ans_2_message'].iloc[cnt])\n",
    "cnt += 1\n",
    "print(sample_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdocagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
