{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('results/PaperTab/rag1/2025-05-19-08-41_results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== User Prompt: \n",
      "You are tasked with summarizing and evaluating the collective responses provided by multiple agents. You have access to the following information:\n",
      "Answers: The individual answers from all agents.\n",
      "Using this information, perform the following tasks:\n",
      "Analyze: Evaluate the quality, consistency, and relevance of each answer. Identify commonalities, discrepancies, or gaps in reasoning.\n",
      "Synthesize: Summarize the most accurate and reliable information based on the evidence provided by the agents and their discussions.\n",
      "Conclude: Provide a final, well-reasoned answer to the question or task. Your conclusion should reflect the consensus (if one exists) or the most credible and well-supported answer.\n",
      "Based on the provided answers from all agents, summarize the final decision clearly. You should only return the final answer in this dictionary format: {\"Answer\": <Your final answer here>}. Don't give other information.\n",
      "==================== General Agent: \n",
      "The datasets used in the experiments are ShapeWorldICE datasets. These datasets are developed to examine different aspects of reasoning abilities in image captioning tasks. The datasets are designed with a similar idea to the \"skill tasks\" in the bAbI framework.\n",
      "==================== Text Agent: \n",
      "To answer the question, I will extract key details related to the datasets used in the text.\n",
      "\n",
      "According to the text, the ShapeWorldICE datasets are used, which have a similar idea to the \"skill tasks\" in the bAbI framework. The text mentions three different types of captioning tasks, each focusing on a distinct aspect of reasoning abilities:\n",
      "\n",
      "1. Existential descriptions\n",
      "2. Spatial descriptions\n",
      "3. Quantiﬁcation descriptions\n",
      "\n",
      "Each type of dataset has two variants, enabling different levels of visual complexity or specific aspects of the same reasoning type. The datasets consist of:\n",
      "\n",
      "* Around 200k training instances\n",
      "* 4,096 validation instances\n",
      "* 4,096 test instances\n",
      "\n",
      "Each training instance consists of an image and a reference caption. At test time, only the test images are available to the evaluated models.\n",
      "==================== Image Agent: \n",
      "The datasets used are the ShapeWorldICE datasets, with a similar idea to the \"skill tasks\" in the bAbI framework. The specific datasets mentioned are Existential-MultiShapes.\n",
      "==================== Response Chat: \n",
      "{\"Answer\": \"The ShapeWorldICE datasets are used in the experiments, which are designed to examine different aspects of reasoning abilities in image captioning tasks. The datasets have a similar idea to the \"skill tasks\" in the bAbI framework and consist of three types of captioning tasks: Existential descriptions, Spatial descriptions, and Quantification descriptions. Each type of dataset has two variants, enabling different levels of visual complexity or specific aspects of the same reasoning type. The datasets consist of around 200k training instances, 4,096 validation instances, and 4,096 test instances. Each training instance consists of an image and a reference caption, and at test time, only the test images are available to the evaluated models.\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    user_chat: str\n",
    "    response_chat: str\n",
    "\n",
    "    user_prompt: str\n",
    "    general_agent: str\n",
    "    text_agent: str\n",
    "    image_agent: str\n",
    "\n",
    "    @staticmethod\n",
    "    def from_json(user_ai_interaction: dict):\n",
    "        user_chat = user_ai_interaction[0]['content'][0]['text']\n",
    "        response_chat = user_ai_interaction[1]['content'][0]['text']\n",
    "        \n",
    "        # Extract agent responses\n",
    "        user_prompt = \"\"\n",
    "        general_agent = \"\"\n",
    "        text_agent = \"\"\n",
    "        image_agent = \"\"\n",
    "        \n",
    "        if \"General Agent:\" in user_chat:\n",
    "            parts = user_chat.split(\"General Agent:\")\n",
    "            if len(parts) > 1:\n",
    "                user_prompt = parts[0].strip()\n",
    "                general_part = parts[1].split(\"Text Agent:\")[0].strip()\n",
    "                general_agent = general_part\n",
    "                \n",
    "        if \"Text Agent:\" in user_chat:\n",
    "            parts = user_chat.split(\"Text Agent:\")\n",
    "            if len(parts) > 1:\n",
    "                text_part = parts[1].split(\"Image Agent:\")[0].strip()\n",
    "                text_agent = text_part\n",
    "                \n",
    "        if \"Image Agent:\" in user_chat:\n",
    "            parts = user_chat.split(\"Image Agent:\")\n",
    "            if len(parts) > 1:\n",
    "                image_part = parts[1].strip()\n",
    "                image_agent = image_part\n",
    "        \n",
    "        return ExperimentResult(\n",
    "            user_chat=user_chat,\n",
    "            user_prompt=user_prompt,\n",
    "            general_agent=general_agent,\n",
    "            text_agent=text_agent,\n",
    "            image_agent=image_agent,\n",
    "            response_chat=response_chat\n",
    "        )\n",
    "\n",
    "    def __str__(self):\n",
    "        return \\\n",
    "            f\"{'='*20} User Prompt: \\n{self.user_prompt}\\n\" \\\n",
    "            f\"{'='*20} General Agent: \\n{self.general_agent}\\n\" \\\n",
    "            f\"{'='*20} Text Agent: \\n{self.text_agent}\\n\" \\\n",
    "            f\"{'='*20} Image Agent: \\n{self.image_agent}\\n\" \\\n",
    "            f\"{'='*20} Response Chat: \\n{self.response_chat}\"\n",
    "\n",
    "sample_result = ExperimentResult.from_json(df['ans_2_message'].iloc[18])\n",
    "print(sample_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== User Prompt: \n",
      "You are tasked with summarizing and evaluating the collective responses provided by multiple agents. You have access to the following information:\n",
      "Answers: The individual answers from all agents.\n",
      "Using this information, perform the following tasks:\n",
      "Analyze: Evaluate the quality, consistency, and relevance of each answer. Identify commonalities, discrepancies, or gaps in reasoning.\n",
      "Synthesize: Summarize the most accurate and reliable information based on the evidence provided by the agents and their discussions.\n",
      "Conclude: Provide a final, well-reasoned answer to the question or task. Your conclusion should reflect the consensus (if one exists) or the most credible and well-supported answer.\n",
      "Based on the provided answers from all agents, summarize the final decision clearly. You should only return the final answer in this dictionary format: {\"Answer\": <Your final answer here>}. Don't give other information.\n",
      "==================== General Agent: \n",
      "The authors compared the performance of deep LSTM models with other models, including:\n",
      "\n",
      "1. Amap model further trained with Amap dataset\n",
      "2. Shenma model trained with sMBR on Amap dataset\n",
      "3. Shenma model + Amap sMBR\n",
      "\n",
      "These models were compared to evaluate the effectiveness of transfer learning and the use of sMBR for improving recognition accuracy in the context of large vocabulary continuous speech recognition (LVCSR).\n",
      "==================== Text Agent: \n",
      "Based on the text, the authors compared the following LSTM models:\n",
      "\n",
      "1. 6-layers model\n",
      "2. 7-layers model\n",
      "3. 8-layers model\n",
      "4. 9-layers model\n",
      "\n",
      "They also compared the performance of these models with and without layer-wise training, Xavier initialization, and sequence discriminative learning (sMBR). Additionally, they compared the performance of a 2-layers model that was trained with and without distillation, and a 2-layers model that was trained with transfer learning using the Shenma model.\n",
      "==================== Image Agent: \n",
      "The authors compared their deep LSTM models with conventional LSTM models.\n",
      "==================== Response Chat: \n",
      "{\"Answer\": \"The authors compared the performance of deep LSTM models with other models, including a 6-layers model, a 7-layers model, an 8-layers model, and a 9-layers model. They also compared the performance of these models with and without layer-wise training, Xavier initialization, and sequence discriminative learning (sMBR). Additionally, they compared the performance of a 2-layers model that was trained with and without distillation, and a 2-layers model that was trained with transfer learning using the Shenma model.\"}\n"
     ]
    }
   ],
   "source": [
    "correct_df = df[df['binary_correctness'] == 1]\n",
    "sample_result = ExperimentResult.from_json(correct_df['ans_2_message'].iloc[cnt])\n",
    "cnt += 1\n",
    "print(sample_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23656/1083039437.py:7: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  hydra.initialize(config_path=\"config\")\n"
     ]
    }
   ],
   "source": [
    "from mydatasets.base_dataset import BaseDataset\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "\n",
    "# Initialize Hydra\n",
    "hydra.initialize(config_path=\"config\")\n",
    "\n",
    "# Compose the configuration\n",
    "cfg = hydra.compose(config_name=\"ptab\")\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cfg.mdoc_agent.cuda_visible_devices\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n",
    "\n",
    "# Create dataset instance\n",
    "dataset = BaseDataset(cfg.dataset)\n",
    "samples = dataset.load_data(use_retreival=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== User Prompt: \n",
      "You are tasked with summarizing and evaluating the collective responses provided by multiple agents. You have access to the following information:\n",
      "Answers: The individual answers from all agents.\n",
      "Using this information, perform the following tasks:\n",
      "Analyze: Evaluate the quality, consistency, and relevance of each answer. Identify commonalities, discrepancies, or gaps in reasoning.\n",
      "Synthesize: Summarize the most accurate and reliable information based on the evidence provided by the agents and their discussions.\n",
      "Conclude: Provide a final, well-reasoned answer to the question or task. Your conclusion should reflect the consensus (if one exists) or the most credible and well-supported answer.\n",
      "Based on the provided answers from all agents, summarize the final decision clearly. You should only return the final answer in this dictionary format: {\"Answer\": <Your final answer here>}. Don't give other information.\n",
      "==================== General Agent: \n",
      "The performance of BERT on the task is not directly provided in the text. However, the text describes the experimental setup and the training process for BERT in the context of Spanish clinical data. To determine the performance of BERT on the task, you would need to refer to the results of the experiments conducted with BERT, which are not provided in the text.\n",
      "==================== Text Agent: \n",
      "According to the text, the performance of BERT on the task is not explicitly stated. However, it is mentioned that BERT has shown an \"outstanding performance in NERC-like tasks, improving the start-of-the-art results for almost every dataset and language.\" This suggests that BERT is a strong performer, but the text does not provide specific metrics or results for the task at hand.\n",
      "\n",
      "Additionally, it is mentioned that the experiments were run with an early-stopping patience of 15 epochs, and the model that performed best against the development set was used to produce the reported results. This implies that the performance of BERT was evaluated and compared to other models, but the specific results are not provided.\n",
      "\n",
      "To answer the question accurately, I would say:\n",
      "\n",
      "\"While BERT has shown outstanding performance in NERC-like tasks, the specific performance of BERT on the task at hand is not explicitly stated in the text. However, it is mentioned that the model was evaluated and compared to other models, suggesting that it performed well.\"\n",
      "==================== Image Agent: \n",
      "The performance of BERT on the task is not applicable.\n",
      "==================== Response Chat: \n",
      "{\"Answer\": \"While BERT has shown outstanding performance in NERC-like tasks, the specific performance of BERT on the task at hand is not explicitly stated in the text. However, it is mentioned that the model was evaluated and compared to other models, suggesting that it performed well.\"}\n",
      "SCORE :  0\n",
      "========================================================================================================================================================================================================\n",
      "{'doc_id': '2003.03106.pdf', 'q_uid': '66c96c297c2cffdf5013bab5e95b59101cb38655', 'question': 'What is the performance of BERT on the task?', 'answer': 'F1 scores are:\\nHUBES-PHI: Detection(0.965), Classification relaxed (0.95), Classification strict (0.937)\\nMedoccan: Detection(0.972), Classification (0.967)', 'answer_2': 'BERT remains only 0.3 F1-score points behind, and would have achieved the second position among all the MEDDOCAN shared task competitors. Taking into account that only 3% of the gold labels remain incorrectly annotated,  Table ', 'answer_3': ' ', 'image-top-10-question': [3, 6, 0, 7, 1, 4, 5, 2, 8], 'image-top-10-question_score': [16.313854217529297, 16.140317916870117, 16.004661560058594, 15.992659568786621, 15.928020477294922, 15.807117462158203, 15.057018280029297, 14.821084976196289, 14.101493835449219], 'text-index-path-question': '.ragatouille/colbert/indexes/PaperTab-question-2003.03106.pdf', 'text-top-10-question': [3, 0, 7, 3, 0, 5, 6, 3, 7, 1], 'text-top-10-question_score': [24.015625, 23.9375, 22.984375, 22.1875, 21.890625, 21.515625, 21.5, 21.015625, 20.859375, 20.65625]}\n",
      "question:  What is the performance of BERT on the task?\n",
      "texts:  ['– preﬁxes and sufﬁxes of 2 and 3 characters; – the length of the token in characters and the length of the sentence in tokens; – whether the token is all-letters, a number, or a se- quence of punctuation marks; – whether the token contains the character ‘@’; – whether the token is the start or end of the sentence; – the token’s casing and the ratio of uppercase charac- ters, digits, and punctuation marks to its length; – and, the lemma, part-of-speech tag, and named-entity tag given by ixa-pipes4 (Agerri et al., 2014) upon analysing the sentence the token belongs to. Noticeably, none of the features used to train the CRF clas- siﬁer is domain-dependent. However, the latter group of features is language dependent. 3.2.3. spaCy spaCy5 is a widely used NLP library that implements state- of-the-art text processing pipelines, including a sequence- labelling pipeline similar to the one described by Strubell et al. (2017). spaCy offers several pre-trained models in Spanish, which perform basic NLP tasks such as Named Entity Recognition (NER). In this paper, we have trained a new NER model to detect NUBES-PHI labels. For this purpose, the new model uses all the labels of the train- ing corpus coded with its context at sentence level. The network optimisation parameters and dropout values are the ones recommended in the documentation for small datasets6. Finally, the model is trained using batches of size 64. No more features are included, so the classiﬁer is language-dependent but not domain-dependent. 3.2.4. BERT As introduced earlier, BERT has shown an outstanding performance in NERC-like tasks, improving the start-of- the-art results for almost every dataset and language. We take the same approach here, by using the model BERT- Base Multilingual Cased7 with a Fully Connected (FC) layer on top to perform a ﬁne-tuning of the whole model for an anonymisation task in Spanish clinical data. Our implementation is built on PyTorch8 and the PyTorch- Transformers library9 (Wolf et al., 2019). The training phase consists in the following steps (roughly depicted in Figure 1): 1. Pre-processing: since we are relying on a pre-trained BERT model, we must match the same conﬁguration by using a speciﬁc tokenisation and vocabulary. BERT also needs that the inputs contains special tokens to signal the beginning and the end of each sequence. 2. Fine-tuning: the pre-processed sequence is fed into the model. BERT outputs the contextual embeddings that encode each of the inputted tokens. This embed- ding representation for each token is fed into the FC 4https://ixa2.si.ehu.es/ixa-pipes 5https://spacy.io 6https://spacy.io/usage/training 7https://github.com/google-research/bert 8https://pytorch.org 9https://github.com/huggingface/transformers Figure 1: Pre-trained BERT with a Fully Connected layer on top to perform the ﬁne-tuning linear layer after a dropout layer (with a 0.1 dropout probability), which in turn outputs the logits for each possible class. The cross-entropy loss function is cal- culated comparing the logits and the gold labels, and the error is back-propagated to adjust the model pa- rameters. We have trained the model using an AdamW optimiser (Loshchilov and Hutter, 2019) with the learning rate set to 3e-5, as recommended by Devlin et al. (2019), and with a gradient clipping of 1.0. We also applied a learning-rate scheduler that warms up the learning rate from zero to its maximum value as the training progresses, which is also a common practice. For each experiment set proposed below, the training was run with an early-stopping patience of 15 epochs. Then, the model that performed best against the development set was used to produce the reported results. The experiments were run on a 64-core server with operat- ing system Ubuntu 16.04, 250GB of RAM memory, and 4 GeForce RTX 2080 GPUs with 11GB of memory. The max- imum sequence length was set at 500 and the batch size at 12. In this setting, each epoch –a full pass through all the training data– required about 10 minutes to complete. 3.3. Experimental design We have conducted experiments with BERT in the two datasets of Spanish clinical narrative presented in Section 3.1. The ﬁrst experiment set uses NUBES-PHI, a corpus of real medical reports manually annotated with sensitive information. Because this corpus is not publicly available, and in order to compare the BERT-based model to other re- lated published systems, the second set of experiments uses the MEDDOCAN 2019 shared task competition dataset. The following sections provide greater detail about the two experimental setups. 3.3.1. Experiment A: NUBES-PHI In this experiment set, we evaluate all the systems presented in Section 3.2., namely, the rule-based baseline, the CRF ']\n",
      "images:  ['./tmp/PaperTab/2003.03106_3.png']\n"
     ]
    }
   ],
   "source": [
    "sample_index = 4\n",
    "sample = samples[sample_index]\n",
    "\n",
    "sample_result = ExperimentResult.from_json(df['ans_2_message'].iloc[sample_index])\n",
    "print(sample_result)\n",
    "print('SCORE : ', df['binary_correctness'].iloc[sample_index])\n",
    "\n",
    "print('='*200)\n",
    "\n",
    "print(sample)\n",
    "question, texts, images = dataset.load_sample_retrieval_data(sample)\n",
    "print('question: ', question)\n",
    "print('texts: ', texts)\n",
    "print('images: ', images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Reflection Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'class_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 133\u001b[39m\n\u001b[32m    130\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mPYTORCH_CUDA_ALLOC_CONF\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mmax_split_size_mb:64\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m cfg = hydra.compose(config_name=\u001b[33m\"\u001b[39m\u001b[33mbase\u001b[39m\u001b[33m\"\u001b[39m)    \n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m mdoc_agent = \u001b[43mReflectionAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreflection_agent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m mdoc_agent.predict_dataset(dataset)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mReflectionAgent.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/MDocAgent/agents/multi_agent_system.py:16\u001b[39m, in \u001b[36mMultiAgentSystem.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mself\u001b[39m.models:\u001b[38;5;28mdict\u001b[39m = {}\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent_config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.agents:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43magent_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclass_name\u001b[49m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.models:\n\u001b[32m     17\u001b[39m         module = importlib.import_module(agent_config.model.module_name)\n\u001b[32m     18\u001b[39m         model_class = \u001b[38;5;28mgetattr\u001b[39m(module, agent_config.model.class_name)\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'class_name'"
     ]
    }
   ],
   "source": [
    "from agents.multi_agent_system import MultiAgentSystem\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import json\n",
    "from mydatasets.base_dataset import BaseDataset\n",
    "import hydra\n",
    "import os\n",
    "\n",
    "class ReflectionAgent(MultiAgentSystem):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "    \n",
    "    def extract_confidence(self, messages: str) -> float:\n",
    "        try:\n",
    "            if \"confidence\" in messages.lower():\n",
    "                confidence_str = messages.split(\"confidence:\")[1].split()[0]\n",
    "                return float(confidence_str)\n",
    "            else:\n",
    "                return 0.0\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting confidence: {e}\")\n",
    "            return 0.0\n",
    "    def predict(self, question: str, texts: list[str], images: list[str]) -> tuple[str, list[dict]]:\n",
    "        general_agent = self.agents[0]\n",
    "\n",
    "        current_iter, current_confidence = 0, 0.0\n",
    "        all_messages = []\n",
    "\n",
    "        while current_iter < self.config.max_reflection_iter:\n",
    "            if current_iter == 0:\n",
    "                current_ans, messages = general_agent.predict(question, texts, images)\n",
    "            else:\n",
    "                reflection_prompt = f\"\"\"\n",
    "                    Based on the following question and my previous answer, please perform a technical analysis and reflection:\n",
    "\n",
    "                    Question: {question}\n",
    "\n",
    "                    My previous answer: {current_ans}\n",
    "\n",
    "                    Please perform a detailed technical analysis:\n",
    "                    1. Technical Accuracy:\n",
    "                    - Are there any technical inaccuracies or misconceptions?\n",
    "                    - Are the technical terms and concepts used correctly?\n",
    "                    - Is the technical depth appropriate for the question?\n",
    "\n",
    "                    2. Technical Completeness:\n",
    "                    - Are there missing technical details or specifications?\n",
    "                    - Are all relevant technical components addressed?\n",
    "                    - Are there any technical dependencies or requirements not mentioned?\n",
    "\n",
    "                    3. Technical Precision:\n",
    "                    - Can the technical explanation be more precise?\n",
    "                    - Are there more specific technical terms that could be used?\n",
    "                    - Are the technical relationships and interactions clearly explained?\n",
    "\n",
    "                    4. Information Gathering:\n",
    "                    - What additional technical information would help improve the answer?\n",
    "                    - Are there specific technical aspects that need more research?\n",
    "                    - What technical details from the provided context (texts/images) could be better utilized?\n",
    "\n",
    "                    If you're uncertain about any technical aspect:\n",
    "                    1. Identify the specific technical points that need clarification\n",
    "                    2. Explain what additional information would help resolve the uncertainty\n",
    "                    3. If possible, gather more information from the provided context (texts/images)\n",
    "                    4. If still uncertain, explicitly state what information is missing and why it's important\n",
    "\n",
    "                    Provide an improved technical answer if needed, or confirm if the original answer is technically sufficient.\n",
    "                    Also, provide your confidence in this technical analysis (0.0 to 1.0) by adding \"Confidence: X.X\" at the end of your response.\n",
    "                \"\"\"\n",
    "                current_ans, messages = general_agent.predict(reflection_prompt, texts, images)\n",
    "\n",
    "            all_messages.extend(messages)\n",
    "            confidence = self.extract_confidence(current_ans)\n",
    "\n",
    "            # Remove confidence score from the answer\n",
    "            if \"confidence:\" in current_ans.lower():\n",
    "                current_ans = current_ans.lower().split(\"confidence:\")[0].strip()\n",
    "            \n",
    "            # Check if we should stop reflecting\n",
    "            if confidence >= self.confidence_threshold:\n",
    "                break\n",
    "                \n",
    "            current_iter += 1\n",
    "        \n",
    "        return current_ans, all_messages\n",
    "\n",
    "    def predict_dataset(self, dataset:BaseDataset, resume_path = None):\n",
    "        samples = dataset.load_data(use_retreival=True)\n",
    "        if resume_path:\n",
    "            assert os.path.exists(resume_path)\n",
    "            with open(resume_path, 'r') as f:\n",
    "                samples = json.load(f)\n",
    "            \n",
    "        sample_no = 0\n",
    "        for sample in tqdm(samples):\n",
    "            if resume_path and self.config.ans_key in sample:\n",
    "                continue\n",
    "            question, texts, images = dataset.load_sample_retrieval_data(sample)\n",
    "            try:\n",
    "                final_ans, final_messages = self.predict(question, texts, images)\n",
    "            except RuntimeError as e:\n",
    "                print(e)\n",
    "                if \"out of memory\" in str(e):\n",
    "                    torch.cuda.empty_cache()\n",
    "                final_ans, final_messages = None, None\n",
    "            \n",
    "            # Sample\n",
    "            return final_messages\n",
    "\n",
    "            sample[self.config.ans_key] = final_ans\n",
    "            if self.config.save_message:\n",
    "                sample[self.config.ans_key+\"_message\"] = final_messages\n",
    "            torch.cuda.empty_cache()\n",
    "            self.clean_messages()\n",
    "            \n",
    "            sample_no += 1\n",
    "            if sample_no % self.config.save_freq == 0:\n",
    "                path = dataset.dump_reults(samples)\n",
    "                print(f\"Save {sample_no} results to {path}.\")\n",
    "        path = dataset.dump_reults(samples)\n",
    "        print(f\"Save final results to {path}.\")\n",
    "\n",
    "hydra.initialize(config_path=\"config\", version_base=\"1.2\")\n",
    "\n",
    "# Compose the configuration\n",
    "cfg = hydra.compose(config_name=\"base\")\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cfg.mdoc_agent.cuda_visible_devices\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n",
    "\n",
    "cfg = hydra.compose(config_name=\"base\")    \n",
    "mdoc_agent = ReflectionAgent(cfg.reflection_agent)\n",
    "mdoc_agent.predict_dataset(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdocagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
